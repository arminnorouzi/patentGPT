<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230007393A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230007393</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17646401</doc-number><date>20211229</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>CN</country><doc-number>202110739195.1</doc-number><date>20210630</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>R</subclass><main-group>3</main-group><subgroup>04</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>21</main-group><subgroup>0216</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>25</main-group><subgroup>78</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>25</main-group><subgroup>21</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>R</subclass><main-group>3</main-group><subgroup>04</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>21</main-group><subgroup>0216</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>25</main-group><subgroup>78</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>25</main-group><subgroup>21</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>2021</main-group><subgroup>02165</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">SOUND PROCESSING METHOD, ELECTRONIC DEVICE AND STORAGE MEDIUM</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Beijing Xiaomi Mobile Software Co., Ltd.</orgname><address><city>Beijing</city><country>CN</country></address></addressbook><residence><country>CN</country></residence></us-applicant><us-applicant sequence="01" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Beijing Xiaomi Pinecone Electronics Co., Ltd.</orgname><address><city>Beijing</city><country>CN</country></address></addressbook><residence><country>CN</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>CAO</last-name><first-name>Chenbin</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>HE</last-name><first-name>Mengnan</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A sound processing method includes: determining a vector of a first residual signal according to a first signal vector and a second signal vector, the first signal vector including a first voice signal and a first noise signal input into the first microphone, the second signal vector including a second voice signal and a second noise signal input into the second microphone, and the first residual signal including the second noise signal and a residual voice signal; determining a gain function of a current frame according to the vector of the first residual signal and the first signal vector; and determining a first voice signal of the current frame according to the first signal vector and the gain function of the current frame.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="80.43mm" wi="134.79mm" file="US20230007393A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="186.86mm" wi="136.82mm" file="US20230007393A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="200.24mm" wi="136.82mm" file="US20230007393A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="215.14mm" wi="132.00mm" file="US20230007393A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION</heading><p id="p-0002" num="0001">The present application claims priority to Chinese Patent Application No. 2021107391951, filed on Jun. 30, 2021. The entire contents of the above-listed application is hereby incorporated by reference for all purposes.</p><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">When terminal devices such as mobile phones perform voice communication and human-machine voice interaction, when a user inputs voice into a microphone, noise will also enter the microphone synchronously, thus forming an input signal in which voice signals and noise signals are mixed. In the related art, an adaptive filter is used to eliminate the above-mentioned noise, but the adaptive filter has a poor effect on noise elimination, so a purer voice signal cannot be obtained.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0004" num="0003">According to a first aspect of an example of the present disclosure, a sound processing method is provided, applied to a terminal device. The terminal device includes a first microphone and a second microphone, and the method includes:</p><p id="p-0005" num="0004">determining a vector of a first residual signal according to a first signal vector and a second signal vector, the first signal vector being input signals of the first microphone and including a first voice signal and a first noise signal, the second signal vector being input signals of the second microphone and including a second voice signal and a second noise signal, and the first residual signal including the second noise signal and a residual voice signal; </p><p id="p-0006" num="0005">determining a gain function of a current frame according to the vector of the first residual signal and the first signal vector; and</p><p id="p-0007" num="0006">determining a first voice signal of the current frame according to the first signal vector and the gain function of the current frame.</p><p id="p-0008" num="0007">According to a second aspect of an example of the present disclosure, an electronic device is provided, including a memory, a processor, a first microphone and a second microphone. The memory is configured to store a computer instruction that may be run on the processor, the processor is configured to realize a sound processing method when executing the computer instruction, and the sound processing method includes:</p><p id="p-0009" num="0008">determining a vector of a first residual signal according to a first signal vector and a second signal vector, the first signal vector including a first voice signal and a first noise signal input into the first microphone, the second signal vector including a second voice signal and a second noise signal input into the second microphone, and the first residual signal including the second noise signal and a residual voice signal;</p><p id="p-0010" num="0009">determining a gain function of a current frame according to the vector of the first residual signal and the first signal vector; and</p><p id="p-0011" num="0010">determining a first voice signal of the current frame according to the first signal vector and the gain function of the current frame.</p><p id="p-0012" num="0011">According to a third aspect of an example of the present disclosure, a non-transitory computer readable storage medium is provided, storing a computer program. The program realizes a sound processing method when being executed by a processor. The method is applied to a terminal device, the terminal device includes a first microphone and a second microphone, and the method includes:</p><p id="p-0013" num="0012">determining a vector of a first residual signal according to a first signal vector and a second signal vector, the first signal vector including a first voice signal and a first noise signal  input into the first microphone, the second signal vector including a second voice signal and a second noise signal input into the second microphone, and the first residual signal including the second noise signal and a residual voice signal;</p><p id="p-0014" num="0013">determining a gain function of a current frame according to the vector of the first residual signal and the first signal vector; and</p><p id="p-0015" num="0014">determining a first voice signal of the current frame according to the first signal vector and the gain function of the current frame.</p><p id="p-0016" num="0015">It should be understood that the above general description and following detailed descriptions are merely exemplary and explanatory and do not limit the present disclosure.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE FIGURES</heading><p id="p-0017" num="0016">The drawings herein are incorporated into the specification and constitute a part of the specification, show examples in accordance with the present disclosure, and together with the specification are used to explain the principle of the present disclosure.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a flow chart of a sound processing method shown by an example of the present disclosure.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a flow chart of determining a vector of a first residual signal shown by an example of the present disclosure.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flow chart of determining a vector of a gain function shown by an example of the present disclosure.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a schematic diagram of an analysis window shown by an example of the present disclosure.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a schematic structural diagram of a sound processing apparatus shown by an example of the present disclosure. </p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a block diagram of an electronic device shown by an example of the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0024" num="0023">Some examples will be described in detail here, and their instances are shown in the accompanying drawings. When the following description refers to the accompanying drawings, unless otherwise indicated, the same numbers in different drawings represent the same or similar elements. The implementations described in the following examples do not represent all implementations consistent with the present disclosure. Rather, they are merely examples of an apparatus and a method consistent with some aspects of the present disclosure.</p><p id="p-0025" num="0024">The terms used in the present disclosure are only for the purpose of describing specific examples, and are not intended to limit the present disclosure. Singular forms of &#x201c;a&#x201d;, &#x201c;said&#x201d; and &#x201c;the&#x201d; used in the present disclosure are also intended to include plural forms, unless the context clearly indicates other meanings. It should also be understood that the term &#x201c;and/or&#x201d; used herein refers to and includes any or all possible combinations of one or more associated listed items.</p><p id="p-0026" num="0025">It should be understood that although the terms first, second, third, etc. may be used in the disclosure to describe various information, the information should not be limited to these terms. These terms are only used to distinguish the same type of information from each other. For example, without departing from the scope of the present disclosure, first information may also be referred to as second information, and similarly, second information may also be referred to as first information. Depending on the context, the word &#x201c;if&#x201d; used herein may be interpreted as &#x201c;at the moment of&#x201d; or &#x201c;when&#x201d; or &#x201c;in response to determining&#x201d;.</p><p id="p-0027" num="0026">Traditional noise suppression methods on mobile phones are generally based on structures of adaptive blocking matrix (BM), adaptive noise canceller (ANC), and post-filtering (PF). The adaptive blocking matrix eliminates a target voice signal in an auxiliary channel and  provides a noise reference signal for the ANC. The adaptive noise canceller eliminates a coherent noise in a main channel. Post-filtering estimates a noise signal in an ANC output signal, and uses spectral enhancement methods such as MMSE or Wiener filtering to further suppress a noise, thus obtaining an enhanced signal with a higher signal-to-noise ratio (SNR).</p><p id="p-0028" num="0027">Traditional BM and ANC are usually realized by using NLMS or RLS adaptive filters. An NLMS algorithm needs to design a variable step size mechanism to control an adaptive rate of a filter to achieve the objective of fast convergence and smaller steady-state errors at the same time, but this objective is almost impossible for practical applications. An RLS algorithm does not need to additionally design variable step sizes, but it does not consider a process noise; and under an influence of actions such as holding and moving of a mobile phone, a transfer function between two microphone channels may frequently change, so a rapid update strategy of an adaptive filter is required. The RLS algorithm is not so robust in dealing with the two problems. The ANC is only applicable to processing the coherent noises in general, that is, a noise source is relatively close to the mobile phone, and direct sound from the noise source to the microphones prevails. A noise environment of mobile phone voice calls is generally a diffuse field, that is, a plurality of noise sources are far away from the microphones of the mobile phone and require multiple spatial reflections to reach the mobile phone. Thus, the ANC is almost ineffective in practical applications.</p><p id="p-0029" num="0028">Based on that, in a first aspect, at least one example of the present disclosure provides a sound processing method. With reference to <figref idref="DRAWINGS">FIG. <b>1</b></figref> which shows a flow of the method, the method includes step S<b>101</b> to step S<b>104</b>.</p><p id="p-0030" num="0029">The sound processing method is applied to a terminal device, and the terminal device may be a mobile phone, a tablet computer or other terminal devices with a communication function and/or a man-machine interaction function. The terminal device includes a first microphone and a second microphone. The first microphone is located at a bottom of the mobile phone, serves as a main channel, is mainly configured to collect a voice signal of a target speaker, and has a higher  signal-to-noise ratio (SNR). The second microphone is located at a top of the mobile phone, serves as an auxiliary channel, is mainly configured to collect an ambient noise signal, including part of voice signals of the target speaker, and has a lower SNR. The purpose of the sound processing method is to use an input signal of the second microphone to eliminate noise from an input signal of the first microphone, thus obtaining a relatively pure voice signal.</p><p id="p-0031" num="0030">The input signals of the microphones are each composed of a near-end signal and a stereo echo signal:</p><p id="p-0032" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>d</i><sub>1</sub>(<i>n</i>)=<i>s</i><sub>1</sub>(<i>n</i>)+<i>v</i><sub>1</sub>(<i>n</i>)+<i>y</i><sub>1</sub>(<i>n</i>)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0033" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>d</i><sub>2</sub>(<i>n</i>)=<i>s</i><sub>2</sub>(<i>n</i>)+<i>v</i><sub>2</sub>(<i>n</i>)+<i>y</i><sub>2</sub>(<i>n</i>)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0034" num="0031">where subscripts i={1,2} represent microphone indexes, 1 is the main channel, 2 is the auxiliary channel, d<sub>i</sub>(n) is an input signal of a microphone, a signal of a near-end speaker s<sub>i</sub>(n) and a background noise v<sub>i</sub>(n) constitute a near-end signal and y<sub>i</sub>(n) is an echo signal. Because noise elimination and suppression is usually performed in an echo-free period or in a case that an echo has been eliminated, an influence of the echo signals does not need to be considered in a subsequent process.</p><p id="p-0035" num="0032">Voice calls are generally used in near-field scenarios, that is, a distance between the target speaker and the microphones of the mobile phone is relatively short, and a relationship between target speaker signals picked up by the two microphones may be expressed through acoustic impulse response (AIR):</p><p id="p-0036" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>s</i><sub>2</sub>(<i>n</i>)=h<sub>t</sub>(<i>n</i>)<i>s</i><sub>1</sub>(<i>n&#x2212;t</i>)=<i>h</i><sup>T</sup>(<i>n</i>)<i>s</i><sub>1</sub>(<i>n</i>)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0037" num="0033">where s<sub>1</sub>(n) and s<sub>2</sub>(n) respectively represents the target speaker signals of the main channel and the auxiliary channel, h(n) is an acoustic transfer function between them, h(n)=[h<sub>0</sub>, h<sub>1</sub>, . . . , h<sub>L&#x2212;1</sub>]<sup>T</sup>, L is a length of the transfer function, and s<sub>1</sub>(n)=[s<sub>1</sub>(n), s<sub>1</sub>(n&#x2212;1), . . . , s<sub>1</sub>(n&#x2212;L+1)]<sup>T </sup>is a vector form of the target speaker signal of the main channel. </p><p id="p-0038" num="0034">For diffuse field noise signals picked up by the two microphones, a relationship between them cannot be simply expressed through the acoustic impulse response, but noise power spectra of the two microphones are highly similar, so a long-term spectral regression method may be used for modeling.</p><p id="p-0039" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>V</i><sub>1</sub>(<i>n</i>)=&#x3a3;<sub>i=0</sub><sup>N&#x2212;1</sup>&#x3a3;<sub>t</sub><sub><sub2>i</sub2></sub><sub>=i&#xb7;L</sub><sup>(i+1)&#xb7;L&#x2212;1</sup><i>h</i><sub>i,t</sub><sub><sub2>i</sub2></sub>(<i>n</i>)<i>V</i><sub>2</sub>(<i>n&#x2212;t</i><sub>i</sub>)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0040" num="0035">where V<sub>1</sub>(n) and V<sub>2</sub>(n) respectively represents noise power spectra of the main channel and the auxiliary channel, and h<sub>i,t</sub>(n) is a relative convolution transfer function between them.</p><p id="p-0041" num="0036">In step S<b>101</b>, a vector of a first residual signal is determined according to a first signal vector and a second signal vector. The first signal vector includes a first voice signal and a first noise signal input into the first microphone, the second signal vector includes a second voice signal and a second noise signal input into the second microphone, and the first residual signal includes the second noise signal and a residual voice signal.</p><p id="p-0042" num="0037">The first microphone and the second microphone are in a same environment, so a signal source of the first voice signal and a signal source of the second voice signal are identical, but a difference between distances from the signal source to the two microphones causes a difference between the first voice signal and the second voice signal. Similarly, a signal source of the first noise signal and a signal source of the second noise signal are identical, but the difference between distances from the signal source to the two microphones causes a difference between the first noise signal and the second noise signal. The first residual signal may be obtained from the input signals of the two microphones through an offset manner. The first residual signal approximates a noise signal of the auxiliary channel, that is, the second noise signal.</p><p id="p-0043" num="0038">In step S<b>102</b>, a gain function of a current frame is determined according to the vector of the first residual signal and the first signal vector. </p><p id="p-0044" num="0039">The gain function is used to perform differential gain on the first residual signal, that is, perform forward gain on the first voice signal in the first residual signal, and perform backward gain on the second voice signal in the first residual signal. Thus, an intensity difference between the first voice signal and the first noise signal is increased, and the signal-to-noise ratio is increased, thus obtaining a pure first voice signal to the greatest extent.</p><p id="p-0045" num="0040">In step S<b>103</b>, a first voice signal of the current frame is determined according to the first signal vector and the gain function of the current frame.</p><p id="p-0046" num="0041">In the step, a product of multiplying the first signal vector by the gain function of the current frame may be converted from a frequency domain form to a time domain form, so as to form the first voice signal of the current frame in the time domain form. For example, a form of inverse Fourier transform as follows may be adopted to perform the conversion from the frequency domain form to the time domain form:</p><p id="p-0047" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>e=if ft</i>(<i>D</i><sub>1</sub>(<i>l</i>).*<i>G</i>(<i>l</i>)).*<i>win </i><?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0048" num="0042">where D<sub>1</sub>(l) and G(l) are respectively vector forms of D<sub>1</sub>(l, k) and G(l, k), e is a time domain enhanced signal with noise eliminated, and if ft(&#x22c5;) is inverse Fourier transform.</p><p id="p-0049" num="0043">In the present disclosure, the first residual signal including the second noise signal and the residual voice signal is determined according to the first signal vector composed of the first voice signal and the first noise signal which are input into the first microphone as well as the second signal vector composed of the second voice signal and the second noise signal which are input into the second microphone; then the gain function of the current frame is determined according to the vector of the first residual signal and the first signal vector; and finally the first voice signal of the current frame is determined according to the first signal vector and the above-mentioned gain function of the current frame. Because the first microphone and the second microphone are at different locations, their ratios of voices to noises are in opposite trends. Thus, noise estimation and suppression may be performed for the first signal vector and the second signal  vector by using a target voice and interference noise offsetting method, thus improving an effect of eliminating noises in the microphone, and a pure voice signal may be obtained.</p><p id="p-0050" num="0044">In some examples of the present disclosure, the vector of the first residual signal may be determined according to the first signal vector and the second signal vector in the manner shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, including step S<b>201</b> to step S<b>203</b>.</p><p id="p-0051" num="0045">In step S<b>201</b>, the first signal vector and the second signal vector are obtained. The first signal vector includes sample points of a first quantity, and the second signal vector includes sample points of a second quantity.</p><p id="p-0052" num="0046">In the step, an input signal of a current frame of the first microphone and an input signal of at least one previous frame of the first microphone may be spliced to form the first signal vector with the quantity of sample points being the first quantity. The first quantity M may represent a length of a spliced signal block. Optionally, signal splicing is performed by using a continuous frame overlap manner to obtain the first signal vector d<sub>1</sub>(l):</p><p id="p-0053" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>d</i><sub>1</sub>(<i>l</i>)=[<i>d</i><sub>1</sub>(<i>n</i>), <i>d</i><sub>1</sub>(<i>n&#x2212;</i>1), . . . , <i>d</i><sub>1</sub>(<i>n&#x2212;M+</i>1)]<sup>T </sup><?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0054" num="0047">where d<sub>1</sub>(n), d<sub>1</sub>(n&#x2212;1), . . . , d<sub>1</sub>(n&#x2212;M +1) are M sample points, and M may be an integer multiple of the quantity R of sample points of each frame of signal.</p><p id="p-0055" num="0048">In the step, an input signal of a current frame of the second microphone and an input signal of at least one previous frame of the second microphone are spliced to form the second signal vector with the quantity of sample points being the second quantity. The second quantity R may represent a length of each frame of signal. Optionally, signal splicing is performed by using a continuous frame overlap manner to obtain the second signal vector d<sub>2</sub>(l):</p><p id="p-0056" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>d</i><sub>2</sub>(<i>l</i>)=[<i>d</i><sub>2</sub>(<i>n</i>), <i>d</i><sub>2</sub>(<i>n&#x2212;</i>1), . . . , <i>d</i><sub>2</sub>(<i>n&#x2212;R+</i>1)]<sup>T </sup><?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0057" num="0049">where d<sub>2</sub>(n), d<sub>2</sub>(n&#x2212;1), . . . , d<sub>2</sub>(n&#x2212;R+1) are R sample points.</p><p id="p-0058" num="0050">In step S<b>202</b>, a vector of a Fourier transform coefficient of the second voice signal is determined according to the first signal vector and a first transfer function of a previous frame. </p><p id="p-0059" num="0051">In the step, d<sub>1</sub>(l) may be converted from a time domain to a frequency domain first, so as to obtain a DFT coefficient of a main channel input signal D<sub>1</sub>(l, k): D<sub>1</sub>(l)=fft(d<sub>1</sub>(l)); and then the vector &#x15c;<sub>2</sub>(l) of the Fourier transform coefficient of the second voice signal is determined according to D<sub>1 </sub>(l, k) and the first transfer function of the previous frame &#x174;<sub>s</sub>(l&#x2212;1, k) based on the following formula: &#x15c;<sub>2</sub>(l)=D<sub>1</sub>(l)&#x174;<sub>s</sub>(l=1, k)</p><p id="p-0060" num="0052">In step S<b>203</b>, the vector of the first residual signal is determined according to the sample points of the second quantity in the second signal vector and in the vector of the Fourier transform coefficient.</p><p id="p-0061" num="0053">In the step, &#x15c;<sub>2</sub>(l) may be converted from a frequency domain to a time domain first: &#x15d;<sub>2</sub>(l)=if ft(&#x15c;<sub>2</sub>(l)), and then the vector v(l) of the first residual signal is obtained based on the following formula: v(l)=d<sub>2</sub>(l)&#x2212;&#x15d;<sub>2</sub>(l, M&#x2212;R+1:M).</p><p id="p-0062" num="0054">Further, after v(l)is obtained, a first transfer function of the current frame may be updated in the following manner.</p><p id="p-0063" num="0055">First, a first Kalman gain coefficient K<sub>S</sub>(l) is determined according to the vector v(l) of the first residual signal, residual signal covariance &#x3d5;<sub>V</sub>(l&#x2212;1) of the previous frame, state estimation error covariance P<sub>V</sub>(l&#x2212;1) of the previous frame, the first signal vector D<sub>1</sub>(l) and a smoothing parameter &#x3b1;.</p><p id="p-0064" num="0056">The first Kalman gain coefficient K<sub>S</sub>(l) may be obtained based on the following formulas in sequence:</p><p id="p-0065" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mrow>  <mrow>   <mrow>    <mi>V</mi>    <mo>&#x2061;</mo>    <mo>(</mo>    <mi>l</mi>    <mo>)</mo>   </mrow>   <mo>=</mo>   <mrow>    <mi>fft</mi>    <mo>&#x2061;</mo>    <mo>(</mo>    <mrow>     <mo>[</mo>     <mrow>      <mn>0</mn>      <mo>;</mo>      <mrow>       <mi>v</mi>       <mo>&#x2061;</mo>       <mo>(</mo>       <mi>l</mi>       <mo>)</mo>      </mrow>     </mrow>     <mo>]</mo>    </mrow>    <mo>)</mo>   </mrow>  </mrow>  <mo>,</mo>  <mrow>   <mrow>    <msub>     <mi>&#x3d5;</mi>     <mi>V</mi>    </msub>    <mo>(</mo>    <mi>l</mi>    <mo>)</mo>   </mrow>   <mo>=</mo>   <mrow>    <mrow>     <mi>&#x3b1;</mi>     <mo>&#x2062;</mo>     <mrow>      <msub>       <mi>&#x3d5;</mi>       <mi>V</mi>      </msub>      <mo>(</mo>      <mrow>       <mi>l</mi>       <mo>-</mo>       <mn>1</mn>      </mrow>      <mo>)</mo>     </mrow>    </mrow>    <mo>+</mo>    <mrow>     <mrow>      <mo>(</mo>      <mrow>       <mn>1</mn>       <mo>-</mo>       <mi>&#x3b1;</mi>      </mrow>      <mo>)</mo>     </mrow>     <mo>&#x2062;</mo>     <msup>      <mrow>       <semantics definitionURL="">        <mo>&#x2758;</mo>        <annotation encoding="Mathematica">"\[LeftBracketingBar]"</annotation>       </semantics>       <mrow>        <mi>V</mi>        <mo>&#x2061;</mo>        <mo>(</mo>        <mi>l</mi>        <mo>)</mo>       </mrow>       <semantics definitionURL="">        <mo>&#x2758;</mo>        <annotation encoding="Mathematica">"\[RightBracketingBar]"</annotation>       </semantics>      </mrow>      <mn>2</mn>     </msup>    </mrow>   </mrow>  </mrow>  <mo>,</mo>  <mtext></mtext>  <mrow>   <mrow>    <mi>and</mi>    <mo>&#x2062;</mo>    <mtext>   </mtext>    <mrow>     <msub>      <mi>K</mi>      <mi>S</mi>     </msub>     <mo>(</mo>     <mi>l</mi>     <mo>)</mo>    </mrow>   </mrow>   <mo>=</mo>   <mrow>    <mrow>     <mi>A</mi>     <mo>&#xb7;</mo>     <mrow>      <msub>       <mi>P</mi>       <mi>V</mi>      </msub>      <mo>(</mo>      <mrow>       <mi>l</mi>       <mo>-</mo>       <mn>1</mn>      </mrow>      <mo>)</mo>     </mrow>    </mrow>    <mo>&#x2062;</mo>    <msup>     <mrow>      <mrow>       <msubsup>        <mi>D</mi>        <mn>1</mn>        <mo>*</mo>       </msubsup>       <mo>(</mo>       <mi>l</mi>       <mo>)</mo>      </mrow>      <mo>[</mo>      <mrow>       <mrow>        <msubsup>         <mi>D</mi>         <mn>1</mn>         <mo>*</mo>        </msubsup>        <mo>(</mo>        <mi>l</mi>        <mo>)</mo>       </mrow>       <mo>+</mo>       <mrow>        <mfrac>         <mi>M</mi>         <mi>R</mi>        </mfrac>        <mo>&#x2062;</mo>        <mrow>         <msub>          <mi>&#x3d5;</mi>          <mi>V</mi>         </msub>         <mo>(</mo>         <mi>l</mi>         <mo>)</mo>        </mrow>       </mrow>      </mrow>      <mo>]</mo>     </mrow>     <mrow>      <mo>-</mo>      <mn>1</mn>     </mrow>    </msup>   </mrow>  </mrow>  <mo>,</mo> </mrow></math></maths></p><p id="p-0066" num="0000">where A is a transition probability and generally takes a value 0&#x3c;&#x3c;A&#x3c;1.</p><p id="p-0067" num="0057">Then the first transfer function &#x174;<sub>S</sub>(l) of the current frame may be determined according to the first Kalman gain coefficient K<sub>S</sub>(l), the first residual signal V(l), and the first transfer function &#x174;<sub>S</sub>(l&#x2212;1)of the previous frame. </p><p id="p-0068" num="0058">The first transfer function of the current frame may be obtained based on the following formulas in sequence: &#x394;W<sub>SU</sub>=K<sub>S</sub>(l)V(l), &#x394;w<sub>s</sub>=if ft(&#x394;W<sub>SU</sub>), &#x394;W<sub>SC</sub>=fft([&#x394;w<sub>s</sub>(1:M&#x2212;R); 0]), and &#x174;<sub>S</sub>(l)=W<sub>S</sub>(l&#x2212;1)+&#x394;W<sub>SC</sub>.</p><p id="p-0069" num="0059">By updating the first transfer function of the current frame, it can be utilized for processing a next frame of signal, because relative to the next frame of signal, the first transfer function of the current frame is the first transfer function of the previous frame. It should be noted that when a processed signal is the first frame, the first transfer function of the previous frame may be randomly preset.</p><p id="p-0070" num="0060">In addition, after v(l) is obtained, a residual signal covariance of the current frame is updated based on the following manner: the residual signal covariance of the current frame is determined according to the first transfer function of the current frame, the first transfer function covariance of the previous frame, the first Kalman gain coefficient, the residual signal covariance of the previous frame, the first quantity and the second quantity.</p><p id="p-0071" num="0061">The residual signal covariance P<sub>V</sub>(l) of the current frame may be obtained based on the following formulas in sequence:</p><p id="p-0072" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mrow>  <mrow>   <mrow>    <msub>     <mi>&#x3d5;</mi>     <mi>WS</mi>    </msub>    <mo>(</mo>    <mi>l</mi>    <mo>)</mo>   </mrow>   <mo>=</mo>   <mrow>    <mrow>     <mi>&#x3b1;</mi>     <mo>&#x2062;</mo>     <mrow>      <msub>       <mi>&#x3d5;</mi>       <mi>WS</mi>      </msub>      <mo>(</mo>      <mrow>       <mi>l</mi>       <mo>-</mo>       <mn>1</mn>      </mrow>      <mo>)</mo>     </mrow>    </mrow>    <mo>+</mo>    <mrow>     <mrow>      <mo>(</mo>      <mrow>       <mn>1</mn>       <mo>-</mo>       <mi>&#x3b1;</mi>      </mrow>      <mo>)</mo>     </mrow>     <mo>&#x2062;</mo>     <msup>      <mrow>       <semantics definitionURL="">        <mo>&#x2758;</mo>        <annotation encoding="Mathematica">"\[LeftBracketingBar]"</annotation>       </semantics>       <mrow>        <msub>         <mover>          <mi>W</mi>          <mo>^</mo>         </mover>         <mi>S</mi>        </msub>        <mo>(</mo>        <mi>l</mi>        <mo>)</mo>       </mrow>       <semantics definitionURL="">        <mo>&#x2758;</mo>        <annotation encoding="Mathematica">"\[RightBracketingBar]"</annotation>       </semantics>      </mrow>      <mn>2</mn>     </msup>    </mrow>   </mrow>  </mrow>  <mo>,</mo>  <mrow>   <mrow>    <msub>     <mi>&#x3d5;</mi>     <mi>&#x394;</mi>    </msub>    <mo>(</mo>    <mi>l</mi>    <mo>)</mo>   </mrow>   <mo>=</mo>   <mrow>    <mrow>     <mo>(</mo>     <mrow>      <mn>1</mn>      <mo>-</mo>      <msup>       <mi>A</mi>       <mn>2</mn>      </msup>     </mrow>     <mo>)</mo>    </mrow>    <mo>&#x2062;</mo>    <mrow>     <msub>      <mi>&#x3d5;</mi>      <mi>WS</mi>     </msub>     <mo>(</mo>     <mi>l</mi>     <mo>)</mo>    </mrow>   </mrow>  </mrow>  <mo>,</mo>  <mtext></mtext>  <mrow>   <mrow>    <mi>and</mi>    <mo>&#x2062;</mo>    <mtext>   </mtext>    <mrow>     <msub>      <mi>P</mi>      <mi>V</mi>     </msub>     <mo>(</mo>     <mi>l</mi>     <mo>)</mo>    </mrow>   </mrow>   <mo>=</mo>   <mrow>    <mrow>     <mrow>      <mi>A</mi>      <mo>&#xb7;</mo>      <mrow>       <mo>[</mo>       <mrow>        <mrow>         <mi>A</mi>         <mo>&#xb7;</mo>         <mi>I</mi>        </mrow>        <mo>-</mo>        <mrow>         <mfrac>          <mi>M</mi>          <mi>R</mi>         </mfrac>         <mo>&#x2062;</mo>         <mrow>          <mi>K</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <mi>l</mi>          <mo>)</mo>         </mrow>         <mo>&#x2062;</mo>         <mrow>          <msub>           <mi>D</mi>           <mn>1</mn>          </msub>          <mo>(</mo>          <mi>l</mi>          <mo>)</mo>         </mrow>        </mrow>       </mrow>       <mo>]</mo>      </mrow>     </mrow>     <mo>&#x2062;</mo>     <mrow>      <msub>       <mi>P</mi>       <mi>V</mi>      </msub>      <mo>(</mo>      <mrow>       <mi>l</mi>       <mo>-</mo>       <mn>1</mn>      </mrow>      <mo>)</mo>     </mrow>    </mrow>    <mo>+</mo>    <mrow>     <msub>      <mi>&#x3d5;</mi>      <mi>&#x394;</mi>     </msub>     <mo>(</mo>     <mi>l</mi>     <mo>)</mo>    </mrow>   </mrow>  </mrow>  <mo>,</mo> </mrow></math></maths></p><p id="p-0073" num="0000">where &#x3d5;<sub>WS</sub>(l) is a covariance of a relative transfer function of a voice between the channels, &#x3b1; is the smoothing parameter, &#x3d5;<sub>&#x394;</sub>(l) is a process noise covariance, P<sub>V</sub>(l) is the state estimation error covariance, and I=[1,1, . . . 1]<sup>T </sup>is a vector composed of 1.</p><p id="p-0074" num="0062">By updating the residual signal covariance of the current frame, it can be utilized for processing the next frame of signal, because relative to the next frame of signal, the residual signal covariance of the current frame is the residual signal covariance of the previous frame. It should be noted that when the processed signal is the first frame, the residual signal covariance of the previous frame may be randomly preset. </p><p id="p-0075" num="0063">In some examples of the present disclosure, the gain function of the current frame may be determined according to the vector of the first residual signal and the first signal vector in the manner shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, including step S<b>301</b> to step S<b>303</b>.</p><p id="p-0076" num="0064">In step S<b>301</b>, the vector of the first residual signal and the first signal vector are converted from a time domain form to a frequency domain form respectively.</p><p id="p-0077" num="0065">The conversion from the time domain form to the frequency domain form may be performed based on Fourier transform as follows:</p><p id="p-0078" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>V</i><sub>2</sub>(<i>l</i>)=<i>fft</i>(<i>v</i><sub>2</sub><i>.* win</i>)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0079" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>D</i><sub>1</sub>(<i>l</i>)=<i>fft</i>(<i>d</i><sub>1</sub><i>.* win</i>)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0080" num="0066">where v<sub>2</sub>(l) is first residual signal containing N sample points, d<sub>1</sub>(l) is the main channel input signal, i.e. the first signal vector, win is a short-term analysis window, and fft(&#x22c5;) is Fourier transform.</p><p id="p-0081" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>v</i><sub>2</sub>(<i>l</i>)=[<i>v</i>(<i>n</i>), <i>v</i>(<i>n&#x2212;</i>1), . . . , <i>v</i>(<i>n&#x2212;N+</i>1)]<sup>T </sup><?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0082" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>d</i><sub>1</sub>(<i>l</i>)=[<i>d</i><sub>1</sub>(<i>n</i>), <i>d</i><sub>1</sub>(n&#x2212;1), . . . , <i>d</i><sub>1</sub>(<i>n&#x2212;N+</i>1)]<sup>T </sup><?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0083" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>win=[</i>0; sqrt(hanning(<i>N&#x2212;</i>1))]<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0084" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>hanning(<i>n</i>)=0.5 [1&#x2212;cos (2 &#x3c0;*<i>n/N</i>)]<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0085" num="0067">where N is a length of an analysis frame, hanning(n) is a hanning window with a length of N&#x2212;1 as shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0086" num="0068">In step S<b>302</b>, a vector of a noise estimation signal is determined according to a posterior state error covariance matrix of the previous frame, a process noise covariance matrix, a second transfer function of the previous frame, the first signal vector, a first residual signal of at least one frame including the current frame and a posterior error variance of the previous frame.</p><p id="p-0087" num="0069">In the step, an apriori state error covariance matrix P(l|l&#x2212;1, k) of the previous frame may be first determined according to the posterior state error covariance matrix of the previous frame and the process noise covariance matrix: P(l|l&#x2212;<b>1</b>, k)={circumflex over (P)}(l&#x2212;1, k)&#x3a6;+<sub>&#x394;</sub>(l, k), where  {circumflex over (P)}(l&#x2212;1, k) is the posterior state error covariance matrix of the previous frame, &#x3a6;<sub>&#x394;</sub>(l, k) is the process noise covariance matrix, &#x3a6;<sub>&#x394;</sub>(l, k)=&#x3c3;<sub>&#x394;</sub><sup>2</sup>(l, k)I, &#x3c3;<sub>&#x394;</sub><sup>2</sup>(l, k) is a parameter for controlling an uncertainty of the first transfer function g(l, k) and may take a value &#x3c3;<sub>w&#x394;</sub><sup>2</sup>(l, k)=1e<sup>&#x2212;4</sup>, and l is a unit matrix. When the current frame is the first frame, the posterior state error covariance matrix of the previous frame may adopt a preset initial value.</p><p id="p-0088" num="0070">Then, a vector of an apriori error signal E(l|l&#x2212;1, k) of the previous frame and an apriori error variance {circumflex over (&#x3c8;)}<sub>E</sub>(l|l&#x2212;1, k) of the previous frame are determined according to the first signal vector, the second transfer function of the previous frame, and vectors of first residual signals of the current frame and previous L&#x2212;1 frames: E(l|l&#x2212;1, k)=D<sub>1</sub>(l, k)&#x2212;V<sub>2 </sub><sup>T</sup>(l,k)&#x11d;(l&#x2212;1, k), and {circumflex over (&#x3c8;)}<sub>E</sub>(l|l&#x2212;1, k)=|D<sub>1</sub>k)&#x11d;(l&#x2212;1, k)|<sup>2</sup>, where V<sub>2</sub>(l, k)=[V(l, k), V(l&#x2212;1, k), . . . , V(l&#x2212;L+1, k)]<sup>T</sup>, L is a length of the second transfer function g(l, k), and the second transfer function is a transfer function between echo estimation and a residual echo. When the current frame is the first frame, the second transfer function of the previous frame may adopt a preset initial value. In the vectors of the first residual signals of the current frame and the previous L&#x2212;1 frames, if there is no L&#x2212;1 frames before the current frame, the quantity of lacking frames may adopt a preset initial value.</p><p id="p-0089" num="0071">Then, a vector {circumflex over (&#x3a6;)}<sub>E</sub>(l, k) of a prediction error power signal of the current frame is determined according to the posterior error variance of the previous frame and the apriori error variance of the previous frame: {circumflex over (&#x3a6;)}<sub>E</sub>(l, k)=&#x3b2;{circumflex over (&#x3c8;)}<sub>E</sub>(l&#x2212;1, k)+(1&#x2212;&#x3b2;){circumflex over (&#x3c8;)}<sub>E</sub>(l|l&#x2212;1, k), where {circumflex over (&#x3c8;)}<sub>E</sub>(l, k) is the posterior error variance, {circumflex over (&#x3c8;)}<sub>E</sub>(l|l&#x2212;1, k) is the apriori error variance, i{circumflex over (&#x3c8;)}<sub>E</sub>(l|l&#x2212;1, k)=|E<sub>1</sub>(l, k), Y<sub>1</sub><sup>T</sup>(l, k)&#x11d;(l&#x2212;1, k)|<sup>2</sup>, &#x3b2; is a forgetting factor, and 0&#x2264;&#x3b2;&#x2264;1. When the current frame is the first frame, the posterior error variance of the previous frame and the apriori error variance of the previous frame may both adopt preset initial values.</p><p id="p-0090" num="0072">Then, a second Kalman gain coefficient K(l, k) is determined according to the apriori state error covariance matrix of the previous frame, the vectors of the first residual signals  of the current frame and the previous L&#x2212;1 frames, and the vector of the prediction error power signal of the current frame: K(l, k)=P(l|l&#x2212;1, k)V*<sub>2</sub>(l, k)[V<sub>2</sub><sup>T</sup>(l, k)P(l|l&#x2212;1, k)V*<sub>2</sub>(l, k)+{circumflex over (&#x3d5;)}(l, k)]<sup>&#x2212;1</sup>. When the current frame is the first frame, the apriori state error covariance matrix of the previous frame may adopt a preset initial value. In the vectors of the first residual signals of the current frame and the previous L&#x2212;1 frames, if there is no L&#x2212;1 frames before the current frame, the quantity of lacking frames may adopt a preset initial value.</p><p id="p-0091" num="0073">Then, a second transfer function of the current frame is determined according to the second Kalman gain coefficient, the vector of the apriori error signal of the previous frame, and the second transfer function of the current frame: &#x11d;(l, k)=&#x11d;(l&#x2212;1, k)+K(l, k)E(l|l&#x2212;1, k). When the current frame is the first frame, the second transfer function of the previous frame may adopt a preset initial value.</p><p id="p-0092" num="0074">Finally, the vector {circumflex over (&#x3d5;)}<sub>R</sub>(l, k) of the noise estimation signal is determined according to a vector of a prediction error power signal of the previous frame, the vectors of the first residual signals of the current frame and the previous L&#x2212;1 frames, and the second transfer function of the current frame: {circumflex over (&#x3d5;)}<sub>R</sub>(l, k)=&#x3bb;{circumflex over (&#x3d5;)}<sub>E</sub>(l&#x2212;1, k)+(1&#x2212; )|V<sub>2</sub><sup>T</sup>(l, k)&#x11d;(l, k)|<sup>2</sup>, where &#x3bb; is a forgetting factor, and 0&#x2264;&#x3bb;&#x2264;1. When the current frame is the first frame, the vector of the prediction error power signal of the previous frame may adopt a preset initial value. In the vectors of the first residual signals of the current frame and the previous L&#x2212;1 frames, if there is no L&#x2212;1 frames before the current frame, the quantity of lacking frames may adopt a preset initial value.</p><p id="p-0093" num="0075">In addition, a posterior state error covariance matrix {circumflex over (P)}(l, k) of the current frame may also be determined according to the second Kalman gain coefficient, the vectors of the first residual signals of the current frame and the previous L&#x2212;1 frames, and the apriori state error covariance matrix of the previous frame: {circumflex over (P)}(l, k)=[I&#x2212;K(l, k)V<sub>2</sub><sup>T</sup>(l, k)]P(l|l&#x2212;1, k). When the current frame is the first frame, the apriori state error covariance matrix of the previous frame may adopt a preset  initial value. In the vectors of the first residual signals of the current frame and the previous L&#x2212;1 frames, if there is no L&#x2212;1 frames before the current frame, the quantity of lacking frames may adopt a preset initial value.</p><p id="p-0094" num="0076">A posterior error variance {circumflex over (&#x3c8;)}(l, k) of the current frame may also be determined according to the first signal vector, the vectors of the first residual signals of the current frame and the previous L&#x2212;1 frames, and the apriori state error covariance matrix of the previous frame: {circumflex over (&#x3c8;)}<sub>E</sub>(l, k)=|D<sub>1</sub>(l, k)&#x2212;V<sub>2</sub><sup>T</sup>(l, k)&#x11d;(l, k)|<sup>2</sup>. When the current frame is the first frame, the apriori state error covariance matrix of the previous frame may adopt a preset initial value. In the vectors of the first residual signals of the current frame and the previous L&#x2212;1 frames, if there is no L&#x2212;1 frames before the current frame, the quantity of lacking frames may adopt a preset initial value.</p><p id="p-0095" num="0077">In step S<b>302</b>, the gain function of the current frame is determined according to the vector of the noise estimation signal, a vector of a first estimation signal of the previous frame, a vector of a voice power estimation signal of the previous frame, a gain function of the previous frame, the first signal vector and a minimum apriori signal to interference ratio.</p><p id="p-0096" num="0078">In the step, a vector {circumflex over (&#x3d5;)}<sub>D</sub>(l, k) of a first estimation signal of the current frame may be first determined according to the vector of the first estimation signal of the previous frame and the first signal vector: {circumflex over (&#x3c8;)}<sub>D</sub>(l, k)=&#x3bb;{circumflex over (&#x3d5;)}<sub>D</sub>(l&#x2212;1, k)+(1&#x2212;&#x3bb;)|D<sub>1</sub>(l, k)|<sup>2</sup>. When the current frame is the first frame, the vector of the first estimation signal of the previous frame may adopt a preset initial value.</p><p id="p-0097" num="0079">Then, a vector {circumflex over (&#x3d5;)}<sub>S</sub>(l, k) of a voice power estimation signal of the current frame is determined according to the vector of the voice power estimation signal of the previous frame, the first signal vector and the gain function of the previous frame: {circumflex over (&#x3c8;)}<sub>D</sub>(l, k)=&#x3bb;{circumflex over (&#x3d5;)}<sub>D</sub>(l&#x2212;1, k)+(1&#x2212;&#x3bb;)|D<sub>1</sub>(l, k)|<sup>2</sup>. When the current frame is the first frame, the vector of the voice power estimation signal of the previous frame may adopt a preset initial value. </p><p id="p-0098" num="0080">Then, a posterior signal to interference ratio &#x3b3;(l, k) is determined according to the vector of the first estimation signal of the current frame and a vector of a noise estimation signal of the current frame:</p><p id="p-0099" num="0000"><maths id="MATH-US-00003" num="00003"><math overflow="scroll"> <mrow>  <mrow>   <mi>&#x3b3;</mi>   <mo>&#x2061;</mo>   <mo>(</mo>   <mrow>    <mi>l</mi>    <mo>,</mo>    <mi>k</mi>   </mrow>   <mo>)</mo>  </mrow>  <mo>=</mo>  <mrow>   <mfrac>    <mrow>     <msub>      <mover>       <mi>&#x3d5;</mi>       <mo>^</mo>      </mover>      <mi>Y</mi>     </msub>     <mo>(</mo>     <mrow>      <mi>l</mi>      <mo>,</mo>      <mi>k</mi>     </mrow>     <mo>)</mo>    </mrow>    <mrow>     <msub>      <mover>       <mi>&#x3d5;</mi>       <mo>^</mo>      </mover>      <mi>R</mi>     </msub>     <mo>(</mo>     <mrow>      <mi>l</mi>      <mo>,</mo>      <mi>k</mi>     </mrow>     <mo>)</mo>    </mrow>   </mfrac>   <mo>.</mo>  </mrow> </mrow></math></maths></p><p id="p-0100" num="0081">Finally, the gain function G(l, k) of the current frame is determined according to the vector of the voice power estimation signal of the current frame, the vector of the noise estimation signal of the current frame, the posterior signal to interference ratio and the minimum apriori signal to interference ratio:</p><p id="p-0101" num="0000"><maths id="MATH-US-00004" num="00004"><math overflow="scroll"> <mrow>  <mrow>   <mrow>    <mi>G</mi>    <mo>&#x2061;</mo>    <mo>(</mo>    <mrow>     <mi>l</mi>     <mo>,</mo>     <mi>k</mi>    </mrow>    <mo>)</mo>   </mrow>   <mo>=</mo>   <msqrt>    <mfrac>     <mrow>      <mi>&#x3be;</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mrow>       <mi>l</mi>       <mo>,</mo>       <mi>k</mi>      </mrow>      <mo>)</mo>     </mrow>     <mrow>      <mn>1</mn>      <mo>+</mo>      <mrow>       <mi>&#x3be;</mi>       <mo>&#x2061;</mo>       <mo>(</mo>       <mrow>        <mi>l</mi>        <mo>,</mo>        <mi>k</mi>       </mrow>       <mo>)</mo>      </mrow>     </mrow>    </mfrac>   </msqrt>  </mrow>  <mo>,</mo> </mrow></math></maths></p><p id="p-0102" num="0000">where</p><p id="p-0103" num="0000"><maths id="MATH-US-00005" num="00005"><math overflow="scroll"> <mrow>  <mrow>   <mrow>    <mi>&#x3be;</mi>    <mo>&#x2061;</mo>    <mo>(</mo>    <mrow>     <mi>l</mi>     <mo>,</mo>     <mi>k</mi>    </mrow>    <mo>)</mo>   </mrow>   <mo>=</mo>   <mrow>    <mrow>     <mi>&#x3b7;</mi>     <mo>&#x2062;</mo>     <mfrac>      <mrow>       <msub>        <mover>         <mi>&#x3d5;</mi>         <mo>^</mo>        </mover>        <mi>S</mi>       </msub>       <mo>(</mo>       <mrow>        <mi>l</mi>        <mo>,</mo>        <mi>k</mi>       </mrow>       <mo>)</mo>      </mrow>      <mrow>       <msub>        <mover>         <mi>&#x3d5;</mi>         <mo>^</mo>        </mover>        <mi>R</mi>       </msub>       <mo>(</mo>       <mrow>        <mi>l</mi>        <mo>,</mo>        <mi>k</mi>       </mrow>       <mo>)</mo>      </mrow>     </mfrac>    </mrow>    <mo>+</mo>    <mrow>     <mrow>      <mo>(</mo>      <mrow>       <mn>1</mn>       <mo>-</mo>       <mi>&#x3b7;</mi>      </mrow>      <mo>)</mo>     </mrow>     <mo>&#x2062;</mo>     <mi>max</mi>     <mo>&#x2062;</mo>     <mrow>      <mo>{</mo>      <mrow>       <mrow>        <mrow>         <mi>&#x3b3;</mi>         <mo>&#x2061;</mo>         <mo>(</mo>         <mrow>          <mi>l</mi>          <mo>,</mo>          <mi>k</mi>         </mrow>         <mo>)</mo>        </mrow>        <mo>-</mo>        <mn>1</mn>       </mrow>       <mo>,</mo>       <msub>        <mi>&#x3be;</mi>        <mi>min</mi>       </msub>      </mrow>      <mo>}</mo>     </mrow>    </mrow>   </mrow>  </mrow>  <mo>,</mo> </mrow></math></maths></p><p id="p-0104" num="0000">&#x3b7; is a forgetting factor, and &#x3be;<sub>min </sub>is the minimum apriori signal to interference ratio, used to control a residual echo suppression amount and a musical noise.</p><p id="p-0105" num="0082">An ambient noise used by the mobile phone is a diffuse field noise, and a correlation between the noise signals picked up by the two microphones of the mobile phone is low, while a target voice signal has a strong correlation. Thus, a linear adaptive filter may be used to estimate a target voice component of a signal of a reference microphone (the second microphone) through a signal of a main microphone (the first microphone), and eliminate it from the reference microphone, thus providing a reliable reference noise signal for a noise estimation process in a speech spectrum enhancement period.</p><p id="p-0106" num="0083">A Kalman adaptive filter has the features of high convergence speed, small filter offset, etc. A complete diagonalization fast frequency domain implementation method of a time-domain Kalman adaptive filter is used to eliminate the target voice signal, including several processes such as filtering, error calculation, Kalman update and Kalman prediction. The filtering process is to use the target voice signal of the main microphone to estimate the target voice component in the reference microphone through an estimation filter, and then subtract it from the reference microphone signal to work out an error signal, that is, the reference noise signal. Kalman update includes calculation of Kalman gain and filter adaptation. Kalman prediction includes calculation  of relative transfer function covariance between the channels, process noise covariance and state estimation error covariance. Compared with traditional adaptive filters such as NLMS, the Kalman filter has a simple adaption process and does not require a complicated step size control mechanism. The complete diagonalization fast frequency domain implementation method is simple to calculate, which further reduces the computational complexity.</p><p id="p-0107" num="0084">An STFT domain Kalman adaptive filter is used to estimate a relative convolution transfer function between noise spectra of the two microphones, so as to estimate a noise spectrum in the main microphone signal through the reference noise signal of the reference microphone, a Wiener filter spectrum enhancement method is used to suppress the noise, and finally an ISTFT method is used to synthesize and enhance the voice signal. The implementation process of STFT domain Kalman adaptive filtering is similar to that of a complete diagonalization fast frequency domain implementation process of the Kalman adaptive filter in target voice signal offset. The difference is that the former implements Kalman adaptive filtering in an STFT domain, and the latter is complete diagonalization fast frequency domain implementation of the time-domain Kalman adaptive filter.</p><p id="p-0108" num="0085">According to a second aspect of an example of the present disclosure, a sound processing apparatus is provided, applied to a terminal device. The terminal device includes a first microphone and a second microphone. With reference to <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the apparatus includes:</p><p id="p-0109" num="0086">a voice cancellation module <b>501</b>, configured to determine a vector of a first residual signal according to a first signal vector and a second signal vector, the first signal vector being input signals of the first microphone and including a first voice signal and a second noise signal, the second signal vector being input signals of the second microphone and including a second voice signal and a second noise signal, and the first residual signal including the second noise signal and a residual voice signal; </p><p id="p-0110" num="0087">a gain module <b>502</b>, configured to determine a gain function of a current frame according to the vector of the first residual signal and the first signal vector; and</p><p id="p-0111" num="0088">a suppressing module <b>503</b>, configured to determine a first voice signal of the current frame according to the first signal vector and the gain function of the current frame.</p><p id="p-0112" num="0089">In some examples of the present disclosure, the voice cancellation module is specifically configured to:</p><p id="p-0113" num="0090">obtain the first signal vector and the second signal vector, the first signal vector including sample points of a first quantity, and the second signal vector including sample points of a second quantity;</p><p id="p-0114" num="0091">determine a vector of a Fourier transform coefficient of the second voice signal according to the first signal vector and a first transfer function of a previous frame; and</p><p id="p-0115" num="0092">determine the vector of the first residual signal according to the sample points of the second quantity in the second signal vector and in the vector of the Fourier transform coefficient.</p><p id="p-0116" num="0093">In some examples of the present disclosure, the voice cancellation module is further configured to:</p><p id="p-0117" num="0094">determine a first Kalman gain coefficient according to the vector of the first residual signal, residual signal covariance of the previous frame, state estimation error covariance of the previous frame, the first signal vector and a smoothing parameter; and</p><p id="p-0118" num="0095">determine a first transfer function of the current frame according to the first Kalman gain coefficient, the first residual signal, and the first transfer function of the previous frame.</p><p id="p-0119" num="0096">In some examples of the present disclosure, the voice cancellation module is further configured to:</p><p id="p-0120" num="0097">determine residual signal covariance of the current frame according to the first transfer function of the current frame, first transfer function covariance of the previous frame, the first  Kalman gain coefficient, the residual signal covariance of the previous frame, the first quantity and the second quantity.</p><p id="p-0121" num="0098">In some examples of the present disclosure, when the voice cancellation module is configured to obtain the first signal vector and the second signal vector, it is specifically configured to:</p><p id="p-0122" num="0099">splice an input signal of a current frame of the first microphone and an input signal of at least one previous frame of the first microphone to form the first signal vector with the quantity of sample points being the first quantity; and</p><p id="p-0123" num="0100">splice an input signal of a current frame of the second microphone and an input signal of at least one previous frame of the second microphone to form the second signal vector with the quantity of sample points being the second quantity.</p><p id="p-0124" num="0101">In some examples of the present disclosure, the gain module is specifically configured to:</p><p id="p-0125" num="0102">convert the vector of the first residual signal and the first signal vector from a time domain form to a frequency domain form respectively;</p><p id="p-0126" num="0103">determine a vector of a noise estimation signal according to a posterior state error covariance matrix of a previous frame, a process noise covariance matrix, a second transfer function of the previous frame, the first signal vector, a first residual signal of at least one frame including the current frame and a posterior error variance of the previous frame; and</p><p id="p-0127" num="0104">determine the gain function of the current frame according to the vector of the noise estimation signal, a vector of a first estimation signal of the previous frame, a vector of a voice power estimation signal of the previous frame, a gain function of the previous frame, the first signal vector and a minimum apriori signal to interference ratio.</p><p id="p-0128" num="0105">In some examples of the present disclosure, when the gain module is configured to determine the vector of the noise estimation signal according to the posterior state error covariance  matrix of the previous frame, the process noise covariance matrix, the second transfer function of the previous frame, the first signal vector, the first residual signal of the at least one frame including the current frame and the posterior error variance of the previous frame, it is specifically configured to:</p><p id="p-0129" num="0106">determine an apriori state error covariance matrix of the previous frame according to the posterior state error covariance matrix of the previous frame and the process noise covariance matrix;</p><p id="p-0130" num="0107">determine a vector of an apriori error signal of the previous frame and an apriori error variance of the previous frame according to the first signal vector, the first transfer function of the previous frame, and vectors of first residual signals of the current frame and previous L&#x2212;1 frames, L being a length of the second transfer function;</p><p id="p-0131" num="0108">determine a vector of a prediction error power signal of the current frame according to the posterior error variance of the previous frame and the apriori error variance of the previous frame;</p><p id="p-0132" num="0109">determine a second Kalman gain coefficient according to the apriori state error covariance matrix of the previous frame, the vectors of the first residual signals of the current frame and the previous L&#x2212;1 frames, and the vector of the prediction error power signal of the current frame;</p><p id="p-0133" num="0110">determine a second transfer function of the current frame according to the second Kalman gain coefficient, the vector of the apriori error signal of the previous frame, and the second transfer function of the previous frame; and</p><p id="p-0134" num="0111">determine the vector of the noise estimation signal according to a vector of a prediction error power signal of the previous frame, the vectors of the first residual signals of the current frame and the previous L&#x2212;1 frames, and the second transfer function of the current frame. </p><p id="p-0135" num="0112">In some examples of the present disclosure, the gain module is specifically configured to:</p><p id="p-0136" num="0113">determine a posterior state error covariance matrix of the current frame according to the second Kalman gain coefficient, the vectors of the first residual signals of the current frame and the previous L&#x2212;1 frames, and the apriori state error covariance matrix of the previous frame; and/or</p><p id="p-0137" num="0114">determine a posterior error variance of the current frame according to the first signal vector, the vectors of the first residual signals of the current frame and the previous L&#x2212;1 frames, and the second transfer function of the current frame.</p><p id="p-0138" num="0115">In some examples of the present disclosure, when the gain module is configured to determine the gain function of the current frame according to the vector of the noise estimation signal, the vector of the first estimation signal of the previous frame, the vector of the voice power estimation signal of the previous frame, the gain function of the previous frame, the first signal vector and the minimum apriori signal to interference ratio, it is specifically configured to:</p><p id="p-0139" num="0116">determine a vector of a first estimation signal of the current frame according to the vector of the first estimation signal of the previous frame and the first signal vector;</p><p id="p-0140" num="0117">determine a vector of a voice power estimation signal of the current frame according to the vector of the voice power estimation signal of the previous frame, the first signal vector and the gain function of the previous frame;</p><p id="p-0141" num="0118">determine a posterior signal to interference ratio according to the vector of the first estimation signal of the current frame and a vector of a noise estimation signal of the current frame; and</p><p id="p-0142" num="0119">determine the gain function of the current frame according to the vector of the voice power estimation signal of the current frame, the vector of the noise estimation signal of the current frame, the posterior signal to interference ratio and the minimum apriori signal to interference ratio.</p><p id="p-0143" num="0120">In some examples of the present disclosure, the suppressing module is specifically configured to: </p><p id="p-0144" num="0121">convert a product of multiplying the first signal vector by the gain function of the current frame from a frequency domain form to a time domain form, so as to form the first voice signal of the current frame in the time domain form.</p><p id="p-0145" num="0122">In regard to the apparatus in the above example, specific manners of executing operations by the modules have been described in detail in the example related to the method in the first aspect, and elaboration and description will not be made here.</p><p id="p-0146" num="0123">According to a third aspect of an example of the present disclosure, <figref idref="DRAWINGS">FIG. <b>6</b></figref> exemplarily illustrates a block diagram of an electronic device. For example, the device <b>600</b> may be a mobile phone, a computer, a digital broadcasting terminal, a messaging device, a game console, a tablet device, a medical device, a fitness device, a personal digital assistant, etc.</p><p id="p-0147" num="0124">With reference to <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the device <b>600</b> may include one or more of the following components: a processing component <b>602</b>, a memory <b>604</b>, a power supply component <b>606</b>, a multimedia component <b>608</b>, an audio component <b>610</b>, an input/output (I/O) interface <b>612</b>, a sensor component <b>614</b>, and a communication component <b>616</b>.</p><p id="p-0148" num="0125">The processing component <b>602</b> generally controls overall operations of the device <b>600</b>, such as operations associated with display, telephone calls, data communication, camera operations, and recording operations. The processing component <b>602</b> may include one or more processors <b>620</b> to execute instructions to complete all or part of the steps of the above-mentioned method. In addition, the processing component <b>602</b> may include one or more modules to facilitate interactions between the processing component <b>602</b> and other components. For example, the processing component <b>602</b> may include a multimedia module to facilitate an interaction between the multimedia component <b>608</b> and the processing component <b>602</b>.</p><p id="p-0149" num="0126">The memory <b>604</b> is configured to store various types of data to support operation of the device <b>600</b>. Instances of these data include instructions of any application program or method operated on the device <b>600</b>, contact data, phone book data, messages, pictures, videos, etc. The  memory <b>604</b> may be implemented by any type of volatile or non-volatile storage devices or their combination, such as a static random access memory (SRAM), an electrically erasable programmable read-only memory (EEPROM), an erasable programmable read-only memory (EPROM), a programmable read-only memory (PROM), a read-only memory (ROM), a magnetic memory, a flash memory, a magnetic disk or an optical disk.</p><p id="p-0150" num="0127">The power supply component <b>606</b> provides power for the components of the device <b>600</b>. The power supply component <b>606</b> may include a power management system, one or more power supplies, and other components associated with generating, managing, and distributing power for the device <b>600</b>.</p><p id="p-0151" num="0128">The multimedia component <b>608</b> includes a screen that provides an output interface between the device <b>600</b> and a user. In some examples, the screen may include a liquid crystal display (LCD) and a touch panel (TP). If the screen includes a touch panel, the screen may be implemented as a touch screen to receive input signals from the user. The touch panel includes one or more touch sensors to sense touch, swipe, and gestures on the touch panel. The touch sensor may not only sense a boundary of a touch or swipe action, but also detect a duration and pressure related to the touch or swipe operation. In some examples, the multimedia component <b>608</b> includes a front camera and/or a rear camera. When the device <b>600</b> is in an operation mode, such as a shooting mode or a video mode, the front camera and/or the rear camera may receive external multimedia data. Each of the front camera and rear camera may be a fixed optical lens system or have a focal length and optical zoom capabilities.</p><p id="p-0152" num="0129">The audio component <b>610</b> is configured to output and/or input audio signals. For example, the audio component <b>610</b> includes a microphone (MIC), and when the device <b>600</b> is in an operation mode, such as a call mode, a recording mode, and a voice recognition mode, the microphone is configured to receive an external audio signal. The received audio signal may be  further stored in the memory <b>604</b> or sent via the communication component <b>616</b>. In some examples, the audio component <b>610</b> further includes a speaker for outputting audio signals.</p><p id="p-0153" num="0130">The I/O interface <b>612</b> provides an interface between the processing component <b>602</b> and a peripheral interface module. The above-mentioned peripheral interface module may be a keyboard, a click wheel, buttons, and the like. These buttons may include, but are not limited to: a home button, a volume button, a start button, and a lock button.</p><p id="p-0154" num="0131">The sensor component <b>614</b> includes one or more sensors for providing the device <b>600</b> with various aspects of state assessment. For example, the sensor component <b>614</b> may detect an open/closed state of the device <b>600</b> and relative positioning of the components. For example, the component is a display and a keypad of the device <b>600</b>. The sensor component <b>614</b> may also detect position change of the device <b>600</b> or a component of the device <b>600</b>, the presence or absence of contact between the user and the device <b>600</b>, an orientation or acceleration/deceleration of the device <b>600</b>, and a temperature change of the device <b>600</b>. The sensor component <b>614</b> may also include a proximity sensor configured to detect the presence of a nearby object when there is no physical contact. The sensor component <b>614</b> may also include a light sensor, such as a CMOS or CCD image sensor, for use in imaging applications. In some examples, the sensor component <b>614</b> may also include an acceleration sensor, a gyroscope sensor, a magnetic sensor, a pressure sensor, or a temperature sensor.</p><p id="p-0155" num="0132">The communication component <b>616</b> is configured to facilitate wired or wireless communication between the device <b>600</b> and other devices. The device <b>600</b> may access a wireless network based on a communication standard, such as WiFi, 2G or 3G, 4G or 5G, or a combination of them. In an example, the communication component <b>616</b> receives a broadcast signal or broadcast-related information from an external broadcast management system via a broadcast channel. In an example, the communication component <b>616</b> further includes a near field communication (NFC) module to facilitate short-range communication. For example, the NFC  module may be implemented based on radio frequency identification (RFID) technology, infrared data association (IrDA) technology, ultra-wideband (UWB) technology, Bluetooth (BT) technology and other technologies.</p><p id="p-0156" num="0133">In an example, the device <b>600</b> may be implemented by one or more application specific integrated circuits (ASICs), digital signal processors (DSPs), digital signal processing devices (DSPDs), programmable logic devices (PLDs), field programmable gate arrays (FPGAs), controllers, microcontrollers, microprocessors or other electronic elements, so as to implement a power supply method of the above-mentioned electronic device.</p><p id="p-0157" num="0134">In a fourth aspect, in an example of the present disclosure, a non-transitory computer readable storage medium including instructions is further provided, for example, a memory <b>604</b> including instructions. The above instructions may be executed by a processor <b>620</b> of a device <b>600</b> to complete a power supply method of the above-mentioned electronic device. For example, the non-transitory computer readable storage medium may be a ROM, a random access memory (RAM), a CD-ROM, a magnetic tape, a floppy disk, an optical data storage device, etc.</p><p id="p-0158" num="0135">After considering the specification and practicing the present disclosure disclosed herein, those of skill in the art will easily think of other implementation schemes of the present disclosure. The present application is intended to cover any variations, applications, or adaptive changes of the present disclosure. These variations, applications, or adaptive changes follow the general principles of the present disclosure and include common knowledge or conventional technical means in the art that are not disclosed in the present disclosure. The specification and the examples are regarded as exemplary only, and the true scope and spirit of the present disclosure are pointed out by the appended claims.</p><p id="p-0159" num="0136">It should be understood that the present disclosure is not limited to the precise structure that has been described above and shown in the drawings, and various modifications  and changes can be made without departing from its scope. The scope of the present disclosure is only limited by the appended claims. </p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230007393A1-20230105-M00001.NB"><img id="EMI-M00001" he="9.48mm" wi="76.20mm" file="US20230007393A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002" nb-file="US20230007393A1-20230105-M00002.NB"><img id="EMI-M00002" he="10.58mm" wi="76.20mm" file="US20230007393A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00003" nb-file="US20230007393A1-20230105-M00003.NB"><img id="EMI-M00003" he="7.79mm" wi="76.20mm" file="US20230007393A1-20230105-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00004" nb-file="US20230007393A1-20230105-M00004.NB"><img id="EMI-M00004" he="7.37mm" wi="76.20mm" file="US20230007393A1-20230105-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00005" nb-file="US20230007393A1-20230105-M00005.NB"><img id="EMI-M00005" he="7.79mm" wi="76.20mm" file="US20230007393A1-20230105-M00005.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A sound processing method, applied to a terminal device, wherein the terminal device comprises a first microphone and a second microphone, and the sound processing method comprises:<claim-text>determining a vector of a first residual signal according to a first signal vector and a second signal vector, wherein the first signal vector comprises a first voice signal and a first noise signal input into the first microphone, the second signal vector comprises a second voice signal and a second noise signal input into the second microphone, and the first residual signal comprises the second noise signal and a residual voice signal;</claim-text><claim-text>determining a gain function of a current frame according to the vector of the first residual signal and the first signal vector; and</claim-text><claim-text>determining a first voice signal of the current frame according to the first signal vector and the gain function of the current frame.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The sound processing method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein determining the vector of the first residual signal according to the first signal vector and the second signal vector comprises:<claim-text>obtaining the first signal vector and the second signal vector, wherein the first signal vector comprises sample points of a first quantity, and the second signal vector comprises sample points of a second quantity;</claim-text><claim-text>determining a vector of a Fourier transform coefficient of the second voice signal according to the first signal vector and a first transfer function of a previous frame; and</claim-text><claim-text>determining the vector of the first residual signal according to the sample points of the second quantity in the second signal vector and in the vector of the Fourier transform coefficient.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The sound processing method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, further comprising:<claim-text>determining a first Kalman gain coefficient according to the vector of the first residual signal, residual signal covariance of the previous frame, state estimation error covariance of the previous frame, the first signal vector and a smoothing parameter; and</claim-text><claim-text>determining a first transfer function of the current frame according to the first Kalman gain coefficient, the first residual signal, and the first transfer function of the previous frame.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The sound processing method according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, further comprising:<claim-text>determining residual signal covariance of the current frame according to the first transfer function of the current frame, first transfer function covariance of the previous frame, the first Kalman gain coefficient, the residual signal covariance of the previous frame, the first quantity and the second quantity.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The sound processing method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein obtaining the first signal vector and the second signal vector comprises:<claim-text>splicing an input signal of a current frame of the first microphone and an input signal of at least one previous frame of the first microphone to form the first signal vector with the quantity of sample points being the first quantity; and</claim-text><claim-text>splicing an input signal of a current frame of the second microphone and an input signal of at least one previous frame of the second microphone to form the second signal vector with the quantity of sample points being the second quantity.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The sound processing method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein determining the gain function of the current frame according to the vector of the first residual signal and the first signal vector comprises:<claim-text>converting the vector of the first residual signal and the first signal vector from a time domain form to a frequency domain form respectively;</claim-text><claim-text>determining a vector of a noise estimation signal according to a posterior state error covariance matrix of a previous frame, a process noise covariance matrix, a second transfer function of the previous frame, the first signal vector, a first residual signal of at least one frame including the current frame and a posterior error variance of the previous frame; and</claim-text><claim-text>determining the gain function of the current frame according to the vector of the noise estimation signal, a vector of a first estimation signal of the previous frame, a vector of a voice power estimation signal of the previous frame, a gain function of the previous frame, the first signal vector and a minimum apriori signal to interference ratio.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The sound processing method according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein determining the vector of the noise estimation signal according to the posterior state error covariance matrix of the previous frame, the process noise covariance matrix, the second transfer function of the previous frame, the first signal vector, the first residual signal of the at least one frame including the current frame and the posterior error variance of the previous frame comprises:<claim-text>determining an apriori state error covariance matrix of the previous frame according to the posterior state error covariance matrix of the previous frame and the process noise covariance matrix;</claim-text><claim-text>determining a vector of an apriori error signal of the previous frame and an apriori error variance of the previous frame according to the first signal vector, a first transfer function of the previous frame, and vectors of first residual signals of the current frame and previous L&#x2212;1 frames, wherein L is a length of the second transfer function;</claim-text><claim-text>determining a vector of a prediction error power signal of the current frame according to the posterior error variance of the previous frame and the apriori error variance of the previous frame;</claim-text><claim-text>determining a second Kalman gain coefficient according to the apriori state error covariance matrix of the previous frame, the vectors of the first residual signals of the current frame and the previous L&#x2212;1 frames, and the vector of the prediction error power signal of the current frame;</claim-text><claim-text>determining a second transfer function of the current frame according to the second Kalman gain coefficient, the vector of the apriori error signal of the previous frame, and the second transfer function of the previous frame; and</claim-text><claim-text>determining the vector of the noise estimation signal according to a vector of a prediction error power signal of the previous frame, the vectors of the first residual signals of the current frame and the previous L&#x2212;1 frames, and the second transfer function of the current frame.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The sound processing method according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, further comprising:<claim-text>determining a posterior state error covariance matrix of the current frame according to the second Kalman gain coefficient, the vectors of the first residual signals of the current frame and the previous L&#x2212;1 frames, and the apriori state error covariance matrix of the previous frame; and</claim-text><claim-text>determining a posterior error variance of the current frame according to the first signal vector, the vectors of the first residual signals of the current frame and the previous L&#x2212;1 frames, and the second transfer function of the current frame.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The sound processing method according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein determining the gain function of the current frame according to the vector of the noise estimation signal, the vector of the first estimation signal of the previous frame, the vector of the voice power estimation signal of the previous frame, the gain function of the previous frame, the first signal vector and the minimum apriori signal to interference ratio comprises:<claim-text>determining a vector of a first estimation signal of the current frame according to the vector of the first estimation signal of the previous frame and the first signal vector;</claim-text><claim-text>determining a vector of a voice power estimation signal of the current frame according to the vector of the voice power estimation signal of the previous frame, the first signal vector and the gain function of the previous frame;</claim-text><claim-text>determining a posterior signal to interference ratio according to the vector of the first estimation signal of the current frame and a vector of a noise estimation signal of the current frame; and</claim-text><claim-text>determining the gain function of the current frame according to the vector of the voice power estimation signal of the current frame, the vector of the noise estimation signal of the current frame, the posterior signal to interference ratio and the minimum apriori signal to interference ratio.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The sound processing method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein determining a first voice signal of the current frame according to the first signal vector and the gain function of the current frame comprises:<claim-text>converting a product of multiplying the first signal vector by the gain function of the current frame from a frequency domain form to a time domain form, so as to form the first voice signal of the current frame in the time domain form.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. An electronic device, comprising a memory, a processor, a first microphone and a second microphone, wherein the memory is configured to store a computer instruction that may be run on the processor, the processor is configured to:<claim-text>determine a vector of a first residual signal according to a first signal vector and a second signal vector, wherein the first signal vector comprises a first voice signal and a first noise signal input into the first microphone, the second signal vector comprises a second voice signal and a second noise signal input into the second microphone, and the first residual signal comprises the second noise signal and a residual voice signal;</claim-text><claim-text>determine a gain function of a current frame according to the vector of the first residual signal and the first signal vector; and</claim-text><claim-text>determine a first voice signal of the current frame according to the first signal vector and the gain function of the current frame.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The electronic device according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the processor is further configured to:<claim-text>obtain the first signal vector and the second signal vector, wherein the first signal vector comprises sample points of a first quantity, and the second signal vector comprises sample points of a second quantity;</claim-text><claim-text>determine a vector of a Fourier transform coefficient of the second voice signal according to the first signal vector and a first transfer function of a previous frame; and</claim-text><claim-text>determine the vector of the first residual signal according to the sample points of the second quantity in the second signal vector and in the vector of the Fourier transform coefficient.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The electronic device according to <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the processor is further configured to:<claim-text>determine a first Kalman gain coefficient according to the vector of the first residual signal, residual signal covariance of the previous frame, state estimation error covariance of the previous frame, the first signal vector and a smoothing parameter; and</claim-text><claim-text>determine a first transfer function of the current frame according to the first Kalman gain coefficient, the first residual signal, and the first transfer function of the previous frame.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The electronic device according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the processor is further configured to:<claim-text>determine residual signal covariance of the current frame according to the first transfer function of the current frame, first transfer function covariance of the previous frame, the first Kalman gain coefficient, the residual signal covariance of the previous frame, the first quantity and the second quantity.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The electronic device according to <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the processor is further configured to:<claim-text>splice an input signal of a current frame of the first microphone and an input signal of at least one previous frame of the first microphone to form the first signal vector with the quantity of sample points being the first quantity; and</claim-text><claim-text>splice an input signal of a current frame of the second microphone and an input signal of at least one previous frame of the second microphone to form the second signal vector with the quantity of sample points being the second quantity.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The electronic device according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the processor is further configured to:<claim-text>convert the vector of the first residual signal and the first signal vector from a time domain form to a frequency domain form respectively;</claim-text><claim-text>determine a vector of a noise estimation signal according to a posterior state error covariance matrix of a previous frame, a process noise covariance matrix, a second transfer function of the previous frame, the first signal vector, a first residual signal of at least one frame including the current frame and a posterior error variance of the previous frame; and</claim-text><claim-text>determine the gain function of the current frame according to the vector of the noise estimation signal, a vector of a first estimation signal of the previous frame, a vector of a voice power estimation signal of the previous frame, a gain function of the previous frame, the first signal vector and a minimum apriori signal to interference ratio.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The electronic device according to <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the processor is further configured to:<claim-text>determine an apriori state error covariance matrix of the previous frame according to the posterior state error covariance matrix of the previous frame and the process noise covariance matrix;</claim-text><claim-text>determine a vector of an apriori error signal of the previous frame and an apriori error variance of the previous frame according to the first signal vector, a first transfer function of the previous frame, and vectors of first residual signals of the current frame and previous L&#x2212;1 frames, wherein L is a length of the second transfer function;</claim-text><claim-text>determine a vector of a prediction error power signal of the current frame according to the posterior error variance of the previous frame and the apriori error variance of the previous frame;</claim-text><claim-text>determine a second Kalman gain coefficient according to the apriori state error covariance matrix of the previous frame, the vectors of the first residual signals of the current frame and the previous L&#x2212;1 frames, and the vector of the prediction error power signal of the current frame;</claim-text><claim-text>determine a second transfer function of the current frame according to the second Kalman gain coefficient, the vector of the apriori error signal of the previous frame, and the second transfer function of the previous frame; and</claim-text><claim-text>determine the vector of the noise estimation signal according to a vector of a prediction error power signal of the previous frame, the vectors of the first residual signals of the current frame and the previous L&#x2212;1 frames, and the second transfer function of the current frame.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The electronic device according to <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the processor is further configured to:<claim-text>determine a posterior state error covariance matrix of the current frame according to the second Kalman gain coefficient, the vectors of the first residual signals of the current frame and the previous L&#x2212;1 frames, and the apriori state error covariance matrix of the previous frame; and</claim-text><claim-text>determine a posterior error variance of the current frame according to the first signal vector, the vectors of the first residual signals of the current frame and the previous L&#x2212;1 frames, and the second transfer function of the current frame.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The electronic device according to <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the processor is further configured to:<claim-text>determine a vector of a first estimation signal of the current frame according to the vector of the first estimation signal of the previous frame and the first signal vector;</claim-text><claim-text>determine a vector of a voice power estimation signal of the current frame according to the vector of the voice power estimation signal of the previous frame, the first signal vector and the gain function of the previous frame;</claim-text><claim-text>determine a posterior signal to interference ratio according to the vector of the first estimation signal of the current frame and a vector of a noise estimation signal of the current frame; and</claim-text><claim-text>determine the gain function of the current frame according to the vector of the voice power estimation signal of the current frame, the vector of the noise estimation signal of the current frame, the posterior signal to interference ratio and the minimum apriori signal to interference ratio.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. A non-transitory computer readable storage medium storing a computer program, wherein the program, when executed by a processor, causes the processor to:<claim-text>determine a vector of a first residual signal according to a first signal vector and a second signal vector, wherein the first signal vector comprises a first voice signal and a first noise signal input into a first microphone, the second signal vector comprises a second voice signal and a second noise signal input into a second microphone, and the first residual signal comprises the second noise signal and a residual voice signal;</claim-text><claim-text>determine a gain function of a current frame according to the vector of the first residual signal and the first signal vector; and</claim-text><claim-text>determine a first voice signal of the current frame according to the first signal vector and the gain function of the current frame.</claim-text></claim-text></claim></claims></us-patent-application>