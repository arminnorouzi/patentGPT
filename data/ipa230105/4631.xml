<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004632A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004632</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17839713</doc-number><date>20220614</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>JP</country><doc-number>2021-108593</doc-number><date>20210630</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>21</main-group><subgroup>32</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>21</main-group><subgroup>32</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc></classifications-cpc><invention-title id="d2e61">FACE AUTHENTICATION SYSTEM</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Hitachi-LG Data Storage, Inc.</orgname><address><city>Tokyo</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>UENO</last-name><first-name>Takaaki</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>NAKAMICHI</last-name><first-name>Takuya</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">The invention provides a face authentication system robust to the individual differences in the face parts or changes in the face orientation when an individual is identified by using a face image in a state of wearing a wearable item. The face authentication system according to the invention identifies an individual by using a synthesized image obtained by deforming a wearable item image to fit a face shape of the individual.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="142.24mm" wi="118.53mm" file="US20230004632A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="217.59mm" wi="166.62mm" file="US20230004632A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="235.03mm" wi="44.11mm" file="US20230004632A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="229.36mm" wi="159.26mm" file="US20230004632A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="158.24mm" wi="120.57mm" file="US20230004632A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="160.27mm" wi="103.72mm" file="US20230004632A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="231.48mm" wi="113.37mm" file="US20230004632A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="200.58mm" wi="133.52mm" file="US20230004632A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="234.61mm" wi="102.19mm" file="US20230004632A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="203.37mm" wi="165.27mm" file="US20230004632A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="147.83mm" wi="128.10mm" file="US20230004632A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="235.80mm" wi="166.54mm" file="US20230004632A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="151.64mm" wi="167.89mm" file="US20230004632A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="227.58mm" wi="155.53mm" file="US20230004632A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="222.17mm" wi="116.50mm" file="US20230004632A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="183.05mm" wi="167.98mm" file="US20230004632A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="241.55mm" wi="161.12mm" file="US20230004632A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="144.78mm" wi="156.80mm" file="US20230004632A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="197.36mm" wi="156.80mm" file="US20230004632A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00019" num="00019"><img id="EMI-D00019" he="197.53mm" wi="153.84mm" file="US20230004632A1-20230105-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00020" num="00020"><img id="EMI-D00020" he="193.89mm" wi="145.63mm" file="US20230004632A1-20230105-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00021" num="00021"><img id="EMI-D00021" he="158.67mm" wi="166.96mm" file="US20230004632A1-20230105-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION</heading><p id="p-0002" num="0001">The present application claims priority from Japanese application JP2021-108593, filed on Jun. 30, 2021, the contents of which is hereby incorporated by reference into this application.</p><heading id="h-0002" level="1">BACKGROUND OF THE INVENTION</heading><heading id="h-0003" level="1">1. Field of the Invention</heading><p id="p-0003" num="0002">The present invention relates to a face authentication system that identifies an individual by using a face image.</p><heading id="h-0004" level="1">2. Description of the Related Art</heading><p id="p-0004" num="0003">Face authentication technique, as one example of biometrics authentication using biometric information, is a technique of registering a face image of an individual user in advance and collating a face image extracted at the time of authentication with the face image registered in advance to determine whether the person matches. The face authentication technique can take the face image from a remote location, and thus has an advantage that the authentication can be performed in a non-contact manner without requiring any authentication operations by the user as in fingerprint authentication or the like. The face authentication has been widely used for access management in offices, educational institutions, event venues, or for surveillance security using surveillance camera images, or the like.</p><p id="p-0005" num="0004">In particular, in recent years, due to the influence of COVID-19, the need to verify identity while wearing masks and goggles has increased, and a system that can perform face authentication even when the person is wearing a mask or other wearable items is required.</p><p id="p-0006" num="0005">In response to the above problems, for example, JP-A-2015-088095 discloses a technique of using an image obtained by synthesizing a wearable item image of a mask, a hat or the like with a real face image into a face image for registration used at the time of collation, so as to reduce burdens on a user at the time of registration and perform face authentication while the user is wearing a wearable item.</p><heading id="h-0005" level="1">SUMMARY OF THE INVENTION</heading><p id="p-0007" num="0006">The method described in JP-A-2015-088095 synthesizes a wearable item image as it is into a real face image, and thus cannot generate a wearable item image that corresponds to individual differences in face parts (structure, size, shape, etc.) and changes in face orientation (rotation to angles in a yaw, roll, or pitch direction), and has poor robustness to the changes in face conditions. In addition, in applications such as simultaneous authentication of multiple individuals using signage and face authentication using surveillance cameras, it is assumed that the face orientation at the time of authentication is not limited to the front face but is in various directions. Thus, a face authentication system having high robustness for the face orientation is desired.</p><p id="p-0008" num="0007">The invention has been made in view of the above problems, and has an object to provide a face authentication system robust to the individual differences in the face parts or changes in the face orientation when an individual is identified by using a face image in a state of wearing a wearable item.</p><p id="p-0009" num="0008">The face authentication system according to the invention identifies an individual by using a synthesized image obtained by deforming a wearable item image to fit the wearable item image with a face shape of the individual.</p><p id="p-0010" num="0009">According to the face authentication system according to the invention, it is possible to provide a highly accurate face authentication system that can improve robustness to individual differences in face parts or changes in face orientation when an individual is to be identified by using a face image in a state of wearing a wearable item.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0006" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram showing an overall configuration of a face authentication system <b>1</b> according to a first embodiment.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>2</b>A</figref> is an example of a real face image <b>401</b>.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>2</b>B</figref> is an example of a wearable item image <b>402</b>.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>2</b>C</figref> is an example of a synthesized image <b>403</b>.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a block diagram showing an internal configuration of a face image registration unit <b>220</b> and a face image collation unit <b>320</b>.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flowchart illustrating a process of registering the real face image <b>401</b> at the time of face registration.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram showing a process of generating the synthesized image <b>403</b> from the real face image <b>401</b>.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flowchart illustrating a process of generating the synthesized image <b>403</b> from the real face image <b>401</b> and recording the synthesized image <b>403</b> in a data storage unit <b>400</b>.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart illustrating a process at the time of face authentication.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>8</b>A</figref> is a diagram showing a comparison result of a similarity when the face is tilted in a yaw direction from the front.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>8</b>B</figref> is a diagram showing a comparison result of the similarity when the face is tilted in a roll direction from the front.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a diagram showing a detailed configuration of the face authentication system <b>1</b> in a second embodiment.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a flowchart illustrating a process at the time of face registration in the second embodiment.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a flowchart illustrating a process at the time of face authentication in the second embodiment.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a configuration diagram of the face authentication system <b>1</b> according to a third embodiment.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a diagram showing an internal processing block of the face authentication system <b>1</b>.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a flowchart illustrating a process of generating and registering the synthesized image <b>403</b> at the time of face registration.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a configuration diagram of a face authentication unit <b>300</b> in the third embodiment.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>16</b></figref> is a flowchart showing operations at the time of face authentication in the third embodiment.</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is a configuration diagram of the face authentication unit <b>300</b> in a fourth embodiment.</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>18</b></figref> is a flowchart illustrating operations at the time of authentication in the fourth embodiment.</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>19</b></figref> is a configuration diagram of a face registration unit <b>200</b> in a fifth embodiment.</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>20</b></figref> is a diagram showing the effect of the fifth embodiment.</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>21</b></figref> is a configuration diagram of the face authentication unit <b>300</b> in the fifth embodiment.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0007" level="1">DESCRIPTION OF THE PREFERRED EMBODIMENTS</heading><heading id="h-0008" level="1">First Embodiment</heading><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram showing an overall configuration of a face authentication system <b>1</b> according to a first embodiment of the invention. The face authentication system <b>1</b> is a system for identifying an individual by using a face image. The face authentication system <b>1</b> includes an imaging unit <b>100</b>, a face registration unit <b>200</b>, a face authentication unit <b>300</b>, and a data storage unit <b>400</b>.</p><p id="p-0036" num="0035">The face registration unit <b>200</b> includes a face area detection unit <b>210</b> and a face image registration unit <b>220</b>. The face authentication unit <b>300</b> includes a face area detection unit <b>310</b> and a face image collation unit <b>320</b>. The data storage unit <b>400</b> stores a real face image <b>401</b>, a wearable item image <b>402</b>, and a synthesized image <b>403</b>. The face registration unit <b>200</b> and the face authentication unit <b>300</b>, as will be described later, may both generate the synthesized image <b>403</b> by synthesizing the real face image <b>401</b> and the wearable item image <b>402</b>, and can serve as a &#x201c;synthesis unit&#x201d; for generating the synthesized image <b>403</b>.</p><p id="p-0037" num="0036">The imaging unit <b>100</b> may any one that is capable of acquiring a two-dimensional image in which luminance information is stored in an xy-direction, or a three-dimensional image in which distance (depth) information in a z-direction is stored in addition to the luminance information in the xy direction.</p><p id="p-0038" num="0037">The synthesized image <b>403</b> is obtained by processing the wearable item image <b>402</b> to fit the real face image <b>401</b> and synthesizing the wearable item image <b>402</b> with the real face image <b>401</b>. A processing example will be described later. A plurality of synthesized images <b>403</b> may be stored for each type of external wearable items such as mask, glasses, sunglasses, goggles, and hat. When the same type of wearable items has a plurality of shape patterns, the synthesized image <b>403</b> may be generated for each shape pattern. For example, as the wearable item, a mask may be a flat mask or a three-dimensional mask, and glasses may have square lenses or round lenses. Further, a plurality of types of external wearable items may be simultaneously synthesized. For example, masks and goggles may be simultaneously synthesized. In the invention, the real face image <b>401</b>, the wearable item image <b>402</b>, and the synthesized image <b>403</b> may be two-dimensional images or three-dimensional images, and may be feature vector quantities instead of images.</p><p id="p-0039" num="0038">The face authentication system <b>1</b> may include two or more data storage units <b>400</b>, and may use the data storage units <b>400</b> properly according to the type of data. For example, the real face image <b>401</b>, the wearable item image <b>402</b>, and the synthesized image <b>403</b> may be registered in different external data storage units <b>400</b> and received wirelessly from a data base station <b>2</b>.</p><p id="p-0040" num="0039">Operations at the time of face registration will be explained. An image captured by the imaging unit <b>100</b> is sent to the face area detection unit <b>210</b> in the face registration unit <b>200</b>. The face area detection unit <b>210</b> extracts a real face image from the captured image and inputs the real face image to the face image registration unit <b>220</b>. The face image registration unit <b>220</b> records the input real face image as registered face data of the user in the data storage unit <b>400</b>. The registered face data refers to the real face image <b>401</b> and the synthesized image <b>403</b>, which may be recorded alone or simultaneously. A plurality of types of the synthesized images <b>403</b> may be simultaneously recorded depending on the types of the wearable items.</p><p id="p-0041" num="0040">Operations at the time of face authentication will be explained. An image captured by the imaging unit <b>100</b> is sent to the face area detection unit <b>310</b> in the face authentication unit <b>300</b>. The face area detection unit <b>310</b> extracts a face image from the captured images and inputs the face image to the face image collation unit <b>320</b>. The face image extracted by the face area detection unit <b>310</b> may be either a real face or a case of wearing a wearable item, depending on the face condition of the user at the time of authentication. The face image collation unit <b>320</b> acquires one or more of the real face images <b>401</b> or the synthesized images <b>403</b> from the data storage unit <b>400</b>. The face image collation unit <b>320</b> performs a face image collation process between each piece of the acquired registered face data and each of the face images extracted from the captured images at the time of authentication, and outputs an authentication result of the collation to an output device or another system. Specifically, the face image collation unit <b>320</b> calculates a similarity between each of the face images extracted by the face area detection unit <b>310</b> and each piece of one or more registered face data acquired from the data storage unit, and the user is authenticated as a registered user when any of the similarities reaches a specified level.</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>2</b>A</figref> is an example of the real face image <b>401</b>. The real face image <b>401</b> is a face image without wearing external wearable items, and is taken as a real face image even when the captured subject is wearing make-up or the like.</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. <b>2</b>B</figref> is an example of the wearable item image <b>402</b>. The wearable item image <b>402</b> is an image of a single wearable item. The wearable item mentioned here is an external artificial object such as a mask, sunglasses, goggles, glasses, a hat, etc. that is worn to the real face (the head of the person to be authenticated), and is an object that hinders face authentication.</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. <b>2</b>C</figref> is an example of the synthesized image <b>403</b>. The synthesized image <b>403</b> is obtained by processing the wearable item image <b>402</b> to fit the face parts and the orientation of the face, and synthesizing the wearable item image <b>402</b> to fit the real face image <b>401</b>. For example, when the real face image <b>401</b> is rotated about a yaw axis, the synthesized image <b>403</b> can be generated by similarly rotating the wearable item image <b>402</b> around the yaw axis and synthesizing the wearable item image <b>402</b> with the real face image <b>401</b>.</p><p id="p-0045" num="0044">The present embodiment can be applied to either <b>1</b>:<b>1</b> authentication that is a one-to-one relation between input biometric information and biometric information to be collated, or 1:N authentication that is a one-to-N relation in which each piece of input biometric information corresponds to multiple pieces of the biometric information to be compared. In an example of 1:1 authentication using a card and biometric information, input biometric information is collated with biometric information registered in the card. In the 1:N authentication, input biometric information is sequentially compared with all biometric information registered in a database, and the closest user is uniquely identified.</p><p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a block diagram showing an internal configuration of the face image registration unit <b>220</b> and a face image collation unit <b>320</b>. The face image registration unit <b>220</b> includes a face landmark detection unit <b>221</b>, a wearable item image processing unit <b>222</b>, and an authentication data generating unit <b>223</b>. The face image collation unit <b>320</b> includes an authentication data generating unit <b>321</b>. Operations of these functional units will be described later.</p><p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flowchart illustrating a process of registering the real face image <b>401</b> at the time of face registration. An image captured by the imaging unit <b>100</b> is input to the face area detection unit <b>210</b> (S<b>401</b>). The face area detection unit <b>210</b> extracts a face area of the user to be registered from the captured image to generate the real face image (S<b>402</b>). The generated real face image is sent to the authentication data generating unit <b>223</b>. The authentication data generating unit <b>223</b> generates authentication data based on the real face image (S<b>403</b>) and records the real face image in the data storage unit <b>400</b> (S<b>404</b>). The authentication data may be the synthesized image <b>403</b> per se, or a feature of the synthesized image <b>403</b> may be used as the authentication data. In the former case, the synthesized image <b>403</b> and the captured image are collated, and in the latter case, the feature of the synthesized image <b>403</b> and the feature of the captured image are collated.</p><p id="p-0048" num="0047">In S<b>402</b>, an image area corresponding to the face area is specified from the luminance information (or the distance information or a combination thereof) in the captured image, and the image area is cut out from the captured image to generate the face image. In S<b>402</b>, when the face image is generated from the face area cut out from the captured image, one or more processing steps for processing the face image into a more appropriate state for the face authentication may be included. For example, image size conversion, image rotation, affine transformation, face orientation angle deformation by non-rigid body deformation method based on free-form deformation (FFD) method or the like, removal of noise and blur contained in the face image, contrast adjustment, distortion correction, background area removal, and the like can be used.</p><p id="p-0049" num="0048">As the face area detection processing method in S<b>402</b>, a known method can be used. For example, Viola-Jopnes method using Haar-like features or a detector based on a neural network (hereinafter referred to as NN) model can be used.</p><p id="p-0050" num="0049">The authentication data generated in S<b>403</b> is converted into a format suitable for the algorithm adopted by the face authentication unit <b>300</b>, and may be a feature vector quantity or be in the form of an image. An example of generating a feature vector will be described below. The authentication data generating unit <b>223</b> inputs the received real face image into a predetermined feature extractor to generate a feature vector representing a feature of the face image. Examples of the feature extractor may be an algorithm based on an NN model trained by training data, an algorithm based on local binary patterns, or the like. A plurality of face images for registration may be simultaneously input to the feature extractor to generate an integrated feature vector quantity. For example, a combination of the real face image <b>401</b> and the plurality of types of synthesized images <b>403</b> is simultaneously input to the feature extractor to generate the feature vector. The feature vector quantity may be a sum of products calculated by respectively multiplying the feature vectors corresponding to the plurality of face images for registration with predetermined weights.</p><p id="p-0051" num="0050">In S<b>404</b>, when data is recorded in the data storage unit <b>400</b>, an encryption process or the like for improving security may be performed.</p><p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram showing a process of generating the synthesized image <b>403</b> from the real face image <b>401</b>. The face landmark detection unit <b>221</b> detects face landmarks <b>401</b>-<b>1</b> from the real face image <b>401</b>. The face landmarks are reference points on the face image. The wearable item image processing unit <b>222</b> generates a processed wearable item image <b>402</b>-<b>1</b> by deforming the wearable item image <b>402</b> to fit the positions of the face landmarks <b>401</b>-<b>1</b>. The authentication data generating unit <b>223</b> generates the synthesized image <b>403</b> by synthesizing the real face image <b>401</b> and the processed wearable item image <b>402</b>-<b>1</b>. The synthesizing procedure is as follows.</p><heading id="h-0009" level="2">Synthesizing Procedure: 1</heading><p id="p-0053" num="0052">The face landmark detection unit <b>221</b> detects a rotation angle of the face (a combination of one or more of yaw, roll, and pitch) based on the face landmarks. The wearable item image processing unit <b>222</b> rotates the wearable item image <b>402</b> to match the rotation angle of the face.</p><heading id="h-0010" level="2">Synthesizing Procedure: 2</heading><p id="p-0054" num="0053">The wearable item image processing unit <b>222</b> deforms the wearable item image such that a vertical size of the wearable item image <b>402</b> matches a vertical size of a wearing position among the face landmarks where the wearable item is worn. For example, a distance between the top portion of a mask (the most protruding part of the portion covering the nose) and the bottom portion of the mask can be the vertical size of the wearable item image <b>402</b>. For example, a distance between a third point from the top among nose landmarks and a point located at the chin tip among chin landmarks can be the vertical size of the wearing position. The wearable item image <b>402</b> is aligned such that both the sizes match with each other. If necessary, affine transformation, homography transformation, or a combination thereof may be used.</p><heading id="h-0011" level="2">Synthesizing Procedure: 3</heading><p id="p-0055" num="0054">The wearable item image processing unit <b>222</b> deforms the wearable item image such that a horizontal size of the wearable item image <b>402</b> matches a horizontal size of the wearing position where the wearable item is worn among the face landmarks. For example, a distance between the left and right ends of the mask can be the horizontal size of the wearable item image <b>402</b>. For example, the distance between the face landmarks on the left and right ends can be set as the horizontal size of the wearing position. The wearable item image <b>402</b> is aligned such that both the sizes match with each other. If necessary, affine transformation, homography transformation, or a combination thereof may be used.</p><heading id="h-0012" level="2">Synthesizing Procedure: 4</heading><p id="p-0056" num="0055">The wearable item image processing unit <b>222</b> may divide the wearable item image <b>402</b> into a plurality of areas to perform the above steps <b>2</b> and <b>3</b>. For example, the wearable item image <b>402</b> may be divided into a right half and a left half. That is, the wearable item image <b>402</b> may be deformed in the left half and the right half to match the vertical size of the wearable item image <b>402</b> with the vertical size of the wearing position, and match the horizontal size of the wearable item image <b>402</b> with the horizontal size of the wearing position. The horizontal size in this case can be, for example, a distance from a point to a straight line from a straight line connecting the third point from the top among the nose landmarks and the point located at the chin tip among the chin landmarks, to a right end (or left end) point among the face landmarks. For example, when the image is not symmetrical with respect to the center of rotation as the processed wearable item image <b>402</b>-<b>1</b>, it is useful to carry out individual deformation for each of the left half and the right half of the wearable item image <b>402</b> in this way.</p><heading id="h-0013" level="2">Synthesizing Procedure: Supplement 1</heading><p id="p-0057" num="0056">The above is an example of the case where the wearable item is a mask, but can also be used for other wearable items. That is, after matching the rotation angle of the face with the rotation angle of the wearable item, the wearable item image <b>402</b> may be deformed to match the vertical/horizontal size of the wearable item with the vertical/horizontal size of the wearing position. The wearing position may be appropriately determined for each type of the wearable item. The same procedure can be used for rotations other than the yaw rotation.</p><heading id="h-0014" level="2">Synthesizing Procedure: Supplement 2</heading><p id="p-0058" num="0057">The above is an example in which the wearable item image <b>402</b> is rotated to match the rotation angle of the face, and then is synthesized to match the vertical/horizontal size of the wearable item and the wearing position. The procedures, however, may be interchanged to determine in advance the vertical/horizontal size of the wearable item according to the wearing position, and then perform the rotation process to synthesize the images.</p><p id="p-0059" num="0058">Other than the above synthesizing procedure example, a synthesizing procedure may be performed by providing a plurality of control points inside the wearable item image <b>402</b> and non-linearly deforming the image to match the control points with reference points of the wearing position determined based on the face landmarks. If necessary, this synthesizing procedure may be combined with the above synthesizing procedure example.</p><p id="p-0060" num="0059"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flowchart illustrating a process of generating the synthesized image <b>403</b> from the real face image <b>401</b> and recording the synthesized image <b>403</b> in the data storage unit <b>400</b>. In <figref idref="DRAWINGS">FIG. <b>6</b></figref>, steps having the same functions as those in <figref idref="DRAWINGS">FIG. <b>4</b></figref> are designated by the same reference numerals, and detailed description thereof will be omitted. Here, an example in which the wearable item image <b>402</b> is a mask will be described. In the flowchart of <figref idref="DRAWINGS">FIG. <b>6</b></figref>, S<b>601</b>, S<b>602</b>, and S<b>603</b> are added as compared to <figref idref="DRAWINGS">FIG. <b>4</b></figref>. These steps will be described below.</p><heading id="h-0015" level="2">FIG. <b>6</b>: Step S<b>601</b></heading><p id="p-0061" num="0060">The real face image <b>401</b> extracted in S<b>402</b> is input to the face landmark detection unit <b>221</b>. The face landmark detection unit <b>221</b> detects the face landmarks <b>401</b>-<b>1</b> from a face area. The face landmarks <b>401</b>-<b>1</b> are feature points indicating the position of each part of the face such as eyes, eyebrows, nose, mouth, chin, and contour. The face landmarks <b>401</b>-<b>1</b> in <figref idref="DRAWINGS">FIG. <b>5</b></figref> show an example in which 68 feature points of face parts are detected, but as long as at least one position of the face part is detected, the position can be used as the face landmark <b>401</b>-<b>1</b>. Known methods can be used as the face landmark detection method. For example, a method using pattern matching based on active appearance model, a feature point extraction method based on regression trees, or a detector based on an NN model can be used. Information of the real face image <b>401</b> generated in S<b>402</b> and face landmarks <b>401</b>-<b>1</b> detected in S<b>601</b> is input to the wearable item image processing unit <b>222</b>.</p><heading id="h-0016" level="2">FIG. <b>6</b>: Step S<b>602</b></heading><p id="p-0062" num="0061">The wearable item image processing unit <b>222</b> acquires the wearable item image <b>402</b> stored in the data storage unit <b>400</b> in advance, and generates the processed wearable item image <b>402</b>-<b>1</b> by processing the wearable item image <b>402</b> based on the face landmarks <b>401</b>-<b>1</b>.</p><heading id="h-0017" level="2">FIG. <b>6</b>: Step S<b>603</b></heading><p id="p-0063" num="0062">The authentication data generating unit <b>223</b> generates the synthesized image <b>403</b> by synthesizing the processed wearable item image <b>402</b>-<b>1</b> to fit the real face image <b>401</b>, and stores the synthesized image <b>403</b> in the data storage unit <b>400</b>.</p><p id="p-0064" num="0063">Although one wearable item image <b>402</b> acquired in S<b>602</b> is used in the above example, a plurality of images may be acquired depending on the type of the wearable item image, and the synthesized image <b>403</b> may be generated for each type of the wearable item image and for each shape pattern of the same type of the wearable item. For example, the synthesized image <b>403</b> in which a plurality of types of wearable items are simultaneously worn may be generated, as in the case where a mask and goggles are simultaneously synthesized with the real face image <b>401</b>.</p><p id="p-0065" num="0064">In the above method, the synthesized image <b>403</b> is automatically generated from the real face image <b>401</b>, and thus, the user only needs to register the real face image <b>401</b> without necessity of capturing an image while wearing the wearable item, and the burdens on the user at the time of registration can be reduced. At the time of face authentication, by matching the face image extracted from the face area detection unit <b>310</b> with each of the real face image <b>401</b> and the synthesized image <b>403</b> stored in the data storage unit <b>400</b>, face authentication is possible even when the user is wearing a wearable item at the time of face authentication. The synthesized image <b>403</b> is obtained by processing the wearable item image <b>402</b> based on the structure of the face parts, the size of each face part, the shape of each face part, the shape of the contour of the face, and the like in the real face image <b>401</b> registered by the user and synthesizing the wearable item image <b>402</b> and the real face image <b>401</b>. Therefore, regardless of the individual difference of the face parts for different users and the face orientation such as the rotation angle in the yaw, roll, or pitch direction, the accuracy of deriving the similarity is improved, which enables highly accurate face authentication which is robust to changes in the face conditions.</p><p id="p-0066" num="0065"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart illustrating a process at the time of face authentication. S<b>701</b> to S<b>704</b> are added as compared to <figref idref="DRAWINGS">FIG. <b>4</b></figref>. In the following, the steps added to <figref idref="DRAWINGS">FIG. <b>4</b></figref> will be mainly described.</p><heading id="h-0018" level="2">FIG. <b>7</b>: Steps S<b>401</b> to S<b>403</b></heading><p id="p-0067" num="0066">At the time of face authentication as well, similar to the time of face registration, a face image generated by the face area detection unit <b>310</b> is input to the authentication data generating unit <b>321</b> in the face image collation unit <b>320</b>, and the authentication data generating unit <b>321</b> generates the authentication data.</p><heading id="h-0019" level="2">FIG. <b>7</b>: Steps S<b>701</b> to S<b>703</b></heading><p id="p-0068" num="0067">The face authentication unit <b>300</b> acquires the real face image <b>401</b> and the synthesized image <b>403</b> for collating with the face image acquired at the time of face authentication from the data storage unit <b>400</b> (S<b>701</b>). In that case, the face authentication unit <b>300</b> confirms whether the authentication data of the face image acquired at the time of face authentication and the authentication data acquired from the data storage unit <b>400</b> are complete (S<b>702</b>). If the authentication data is not complete, the process returns to S<b>701</b> again. When the authentication data is complete, the face image acquired at the time of authentication and the plurality of pieces of registered face data acquired from the data storage unit <b>400</b> are compared with each other by calculating the similarity (S<b>703</b>).</p><heading id="h-0020" level="2">FIG. <b>7</b>: Steps S<b>702</b>: Supplement</heading><p id="p-0069" num="0068">The authentication data being complete means that the authentication data has already been generated for all the individuals assumed by the face authentication system <b>1</b> as the individuals to be authenticated.</p><heading id="h-0021" level="2">FIG. <b>7</b>: Steps S<b>703</b>: Supplement 1</heading><p id="p-0070" num="0069">As an example of calculating the similarity, when the authentication data is feature vector quantities, a known method such as Euclidean norm, cosine similarity, or the like can be used. In the similarity calculation of a pixel set such as a two-dimensional image or a three-dimensional image, a known method based on block matching, a method based on luminance distribution vector of the image, or the like can be used. In the authentication based on the feature vector quantity, the calculation amount is generally reduced as compared to the case of using the pixel set, and thus the similarity calculation using the feature vector quantity is more effective when it is desired to speed up the collation process.</p><heading id="h-0022" level="2">FIG. <b>7</b>: Steps S<b>703</b>: Supplement 2</heading><p id="p-0071" num="0070">In this step, two or more among the feature vector quantity, the two-dimensional image, and the three-dimensional image may be synthesized to comprehensively calculate the similarity. For example, when the feature vector quantity and the two-dimensional image are used, predetermined similarities may be calculated for each, and a value finally summed based on predetermined weights may be used as the similarity.</p><heading id="h-0023" level="2">FIG. <b>7</b>: Step S<b>704</b></heading><p id="p-0072" num="0071">The face authentication unit <b>300</b> performs the face authentication by confirming whether the calculated similarity satisfies a condition of a predetermined threshold value. The face authentication unit <b>300</b> outputs the authentication result to the output device, another system, or the like. For example, the following can be used as the authentication method. In the case of the 1:1 authentication, the user is a registered user when the calculated similarity satisfies the condition of the predetermined threshold value. It is determined that &#x201c;the person does not match&#x201d; when the condition of the threshold value is not satisfied. In the case of the 1:N authentication, the registered user who has the largest calculated similarity and satisfies the condition of the predetermined threshold value is determined to be the user. It is determined as &#x201c;unregistered&#x201d; when the condition of the threshold value is not satisfied.</p><p id="p-0073" num="0072"><figref idref="DRAWINGS">FIG. <b>8</b>A to <b>8</b>B</figref> show results of comparing a case of authentication using the synthesized image <b>403</b> generated by processing the wearable item image <b>402</b> based on the face landmarks in the invention to fit the real face image <b>401</b> and a case where the wearable item image <b>402</b> is not processed and is directly synthesized with the real face image <b>401</b> to perform the authentication. Here, the wearable item image was a mask, and the same person was compared. At the time of registration, a real face image facing a specific angle is registered, and the mask is synthesized based on a predetermined method. At the time of authentication, a real mask was worn, and the face was turned at the predetermined angle, and the similarity with the face image after synthesizing the mask was calculated. A feature vector generated from a feature extractor based on an NN model was used for the authentication data, and Euclidean norm was used for the calculation of the similarity. The vertical axis shows a relative similarity for each face angle with the similarity when the authentication is performed facing the front as 0. A smaller similarity indicates more accurate detection.</p><p id="p-0074" num="0073"><figref idref="DRAWINGS">FIG. <b>8</b>A</figref> is a diagram showing a comparison result of the similarity when the face is tilted in the yaw direction from the front, and <figref idref="DRAWINGS">FIG. <b>8</b>B</figref> is a diagram showing a comparison result of the similarity when the face is tilted in the roll direction from the front. The dotted line shows the result without processing the wearable item image, and the solid line shows the result applied with the wearable item image processing of the invention. The similarity when the synthesized image <b>403</b> generated based on the face landmarks is used at the time of authentication is smaller than the case without processing the wearable item. In particular, looking at the graph with respect to the angle in the roll direction, the difference in the similarity is about 0.05, and the invention tends to be particularly effective in the roll direction. On the other hand, when the wearable item image <b>402</b> is synthesized as it is without being processed according to the face parts and orientation, the difference in the similarity tends to increase as the angle of the face orientation increases, and setting a certain threshold value adversely affects the authentication accuracy (for example, in a case where the threshold value of the similarity is 0.15, when the face rotates more than a certain amount from the front in either the yaw direction or the roll direction, the similarity reaches the threshold value and it is determined that &#x201c;the person does not match&#x201d;). Thus, it can be seen that the invention can reduce the influence on the derivation of the similarity even when the angle of the face changes, and can improve the robustness to the angle of the face orientation.</p><heading id="h-0024" level="1">Second Embodiment</heading><p id="p-0075" num="0074">A second embodiment of the invention will describe a configuration example in which a pattern of a wearable item is detected to determine whether a wearable item is worn, and when the wearable item is worn, the wearable item image <b>402</b> corresponding to the pattern is synthesized.</p><p id="p-0076" num="0075">Here, a wearable item shape pattern includes not only the type of the wearable item (mask, glasses, goggles, etc.) but also those having different shapes even in the same type of the wearable item (for example, a mask may be a flat mask, a three-dimensional mask, a children's mask, a women's mask, etc.).</p><p id="p-0077" num="0076"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a diagram showing a detailed configuration of the face authentication system <b>1</b> in the second embodiment. Compared to the first embodiment shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the internal configurations of the face image registration unit <b>220</b> and the face image collation unit <b>320</b> are different. Others are the same as those in the first embodiment.</p><p id="p-0078" num="0077">The face image registration unit <b>220</b> has a face image recording unit <b>224</b> in an internal block. The face image collation unit <b>320</b> includes an authentication data generating unit <b>321</b>, a wearable item shape pattern detection unit <b>322</b>, a wearable item image acquisition unit <b>323</b>, a face landmark detection unit <b>324</b>, and a wearable item image processing unit <b>325</b>. The description of the blocks having the same process as the blocks of the first embodiment will be omitted, and only the internal process of the face image registration unit <b>220</b> and the face image collation unit <b>320</b> will be described.</p><p id="p-0079" num="0078"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a flowchart illustrating a process at the time of face registration in the second embodiment. The same steps as those in the first embodiment are designated by the same reference numerals, and the description thereof will be omitted. In S<b>1001</b>, the real face image generated by the face area detection unit <b>210</b> is sent to the face image recording unit <b>224</b> in the face image registration unit <b>220</b> and registered in the data storage unit <b>400</b>. In this case, the form of the real face image to be registered may be the two-dimensional image or the three-dimensional image. If the feature vector quantity is required at the time of authentication, S<b>403</b> may be added after S<b>402</b> to generate the feature vector using the feature extractor, or the real face image may be simultaneously recorded with the above data in the form of an image. When the real face image is recorded in the data storage unit <b>400</b> in the form of an image, the face image to be registered may be subjected to a lossless compression process before recording in order to reduce the data capacity.</p><p id="p-0080" num="0079"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a flowchart illustrating a process at the time of face authentication in the second embodiment. The same step numbers as those described in the first embodiment are assigned the same step numbers, and the description thereof will be omitted. Hereinafter, the differences in the second embodiment will be mainly described.</p><p id="p-0081" num="0080">The face image generated by the face area detection unit <b>310</b> in S<b>402</b> is sent to the wearable item shape pattern detection unit <b>322</b>. In S<b>1101</b>, the wearable item shape pattern detection unit <b>322</b> detects the wearable item shape from the face image. In S<b>1102</b>, the wearable item shape pattern detection unit <b>322</b> confirms the presence/absence of the wearable item based on the detection result. As an example of the pattern detection, a known method such as geometric shape pattern matching or a detector based on an NN model, or a combination thereof can be used. The presence/absence of the wearable item is determined such that, for example, in the case of the geometric shape pattern matching, it is determined that no wearable item is worn when there is no matching pattern. The detection method is not limited thereto as long as the shape and the presence/absence of the wearable item can be detected.</p><p id="p-0082" num="0081">After S<b>1101</b>, the face image extracted from the captured image is sent to the authentication data generating unit <b>321</b> and the authentication data generating unit <b>321</b> generates the authentication data (S<b>403</b>).</p><p id="p-0083" num="0082">When it is determined in S<b>1102</b> that the wearable item is worn, in S<b>1103</b>, the wearable item image acquisition unit <b>323</b> selects the wearable item image <b>402</b> that best matches the detected shape pattern, acquires the wearable item image <b>402</b> from the data storage unit <b>400</b>, and acquires the real face image <b>401</b> registered in advance. The acquired image is sent to the face landmark detection unit <b>324</b>, and the face landmarks <b>401</b>-<b>1</b> are detected based on the real face image <b>401</b> (S<b>601</b>). Based on the detected face landmarks <b>401</b>-<b>1</b>, the acquired wearable item image <b>402</b> is processed (S<b>602</b>), and the synthesized image <b>403</b> is generated to fit the real face image <b>401</b> (S<b>603</b>). The synthesized image <b>403</b> generated by the wearable item image processing unit <b>325</b> is sent to the authentication data generating unit <b>321</b> to generate the authentication data (S<b>403</b>). The individual is authenticated by collating the authentication data based on the face image extracted from the captured image at the time of authentication with the authentication data based on the generated synthesized image <b>403</b>.</p><p id="p-0084" num="0083">In S<b>1102</b>, when it is determined that no wearable item is worn at the time of authentication, the wearable item image acquisition unit <b>323</b> acquires only the registered real face image <b>401</b> without acquiring the wearable item image <b>402</b> (S<b>1104</b>), and the authentication data generating unit <b>321</b> generates the authentication data of only the real face image <b>401</b> (S<b>403</b>). By providing S<b>1102</b>, S<b>601</b> to S<b>603</b> can be skipped, and the collation speed can be increased.</p><p id="p-0085" num="0084">Although not shown in the drawings, the wearable item image acquisition unit <b>323</b> may generate a wearable item image having the same shape pattern as the detected wearable item shape pattern instead of acquiring the wearable item image <b>402</b> that best matches the wearable item shape pattern, and use the wearable item image having the same shape pattern as the wearable item image <b>402</b>. For example, when the user wears a mask at the time of authentication, a mask having the same shape as the detected mask may be generated and used as the wearable item image <b>402</b>. This operation enables to cope with a wearable item of any shapes instead of preparing multiple wearable item images <b>402</b> in the data storage unit <b>400</b> in advance. The wearable item image <b>402</b> generated or extracted in this case may be registered in the data storage unit <b>400</b> as it is, such that the wearable item image <b>402</b> can be used when generating another synthesized image <b>403</b>.</p><heading id="h-0025" level="1">Second Embodiment: Summary</heading><p id="p-0086" num="0085">The face authentication system <b>1</b> according to the second embodiment detects the wearable item shape pattern of the individual to be authenticated by the wearable item shape pattern detection unit <b>322</b>, and acquires only the wearable item image corresponding to the shape pattern from the data storage unit <b>400</b> to generate the synthesized image. As a result, the number of the synthesized images used at the time of face authentication is reduced, which can speed up the authentication process. This is particularly useful when the range of the individuals to be authenticated is specified in advance, such as the 1:1 authentication. On the other hand, when personal identification is performed for an unspecified number of individuals as in the 1:N authentication, the right half of <figref idref="DRAWINGS">FIG. <b>11</b></figref> is performed for all the individuals to be authenticated, and thus, the calculation load may become excessive. Thus, the second embodiment is particularly useful in the former.</p><heading id="h-0026" level="1">Third Embodiment</heading><p id="p-0087" num="0086"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a configuration diagram of the face authentication system <b>1</b> according to a third embodiment. In the third embodiment, in addition to the configurations described in the first and second embodiments, a three-dimensional information imaging unit <b>110</b> is newly provided. The three-dimensional information imaging unit <b>110</b> acquires distance (depth) information (that is, a distance from the three-dimensional information imaging unit <b>110</b> to a captured location) in addition to luminance information of a captured subject. With this configuration, it is possible to process and synthesize a wearable item closer to the real world, improve accuracy of deriving a similarity, and improve the robustness when a face condition changes.</p><p id="p-0088" num="0087">The three-dimensional information imaging unit <b>110</b> may be any device as long as the device can acquire the distance information such as a time-of-flat (ToF) camera or an infrared camera (IR camera). The imaging unit <b>100</b> and the three-dimensional information imaging unit <b>110</b> may be integrated to simultaneously acquire the luminance information and the distance information. Alternatively, the distance information may be acquired from parallax between pixels obtained from two imaging units by using two imaging units <b>100</b> that acquire the luminance information, such as stereo cameras or the like.</p><p id="p-0089" num="0088"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a diagram showing an internal processing block of the face authentication system <b>1</b>. Compared to the first embodiment, the internal configuration of the face image registration unit <b>220</b> and the operation in the registration flow of the synthesized image <b>403</b> are different. Since the operations of other blocks are the same as those of the first embodiment, the internal process of the face image registration unit <b>220</b> and the operations in the registration flow of the synthesized image <b>403</b> will be mainly described.</p><p id="p-0090" num="0089">The internal process of the face image registration unit <b>220</b> will be described. The face image registration unit <b>220</b> includes a three-dimensional face landmark detection unit <b>225</b>, a wearable item image processing unit <b>226</b>, and an authentication data generating unit <b>223</b>. As compared to the first embodiment, the information to be processed is expanded to three dimensions including the distance information in addition to the luminance information.</p><p id="p-0091" num="0090">The real face image generated by the face area detection unit <b>210</b> may be a three-dimensional image having the luminance information, and three-dimensional information of a distance image, or a combination of a two-dimensional image storing the luminance information and a two-dimensional image storing the distance information, and may be any form of image as long as the image has the three-dimensional information such as the luminance information and the distance information.</p><p id="p-0092" num="0091"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a flowchart illustrating a process of generating and registering the synthesized image <b>403</b> at the time of face registration. In the flowchart of <figref idref="DRAWINGS">FIG. <b>14</b></figref>, the steps having the same operations as those of the first embodiment are designated by the same reference numerals, and the description thereof will be omitted.</p><p id="p-0093" num="0092">The real face image is sent to the three-dimensional face landmark detection unit <b>225</b>. The three-dimensional face landmark detection unit <b>225</b> three-dimensionally detects the landmarks of the face based on the distance information acquired by the three-dimensional information imaging unit <b>110</b> (S<b>1401</b>). The method for detecting the three-dimensional face landmarks in S<b>1401</b> may be a known method based on geometric pattern matching, a detector based on the NN model, or the like, but is not limited to these cases as long as the face landmarks can be three-dimensionally detected.</p><p id="p-0094" num="0093">The detected three-dimensional face landmark information is sent to the wearable item image processing unit <b>226</b> together with the real face image generated by the face area detection unit <b>210</b>. The wearable item image processing unit <b>226</b> acquires the wearable item image <b>402</b> from the data storage unit <b>400</b>, three-dimensionally processes the wearable item based on the three-dimensional face landmarks (S<b>1402</b>), three-dimensionally synthesizes the wearable item to fit the face image (S<b>1403</b>), and sends the generated face image synthesized with the wearable item to the authentication data generating unit <b>223</b>.</p><p id="p-0095" num="0094">In S<b>1402</b>, the wearable item image <b>402</b> acquired from the data storage unit <b>400</b> may be either a three-dimensional image or a two-dimensional image as long as the wearable item can be three-dimensionally processed and synthesized with a three-dimensional face image. The real face image <b>401</b> and the synthesized image <b>403</b> recorded in the data storage unit <b>400</b> may be feature vectors generated by a feature extractor, three-dimensional images, two-dimensional images converted from three-dimensional images, or any combination thereof. When generating the feature vectors, the images to be input to the feature extractor may be in the form of three-dimensional images or two-dimensional images, or a combination thereof may be input to the feature extractor to generate integrated feature vectors.</p><p id="p-0096" num="0095"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a configuration diagram of the face authentication unit <b>300</b> in the third embodiment. Compared to <figref idref="DRAWINGS">FIG. <b>9</b></figref> of the second embodiment, the internal process of the face image collation unit <b>320</b> is different, and the operations of the other blocks are the same as those of the second embodiment. In the third embodiment, the face landmark detection unit <b>324</b> and the wearable item image processing unit <b>325</b> in <figref idref="DRAWINGS">FIG. <b>9</b></figref> are replaced with a three-dimensional face landmark detection unit <b>326</b> and a wearable item image processing unit <b>327</b>, respectively.</p><p id="p-0097" num="0096"><figref idref="DRAWINGS">FIG. <b>16</b></figref> is a flowchart showing operations at the time of face authentication in the third embodiment. In the flowchart of <figref idref="DRAWINGS">FIG. <b>16</b></figref>, the steps having the same operations as those of the second embodiment are designated by the same reference numerals, and the description thereof will be omitted.</p><p id="p-0098" num="0097">In S<b>1102</b>, when it is determined that the wearable item is worn, the real face image <b>401</b> and the wearable item image <b>402</b> (S<b>1103</b>) acquired by the wearable item image acquisition unit <b>323</b> are sent to the three-dimensional face landmark detection unit <b>326</b>, and the face landmarks are three-dimensionally detected (S<b>1401</b>). The detected three-dimensional face landmark information, the real face image <b>401</b>, and the wearable item image <b>402</b> are sent to the wearable item image processing unit <b>327</b>. The wearable item image <b>402</b> is three-dimensionally processed based on the three-dimensional face landmark information (S<b>1402</b>), and is three-dimensionally synthesized with the real face image <b>401</b> to fit the real face image <b>401</b> (S<b>1403</b>).</p><heading id="h-0027" level="1">Third Embodiment: Summary</heading><p id="p-0099" num="0098">Compared to the case where the face landmarks are two-dimensionally detected and the wearable item is two-dimensionally synthesized in the first and second embodiments, since the face authentication system <b>1</b> according to the third embodiment three-dimensionally processes and synthesizes the wearable item image based on the three-dimensional face landmarks, and can thus realistically reproduce the curved surface portions of the wearable item even when the face orientation changes, and can synthesize the wearable item image <b>402</b> with the real face image <b>401</b> with higher accuracy. This enables to improve the accuracy of deriving the similarity by realistically reproducing the wearing state of the wearable item and to improve the robustness of the similarity by the face orientation. Further, at the time of face authentication, the distance information is also included in matching data, so that the authentication accuracy of the real face image alone can be improved as well as the case where the wearable item is worn.</p><heading id="h-0028" level="1">Fourth Embodiment</heading><p id="p-0100" num="0099">In a fourth embodiment of the invention, a configuration for improving the collation efficiency and speeding up the collation process in the first embodiment will be described. In the first embodiment, as a variation of the wearable item increases, the number of synthesized images <b>403</b> registered in the data storage unit <b>400</b> also increases proportionally (especially in the 1:N authentication, the registered face data increases depending on the number of registered individuals). Therefore, sequentially collating all the registered face data stored in the data storage unit <b>400</b> increases the time required for collation. The fourth embodiment is intended to speed up the collation process.</p><p id="p-0101" num="0100"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is a configuration diagram of the face authentication unit <b>300</b> in the fourth embodiment. The difference from the first embodiment is that a collation category detection unit <b>328</b> and a collation data acquisition unit <b>329</b> are added in the face image collation unit <b>320</b>. The merits of adding the collation category detection unit <b>328</b> include, for example, that the presence/absence of the wearable item (whether a real face or wearing a wearable item), the type of the wearable item (mask, goggles, glasses, hat, etc.), attributes of the individual (age, gender, etc.), the face orientation (a combination of one or more of yaw, roll, and pitch), and the like can be detected in advance, and the category of the registered face data used for collation can be narrowed down to the category corresponding to the detected attributes. As a result, the collation efficiency is improved, and the collation time can be reduced as compared to the case where all the registered images are sequentially authenticated.</p><p id="p-0102" num="0101"><figref idref="DRAWINGS">FIG. <b>18</b></figref> is a flowchart illustrating operations at the time of authentication in the fourth embodiment. In the flowchart of <figref idref="DRAWINGS">FIG. <b>18</b></figref>, the steps having the same operations as those of the first embodiment are designated by the same reference numerals, and the description thereof will be omitted.</p><p id="p-0103" num="0102">The face image generated by the face area detection unit <b>310</b> is sent to the collation category detection unit <b>328</b>. The collation category detection unit <b>328</b> detects the category of the face image (S<b>1801</b>). Examples of collation categories include the presence/absence of the wearable item, the type of the wearable item, the attributes of the individual, the face orientation, or a combination thereof. As the category detection method, known methods such as geometric shape pattern matching, a detector based on an NN model, or a combination thereof can be used. Two or more NN model detectors may be combined. Examples thereof include a combination of an NN model detector that determines the type of a wearable item and a dedicated NN model detector that determines the classification of gender. Information of the detected category is sent to the collation data acquisition unit <b>329</b>. The collation data acquisition unit <b>329</b> acquires a registered image (real face image <b>401</b> or synthesized image <b>403</b>) suitable for the category detected from the data storage unit <b>400</b> (S<b>1802</b>), and sends the registered image to the authentication data generating unit <b>321</b>.</p><heading id="h-0029" level="1">Fourth Embodiment: Summary</heading><p id="p-0104" num="0103">The face authentication system <b>1</b> according to the fourth embodiment acquires only the real face image <b>401</b> or the synthesized image <b>403</b> corresponding to the type of the wearable item or the attributes of the individual to be authenticated from the data storage unit <b>400</b>, and uses the image to perform face authentication. As a result, the registered face data used at the time of collation can be narrowed down, and the number of collations can be reduced, so that the collation efficiency can be improved, and the collation speed can be increased.</p><heading id="h-0030" level="1">Fifth Embodiment</heading><p id="p-0105" num="0104">A fifth embodiment of the invention will describe a configuration in which robustness to face orientation is improved by changing the face orientation in order to carry out face authentication more robust to the face orientation. In addition, in applications such as simultaneous face authentication of a plurality of users using signage and face authentication using surveillance cameras, the face orientation at the time of authentication is not limited to the front face, and the face orientation angle varies greatly. Thus, for example, authentication is required to be performed when the face is facing diagonally. The fifth embodiment is intended to further improve the authentication robustness in such a case.</p><p id="p-0106" num="0105"><figref idref="DRAWINGS">FIG. <b>19</b></figref> is a configuration diagram of the face registration unit <b>200</b> in the fifth embodiment. The difference from the first embodiment is that a face orientation changing unit <b>227</b> is added in the face image registration unit <b>220</b>. The merit of adding the face orientation changing unit <b>227</b> is that a large number of real face images <b>401</b> having various face orientations and the synthesized images <b>403</b> can be stored in the data storage unit <b>400</b> in advance, and highly accurate authentication can be implemented regardless of the face orientation at the time of authentication. The description of the blocks common to the first embodiment will be omitted, and the face orientation changing unit <b>227</b> having different operations will be described below.</p><p id="p-0107" num="0106">The real face image generated by the face area detection unit <b>210</b> is first sent to the face orientation changing unit <b>227</b>. The face orientation changing unit <b>227</b> changes the face orientation angle of the input face image. The face image whose face orientation has been changed is sent to the authentication data generating unit <b>223</b> as it is when the real face image <b>401</b> is to be generated, and is sent to the face landmark detection unit <b>221</b> when the synthesized image <b>403</b> is to be generated.</p><p id="p-0108" num="0107">The face orientation changing unit <b>227</b> can use a known method such as affine transformation or face orientation transformation by non-rigid body deformation method based on FFD. In the non-rigid body deformation method based on FFD, the face image is provided with a control point thereon, and the face image is deformed by moving the control point. When a face landmark is used as the control point, the face landmark detection unit <b>221</b> may be arranged previous to the face orientation changing unit <b>227</b>, so that the face landmark detection is performed first and then the face orientation is changed. The face orientation changing unit <b>227</b> may generate face images in a plurality of orientations, and then complement and generate face images in the middle orientation from the plurality of face images having different face orientations. For example, a face image diagonally oriented in the middle is generated from the front and sideways face images.</p><p id="p-0109" num="0108"><figref idref="DRAWINGS">FIG. <b>20</b></figref> is a diagram showing the effect of the fifth embodiment. The diagram shows an example in which the face orientation is changed in two directions by the face orientation changing unit <b>227</b>, and then a mask image is synthesized as the wearable item image <b>402</b>. Based on a real face image <b>20</b>-<b>1</b> generated by the face area detection unit <b>210</b>, by changing the face orientation, a real face image <b>20</b>-<b>2</b> tilted at an angle in the roll direction and a real face image <b>20</b>-<b>3</b> tilted at an angle in the yaw direction are generated. The face landmarks are detected, and the wearable item image is processed and synthesized to fit the face orientation for each of the real face images <b>20</b>-<b>1</b>, <b>20</b>-<b>2</b>, and <b>20</b>-<b>3</b>, such that wearing-item-synthesized face images <b>20</b>-<b>4</b>, <b>20</b>-<b>5</b>, <b>20</b>-<b>6</b> are generated, respectively. By recording the images <b>20</b>-<b>1</b> to <b>20</b>-<b>6</b> in the data storage unit <b>400</b>, face authentication corresponding to the face orientation angle in three directions including before and after the face orientation change at the time of authentication is possible. In the example of <figref idref="DRAWINGS">FIG. <b>20</b></figref>, the face orientation is changed in two directions, but may also be changed in any direction, and the number thereof is not limited.</p><p id="p-0110" num="0109">The fifth embodiment may speed up the collation at the time of authentication in combination with the fourth embodiment. In a collation category detection at the time of face authentication, the collation efficiency can be improved by detecting the face orientation and acquiring the registered face data having the same face orientation as the detected face orientation from the data storage unit <b>400</b>.</p><p id="p-0111" num="0110"><figref idref="DRAWINGS">FIG. <b>21</b></figref> is a configuration diagram of the face authentication unit <b>300</b> in the fifth embodiment. The difference from the second embodiment is that a face orientation changing unit <b>331</b> is added to the face image collation unit <b>320</b>, and the wearable item shape pattern detection unit <b>322</b> is replaced with a wearable item shape pattern detection unit <b>330</b>. The merit of adding the face orientation changing unit <b>331</b> as compared to the second embodiment is that the face orientation of the face image registered in the data storage unit <b>400</b> can be changed according to the face orientation at the time of authentication. As a result, it is possible to generate a synthesized image having the same face orientation angle as the face orientation angle of the user at the time of authentication, so that the accuracy of deriving the similarity is improved. Hereinafter, the description of the blocks common to the second embodiment will be omitted, and the blocks having different operations will be described.</p><p id="p-0112" num="0111">From the face image generated by the face area detection unit <b>310</b>, the wearable item shape pattern detection unit <b>330</b> detects patterns of the worn wearable item and the face orientation at the time of authentication. Based on detected face orientation information, the face orientation changing unit <b>331</b> changes the acquired real face image <b>401</b>. The real face image <b>401</b> whose face orientation is changed according to the face orientation angle at the time of authentication is sent to the face landmark detection unit <b>324</b> for processing and synthesizing the wearable item. If the user does not wear the wearable item at the time of authentication, the real face image <b>401</b> whose face orientation is changed is sent to the authentication data generating unit <b>321</b> as it is.</p><p id="p-0113" num="0112">The wearable item shape pattern detection unit <b>330</b> also detects the face orientation in addition to the detection of the wearable item shape pattern described in S<b>1101</b> of <figref idref="DRAWINGS">FIG. <b>11</b></figref>. The face orientation detection may be performed with a known method such as a method of detecting face landmarks and comparing the face landmarks with face landmarks when facing the front at a predetermined point to estimate the face orientation angle, a detector based on an NN model, or the like.</p><heading id="h-0031" level="1">Fifth Embodiment: Summary</heading><p id="p-0114" num="0113">In both cases of the first embodiment and the second embodiment, the face authentication system <b>1</b> according to the fifth embodiment can flexibly change the face orientation angles of the real face image <b>401</b> and the synthesized image <b>403</b> used at the time of authentication by the face orientation changing unit <b>331</b>, and then store the images in the data storage unit <b>400</b> in advance. As a result, it is possible to implement a face authentication system that enables highly accurate face authentication and is more robust to the face orientation angle even when the face orientation of the user faces various directions at the time of authentication.</p><heading id="h-0032" level="2">Modifications of Invention</heading><p id="p-0115" num="0114">The invention is not limited to the embodiments described above, and includes various modifications. For example, the embodiments described above have been described in detail for easily understanding the invention, and the invention is not necessarily limited to those including all the configurations described above. Further, a part of the configuration of one embodiment can be replaced with the configuration of another embodiment, and the configuration of another embodiment can be added to the configuration of one embodiment. In addition, a part of the configurations of each embodiment could be added, deleted, or replaced with other configurations.</p><p id="p-0116" num="0115">The above embodiments have described an example in which the face registration unit <b>200</b> and the face authentication unit <b>300</b> are connected to the same imaging unit <b>100</b>, but the face registration unit <b>200</b> and the face authentication unit <b>300</b> may be each provided with an imaging unit <b>100</b>.</p><p id="p-0117" num="0116">The above embodiments have shown an integral face authentication system <b>1</b>, but the imaging unit <b>100</b>, the face registration unit <b>200</b>, the face authentication unit <b>300</b>, and the data storage unit <b>400</b> may be arranged in different locations in space, and may be configured to wirelessly communicate data with the data base station <b>2</b>.</p><p id="p-0118" num="0117">In the above embodiments, the authentication data generating unit may extract the features of the synthesized image by using a feature extractor that differs depending on the type of wearable item (including the presence/absence of the wearable item), attributes of an individual, face orientation, or a combination thereof. For example, in the first embodiment, a feature extractor for real face image is used when the individual to be authenticated does not wear a wearable item, and a feature extractor for masks is used when the individual to be authenticated wears a mask. Alternatively, in the fourth embodiment, a feature extractor for males is used when the attribute of the individual to be authenticated is &#x201c;male&#x201d;. Alternatively, a feature extractor for masks and males is used when the individual to be authenticated is &#x201c;mask, male&#x201d;. Similarly, when registering the face image, a feature extractor that differs depending on the type of the wearable item and the attributes of the individual may be used.</p><p id="p-0119" num="0118">In the embodiments described above, the face registration unit <b>200</b> and the face authentication unit <b>300</b> (and each functional unit arranged as an internal block thereof) may be implemented by hardware such as a circuit device that implements these functions, or may also be provided by executing software that implements these functions by an arithmetic unit (for example, central processing unit (CPU)).</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A face authentication system for identifying an individual by using a face image, the face authentication system comprising:<claim-text>a synthesis unit configured to generate a synthesized image by synthesizing a wearable item image with a real face image of the individual, the wearable item image being an image of a wearable item worn by the individual; and</claim-text><claim-text>a face authentication unit configured to identify the individual by using the synthesized image, wherein</claim-text><claim-text>the synthesis unit is configured to generate the synthesized image by deforming the wearable item image to fit a face shape of the individual.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The face authentication system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the synthesis unit is configured to deform the wearable item image to fit the face shape of the individual with reference to a landmark on the real face image.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The face authentication system according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein<claim-text>the synthesis unit is configured to detect, according to the landmark, at least one of a yaw angle, a roll angle, or a pitch angle at which the face image of the individual is rotated with reference to a time when the individual faces the front, and</claim-text><claim-text>the synthesis unit is configured to deform the wearable item image to fit the face shape of the individual by rotating the wearable item image according to the detected angle.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The face authentication system according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein<claim-text>the synthesis unit is configured to deform the wearable item image such that a vertical size of the wearable item image matches a vertical size of a position of wearing the wearable item among landmarks,</claim-text><claim-text>the synthesis unit is configured to deform the wearable item image such that a horizontal size of the wearable item image matches a horizontal size of the position of wearing the wearable item among the landmarks, and</claim-text><claim-text>the synthesis unit is configured to deform the wearable item image for each of a plurality of divided areas of the wearable item image to match the vertical size of the wearable item image with the vertical size of the wearing position, and match the horizontal size of the wearable item image with the horizontal size of the wearing position.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The face authentication system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>a wearable item shape pattern detection unit configured to detect a wearable item shape pattern from the face image of the individual, wherein</claim-text><claim-text>the face authentication unit is configured to identify the individual by using the synthesized image when the face authentication unit detects the wearable item shape pattern from the face image, and</claim-text><claim-text>the face authentication unit is configured to identify the individual by using the real face image when the face authentication unit does not detect the wearable item shape pattern from the face image.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The face authentication system according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, further comprising:<claim-text>a data storage unit configured to store a plurality of types of wearable item images, wherein</claim-text><claim-text>the face authentication unit is configured to acquire only the wearable item image corresponding to the detected wearable item shape pattern from the data storage unit among the plurality of types of wearable item images, and</claim-text><claim-text>the synthesis unit is configured to generate the synthesized image by using only the wearable item image acquired from the data storage unit among the plurality of types of wearable item images.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The face authentication system according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein<claim-text>the synthesis unit is configured to acquire, as the real face image, a three-dimensional image including luminance information and distance information of the face image of the individual,</claim-text><claim-text>the synthesis unit is configured to identify, as the landmark, a feature point in a three-dimensional coordinate system on the real face image, and</claim-text><claim-text>the synthesis unit is configured to fit the wearable item image with the face shape of the individual by transforming the wearable item image in the three-dimensional coordinate system with reference to the landmark in the three-dimensional coordinate system.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The face authentication system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>a data storage unit configured to store the synthesized image for each type of the wearable item; and</claim-text><claim-text>a category detection unit configured to detect the type of the wearable item, wherein</claim-text><claim-text>the synthesis unit is configured to acquire only the synthesized image corresponding to the detected type of the wearable item from the data storage unit, and</claim-text><claim-text>the face authentication unit is configured to identify the individual by using the synthesized image acquired from the data storage unit.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The face authentication system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>a data storage unit configured to store the real face image and the synthesized image for each individual attribute; and</claim-text><claim-text>a category detection unit configured to detect an attribute of the individual, wherein</claim-text><claim-text>the synthesis unit is configured to acquire only the real face image corresponding to the detected attribute of the individual or the synthesized image corresponding to the detected attribute of the individual from the data storage unit, and</claim-text><claim-text>the face authentication unit is configured to identify the individual by using the real face image or the synthesized image acquired from the data storage unit.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The face authentication system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>a face orientation changing unit configured to change a face orientation of the real face image, wherein</claim-text><claim-text>the synthesis unit is configured to perform the same orientation change as the face orientation changed by the face orientation changing unit on the wearable item image, and</claim-text><claim-text>the synthesis unit is configured to generate the synthesized image by synthesizing the wearable item image applied with the orientation change with respect to the real face image.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The face authentication system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>a data storage unit configured to store the real face image, wherein</claim-text><claim-text>the data storage unit is configured to store at least the real face image in which the individual faces the front and the real face image in which the individual tilts a face at an angle.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The face authentication system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the wearable item is an object that covers at least a part of a face of the individual when worn by the individual.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The face authentication system according to <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein<claim-text>the wearable item is at least one of a mask, glasses, goggles, or a hat.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The face authentication system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>an authentication data generating unit configured to extract a feature of the synthesized image as authentication data used to identify the individual, wherein</claim-text><claim-text>the face authentication unit is configured to identify the individual by comparing a feature of the face image of the individual with the feature extracted from the synthesized image.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The face authentication system according to <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein<claim-text>the authentication data generating unit is configured to extract the feature of the synthesized image by using a feature extractor corresponding to a type of the wearable item, a feature extractor corresponding to an attribute of the individual, or a feature extractor corresponding to a face orientation.</claim-text></claim-text></claim></claims></us-patent-application>