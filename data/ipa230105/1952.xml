<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230001953A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230001953</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17570168</doc-number><date>20220106</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>B</section><class>60</class><subclass>W</subclass><main-group>60</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>B</section><class>60</class><subclass>W</subclass><main-group>50</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>B</section><class>60</class><subclass>W</subclass><main-group>10</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>B</section><class>60</class><subclass>W</subclass><main-group>10</main-group><subgroup>18</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>B</section><class>60</class><subclass>W</subclass><main-group>10</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>05</class><subclass>B</subclass><main-group>13</main-group><subgroup>02</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>05</class><subclass>B</subclass><main-group>13</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20200201</date></cpc-version-indicator><section>B</section><class>60</class><subclass>W</subclass><main-group>60</main-group><subgroup>0027</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>60</class><subclass>W</subclass><main-group>50</main-group><subgroup>0098</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>60</class><subclass>W</subclass><main-group>10</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>60</class><subclass>W</subclass><main-group>10</main-group><subgroup>18</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>60</class><subclass>W</subclass><main-group>10</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>05</class><subclass>B</subclass><main-group>13</main-group><subgroup>0265</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>05</class><subclass>B</subclass><main-group>13</main-group><subgroup>048</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20200201</date></cpc-version-indicator><section>B</section><class>60</class><subclass>W</subclass><main-group>2554</main-group><subgroup>4029</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>60</class><subclass>W</subclass><main-group>2050</main-group><subgroup>0022</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>60</class><subclass>W</subclass><main-group>2050</main-group><subgroup>0028</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>60</class><subclass>W</subclass><main-group>2710</main-group><subgroup>18</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>60</class><subclass>W</subclass><main-group>2710</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">PLANNING-AWARE PREDICTION FOR CONTROL-AWARE AUTONOMOUS DRIVING MODULES</invention-title><us-related-documents><us-provisional-application><document-id><country>US</country><doc-number>63250957</doc-number><date>20210930</date></document-id></us-provisional-application><us-provisional-application><document-id><country>US</country><doc-number>63212032</doc-number><date>20210617</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>TOYOTA RESEARCH INSTITUTE, INC.</orgname><address><city>Los Altos</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant><us-applicant sequence="01" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>THE REGENTS OF THE UNIVERSITY OF CALIFORNIA</orgname><address><city>Oakland</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>MCALLISTER</last-name><first-name>Rowan Thomas</first-name><address><city>Berkeley</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>WULFE</last-name><first-name>Blake Warren</first-name><address><city>San Francisco</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>MERCAT</last-name><first-name>Jean</first-name><address><city>Mountain View</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>ELLIS</last-name><first-name>Logan Michael</first-name><address><city>San Jose</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>LEVINE</last-name><first-name>Sergey</first-name><address><city>Berkeley</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="05" designation="us-only"><addressbook><last-name>GAIDON</last-name><first-name>Adrien David</first-name><address><city>San Jose</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>TOYOTA RESEARCH INSTITUTE, INC.</orgname><role>02</role><address><city>Los Altos</city><state>CA</state><country>US</country></address></addressbook></assignee><assignee><addressbook><orgname>THE REGENTS OF THE UNIVERSITY OF CALIFORNIA</orgname><role>02</role><address><city>Oakland</city><state>CA</state><country>US</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A method of generating an output trajectory of an ego vehicle includes recording trajectory data of the ego vehicle and pedestrian agents from a scene of a training environment of the ego vehicle. The method includes identifying at least one pedestrian agent from the pedestrian agents within the scene of the training environment of the ego vehicle causing a prediction-discrepancy by the ego vehicle greater than the pedestrian agents within the scene. The method includes updating parameters of a motion prediction model of the ego vehicle based on a magnitude of the prediction-discrepancy caused by the at least one pedestrian agent on the ego vehicle to form a trained, control-aware prediction objective model. The method includes selecting a vehicle control action of the ego vehicle in response to a predicted motion from the trained, control-aware prediction objective model regarding detected pedestrian agents within a traffic environment of the ego vehicle.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="121.92mm" wi="158.75mm" file="US20230001953A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="245.53mm" wi="159.60mm" file="US20230001953A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="201.17mm" wi="162.39mm" orientation="landscape" file="US20230001953A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="233.60mm" wi="158.33mm" file="US20230001953A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="204.39mm" wi="120.73mm" orientation="landscape" file="US20230001953A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="240.54mm" wi="136.91mm" file="US20230001953A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="229.36mm" wi="153.84mm" orientation="landscape" file="US20230001953A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="229.45mm" wi="153.84mm" orientation="landscape" file="US20230001953A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="251.54mm" wi="151.98mm" orientation="landscape" file="US20230001953A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="231.31mm" wi="140.21mm" file="US20230001953A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION</heading><p id="p-0002" num="0001">The present application claims the benefit of U.S. Provisional Patent Application No. 63/250,957, filed Sep. 30, 2021, and titled &#x201c;CONTROL-AWARE PREDICTION OBJECTIVES FOR AUTONOMOUS DRIVING,&#x201d; and U.S. Provisional Patent Application No. 63/212,032, filed Jun. 17, 2021, and titled &#x201c;BACKPROPAGATION OF CONCERNS: CONTROL-AWARE METRICS FOR AUTONOMOUS DRIVING MODULES,&#x201d; the disclosures of which are expressly incorporated by reference herein in their entireties.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND</heading><heading id="h-0003" level="1">Field</heading><p id="p-0003" num="0002">Certain aspects of the present disclosure generally relate to autonomous vehicle technology and, more particularly, to a prediction metric that improves the accuracy of learned motion-prediction models for autonomous vehicles.</p><heading id="h-0004" level="1">Background</heading><p id="p-0004" num="0003">Autonomous agents rely on machine vision for sensing a surrounding environment by analyzing areas of interest in a scene from images of the surrounding environment. Although scientists spent decades studying the human visual system, a solution for realizing equivalent machine vision remains elusive, but is a goal for enabling truly autonomous agents. Machine vision, however, is distinct from the field of digital image processing. In particular, machine vision involves recovering a three-dimensional (3D) structure of the world from images and using the 3D structure for fully understanding a scene. That is, machine vision strives to provide a high-level understanding of a surrounding environment, as performed by the human visual system.</p><p id="p-0005" num="0004">Autonomous agents, such as driverless cars and robots, quickly evolved and are a reality in this decade. Because autonomous agents interact with humans, however, many critical concerns arise. For example, one critical concern is how to design vehicle control of an autonomous vehicle using machine learning. Unfortunately, vehicle control by machine learning is less effective in complicated traffic environments involving complex interactions between vehicles (e.g., in situations where an ego vehicle maneuvers through roadway traffic and intersections).</p><p id="p-0006" num="0005">Human drivers navigate busy roads by carefully observing, anticipating, and reacting to the potential actions of other pedestrians and/or vehicles. Similarly, autonomous vehicles (AVs) use learned perceptual and predictive components for detecting and forecasting surrounding road users to plan safe motions. In particular, safe operation involves learned components that are well trained, for example, by reducing certain classification or regression errors on training data. Nevertheless, not all errors are equally important: some errors have a minimal effect on downstream decisions, while other errors may be catastrophic. For example, errors in detecting vehicles and forecasting pedestrians have resulted in fatal collisions, while errors associated with those unlikely to interact with an autonomous vehicle (AV) are likely inconsequential and uncorrelated with overall vehicle performance. While no model is perfect, a model that considers how errors propagate downstream to identify errors likely having significant real-life costs is desired for mitigating these significant errors and improving overall AV performance.</p><heading id="h-0005" level="1">SUMMARY</heading><p id="p-0007" num="0006">A method of generating an output trajectory of an ego vehicle includes recording trajectory data of the ego vehicle and pedestrian agents from a scene of a training environment of the ego vehicle. The method includes identifying at least one pedestrian agent from the pedestrian agents within the scene of the training environment of the ego vehicle causing a prediction-discrepancy by the ego vehicle greater than the pedestrian agents within the scene. The method includes updating parameters of a motion prediction model of the ego vehicle based on a magnitude of the prediction-discrepancy caused by the at least one pedestrian agent on the ego vehicle to form a trained, control-aware prediction objective model. The method includes selecting a vehicle control action of the ego vehicle in response to a predicted motion from the trained, control-aware prediction objective model regarding detected pedestrian agents within a traffic environment of the ego vehicle.</p><p id="p-0008" num="0007">A non-transitory computer-readable medium having program code recorded thereon for generating an output trajectory of an ego vehicle is described. The program code is executed by a processor. The non-transitory computer-readable medium includes program code to record trajectory data of the ego vehicle and pedestrian agents from a scene of a training environment of the ego vehicle. The non-transitory computer-readable medium also includes program code to identify at least one pedestrian agent from the pedestrian agents within the scene of the training environment of the ego vehicle causing a prediction-discrepancy by the ego vehicle greater than the pedestrian agents within the scene. The non-transitory computer-readable medium further includes program code to update parameters of a motion prediction model of the ego vehicle based on a magnitude of the prediction-discrepancy caused by the at least one pedestrian agent on the ego vehicle to form a trained, control-aware prediction objective model. The non-transitory computer-readable medium also includes program code to select a vehicle control action of the ego vehicle in response to a predicted motion from the trained, control-aware prediction objective model regarding detected pedestrian agents within a traffic environment of the ego vehicle.</p><p id="p-0009" num="0008">A system for generating an output trajectory of an ego vehicle is described. The system includes a vehicle perception module to record trajectory data of the ego vehicle and pedestrian agents from a scene of a training environment of the ego vehicle. The system also includes a control-aware prediction objective model to identify at least one pedestrian agent from the pedestrian agents within the scene of the training environment of the ego vehicle causing a prediction-discrepancy by the ego vehicle greater than the pedestrian agents within the scene. The system further includes a model parameter update module to update parameters of a motion prediction model of the ego vehicle based on a magnitude of the prediction-discrepancy caused by the at least one pedestrian agent on the ego vehicle to form a trained, control-aware prediction objective model. The system also includes a vehicle action selection module to select a vehicle control action of the ego vehicle in response to a predicted motion from the trained, control-aware prediction objective model regarding detected pedestrian agents within a traffic environment of the ego vehicle.</p><p id="p-0010" num="0009">This has outlined, rather broadly, the features and technical advantages of the present disclosure in order that the detailed description that follows may be better understood. Additional features and advantages of the present disclosure will be described below. It should be appreciated by those skilled in the art that the present disclosure may be readily utilized as a basis for modifying or designing other structures for carrying out the same purposes of the present disclosure. It should also be realized by those skilled in the art that such equivalent constructions do not depart from the teachings of the present disclosure as set forth in the appended claims. The novel features, which are believed to be characteristic of the present disclosure, both as to its organization and method of operation, together with further objects and advantages, will be better understood from the following description when considered in connection with the accompanying figures. It is to be expressly understood, however, that each of the figures is provided for the purpose of illustration and description only and is not intended as a definition of the limits of the present disclosure.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0006" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0011" num="0010">The features, nature, and advantages of the present disclosure will become more apparent from the detailed description set forth below when taken in conjunction with the drawings in which like reference characters identify correspondingly throughout.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates an example implementation of designing a neural network using a system-on-a-chip (SOC) for control-aware motion prediction in an autonomous vehicle planner system, in accordance with aspects of the present disclosure.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram illustrating a software architecture that may modularize artificial intelligence (AI) functions for control-aware motion prediction in an action planner system of an autonomous agent, according to aspects of the present disclosure.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram illustrating an example of a hardware implementation for a control-aware motion prediction objective in a vehicle action planner system, according to aspects of the present disclosure.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram illustrating an overview of a roadway environment, including an ego vehicle having a data driven trajectory planner, according to aspects of the present disclosure.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIGS. <b>5</b>A and <b>5</b>B</figref> are block diagrams illustrating a gate recurrent unit (GRU) encoder/decoder architecture for trajectory prediction based on a trained attention model, according to aspects of the present disclosure.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a drawing of a roadway environment illustrating a pedestrian crossing scenario relative to an ego vehicle approaching a crosswalk in which a collision is predicted by the ego vehicle, according to aspects of the present disclosure.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a drawing of the roadway environment illustrating the pedestrian crossing scenario relative to the ego vehicle approaching the crosswalk of <figref idref="DRAWINGS">FIG. <b>6</b></figref>, in which no collision is predicted by the ego vehicle, according to aspects of the present disclosure.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is an overhead view of a roadway environment illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref> of the pedestrian crossing scenario, according to aspects of the present disclosure.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a flowchart illustrating a method of generating an output trajectory of an ego vehicle, according to aspects of the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0007" level="1">DETAILED DESCRIPTION</heading><p id="p-0021" num="0020">The detailed description set forth below, in connection with the appended drawings, is intended as a description of various configurations and is not intended to represent the only configurations in which the concepts described herein may be practiced. The detailed description includes specific details for the purpose of providing a thorough understanding of the various concepts. It will be apparent to those skilled in the art, however, that these concepts may be practiced without these specific details. In some instances, well-known structures and components are shown in block diagram form in order to avoid obscuring such concepts.</p><p id="p-0022" num="0021">Based on the teachings, one skilled in the art should appreciate that the scope of the present disclosure is intended to cover any aspect of the present disclosure, whether implemented independently of or combined with any other aspect of the present disclosure. For example, an apparatus may be implemented or a method may be practiced using any number of the aspects set forth. In addition, the scope of the present disclosure is intended to cover such an apparatus or method practiced using other structure, functionality, or structure and functionality in addition to, or other than the various aspects of the present disclosure set forth. It should be understood that any aspect of the present disclosure disclosed may be embodied by one or more elements of a claim.</p><p id="p-0023" num="0022">Although particular aspects are described herein, many variations and permutations of these aspects fall within the scope of the present disclosure. Although some benefits and advantages of the preferred aspects are mentioned, the scope of the present disclosure is not intended to be limited to particular benefits, uses, or objectives. Rather, aspects of the present disclosure are intended to be broadly applicable to different technologies, system configurations, networks and protocols, some of which are illustrated by way of example in the figures and in the following description of the preferred aspects. The detailed description and drawings are merely illustrative of the present disclosure, rather than limiting the scope of the present disclosure being defined by the appended claims and equivalents thereof.</p><p id="p-0024" num="0023">Autonomous agents, such as driverless cars and robots, quickly evolved and are a reality in this decade. Because autonomous agents interact with humans, however, many critical concerns arise. For example, one critical concern is how to design vehicle control of an autonomous vehicle using machine learning. Unfortunately, vehicle control by machine learning is less effective in complicated traffic environments involving complex interactions between vehicles. For example, an ego vehicle may rely on a planner to safely maneuver through complex traffic environments and traffic intersections.</p><p id="p-0025" num="0024">Human drivers navigate busy roads by carefully observing, anticipating, and reacting to the potential actions of other pedestrians and/or vehicles. Similarly, autonomous vehicles (AVs) use learned perceptual and predictive components for detecting and forecasting surrounding road users to plan safe motions. In particular, safe autonomous vehicle operation involves learned components that are well trained, for example, by reducing certain classification or regression errors on training data; however, all errors are not equally important. For example, some errors have a minimal effect on downstream decisions, while other errors may be catastrophic. In fact, errors in detecting vehicles and forecasting pedestrians have resulted in fatal collisions, while errors associated with those unlikely to interact with an autonomous vehicle (AV) are likely inconsequential and uncorrelated with overall vehicle performance. While no model is perfect, a model that considers how errors propagate downstream to identify errors likely having a significant real-life cost is desired for mitigating these significant errors and improving overall AV performance.</p><p id="p-0026" num="0025">Whether trained independently or as part of multi-task end-to-end architectures, multi-agent trajectory forecasting models typically optimize prediction-specific objectives based on regressing recorded future trajectories by considering all agents equally important a priori. Nevertheless, when considering the target control task of autonomous navigation, some predictions warrant more attention than others when deciding safety controls. Consequently, control-agnostic optimizing of prediction models may not result in improved downstream navigation performance due to limited data, model capacity, rare events, or computational constraints. Even with end-to-end training, multi-task objectives might not be aligned, thus resulting in performance degradation due to task interference.</p><p id="p-0027" num="0026">Aspects of the present disclosure are directed to control-aware prediction objectives (CAPOs) to train prediction models that more accurately reflect the relative effects of predictive errors on downstream control. Computing these downstream effects involves forward passes without backpropagation between modules. This improves applicability with real-world AV planning and control systems, which might not be fully differentiable due to complex design constraints (e.g., verifiability, interpretability, comfort and safety constraints). Our method introduces importance-weighted prediction likelihood objectives using forward passes of the prediction model and planner. In aspects of the present disclosure, two weighting methods that can be trained with backpropagation. The first assigns weights based on control variations due to prediction changes. The second uses learned attention weights between agent predictions and AV controls.</p><p id="p-0028" num="0027">Some aspects of the present disclosure involve training prediction models with control-aware objectives that lead to improved controller performance in complex multi-agent urban driving scenarios. By evaluating existing prediction models based on these control-aware metrics, the models according to aspects of the present disclosure are more likely to avoid precisely those errors that would maximally influence downstream decisions. In particular, some aspects of the present disclosure directly identify those errors that would maximally influence downstream decisions, including prediction algorithms that treat everything equally.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates an example implementation of a control-aware prediction objective module for a vehicle action planner using a system-on-a-chip (SOC) <b>100</b> of an autonomous vehicle <b>150</b>. The SOC <b>100</b> may include a single processor or multi-core processors (e.g., a central processing unit (CPU) <b>102</b>), in accordance with certain aspects of the present disclosure. Variables (e.g., neural signals and synaptic weights), system parameters associated with a computational device (e.g., neural network with weights), delays, frequency bin information, and task information may be stored in a memory block. The memory block may be associated with a neural processing unit (NPU) <b>108</b>, a CPU <b>102</b>, a graphics processing unit (GPU) <b>104</b>, a digital signal processor (DSP) <b>106</b>, a dedicated memory block <b>118</b>, or may be distributed across multiple blocks. Instructions executed at a processor (e.g., CPU <b>102</b>) may be loaded from a program memory associated with the CPU <b>102</b> or may be loaded from the dedicated memory block <b>118</b>.</p><p id="p-0030" num="0029">The SOC <b>100</b> may also include additional processing blocks configured to perform specific functions, such as the GPU <b>104</b>, the DSP <b>106</b>, and a connectivity block <b>110</b>, which may include fifth generation (5G) cellular network technology, fourth generation long term evolution (4G LTE) connectivity, unlicensed WiFi connectivity, USB connectivity, Bluetooth&#xae; connectivity, and the like. In addition, a multimedia processor <b>112</b> in combination with a display <b>130</b> may, for example, apply a temporal component of a current traffic state to select a vehicle behavior control action, according to the display <b>130</b> illustrating a view of a vehicle. In some aspects, the NPU <b>108</b> may be implemented in the CPU <b>102</b>, DSP <b>106</b>, and/or GPU <b>104</b>. The SOC <b>100</b> may further include a sensor processor <b>114</b>, image signal processors (ISPs) <b>116</b>, and/or navigation <b>120</b>, which may, for instance, include a global positioning system.</p><p id="p-0031" num="0030">The SOC <b>100</b> may be based on an Advanced Risk Machine (ARM) instruction set or the like. In another aspect of the present disclosure, the SOC <b>100</b> may be a server computer in communication with the autonomous vehicle <b>150</b>. In this arrangement, the autonomous vehicle <b>150</b> may include a processor and other features of the SOC <b>100</b>. In this aspect of the present disclosure, instructions loaded into a processor (e.g., CPU <b>102</b>) or the NPU <b>108</b> of the autonomous vehicle <b>150</b> may include program code to determine one or more merge gaps between vehicles in a target lane of a multilane highway based on images processed by the sensor processor <b>114</b>. The instructions loaded into a processor (e.g., CPU <b>102</b>) may also include program code executed by the processor.</p><p id="p-0032" num="0031">In aspects of the present disclosure, the instructions include program code to record trajectory data of an ego vehicle and agents from a scene of a training environment of the ego vehicle. The instructions also include program code to identify at least one agent from the agents within the scene of the training environment of the ego vehicle causing a prediction-discrepancy by the ego vehicle greater than the agents within the scene of the training environment. The instructions also include program code to update parameters of a motion prediction model of the ego vehicle based on a magnitude of the prediction-discrepancy caused by the at least one agent on the ego vehicle to form a trained, control-aware prediction module. The instructions also include program code to select a vehicle control action of the ego vehicle in response to a predicted motion from the trained, control-aware prediction module regarding detected agents within a traffic environment of the ego vehicle. These aspects of the present disclosure optimize a likelihood of known actions to direct a control-aware motion prediction model's attention towards accurately predicting a subset of states that help to predict the ego vehicle control decisions well.</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram illustrating a software architecture <b>200</b> that may modularize artificial intelligence (AI) functions for learning control-aware prediction objective models, according to aspects of the present disclosure. Using the architecture, a planner application <b>202</b> may be designed such that it may cause various processing blocks of a system-on-a-chip (SOC) <b>220</b> (for example a CPU <b>222</b>, a DSP <b>224</b>, a GPU <b>226</b>, and/or an NPU <b>228</b>) to perform supporting computations during run-time operation of the planner application <b>202</b>. While <figref idref="DRAWINGS">FIG. <b>2</b></figref> describes the software architecture <b>200</b> for planning-aware prediction to select vehicle control actions of an autonomous agent in response to control-aware prediction objective models, it should be recognized that vehicle action control functionality is not limited to autonomous agents. According to aspects of the present disclosure, vehicle action planning functionality is applicable to any vehicle type, provided the vehicle is equipped with appropriate machine learning functions.</p><p id="p-0034" num="0033">The planner application <b>202</b> may be configured to call functions defined in a user space <b>204</b> that may, for example, provide vehicle action planning services (e.g., throttling, steering, and braking). The planner application <b>202</b> may request to compile program code associated with a library defined in a control-aware prediction objective model application programming interface (API) <b>206</b>. In these aspects of the present disclosure, the control-aware prediction objective model API <b>206</b> predicts a future motion of a detected agent within a training environment of an ego vehicle. Alternatively, the control-aware prediction objective model API <b>206</b> predicts a future action of the ego vehicle. For example, a perception model detects the agent from an image of the training environment of the ego vehicle.</p><p id="p-0035" num="0034">Nevertheless, certain detections and predictions matter more than others from the perspective of the ego vehicle. For example, one way to determine a prediction relevancy to an ego vehicle is whether the prediction influences the ego vehicle's motion planning. Alternatively, the prediction relevancy to the ego vehicle is whether the prediction influences the ego vehicle's motion prediction. In some aspects of the present disclosure, at least one agent from the agents within the scene of the training environment of the ego vehicle is identified if the agent caused a prediction-discrepancy by the ego vehicle greater than the agents within the scene of the training environment. In other words, that which an autonomous vehicle's controller is sensitive to is more important to predict accurately.</p><p id="p-0036" num="0035">The planner application <b>202</b> may request to compile program code associated with a library defined in a model parameter update API <b>207</b>. In these aspects of the present disclosure, the model parameter update API <b>207</b> updates parameters of a motion prediction model based on a magnitude of a prediction-discrepancy caused by at least one agent on the ego vehicle to form a trained, control-aware prediction module. For example, the prediction-discrepancy may be based on a difference between a predicted vehicle action based on a future motion predicted for the at least one agent and an expected action of the training environment to form the trained, control-aware prediction model. Alternatively, the control-aware prediction model learns correlations between the planner's trajectories and the agents' trajectories, in which larger attention coefficients are given to the agents that cause larger reactions from the ego vehicle controller based on the prediction-discrepancy. Once trained, the planner application <b>202</b> selects a vehicle control action of the ego vehicle in response to a predicted motion from the trained, control-aware prediction module regarding detected agents within a traffic environment of the ego vehicle.</p><p id="p-0037" num="0036">A run-time engine <b>208</b>, which may be compiled code of a runtime framework, may be further accessible to the planner application <b>202</b>. The planner application <b>202</b> may cause the run-time engine <b>208</b>, for example, to take actions for controlling the autonomous agent. When an ego vehicle enters a traffic environment, the run-time engine <b>208</b> may in turn send a signal to an operating system <b>210</b>, such as a Linux Kernel <b>212</b>, running on the SOC <b>220</b>. <figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates the Linux Kernel <b>212</b> as software architecture for implementing trajectory planning of an autonomous agent using an automaton generative network. It should be recognized, however, aspects of the present disclosure are not limited to this exemplary software architecture. For example, other kernels may be used to provide the software architecture to support vehicle planning trajectory selection functionality.</p><p id="p-0038" num="0037">The operating system <b>210</b>, in turn, may cause a computation to be performed on the CPU <b>222</b>, the DSP <b>224</b>, the GPU <b>226</b>, the NPU <b>228</b>, or some combination thereof. The CPU <b>222</b> may be accessed directly by the operating system <b>210</b>, and other processing blocks may be accessed through a driver, such as drivers <b>214</b>-<b>218</b> for the DSP <b>224</b>, for the GPU <b>226</b>, or for the NPU <b>228</b>. In the illustrated example, the deep neural network may be configured to run on a combination of processing blocks, such as the CPU <b>222</b> and the GPU <b>226</b>, or may be run on the NPU <b>228</b>, if present.</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram illustrating an example of a hardware implementation for a vehicle action planner system <b>300</b>, according to aspects of the present disclosure. The vehicle action planner system <b>300</b> may be configured with a control-aware prediction objective model for a vehicle action planner of an ego vehicle. The vehicle action planner system <b>300</b> may be a component of a vehicle, a robotic device, or other autonomous device (e.g., autonomous vehicles, ride-share cars, etc.). For example, as shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the vehicle action planner system <b>300</b> is a component of an autonomous vehicle <b>350</b>.</p><p id="p-0040" num="0039">Aspects of the present disclosure are not limited to the vehicle action planner system <b>300</b> being a component of the autonomous vehicle <b>350</b>. Other devices, such as a bus, motorcycle, or other like autonomous vehicle, are also contemplated for implementing the vehicle action planner system <b>300</b>. In this example, the autonomous vehicle <b>350</b> may be semi-autonomous; however, other configurations for the autonomous vehicle <b>350</b> are contemplated, such as an advanced driver assistance system (ADAS).</p><p id="p-0041" num="0040">The vehicle action planner system <b>300</b> may be implemented with an interconnected architecture, represented generally by an interconnect <b>336</b>. The interconnect <b>336</b> may include any number of point-to-point interconnects, buses, and/or bridges depending on the specific application of the vehicle action planner system <b>300</b> and the overall design constraints. The interconnect <b>336</b> links together various circuits including one or more processors and/or hardware modules, represented by a sensor module <b>302</b>, a vehicle action planner <b>310</b>, a processor <b>320</b>, a computer-readable medium <b>322</b>, a communication module <b>324</b>, a controller module <b>326</b>, a locomotion module <b>328</b>, an onboard unit <b>330</b>, and a location module <b>340</b>. The interconnect <b>336</b> may also link various other circuits such as timing sources, peripherals, voltage regulators, and power management circuits, which are well known in the art, and therefore, will not be described any further.</p><p id="p-0042" num="0041">The vehicle action planner system <b>300</b> includes a transceiver <b>332</b> coupled to the sensor module <b>302</b>, the vehicle action planner <b>310</b>, the processor <b>320</b>, the computer-readable medium <b>322</b>, the communication module <b>324</b>, the controller module <b>326</b>, the locomotion module <b>328</b>, the location module <b>340</b>, and the onboard unit <b>330</b>. The transceiver <b>332</b> is coupled to antenna <b>334</b>. The transceiver <b>332</b> communicates with various other devices over a transmission medium. For example, the transceiver <b>332</b> may receive commands via transmissions from a user or a connected vehicle. In this example, the transceiver <b>332</b> may receive/transmit vehicle-to-vehicle traffic state information for the vehicle action planner <b>310</b> to/from connected vehicles within the vicinity of the autonomous vehicle <b>350</b>.</p><p id="p-0043" num="0042">The vehicle action planner system <b>300</b> includes the processor <b>320</b> coupled to the computer-readable medium <b>322</b>. The processor <b>320</b> performs processing, including the execution of software stored on the computer-readable medium <b>322</b> to provide vehicle action planning functionality, according to the present disclosure. The software, when executed by the processor <b>320</b>, causes the vehicle action planner system <b>300</b> to perform the various functions described for vehicle behavior planning (e.g., vehicle action selection) of the autonomous vehicle <b>350</b>, or any of the modules (e.g., <b>302</b>, <b>310</b>, <b>324</b>, <b>326</b>, <b>328</b>, <b>330</b>, and/or <b>340</b>). The computer-readable medium <b>322</b> may also be used for storing data that is manipulated by the processor <b>320</b> when executing the software.</p><p id="p-0044" num="0043">The sensor module <b>302</b> may obtain measurements via different sensors, such as a first sensor <b>306</b> and a second sensor <b>304</b>. The first sensor <b>306</b> may be a vision sensor (e.g., a stereoscopic camera or a red-green-blue (RGB) camera) for capturing 2D images. The second sensor <b>304</b> may be a ranging sensor, such as a light detection and ranging (LiDAR) sensor or a radio detection and ranging (RADAR) sensor. Of course, aspects of the present disclosure are not limited to the aforementioned sensors as other types of sensors (e.g., thermal, sonar, and/or lasers) are also contemplated for either of the first sensor <b>306</b> or the second sensor <b>304</b>.</p><p id="p-0045" num="0044">The measurements of the first sensor <b>306</b> and the second sensor <b>304</b> may be processed by the processor <b>320</b>, the sensor module <b>302</b>, the vehicle action planner <b>310</b>, the communication module <b>324</b>, the controller module <b>326</b>, the locomotion module <b>328</b>, the onboard unit <b>330</b>, and/or the location module <b>340</b>. In conjunction with the computer-readable medium <b>322</b>, the measurements of the first sensor <b>306</b> and the second sensor <b>304</b> are processed to implement the functionality described herein. In one configuration, the data captured by the first sensor <b>306</b> and the second sensor <b>304</b> may be transmitted to a connected vehicle via the transceiver <b>332</b>. The first sensor <b>306</b> and the second sensor <b>304</b> may be coupled to the autonomous vehicle <b>350</b> or may be in communication with the autonomous vehicle <b>350</b>.</p><p id="p-0046" num="0045">The location module <b>340</b> may determine a location of the autonomous vehicle <b>350</b>. For example, the location module <b>340</b> may use a global positioning system (GPS) to determine the location of the autonomous vehicle <b>350</b>. The location module <b>340</b> may implement a dedicated short-range communication (DSRC)-compliant GPS unit. A DSRC-compliant GPS unit includes hardware and software to make the autonomous vehicle <b>350</b> and/or the location module <b>340</b> compliant with one or more of the following DSRC standards, including any derivative or fork thereof: EN 12253:2004 Dedicated Short-Range Communication&#x2014;Physical layer using microwave at 5.8 GHz (review); EN 12795:2002 Dedicated Short-Range Communication (DSRC)&#x2014;DSRC Data link layer: Medium Access and Logical Link Control (review); EN 12834:2002 Dedicated Short-Range Communication&#x2014;Application layer (review); EN 13372:2004 Dedicated Short-Range Communication (DSRC)&#x2014;DSRC profiles for RTTT applications (review); and EN ISO 14906:2004 Electronic Fee Collection&#x2014;Application interface.</p><p id="p-0047" num="0046">The communication module <b>324</b> may facilitate communications via the transceiver <b>332</b>. For example, the communication module <b>324</b> may be configured to provide communication capabilities via different wireless protocols, such as 5G, WiFi, long term evolution (LTE), 4G, 3G, etc. The communication module <b>324</b> may also communicate with other components of the autonomous vehicle <b>350</b> that are not modules of the vehicle action planner system <b>300</b>. The transceiver <b>332</b> may be a communications channel through a network access point <b>360</b>. The communications channel may include DSRC, LTE, LTE-D2D, mmWave, WiFi (infrastructure mode), WiFi (ad-hoc mode), visible light communication, TV white space communication, satellite communication, full-duplex wireless communications, or any other wireless communications protocol such as those mentioned herein.</p><p id="p-0048" num="0047">In some configurations, the network access point <b>360</b> includes Bluetooth&#xae; communication networks or a cellular communications network for sending and receiving data including via short messaging service (SMS), multimedia messaging service (MMS), hypertext transfer protocol (HTTP), direct data connection, wireless application protocol (WAP), e-mail, DSRC, full-duplex wireless communications, mmWave, WiFi (infrastructure mode), WiFi (ad-hoc mode), visible light communication, TV white space communication, and satellite communication. The network access point <b>360</b> may also include a mobile data network that may include 3G, 4G, 5G, LTE, LTE-V2X, LTE-D2D, VoLTE, or any other mobile data network or combination of mobile data networks. Further, the network access point <b>360</b> may include one or more IEEE 802.11 wireless networks.</p><p id="p-0049" num="0048">The vehicle action planner system <b>300</b> also includes the controller module <b>326</b> for planning a route and controlling the locomotion of the autonomous vehicle <b>350</b>, via the locomotion module <b>328</b> for autonomous operation of the autonomous vehicle <b>350</b>. In one configuration, the controller module <b>326</b> may override a user input when the user input is expected (e.g., predicted) to cause a collision according to an autonomous level of the autonomous vehicle <b>350</b>. The modules may be software modules running in the processor <b>320</b>, resident/stored in the computer-readable medium <b>322</b>, and/or hardware modules coupled to the processor <b>320</b>, or some combination thereof.</p><p id="p-0050" num="0049">The National Highway Traffic Safety Administration (&#x201c;NHTSA&#x201d;) has defined different &#x201c;levels&#x201d; of autonomous vehicles (e.g., Level 0, Level 1, Level 2, Level 3, Level 4, and Level 5). For example, if an autonomous vehicle has a higher level number than another autonomous vehicle (e.g., Level 3 is a higher level number than Levels 2 or 1), then the autonomous vehicle with a higher level number offers a greater combination and quantity of autonomous features relative to the vehicle with the lower level number. These different levels of autonomous vehicles are described briefly below.</p><p id="p-0051" num="0050">Level 0: In a Level 0 vehicle, the set of advanced driver assistance system (ADAS) features installed in a vehicle provide no vehicle control, but may issue warnings to the driver of the vehicle. A vehicle which is Level 0 is not an autonomous or semi-autonomous vehicle.</p><p id="p-0052" num="0051">Level 1: In a Level 1 vehicle, the driver is ready to take driving control of the autonomous vehicle at any time. The set of ADAS features installed in the autonomous vehicle may provide autonomous features such as: adaptive cruise control (&#x201c;ACC&#x201d;); parking assistance with automated steering; and lane keeping assistance (&#x201c;LKA&#x201d;) type II, in any combination.</p><p id="p-0053" num="0052">Level 2: In a Level 2 vehicle, the driver is obliged to detect objects and events in the roadway environment and respond if the set of ADAS features installed in the autonomous vehicle fail to respond properly (based on the driver's subjective judgement). The set of ADAS features installed in the autonomous vehicle may include accelerating, braking, and steering. In a Level 2 vehicle, the set of ADAS features installed in the autonomous vehicle can deactivate immediately upon takeover by the driver.</p><p id="p-0054" num="0053">Level 3: In a Level 3 ADAS vehicle, within known, limited environments (such as freeways), the driver can safely turn their attention away from driving tasks, but must still be prepared to take control of the autonomous vehicle when needed.</p><p id="p-0055" num="0054">Level 4: In a Level 4 vehicle, the set of ADAS features installed in the autonomous vehicle can control the autonomous vehicle in all but a few environments, such as severe weather. The driver of the Level 4 vehicle enables the automated system (which is comprised of the set of ADAS features installed in the vehicle) only when it is safe to do so. When the automated Level 4 vehicle is enabled, driver attention is not required for the autonomous vehicle to operate safely and consistent within accepted norms.</p><p id="p-0056" num="0055">Level 5: In a Level 5 vehicle, other than setting the destination and starting the system, no human intervention is involved. The automated system can drive to any location where it is legal to drive and make its own decision (which may vary based on the jurisdiction where the vehicle is located).</p><p id="p-0057" num="0056">A highly autonomous vehicle (&#x201c;HAV&#x201d;) is an autonomous vehicle that is Level 3 or higher. Accordingly, in some configurations the autonomous vehicle <b>350</b> is one of the following: a Level 1 autonomous vehicle; a Level 2 autonomous vehicle; a Level 3 autonomous vehicle; a Level 4 autonomous vehicle; a Level 5 autonomous vehicle; and an HAV.</p><p id="p-0058" num="0057">The vehicle action planner <b>310</b> may be in communication with the sensor module <b>302</b>, the processor <b>320</b>, the computer-readable medium <b>322</b>, the communication module <b>324</b>, the controller module <b>326</b>, the locomotion module <b>328</b>, the location module <b>340</b>, the onboard unit <b>330</b>, and the transceiver <b>332</b>. In one configuration, the vehicle action planner <b>310</b> receives sensor data from the sensor module <b>302</b>. The sensor module <b>302</b> may receive the sensor data from the first sensor <b>306</b> and the second sensor <b>304</b>. According to aspects of the disclosure, the sensor module <b>302</b> may filter the data to remove noise, encode the data, decode the data, merge the data, extract frames, or perform other functions. In an alternate configuration, the vehicle action planner <b>310</b> may receive sensor data directly from the first sensor <b>306</b> and the second sensor <b>304</b> to determine, for example, input traffic data images.</p><p id="p-0059" num="0058">Human drivers navigate busy roads by carefully observing, anticipating, and reacting to the potential actions of other pedestrians and/or vehicles. Similarly, autonomous vehicles (AVs) such as the autonomous vehicle <b>350</b> use learned perceptual and predictive components for detecting and forecasting surrounding road users to plan safe motions. In particular, safe autonomous vehicle operation involves learned components that are well trained, for example, by reducing certain classification or regression errors on training data; however, all errors are not equally important. For example, some errors have a minimal effect on downstream decisions, while other errors may be catastrophic. In fact, errors in detecting vehicles and forecasting pedestrians have resulted in fatal collisions, while errors associated with those unlikely to interact with the autonomous vehicle <b>350</b> are likely inconsequential and uncorrelated with overall vehicle performance. While no model is perfect, a model that considers how errors propagate downstream to identify errors likely having a significant real-life cost is desired for mitigating these significant errors and improving overall AV performance of the autonomous vehicle <b>350</b>.</p><p id="p-0060" num="0059">As noted, certain detections and predictions matter more than others from the perspective of the autonomous vehicle <b>350</b>. For example, one way to determine a prediction relevancy to an ego vehicle is whether the prediction influences the ego vehicle's motion planning. In other words, that which an autonomous vehicle's controller is sensitive to is more important to predict accurately. Some aspects of the present disclosure are directed to control-aware prediction objectives (CAPOs) to train prediction models that more accurately reflect the relative effects of predictive errors on downstream control. Computing these downstream effects involves forward passes without backpropagation between modules.</p><p id="p-0061" num="0060">As indicated above, predictive models of conventional autonomous vehicles are typically trained using metrics that are independent to other components in the autonomous vehicle's system, such as the planner. In particular, the metrics are oblivious to how the predictions they make are eventually applied. By contrast, aspects of the present disclosure propose a novel prediction metric that is planning aware. Given a planning algorithm, aspects of the present disclosure seek to improve a predictive accuracy in areas in which the planner specifies accuracy. These aspects of the present disclosure train a predictive model to maximize the likelihood of the true actions of the autonomous vehicle <b>350</b> from a training set of prior autonomous driving data.</p><p id="p-0062" num="0061">Aspects of the present disclosure optimize inputs (e.g., scene predictions) to a vehicle action planner <b>310</b> of the autonomous vehicle <b>350</b>, such that outputs (e.g., vehicle actions) of the vehicle action planner <b>310</b> are accurate. Some aspects of the present disclosure involve training prediction models with control-aware objectives that lead to improved controller performance in complex multi-agent urban driving scenarios. By evaluating existing prediction models based on these control-aware metrics, the models according to aspects of the present disclosure are more likely to avoid precisely those errors that would maximally influence downstream decisions. In particular, some aspects of the present disclosure directly identify those errors that would maximally influence downstream decisions, including prediction algorithms that treat everything equally. For example, any predictions that do not affect planning do not receive a significant weight, which enables the prediction model to safely allocate model capacity elsewhere. Although the best prediction metric for a control-aware prediction is difficult to ascertain, some potential prediction metrics are proposed for the vehicle action planner <b>310</b>, according to aspects of the present disclosure.</p><p id="p-0063" num="0062">As shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the vehicle action planner <b>310</b> of the autonomous vehicle <b>350</b> includes a vehicle perception module <b>312</b>, a control-aware prediction objective model <b>314</b>, a model parameter update module <b>316</b>, and a vehicle action selection module <b>318</b>. The vehicle perception module <b>312</b>, the control-aware prediction objective model <b>314</b>, the model parameter update module <b>316</b>, and the vehicle action selection module <b>318</b> may be components of a same or different artificial neural network, such as a deep convolutional neural network (CNN). The vehicle action planner <b>310</b> is not limited to a CNN. The vehicle action planner <b>310</b> receives a data stream from the first sensor <b>306</b> and/or the second sensor <b>304</b>. The data stream may include a 2D RGB image from the first sensor <b>306</b> and LIDAR data points from the second sensor <b>304</b>. The data stream may include multiple frames, such as image frames of traffic data.</p><p id="p-0064" num="0063">The control-aware prediction objective model <b>314</b> may be configured to identify at least one agent from the agents within the scene of the training environment of the autonomous vehicle <b>350</b> causing a prediction-discrepancy by the autonomous vehicle <b>350</b> greater than the agents within the scene of the training environment. For example, the control-aware prediction objective model <b>314</b> may predict a future motion of agents detected by the vehicle perception module <b>312</b>, as well as the autonomous vehicle <b>350</b> based on the predicted future motion of the agents within the scene. In these aspects of the present disclosure, the model parameter update module <b>316</b> is configured to update parameters of the control-aware prediction objective model <b>314</b> based on a magnitude of the prediction-discrepancy caused by the at least one agent on the ego vehicle to form a trained, control-aware prediction module.</p><p id="p-0065" num="0064">For example, the model parameter update module <b>316</b> is configured to update parameters of control-aware prediction objective model <b>314</b> based on a difference between a predicted vehicle action based on a future motion predicted for a detected agent and an expected action of a training environment to train the control-aware prediction objective model <b>314</b>. In addition, the vehicle action selection module <b>318</b> is configured to select a vehicle control action of the autonomous vehicle <b>350</b> in response to a predicted motion from the control-aware prediction objective model <b>314</b> regarding detected agents within a traffic environment of the autonomous vehicle <b>350</b>. These aspects of the present disclosure optimize a likelihood of expert action to direct the control-aware prediction objective model <b>314</b> towards accurately predicting a subset of states that help to predict the decisions of the autonomous vehicle <b>350</b>. A vehicle behavior of the autonomous vehicle <b>350</b> may be controlled by the vehicle action planner <b>310</b> in a manner for motion planning and maneuvering of the autonomous vehicle <b>350</b> to perform a driving maneuver, for example, as shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0066" num="0065"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram illustrating an overview of a roadway environment, including an ego vehicle having a data-driven trajectory planner, according to aspects of the present disclosure. In this example, a roadway environment <b>400</b> includes a roadway <b>410</b>, having a first lane <b>412</b> in which an ego vehicle <b>420</b> is traveling in a second lane <b>414</b>. In addition, the first lane <b>412</b> also includes an obstruction <b>402</b>. In this example, the ego vehicle <b>420</b> is configured to monitor the dynamics of both vehicles/obstructions in the first lane <b>412</b>, as well as vehicles/obstructions in the second lane <b>414</b> of the roadway <b>410</b>. In this example, the ego vehicle <b>420</b>, may be the autonomous vehicle <b>350</b>, shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0067" num="0066">In one aspect of the present disclosure, maneuvering of the ego vehicle <b>420</b> is essentially controlled by a vehicle planner (e.g., the vehicle action planner <b>310</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>). In this example, the ego vehicle <b>420</b> (e.g., the vehicle perception module <b>312</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>) identifies the obstruction <b>402</b> in the first lane <b>412</b> of the roadway <b>410</b>. That is, the ego vehicle <b>420</b> is configured to identify a driving trajectory for avoiding the obstruction <b>402</b> in the first lane <b>412</b> of the roadway <b>410</b>. According to aspects of the present disclosure, the ego vehicle <b>420</b> is configured to identify a trajectory for a driving maneuver performed by the ego vehicle <b>420</b> to avoid the obstruction <b>402</b> in the first lane <b>412</b>. In some aspects of the present disclosure, the ego vehicle <b>420</b> is deployed using a control-aware prediction objective model <b>314</b> of the vehicle action planner <b>310</b>, as shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0068" num="0067">In operation, autonomous vehicles (AVs) navigate busy roads using predictive models to anticipate what surrounding pedestrians and vehicles might do in order to plan safe trajectories around these agents. Safe operation involves well calibrated predictive model components, typically by minimizing some regression error on training data. Nevertheless, not all errors made by these prediction modules are equally important. In particular, some errors have minimal effect on downstream decisions, while some perceptual errors and predictive errors can have fatal outcomes. As no model is perfect, it is important to identify the prediction errors that are safety-critical in order to ensure autonomous vehicle operation safety.</p><p id="p-0069" num="0068">Whether trained independently or as part of multi-task end-to-end architectures, multi-agent trajectory forecasting models typically optimize prediction-specific objectives based on regressing recorded future trajectories by considering all agents equally important a priori. Nevertheless, when considering the target control task of autonomous navigation, some predictions warrant more attention than others when deciding safety controls. Consequently, control-agnostic optimizing of prediction models may not result in improved downstream navigation performance due to limited data, model capacity, rare events, or computational constraints. Even with end-to-end training, multi-task objectives might not be aligned, thus resulting in performance degradation due to task interference.</p><p id="p-0070" num="0069">Aspects of the present disclosure are directed to control-aware prediction objectives (CAPOs) to train prediction models that more accurately reflect the relative effects of predictive errors on downstream control. Computing these downstream effects involves forward passes without backpropagation between modules. This improves applicability with real-world AV planning and control systems, which might not be fully differentiable due to complex design constraints (e.g., verifiability, interpretability, comfort and safety constraints). A disclosed method introduces importance-weighted prediction likelihood objectives using forward passes of the prediction model and planner. In aspect of the present disclosure, two weighting methods are investigated for training using backpropagation. The first assigns weights based on control variations due to prediction changes. The second uses learned attention weights between agent predictions and AV controls.</p><p id="p-0071" num="0070">Some aspects of the present disclosure involve training prediction models with control-aware objectives that lead to improved controller performance in complex multi-agent urban driving scenarios. By evaluating existing prediction models based on these control-aware metrics, the models according to aspects of the present disclosure are more likely to avoid precisely those errors that would maximally influence downstream decisions. In particular, some aspects of the present disclosure directly identify those errors that would maximally influence downstream decisions, including prediction algorithms that treat everything equally.</p><p id="p-0072" num="0071">The following example formalizes notation and the task of scene prediction, according to aspects of the present disclosure. The following examples involve a multi-agent system of N agents (including an ego vehicle) that interact in a continuous space, discrete time setting. For example, Let x&#x3f5;X denote past trajectory information about all agent's in a scene, used to make probabilistic predictions &#x177;&#x3f5;<img id="CUSTOM-CHARACTER-00001" he="2.79mm" wi="2.46mm" file="US20230001953A1-20230105-P00001.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>y </sub>about the future multi-agent trajectories y&#x3f5;y. Trajectories are predicted up to time horizon T, and y<sub>T </sub>denotes the future state at time T As the intents of other agents are usually uncertain, a probabilistic prediction model q<sub>&#x3b8; </sub>with trainable parameters &#x3b8; is used to sample the motion of others: &#x177;&#x2dc;q<sub>&#x3b8;</sub>(&#x177;|x). If multiple samples are taken, &#x177;<sup>k </sup>refers to the kth sample, and to single out the nth agent is overload by using the notation &#x177;<sub>n</sub>, and use y<sub>ego </sub>as the AV's future trajectory. Given such predictions, the AV controller &#x3c0; outputs ego controls u&#x3f5;<img id="CUSTOM-CHARACTER-00002" he="2.46mm" wi="2.79mm" file="US20230001953A1-20230105-P00002.TIF" alt="custom-character" img-content="character" img-format="tif"/> to anticipate and avoid colliding with other agents' future trajectories: u=&#x3c0;(y).</p><p id="p-0073" num="0072">Aspects of the present disclosure are provided based on the assumption that an autonomous vehicle (AV) stack (e.g., perception, prediction, and planning) performs behavior prediction before control. While conditioning behavior prediction on an ego vehicle's intent to provide more accurate prediction, aspects of the present disclosure assume that other agents do not anticipate the autonomous vehicle's future trajectory. In these aspects of the present disclosure, the autonomous vehicle anticipates the other agents' future trajectories in order to avoid collisions. For example, because the latent intent of other drivers is usually uncertain, probabilistic models are selected to forecast the motion of others by training using the negative log likelihood (NLL) of past motion data, for example, as shown in Table I.</p><p id="p-0074" num="0000"><tables id="TABLE-US-00001" num="00001"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="center"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE 1</entry></row></thead><tbody valign="top"><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>Prediction Metrics</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="105pt" align="left"/><colspec colname="2" colwidth="112pt" align="center"/><tbody valign="top"><row><entry>Metric Name</entry><entry>Metric Equation</entry></row><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row><row><entry>Average Displacement Error (ADE)</entry><entry>||&#x177; &#x2212; y||<sub>2</sub></entry></row><row><entry>Final Displacement Error (FDE)</entry><entry>||&#x177;<sub>T </sub>&#x2212;y<sub>T</sub>||<sub>2</sub></entry></row><row><entry>Minimum-ADE (minADE)</entry><entry>min<sub>k&#x2208;[K]</sub>||&#x177;<sup>k </sup>&#x2212; y||<sub>2</sub></entry></row><row><entry>Minimum-FDE (minFDE)</entry><entry>min<sub>k&#x2208;[K]</sub>||&#x177;<sub>T</sub><sup>k </sup>&#x2212; y<sub>T</sub>||<sub>2</sub></entry></row><row><entry> </entry></row><row><entry>Miss Rate (MR) Negative Log Likelihood (NLL):</entry><entry><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mrow>  <mrow>   <mfrac>    <mn>1</mn>    <mi>K</mi>   </mfrac>   <mo>&#x2062;</mo>   <mrow>    <msub>     <mo>&#x2211;</mo>     <mi>k</mi>    </msub>    <mrow>          <mo>[</mo>     <mrow>      <msub>       <mrow>        <mo>&#xf605;</mo>        <mrow>         <msubsup>          <mover>           <mi>y</mi>           <mo>^</mo>          </mover>          <mi>T</mi>          <mi>k</mi>         </msubsup>         <mo>-</mo>         <msub>          <mi>y</mi>          <mi>T</mi>         </msub>        </mrow>        <mo>&#xf606;</mo>       </mrow>       <mn>2</mn>      </msub>      <mo>&#x3e;</mo>      <mi>&#x3b1;</mi>     </mrow>     <mo>]</mo>    </mrow>   </mrow>  </mrow>  <mo>-</mo>  <mrow>   <msub>    <mi>log</mi>    <mrow>     <mi>q</mi>     <mo>&#x2062;</mo>     <mi>&#x3b8;</mi>    </mrow>   </msub>   <mo>(</mo>   <mrow>    <mi>y</mi>    <mo>|</mo>    <mi>x</mi>   </mrow>   <mo>)</mo>  </mrow> </mrow></math></maths></entry></row><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0075" num="0073">For example, a metric may compare the Euclidean distance between either a full predicted state-sequence &#x177; (or final state &#x177;<sub>T</sub>) with the true sequence y (or final state y<sub>T</sub>) an agent took, as recorded in data. Probabilistic models are typically trained to minimize the negative log likelihood (NLL) of the data. These metrics, however, are agnostic to road geometry and downstream planning, which implicitly assumes that all other agents' forecasts are equally relevant. For example, consider two pedestrians: one walking ahead of the ego vehicle and one behind. Assuming independent pedestrian motion, the NLL objective factorizes as: &#x2212;log<sub>q&#x3b8;</sub>(y<sup>ahead</sup>,y<sup>behind</sup>|x)=&#x2212;log<sub>q&#x3b8;</sub>(y<sup>ahead</sup>|x)&#x2212;log<sub>q&#x3b8;</sub>(y<sup>behind</sup>|x). It should be recognized that this prediction metric is equally concerned with both y<sup>ahead </sup>and y<sup>behind</sup>. Intuitively, accurate prediction of the pedestrian ahead of the ego vehicle is more important for safe motion planning since the ego's planned path is more likely to intersect with y<sup>ahead </sup>than y<sup>behind </sup>Control-aware prediction objects are aware that errors in predicting y<sup>ahead </sup>have greater downstream consequences than errors in y<sup>behind</sup>, according to aspects of the present disclosure.</p><heading id="h-0008" level="2">Control-Aware Prediction Objectives</heading><p id="p-0076" num="0074">Predictive models are typically trained using metrics that are independent to everything else in the autonomous vehicle's system, such as the planner of the autonomous vehicle. In particular, the metrics are oblivious to how the predictions they make are used. Aspects of the present disclosure are directed to a novel prediction metric that is planning aware. Given a planning algorithm, aspects of the present disclosure improve predictive accuracy in areas in which the planner specifies accuracy. In some aspects of the present disclosure, a predictive model is trained to increase the likelihood of the ego's true actions from a training set of prior autonomous driving data. This involves improving the planner's inputs (e.g., scene predictions), such that the planner's outputs (e.g., ego actions) are accurate. Training in this way, with gradients passing through a fixed planning algorithm to improve various predictions, inherently weights those predictions that contribute to higher planning accuracy. In these aspects of the present disclosure, any predictions that do not affect planning are weighted less so the prediction model can safely allocate model capacity elsewhere.</p><p id="p-0077" num="0075">Some aspects of the present disclosure are directed to a novel prediction loss function that considers how predictions are used downstream to improve predictive accuracy wherever predictive errors would cause a larger change in the control outputs. In Bayesian decision theory, a decision is evaluated as the expected utility of a decision u or controller &#x3c0;, integrating out any uncertainties. In particular, it is the future trajectories of other agents that are unknown but can be probabilistically predicted according to a model with parameters &#x3b8;. These aspects of the present disclosure involve loss-calibrated variational inference by defining the gain of a decision or controller's value as a function of the model parameters &#x3b8; to train.</p><p id="p-0078" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>Gain(&#x3b8;)=&#x222b; utility(&#x3c0;,<i>y,&#x177;,x</i>)<sub>q&#x3b8;</sub>(<i>&#x177;|x</i>)<i>d&#x177;.</i>&#x2003;&#x2003;(1)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0079" num="0000">The choice of utility function in Equation (1) is an open one; it defines how desirable a course of actions would be given x and &#x177;. Alternatively, an existing metric like the NLL can simply be weighted without integration. In the next subsection discuss some baseline choices for the utility or weight, and after, two methods are proposed for computing these weights: a self-attention method and a counterfactual method.</p><heading id="h-0009" level="2">A. Baseline Objectives</heading><p id="p-0080" num="0076">Conventional predictive metrics are agnostic to the decision u and simply use a delta function to score correct trajectory predictions, recovering the standard log likelihood metric:</p><p id="p-0081" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>Gain<sub>y</sub>(&#x3b8;)=&#x222b;&#x3b4;(<i>y,&#x177;</i>)<i>q</i><sub>&#x3b8;</sub>(<i>&#x177;|x</i>)<i>d&#x177;=q</i><sub>&#x3b8;</sub>(<i>y|x</i>)&#x2003;&#x2003;(2)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0082" num="0077">Nevertheless, aspects of the present disclosure are directed to utilities that are a function of the decision u in order to weight predictions by their downstream effect on the ego's control. For instance, trajectory predictions may be scored based on the resultant ego controls &#x3c0;(&#x177;) matching the ego's behavior under knowledge of the true future trajectories &#x3c0;(&#x177;):</p><p id="p-0083" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>Gain<sub>&#x3c0;</sub>(&#x3b8;)=&#x222b;&#x3b4;(&#x3c0;(<i>y</i>),&#x3c0;(<i>&#x177;</i>))<i>q</i><sub>&#x3b8;</sub>(<i>&#x177;|x</i>)<i>d&#x177;</i>&#x2003;&#x2003;(3)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0084" num="0078">This integral, unfortunately, is intractable to derive or estimate, but softer utility functions may be used instead. One example is &#x2225;&#x3c0;(&#x177;)&#x2212;&#x3c0;(y)&#x2225;<sub>1</sub>, which may provide a baseline. Optimizing this controller output error guides the learning process towards predicting controller inputs (predicted trajectories) accurately, insofar as they result in the correct control. Any trajectory errors that do not induce a change in the AV's control are thus considered inconsequential and ignored. Some aspects of the present disclosure are directed to an instantiation of a cost function using just the gradients of the controller with respect to the predicted trajectory &#x2225;&#x2207;<sub>&#x177;</sub>&#x3c0;(&#x177;)&#x2225;<sub>1</sub>, or true trajectory &#x2225;&#x2207;<sub>y</sub>&#x3c0;(y)&#x2225;<sub>1</sub>. Aspects of the present disclosure illustrate that is not necessary to have differentiable controllers to promote predictive accuracy wherever relevant to control.</p><heading id="h-0010" level="2">B. Proposed Attention Objective</heading><p id="p-0085" num="0079">In some aspects of the present disclosure, a method of weighting agent prediction using attention weights between agents x and the AV's future trajectory y<sub>ego</sub>. The predictive model is a function with learned parameters &#x3b8; noted q<sub>&#x3b8;</sub>:X&#x2192;<img id="CUSTOM-CHARACTER-00003" he="2.79mm" wi="2.46mm" file="US20230001953A1-20230105-P00001.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>y&#xd7;y</sub><sub><sub2>ego</sub2></sub>. Where X is the past observation space and <img id="CUSTOM-CHARACTER-00004" he="2.79mm" wi="2.46mm" file="US20230001953A1-20230105-P00001.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>y&#xd7;y</sub><sub><sub2>ego </sub2></sub>probability spaces of pedestrian future trajectories <img id="CUSTOM-CHARACTER-00005" he="2.79mm" wi="2.46mm" file="US20230001953A1-20230105-P00001.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>y </sub>and the ego future trajectories <img id="CUSTOM-CHARACTER-00006" he="2.79mm" wi="2.46mm" file="US20230001953A1-20230105-P00001.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>y</sub><sub><sub2>ego</sub2></sub>.</p><p id="p-0086" num="0080"><figref idref="DRAWINGS">FIGS. <b>5</b>A and <b>5</b>B</figref> are block diagrams illustrating a gate recurrent unit (GRU) encoder/decoder architecture for trajectory prediction based on a trained attention model, according to aspects of the present disclosure. As shown in <figref idref="DRAWINGS">FIG. <b>5</b>A</figref>, the GRU encoder-decoder architecture <b>500</b> for providing trajectory prediction is configured to perform interactions using attention. For example, the GRU encoder-decoder architecture <b>500</b> may support an attention model. Configuration of the GRU encoder-decoder architecture <b>500</b> may be performed using a method to train the attention model with multi-head attention on pedestrian agents and a car ego agent.</p><p id="p-0087" num="0081">In some aspects of the present disclosure, the GRU encoder-decoder architecture <b>500</b> includes an ego encoder <b>502</b> for an ego agent <b>510</b>. In addition, the GRU encoder-decoder architecture <b>500</b> includes agent encoders <b>520</b> (<b>520</b>-<b>1</b>, . . . , <b>520</b>-N) for pedestrian agents <b>530</b> (<b>530</b>-<b>1</b>, . . . , <b>530</b>-N). An ego encoding <b>504</b> from the ego encoder <b>502</b> and agent encodings <b>522</b> (<b>522</b>-<b>1</b>, . . . , <b>522</b>-N) are provided to ego attention blocks <b>540</b> (<b>540</b>-<b>1</b>, . . . , <b>540</b>-N) from the agent encoders <b>520</b>. In this configuration, the GRU encoder-decoder architecture <b>500</b> also includes an ego decoder <b>572</b> that receives an encoded output of the ego attention blocks <b>540</b> to generate an ego prediction <b>570</b>. The GRU encoder-decoder architecture <b>500</b> also includes agent decoders <b>580</b> (<b>580</b>-<b>1</b>, . . . , <b>580</b>-N). The agent decoders <b>580</b> receive the agent encodings <b>522</b> from the agent encoders <b>520</b> to generate agent predictions <b>590</b>. In aspects of the present disclosure, the ego prediction <b>570</b> provides a future trajectory prediction of the ego agent <b>510</b>, and the agent encodings <b>522</b> provide future trajectory predictions of the pedestrian agents <b>530</b>.</p><p id="p-0088" num="0082"><figref idref="DRAWINGS">FIG. <b>5</b>B</figref> further illustrates the ego attention blocks <b>540</b> of <figref idref="DRAWINGS">FIG. <b>5</b>A</figref>, in some aspects of the present disclosure. In this configuration, the ego attention blocks <b>540</b> receive the ego encoding <b>504</b> and the agent encodings <b>522</b>. For example, the ego encoding <b>504</b> is provided to a first matrix multiplier <b>542</b> to generate an output q<sub>0</sub>. In addition, the ego encoding <b>504</b> is provided to a second matrix multiplier <b>544</b> to generate an output k<sub>0</sub>, and the ego encoding <b>504</b> is provided to a third matrix multiplier <b>546</b> to generate an output v<sub>0</sub>. The agent encodings <b>522</b> are provided to a matrix multiplier <b>550</b> to generate an output k<sub>n</sub>, and to a matrix multiplier <b>552</b> to generate an output v<sub>n</sub>.</p><p id="p-0089" num="0083">In some aspects of the present disclosure, the ego attention blocks <b>540</b> in <figref idref="DRAWINGS">FIG. <b>5</b>A</figref> are heads of a multi-head attention mechanism. The computation performed by each head is an attention vector according to a Q vector (Q=(q<sub>0</sub>)), a K vector (K=(k<sub>0</sub>, . . . , k<sub>n</sub>)), and a V vector (V=(v0, . . . , vn)) to generate an encoded output <b>560</b>:</p><p id="p-0090" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mi>&#x3b1;</mi>      <mo>=</mo>      <mrow>       <mrow>        <mi>&#x3c3;</mi>        <mo>&#x2061;</mo>        <mo>(</mo>        <mfrac>         <msup>          <mi>QK</mi>          <mo>&#x22a4;</mo>         </msup>         <msqrt>          <msub>           <mi>d</mi>           <mi>k</mi>          </msub>         </msqrt>        </mfrac>        <mo>)</mo>       </mrow>       <mo>=</mo>       <mrow>        <mo>[</mo>        <mrow>         <msub>          <mi>&#x3b1;</mi>          <mn>0</mn>         </msub>         <mo>,</mo>         <mo>&#x2026;</mo>         <mtext>   </mtext>         <mo>,</mo>         <msub>          <mi>&#x3b1;</mi>          <mi>N</mi>         </msub>        </mrow>        <mo>]</mo>       </mrow>      </mrow>     </mrow>     <mo>,</mo>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>4</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0091" num="0000">where &#x3c3; denotes the softmax operation that normalizes the attention vector. The encoded output=&#x3b1;V is a weighted mean of the value vectors over the agents (including ego).</p><p id="p-0092" num="0084">In some aspects of the present disclosure, the attention model of the GRU encoder-decoder architecture <b>500</b> produces outputs in the form of a sequence of Gaussian mixtures for each agent. In addition, the attention model is trained to minimize the negative log likelihood (NLL) for all agents and the ego trajectory predictions. These aspects of the present disclosure use attention coefficients &#x3b1; as importance factors in a weighted sum of per-human state prediction loss (as opposed to uniform weighting). An algorithm shown in Table 1 summarizes how the attention model is trained with importance weighting. For example, if multiple heads are used, they are averaged:</p><p id="p-0093" num="0000"><maths id="MATH-US-00003" num="00003"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <msub>      <mi>qw</mi>      <mi>n</mi>     </msub>     <mo>=</mo>     <mrow>      <mfrac>       <mn>1</mn>       <mi>H</mi>      </mfrac>      <mo>&#x2062;</mo>      <mrow>       <munderover>        <mo>&#x2211;</mo>        <mrow>         <mi>h</mi>         <mo>=</mo>         <mn>1</mn>        </mrow>        <mi>H</mi>       </munderover>       <msubsup>        <mi>&#x3b1;</mi>        <mi>n</mi>        <mrow>         <mo>(</mo>         <mi>h</mi>         <mo>)</mo>        </mrow>       </msubsup>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>5</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0094" num="0085">In some aspects of the present disclosure, an attention predictor according to Equation (4) imitates a planner that interacts with the pedestrians to avoid collision. One way for the predictor to interact with the pedestrians is through attention. Therefore, as the attention model learns the correlations between the planner's trajectories and the agents trajectories, larger attention coefficients are given to the agents that cause larger reactions from the controller. In these aspects of the present disclosure, the attention model learns the agents that cause larger reactions from the controller offline and does not access the controller nor its gradient.</p><p id="p-0095" num="0086">Predicting both the ego trajectory and the pedestrians at the same time allows the use of attention coefficients for concern weighting of the pedestrians in a single run. Note that the ego's self-attention &#x3b1;<sub>0 </sub>is not used to weight the loss; it quantifies how independent the ego is from the other agents.</p><p id="p-0096" num="0087">Algorithm 1, as shown, provides a training method to define a concern about an agent but not about specific trajectories of that agent. For example, the training method may define the concern without using the controller because the training method uses an offline-learned model that imitates the controller.</p><p id="p-0097" num="0000"><tables id="TABLE-US-00002" num="00002"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="center"/><thead><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>Algorithm 1 Control-Aware Prediction Objectives (Attention)</entry></row><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="203pt" align="left"/><tbody valign="top"><row><entry/><entry>Input: Controller: &#x3c0; : &#x3c7; &#x2192;&#x2009;<img id="CUSTOM-CHARACTER-00007" he="2.46mm" wi="2.79mm" file="US20230001953A1-20230105-P00003.TIF" alt="custom-character" img-content="character" img-format="tif"/> </entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="14pt" align="left"/><colspec colname="3" colwidth="189pt" align="left"/><tbody valign="top"><row><entry/><entry>1:</entry><entry>Record trajectory data&#x2009;<img id="CUSTOM-CHARACTER-00008" he="2.46mm" wi="2.46mm" file="US20230001953A1-20230105-P00004.TIF" alt="custom-character" img-content="character" img-format="tif"/> &#x2009;= {x, y}<sub>i</sub></entry></row><row><entry/><entry>2:</entry><entry>while training do</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="28pt" align="left"/><colspec colname="3" colwidth="175pt" align="left"/><tbody valign="top"><row><entry/><entry>3:</entry><entry>Sample batch x, y~D</entry></row><row><entry/><entry>4:</entry><entry>Run attention model to estimate &#x177;<sub>ego </sub>and &#x177; from x</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="4"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="28pt" align="left"/><colspec colname="3" colwidth="126pt" align="left"/><colspec colname="4" colwidth="49pt" align="left"/><tbody valign="top"><row><entry/><entry>5:</entry><entry>Get attention: &#x3b1;(x)</entry><entry><img id="CUSTOM-CHARACTER-00009" he="2.46mm" wi="3.22mm" file="US20230001953A1-20230105-P00005.TIF" alt="custom-character" img-content="character" img-format="tif"/> &#x2009;Eq. (4)</entry></row><row><entry/><entry>6:</entry><entry>Compute weight: w(&#x3b1;(x))</entry><entry><img id="CUSTOM-CHARACTER-00010" he="2.46mm" wi="3.22mm" file="US20230001953A1-20230105-P00005.TIF" alt="custom-character" img-content="character" img-format="tif"/> &#x2009;Eq. (5)</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="28pt" align="left"/><colspec colname="3" colwidth="175pt" align="left"/><tbody valign="top"><row><entry/><entry>7:</entry><entry>Update model:</entry></row><row><entry/><entry/><entry>&#x3b8; &#x2190; &#x3b8; + w(x)&#x2207;<sub>&#x3b8;</sub> log q<sub>&#x3b8;</sub>(y|x) + &#x2207;<sub>&#x3b8;</sub> log q<sub>&#x3b8;</sub> (y<sub>ego</sub>|x)</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="203pt" align="left"/><tbody valign="top"><row><entry/><entry>Output: Predictive model q<sub>&#x3b8;</sub>: &#x3c7; &#x2192;&#x2009;<img id="CUSTOM-CHARACTER-00011" he="2.46mm" wi="2.12mm" file="US20230001953A1-20230105-P00006.TIF" alt="custom-character" img-content="character" img-format="tif"/> &#x2009;<sub>yxy</sub><sub><sub2>ego</sub2></sub></entry></row><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0098" num="0088">In this implementation, Controller: &#x3c0;:X&#x2192;<img id="CUSTOM-CHARACTER-00012" he="3.22mm" wi="3.22mm" file="US20230001953A1-20230105-P00007.TIF" alt="custom-character" img-content="character" img-format="tif"/> is provided as an input to the control-aware prediction objective process shown in Table 1. In this example, x&#x3f5;X denotes past trajectory information about all agents in a scene, and the AV controller &#x3c0; outputs ego controls u&#x3f5;<img id="CUSTOM-CHARACTER-00013" he="3.22mm" wi="3.22mm" file="US20230001953A1-20230105-P00008.TIF" alt="custom-character" img-content="character" img-format="tif"/> to anticipate and avoid colliding with other agents' future trajectories: u=&#x3c0;(y). At step <b>1</b>, trajectory data <img id="CUSTOM-CHARACTER-00014" he="2.79mm" wi="2.79mm" file="US20230001953A1-20230105-P00009.TIF" alt="custom-character" img-content="character" img-format="tif"/>={x,y}<sub>i </sub>is recorded. At step <b>2</b>, a training loop is initiated which repeats steps <b>3</b> to <b>6</b> until a predictive model is output. At step <b>3</b>, a batch of past agent trajectory information (x) and future agent trajectory information (y) is sampled from the recorded trajectory data x, y&#x2dc;<img id="CUSTOM-CHARACTER-00015" he="2.79mm" wi="2.79mm" file="US20230001953A1-20230105-P00010.TIF" alt="custom-character" img-content="character" img-format="tif"/>. At step <b>4</b>, an attention model is run to estimate a future ego trajectory &#x177;<sub>ego </sub>and a future agent trajectory &#x177; from the past agent trajectories x. At step <b>5</b>, an attention: &#x3b1;(x) is computed according to Equation (4). At step <b>6</b>, a weight: w(&#x3b1;(x)) is computed according to Equation (5). At step <b>7</b>, the model is updated: &#x3b8;&#x2190;&#x3b8;+w(x)&#x2207;<sub>&#x3b8;</sub>log q<sub>&#x3b8;</sub>(y|x)+&#x2207;<sub>&#x3b8;</sub>log q<sub>&#x3b8;</sub>(y<sub>ego</sub>|x) based on the weight w(x) and the NLLs of the known previous x and future trajectories y and the estimated future ego trajectory &#x177;<sub>ego</sub>. The control-aware prediction objective process outputs the predictive model q<sub>&#x3b8;</sub>:X&#x2192;<img id="CUSTOM-CHARACTER-00016" he="2.79mm" wi="2.46mm" file="US20230001953A1-20230105-P00001.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>y&#xd7;y</sub><sub><sub2>ego</sub2></sub>.</p><heading id="h-0011" level="2">C. Proposed Counterfactual Action Discrepancy Objective</heading><p id="p-0099" num="0089">This second proposal can also be formulated as a re-weighted maximization objective, where the log likelihood of each agent's trajectory in a scene is weighted by its individual contribution to the ego's control decision. In some aspects of the present disclosure, weighting of the agent's trajectory based on its individual contribution to the ego's control decision is performed as follows. First, enumerating is performed through each agent in a scene, and counterfactual outputs from the AV's controller are computed as if every agent traversed their individual trajectory as recorded in the replay buffer, except for agent n. Next, the trajectory that the nth agent might otherwise have taken, &#x177;<sub>n</sub><sup>k</sup>&#x2dc;q<sub>&#x3b8;</sub>(&#x177;<sub>n</sub>|x), is resampled and the control output that would result is computed as:</p><p id="p-0100" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>&#xfb;</i><sub>n</sub><sup>k</sup>=&#x3c0;({<i>&#x177;</i><sub>n</sub><sup>k</sup><i>}&#x222a;y\{y</i><sub>n</sub>}),&#x2003;&#x2003;(6)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0101" num="0000">to compare against the control had no agent deviated from their recorded trajectories:</p><p id="p-0102" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>u</i>=&#x3c0;(<i>y</i>).&#x2003;&#x2003;(7)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0103" num="0090">The difference in these two hypothetical controls corresponds to how much an individual agent affects the ego vehicle, and can represent the concern associated with predicting this particular agent in this particular instance accurately. If the model is probabilistic, then taking multiple samples (K&#x3e;1) helps ensure high importance even if the pedestrian only might cause a control deviation:</p><p id="p-0104" num="0000"><maths id="MATH-US-00004" num="00004"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <msub>       <mi>w</mi>       <mi>n</mi>      </msub>      <mo>=</mo>      <mrow>       <munder>        <mi>max</mi>        <mrow>         <mi>k</mi>         <mo>&#x2208;</mo>         <mrow>          <mo>{</mo>          <mrow>           <mn>1</mn>           <mo>&#x2062;</mo>           <mo>&#x2026;</mo>           <mo>&#x2062;</mo>           <mi>K</mi>          </mrow>          <mo>}</mo>         </mrow>        </mrow>       </munder>       <msub>        <mrow>         <mo>&#xf605;</mo>         <mrow>          <mi>u</mi>          <mo>-</mo>          <msubsup>           <mover>            <mi>u</mi>            <mo>^</mo>           </mover>           <mi>n</mi>           <mi>k</mi>          </msubsup>         </mrow>         <mo>&#xf606;</mo>        </mrow>        <mn>1</mn>       </msub>      </mrow>     </mrow>     <mo>,</mo>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>8</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0105" num="0000">which are used as weights for predictive model training:</p><p id="p-0106" num="0000"><maths id="MATH-US-00005" num="00005"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <msup>      <mi>&#x3b8;</mi>      <mo>*</mo>     </msup>     <mo>=</mo>     <mrow>      <munder>       <mrow>        <mi>arg</mi>        <mo>&#x2062;</mo>        <mi>max</mi>       </mrow>       <mi>&#x3b8;</mi>      </munder>      <mo>&#x2062;</mo>      <mrow>       <munderover>        <mo>&#x2211;</mo>        <mrow>         <mi>n</mi>         <mo>=</mo>         <mn>1</mn>        </mrow>        <mi>N</mi>       </munderover>       <mrow>        <msub>         <mi>w</mi>         <mi>n</mi>        </msub>        <mo>&#x2062;</mo>        <mi>log</mi>        <mo>&#x2062;</mo>        <mrow>         <msub>          <mi>q</mi>          <mi>&#x3b8;</mi>         </msub>         <mo>(</mo>         <mrow>          <msub>           <mi>y</mi>           <mi>n</mi>          </msub>          <mo>&#x2758;</mo>          <mi>x</mi>         </mrow>         <mo>)</mo>        </mrow>       </mrow>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>9</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0107" num="0091">In one aspect of the present disclosure, a method for counterfactual action discrepancy is summarized in the pseudo code for control-aware prediction objects as shown in Algorithm 2. Implementation of a predictive model q<sub>&#x3b8;</sub>, in some aspects of the present disclosure, takes as inputs the previous three locations of a human agent, passing through three fully connected layers, to output a Gaussian distribution delta state at the next point in time. For full state predictions, the prediction is bootstrapped T times.</p><p id="p-0108" num="0000"><tables id="TABLE-US-00003" num="00003"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="center"/><thead><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>Algorithm 2 Control-Aware Prediction Objectives (Weighted)</entry></row><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="203pt" align="left"/><tbody valign="top"><row><entry/><entry>Input: Controller: &#x3c0; : &#x3c7; &#x2192;&#x2009;<img id="CUSTOM-CHARACTER-00017" he="2.46mm" wi="2.79mm" file="US20230001953A1-20230105-P00011.TIF" alt="custom-character" img-content="character" img-format="tif"/> </entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="14pt" align="left"/><colspec colname="3" colwidth="189pt" align="left"/><tbody valign="top"><row><entry/><entry>1:</entry><entry>Record trajectory data&#x2009;<img id="CUSTOM-CHARACTER-00018" he="2.46mm" wi="2.46mm" file="US20230001953A1-20230105-P00012.TIF" alt="custom-character" img-content="character" img-format="tif"/> &#x2009;= {x, y}<sub>i</sub></entry></row><row><entry/><entry>2:</entry><entry>while training do</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="28pt" align="left"/><colspec colname="3" colwidth="175pt" align="left"/><tbody valign="top"><row><entry/><entry>3:</entry><entry>Sample batch x, y~D</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="4"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="28pt" align="left"/><colspec colname="3" colwidth="112pt" align="left"/><colspec colname="4" colwidth="63pt" align="left"/><tbody valign="top"><row><entry/><entry>4:</entry><entry>Compute hypothetical controls:: u, &#xfb;<sub>n</sub><sup>k </sup></entry><entry><img id="CUSTOM-CHARACTER-00019" he="2.46mm" wi="3.22mm" file="US20230001953A1-20230105-P00013.TIF" alt="custom-character" img-content="character" img-format="tif"/> &#x2009;Eq. (6) - (7)</entry></row><row><entry/><entry>5:</entry><entry>Compute weight: w(u, &#xfb;<sub>n</sub><sup>k</sup>)</entry><entry>&#x2003;<img id="CUSTOM-CHARACTER-00020" he="2.46mm" wi="3.22mm" file="US20230001953A1-20230105-P00013.TIF" alt="custom-character" img-content="character" img-format="tif"/> &#x2009;Eq. (8)</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="28pt" align="left"/><colspec colname="3" colwidth="175pt" align="left"/><tbody valign="top"><row><entry/><entry>6:</entry><entry>Update model: &#x3b8; &#x2190; &#x3b8; + w(u, &#xfb;<sub>n</sub><sup>k</sup>)&#x2207;<sub>&#x3b8;</sub> log q<sub>&#x3b8;</sub>(y|x)</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="left"/><colspec colname="2" colwidth="203pt" align="left"/><tbody valign="top"><row><entry/><entry>Output: Predictive model q<sub>&#x3b8;</sub>: &#x3c7; &#x2192;&#x2009;<img id="CUSTOM-CHARACTER-00021" he="2.46mm" wi="2.12mm" file="US20230001953A1-20230105-P00014.TIF" alt="custom-character" img-content="character" img-format="tif"/> &#x2009;<sub>y</sub></entry></row><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0109" num="0092">In this implementation, Controller: &#x3c0;:X&#x398;<img id="CUSTOM-CHARACTER-00022" he="3.22mm" wi="3.22mm" file="US20230001953A1-20230105-P00015.TIF" alt="custom-character" img-content="character" img-format="tif"/> is provided as an input to the control-aware prediction objective process shown in Table 1. In this example, x&#x3f5;X denotes past trajectory information about all agents in a scene, and the AV controller &#x3c0; outputs ego controls u&#x3f5;U to anticipate and avoid colliding with other agents' future trajectories: u=&#x3c0;(y). At step <b>1</b>, trajectory data <img id="CUSTOM-CHARACTER-00023" he="2.79mm" wi="2.79mm" file="US20230001953A1-20230105-P00016.TIF" alt="custom-character" img-content="character" img-format="tif"/>={x,y}<sub>i </sub>is recorded. At step <b>2</b>, a training loop is initiated which repeats steps <b>3</b> to <b>6</b> until a predictive model is output. At step <b>3</b>, a batch of past agent trajectory information (x) and future agent trajectory information (y) is sampled from the recorded trajectory data x,y&#x2dc;<img id="CUSTOM-CHARACTER-00024" he="2.79mm" wi="2.79mm" file="US20230001953A1-20230105-P00017.TIF" alt="custom-character" img-content="character" img-format="tif"/>. At step <b>4</b>, hypothetical controls u, &#xfb;<sub>n</sub><sup>k </sup>are computed using Equations (6) and (7). At step <b>5</b>, a weight: w(u, &#xfb;<sub>n</sub><sup>k</sup>) is computed according to Equation (8). At step <b>6</b>, the model is updated &#x3b8;&#x2190;&#x3b8;+w(u,&#xfb;<sub>n</sub><sup>k</sup>)&#x2207;<sub>&#x3b8; </sub>log q<sub>&#x3b8;</sub>(y|x) based on the weight: w(u, &#xfb;<sub>n</sub><sup>k</sup>) and the NLLs of the known previous x and future trajectories y. The control-aware prediction objective process outputs the predictive model q<sub>&#x3b8;</sub>:X&#x2192;<img id="CUSTOM-CHARACTER-00025" he="3.89mm" wi="3.56mm" file="US20230001953A1-20230105-P00018.TIF" alt="custom-character" img-content="character" img-format="tif"/> <sub>y</sub>.</p><heading id="h-0012" level="2">D. Summary of Objectives</heading><p id="p-0110" num="0093">There are various choices for utilities, or weights for traditional module metrics. In Table II several baseline methods are summarized, including NLL and instantiations of prior work as well as attention based weighting, according to aspects of the present disclosure.</p><p id="p-0111" num="0000"><tables id="TABLE-US-00004" num="00004"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="center"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE II</entry></row></thead><tbody valign="top"><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>Comparison of utilities and weighted objectives</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="4"><colspec colname="1" colwidth="63pt" align="left"/><colspec colname="2" colwidth="21pt" align="left"/><colspec colname="3" colwidth="56pt" align="left"/><colspec colname="4" colwidth="77pt" align="left"/><tbody valign="top"><row><entry>Method</entry><entry>Cite</entry><entry>Utility or Weight</entry><entry>Objective <img id="CUSTOM-CHARACTER-00026" he="2.46mm" wi="2.12mm" file="US20230001953A1-20230105-P00019.TIF" alt="custom-character" img-content="character" img-format="tif"/> (&#x3b8;)</entry></row><row><entry namest="1" nameend="4" align="center" rowsep="1"/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="center"/><tbody valign="top"><row><entry>Baselines:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="4"><colspec colname="1" colwidth="63pt" align="left"/><colspec colname="2" colwidth="21pt" align="left"/><colspec colname="3" colwidth="56pt" align="left"/><colspec colname="4" colwidth="77pt" align="left"/><tbody valign="top"><row><entry>R2P2 Gain<sub>y</sub></entry><entry>[29]</entry><entry>&#x3b4;(y, &#x177;)</entry><entry>q<sub>&#x3b8;</sub>(y|x)</entry></row><row><entry>R2P2 Gain<sub>&#x3c0;1</sub></entry><entry/><entry>&#x2225;&#x3c0;(y) &#x2212; &#x3c0;(&#x177;)&#x2225;<sub>1</sub></entry><entry><img id="CUSTOM-CHARACTER-00027" he="2.79mm" wi="2.12mm" file="US20230001953A1-20230105-P00020.TIF" alt="custom-character" img-content="character" img-format="tif"/> <sub>&#x177;</sub>[&#x2225;&#x3c0;(y) &#x2212; &#x3c0;(&#x177;)&#x2225;<sub>1</sub>]</entry></row><row><entry>R2P2 Weight<sub>&#x2207;</sub>{circumflex over (<sub>y</sub>)}</entry><entry>[13]</entry><entry>&#x2225;&#x2207;{circumflex over (<sub>y</sub>)} &#x3c0;(&#x177;)&#x2225;<sub><sub2>1</sub2></sub></entry><entry><img id="CUSTOM-CHARACTER-00028" he="2.79mm" wi="2.12mm" file="US20230001953A1-20230105-P00020.TIF" alt="custom-character" img-content="character" img-format="tif"/> <sub>&#x177;</sub>[&#x2225;&#x2207;{circumflex over (<sub>y</sub>)}&#x3c0;(&#x177;)&#x2225;<sub><sub2>1</sub2></sub>]q<sub>&#x3b8;</sub>(y|x)</entry></row><row><entry>R2P2 Weight<sub>&#x2207;y</sub></entry><entry>[13]</entry><entry>&#x2225;&#x2207;<sub>y </sub>&#x3c0;(y)&#x2225;<sub><sub2>1</sub2></sub></entry><entry>&#x2225;&#x2207;<sub>y</sub>&#x3c0;(y)&#x2225;<sub>1q&#x3b8;</sub>(y|x)</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="center"/><tbody valign="top"><row><entry>Ours:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="4"><colspec colname="1" colwidth="63pt" align="left"/><colspec colname="2" colwidth="21pt" align="left"/><colspec colname="3" colwidth="56pt" align="left"/><colspec colname="4" colwidth="77pt" align="left"/><tbody valign="top"><row><entry>R2P2 Weight<sub>&#x3c0;</sub></entry><entry/><entry>&#x2225;&#x3c0;(y) &#x2212; &#x3c0;(&#x177;)&#x2225;<sub>1</sub></entry><entry><img id="CUSTOM-CHARACTER-00029" he="2.79mm" wi="2.12mm" file="US20230001953A1-20230105-P00020.TIF" alt="custom-character" img-content="character" img-format="tif"/> <sub>&#x177;</sub>[&#x2225;&#x3c0;(y) &#x2212; &#x3c0;(&#x177;)&#x2225;<sub>1</sub>]q<sub>&#x3b8;</sub>(y|x)</entry></row><row><entry>R2P2 Weight<sub>&#x3c0;k</sub></entry><entry/><entry>max<sub>k</sub>&#x2225;&#x3c0;(y) &#x2212; &#x3c0;</entry><entry>max<sub>k</sub>&#x2225;&#x3c0;(y) &#x2212; &#x3c0; </entry></row><row><entry/><entry/><entry>(&#x177;<sup>k</sup>)&#x2225;<sub><sub2>y</sub2></sub></entry><entry>(&#x177;<sup>k</sup>)&#x2225;<sub><sub2>1</sub2></sub>q<sub>&#x3b8;</sub>(y|x)</entry></row><row><entry>AttentionWeight</entry><entry/><entry>&#x3b1;(x)</entry><entry>&#x3b1;(x)q<sub>&#x3b8;</sub>(y|x)</entry></row><row><entry namest="1" nameend="4" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0112" num="0094">Evaluation of the disclosed control-aware prediction objective (CAPO) methods involves consideration of pedestrian trajectory prediction, which is a representative scenario that is commonplace in autonomous driving. In practice, the majority of pedestrian behaviors can safely be ignored by the autonomous vehicle's autonomy stack; however, in rare cases of pedestrian-ego interaction (e.g., road crossings), accurate prediction of pedestrian behavior becomes crucial in avoiding collisions. This sparsity of interaction showcases how predictive models may perform well with respect to traditional metrics (e.g., average displacement error (ADE)) while still leading to suboptimal ego behavior when it matters most. The following description details experimental evaluation and implementation of the aforementioned scenario within an autonomous driving simulator. Next, results are compared between the disclosed CAPO methods and the various baselines discussed in Table II. Aspects of the present disclosure illustrate that the predictive models trained using the disclosed CAPO methods produce safe behavior with fewer collisions relative to other baselines noted in Table III.</p><p id="p-0113" num="0095">In the following description, several scenarios are devised in the following examples that include other agents, whose behavior is context-dependent, and whose apparent behavior depends on their proximity to the ego vehicle (e.g., the closer they are, the less the observation noise on their motions). The following three scenarios are described, which involve pedestrians and other vehicles. A single vehicle is commanded to drive down a road that is adjacent to sidewalks which are populated with pedestrians. Occasionally, a pedestrian will cross the street and the ego agent must slow to avoid a collision when necessary.</p><heading id="h-0013" level="2">Pedestrian Crossing Scenario</heading><p id="p-0114" num="0096"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a drawing of a roadway environment <b>600</b> illustrating a pedestrian crossing scenario relative to an ego vehicle <b>420</b> approaching a crosswalk <b>602</b>, in which a collision is predicted by the ego vehicle <b>420</b>, according to aspects of the present disclosure. In this scenario, an ego vehicle <b>420</b> is driving along a road <b>604</b> with many pedestrians <b>610</b> nearby. The pedestrians <b>610</b> are generally walking along on a sidewalk <b>620</b>, and some cross the road <b>604</b>. In this example, the pedestrians <b>610</b> walk at different speeds (e.g., between 0-2 m/s) on the sidewalk <b>620</b>, walk around each other to avoid collisions, sometimes pausing outside shops, and sometimes crossing the road.</p><p id="p-0115" num="0097">In the pedestrian prediction scenario shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, pedestrians <b>610</b> spawn on the sidewalk <b>620</b> and the ego vehicle <b>420</b> predicts the pedestrian trajectories <b>630</b> (e.g., <b>630</b>-<b>1</b> and <b>630</b>-<b>2</b>) within the next 3 seconds. Some of the pedestrians <b>610</b> may cross the road <b>604</b> at right angles, as shown by a crossing pedestrian trajectory <b>630</b>-<b>1</b>. In this example, a planner of the ego vehicle <b>420</b> predicts a collision with a crossing pedestrian <b>610</b> and starts slowing down, as shown by an ego trajectory prediction <b>640</b> up to the crossing pedestrian trajectory <b>630</b>-<b>1</b>, but not further).</p><p id="p-0116" num="0098">As shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the crossing pedestrian <b>610</b>-<b>1</b> randomly decides to cross the road <b>604</b> at the crosswalk <b>602</b> and does so quickly (e.g., 2 m/s) in the shortest path possible (perpendicular to the road direction). In this example, the ego vehicle <b>420</b> is approaching the crosswalk <b>602</b>, in which a collision is predicted by the ego vehicle <b>420</b> with the crossing pedestrian <b>610</b>-<b>1</b> unless the ego vehicle <b>420</b> performs the action of applying the brakes.</p><p id="p-0117" num="0099"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a drawing of a roadway environment <b>700</b> illustrating the pedestrian crossing scenario relative to an ego vehicle approaching the crosswalk <b>602</b> of <figref idref="DRAWINGS">FIG. <b>6</b></figref>, in which no collision is predicted by the ego vehicle, according to aspects of the present disclosure. In these example, no collision is predicted because the crossing pedestrian <b>610</b>-<b>1</b> has crossed the crosswalk <b>602</b>. These aspects of the present disclosure provide a control-aware prediction objective model that recognizes only the (simple) road-crossing behavior is important to model. All the (complex) sidewalk motions, such as a second pedestrian <b>610</b>-<b>2</b> are not important with respect to planning of the ego vehicle <b>420</b>.</p><p id="p-0118" num="0100"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is an overhead view of a roadway environment <b>800</b> illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref> of the pedestrian crossing scenario, according to aspects of the present disclosure. In this example, the ego vehicle <b>420</b> drives along the road <b>604</b> while reacting to the pedestrians <b>610</b> (<b>610</b>-<b>1</b>, <b>610</b>-<b>2</b>, <b>610</b>-<b>3</b>, and <b>610</b>-<b>4</b>) with predicted pedestrian trajectories <b>630</b> (<b>630</b>-<b>1</b>, <b>630</b>-<b>2</b>, <b>630</b>-<b>3</b>, and <b>630</b>-<b>4</b>). In aspects of the present disclosure, the disclosed, control-aware prediction objectives (CAPO) process learns to capture which trajectory predictions should have more influence on the vehicle's controls. In this example, a primary attention <b>650</b>-<b>1</b> of the ego vehicle <b>420</b> is focused of the crossing pedestrian <b>610</b>-<b>1</b>. In addition, a secondary attention <b>650</b>-<b>2</b> of the ego vehicle <b>420</b> is focused of the crossing pedestrian <b>610</b>-<b>2</b>.</p><p id="p-0119" num="0101">As shown in <figref idref="DRAWINGS">FIGS. <b>6</b>-<b>8</b></figref>, the pedestrians <b>610</b> spawn at random locations on the sidewalk <b>620</b> and are then provided a long-range navigation goal that is also uniformly sampled from the sidewalk <b>620</b>. When the long-range goal is reached, another is sampled to replace it. To induce pseudo-random motion, a short-range goal is also generated at each time step. This goal is generated by projecting point 4 meters along the path to the long-range goal, starting at the pedestrian's location. The lateral offset &#x3b2;<sub>t+1 </sub>of the short-range goal is generated by sampling from a normal distribution centered about the previous lateral offset &#x3b2;<sub>t </sub>after it has been scaled down (to drive it towards the long-range goal):</p><p id="p-0120" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>&#x3b2;<sub>t+1</sub>=(1&#x2212;&#x221a;)&#x3b2;<sub>t</sub>+<img id="CUSTOM-CHARACTER-00030" he="2.46mm" wi="2.79mm" file="US20230001953A1-20230105-P00021.TIF" alt="custom-character" img-content="character" img-format="tif"/>(0,&#x3c3;<sup>2</sup>),&#x2003;&#x2003;(10)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0121" num="0000">where &#x3c3; is the variance of the noise, and &#x3f5;&#x3f5;[0, 1) is the commitment to the long-range goal.</p><p id="p-0122" num="0102">When on the sidewalk <b>620</b>, the pedestrians <b>610</b> are programmed to walk at speeds sampled about 2 meters per second (m/s) while navigating around other pedestrians <b>610</b> to avoid collisions and, occasionally, pausing outside of shops. Each different kind of the pedestrians <b>610</b> is defined with various noise levels, commitment, and stopping chance. For example, the pedestrians <b>610</b> may randomly decide to cross the road <b>604</b>. The probability increases if their velocity vector points towards the road <b>604</b> and increases greatly when the pedestrians <b>610</b> are close to the road <b>604</b>. While crossing, the pedestrians <b>610</b> travel at 2 m/s in the shortest path possible (e.g., perpendicular to the road direction. To increase task difficulty, the probability that the pedestrians <b>610</b> cross the road <b>604</b> is increased at test time.</p><heading id="h-0014" level="2">Compared Models</heading><p id="p-0123" num="0103">1) Oracle distribution The pedestrian behavior is modeled with a known distribution at each time step. However, the planner needs predictions over several time steps. The trajectory distribution is approximated by sampling <b>5</b> trajectories for each pedestrian. The planner reacts to the trajectory that would cause the closest intersection with its desired path. This is the true distribution but it is not biased toward the concern and using it with a few samples does not give the best results.</p><p id="p-0124" num="0104">2) Attention weighting As presented in section B, this model is trained with the algorithm 1 and as a baseline, it is compared with a training using uniform weights instead of those computed from attention.</p><p id="p-0125" num="0105">3) Reparametrized Push forward Policy (R2P2) The likelihood-based multi-agent prediction algorithm R2P2 is used as baseline Gain<sub>y</sub>, and also R2P2 is used as the base model for all other predictive models apart from the attention model. R2P2 is an autoregressive normalizing flow, capable of expressing multimodal agent trajectories, trained with NLL. In this example, R2P2 are parameterized to predict 30 steps with data at 10 Hz, corresponding to a 3s prediction for all pedestrians. When sampling, K=10 samples are taken.</p><p id="p-0126" num="0000"><tables id="TABLE-US-00005" num="00005"><table frame="none" colsep="0" rowsep="0" pgwide="1"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="294pt" align="center"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE III</entry></row></thead><tbody valign="top"><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>Scenario results. 100 episodes. </entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="7"><colspec colname="1" colwidth="63pt" align="left"/><colspec colname="2" colwidth="28pt" align="center"/><colspec colname="3" colwidth="35pt" align="center"/><colspec colname="4" colwidth="42pt" align="center"/><colspec colname="5" colwidth="42pt" align="center"/><colspec colname="6" colwidth="42pt" align="center"/><colspec colname="7" colwidth="42pt" align="center"/><tbody valign="top"><row><entry>Predictive</entry><entry>Success</entry><entry>Collisions</entry><entry>Speed</entry><entry>Jerk</entry><entry>ADE</entry><entry>Control</entry></row><row><entry>Model</entry><entry>Rate &#x2191;</entry><entry>&#x2193;</entry><entry>(m/s) &#x2191;</entry><entry>(m/s<img id="CUSTOM-CHARACTER-00031" he="2.46mm" wi="2.46mm" file="US20230001953A1-20230105-P00899.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/> ) &#x2193;</entry><entry>(m) &#x2193;</entry><entry>Error &#x2193;</entry></row><row><entry namest="1" nameend="7" align="center" rowsep="1"/></row><row><entry>baselines</entry><entry/><entry/><entry/><entry/><entry/><entry/></row><row><entry>R2P2 Gain<img id="CUSTOM-CHARACTER-00032" he="2.46mm" wi="2.46mm" file="US20230001953A1-20230105-P00899.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/> </entry><entry>89.0%</entry><entry>11</entry><entry>&#x2002;9.97 &#xb1; 0.222</entry><entry>8.02 &#xb1; 0.250</entry><entry>2.00 &#xb1; 0.024</entry><entry>0.59 &#xb1; 0.012</entry></row><row><entry>R2P2 Gain<img id="CUSTOM-CHARACTER-00033" he="2.46mm" wi="2.46mm" file="US20230001953A1-20230105-P00899.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/> </entry><entry>85.0%</entry><entry>14</entry><entry>10.45 &#xb1; 0.208</entry><entry>6.65 &#xb1; 0.196</entry><entry>3.48 &#xb1; 0.038</entry><entry>0.63 &#xb1; 0.016</entry></row><row><entry>R2P2 Weight&#x2207;<img id="CUSTOM-CHARACTER-00034" he="2.46mm" wi="2.46mm" file="US20230001953A1-20230105-P00899.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/> </entry><entry>94.0%</entry><entry>&#x2002;4</entry><entry>&#x2002;9.53 &#xb1; 0.216</entry><entry>8.21 &#xb1; 0.140</entry><entry><u style="single">1.98</u> &#xb1; 0.024 </entry><entry>0.60 &#xb1; 0.012</entry></row><row><entry>R2P2 Weight&#x2207;<img id="CUSTOM-CHARACTER-00035" he="2.46mm" wi="2.46mm" file="US20230001953A1-20230105-P00899.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/> </entry><entry>91.0%</entry><entry>&#x2002;9</entry><entry>&#x2002;9.74 &#xb1; 0.216</entry><entry>8.74 &#xb1; 0.184</entry><entry>2.00 &#xb1; 0.025</entry><entry>0.60 &#xb1; 0.011</entry></row><row><entry>Attention</entry><entry>89.0%</entry><entry>11</entry><entry><u style="single">13.79</u> &#xb1; 0.214</entry><entry><u style="single">4.48</u> &#xb1; 0.147</entry><entry>2.61 &#xb1; 0.050</entry><entry>0.63 &#xb1; 0.026</entry></row><row><entry>our methods</entry><entry/><entry/><entry/><entry/><entry/><entry/></row><row><entry>R2P2 Weight<img id="CUSTOM-CHARACTER-00036" he="2.46mm" wi="2.46mm" file="US20230001953A1-20230105-P00899.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/> </entry><entry>93.0%</entry><entry>&#x2002;7</entry><entry>&#x2002;8.66 &#xb1; 0.188</entry><entry>9.26 &#xb1; 0.194</entry><entry>2.29 &#xb1; 0.022</entry><entry>0.58 &#xb1; 0.010</entry></row><row><entry>R2P2 Weight<img id="CUSTOM-CHARACTER-00037" he="2.46mm" wi="2.46mm" file="US20230001953A1-20230105-P00899.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/> </entry><entry><b>99.0%</b></entry><entry>&#x2002;<b>1</b></entry><entry>&#x2002;9.46 &#xb1; 0.196</entry><entry>7.89 &#xb1; 0.159</entry><entry>2.14 &#xb1; 0.018</entry><entry><u style="single">0.55</u> &#xb1; 0.011</entry></row><row><entry>Attention Weight<img id="CUSTOM-CHARACTER-00038" he="2.46mm" wi="2.46mm" file="US20230001953A1-20230105-P00899.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/> </entry><entry>91.0%</entry><entry>&#x2002;9</entry><entry><b>14.36</b> &#xb1; 0.217</entry><entry><b>4.22</b> &#xb1; 0.154</entry><entry>2.58 &#xb1; 0.053</entry><entry>0.64 &#xb1; 0.024</entry></row><row><entry>oracle distribution</entry><entry><u style="single">98.0%</u></entry><entry>&#x2002;<u style="single">2</u></entry><entry>10.54 &#xb1; 0.231</entry><entry>6.80 &#xb1; 0.180</entry><entry><b>1.58</b> &#xb1; 0.036</entry><entry><b>0.51</b> &#xb1; 0.013</entry></row><row><entry namest="1" nameend="7" align="center" rowsep="1"/></row><row><entry namest="1" nameend="7" align="left" id="FOO-00001">Arrows indicate higher/lower preferred.</entry></row><row><entry namest="1" nameend="7" align="left" id="FOO-00002">Standard errors shown.</entry></row><row><entry namest="1" nameend="7" align="left" id="FOO-00003"><b>Best</b>.</entry></row><row><entry namest="1" nameend="7" align="left" id="FOO-00004"><u style="single">second</u>.</entry></row><row><entry namest="1" nameend="7" align="left" id="FOO-00005"><img id="CUSTOM-CHARACTER-00039" he="2.46mm" wi="2.46mm" file="US20230001953A1-20230105-P00899.TIF" alt="text missing or illegible when filed" img-content="character" img-format="tif"/>  indicates data missing or illegible when filed</entry></row></tbody></tgroup></table></tables></p><heading id="h-0015" level="2">Metrics</heading><p id="p-0127" num="0106">Table III presents results for 100 sequences. The performance of the system (e.g., prediction and planner) is tracked according to a success rate and a number of collisions. In this example, three conditions may end a sequence:<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0000">    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0107">Success: vehicles traverse 200 meter road without incident.</li>        <li id="ul0002-0002" num="0108">Collision: a pedestrian was hurt.</li>        <li id="ul0002-0003" num="0109">Time out: the car was too slow (&#x3e;60 s).</li>    </ul>    </li></ul></p><p id="p-0128" num="0110">This comparison scores efficiency and comfort indicators by average speed and average jerk, respectively. Finally, the average pedestrian trajectory prediction errors are computed as well as their downstream effect on the planner with an average displacement error (ADE) and a control error equal to &#x2225;&#x3c0;(y)&#x2212;&#x3c0;(&#x177;)&#x2225;<sub>1</sub>. The control error measures the downstream effect of the prediction error on the ego's plans.</p><p id="p-0129" num="0111">The results in Table III show that all methods do reasonably well; specifically, weighting predictive objectives by their downstream effect does improve downstream performance as illustrated by a low collision count and control error. While methods such as R2P2 Weight<sub>&#x2207;&#x177; assume a differentiable controller, this assumption does not need to be made, and the disclosed CAPO methods can work with any type of controller. While the disclosed CAPO methods did not score as well on the ADE metric of agents' trajectories, they did score best on the metric that matters more: the control error, thus mitigating error propagated downstream and improving the end task performance. The disclosed CAPO methods take into account the full predictive distribution when computing a weighting. </sub></p><p id="p-0130" num="0112">Modular autonomous systems (such as those commonly used in autonomous vehicles) provide a number of advantages, but generally incur the disadvantage that individual components typically do not directly optimize for system-wide or downstream performance metrics. Aspects of the present disclosure propose metrics for learning prediction models that account for the downstream objective without imposing stringent specifications on downstream components (such as end-to-end differentiability). These metrics weight the usual likelihood objective, either using attention weights derived from a behavior-cloned policy, or using the impact that substituting predicted trajectories for ground-truth trajectories has on planner output. Accounting for the downstream objective in this manner encourages prediction models to focus on what is important&#x2014;either at the agent or individual trajectory level&#x2014;and, as a result, improves system-wide performance, as demonstrated empirically in a realistic pedestrian jaywalking scenario described above.</p><p id="p-0131" num="0113"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a flowchart illustrating a method of generating an output trajectory of an ego vehicle, according to aspects of the present disclosure. A method <b>900</b> begins at block <b>902</b>, in which trajectory data of the ego vehicle and pedestrian agents is recorded from a scene of a training environment of the ego vehicle. At step <b>1</b> of Algorithm 1, trajectory data <img id="CUSTOM-CHARACTER-00040" he="2.46mm" wi="2.46mm" file="US20230001953A1-20230105-P00022.TIF" alt="custom-character" img-content="character" img-format="tif"/>={x,y}<sub>i</sub>, of past agent trajectory information (x) and future agent trajectory information (y) is recorded. At step <b>2</b>, a training loop is initiated which repeats steps <b>3</b> to <b>6</b> until a predictive model is output. At step <b>3</b>, a batch of past agent trajectory information (x) and future agent trajectory information (y) is sampled from the recorded trajectory data x,y&#x2dc;<img id="CUSTOM-CHARACTER-00041" he="2.46mm" wi="2.46mm" file="US20230001953A1-20230105-P00023.TIF" alt="custom-character" img-content="character" img-format="tif"/>.</p><p id="p-0132" num="0114">At block <b>904</b>, a pedestrian agent from the pedestrian agents within the scene of the training environment of the ego vehicle causing a prediction-discrepancy by the ego vehicle greater than the pedestrian agents within the scene is identified. For example, as shown in Algorithm 1, at step <b>4</b>, an attention model is run to estimate a future ego trajectory &#x177;<sub>ego </sub>and a future agent trajectory &#x177; from the past agent trajectories x. At step <b>5</b>, an attention: &#x3b1;(x) is computed according to Equation (4). At step <b>6</b>, a weight: w(&#x3b1;(x)) is computed according to Equation (5). These aspects of the present disclosure use attention coefficients &#x3b1; as importance factors in a weighted sum of per-human state prediction loss (as opposed to uniform weighting). Therefore, as the attention model learns the correlations between the planner's trajectories and the agents' trajectories, larger attention coefficients are given to the agents that cause larger reactions from the controller. In these aspects of the present disclosure, the attention model learns the agents that cause larger reactions from the controller offline and does not access the controller nor the controller gradient.</p><p id="p-0133" num="0115">According to the Algorithm 2, at step <b>4</b>, hypothetical controls u, &#xfb;<sub>n</sub><sup>k </sup>are computed using Equations (6) and (7). At step <b>5</b>, a weight: w(u,&#xfb;<sub>n</sub><sup>k</sup>) is computed according to Equation (8). In this aspect of the present disclosure, a difference in these two hypothetical controls u, &#xfb;<sub>n</sub><sup>k </sup>corresponds to how much an individual agent affects the ego vehicle, and can represent the concern associated with predicting this particular agent in this particular instance accurately. This second proposal can also be formulated as a re-weighted maximization objective, where the log likelihood of each agent's trajectory in a scene is weighted by its individual contribution to the ego's control decision.</p><p id="p-0134" num="0116">At block <b>906</b>, parameters of a motion prediction model of the ego vehicle are updated based on a magnitude of the prediction-discrepancy caused by the at least one pedestrian agent on the ego vehicle to form a trained, control-aware prediction objective model. For example, as shown in Algorithm 1, at step <b>7</b>, the model is updated &#x3b8;&#x2190;&#x3b8;+w(x)&#x2207;<sub>&#x3b8;</sub> log q<sub>&#x3b8;</sub>(y|x)+&#x2207;<sub>&#x3b8; </sub>log q<sub>&#x3b8;</sub>(y<sub>ego</sub>|x) based on the weight w(x) and the NLLs of the known previous x and future trajectories y and the estimated future ego trajectory &#x177;<sub>ego</sub>. The control-aware prediction objective process outputs the predictive model q<sub>&#x3b8;</sub>:X&#x2192;<img id="CUSTOM-CHARACTER-00042" he="2.79mm" wi="2.46mm" file="US20230001953A1-20230105-P00001.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>y&#xd7;y</sub><sub><sub2>ego</sub2></sub>. By contrast, according to Algorithm 2, at step <b>6</b>, the model &#x3b8;&#x2190;&#x3b8;+w(u,&#xfb;<sub>n</sub><sup>k</sup>)&#x2207;<sub>&#x3b8; </sub>log q<sub>&#x3b8;</sub>(y|x) is updated based on the weight: w(u,&#xfb;<sub>n</sub><sup>k</sup>) and the NLLs of the known previous x and future trajectories y. The control-aware prediction objective process outputs the predictive model q<sub>&#x3b8;</sub>:X&#x2192;<img id="CUSTOM-CHARACTER-00043" he="2.79mm" wi="2.46mm" file="US20230001953A1-20230105-P00001.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>y</sub>.</p><p id="p-0135" num="0117">At block <b>908</b>, a vehicle control action of the ego vehicle is selected in response to a predicted motion from the trained, control-aware prediction objective model regarding detected pedestrian agents within a traffic environment of the ego vehicle. For example, as shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the crossing pedestrian <b>610</b>-<b>1</b> randomly decides to cross the road <b>604</b> at the crosswalk <b>602</b> and does so quickly (e.g., 2 m/s) in the shortest path possible (perpendicular to the road direction). In this example, the ego vehicle <b>420</b> is approaching the crosswalk <b>602</b>, in which a collision is predicted by the ego vehicle <b>420</b> with the crossing pedestrian <b>610</b>-<b>1</b> unless the ego vehicle <b>420</b> performs an action of applying the brakes. As shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, no collision is predicted by the ego vehicle <b>420</b>. In this example, no collision is predicted because the crossing pedestrian <b>610</b>-<b>1</b> has crossed the crosswalk <b>602</b>. These aspects of the present disclosure provide a control-aware prediction objective model that recognizes only the (simple) road-crossing behavior is important to model. All the (complex) sidewalk motions, such as the second pedestrian <b>610</b>-<b>2</b>, are not important with respect to planning of the ego vehicle <b>420</b>.</p><p id="p-0136" num="0118">The method <b>900</b> also includes selecting an nth pedestrian agent in the scene: enumerating each of the pedestrian agents in the scene other than the nth pedestrian agent. The method <b>900</b> further includes computing control outputs from a controller of the pedestrian agents according to the recorded trajectory data. The method <b>900</b> also includes resampling a different trajectory from a known trajectory of the nth agent, &#x177;<sub>u</sub><sup>k</sup>&#x2dc;q<sub>&#x3b8;</sub>(&#x177;<sub>n</sub>|x); computing a control output from the ego vehicle controller according to the different trajectory &#xfb;<sub>n</sub><sup>k</sup>=&#x3c0;({&#x177;<sub>n</sub><sup>k</sup>}&#x222a;y\{y<sub>n</sub>}). The method <b>900</b> further includes comparing the control output against control outputs of the pedestrian agents according to the recorded trajectory data u=&#x3c0;(y).</p><p id="p-0137" num="0119">Autonomous vehicle software is typically structured as a modular pipeline of individual components (e.g., perception, prediction, and planning) to help separate concerns into interpretable sub-tasks. Even when end-to-end training is possible, each module has its own set of objectives used for safety assurance, sample efficiency, regularization, or interpretability. Nevertheless, intermediate objectives do not always align with overall system performance. For example, optimizing the likelihood of a trajectory prediction module might focus more on easy-to-predict agents than safety-critical or rare behaviors (e.g., jaywalking). Some aspects of the present disclosure present control-aware prediction objectives (CAPOs), to evaluate the downstream effect of predictions on control without specifying a differentiable planner. These aspects of the present disclosure propose two types of importance weights that weight the predictive likelihood: (1) one using an attention model between agents, and (2) another based on control variation when exchanging predicted trajectories for ground truth trajectories.</p><p id="p-0138" num="0120">In some aspects, the methods shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref> may be performed by the SOC <b>100</b> (<figref idref="DRAWINGS">FIG. <b>1</b></figref>) or the software architecture <b>200</b> (<figref idref="DRAWINGS">FIG. <b>2</b></figref>) of the autonomous vehicle <b>150</b>. That is, each of the elements or method may, for example, but without limitation, be performed by the SOC <b>100</b>, the software architecture <b>200</b>, the processor (e.g., CPU <b>102</b>) and/or other components included therein of the autonomous vehicle <b>150</b>, or the vehicle action planner system <b>300</b>.</p><p id="p-0139" num="0121">The various operations of methods described above may be performed by any suitable means capable of performing the corresponding functions. The means may include various hardware and/or software component(s) and/or module(s), including, but not limited to, a circuit, an application specific integrated circuit (ASIC), or processor. Generally, where there are operations illustrated in the figures, those operations may have corresponding counterpart means-plus-function components with similar numbering.</p><p id="p-0140" num="0122">As used herein, the term &#x201c;determining&#x201d; encompasses a wide variety of actions. For example, &#x201c;determining&#x201d; may include calculating, computing, processing, deriving, investigating, looking up (e.g., looking up in a table, a database or another data structure), ascertaining, and the like. Additionally, &#x201c;determining&#x201d; may include receiving (e.g., receiving information), accessing (e.g., accessing data in a memory), and the like. Furthermore, &#x201c;determining&#x201d; may include resolving, selecting, choosing, establishing, and the like.</p><p id="p-0141" num="0123">As used herein, a phrase referring to &#x201c;at least one of&#x201d; a list of items refers to any combination of those items, including single members. As an example, &#x201c;at least one of: a, b, or c&#x201d; is intended to cover: a, b, c, a-b, a-c, b-c, and a-b-c.</p><p id="p-0142" num="0124">The various illustrative logical blocks, modules, and circuits described in connection with the present disclosure may be implemented or performed with a processor configured according to the present disclosure, a digital signal processor (DSP), an application specific integrated circuit (ASIC), a field programmable gate array signal (FPGA) or other programmable logic device (PLD), discrete gate or transistor logic, discrete hardware components or any combination thereof designed to perform the functions described herein. The processor may be a microprocessor, but, in the alternative, the processor may be any commercially available processor, controller, microcontroller, or state machine specially configured as described herein. A processor may also be implemented as a combination of computing devices, e.g., a combination of a DSP and a microprocessor, a plurality of microprocessors, one or more microprocessors in conjunction with a DSP core, or any other such configuration.</p><p id="p-0143" num="0125">The steps of a method or algorithm described in connection with the present disclosure may be embodied directly in hardware, in a software module executed by a processor, or in a combination of the two. A software module may reside in any form of storage medium that is known in the art. Some examples of storage media that may be used include random access memory (RAM), read only memory (ROM), flash memory, erasable programmable read-only memory (EPROM), electrically erasable programmable read-only memory (EEPROM), registers, a hard disk, a removable disk, a CD-ROM, and so forth. A software module may comprise a single instruction, or many instructions, and may be distributed over several different code segments, among different programs, and across multiple storage media. A storage medium may be coupled to a processor such that the processor can read information from, and write information to, the storage medium. In the alternative, the storage medium may be integral to the processor.</p><p id="p-0144" num="0126">The methods disclosed herein comprise one or more steps or actions for achieving the described method. The method steps and/or actions may be interchanged with one another without departing from the scope of the claims. In other words, unless a specific order of steps or actions is specified, the order and/or use of specific steps and/or actions may be modified without departing from the scope of the claims.</p><p id="p-0145" num="0127">The functions described may be implemented in hardware, software, firmware, or any combination thereof. If implemented in hardware, an example hardware configuration may comprise a processing system in a device. The processing system may be implemented with a bus architecture. The bus may include any number of interconnecting buses and bridges depending on the specific application of the processing system and the overall design constraints. The bus may link together various circuits including a processor, machine-readable media, and a bus interface. The bus interface may connect a network adapter, among other things, to the processing system via the bus. The network adapter may implement signal processing functions. For certain aspects, a user interface (e.g., keypad, display, mouse, joystick, etc.) may also be connected to the bus. The bus may also link various other circuits such as timing sources, peripherals, voltage regulators, power management circuits, and the like, which are well known in the art, and therefore, will not be described any further.</p><p id="p-0146" num="0128">The processor may be responsible for managing the bus and processing, including the execution of software stored on the machine-readable media. Examples of processors that may be specially configured according to the present disclosure include microprocessors, microcontrollers, DSP processors, and other circuitry that can execute software. Software shall be construed broadly to mean instructions, data, or any combination thereof, whether referred to as software, firmware, middleware, microcode, hardware description language, or otherwise. Machine-readable media may include, by way of example, random access memory (RAM), flash memory, read only memory (ROM), programmable read-only memory (PROM), erasable programmable read-only memory (EPROM), electrically erasable programmable read-only memory (EEPROM), registers, magnetic disks, optical disks, hard drives, or any other suitable storage medium, or any combination thereof. The machine-readable media may be embodied in a computer-program product. The computer-program product may comprise packaging materials.</p><p id="p-0147" num="0129">In a hardware implementation, the machine-readable media may be part of the processing system separate from the processor. However, as those skilled in the art will readily appreciate, the machine-readable media, or any portion thereof, may be external to the processing system. By way of example, the machine-readable media may include a transmission line, a carrier wave modulated by data, and/or a computer product separate from the device, all which may be accessed by the processor through the bus interface. Alternatively, or in addition, the machine-readable media, or any portion thereof, may be integrated into the processor, such as the case may be with cache and/or specialized register files. Although the various components discussed may be described as having a specific location, such as a local component, they may also be configured in various ways, such as certain components being configured as part of a distributed computing system.</p><p id="p-0148" num="0130">The processing system may be configured with one or more microprocessors providing the processor functionality and external memory providing at least a portion of the machine-readable media, all linked together with other supporting circuitry through an external bus architecture. Alternatively, the processing system may comprise one or more neuromorphic processors for implementing the neuron models and models of neural systems described herein. As another alternative, the processing system may be implemented with an application specific integrated circuit (ASIC) with the processor, the bus interface, the user interface, supporting circuitry, and at least a portion of the machine-readable media integrated into a single chip, or with one or more field programmable gate arrays (FPGAs), programmable logic devices (PLDs), controllers, state machines, gated logic, discrete hardware components, or any other suitable circuitry, or any combination of circuits that can perform the various functions described throughout the present disclosure. Those skilled in the art will recognize how best to implement the described functionality for the processing system depending on the particular application and the overall design constraints imposed on the overall system.</p><p id="p-0149" num="0131">The machine-readable media may comprise a number of software modules. The software modules include instructions that, when executed by the processor, cause the processing system to perform various functions. The software modules may include a transmission module and a receiving module. Each software module may reside in a single storage device or be distributed across multiple storage devices. By way of example, a software module may be loaded into RAM from a hard drive when a triggering event occurs. During execution of the software module, the processor may load some of the instructions into cache to increase access speed. One or more cache lines may then be loaded into a special purpose register file for execution by the processor. When referring to the functionality of a software module below, it will be understood that such functionality is implemented by the processor when executing instructions from that software module. Furthermore, it should be appreciated that aspects of the present disclosure result in improvements to the functioning of the processor, computer, machine, or other system implementing such aspects.</p><p id="p-0150" num="0132">If implemented in software, the functions may be stored or transmitted over as one or more instructions or code on a non-transitory computer-readable medium. Computer-readable media include both computer storage media and communication media including any medium that facilitates transfer of a computer program from one place to another. A storage medium may be any available medium that can be accessed by a computer. By way of example, and not limitation, such computer-readable media can comprise RAM, ROM, EEPROM, CD-ROM or other optical disk storage, magnetic disk storage or other magnetic storage devices, or any other medium that can carry or store desired program code in the form of instructions or data structures and that can be accessed by a computer. Additionally, any connection is properly termed a computer-readable medium. For example, if the software is transmitted from a website, server, or other remote source using a coaxial cable, fiber optic cable, twisted pair, digital subscriber line (DSL), or wireless technologies such as infrared (IR), radio, and microwave, then the coaxial cable, fiber optic cable, twisted pair, DSL, or wireless technologies such as infrared, radio, and microwave are included in the definition of medium. Disk and disc, as used herein, include compact disc (CD), laser disc, optical disc, digital versatile disc (DVD), floppy disk, and Blu-ray&#xae; disc, where disks usually reproduce data magnetically, while discs reproduce data optically with lasers. Thus, in some aspects computer-readable media may comprise non-transitory computer-readable media (e.g., tangible media). In addition, for other aspects, computer-readable media may comprise transitory computer-readable media (e.g., a signal). Combinations of the above should also be included within the scope of computer-readable media.</p><p id="p-0151" num="0133">Thus, certain aspects may comprise a computer program product for performing the operations presented herein. For example, such a computer program product may comprise a computer-readable medium having instructions stored (and/or encoded) thereon, the instructions being executable by one or more processors to perform the operations described herein. For certain aspects, the computer program product may include packaging material.</p><p id="p-0152" num="0134">Further, it should be appreciated that modules and/or other appropriate means for performing the methods and techniques described herein can be downloaded and/or otherwise obtained by a user terminal and/or base station as applicable. For example, such a device can be coupled to a server to facilitate the transfer of means for performing the methods described herein. Alternatively, various methods described herein can be provided via storage means (e.g., RAM, ROM, a physical storage medium such as a compact disc (CD) or floppy disk, etc.), such that a user terminal and/or base station can obtain the various methods upon coupling or providing the storage means to the device. Moreover, any other suitable technique for providing the methods and techniques described herein to a device can be utilized.</p><p id="p-0153" num="0135">It is to be understood that the claims are not limited to the precise configuration and components illustrated above. Various modifications, changes, and variations may be made in the arrangement, operation, and details of the methods and apparatus described above without departing from the scope of the claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230001953A1-20230105-M00001.NB"><img id="EMI-M00001" he="5.25mm" wi="38.78mm" file="US20230001953A1-20230105-M00001.TIF" alt="embedded image " img-content="table" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002" nb-file="US20230001953A1-20230105-M00002.NB"><img id="EMI-M00002" he="7.03mm" wi="76.20mm" file="US20230001953A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00003" nb-file="US20230001953A1-20230105-M00003.NB"><img id="EMI-M00003" he="8.13mm" wi="76.20mm" file="US20230001953A1-20230105-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00004" nb-file="US20230001953A1-20230105-M00004.NB"><img id="EMI-M00004" he="4.91mm" wi="76.20mm" file="US20230001953A1-20230105-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00005" nb-file="US20230001953A1-20230105-M00005.NB"><img id="EMI-M00005" he="8.13mm" wi="76.20mm" file="US20230001953A1-20230105-M00005.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method of generating an output trajectory of an ego vehicle, the method comprising:<claim-text>recording trajectory data of the ego vehicle and pedestrian agents from a scene of a training environment of the ego vehicle;</claim-text><claim-text>identifying at least one pedestrian agent from the pedestrian agents within the scene of the training environment of the ego vehicle causing a prediction-discrepancy by the ego vehicle greater than the pedestrian agents within the scene;</claim-text><claim-text>updating parameters of a motion prediction model of the ego vehicle based on a magnitude of the prediction-discrepancy caused by the at least one pedestrian agent on the ego vehicle to form a trained, control-aware prediction objective model; and</claim-text><claim-text>selecting a vehicle control action of the ego vehicle in response to a predicted motion from the trained, control-aware prediction objective model regarding detected pedestrian agents within a traffic environment of the ego vehicle.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, in which identifying the at least one pedestrian agent comprises:<claim-text>predicting, using the motion prediction model, a future motion of the pedestrian agents and a future motion of the ego vehicle based on the recorded trajectory data; and</claim-text><claim-text>computing an attention vector according to the future motion of the pedestrian agents and a future motion of the ego vehicle.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, in which the updating of parameters comprises:<claim-text>computing a weighted sum according to the attention vector; and</claim-text><claim-text>training the motion prediction model according to the weight sum to learn correlations between planner trajectories and agent trajectories; and</claim-text><claim-text>assigning larger attention coefficients to the at least one agent causing the prediction-discrepancy from a controller of the ego vehicle.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, in which identifying the at least one pedestrian agent comprises:<claim-text>selecting an nth pedestrian agent in the scene:</claim-text><claim-text>enumerating each of the pedestrian agents in the scene other than the nth pedestrian agent;</claim-text><claim-text>computing control outputs from a controller of the pedestrian agents according to the recorded trajectory data;</claim-text><claim-text>resampling a different trajectory from a known trajectory of the nth agent, &#x177;<sub>n</sub><sup>k</sup>&#x2dc;q<sub>&#x3b8;</sub>(&#x177;<sub>n</sub>|x);</claim-text><claim-text>computing a control output from the ego vehicle controller according to the different trajectory &#xfb;<sub>n</sub><sup>k</sup>=&#x3c0;({&#x177;<sub>n</sub><sup>k</sup>}&#x222a;y\{y<sub>n</sub>});</claim-text><claim-text>comparing the control output against control outputs of the pedestrian agents according to the recorded trajectory data u=&#x3c0;(y).</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, in which the updating of parameters comprises:<claim-text>computing a weight for the nth agent according to the control output of the nth agent relative to the control outputs of the pedestrian agents; and</claim-text><claim-text>updating the motion prediction model according to the computed weight.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising performing the vehicle control action to maneuver the ego vehicle according to the predicted motion of the detected pedestrian agents within the traffic environment of the ego vehicle.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, in which the vehicle control action comprises throttling, steering, and/or braking.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, in which updating the parameters comprises training the trained, control-aware prediction objective model to weight a log likelihood of a trajectory of each of the pedestrian agents in the scene by a respective contribution of the pedestrian agents to a control decision of the ego vehicle.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. A non-transitory computer-readable medium having program code recorded thereon for generating an output trajectory of an ego vehicle, the program code being executed by a processor and comprising:<claim-text>program code to record trajectory data of the ego vehicle and pedestrian agents from a scene of a training environment of the ego vehicle;</claim-text><claim-text>program code to identify at least one pedestrian agent from the pedestrian agents within the scene of the training environment of the ego vehicle causing a prediction-discrepancy by the ego vehicle greater than the pedestrian agents within the scene;</claim-text><claim-text>program code to update parameters of a motion prediction model of the ego vehicle based on a magnitude of the prediction-discrepancy caused by the at least one pedestrian agent on the ego vehicle to form a trained, control-aware prediction objective model; and</claim-text><claim-text>program code to select a vehicle control action of the ego vehicle in response to a predicted motion from the trained, control-aware prediction objective model regarding detected pedestrian agents within a traffic environment of the ego vehicle.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00009">claim 9</claim-ref>, in which the program code to identify the at least one pedestrian agent comprises:<claim-text>program code to predict, using the motion prediction model, a future motion of the pedestrian agents and a future motion of the ego vehicle based on the recorded trajectory data; and</claim-text><claim-text>program code to compute an attention vector according to the future motion of the pedestrian agents and a future motion of the ego vehicle.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00010">claim 10</claim-ref>, in which the program code to update the parameters comprises:<claim-text>program code to compute a weighted sum according to the attention vector; and</claim-text><claim-text>program code to train the motion prediction model according to the weight sum to learn correlations between planner trajectories and agent trajectories; and</claim-text><claim-text>program code to assign larger attention coefficients to the at least one agent causing the prediction-discrepancy from a controller of the ego vehicle.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00009">claim 9</claim-ref>, in which the program code to identify the at least one pedestrian agent comprises:<claim-text>program code to select an nth pedestrian agent in the scene:</claim-text><claim-text>program code to enumerate each of the pedestrian agents in the scene other than the nth pedestrian agent;</claim-text><claim-text>program code to compute control outputs from a controller of the pedestrian agents according to the recorded trajectory data;</claim-text><claim-text>program code to resample a different trajectory from a known trajectory of the nth agent, &#x177;<sub>n</sub><sup>k</sup>&#x2dc;q<sub>&#x3b8;</sub>(&#x177;<sub>n</sub>|x);</claim-text><claim-text>computing a control output from the ego vehicle controller according to the different trajectory &#xfb;<sub>n</sub><sup>k</sup>=&#x3c0;({&#x177;<sub>n</sub><sup>k</sup>}&#x222a;y\{y<sub>n</sub>});</claim-text><claim-text>comparing the control output against control outputs of the pedestrian agents according to the recorded trajectory data u=&#x3c0;(y).</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00012">claim 12</claim-ref>, in which the program code to update the parameters comprises:<claim-text>program code to compute a weight for the nth agent according to the control output of the nth agent relative to the control outputs of the pedestrian agents; and</claim-text><claim-text>program code to update the motion prediction model according to the computed weight.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00009">claim 9</claim-ref>, further comprising program code to perform the vehicle control action to maneuver the ego vehicle according to the predicted motion of the detected pedestrian agents within the traffic environment of the ego vehicle.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00009">claim 9</claim-ref>, in which the vehicle control action comprises throttling, steering, and/or braking.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00009">claim 9</claim-ref>, in which the program code to update the parameters comprises program code to train the trained, control-aware prediction objective model to weight a log likelihood of a trajectory of each of the pedestrian agents in the scene by a respective contribution of the pedestrian agents to a control decision of the ego vehicle.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. A system for generating an output trajectory of an ego vehicle, the system comprising:<claim-text>a vehicle perception module to record trajectory data of the ego vehicle and pedestrian agents from a scene of a training environment of the ego vehicle;</claim-text><claim-text>a control-aware prediction objective model to identify at least one pedestrian agent from the pedestrian agents within the scene of the training environment of the ego vehicle causing a prediction-discrepancy by the ego vehicle greater than the pedestrian agents within the scene;</claim-text><claim-text>a model parameter update module to update parameters of a motion prediction model of the ego vehicle based on a magnitude of the prediction-discrepancy caused by the at least one pedestrian agent on the ego vehicle to form a trained, control-aware prediction objective model; and</claim-text><claim-text>a vehicle action selection module to select a vehicle control action of the ego vehicle in response to a predicted motion from the trained, control-aware prediction objective model regarding detected pedestrian agents within a traffic environment of the ego vehicle.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The system of <claim-ref idref="CLM-00017">claim 17</claim-ref>, in which the vehicle action selection module is further to perform the vehicle control action to maneuver the ego vehicle according to the predicted motion of the detected pedestrian agents within the traffic environment of the ego vehicle.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The system of <claim-ref idref="CLM-00017">claim 17</claim-ref>, in which the vehicle control action comprises throttling, steering, and/or braking.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The system of <claim-ref idref="CLM-00017">claim 17</claim-ref>, in which the model parameter update module is further to train the trained, control-aware prediction objective model to weight a log likelihood of a trajectory of each of the pedestrian agents in the scene by a respective contribution of the pedestrian agents to a control decision of the ego vehicle.</claim-text></claim></claims></us-patent-application>