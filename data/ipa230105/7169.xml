<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230007170A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230007170</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17931225</doc-number><date>20220912</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>JP</country><doc-number>2018-245750</doc-number><date>20181227</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>232</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>02</class><subclass>B</subclass><main-group>7</main-group><subgroup>10</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>235</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>23232</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>02</class><subclass>B</subclass><main-group>7</main-group><subgroup>102</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>2353</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>02</class><subclass>B</subclass><main-group>7</main-group><subgroup>14</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">IMAGING ELEMENT, IMAGING APPARATUS, IMAGING METHOD, AND PROGRAM</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>17348780</doc-number><date>20210616</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11509823</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17931225</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>PCT/JP2019/049221</doc-number><date>20191216</date></document-id><parent-status>PENDING</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>17348780</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>FUJIFILM CORPORATION</orgname><address><city>Tokyo</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>KAWAI</last-name><first-name>Tomoyuki</first-name><address><city>Saitama</city><country>JP</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>HASEGAWA</last-name><first-name>Ryo</first-name><address><city>Saitama</city><country>JP</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>SAKURABU</last-name><first-name>Hitoshi</first-name><address><city>Saitama</city><country>JP</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>KOBAYASHI</last-name><first-name>Makoto</first-name><address><city>Saitama</city><country>JP</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">An imaging element incorporates a reading portion that reads out captured image data at a first frame rate, a storage portion that stores the image data, a processing portion that processes the image data, and an output portion that outputs the processed image data at a second frame rate lower than the first frame rate. The reading portion reads out the image data of each of a plurality of frames in parallel. The storage portion stores, in parallel, each image data read out in parallel by the reading portion. The processing portion performs generation processing of generating output image data of one frame using the image data of each of the plurality of frames stored in the storage portion.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="96.35mm" wi="157.73mm" file="US20230007170A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="225.81mm" wi="150.88mm" orientation="landscape" file="US20230007170A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="173.99mm" wi="163.15mm" file="US20230007170A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="177.80mm" wi="99.91mm" file="US20230007170A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="147.24mm" wi="163.58mm" file="US20230007170A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="187.96mm" wi="163.91mm" file="US20230007170A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="187.45mm" wi="149.01mm" file="US20230007170A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="187.71mm" wi="149.86mm" file="US20230007170A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="164.42mm" wi="159.77mm" file="US20230007170A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="177.38mm" wi="136.23mm" file="US20230007170A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="156.38mm" wi="123.44mm" file="US20230007170A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="196.43mm" wi="163.41mm" file="US20230007170A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="195.66mm" wi="153.50mm" file="US20230007170A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="195.50mm" wi="153.50mm" file="US20230007170A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="208.96mm" wi="159.68mm" file="US20230007170A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="208.11mm" wi="157.31mm" file="US20230007170A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="170.60mm" wi="159.34mm" file="US20230007170A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="168.91mm" wi="163.15mm" file="US20230007170A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="193.63mm" wi="163.41mm" file="US20230007170A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00019" num="00019"><img id="EMI-D00019" he="217.34mm" wi="159.26mm" file="US20230007170A1-20230105-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00020" num="00020"><img id="EMI-D00020" he="162.90mm" wi="156.63mm" file="US20230007170A1-20230105-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00021" num="00021"><img id="EMI-D00021" he="172.13mm" wi="154.35mm" file="US20230007170A1-20230105-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00022" num="00022"><img id="EMI-D00022" he="172.21mm" wi="154.35mm" file="US20230007170A1-20230105-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00023" num="00023"><img id="EMI-D00023" he="221.06mm" wi="163.41mm" file="US20230007170A1-20230105-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00024" num="00024"><img id="EMI-D00024" he="174.92mm" wi="146.73mm" file="US20230007170A1-20230105-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application is a continuation application of U.S. application Ser. No. 17/348,780, filed Jun. 16, 2021, which is a continuation application of International Application No. PCT/JP2019/049221, filed Dec. 16, 2019, the disclosures of which are incorporated herein by reference in their entireties. Further, this application claims priority from Japanese Patent Application No. 2018-245750, filed Dec. 27, 2018, the disclosure of which is incorporated herein by reference in its entirety.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND</heading><heading id="h-0003" level="1">1. Technical Field</heading><p id="p-0003" num="0002">The technology of the present disclosure relates to an imaging element, an imaging apparatus, an imaging method, and a program.</p><heading id="h-0004" level="1">2. Description of the Related Art</heading><p id="p-0004" num="0003">JP2015-126043A discloses an electronic device comprising a first semiconductor chip including a sensor portion that is formed by arranging a plurality of sensors, and a second semiconductor chip including a signal processing portion that processes signals acquired by the sensors. In the electronic device disclosed in JP2015-126043A, the first semiconductor chip and the second semiconductor chip are laminated.</p><p id="p-0005" num="0004">In the electronic device disclosed in JP2015-126043A, a predetermined type of signal processing including AD conversion is performed on the signal read out from each sensor for each sensor row parallelly in units of sensor columns, and digitized image data is transferred to a memory portion by pipeline. By doing so, a large amount of information can be processed, and the electronic device as a whole can achieve low power consumption. Consequently, a decrease in performance of the sensors due to thermal noise can also be suppressed.</p><p id="p-0006" num="0005">JP2017-225084A discloses an imaging element in which a plurality of pixels are broadly divided into a first pixel region and a second region. In the imaging element disclosed in JP2017-225084A, the first pixel region is a pixel region in which a first image signal is generated by performing imaging in a first exposure time period. The second region is a pixel region in which a second image signal from which information related to a subject is extracted is generated by performing imaging in a second exposure time period longer than the first exposure time period.</p><heading id="h-0005" level="1">SUMMARY</heading><p id="p-0007" num="0006">One embodiment according to the technology of the present disclosure provides an imaging element, an imaging apparatus, an imaging method, and a program capable of outputting a smooth motion picture image, compared to a case of outputting an image captured in an exposure time period shorter than an exposure time period corresponding to an output frame rate.</p><p id="p-0008" num="0007">An imaging element according to a first aspect comprises a reading portion that reads out, at a first frame rate, image data of each frame obtained by imaging a subject and is incorporated in the imaging element, a storage portion that stores the image data read out by the reading portion and is incorporated in the imaging element, a processing portion that processes the image data and is incorporated in the imaging element, and an output portion that outputs the image data processed by the processing portion at a second frame rate and is incorporated in the imaging element, in which first frame rate is a frame rate higher than the second frame rate, the reading portion reads out the image data of each of a plurality of frames in parallel within an output period that is defined by the second frame rate as a period in which the image data of one frame is output, the storage portion stores, in parallel, each image data read out in parallel by the reading portion, the processing portion performs generation processing of generating output image data of one frame using the image data of each of the plurality of frames stored in the storage portion, and the output portion outputs the output image data generated by the generation processing at the second frame rate.</p><p id="p-0009" num="0008">Accordingly, a smooth motion picture image can be output, compared to a case of outputting an image captured in an exposure time period shorter than an exposure time period corresponding to an output frame rate.</p><p id="p-0010" num="0009">In the imaging element according to a second aspect, the first frame rate is changed in connection with an exposure time period.</p><p id="p-0011" num="0010">Accordingly, a change in brightness of the subject can be handled.</p><p id="p-0012" num="0011">In the imaging element according to a third aspect, the first frame rate is increased as the exposure time period is decreased.</p><p id="p-0013" num="0012">Accordingly, a relatively bright subject can be handled.</p><p id="p-0014" num="0013">In the imaging element according to a fourth aspect, after a start of exposure, the exposure for imaging is restarted after reading processing for the image data of at least one pixel by the reading portion is completed.</p><p id="p-0015" num="0014">Accordingly, a time period of non-exposure between the previous exposure and the subsequent exposure can be relatively decreased regardless of the exposure time period.</p><p id="p-0016" num="0015">In the imaging element according to a fifth aspect, the reading portion changes a reading speed of the image data in accordance with the number of frames in which the image data is read out in parallel.</p><p id="p-0017" num="0016">Accordingly, the image data can be processed without delay.</p><p id="p-0018" num="0017">In the imaging element according to a sixth aspect, the reading portion changes the reading speed of the image data in accordance with the number of frames in which the image data is read out in parallel, and the number of AD conversion circuits performing AD conversion on the read image data.</p><p id="p-0019" num="0018">Accordingly, even in a case where the number of AD conversion circuits is limited, the image data can be processed without delay.</p><p id="p-0020" num="0019">In the imaging element according a seventh aspect, the reading portion changes a data amount in a case of performing AD conversion processing on the image data, in accordance with the number of frames in which the image data is read out in parallel, and the number of AD conversion circuits performing AD conversion on the read image data.</p><p id="p-0021" num="0020">Accordingly, even in a case where the number of AD conversion circuits is limited, the image data can be processed without delay.</p><p id="p-0022" num="0021">In the imaging element according to an eighth aspect, the storage portion includes a plurality of storage regions individually storing each of the plurality of pieces of image data.</p><p id="p-0023" num="0022">Accordingly, the plurality of pieces of image data can be stored in parallel.</p><p id="p-0024" num="0023">In the imaging element according to a ninth aspect, the generation processing is processing of generating image data of one frame obtained by calculating an arithmetic mean of at least a part of the image data of each of the plurality of frames stored in the storage portion in units of pixels.</p><p id="p-0025" num="0024">Accordingly, overexposure and deterioration of image quality can be suppressed.</p><p id="p-0026" num="0025">In the imaging element according to a tenth aspect, in the generation processing, the output image data of one frame is generated by combining partial image data that is a part of the image data, from a plurality of pieces of the image data.</p><p id="p-0027" num="0026">Accordingly, the output image data of which a part is temporally new can be output.</p><p id="p-0028" num="0027">In the imaging element according to an eleventh aspect, at least a photoelectric conversion element and the storage portion are formed in one chip.</p><p id="p-0029" num="0028">Accordingly, portability of the imaging element can be increased, compared to an imaging element in which the photoelectric conversion element and the storage portion are not formed in one chip.</p><p id="p-0030" num="0029">In the imaging element according to a twelfth aspect, the imaging element is a laminated imaging element in which the photoelectric conversion element is laminated with the storage portion.</p><p id="p-0031" num="0030">Accordingly, a load exerted on processing between the photoelectric conversion element and the storage portion can be reduced, compared to a an imaging element in which the photoelectric conversion element and the storage portion are not laminated.</p><p id="p-0032" num="0031">An imaging apparatus according to a thirteenth aspect comprises the imaging element according to any one of the first to twelfth aspects, and a control portion that performs a control for displaying an image based on the output image data output by the output portion on a display portion.</p><p id="p-0033" num="0032">Accordingly, a user can visually recognize the image based on the plurality of pieces of image data output by the output portion.</p><p id="p-0034" num="0033">An imaging method according to a fourteenth aspect is an imaging method comprising a step of reading out, at a first frame rate, image data of each frame obtained by imaging a subject, a step of storing the read image data, a step of processing the image data, and a step of outputting the processed image data at a second frame rate lower than the first frame rate, in which in the step of reading out, the image data of each of a plurality of frames is read out in parallel within an output period that is defined by the second frame rate as a period in which the image data of one frame is output, in the step of storing, each image data read out in parallel is stored in parallel, in the step of processing, output image data of one frame is generated using the stored image data of each of a plurality of frames, and in the step of outputting, the generated output image data is output at the second frame rate.</p><p id="p-0035" num="0034">Accordingly, a smooth motion picture image can be output, compared to a case of outputting an image captured in an exposure time period shorter than an exposure time period corresponding to an output frame rate.</p><p id="p-0036" num="0035">A program according to a fifteenth aspect is a program causing a computer to execute a procedure of reading out, at a first frame rate, image data of each frame obtained by imaging a subject, a procedure of storing the read image data, a procedure of processing the image data, and a procedure of outputting the processed image data at a second frame rate lower than the first frame rate, in which in the procedure of reading out, the image data of each of a plurality of frames is read out in parallel within an output period that is defined by the second frame rate as a period in which the image data of one frame is output, in the procedure of storing, each image data read out in parallel is stored in parallel, in the procedure of processing, output image data of one frame is generated using the stored image data of each of a plurality of frames, and in the procedure of outputting, the generated output image data is output at the second frame rate.</p><p id="p-0037" num="0036">Accordingly, a smooth motion picture image can be output, compared to a case of outputting an image captured in an exposure time period shorter than an exposure time period corresponding to an output frame rate.</p><p id="p-0038" num="0037">An imaging element according to one embodiment of the present disclosure comprises a memory that stores image data and is incorporated in the imaging element, and a processor configured to read out, at a first frame rate, image data of each frame obtained by imaging a subject, process the image data, and output the image data processed by the processing portion at a second frame rate lower than the first frame rate, in which the processor is configured to read out the image data of each of a plurality of frames in parallel within an output period that is defined by the second frame rate as a period in which the image data of one frame is output, store, in parallel, each image data read out in parallel in the memory, generate output image data of one frame using the stored image data of each of the plurality of frames, and output the generated output image data at the second frame rate.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0006" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0039" num="0038">Exemplary embodiments of the technology of the disclosure will be described in detail based on the following figures, wherein:</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram illustrating an example of a hardware configuration of an imaging apparatus according to a first embodiment;</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>2</b>A</figref> is a schematic configuration diagram illustrating an example of a schematic configuration of an imaging element included in the imaging apparatus according to the first embodiment;</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>2</b>B</figref> is a diagram illustrating a plurality of storage regions of a memory;</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a block diagram illustrating an example of a main configuration of the imaging element included in the imaging apparatus according to the first embodiment;</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. <b>4</b>A</figref> is a conceptual diagram schematically illustrating an operation of exposure, reading, storage, and output of the imaging element;</p><p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. <b>4</b>B</figref> is a detailed descriptive diagram illustrating an example of an operation from exposure to resetting of the imaging element;</p><p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. <b>5</b>A</figref> is a schematic diagram for describing how a motion picture image is seen in the technology of the related art;</p><p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. <b>5</b>B</figref> is a schematic diagram for describing how the motion picture image is seen in the technology of the related art;</p><p id="p-0048" num="0047"><figref idref="DRAWINGS">FIG. <b>5</b>C</figref> is a schematic diagram for describing how the motion picture image is seen in the technology of the related art;</p><p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. <b>5</b>D</figref> is a schematic diagram for describing how the motion picture image is seen in the technology of the related art;</p><p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is an operation conceptual diagram illustrating an example of exposure, reading, storage, and output of the imaging element according to the first embodiment;</p><p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart illustrating an example of imaging processing according to the first embodiment;</p><p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a flowchart illustrating an example of exposure and reading processing according to the first embodiment;</p><p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a flowchart illustrating an example of output image generation processing according to the first embodiment;</p><p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. <b>10</b>A</figref> is a schematic diagram for describing how a motion picture image output from the imaging element according to the first embodiment is seen;</p><p id="p-0055" num="0054"><figref idref="DRAWINGS">FIG. <b>10</b>B</figref> is a schematic diagram for describing how the motion picture image output from the imaging element according to the first embodiment is seen;</p><p id="p-0056" num="0055"><figref idref="DRAWINGS">FIG. <b>10</b>C</figref> is a schematic diagram for describing how the motion picture image output from the imaging element according to the first embodiment is seen;</p><p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. <b>10</b>D</figref> is a schematic diagram for describing how the motion picture image output from the imaging element according to the first embodiment is seen;</p><p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. <b>11</b>A</figref> is a conceptual diagram of an operation of performing AD conversion on one piece of image data by an AD conversion column according to the first embodiment;</p><p id="p-0059" num="0058"><figref idref="DRAWINGS">FIG. <b>11</b>B</figref> is a conceptual diagram of an operation of performing AD conversion on two pieces of image data by the AD conversion column according to the first embodiment;</p><p id="p-0060" num="0059"><figref idref="DRAWINGS">FIG. <b>11</b>C</figref> is a conceptual diagram of an operation of performing AD conversion on three pieces of image data by the AD conversion column according to the first embodiment;</p><p id="p-0061" num="0060"><figref idref="DRAWINGS">FIG. <b>11</b>D</figref> is a conceptual diagram of an operation of performing AD conversion on four pieces of image data by the AD conversion column according to the first embodiment;</p><p id="p-0062" num="0061"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a schematic diagram illustrating an example of a problem in a case where an exposure time period of the imaging element is short;</p><p id="p-0063" num="0062"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a schematic diagram illustrating an example of AD conversion processing of an imaging element according to a second embodiment that resolves the problem illustrated in <figref idref="DRAWINGS">FIG. <b>12</b></figref>;</p><p id="p-0064" num="0063"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a flowchart illustrating an example of exposure and reading processing according to the second embodiment;</p><p id="p-0065" num="0064"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a flowchart illustrating an example of exposure and reading processing according to a third embodiment;</p><p id="p-0066" num="0065"><figref idref="DRAWINGS">FIG. <b>16</b></figref> is a schematic diagram illustrating an example of reading processing and output according to a fourth embodiment;</p><p id="p-0067" num="0066"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is a schematic diagram illustrating an example of reading processing and output according to a fifth embodiment;</p><p id="p-0068" num="0067"><figref idref="DRAWINGS">FIG. <b>18</b></figref> is a schematic diagram illustrating an example of reading processing and output according to a modification example of the fifth embodiment;</p><p id="p-0069" num="0068"><figref idref="DRAWINGS">FIG. <b>19</b></figref> is a flowchart illustrating an example of output image generation processing according to the fifth embodiment;</p><p id="p-0070" num="0069"><figref idref="DRAWINGS">FIG. <b>20</b></figref> is a conceptual diagram illustrating an example of an aspect of installing a program on the imaging element according to each embodiment from a storage medium storing the program; and</p><p id="p-0071" num="0070"><figref idref="DRAWINGS">FIG. <b>21</b></figref> is a block diagram illustrating an example of a schematic configuration of a smart device incorporating the imaging element according to each embodiment.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0007" level="1">DETAILED DESCRIPTION</heading><p id="p-0072" num="0071">Hereinafter, an example of embodiments of the technology of the present disclosure will be described with reference to the drawings.</p><p id="p-0073" num="0072">First, terms used in the following description will be described. In the following description, the abbreviation &#x201c;AD&#x201d; stands for &#x201c;Analog-to-Digital&#x201d;. The abbreviation &#x201c;OVF&#x201d; stands for &#x201c;Optical View Finder&#x201d;. The abbreviation &#x201c;EVF&#x201d; stands for &#x201c;Electronic View Finder&#x201d;. The abbreviation &#x201c;AE&#x201d; stands for &#x201c;Auto Exposure&#x201d;. The abbreviation &#x201c;CMOS&#x201d; stands for &#x201c;Complementary Metal Oxide Semiconductor&#x201d;. The abbreviation &#x201c;CCD&#x201d; stands for &#x201c;Charge Coupled Device&#x201d;. The abbreviation &#x201c;LSI&#x201d; stands for &#x201c;Large-Scale Integration&#x201d;. The abbreviation &#x201c;CPU&#x201d; stands for &#x201c;Central Processing Unit&#x201d;. The abbreviation &#x201c;ROM&#x201d; stands for &#x201c;Read Only Memory&#x201d;. The abbreviation &#x201c;RAM&#x201d; stands for &#x201c;Random Access Memory&#x201d;. The abbreviation &#x201c;I/F&#x201d; stands for &#x201c;Interface&#x201d;. The abbreviation &#x201c;ASIC&#x201d; stands for &#x201c;Application Specific Integrated Circuit&#x201d;. The abbreviation &#x201c;PLD&#x201d; stands for &#x201c;Programmable Logic Device&#x201d;. The abbreviation &#x201c;FPGA&#x201d; stands for &#x201c;Field Programmable Gate Array&#x201d;. The abbreviation &#x201c;SSD&#x201d; stands for &#x201c;Solid State Drive&#x201d;. The abbreviation &#x201c;USB&#x201d; stands for &#x201c;Universal Serial Bus&#x201d;. The abbreviation &#x201c;CD-ROM&#x201d; stands for &#x201c;Compact Disc Read Only Memory&#x201d;. The abbreviation &#x201c;IC&#x201d; stands for &#x201c;Integrated Circuit&#x201d;. The abbreviation &#x201c;HDD&#x201d; stands for &#x201c;Hard Disc Drive&#x201d;. The abbreviation &#x201c;DRAM&#x201d; stands for &#x201c;Dynamic Random Access Memory&#x201d;. The abbreviation &#x201c;SRAM&#x201d; stands for &#x201c;Static Random Access Memory&#x201d;. The abbreviation &#x201c;PC&#x201d; stands for &#x201c;Personal Computer&#x201d;. The abbreviation &#x201c;fps&#x201d; stands for &#x201c;frame per second&#x201d;.</p><heading id="h-0008" level="1">First Embodiment</heading><p id="p-0074" num="0073">Hereinafter, an example of embodiments of an imaging apparatus according to the embodiments of the technology of the present disclosure will be described in accordance with the appended drawings.</p><p id="p-0075" num="0074">As illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref> as an example, an imaging apparatus <b>10</b> is an interchangeable lens camera. The imaging apparatus <b>10</b> includes an imaging apparatus main body <b>12</b> and an interchangeable lens <b>14</b> that is interchangeably mounted on the imaging apparatus main body <b>12</b>. The interchangeable lens <b>14</b> includes an imaging lens <b>18</b> including a focus lens <b>16</b> that is movable in a direction of an optical axis L<b>1</b> by a manual operation.</p><p id="p-0076" num="0075">A hybrid finder (registered trademark) <b>21</b> is disposed in the imaging apparatus main body <b>12</b>. For example, the hybrid finder <b>21</b> here refers to a finder in which an optical viewfinder (hereinafter, referred to as the OVF) and an electronic viewfinder (hereinafter, referred to as the EVF) are selectively used. The EVF includes a second display <b>80</b>.</p><p id="p-0077" num="0076">The interchangeable lens <b>14</b> is interchangeably mounted on the imaging apparatus main body <b>12</b>. A focus ring <b>22</b> that is used in a case of a manual focus mode is disposed in a lens barrel of the interchangeable lens <b>14</b>. The focus lens <b>16</b> moves in the direction of the optical axis L<b>1</b> in accordance with a manual rotation operation of the focus ring <b>22</b>. The focus lens <b>16</b> is stopped at a focal position corresponding to a subject distance. In the present embodiment, the &#x201c;focal position&#x201d; refers to a position of the focus lens <b>16</b> on the optical axis L<b>1</b> in a focused state.</p><p id="p-0078" num="0077">A touch panel display <b>30</b> is disposed on a rear surface of the imaging apparatus main body <b>12</b>. The touch panel display <b>30</b> comprises a liquid crystal display (hereinafter, referred to as a &#x201c;first display&#x201d;) <b>40</b> and a touch panel <b>42</b>. The first display <b>40</b> or the second display <b>80</b> is an example of a &#x201c;display portion&#x201d; according to the embodiments of the technology of the present disclosure.</p><p id="p-0079" num="0078">The first display <b>40</b> displays images, text information, and the like. The first display <b>40</b> is used for displaying the live view image (live preview image) that is an example of a consecutive frame image obtained by imaging in consecutive frames in a case of the imaging mode. In addition, the first display <b>40</b> is used for displaying a still picture image that is an example of a single frame image obtained by imaging in a single frame in a case where a still picture imaging instruction is provided. Furthermore, the first display <b>40</b> is used for displaying a playback image and displaying a menu screen and the like in the playback mode.</p><p id="p-0080" num="0079">The touch panel <b>42</b> is a transmissive touch panel and is overlaid on a surface of a display region of the first display <b>40</b>. The touch panel <b>42</b> detects a contact of an instruction object such as a finger or a stylus pen.</p><p id="p-0081" num="0080">The imaging apparatus <b>10</b> includes a mount <b>44</b> comprised in the imaging apparatus main body <b>12</b>, and a mount <b>46</b> on an interchangeable lens <b>14</b> side corresponding to the mount <b>44</b>. The interchangeable lens <b>14</b> is interchangeably mounted on the imaging apparatus main body <b>12</b> by joining the mount <b>46</b> to the mount <b>44</b>.</p><p id="p-0082" num="0081">The imaging lens <b>18</b> includes a stop <b>47</b>. The stop <b>47</b> is arranged closer to the imaging apparatus main body <b>12</b> side than the focus lens <b>16</b> and is connected to a motor <b>49</b>. The stop <b>47</b> operates by receiving motive power of the motor <b>49</b> and adjusts the exposure.</p><p id="p-0083" num="0082">The interchangeable lens <b>14</b> includes a sliding mechanism <b>48</b> and a motor <b>50</b>. The sliding mechanism <b>48</b> moves the focus lens <b>16</b> along the optical axis L<b>1</b> by operating the focus ring <b>22</b>. The focus lens <b>16</b> is attached to the sliding mechanism <b>48</b> in a slidable manner along the optical axis L<b>1</b>. The motor <b>50</b> is connected to the sliding mechanism <b>48</b>, and the sliding mechanism <b>48</b> slides the focus lens <b>16</b> along the optical axis L<b>1</b> by receiving motive power of the motor <b>50</b>.</p><p id="p-0084" num="0083">The motors <b>49</b> and <b>50</b> are connected to the imaging apparatus main body <b>12</b> through the mounts <b>44</b> and <b>46</b>, and driving thereof is controlled in accordance with a command from the imaging apparatus main body <b>12</b>. In the present embodiment, stepping motors are applied as an example of the motors <b>49</b> and <b>50</b>. Accordingly, the motors <b>49</b> and <b>50</b> operate in synchronization with pulse electric power in accordance with the command from the imaging apparatus main body <b>12</b>.</p><p id="p-0085" num="0084">The imaging apparatus <b>10</b> is a digital camera that records a still picture image and a motion picture image obtained by imaging a subject. The imaging apparatus main body <b>12</b> comprises an operation portion <b>54</b>, an external interface (I/F) <b>63</b>, and a rear stage circuit <b>90</b>. The rear stage circuit <b>90</b> is a circuit on a side of receiving data transmitted from the imaging element <b>20</b>. In the present embodiment, an IC is employed as the rear stage circuit <b>90</b>. An LSI is illustrated as an example of the IC.</p><p id="p-0086" num="0085">The rear stage circuit <b>90</b> includes a CPU <b>52</b>, an I/F <b>56</b>, a primary storage portion <b>58</b>, a secondary storage portion <b>60</b>, an image processing portion <b>62</b>, a first display control portion <b>64</b>, a second display control portion <b>66</b>, a position detection portion <b>70</b>, and a device control portion <b>74</b>. A single CPU is illustrated as the CPU <b>52</b> in the present embodiment. However, the technology of the present disclosure is not limited thereto, and a plurality of CPUs may be employed instead of the CPU <b>52</b>. The CPU <b>52</b> is an example of a &#x201c;control portion (control processor)&#x201d; according to the embodiments of the technology of the present disclosure.</p><p id="p-0087" num="0086">In the present embodiment, each of the image processing portion <b>62</b>, the first display control portion <b>64</b>, the second display control portion <b>66</b>, the position detection portion <b>70</b>, and the device control portion <b>74</b> is implemented by an ASIC. However, the technology of the present disclosure is not limited thereto. For example, at least one of a PLD or an FPGA may be employed instead of the ASIC. In addition, at least two of the ASIC, the PLD, or the FPGA may be employed. In addition, a computer including a CPU, a ROM, and a RAM may be employed. The number of CPUs may be singular or plural. In addition, at least one of the image processing portion <b>62</b>, the first display control portion <b>64</b>, the second display control portion <b>66</b>, the position detection portion <b>70</b>, or the device control portion <b>74</b> may be implemented by a combination of a hardware configuration and a software configuration.</p><p id="p-0088" num="0087">The CPU <b>52</b>, the I/F <b>56</b>, the primary storage portion <b>58</b>, the secondary storage portion <b>60</b>, the image processing portion <b>62</b>, the first display control portion <b>64</b>, the second display control portion <b>66</b>, the operation portion <b>54</b>, the external I/F <b>63</b>, and the touch panel <b>42</b> are connected to each other through a bus <b>68</b>.</p><p id="p-0089" num="0088">The CPU <b>52</b> controls the entire imaging apparatus <b>10</b>. In the imaging apparatus <b>10</b> according to the present embodiment, in a case of the auto-focus mode, the CPU <b>52</b> performs the focusing control by controlling driving of the motor <b>50</b> such that a contrast value of the image obtained by imaging is maximized. In addition, in a case of the auto-focus mode, the CPU <b>52</b> calculates AE information that is a physical quantity indicating brightness of the image obtained by imaging. In the imaging mode, the CPU <b>52</b> derives an exposure time period (shutter speed) and an F number corresponding to the brightness of the image indicated by the AE information. An exposure state is set by controlling each related portion to achieve the derived exposure time period and the F number.</p><p id="p-0090" num="0089">The primary storage portion <b>58</b> means a volatile memory and refers to, for example, a RAM. The secondary storage portion <b>60</b> means a non-volatile memory and refers to, for example, a flash memory or an HDD.</p><p id="p-0091" num="0090">The secondary storage portion <b>60</b> stores an imaging program <b>60</b>A. The CPU <b>52</b> reads out the imaging program <b>60</b>A from the secondary storage portion <b>60</b> and loads the read imaging program <b>60</b>A into the primary storage portion <b>58</b>. The CPU <b>52</b> executes processing from imaging to displaying in accordance with the imaging program <b>60</b>A loaded in the primary storage portion <b>58</b>.</p><p id="p-0092" num="0091">The operation portion <b>54</b> is a user interface that is operated by the user in a case of issuing various instructions to the rear stage circuit <b>90</b>. The various instructions received by the operation portion <b>54</b> are output to the CPU <b>52</b> as an operation signal. The CPU <b>52</b> executes processing corresponding to the operation signal input from the operation portion <b>54</b>.</p><p id="p-0093" num="0092">The position detection portion <b>70</b> is connected to the CPU <b>52</b>. The position detection portion <b>70</b> is connected to the focus ring <b>22</b> through the mounts <b>44</b> and <b>46</b>, detects a rotation angle of the focus ring <b>22</b>, and outputs rotation angle information indicating the rotation angle that is a detection result, to the CPU <b>52</b>. The CPU <b>52</b> executes processing corresponding to the rotation angle information input from the position detection portion <b>70</b>.</p><p id="p-0094" num="0093">In a case where the imaging mode is set, an image of subject light is formed on a light receiving surface of the color imaging element <b>20</b> through the imaging lens <b>18</b> including the focus lens <b>16</b>, and a mechanical shutter <b>72</b>.</p><p id="p-0095" num="0094">The device control portion <b>74</b> is connected to the CPU <b>52</b>. In addition, the device control portion <b>74</b> is connected to the imaging element <b>20</b> and the mechanical shutter <b>72</b>. Furthermore, the device control portion <b>74</b> is connected to the motors <b>49</b> and <b>50</b> of the imaging lens <b>18</b> through the mounts <b>44</b> and <b>46</b>. The device control portion <b>74</b> controls the imaging element <b>20</b>, the mechanical shutter <b>72</b>, and the motors <b>49</b> and <b>50</b> under control of the CPU <b>52</b>.</p><p id="p-0096" num="0095">The imaging element <b>20</b> is an example of a &#x201c;laminated imaging element&#x201d; according to the embodiments of the technology of the present disclosure. For example, the imaging element <b>20</b> is a CMOS image sensor. As illustrated in <figref idref="DRAWINGS">FIG. <b>2</b>A</figref> as an example, the imaging element <b>20</b> incorporates a photoelectric conversion element <b>92</b>, a processing circuit <b>94</b>, and a memory <b>96</b>. It is preferable that at least the photoelectric conversion element <b>92</b> and the memory <b>96</b> are formed in one chip in the imaging element <b>20</b>. By forming at least the photoelectric conversion element <b>92</b> and the memory <b>96</b> in one chip, portability of the imaging element can be increased, compared to an imaging element in which the photoelectric conversion element <b>92</b> and the memory <b>96</b> are not formed in one chip. <figref idref="DRAWINGS">FIG. <b>2</b>A</figref> illustrates the imaging element <b>20</b> in which the photoelectric conversion element <b>92</b>, the processing circuit <b>94</b>, and the memory <b>96</b> are formed in one chip by lamination. Specifically, the photoelectric conversion element <b>92</b> and the processing circuit <b>94</b> are electrically connected to each other by a bump (not illustrated) of copper or the like having conductivity. The processing circuit <b>94</b> and the memory <b>96</b> are also electrically connected to each other by a bump (not illustrated) of copper or the like having conductivity.</p><p id="p-0097" num="0096">The imaging element <b>20</b> is a laminated imaging element in which the photoelectric conversion element <b>92</b> and the memory <b>96</b> are laminated. By laminating the photoelectric conversion element <b>92</b> with the memory <b>96</b>, a load exerted on processing between the photoelectric conversion element <b>92</b> and the memory <b>96</b> is reduced, compared to an imaging element in which the photoelectric conversion element <b>92</b> and the memory <b>96</b> are not laminated.</p><p id="p-0098" num="0097">As illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref> as an example, the processing circuit <b>94</b> is an example of a &#x201c;processor&#x201d; according to the embodiments of the technology of the present disclosure and includes a photoelectric conversion element driving circuit <b>94</b>A, an AD conversion circuit <b>94</b>B, an image processing circuit <b>94</b>C, and an output circuit <b>94</b>D. The processing circuit <b>94</b> operates under control of the CPU <b>52</b> through the device control portion <b>74</b>.</p><p id="p-0099" num="0098">The photoelectric conversion element driving circuit <b>94</b>A is connected to the photoelectric conversion element <b>92</b> and the AD conversion circuit <b>94</b>B. The memory <b>96</b> is connected to the AD conversion circuit <b>94</b>B and the image processing circuit <b>94</b>C. The image processing circuit <b>94</b>C is connected to the output circuit <b>94</b>D. The output circuit <b>94</b>D is connected to the I/F <b>56</b> of the rear stage circuit <b>90</b>.</p><p id="p-0100" num="0099">The photoelectric conversion element driving circuit <b>94</b>A controls the photoelectric conversion element <b>92</b> and reads out captured image data from the photoelectric conversion element <b>92</b> under control of the CPU <b>52</b>. The &#x201c;captured image data&#x201d; here refers to analog image data indicating the subject. The AD conversion circuit <b>94</b>B digitizes, by AD conversion, the analog image data read out by the photoelectric conversion element driving circuit <b>94</b>A and stores the digitized image data in the memory <b>96</b>. The memory <b>96</b> can store image data of a plurality of frames. The image processing circuit <b>94</b>C processes the image data stored in the memory <b>96</b>. The photoelectric conversion element driving circuit <b>94</b>A is an example of a &#x201c;reading portion&#x201d; according to the embodiments of the technology of the present disclosure. The memory <b>96</b> is an example of a &#x201c;storage portion&#x201d; according to the embodiments of the technology of the present disclosure. The image processing circuit <b>94</b>C is an example of a &#x201c;processing portion&#x201d; according to the embodiments of the technology of the present disclosure. The output circuit <b>94</b>D is an example of an &#x201c;output portion&#x201d; according to the embodiments of the technology of the present disclosure.</p><p id="p-0101" num="0100">The processing circuit <b>94</b> is, for example, an LSI, and the memory <b>96</b> is, for example, a RAM. A DRAM is employed as an example of the memory <b>96</b> in the present embodiment. However, the technology of the present disclosure is not limited thereto, and an SRAM may be used.</p><p id="p-0102" num="0101">In the present embodiment, the processing circuit <b>94</b> is implemented by an ASIC. However, the technology of the present disclosure is not limited thereto. For example, at least one of a PLD or an FPGA may be employed instead of the ASIC. In addition, at least two of the ASIC, the PLD, or the FPGA may be employed. In addition, a computer including a CPU, a ROM, and a RAM may be employed. The number of CPUs may be singular or plural. In addition, the processing circuit <b>94</b> may be implemented by a combination of a hardware configuration and a software configuration.</p><p id="p-0103" num="0102">The photoelectric conversion element <b>92</b> includes a plurality of photosensors (hereinafter, referred to as pixels) arranged in a matrix form. In the present embodiment, photodiodes are employed as an example of the photosensors. In addition, photodiodes of &#x201c;4896 columns&#xd7;3265 rows&#x201d; pixels are illustrated as an example of the plurality of photosensors. Hereinafter, a row of pixels may be referred to as a line.</p><p id="p-0104" num="0103">The photoelectric conversion element <b>92</b> comprises color filters. The color filters include a G filter corresponding to green (G) that most contributes to obtaining a brightness signal, an R filter corresponding to red (R), and a B filter corresponding to blue (B). In the present embodiment, the G filter, the R filter, and the B filter are arranged with predetermined periodicity in each of a row direction (horizontal direction) and a column direction (vertical direction) for the plurality of photodiodes of the photoelectric conversion element <b>92</b>. Thus, the imaging apparatus <b>10</b> can perform processing in accordance with a repeating pattern in a case of performing demosaicing and the like on R, G, and B signals. The demosaicing refers to processing of calculating every color information for each pixel from a mosaic image corresponding to color filter arrangement of a single plate color imaging element. For example, in a case of an imaging element consisting of color filters of three colors of R, G, and B, the demosaicing means processing of calculating color information about all of R, G, and B for each pixel from a mosaic image consisting of R, G, and B.</p><p id="p-0105" num="0104">While the CMOS image sensor is illustrated here as the imaging element <b>20</b>, the technology of the present disclosure is not limited thereto. For example, the technology of the present disclosure is also established in a case where the photoelectric conversion element <b>92</b> is a CCD image sensor.</p><p id="p-0106" num="0105">The imaging element <b>20</b> has a so-called electronic shutter function and controls an electric charge accumulation time period of each photodiode in the photoelectric conversion element <b>92</b> by performing the electronic shutter function under control of the device control portion <b>74</b>. The electric charge accumulation time period refers to a so-called exposure time period.</p><p id="p-0107" num="0106">In the imaging apparatus <b>10</b>, imaging for the still picture image and imaging for the motion picture image such as the live view image are performed using a rolling shutter method. The imaging for the still picture image is implemented by performing the electronic shutter function and operating the mechanical shutter <b>72</b>. The imaging for the motion picture image is implemented by performing the electronic shutter function without operating the mechanical shutter <b>72</b>.</p><p id="p-0108" num="0107">The processing circuit <b>94</b> is controlled by the CPU <b>52</b> through the device control portion <b>74</b>. The processing circuit <b>94</b> reads out the analog image data of each frame obtained by imaging the subject by the photoelectric conversion element <b>92</b>. The analog image data is data based on signal electric charges accumulated in the photoelectric conversion element <b>92</b>. The processing circuit <b>94</b> performs the AD conversion on the analog image data read out from the photoelectric conversion element <b>92</b>. The processing circuit <b>94</b> stores digital image data obtained by performing the AD conversion on the analog image data in the memory <b>96</b>. The processing circuit <b>94</b> acquires the digital image data from the memory <b>96</b>, processes the acquired image data, and outputs the image data to the I/F <b>56</b> of the rear stage circuit <b>90</b> as output image data. Hereinafter, the analog or digital image data will be simply referred to as the &#x201c;image data&#x201d;.</p><p id="p-0109" num="0108">A first frame rate is a frame rate related to a time period from a start of exposure in the photoelectric conversion element <b>92</b>, then reading of the image data of one frame captured by the exposure from the photoelectric conversion element <b>92</b>, performing of the AD conversion on the read image data, and storage of the image data subjected to the AD conversion in the memory <b>96</b>. A second frame rate is a frame rate related to a time period required for outputting the output image data of one frame to an outside of the imaging element <b>20</b>. For example, the &#x201c;outside of the imaging element <b>20</b>&#x201d; here refers to the I/F <b>56</b> of the rear stage circuit <b>90</b>. The first frame rate is a frame rate higher than the second frame rate.</p><p id="p-0110" num="0109">In the first embodiment, 60 frames per second (fps) is employed as an example of the second frame rate. However, the technology of the present disclosure is not limited thereto. The second frame rate can be changed as long as a relationship &#x201c;second frame rate <b>21</b> first frame rate&#x201d; is satisfied. In addition, the first frame rate can be changed within a range of not less than or equal to the second frame rate. Hereinafter, an output period of one frame output at the second frame rate will be simply referred to as the &#x201c;output period&#x201d;. In a case where the second frame rate is 60 fps, the output period is 1/60 second (16.667 milliseconds).</p><p id="p-0111" num="0110">The image data of one frame is obtained by performing exposure of the imaging element <b>20</b> once. In the first embodiment, the exposure and reading processing for the image data are performed for each line using the rolling shutter method. In a case where the exposure of one line is finished, reading of electric charges of the line, the AD conversion, storage of the image data subjected to the AD conversion in the memory <b>96</b>, and resetting are executed. The reading to the resetting will be referred to as the reading processing for the image data.</p><p id="p-0112" num="0111">Here, a problem in a case where the first frame rate and the second frame rate are the same will be described before describing a detailed operation of the imaging element <b>20</b> according to the first embodiment.</p><p id="p-0113" num="0112">As illustrated in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref> as an example, a case of performing the reading processing and the output in a case where the first frame rate and the second frame rate are the same 60 fps will be described. A horizontal axis in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref> denotes time. A case where brightness of the subject is gradually increased from a first frame to a fourth frame, and where the exposure time period is gradually decreased in an order of T<b>1</b>, T<b>2</b>, T<b>3</b>, and T<b>4</b> in order to suppress overexposure is assumed. For example, an exposure time period T<b>1</b> of a first frame is 1/60 second. An exposure time period T<b>2</b> of a second frame is shorter than T<b>1</b>. An exposure time period T<b>3</b> of a third frame is shorter than T<b>2</b>, and an exposure time period T<b>4</b> of a fourth frame is shorter than T<b>3</b>.</p><p id="p-0114" num="0113">In <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, a start time of the exposure of a first line to an N-th line of the photoelectric conversion element <b>92</b>, that is, resetting for the exposure, is illustrated by one diagonal line. However, specifically, as illustrated in <figref idref="DRAWINGS">FIG. <b>4</b>B</figref>, an operation of the exposure after the resetting, the reading, the AD conversion, the storage in the memory, and the resetting is performed for each line. The storage of the image data in the memory <b>96</b> is performed after the AD conversion is finished, and thus, does not temporally overlap with the exposure time period. However, for description, the storage is illustrated by a bold diagonal line in the same period as the diagonal line illustrating the start time of the exposure. Furthermore, the output of the stored image data is illustrated by a bold diagonal line in the same period. In the following description, the operation illustrated in <figref idref="DRAWINGS">FIG. <b>4</b>B</figref> will be represented as illustrated in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref> for simplification.</p><p id="p-0115" num="0114">As illustrated in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, in a case where the exposure time period is T<b>1</b>, first, the first line is exposed for the time period T<b>1</b>, read out and subjected to the AD conversion after the exposure, and stored in the memory <b>96</b> as the image data of one line. The second line is exposed for the time period T<b>1</b> later than the first line, read out and subjected to the AD conversion after the exposure, and stored in the memory <b>96</b> as the image data of one line. This reading processing is sequentially performed to the last N-th line, and the image data of all lines is stored in the memory <b>96</b> as the image data of the first frame. This operation is repeated in each frame.</p><p id="p-0116" num="0115">The first frame rate does not change even in a case where the exposure time period is decreased. Thus, for example, in the first line, after the reading and the resetting are performed after the exposure of the exposure time period T<b>2</b> for the second frame, a waiting time period TA occurs until the exposure for the third frame is started. Similarly, in the first line, after the reading and the resetting are performed after the exposure of the exposure time period T<b>3</b> for the third frame, a waiting time period of a time period TB occurs until the exposure for the fourth frame is started. In addition, in the first line, after the reading and the resetting are performed after the exposure of the exposure time period T<b>4</b> for the fourth frame, a waiting time period of a time period TC occurs until the exposure for a fifth frame is started.</p><p id="p-0117" num="0116">As the exposure time period is decreased, a waiting time period until the exposure of the subsequent frame is started is increased. The waiting time period occurs in common from the first line to the last N-th line. Thus, in a case of imaging a moving subject, the subject moves during the waiting time period. Thus, in a case where the second frame to the fourth frame are illustrated for each frame, for example, the image is intermittent as illustrated in <figref idref="DRAWINGS">FIG. <b>5</b>A</figref> to <figref idref="DRAWINGS">FIG. <b>5</b>C</figref>. In a case where the image is output as the motion picture image, a motion feels awkward such that the subject discontinuously moves as illustrated in <figref idref="DRAWINGS">FIG. <b>5</b>D</figref>.</p><p id="p-0118" num="0117">In order to display a smooth motion in the motion picture image, setting the exposure time period of one frame close to the output period is considered. Setting the exposure time period of one frame close to the output period will be referred to as widening a time period opening. By widening the time period opening, the waiting time period is decreased. Thus, the motion of the subject in one frame image is continuous with the motion of the subject in the subsequent frame, and the subject is seen as moving continuously and smoothly. However, in order to suppress the overexposure, the exposure time period is set to be decreased as the subject becomes bright. Thus, it may be difficult to widen the time period opening.</p><p id="p-0119" num="0118">An operation of the imaging element <b>20</b> according to the first embodiment that resolves the above problem will be described. In the first embodiment, the first frame rate is changed to a frame rate higher than the second frame rate in connection with the exposure time period. Accordingly, a change in brightness of the subject can be handled. More specifically, the first frame rate is changed to be increased in accordance with a decrease in exposure time period. Accordingly, a relatively bright subject can be handled. However, the first frame rate is changed to a value higher than the second frame rate only in a case where the exposure time period is shorter than the output period. Changing the first frame rate to be higher than the second frame rate is performing the exposure a plurality of number of times, that is, more than once, within the output period.</p><p id="p-0120" num="0119">As illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref> as an example, a case where the subject becomes bright with an elapse of time is considered. A horizontal axis in <figref idref="DRAWINGS">FIG. <b>6</b></figref> denotes time. The second frame rate is set to 60 fps. In a case where the subject becomes gradually bright, the exposure time period of exposure E<b>1</b> to exposure E<b>6</b> is gradually decreased from T<b>1</b> to T<b>6</b>. However, in the present embodiment, unlike a method illustrated in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, the exposure and the reading processing are continuously performed without the waiting time period. By doing so, the image data of a plurality of frames may be read out in parallel in one output period. For example, in the output period of the third frame illustrated in the drawing, the image data of maximum three frames related to the exposure E<b>3</b>, E<b>4</b>, and E<b>5</b> are read out in parallel and stored. That is, the photoelectric conversion element driving circuit <b>94</b>A reads out, in parallel, the image data of each of the plurality of frames captured within the output period that is defined by the second frame rate as a period in which the image data of one frame is output.</p><p id="p-0121" num="0120">The exposure for imaging is restarted after the reading processing for the image data of one line is completed by the photoelectric conversion element driving circuit <b>94</b>A after the start of the previous exposure. Particularly, it is preferable to restart the exposure without the waiting time period after the reading processing for the image data based on the previous exposure is completed. Accordingly, it is possible to set a state close to constant exposure, and a time period of non-exposure between the previous exposure and the subsequent exposure can be relatively decreased regardless of the exposure time period. While the image data is read out one line at a line in the first embodiment, one pixel may be read out at a time.</p><p id="p-0122" num="0121">As illustrated in <figref idref="DRAWINGS">FIG. <b>2</b>B</figref>, the memory <b>96</b> includes a first region <b>96</b>A, a second region <b>96</b>B, a third region <b>96</b>C, . . . that are a plurality of storage regions individually storing each image data. Hereinafter, the &#x201c;storage region&#x201d; will be referred to as the &#x201c;region&#x201d;. As illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the image data obtained by the first exposure E<b>1</b> (exposure time period T<b>1</b>) is read out by the photoelectric conversion element driving circuit <b>94</b>A and subjected to the AD conversion by the AD conversion circuit <b>94</b>B. The image data subjected to the AD conversion is stored in the first region <b>96</b>A of the memory <b>96</b>. The stored image data is output as the output image data of the first frame by the output circuit <b>94</b>D and, for example, displayed on the first display <b>40</b>. Accordingly, the user can visually recognize an image based on a plurality of pieces of image data output by the output circuit <b>94</b>D.</p><p id="p-0123" num="0122">After the image data of the first line is read out, the image data is reset, and the reading processing is finished. In a case where the reading processing is finished, the second exposure E<b>2</b> (exposure time period T<b>2</b>) is started without the waiting time period. This will be referred to as continuous performing of the exposure and the reading processing. The above processing is performed for each line.</p><p id="p-0124" num="0123">The image data obtained by the exposure E<b>2</b> is read out, subjected to the AD conversion, and then, stored in the second region <b>96</b>B of the memory <b>96</b> different from the first region <b>96</b>A. After the image data obtained by the exposure E<b>2</b> is read out, the image data is reset, and the exposure E<b>3</b> is started. Reading of the image data obtained by the exposure E<b>3</b> temporally overlaps in the middle of reading in the exposure E<b>2</b> as illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. Thus, the image data obtained by the exposure E<b>3</b> is stored in the first region <b>96</b>A of the memory <b>96</b> different from the second region <b>96</b>B in which storage processing for the image data obtained by the exposure E<b>2</b> is being executed. That is, the memory <b>96</b> stores, in parallel, each image data that is read out in parallel by the photoelectric conversion element driving circuit <b>94</b>A.</p><p id="p-0125" num="0124">As described above, the exposure and the reading processing are continuously performed from the exposure E<b>1</b> to the exposure E<b>6</b>. The storage processing in the memory <b>96</b> is executed by selecting different regions among the first region <b>96</b>A, the second region <b>96</b>B, the third region <b>96</b>C, . . . set in the memory <b>96</b> such that the storage processing is executed in parallel. By continuously performing the exposure and the reading processing, a plurality of images, that is, more than one image, can be captured within one output period.</p><p id="p-0126" num="0125">The image data obtained by the exposure E<b>2</b> is output as the output image data for the second frame. The output image data for the third frame is image data in which the image data obtained by the exposure E<b>3</b> and the image data obtained by the exposure E<b>4</b> are combined. In addition, the output image data for the fourth frame is image data in which the image data obtained by the exposure E<b>5</b> and the image data obtained by the exposure E<b>6</b> are combined. The image processing circuit <b>94</b>C performs generation processing of generating the output image data of one frame by combining the image data of each of the plurality of frames stored in the memory <b>96</b>.</p><p id="p-0127" num="0126">The generation processing of combining the image data for the output image data can be performed using a well-known method. For example, the image processing circuit <b>94</b>C generates the image data of one frame obtained by calculating an arithmetic mean of at least a part of the image data of each of the plurality of frames stored in the memory <b>96</b> in units of pixels. For example, in a case where a noise is superimposed on one of pieces of image data for calculating the arithmetic mean, the noise is reduced by calculating the arithmetic mean. Thus, deterioration of image quality can be prevented. In addition, while simple addition increases pixel values and causes the overexposure, calculation of the arithmetic means can prevent the overexposure. The combining processing can be performed on common pixels of at least a part of the plurality of pieces of image data. The combined output image data is stored in the memory <b>96</b>. The output image data may be stored in a storage device other than the memory <b>96</b>.</p><p id="p-0128" num="0127">The output circuit <b>94</b>D outputs the output image data, which is generated by the image processing circuit <b>94</b>C and stored in the memory <b>96</b>, to the rear stage circuit <b>90</b> at the second frame rate. The CPU <b>52</b> stores the output image data, which is output, in the primary storage portion <b>58</b> and displays the output image data on the first display <b>40</b> by the first display control portion <b>64</b>.</p><p id="p-0129" num="0128">The above processing will be described using a flowchart. First, imaging processing performed by the CPU <b>52</b> of the rear stage circuit <b>90</b> will be described using <figref idref="DRAWINGS">FIG. <b>7</b></figref>. In step S<b>10</b>, the CPU <b>52</b> derives the exposure time period corresponding to the brightness of the image indicated by the AE information and stores the exposure time period in the primary storage portion <b>58</b>. Next, in step S<b>11</b>, the CPU <b>52</b> outputs the stored exposure time period to the device control portion <b>74</b>. Next, in step S<b>12</b>, the CPU <b>52</b> determines whether or not an output timing of a vertical synchronization signal corresponding to the second frame rate is reached. In step S<b>12</b>, in a case where a negative determination is made, a transition is made to step S<b>15</b>. In step S<b>12</b>, in a case where a positive determination is made, a transition is made to step S<b>13</b>, and the CPU <b>52</b> outputs the vertical synchronization signal to the device control portion <b>74</b>. Next, in step S<b>14</b>, the CPU <b>52</b> displays the output image data input from the imaging element <b>20</b> on the first display <b>40</b> by controlling the first display control portion <b>64</b>. Then, the imaging processing transitions to step S<b>15</b>.</p><p id="p-0130" num="0129">In step S<b>15</b>, the CPU <b>52</b> determines whether or not an imaging finish condition is established. In a case where a positive determination is made, an output indicating establishment of the imaging finish condition is provided to the device control portion <b>74</b>, and then, the imaging processing is finished. For example, a case where a positive determination is made is a case where the user issues an instruction to finish imaging from the operation portion <b>54</b>. In step S<b>15</b>, in a case where a negative determination is made, a return is made to step S<b>10</b>. The above processing is executed by executing the imaging program <b>60</b>A by the CPU <b>52</b>.</p><p id="p-0131" num="0130">Next, exposure and reading processing executed by the imaging element <b>20</b> under control of the CPU <b>52</b> will be described using <figref idref="DRAWINGS">FIG. <b>8</b></figref>. First, in step S<b>16</b>, the photoelectric conversion element driving circuit <b>94</b>A acquires the exposure time period input into the device control portion <b>74</b> from the CPU <b>52</b>. Next, in step S<b>17</b>, the photoelectric conversion element driving circuit <b>94</b>A controls the photoelectric conversion element <b>92</b> to perform the exposure for the acquired exposure time period. Next, in step S<b>18</b>, the photoelectric conversion element driving circuit <b>94</b>A reads out the image data obtained by the exposure. The AD conversion circuit <b>94</b>B performs AD conversion processing on the read image data and stores the image data subjected to the AD conversion in the memory <b>96</b>. Next, in step S<b>19</b>, the photoelectric conversion element driving circuit <b>94</b>A determines whether or not an exposure finish condition is established. For example, the exposure finish condition is established in a case where an input indicating establishment of the imaging finish condition is provided to the device control portion <b>74</b> from the CPU <b>52</b>. In a case where a positive determination is made, the photoelectric conversion element driving circuit <b>94</b>A and the AD conversion circuit <b>94</b>B finish the exposure and reading processing. In a case where a negative determination is made, a return is made to step S<b>16</b>.</p><p id="p-0132" num="0131">Next, output image generation processing executed by the imaging element <b>20</b> under control of the CPU <b>52</b> will be described using <figref idref="DRAWINGS">FIG. <b>9</b></figref>. In step S<b>20</b>, the image processing circuit <b>94</b>C determines whether or not the vertical synchronization signal is input into the device control portion <b>74</b> from the CPU <b>52</b>. In a case where a negative determination is made, step S<b>20</b> is repeated. In a case where a positive determination is made, a transition is made to step S<b>21</b>. The image processing circuit <b>94</b>C determines whether or not the number of pieces of outputtable image data currently stored in the memory <b>96</b> is plural. In a case where a plurality of pieces of outputtable image data are stored, a positive determination is made, and a transition is made to step S<b>22</b>. The image processing circuit <b>94</b>C generates one piece of output image data by combining the plurality of pieces of image data. The generated output image data is stored in the memory <b>96</b> or another storage portion. In step S<b>21</b>, in a case where a negative determination is made, a transition is made to step S<b>23</b>. The image processing circuit <b>94</b>C generates one piece of output image data from one piece of image data. The generated output image data is stored in the memory <b>96</b> or the other storage portion.</p><p id="p-0133" num="0132">Next, in step S<b>24</b>, the output circuit <b>94</b>D outputs the generated output image data to the I/F <b>56</b> of the rear stage circuit <b>90</b>. Next, in step S<b>25</b>, the image processing circuit <b>94</b>C determines whether or not an output image generation finish condition is established. For example, the output image generation finish condition is established in a case where an input indicating establishment of the imaging finish condition is provided to the device control portion <b>74</b> from the CPU <b>52</b>. In step S<b>25</b>, in a case where a positive determination is made, the image processing circuit <b>94</b>C finishes the output image generation processing. In a case where a negative determination is made, a return is made to step S<b>20</b>.</p><p id="p-0134" num="0133">As described above, by generating the output image data of one frame using the plurality of pieces of image data, an image in which a trajectory (afterimage) of movement of the subject in one output period is captured can be obtained. For example, in the second frame illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the exposure time period is T<b>2</b>, and the subject imaged in the exposure time period T<b>2</b> is captured as illustrated in <figref idref="DRAWINGS">FIG. <b>10</b>A</figref>. In the third frame illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the images in the exposure E<b>3</b> and the exposure E<b>4</b> are combined. Thus, an image, illustrated in <figref idref="DRAWINGS">FIG. <b>10</b>B</figref>, in which the trajectory of movement of the subject in a total time period of the exposure time periods T<b>3</b> and T<b>4</b> is captured is obtained. Similarly, in the fourth frame, the images in the exposure E<b>5</b> and the exposure E<b>6</b> are combined. Thus, an image, illustrated in <figref idref="DRAWINGS">FIG. <b>10</b>C</figref>, in which the trajectory of movement of the subject in a total time period of the exposure time periods T<b>5</b> and T<b>6</b> is captured is obtained. In a case where these images are seen as a motion picture image, the subject is seen as smoothly moving as illustrated in <figref idref="DRAWINGS">FIG. <b>10</b>D</figref>.</p><p id="p-0135" num="0134">In a case where the image processing circuit <b>94</b>C combines the output image data, it is preferable to combine the output image data by adding up the image data such that the total exposure time period within one output frame is an exposure time period as close to the output period as possible. By doing so, the trajectory of movement of the subject within a time period corresponding to the output period can be combined in one output frame, and a more natural motion picture image can be obtained.</p><p id="p-0136" num="0135">The output image data obtained by the above processing is stored in the memory <b>96</b> or the other storage portion. In addition, the output image data is displayed on the first display <b>40</b> as the live view image.</p><p id="p-0137" num="0136">According to the imaging element <b>20</b> according to the first embodiment, the image data of which the total exposure time period is closer to the output period can be combined. Accordingly, a smooth motion picture image can be output, compared to a case of outputting an image captured in an exposure time period shorter than an exposure time period corresponding to an output frame rate.</p><heading id="h-0009" level="1">Second Embodiment</heading><p id="p-0138" num="0137">As the exposure time period is decreased, the number of times of exposure in one output period is increased. Thus, the number of pieces of image data read out in parallel in one output period is increased. Meanwhile, the number of AD conversion columns performing the AD conversion may be limited. In a case where the number of pieces of image data read out in parallel in one output period is greater than the number of pieces of image data on which the AD conversion can be performed at the same time, the AD conversion is delayed, and the image data cannot be read out in parallel. Therefore, in the present embodiment, the photoelectric conversion element driving circuit <b>94</b>A changes a reading speed of the image data in accordance with the number of pieces of image data read out in parallel. In other words, the reading speed of the image data is changed in accordance with the number of frames in which the image data is read out in parallel. The reading speed is a speed at which the reading processing is performed. Accordingly, the image data can be processed without delay.</p><p id="p-0139" num="0138">Specifically, in the present embodiment, the photoelectric conversion element driving circuit <b>94</b>A changes the reading speed of the image data in accordance with the number of frames in which the image data is read out in parallel, that is, the number of pieces of image data, and the number of AD conversion circuits, that is, the AD conversion columns, performing the AD conversion on the read image data.</p><p id="p-0140" num="0139">In a second embodiment, as illustrated in <figref idref="DRAWINGS">FIG. <b>11</b>A</figref>, the imaging element <b>20</b> includes total eight AD conversion columns including T<b>0</b>, T<b>1</b>, T<b>2</b>, T<b>3</b>, B<b>0</b>, B<b>1</b>, B<b>2</b>, and B<b>3</b> as the AD conversion circuit <b>94</b>B. The imaging element <b>20</b> executes the AD conversion processing on two lines in parallel using two AD conversion columns as one set at the same time. There are four sets of the AD conversion columns including the AD conversion columns T<b>0</b> and B<b>0</b>, the AD conversion columns T<b>1</b> and B<b>1</b>, the AD conversion columns T<b>2</b> and B<b>2</b>, and the AD conversion columns T<b>3</b> and B<b>3</b>. Hereinafter, the eight AD conversion columns T<b>0</b> to T<b>3</b> and B<b>0</b> to B<b>3</b> will be referred to as the AD conversion circuit <b>94</b>B unless otherwise necessary to distinguish therebetween.</p><p id="p-0141" num="0140">As illustrated in <figref idref="DRAWINGS">FIG. <b>11</b>A</figref>, in a case of performing the AD conversion on only image data of a first image, the AD conversion is performed on two lines in parallel using one set of the AD conversion columns T<b>0</b> and B<b>0</b> among the eight AD conversion columns. In addition, as illustrated in <figref idref="DRAWINGS">FIG. <b>11</b>B</figref>, in a case of performing the AD conversion on two pieces of image data of the first image and a second image, the AD conversion is performed on each of the two pieces of image data two lines at a time in parallel using two sets of the AD conversion columns T<b>0</b> and B<b>0</b> and the AD conversion columns T<b>1</b> and B<b>1</b>. Similarly, as illustrated in <figref idref="DRAWINGS">FIG. <b>11</b>C</figref>, in a case of performing the AD conversion on three pieces of image data of the first image to a third image, the AD conversion is performed on the three pieces of image data in parallel by three sets of the AD conversion columns by adding one set of the AD conversion columns T<b>2</b> and B<b>2</b> to the example illustrated in <figref idref="DRAWINGS">FIG. <b>11</b>B</figref>. Similarly, as illustrated in <figref idref="DRAWINGS">FIG. <b>11</b>D</figref>, in a case of performing the AD conversion on four pieces of image data of the first image to a fourth image, the AD conversion is performed on the four pieces of image data in parallel by further adding one set of the AD conversion columns T<b>3</b> and B<b>3</b>.</p><p id="p-0142" num="0141">By using the AD conversion columns as described above, the AD conversion processing can be performed on the image data of four frames, maximum eight lines, in parallel. In other words, the reading processing can be performed on the image data of four frames, maximum eight lines, in parallel.</p><p id="p-0143" num="0142">Here, as illustrated in <figref idref="DRAWINGS">FIG. <b>12</b></figref> as an example, a case where the exposure time period continuously changes to T<b>4</b>, T<b>5</b>, T<b>6</b>, T<b>7</b>, and T<b>8</b> that are shorter than a predetermined first threshold value is considered. The exposure time periods T<b>1</b> to T<b>3</b> are exposure time periods longer than or equal to the first threshold value. The first threshold value can be set as an exposure time period in which images of the number of frames on which the AD conversion processing can be performed in parallel are acquired in one output period.</p><p id="p-0144" num="0143">In this case, in a time period TX illustrated by a dotted line frame in <figref idref="DRAWINGS">FIG. <b>12</b></figref>, the reading processing for the image data of the exposure time periods T<b>4</b> to T<b>8</b> overlaps, and it is necessary to perform the reading processing for the image data of <b>10</b> lines, that is, five frames. However, as described above, the AD conversion columns are limited to the AD conversion processing for the image data of eight lines, that is, four frames. Thus, it is not possible to perform the AD conversion processing on the image data of five frames in parallel.</p><p id="p-0145" num="0144">Therefore, the reading processing is performed by decreasing the reading speed of the image data. By doing so, the reading processing is performed on the image data of five or more frames in parallel. Specifically, in a case described using <figref idref="DRAWINGS">FIG. <b>12</b></figref>, the image data for which the exposure is performed in the exposure time periods T<b>1</b> to T<b>3</b> is processed at a normal processing speed of the AD conversion. Meanwhile, the image data for which the exposure is performed in the exposure time periods T<b>4</b> to T<b>8</b> shorter than the predetermined first threshold value is processed by decreasing the processing speed of the AD conversion from the normal processing speed.</p><p id="p-0146" num="0145">Here, decreasing the processing speed of the AD conversion for the image data means that the AD conversion for each line of the image data of one frame is not continuously performed and is intermittently performed. The number of lines on which the four sets of the AD conversion columns can perform the AD conversion at the same time at a certain time is eight lines, and the number of pieces of image data on which the AD conversion processing can be performed in parallel is up to four. Therefore, for example, in a case of performing the AD conversion processing on the image data of five frames, the AD conversion processing is performed by allocating the four sets of the AD conversion columns in order for each line of the image data of five frames. Thus, image data on which the AD conversion processing is not performed occurs at a certain time. Consequently, the processing speed of the AD conversion for one piece of image data is apparently decreased.</p><p id="p-0147" num="0146">Specifically, as illustrated in <figref idref="DRAWINGS">FIG. <b>13</b></figref>, the image data of five frames on which the AD conversion processing is to be performed in parallel is denoted by first data to fifth data. In the first AD conversion processing <b>1</b>, the AD conversion is performed on a first line L<b>1</b> (hereinafter, the first line will be referred to as L<b>1</b>, and a second line will be referred to as L<b>2</b>) and L<b>2</b> of the first data to the fourth data. The AD conversion is not performed on the fifth data. In the subsequent AD conversion processing <b>2</b>, the AD conversion is performed on L<b>3</b> and L<b>4</b> of the second data to the fourth data and L<b>1</b> and L<b>2</b> of the fifth data. The AD conversion is not performed on the first data. In the subsequent AD conversion processing <b>3</b>, the AD conversion is performed on L<b>5</b> and L<b>6</b> of the third data and the fourth data and L<b>3</b> and L<b>4</b> of the fifth data and the first data. The AD conversion is not performed on the second data. In a case where such processing is performed, an AD conversion processing time period per piece of image data is apparently increased. Thus, a reading processing time period of each image data is also increased from a normal reading processing time period. However, the reading processing can be performed on the image data of five frames in parallel.</p><p id="p-0148" num="0147">The exposure and reading processing according to the second embodiment will be described using <figref idref="DRAWINGS">FIG. <b>14</b></figref>. Steps in which the same processing as <figref idref="DRAWINGS">FIG. <b>8</b></figref> described in the first embodiment is executed will be designated by the same step numbers in <figref idref="DRAWINGS">FIG. <b>14</b></figref> and will not be described. Step S<b>16</b> and step S<b>17</b> are the same as a flow described using <figref idref="DRAWINGS">FIG. <b>8</b></figref>. Next, in step S<b>32</b>, the AD conversion circuit <b>94</b>B determines whether or not the exposure time period is shorter than the predetermined first threshold value. In a case where a positive determination is made, a transition is made to step S<b>34</b>. In step S<b>34</b>, the AD conversion circuit <b>94</b>B performs the AD conversion processing by decreasing the processing speed of the AD conversion. Then, the exposure and reading processing transitions to step S<b>19</b>. Meanwhile, in step S<b>32</b>, in a case where a negative determination is made, a transition is made to step S<b>36</b>. The AD conversion circuit <b>94</b>B performs the AD conversion processing at the normal processing speed of the AD conversion. Then, the exposure and reading processing transitions to step S<b>19</b>. A flow from step S<b>19</b> is the same as described using <figref idref="DRAWINGS">FIG. <b>8</b></figref>.</p><p id="p-0149" num="0148">As described above, in a case of performing the reading processing for the number of pieces of image data exceeding the number of pieces of image data on which the AD conversion can be performed in parallel, the reading processing for the image data can be performed in parallel by decreasing the processing speed of the AD conversion for each image data from the normal processing speed of the AD conversion.</p><p id="p-0150" num="0149">The number of AD conversion columns T<b>0</b> to T<b>3</b> and B<b>0</b> to B<b>3</b> is for illustrative purposes and is not limited to eight. In a case where the number of AD conversion columns is increased, the AD conversion processing can be performed on the image data of more frames. However, since cost is increased, it is not preferable to excessively increase the number of AD conversion columns. Thus, in the present embodiment, in order to process the image data without delay, the reading speed of the image data is decreased by narrowing down the number of AD conversion columns to an appropriate number.</p><p id="p-0151" num="0150">According to the second embodiment described above, even in a case where the number of AD conversion columns is limited, the image data can be processed without delay.</p><heading id="h-0010" level="1">Third Embodiment</heading><p id="p-0152" num="0151">As described in the second embodiment, the number of AD conversion columns performing the AD conversion processing in the reading processing is limited. In the present embodiment, a data amount in a case of performing the AD conversion processing on the image data is changed in accordance with the number of frames in which the image data is read out in parallel, and the number of AD conversion columns performing the AD conversion on the read image data. Specifically, in a case where the number of frames in which the image data is read out in parallel is increased, a conversion bit accuracy that is a data amount in a case of performing the AD conversion processing on the image data is decreased. In a case where the conversion bit accuracy is decreased, a processing time period of the AD conversion processing can be decreased. Thus, the AD conversion can be performed on the image data of more frames within the same time period. The conversion bit accuracy is the number of bits of the image data processed in the AD conversion processing performed once.</p><p id="p-0153" num="0152">For example, in a case where the exposure time period is longer than a second threshold value (exposure time period&#x3e;second threshold value), the conversion bit accuracy is set to 14 bits. In addition, in a case where the exposure time period is shorter than or equal to the second threshold value and longer than a third threshold value (third threshold value&#x3c;exposure time period&#x2264;second threshold value), the conversion bit accuracy is set to 12 bits. Furthermore, in a case where the exposure time period is shorter than or equal to the third threshold value (exposure time period&#x2264;third threshold value), the conversion bit accuracy is set to 10 bits. For example, the second threshold value is a time period of &#x2153; of one output period. In addition, for example, the third threshold value is a time period of &#x2155; of one output period.</p><p id="p-0154" num="0153">As the exposure time period is decreased, the number of frames in which the image data is read out in parallel is increased. In the present embodiment, as described above, as the exposure time period is decreased, the conversion bit accuracy of the AD conversion processing is decreased. That is, the conversion bit accuracy of the AD conversion processing is changed in accordance with the number of frames in which the image data is read out in parallel. As the conversion bit accuracy is decreased, processing data is decreased. Thus, the processing time period of the AD conversion processing is decreased.</p><p id="p-0155" num="0154">A described numerical value of the conversion bit accuracy is for illustrative purposes, and a specific numerical value is not limited to the described numerical value. The number of frames in which it is necessary to perform the AD conversion processing on the image data in parallel based on the exposure time period depends on the number and processing performance of AD conversion columns and the second frame rate. In addition, a degree to which the processing time period of the AD conversion processing can be decreased by changing the conversion bit accuracy depends on the processing performance of the AD conversion columns. Thus, considering these numerical values, the conversion bit accuracy of the AD conversion processing and the second threshold value and the third threshold value for changing the conversion bit accuracy are appropriately set. In addition, the number of threshold values is not limited to two. For example, two conversion bit accuracies may be defined by one threshold value.</p><p id="p-0156" num="0155">The exposure and reading processing in which the conversion bit accuracy of the AD conversion is changed will be described using <figref idref="DRAWINGS">FIG. <b>15</b></figref>. Steps in which the same processing as <figref idref="DRAWINGS">FIG. <b>8</b></figref> described in the first embodiment is executed will be designated by the same step numbers in <figref idref="DRAWINGS">FIG. <b>15</b></figref> and will not be described. Step S<b>16</b> and step S<b>17</b> are the same as in <figref idref="DRAWINGS">FIG. <b>8</b></figref>. Next, in step S<b>52</b>, the AD conversion circuit <b>94</b>B determines whether or not the exposure time period is shorter than or equal to the predetermined second threshold value. In a case where the exposure time period is shorter than or equal to the second threshold value, a transition is made to step S<b>54</b>. Meanwhile, in a case where the exposure time period is longer than the second threshold value, a transition is made to step S<b>58</b>.</p><p id="p-0157" num="0156">In step S<b>58</b>, the AD conversion circuit <b>94</b>B performs the AD conversion processing on the image data read out by the photoelectric conversion element driving circuit <b>94</b>A with the conversion bit accuracy of <b>14</b> bits. The AD conversion circuit <b>94</b>B stores the image data subjected to the AD conversion in the memory <b>96</b>, and the exposure and reading processing transitions to step S<b>19</b>.</p><p id="p-0158" num="0157">In step S<b>54</b>, the AD conversion circuit <b>94</b>B determines whether or not the exposure time period is shorter than or equal to the predetermined third threshold value. In a case where the exposure time period is shorter than or equal to the third threshold value, a transition is made to step S<b>56</b>. In step S<b>56</b>, the AD conversion circuit <b>94</b>B performs the AD conversion processing on the image data read out by the photoelectric conversion element driving circuit <b>94</b>A with the conversion bit accuracy of <b>10</b> bits. The AD conversion circuit <b>94</b>B stores the image data subjected to the AD conversion in the memory <b>96</b>, and the exposure and reading processing transitions to step S<b>19</b>.</p><p id="p-0159" num="0158">Meanwhile, in step S<b>54</b>, in a case where the exposure time period is not shorter than or equal to the third threshold value, a transition is made to step S<b>60</b>. In step S<b>60</b>, the AD conversion circuit <b>94</b>B performs the AD conversion processing on the image data read out by the photoelectric conversion element driving circuit <b>94</b>A with the conversion bit accuracy of <b>12</b> bits. The AD conversion circuit <b>94</b>B stores the image data subjected to the AD conversion in the memory <b>96</b>, and the exposure and reading processing transitions to step S<b>19</b>. A flow from step S<b>19</b> is the same as described in the first embodiment using <figref idref="DRAWINGS">FIG. <b>8</b></figref>.</p><p id="p-0160" num="0159">According to the third embodiment described above, even in a case where the number of AD conversion columns is limited, the image data can be processed without delay. In addition, since the reading speed of the image data is not decreased from the normal reading speed unlike the second embodiment, there is no concern of an increase in rolling distortion.</p><heading id="h-0011" level="1">Fourth Embodiment</heading><p id="p-0161" num="0160">In a case where imaging is performed for a long time period, the stored image data is increased, and a storage capacity of the memory <b>96</b> is exhausted. In the present embodiment, the CPU <b>52</b> outputs the output image data and then, deletes image data used for combining the output image data, which is output, from the image data stored in the memory <b>96</b>. Accordingly, the memory <b>96</b> can be effectively used.</p><p id="p-0162" num="0161">As illustrated in <figref idref="DRAWINGS">FIG. <b>16</b></figref> as an example, the output image data output as the first frame is created based on image data D<b>1</b> that is obtained and stored by the exposure E<b>1</b>. Therefore, the image data D<b>1</b> is deleted after the first frame is output. In <figref idref="DRAWINGS">FIG. <b>16</b></figref>, deletion processing is illustrated by a frame with diagonal lines. Similarly, image data D<b>2</b> that is used for creating the output image data output as the second frame and is based on the exposure E<b>2</b> is deleted after the output image data of the second frame is output. The output image data of the third frame is obtained by combining image data D<b>3</b> and image data D<b>4</b> based on the exposure E<b>3</b> and the exposure E<b>4</b>. Therefore, the image data D<b>3</b> and the image data D<b>4</b> are deleted after the third frame is output. Similarly, image data D<b>5</b> and image data D<b>6</b> that are used for the output image data of the fourth frame are deleted after the fourth frame is output. The image data D<b>5</b> and the image data D<b>6</b> are respectively stored in the first region <b>96</b>A of the memory <b>96</b> from which the image data D<b>1</b> is deleted, and the second region <b>96</b>B of the memory <b>96</b> from which the image data D<b>2</b> is deleted. By deleting the image data from the memory <b>96</b> after output, the memory <b>96</b> can be effectively used.</p><heading id="h-0012" level="1">Fifth Embodiment</heading><p id="p-0163" num="0162">In a case where the exposure time period is decreased, and one piece of output image data is combined from the image data based on the exposure performed a plurality of number of times, a time lag may occur in an output image depending on a time at which the output image data is created by combining images. The time lag is a difference between an exposed time period and an output time period. In a case where combining is performed using the image data for which the exposure is performed at a time more away from a timing of output, the time lag of the output image is further increased.</p><p id="p-0164" num="0163">In the present embodiment, partial image data that is a part of image data is combined from the plurality of pieces of image data. Specifically, for example, a case where image data for which the exposure is performed earlier than the output period or the exposure is started earlier than the output period is output as the output image data is considered. In this case, the image processing circuit <b>94</b>C generates the output image data by combining the remaining image data of the output image data that is output so far in the middle of output of the output image data, with the partial image data of the image data for which the exposure is performed at a time closer to the output. For example, the image data for which the exposure is performed at a time closer to the output is image data that is being newly stored during the output. In addition, the partial image data to be combined is partial image data that corresponds in pixel position to the remaining image data of the output image data output so far. By using this method, the output image data of which a part is temporally new can be output.</p><p id="p-0165" num="0164">As illustrated in <figref idref="DRAWINGS">FIG. <b>17</b></figref> as an example, partial image data <b>4</b>A of the first line to an n-th line of the image data D<b>1</b> stored in the memory <b>96</b> based on the exposure E<b>1</b> is being output as the third frame. The image data D<b>2</b> that can be output in the third frame and is based on the exposure E<b>2</b> is being stored in the memory <b>96</b> at a time at which output of the partial image data <b>4</b>A is finished. Thus, the image processing circuit <b>94</b>C combines the image data D<b>2</b> with the image data D<b>1</b>. In a case where the number of lines is denoted by N, n is an integer greater than or equal to 2 and less than N.</p><p id="p-0166" num="0165">Specifically, the image processing circuit <b>94</b>C generates partial image data <b>4</b>D by combining partial image data <b>4</b>B of the (n+1)-th line to the N-th line (last line) of the image data D<b>1</b> based on the exposure E<b>1</b> with partial image data <b>4</b>C of the (n+1)-th line to the N-th line of the image data D<b>2</b> based on the exposure E<b>2</b>. The output circuit <b>94</b>D outputs the generated partial image data <b>4</b>D subsequently to the partial image data <b>4</b>A. For example, the combining is combining based on the arithmetic mean.</p><p id="p-0167" num="0166">As described above, by performing combining by combining the partial image data of common pixels between the image data D<b>1</b> and the temporally new image data D<b>2</b>, the output image data of which the part is temporally new can be output as the third frame.</p><heading id="h-0013" level="1">Modification Example</heading><p id="p-0168" num="0167">In the fifth embodiment, instead of combining the output image data with newly stored image data in the middle of the output, replacement with the image data stored during the output can be performed.</p><p id="p-0169" num="0168">As illustrated in <figref idref="DRAWINGS">FIG. <b>18</b></figref> as an example, the third frame is output by performing replacement with the partial image data <b>4</b>C of the image data D<b>2</b> based on the exposure E<b>2</b> after the partial image data <b>4</b>A of the image data D<b>1</b> based on the exposure E<b>1</b> is output. The partial image data <b>4</b>A and the partial image data <b>4</b>C are described above. Accordingly, the output image data of which the part is temporally new can be output as the third frame.</p><p id="p-0170" num="0169">Output image generation processing of the fifth embodiment and the modification example will be described using <figref idref="DRAWINGS">FIG. <b>19</b></figref>. Steps in which the same processing as <figref idref="DRAWINGS">FIG. <b>9</b></figref> described in the first embodiment is executed will be designated by the same step numbers in <figref idref="DRAWINGS">FIG. <b>19</b></figref> and will not be described. Steps S<b>20</b>, S<b>21</b>, S<b>22</b>, and S<b>23</b> are the same as a flow described using <figref idref="DRAWINGS">FIG. <b>9</b></figref>. Next, in step S<b>70</b>, the output circuit <b>94</b>D starts outputting the output image data. Next, in step S<b>72</b>, the image processing circuit <b>94</b>C determines whether or not new image data that can be output in the middle of the output is present. In a case where a positive determination is made, the output image generation processing transitions to step S<b>74</b>. The image processing circuit <b>94</b>C combines partial image data that is a part of the new image data, or newly generates partial image data for the remaining output by performing replacement with the partial image data. Next, in step S<b>76</b>, the output circuit <b>94</b>D outputs the generated remaining partial image data.</p><p id="p-0171" num="0170">Meanwhile, in step S<b>72</b>, in a case where a negative determination is made, the output image generation processing transitions to step S<b>76</b>, and the output circuit <b>94</b>D outputs the remaining output image data. After step S<b>76</b>, the output image generation processing transitions to step S<b>25</b>. A flow from step S<b>25</b> is the same as described using <figref idref="DRAWINGS">FIG. <b>9</b></figref>.</p><p id="p-0172" num="0171">According to the fifth embodiment described above, the output image data of which the part is temporally new can be output.</p><p id="p-0173" num="0172">For example, as illustrated in <figref idref="DRAWINGS">FIG. <b>20</b></figref>, a program <b>200</b> of various types causing a computer <b>20</b>A incorporated in the imaging element <b>20</b> to execute the imaging processing, the exposure and reading processing, and the output image generation processing is stored in a storage medium <b>210</b>. The computer <b>20</b>A comprises a CPU <b>20</b>A<b>1</b>, a ROM <b>20</b>A<b>2</b>, and a RAM <b>20</b>A<b>3</b>. The program <b>200</b> of the storage medium <b>210</b> is installed on the computer <b>20</b>A. The CPU <b>20</b>A<b>1</b> of the computer <b>20</b>A executes the imaging processing, the exposure and reading processing, the output image generation processing, and the like in accordance with the program <b>200</b>. A single CPU is illustrated as the CPU <b>20</b>A<b>1</b>. However, the technology of the present disclosure is not limited thereto, and a plurality of CPUs may be employed instead of the CPU <b>20</b>A<b>1</b>.</p><p id="p-0174" num="0173">Any portable storage medium such as an SSD or a USB memory is illustrated as an example of the storage medium <b>210</b>.</p><p id="p-0175" num="0174">In addition, the program <b>200</b> may be stored in a storage portion of another computer, a server apparatus, or the like connected to the computer <b>20</b>A through a communication network (not illustrated), and the program <b>200</b> may be downloaded in response to a request from the imaging apparatus <b>10</b> or the like. In this case, the downloaded program <b>200</b> is executed by the computer <b>20</b>A.</p><p id="p-0176" num="0175">In addition, the computer <b>20</b>A may be disposed on the outside of imaging element <b>20</b>. In this case, the computer <b>20</b>A may control the processing circuit <b>94</b> in accordance with the program <b>200</b>.</p><p id="p-0177" num="0176">Various processors illustrated below can be used as a hardware resource for executing various types of processing described in each of the embodiments. For example, as described above, a CPU that is a general-purpose processor functioning as the hardware resource for executing the various types of processing according to the embodiments of the technology of the present disclosure by executing software, that is, the program, is illustrated as a processor. In addition, a dedicated electric circuit such as an FPGA, a PLD, or an ASIC that is a processor having a circuit configuration dedicatedly designed to execute a specific type of processing is illustrated as a processor.</p><p id="p-0178" num="0177">The hardware resource for executing the various types of processing according to the embodiments of the technology of the present disclosure may be configured with one of those various processors or may be configured with a combination of two or more processors of the same type or different types (for example, a combination of a plurality of FPGAs or a combination of a CPU and an FPGA). In addition, the hardware resource for executing various types of processing according to the embodiments of the technology of the present disclosure may be one processor.</p><p id="p-0179" num="0178">As an example of a configuration with one processor, first, as represented by a computer such as a client and a server, a form in which one processor is configured with a combination of one or more CPUs and software, and in which this processor functions as the hardware resource for executing the various types of processing according to the embodiments of the technology of the present disclosure is available. Second, as represented by a system-on-a-chip (SoC) or the like, a form of using a processor that implements, by one IC chip, a function of the entire system including a plurality of hardware resources for executing the various types of processing according to the embodiments of the technology of the present disclosure is available. The various types of processing according to the embodiments of the technology of the present disclosure are implemented using one or more of the various processors as the hardware resource.</p><p id="p-0180" num="0179">Furthermore, as a hardware structure of those various processors, more specifically, an electric circuit in which circuit elements such as semiconductor elements are combined can be used.</p><p id="p-0181" num="0180">While an interchangeable lens camera is illustrated as the imaging apparatus <b>10</b> in each of the embodiments, the technology of the present disclosure is not limited thereto. For example, the technology of the present disclosure may be applied to a smart device <b>300</b> illustrated in <figref idref="DRAWINGS">FIG. <b>21</b></figref>. The smart device <b>300</b> illustrated in <figref idref="DRAWINGS">FIG. <b>21</b></figref> is an example of the imaging apparatus according to the embodiments of the technology of the present disclosure. The imaging element <b>20</b> described in each of the embodiments is mounted in the smart device <b>300</b>. Even with the smart device <b>300</b> configured in such a manner, the same actions and effects as the imaging apparatus <b>10</b> described in each of the embodiments are obtained. The technology of the present disclosure can be applied to not only the smart device <b>300</b> but also a PC or a wearable terminal apparatus.</p><p id="p-0182" num="0181">While the first display <b>40</b> and the second display <b>80</b> are illustrated as a display device in each of the embodiments, the technology of the present disclosure is not limited thereto. For example, a separate display that is retrofit into the imaging apparatus main body <b>12</b> may be used as the &#x201c;display portion (display)&#x201d; according to the embodiments of the technology of the present disclosure.</p><p id="p-0183" num="0182">In addition, the imaging processing, the exposure and reading processing, and the output image generation processing described in each of the embodiments are merely an example. Accordingly, unnecessary steps may be deleted, new steps may be added, or a processing order may be rearranged without departing from a gist of the present disclosure.</p><p id="p-0184" num="0183">All documents, patent applications, and technical standards disclosed in the present specification are incorporated in the present specification by reference to the same extent as in a case where each of the documents, patent applications, technical standards is specifically and individually indicated to be incorporated by reference.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An imaging element comprising:<claim-text>a processor that is configured to read out, at a first frame rate, image data of each frame obtained by imaging a subject, process the image data, and output the processed image data at a second frame rate; and</claim-text><claim-text>a memory that stores the image data read out by the processor,</claim-text><claim-text>wherein the first frame rate is a frame rate higher than the second frame rate,</claim-text><claim-text>the processor is configured to read out the image data of a plurality of frames in parallel within an output period that is defined by the second frame rate as a period in which the image data of one frame is output,</claim-text><claim-text>the memory stores, in parallel, the image data read out in parallel by the processor, and</claim-text><claim-text>the processor is configured to perform generation processing of generating output image data of one frame using the image data of the plurality of frames stored in the memory and output the output image data generated by the generation processing at the second frame rate.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The imaging element according to <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein the first frame rate is changed in connection with an exposure time period.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The imaging element according to <claim-ref idref="CLM-00002">claim 2</claim-ref>,<claim-text>wherein the first frame rate is increased as the exposure time period is decreased.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The imaging element according to <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein after a start of exposure, the exposure for imaging is restarted after reading processing for the image data of at least one pixel by the processor is completed.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The imaging element according to <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein the processor is configured to change a reading speed of the image data in accordance with the number of frames in which the image data is read out in parallel.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The imaging element according to <claim-ref idref="CLM-00005">claim 5</claim-ref>,<claim-text>wherein the processor is configured to change the reading speed of the image data in accordance with the number of frames in which the image data is read out in parallel, and a number of AD conversion circuits performing AD conversion on the read image data.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The imaging element according to <claim-ref idref="CLM-00005">claim 5</claim-ref>,<claim-text>wherein the processor is configured to change a data amount in a case of performing AD conversion processing on the image data, in accordance with the number of frames in which the image data is read out in parallel, and the number of AD conversion circuits performing AD conversion on the read image data.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The imaging element according to <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein the memory includes a plurality of storage regions individually storing the plurality of pieces of image data.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The imaging element according to <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein the generation processing is processing of generating image data of one frame obtained by calculating an arithmetic mean of at least a part of the image data of the plurality of frames stored in the memory in units of pixels.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The imaging element according to <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein in the generation processing, the output image data of one frame is generated by combining partial image data that is a part of the image data, from a plurality of pieces of the image data.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The imaging element according to <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein at least a photoelectric conversion element and the memory are formed in one chip.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The imaging element according to <claim-ref idref="CLM-00011">claim 11</claim-ref>,<claim-text>wherein the imaging element is a laminated imaging element in which the photoelectric conversion element is laminated with the memory.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. An imaging apparatus comprising:<claim-text>the imaging element according to <claim-ref idref="CLM-00001">claim 1</claim-ref>; and</claim-text><claim-text>a control processor configured to perform a control for displaying an image based on the output image data output by the processor on a display.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. An imaging method comprising:<claim-text>a step of reading out, at a first frame rate, image data of each frame obtained by imaging a subject;</claim-text><claim-text>a step of storing the image data;</claim-text><claim-text>a step of processing the image data; and</claim-text><claim-text>a step of outputting the processed image data at a second frame rate lower than the first frame rate,</claim-text><claim-text>wherein in the step of reading out, the image data of a plurality of frames is read out in parallel within an output period that is defined by the second frame rate as a period in which the image data of one frame is output,</claim-text><claim-text>in the step of storing, image data read out in parallel is stored in parallel,</claim-text><claim-text>in the step of processing, output image data of one frame is generated using the stored image data of the plurality of frames, and</claim-text><claim-text>in the step of outputting, the generated output image data is output at the second frame rate.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. A non-transitory computer-readable storage medium storing a program causing a computer to execute:<claim-text>a procedure of reading out, at a first frame rate, image data of each frame obtained by imaging a subject;</claim-text><claim-text>a procedure of storing the image data;</claim-text><claim-text>a procedure of processing the image data; and</claim-text><claim-text>a procedure of outputting the processed image data at a second frame rate lower than the first frame rate,</claim-text><claim-text>wherein in the procedure of reading out, the image data of a plurality of frames is read out in parallel within an output period that is defined by the second frame rate as a period in which the image data of one frame is output,</claim-text><claim-text>in the procedure of storing, image data read out in parallel is stored in parallel,</claim-text><claim-text>in the procedure of processing, output image data of one frame is generated using the stored image data of the plurality of frames, and in the procedure of outputting, the generated output image data is output at the second frame rate.</claim-text></claim-text></claim></claims></us-patent-application>