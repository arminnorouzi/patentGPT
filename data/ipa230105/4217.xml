<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004218A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004218</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17778781</doc-number><date>20200915</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>CN</country><doc-number>201911149777.3</doc-number><date>20191121</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>01</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>02</class><subclass>B</subclass><main-group>27</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>02</class><subclass>B</subclass><main-group>27</main-group><subgroup>01</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>013</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>017</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>02</class><subclass>B</subclass><main-group>27</main-group><subgroup>0093</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>02</class><subclass>B</subclass><main-group>27</main-group><subgroup>0179</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>02</class><subclass>B</subclass><main-group>27</main-group><subgroup>0101</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>02</class><subclass>B</subclass><main-group>27</main-group><subgroup>017</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>02</class><subclass>B</subclass><main-group>2027</main-group><subgroup>0132</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>02</class><subclass>B</subclass><main-group>2027</main-group><subgroup>0138</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>02</class><subclass>B</subclass><main-group>2027</main-group><subgroup>014</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>02</class><subclass>B</subclass><main-group>2027</main-group><subgroup>0187</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">VIRTUAL REALITY SYSTEM</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Qingdao Pico Technology Co., Ltd.</orgname><address><city>Qingdao</city><country>CN</country></address></addressbook><residence><country>CN</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>ZHANG</last-name><first-name>Xiuzhi</first-name><address><city>Qingdao</city><country>CN</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>ZHOU</last-name><first-name>Hongwei</first-name><address><city>Qingdao</city><country>CN</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>LIU</last-name><first-name>Guanghui</first-name><address><city>Qingdao</city><country>CN</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>GUO</last-name><first-name>Hengjiang</first-name><address><city>Qingdao</city><country>CN</country></address></addressbook></inventor></inventors></us-parties><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/CN2020/115302</doc-number><date>20200915</date></document-id><us-371c12-date><date>20220520</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">The present disclosure provides a virtual reality system. The virtual reality system of the disclosure, comprising: a head-mounted display, an input device and a positioning module, wherein the head-mounted display comprises a central processing unit, a camera module connected with the central processing unit and a wireless connection module; the camera module comprises a binocular fisheye camera, an IR camera and a TOF camera, the positioning module comprises a first inertial measurement unit and an electromagnetic receiver provided on the head-mounted display, a second inertial measurement unit and an electromagnetic transmitter provided on the input device; the central processing unit, configured to implement data interaction and command control with the binocular fisheye camera, the IR camera, the TOF camera, the wireless connection module, the first inertial measurement unit and the electromagnetic receiver.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="86.11mm" wi="115.74mm" file="US20230004218A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="203.54mm" wi="157.23mm" file="US20230004218A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="208.53mm" wi="139.62mm" file="US20230004218A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="167.98mm" wi="140.21mm" file="US20230004218A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="159.17mm" wi="118.87mm" file="US20230004218A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="101.01mm" wi="117.77mm" file="US20230004218A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS REFERENCE</heading><p id="p-0002" num="0001">This application is the U.S. National Stage of International Application No. PCT/CN2020/115302, filed on Sep. 15, 2020, which claims priority to the Chinese patent application No. 201911149777.3, filed on Nov. 21, 2019, the disclosure of which is incorporated herein by reference in its entirety.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">FIELD</heading><p id="p-0003" num="0002">The present disclosure relates to a virtual reality system.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">A virtual reality technology is to use a computer technology to integrate and reconstruct various information such as vision, hearing, touch, etc., so as to generate human-machine interactive virtual scenes. Users can obtain immersive experiences through real-time dynamic three-dimensional images displayed by virtual reality (VR) devices.</p><p id="p-0005" num="0004">In the existing mobile VR all-in-one design, most systems support 6 degrees of freedom, namely 6-DoF function of heads and 3-DoF function of hands. Even if some head-mounted devices support the 6-DoF function of the hands, they are mainly realized based on optical solutions and ultrasonic solutions. These solutions are limited by functions such as field of view (FOV), which cannot adapt to the needs of panoramic PC games.</p><p id="p-0006" num="0005">At present, in an inside-out solution of a mobile terminal, game content is a bottleneck problem. Most 6-DoF games are designed based on PC, and existing mobile terminals that meet the 6-DoF function of the PC games are wired solutions, that is, they are connected with a PC terminal in a wired mode. This wired solution, firstly, is inconvenient to use; secondly, the moving distance is subject to many restrictions; and thirdly, the wired solution is based on the design of an external camera or a light tower, with high cost.</p><p id="p-0007" num="0006">Therefore, there is an urgent need for a solution to meet the needs of the PC games and support inside-out 6 DoF of the heads and 6 DoF of the hands.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0008" num="0007">The purpose of the present disclosure is to provide a virtual reality system.</p><p id="p-0009" num="0008">The present disclosure provides a virtual reality system, comprising: a head-mounted display, an input device and a positioning module, wherein the head-mounted display is comprises a central processing unit, a camera module connected with the central processing unit and a wireless connection module;</p><p id="p-0010" num="0009">the camera module comprises a binocular fisheye camera, an IR camera and a TOF camera, the positioning module comprises a first inertial measurement unit provided on the head-mounted display, an electromagnetic receiver provided on the head-mounted display, a second inertial measurement unit provided on the input device and an electromagnetic transmitter provided on the input device; and</p><p id="p-0011" num="0010">the central processing unit is further connected with the first inertial measurement unit and the electromagnetic receiver respectively, and configured to implement data interaction and command control with the binocular fisheye camera, the IR camera, the TOF camera, the wireless connection module, the first inertial measurement unit and the electromagnetic receiver, so as to identify 6-DoF spatial positioning of the head-mounted display, 6-DoF spatial positioning of the input device, and implement an obstacle avoidance function, a gesture recognition function, an eyeball tracking function and a wireless transmission function of the virtual reality system.</p><p id="p-0012" num="0011">The present disclosure at least realizes the following technical effects:</p><p id="p-0013" num="0012">1. The present disclosure can implement tracking of a spatial position and a spatial attitude of the virtual reality system through two fisheye cameras and an inertial measurement unit (IMU), and implement high-precision tracking within a 360-degree panoramic range through an input device; and through the combination of the two, the present disclosure can implement position tracking of Roomscale level, and is very suitable for a game scene of Roomscale.</p><p id="p-0014" num="0013">2. The present disclosure can import high-throughput and low-latency wireless data through a wireless connection module, and can apply the virtual reality system to a PC game platform. Combined with the advantages of 6-DoF implementation of the heads and the hands, a practical and convenient head-mounted display (HMD) all-in-one machine with low latency, high precision, and no need to add peripheral batteries can be realized.</p><p id="p-0015" num="0014">3. The present disclosure can implement high-precision plane and obstacle detection by introducing a Time Of Flight (TOF) camera, which can better utilize the use advantages of the Roomscale of the virtual reality system, and can also be used for gesture detection to further increase an interaction mode of the virtual reality system. Moreover, the eyeball tracking function can be implemented through an infrared camera (IR camera), which can realize software interpupillary distance adjustment, and annotation point rendering improves the system frame rate and brings a better Roomscale experience.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0016" num="0015">In order to illustrate the technical solutions of the embodiments of the present disclosure more clearly, the accompanying drawings required in the embodiments will be briefly introduced below. It is to be understood that the following drawings illustrate only certain embodiments of the disclosure and are therefore not to be considered limiting of its scope. For those of ordinary skill in the art, other related drawings can also be obtained from these drawings without any creative effort.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a structural block diagram of a virtual reality system shown in an embodiment of the present disclosure.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a schematic structural diagram of hardware of a virtual reality system shown in an embodiment of the present disclosure.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a schematic diagram of an electronic circuit of a virtual reality system shown in an embodiment of the present disclosure.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a schematic diagram of 6-DoF spatial positioning of a head-mounted display shown in an embodiment of the present disclosure.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a schematic diagram of 6-DoF spatial positioning of an input device shown in an embodiment of the present disclosure.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a schematic diagram of a wireless streaming function shown in an embodiment of the present disclosure.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a schematic diagram of gesture recognition shown in an embodiment of the present disclosure.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a schematic diagram of an obstacle avoidance function shown in an embodiment of the present disclosure.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a schematic diagram of an eyeball tracking function shown in an embodiment of the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DESCRIPTION OF THE EMBODIMENTS</heading><p id="p-0026" num="0025">Various exemplary embodiments of the present disclosure will now be described in detail with reference to the accompanying drawings. It should be noted that the relative arrangement of components and steps, the numerical expressions and numerical values set forth in these embodiments do not limit the scope of the disclosure unless specifically stated otherwise.</p><p id="p-0027" num="0026">The following description of at least one exemplary embodiment is merely illustrative in nature and is in no way intended to limit the disclosure, its application, or uses.</p><p id="p-0028" num="0027">Techniques, methods, and apparatus known to those of ordinary skill in the relevant art may not be discussed in detail, but where appropriate, techniques, methods, and apparatus should be considered part of the specification.</p><p id="p-0029" num="0028">In all examples shown and discussed herein, any specific values should be construed as illustrative only and not limiting. Accordingly, other instances of the exemplary embodiment may have different values.</p><p id="p-0030" num="0029">It should be noted that like numerals and letters refer to like items in the following figures, so once an item is defined in one figure, it does not require further discussion in subsequent figures.</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic structural diagram of hardware of a virtual reality system shown in an embodiment of the present disclosure. As shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the virtual reality system of the embodiment includes: a head-mounted display <b>100</b>, an input device <b>200</b> and a positioning module <b>300</b>, and the head-mounted display <b>100</b> comprises a central processing unit <b>110</b>, a camera module <b>120</b> connected with the central processing unit <b>110</b> and a wireless connection module <b>130</b>;</p><p id="p-0032" num="0031">the camera module <b>120</b> includes a binocular fisheye camera <b>121</b>, an IR camera <b>122</b> and a TOF camera <b>123</b>, and the positioning module <b>300</b> includes a first inertial measurement unit (IMU<b>1</b>) <b>310</b> provided on the head-mounted display <b>100</b>, an electromagnetic receiver <b>320</b> provided on the head-mounted display <b>100</b>, a second inertial measurement unit (IMU<b>2</b>) <b>330</b> provided on the input device <b>200</b> and an electromagnetic transmitter <b>340</b> provided on the input device <b>200</b>; and</p><p id="p-0033" num="0032">the central processing unit <b>110</b> is further connected with the IMU<b>1</b> <b>310</b> and the electromagnetic receiver <b>320</b> respectively, and configured to implement data interaction and command control with the binocular fisheye camera <b>121</b>, the IR camera <b>122</b>, the TOF camera <b>123</b>, the wireless connection module <b>130</b>, the IMU<b>1</b> <b>310</b> and the electromagnetic receiver <b>320</b>, so as to identify 6-DoF spatial positioning of the head-mounted display, 6-DoF spatial positioning of the input device, and implement an obstacle avoidance function, a gesture recognition function, an eyeball tracking function and a wireless transmission function of the virtual reality system.</p><p id="p-0034" num="0033">The embodiment can implement tracking of a spatial position and a spatial attitude of the virtual reality system through two fisheye cameras and an IMU<b>1</b>, and implement high-precision tracking within a 360-degree panoramic range through the input device; and through the combination of the two, the embodiment can implement position tracking of Roomscale level, and is very suitable for a game scene of Roomscale. The embodiment can import high-throughput and low-latency wireless data through the wireless connection module, and can apply the virtual reality system to a PC game platform. Combined with the advantages of 6-DoF implementation of the heads and the hands, a practical and convenient head-mounted display (HMD) all-in-one machine with low latency, high precision, and no need to add peripheral batteries can be implemented. The embodiment can implement high-precision plane and obstacle detection by introducing the TOF camera, which can better utilize the use advantages of the Roomscale of the virtual reality system, and can also be used for gesture detection to further increase an interaction mode of the virtual reality system. Moreover, the eyeball tracking function can be implemented through the IR camera, which can implement software interpupillary distance adjustment, and annotation point rendering improves the system frame rate and brings a better Roomscale experience.</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a schematic structural diagram of hardware of a virtual reality system shown in an embodiment of the present disclosure. As shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, in the embodiment, the fisheye camera, the IR camera, the TOF camera, the wireless connection module (namely 60 GHz Wifi shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>), the IMU<b>1</b> and the electromagnetic receiver (namely an electromagnetic receiving coil shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>) are provided on the head-mounted display (HMD). However, the IMU<b>2</b> and the electromagnetic transmitter (namely an electromagnetic transmitting coil shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>) are provided on the input device, namely an electromagnetic handle shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a schematic diagram of an electronic circuit of a virtual reality system shown in an embodiment of the present disclosure. As shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the central processing unit in the embodiment may be implemented by a chip of a type of Qualcom mm 845, and the wireless connection module may be realized by a 11ad chip. As shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the virtual reality system of the embodiment further includes other necessary components, such as a microphone, a loudspeaker, a display supporting 4K (3840*2160) high-definition displaying (namely a 4k Display shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>), a flash memory/dynamic random memory UFS/LPDDR, 2.4G/5G Wifi and various communication lines, I2S (an Inter-IC Sound integrated circuit built-in audio bus), an SPI (serial peripheral interface), an MIPI (mobile industry processor interface) and a USB (universal serial bus) as shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0037" num="0036">Referring to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, in the embodiment, functional components provided on the head-mounted display are connected with a central processing unit chip through corresponding buses and interfaces, such as, the microphone and the loudspeaker which are connected with the central processing unit chip through the I2S, the IMU connected with the central processing unit chip through the SPI, a CMOS (complementary metal-oxide-semiconductor) sensor connected with the central processing unit chip through the MIPI, a wireless connection chip connected with the central processing unit chip through the USB, and the 2.4/5G Wifi and the 60 GHz Wifi which are connected with the central processing unit chip through PCIe (peripheral component interconnect express).</p><p id="p-0038" num="0037">In one embodiment, the binocular fisheye camera is configured to obtain an image and send the image to the central processing unit; the first inertial measurement unit IMU<b>1</b> is configured to obtain spatial attitude data of the head-mounted display and send the spatial attitude data to the central processing unit; and the central processing unit is configured to identify 6-DoF spatial positioning of the head-mounted display according to the received image and spatial attitude data.</p><p id="p-0039" num="0038">As shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, a frame rate of the data obtained by the fisheye camera is 30 fps, a frequency of the spatial attitude data obtained by the IMU<b>1</b> is 1000 Hz, and an SLAM (simultaneous localization and mapping) function supporting Roomscale level of the HMD is implemented through the data obtained by the IMU<b>1</b> and the fisheye camera. Algorithm processing involved runs in the Qualcom mm 845 of the central processing unit.</p><p id="p-0040" num="0039">Referring to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, a fisheye left camera and a fisheye right camera obtain image data at the frame rate of 30 fps, and send the image data to the Qualcom mm 845 through the MIPI, the IMU<b>1</b> obtains the spatial attitude data of the HMD at the frequency of 1000 Hz, and send the spatial attitude data to the Qualcom mm 845 through the SPI, the Qualcom mm 845 performs image processing (namely camera feature processing shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>) on the image data, and performs data processing (namely IMU data processing shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>) on the spatial attitude data. Based on a preset algorithm, the two kinds of data are fused, such as processing of relocalization, continuous map optimization and environment mapping, pose generation and prediction is realized, so that a to-be-corrected image (namely warp image based on the latest head pose shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>) and application rendering are obtained, the to-be-corrected image is corrected by the application rendering, and the corrected image is displayed through a display.</p><p id="p-0041" num="0040">In one embodiment, a wireless transmission module is further provided on the input device and configured to be matched with the wireless connection module on the HMD, so as to implement data wireless transmission between the HMD and the input device:</p><p id="p-0042" num="0041">the IMU<b>2</b> is configured to obtain spatial attitude data of the input device and send the spatial attitude data to the central processing unit through the wireless transmission module and the wireless connection module;</p><p id="p-0043" num="0042">the electromagnetic transmitter is configured to transmit electromagnetic data;</p><p id="p-0044" num="0043">the electromagnetic receiver is configured to receive the electromagnetic data transmitted by the electromagnetic transmitter and send the electromagnetic data to the central processing unit; and</p><p id="p-0045" num="0044">the central processing unit is configured to calculate a relative positional relationship between the input device and the head-mounted display according to the received electromagnetic data, so as to identify the 6-DoF spatial positioning of the input device based on the relative positional relationship and the received spatial attitude data.</p><p id="p-0046" num="0045">The principle of 6-DoF spatial positioning of the input device is shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>. Three sinusoidal signals with different frequencies are generated on an X-axis, a Y-axis and a Z-axis of the electromagnetic transmitter (namely a transmitting sensor shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>), and due to the change of magnetic induction intensity, an induced electromotive force is generated on an X&#x2032;-axis, a Y&#x2032;-axis and a Z&#x2032;-axis of the electromagnetic receiver (namely a receiving sensor shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>). Three inductive signals are received through the electromagnetic receiver, and relative positional information and relative attitude information of the electromagnetic receiver and the electromagnetic transmitter are calculated by a positioning algorithm. Since the electromagnetic transmitter is provided on the input device, the electromagnetic receiver is provided on the HMD, and has a fixed coordinate system relationship with a display screen. A coordinate system relationship between the input device and the display screen may be calculated through coordinate system conversion, so that the 6-DoF spatial positioning of the input device can be implemented, and when the input device comprises the electromagnetic handle, 6-DoF spatial positioning of hands can be implemented.</p><p id="p-0047" num="0046">In one embodiment, the wireless connection module is a wireless connection module with WiGig (wireless gigabit alliance) and Wifi protocols, namely the 60 GHz Wifi shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. The central processing unit is configured to send 6-Dof spatial positioning data, such as 6-DoF spatial positioning data of the HMD and/or 6-DoF spatial positioning data of the input device, to an opposite terminal device of the virtual reality system, namely a PC device as shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, through the wireless connection module, so as to enable the opposite terminal device to render and compress a to-be-processed image according to the 6-DoF spatial positioning data, and send the processed image data to the central processing unit through the wireless connection module; and the central processing unit is configured to receive the image data sent by the opposite terminal device through the wireless connection module and displays the image data.</p><p id="p-0048" num="0047">For example, the wireless connection module adopts the 11ad chip, which supports 4.6 Gbps data transmission at most, so as to ensure large data bandwidth and low data transmission delay. The HMD transmits the 6-DoF data to a PC terminal through the 60 GHz Wifi, the PC terminal performs rendering, coding and compression, and then sends the 6-DoF data to the 11ad chip through USB3.0/Ethernet/PCIe, the 11ad chip sends the 6-DoF data to the HMD through the 60 GHz Wifi, and the image may be displayed after decoded by the HMD.</p><p id="p-0049" num="0048">In one embodiment, the TOF camera is configured to obtain a depth image and send the depth image to the central processing unit; and the central processing unit is configured to implement the gesture recognition function according to the depth image.</p><p id="p-0050" num="0049">The principle of gesture recognition of the embodiment is as shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>. A depth processor of the TOF camera controls emission time and an emission pulse width of a laser transmitter. A CCD (charge coupled device) sensor is controlled to obtain IR images in different time periods, an infrared image of an external environment and an infrared image after reflection of an object are calculated respectively, so that time of flight (TOF) of reflected lasers at each point of an external object is calculated, depth information of each pixel point may be calculated, after the depth information (640*480) and the IR images (640*480) are combined into an image, the image is sent to the central processing unit, namely the Qualcom mm 845 chip, through the MIPI, and judgment of the gesture recognition function is completed based on a gesture recognition algorithm.</p><p id="p-0051" num="0050">Since the calculation of the depth information depends on the IR images, and the IR images and the depth information are completely synchronized, the gesture recognition function can be better implemented.</p><p id="p-0052" num="0051">In one embodiment, the TOF camera obtains a depth image and sends the depth image to the central processing unit; the binocular fisheye camera obtains an image and sends the image to the central processing unit, and the image includes a gray image; and the central processing unit implements the obstacle avoidance function according to the depth image and the gray image.</p><p id="p-0053" num="0052">The implementation principle of the obstacle avoidance function of the embodiment is shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>. In an application scene, if there is an obstacle (such as a conference table), in the process of modeling, contour information of the obstacle is obtained through depth data obtained by the TOF camera. In order to solve the problem of the IR images when the environment is dark, the embodiment obtains the gray image of the obstacle in combination with the fisheye camera, and virtualizes the obstacle into the scene during modeling, so as to implement the accurate obstacle avoidance function of the virtual reality system.</p><p id="p-0054" num="0053">In one embodiment, infrared transmitters are provided around two eye cameras of the HMD, for example, a circle of infrared transmitters are respectively provided around each eye camera, and the infrared transmitters transmit infrared rays to the eyes through the eye cameras; at this time, the infrared camera obtains infrared images of the eyes and sends the infrared images to the central processing unit; and the central processing unit implements the eyeball tracking function according to the infrared images.</p><p id="p-0055" num="0054">The implementation principle of the eyeball tracking function of the embodiment is shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref>. The infrared transmitters integrated in the HMD actively transmit infrared signals, and reflect the infrared signals through a color selective mirror, the two groups of IR cameras provided on corresponding positions of the two eye cameras obtain the IR images and send the IR images to the central processing unit, and the central processing unit analyzes angles of eyeballs through the IR images so as to realize the purpose of eyeball tracking.</p><p id="p-0056" num="0055">To sum up, the embodiment can implement tracking of the spatial position and the spatial attitude of the virtual reality system through the two fisheye cameras and the IMU<b>1</b>, and implement high-precision tracking within the 360-degree panoramic range through the input device; and through the combination of the two, the embodiment can implement position tracking of Roomscale level, and is very suitable for the game scene of Roomscale. The embodiment can import high-throughput and low-latency wireless data through the wireless connection module, and can apply the virtual reality system to a PC game platform. Combined with the advantages of 6-DoF implementation of the heads and the hands, the practical and convenient HMD all-in-one machine with low latency, high precision, and no need to add peripheral batteries can be implemented. The embodiment can implement high-precision plane and obstacle detection by introducing the TOF camera, which can better utilize the use advantages of the Roomscale of the virtual reality system, and can also be used for gesture detection to further increase an interaction mode of the virtual reality system. Moreover, the eyeball tracking function can be implemented through the IR camera, which can implement software interpupillary distance adjustment, and annotation point rendering improves the system frame rate and brings the better Roomscale experience.</p><p id="p-0057" num="0056">In order to clearly describe the technical solutions of the embodiments of the present disclosure, in the embodiments of the present disclosure, words such as &#x201c;first&#x201d; and &#x201c;second&#x201d; are used to distinguish the same items or similar items that have basically the same functions and functions. Persons can understand that words such as &#x201c;first&#x201d; and &#x201c;second&#x201d; do not limit the quantity and execution order.</p><p id="p-0058" num="0057">The above descriptions are only specific embodiments of the present disclosure, and those skilled in the art can make other improvements or modifications on the basis of the above embodiments under the above teachings of the present disclosure. Those skilled in the art should understand that the above-mentioned specific description is only for better explaining the purpose of the present disclosure, and the protection scope of the present disclosure is subject to the protection scope of the claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A virtual reality system, wherein comprising: a head-mounted display, an input device and a positioning module, wherein the head-mounted display comprises a central processing unit, a camera module connected with the central processing unit and a wireless connection module;<claim-text>the camera module comprises a binocular fisheye camera, an IR camera and a TOF camera, the positioning module comprises a first inertial measurement unit provided on the head-mounted display, an electromagnetic receiver provided on the head-mounted display, a second inertial measurement unit provided on the input device and an electromagnetic transmitter provided on the input device; and</claim-text><claim-text>the central processing unit is further connected with the first inertial measurement unit and the electromagnetic receiver respectively, and configured to implement data interaction and command control with the binocular fisheye camera, the IR camera, the TOF camera, the wireless connection module, the first inertial measurement unit and the electromagnetic receiver, so as to identity 6-DoF spatial positioning of the head-mounted display, 6-DoF spatial positioning of the input device, and implement an obstacle avoidance function, a gesture recognition function, an eyeball tracking function and a wireless transmission function of the virtual reality system.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The virtual reality system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the binocular fisheye camera is configured to obtain an image and send the image to the central processing unit;</claim-text><claim-text>the first inertial measurement unit is configured to obtain spatial attitude data of the head-mounted display and send the spatial attitude data to the central processing unit; and</claim-text><claim-text>the central processing unit is configured to identity the 6-DoF spatial positioning of the head-mounted display according to the received image and the spatial attitude data.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The virtual reality system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the input device is further provided with a wireless transmission module;<claim-text>the second inertial measurement unit is configured to obtain spatial attitude data of the input device and send the spatial attitude data to the central processing unit through the wireless transmission module and the wireless connection module;</claim-text><claim-text>the electromagnetic transmitter is configured to transmit electromagnetic data;</claim-text><claim-text>the electromagnetic receiver is configured to receive the electromagnetic data transmitted by the electromagnetic transmitter and send the electromagnetic data to the central processing unit; and</claim-text><claim-text>the central processing unit is configured to calculate a relative positional relationship between the input device and the head-mounted display according to the received electromagnetic data, thereby identify the 6-DoF spatial positioning of the input device based on the relative positional relationship and the received spatial attitude data.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The virtual reality system according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the input device comprises an electromagnetic handle.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The virtual reality system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the wireless connection module is a wireless connection module with WiGig and Wifi protocols.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The virtual reality system according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein<claim-text>the central processing unit is configured to send 6-DoF spatial positioning data to an opposite terminal device of the virtual reality system through the wireless connection module, so as to enable the opposite terminal device to render and compress a to-be-processed image according to the 6-DoF spatial positioning data and send processed image data to the central processing unit through the wireless connection module; and</claim-text><claim-text>the central processing unit is configured to receive the image data sent by the opposite terminal device through the wireless connection module and displays the image data.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The virtual reality system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the TOF camera is configured to obtain a depth image and send the depth image to the central processing unit; and</claim-text><claim-text>the central processing unit is configured to implement the gesture recognition function according to the depth image.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The virtual reality system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the TOF camera is configured to obtain a depth image and send the depth image to the central processing unit;</claim-text><claim-text>the binocular fisheye camera is configured to obtain an image and send the image to the central processing unit, and the image comprises a gray image; and</claim-text><claim-text>the central processing unit is configured to implement the obstacle avoidance function according to the depth image and the gray image.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The virtual reality system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein infrared transmitters are provided around two eye cameras of the head-mounted display respectively;<claim-text>the infrared transmitters are configured to transmit infrared rays to eyes through the eye cameras;</claim-text><claim-text>the IR camera is configured to obtain an infrared image of the eyes and send the infrared image to the central processing unit; and</claim-text><claim-text>the central processing unit is configured to implement the eyeball tracking function according to the infrared image.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The virtual reality system according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the head-mounted display comprises two groups of IR cameras, and the two groups of IR cameras are provided in corresponding positions of the two eye cameras respectively.</claim-text></claim></claims></us-patent-application>