<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004307A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004307</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17903772</doc-number><date>20220906</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>06</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20160101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>12</main-group><subgroup>1009</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>0616</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>0659</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>0673</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>12</main-group><subgroup>1009</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>0635</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>2212</main-group><subgroup>1036</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">MEMORY OPERATIONS WITH CONSIDERATION FOR WEAR LEVELING</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16727196</doc-number><date>20191226</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11442631</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17903772</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Micron Technology, Inc.</orgname><address><city>Boise</city><state>ID</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Gupta</last-name><first-name>Rajesh N.</first-name><address><city>Bengaluru</city><country>IN</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">As described herein, an apparatus may include a memory that includes a first portion, a second portion, and a third portion. The apparatus may also include a memory controller that includes a first logical-to-physical table stored in a buffer memory. The memory controller may determine that the first portion is accessed sequential to the second portion and may adjust the first logical-to-physical table to cause a memory transaction performed by the memory controller to access the third portion as opposed to the first portion.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="115.40mm" wi="148.59mm" file="US20230004307A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="128.27mm" wi="150.62mm" file="US20230004307A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="246.63mm" wi="161.80mm" orientation="landscape" file="US20230004307A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="246.63mm" wi="161.80mm" orientation="landscape" file="US20230004307A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="237.41mm" wi="163.91mm" file="US20230004307A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><p id="p-0002" num="0001">The present application is a continuation of U.S. application Ser. No. 16/727,196, entitled &#x201c;Memory Operations With Consideration For Wear Leveling,&#x201d; and filed Dec. 26, 2019, now U.S. Pat. No. 11,442,631, which issues on Sep. 13, 2022, the entirety of which is incorporated by reference herein for all purposes.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">BACKGROUND</heading><p id="p-0003" num="0002">This section is intended to introduce the reader to various aspects of art that may be related to various aspects of the present techniques, which are described and/or claimed below. This discussion is believed to be helpful in providing the reader with background information to facilitate a better understanding of the various aspects of the present disclosure. Accordingly, it should be understood that these statements are to be read in this light and not as admissions of prior art.</p><p id="p-0004" num="0003">Generally, a computing system includes processing circuitry, such as one or more processors or other suitable components, and memory devices, such as chips or integrated circuits. One or more memory devices may be used on a memory module, such as a dual in-line memory module (DIMM), to store data accessible to the processing circuitry. For example, based on a user input to the computing system, the processing circuitry may request that a memory module retrieve data corresponding to the user input from its memory devices. In some instances, the retrieved data may include firmware, or instructions executable by the processing circuitry to perform an operation, and/or may include data to be used as an input for the operation. In addition, in some cases, data output from the operation may be stored in memory, for example, to enable subsequent retrieval. In instances where firmware is retrieved from non-volatile memory (e.g., media, storage), a pattern of retrieval of the information stored in memory may be inefficient. Each memory chip is made up of sub-units sometimes referred to as memory banks. Memory banks may share input/output circuitry but may otherwise operate independent of each other. In this way, a computing system may reference a portion of one memory bank without referencing a portion of another memory bank. A memory unit may be a single memory chip or a collection of memory chips. Memory units may be thought to be made up of memory &#x201c;banks.&#x201d; Since memory banks may operate independently, a read or write instruction to one memory bank may proceed to execute while another memory bank is busy processing a previous read/write instruction. This means that a memory chip may operate simultaneous operations in multiple banks. However, if operations are issued to the same bank, the memory chip may wait to process next operation until any previous operations are finished. Thus, a read/write speed of a given memory system (e.g., one or more memory units) may depend on how data being transferred to/from the memory is distributed across different banks. For example, if all data is stored in the same bank, a total duration of time used for performing memory operations is expected to be longer relative to a total duration of time used for performing memory operations when the data is stored and/or distributed across multiple banks.</p><p id="p-0005" num="0004">In storage systems, an address translation table may be used to map memory addresses from logical to physical addresses. For example, data (e.g., the information stored in the memory) may be mapped from logical to physical addresses of the memory using a logical-to-physical (L2P) translation table. Over time, some physical addresses of the memory may be accessed more often than other physical addresses of the memory in response to memory access patterns, which may age portions of the memory corresponding to the more accessed physical addresses at a relatively faster rate than other portions of the memory. Uneven aging of a memory is generally undesirable. A more efficient memory accessing operation may be desired to improve memory management operations (e.g., improve performance, reduce an amount of time used to perform memory operations) and improve wear leveling (e.g., reduction of uneven access patterns, evening out of access patterns as to promote even aging of the memory).</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0002" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0006" num="0005">Various aspects of this disclosure may be better understood upon reading the following detailed description and upon reference to the drawings in which:</p><p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram of a computing system that includes client devices and one or more computing devices, in accordance with an embodiment;</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram of a memory module that may be used in a computing device of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, in accordance with an embodiment;</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a block diagram of the memory module of <figref idref="DRAWINGS">FIG. <b>2</b></figref> after a duration of time such that the memory module is affected by relatively uneven access patterns, in accordance with an embodiment;</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is an illustration of a controller of the memory module of <figref idref="DRAWINGS">FIG. <b>2</b></figref> operating to adjust a current logical-to-physical table (L2P table) to preemptively compensate for uneven access patterns, in accordance with an embodiment; and</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flowchart of a process for operating the controller of <figref idref="DRAWINGS">FIG. <b>4</b></figref> to preemptively compensate for uneven access patterns, in accordance with an embodiment.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0003" level="1">DETAILED DESCRIPTION</heading><p id="p-0012" num="0011">When introducing elements of various embodiments of the present disclosure, the articles &#x201c;a,&#x201d; &#x201c;an,&#x201d; &#x201c;the,&#x201d; and &#x201c;said&#x201d; are intended to mean that there are one or more of the elements. The terms &#x201c;comprising,&#x201d; &#x201c;including,&#x201d; and &#x201c;having&#x201d; are intended to be inclusive and mean that there may be additional elements other than the listed elements. One or more specific embodiments of the present embodiments described herein will be described below. In an effort to provide a concise description of these embodiments, all features of an actual implementation may not be described in the specification. It should be appreciated that in the development of any such actual implementation, as in any engineering or design project, numerous implementation-specific decisions must be made to achieve the developers' specific goals, such as compliance with system-related and business-related constraints, which may vary from one implementation to another. Moreover, it should be appreciated that such a development effort might be complex and time consuming, but would nevertheless be a routine undertaking of design, fabrication, and manufacture for those of ordinary skill having the benefit of this disclosure.</p><p id="p-0013" num="0012">Generally, hardware of a computing system includes processing circuitry and memory implemented using one or more processors and/or one or more memory devices (e.g., as chips, as integrated circuits). During operation of the computing system, the processing circuitry may perform various operations (e.g., tasks) by executing corresponding instructions, for example, based on a user input to determine output data by performing operations on input data. To facilitate operation of the computing system, data accessible to the processing circuitry may be stored in a memory device, such that the memory device stores the input data, the output data, data indicating the executable instructions, or any combination thereof.</p><p id="p-0014" num="0013">Additionally, in some instances, memory devices may be implemented using different memory types. For example, a memory device may be implemented as volatile memory, such as dynamic random-access memory (DRAM) or static random-access memory (SRAM). Alternatively, the memory device may be implemented as non-volatile memory, such as flash (e.g., NAND, NOR) memory, phase-change memory (e.g., 3D XPoint&#x2122;), or ferroelectric random access memory (FeRAM). In any case, memory devices generally include at least one memory die (e.g., an array of memory cells configured on a portion or &#x201c;die&#x201d; of a semiconductor wafer) to store data bits (e.g., &#x201c;0&#x201d; bit or &#x201c;1&#x201d; bit) transmitted to the memory device through a channel (e.g., data channel, communicative coupling, bus interface) and may be functionally similar from the perspective of the processing circuitry even when the memory devices include different memory types.</p><p id="p-0015" num="0014">During operation of the host device, applications or programs of the host device, or other components of the host device, may generate or access information stored in the memory. Information stored as the data within the memory may be stored at physical locations. These physical locations within the memory may be accessed by components of the host device via referenceable logical addresses. A memory controller may control operation of the memory and/or act as an intermediary device between the memory and the host device. In this way, when the memory receives a command from the host device, the command may include an instruction (e.g., read instruction, write instruction) and an indication of a logical address (e.g., a string of bits that indicate a location in memory that the component of the host device desires to access). The memory controller, after receiving the command, may reference a logical-to-physical translation table (L2P table) to determine the physical address that corresponds to the logical address of the command, where the physical address is the physical location within the memory at which the host device desires to access with the command.</p><p id="p-0016" num="0015">Over time, some physical addresses of the memory may be accessed more often than other physical addresses of the memory. Unequal access distributions and/or uneven access patterns of accessing the memory may age some portions of the memory at a relatively faster rate than other portions of the memory. Uneven aging of the memory is generally undesirable since it may shorten a lifespan of a device and operations to even aging of the memory (e.g., evening access to the memory) may be referred to as &#x201c;wear leveling&#x201d; operations.</p><p id="p-0017" num="0016">As described herein, to compensate for memory access patterns, such as to reduce uneven wear from uneven memory accesses and/or to improve a total duration of time used to process memory commands, the memory controller may adjust the L2P table based on commands issued by the host device. For example, the memory controller may adjust the L2P table with consideration for physical addresses that are accessed relatively more often than other physical addresses and/or with consideration for logical addresses that are commonly access sequential, or a duration of time subsequent to each other such that processing of a first command is still ongoing as to delay an initiation of processing of the subsequent command. By adjusting the L2P table based on address access patterns (e.g., traffic patterns), the memory controller may preemptively reduce or eliminate uneven wear and promote wear leveling and/or may improve speeds of performing memory operations (e.g., by increasing a number of memory accesses that may be performed in parallel). In some embodiments, the memory controller may also consider performance when adjusting the L2P table to improve (e.g., make more even) physical address access distributions and to improve wear leveling (e.g., make accesses more equal in number).</p><p id="p-0018" num="0017">In this way, the L2P translation may be used to improve performance of a memory system (e.g., reduce a time used to perform memory operations). Since the L2P table provides the ability to store data in arbitrary physical locations in memory while the data may still be in contiguous logical address space, the L2P table may be leveraged to optimize and/or improve memory access patterns. In some cases, an optimal data storage pattern is memory access dependent, thus each software application of a computing system (e.g., each software application that has or uses access to the memory system) may have its own optimal pattern. Thus, in some embodiments, the memory system and/or the computing system may analyze software application access of the memory system to determine traffic patterns. Through deployment of deep learning algorithms, the traffic patterns may be used to generate L2P translation tables designed to improve access of the memory system based on actual access tendencies of the software application.</p><p id="p-0019" num="0018">In some cases, a L2P table may be generated that represents an optimum behavior or relatively improved performance for multiple software applications. The L2P table generated based on traffic patterns for two or more software applications may be used and/or accessed as a default L2P table for the memory system. By using L2P tables adjusted based on traffic patterns of software application, performance of the memory system may improve since logical addresses that are relatively frequently accessed subsequent to one another may be defined to reference physical addresses in different banks. These L2P tables may also be used to manage wear levelling, such as by distributing memory access across one or more memory banks. Furthermore, it is noted that since these wear levelling algorithms modify the L2P table during operation of the memory system to optimize for wear levelling and/or expected sequence of memory accesses, memory operations do not need to be delayed while these determinations are being performed.</p><p id="p-0020" num="0019">To help illustrate, <figref idref="DRAWINGS">FIG. <b>1</b></figref> depicts an example of a computing system <b>10</b>, which includes one or more remote computing devices <b>12</b>. As in the depicted embodiment, the remote computing devices <b>12</b> may be communicatively coupled to the one or more client devices <b>14</b> via a communication network <b>16</b>. It should be appreciated that the depicted embodiment is merely intended to be illustrative and not limiting. For example, in other embodiments, the remote computing devices <b>12</b> may be communicatively coupled to a single client device <b>14</b> or more than two client devices <b>14</b>. Furthermore, depending on the computing system <b>10</b>, the memory controller <b>34</b> may not be just on the memory module <b>26</b>. In this way, depicted is a generic use of the described techniques where the memory controller <b>34</b> is wholly on the memory module <b>26</b>. However, other examples may include a memory controller without a memory module and/or may use a processing circuit <b>24</b> as the memory controller <b>34</b>.</p><p id="p-0021" num="0020">In any case, the communication network <b>16</b> may enable data communication between the client devices <b>14</b> and the remote computing devices <b>12</b>. In some embodiments, the client devices <b>14</b> may be physically remote (e.g., separate) from the remote computing devices <b>12</b>, for example, such that the remote computing devices <b>12</b> are located at a centralized data center. Thus, in some embodiments, the communication network <b>16</b> may be a wide area network (WAN), such as the Internet. To facilitate communication via the communication network <b>16</b>, the remote computing devices <b>12</b> and the client devices <b>14</b> may each include a network interface <b>18</b>.</p><p id="p-0022" num="0021">In addition to the network interface <b>18</b>, a client device <b>14</b> may include input devices <b>20</b> and/or an electronic display <b>22</b> to enable a user to interact with the client device <b>14</b>. For example, the input devices <b>20</b> may receive user inputs and, thus, may include buttons, keyboards, mice, trackpads, and/or the like. Additionally or alternatively, the electronic display <b>22</b> may include touch sensing components that receive user inputs by detecting occurrence and/or position of an object touching its screen (e.g., surface of the electronic display <b>22</b>). In addition to enabling user inputs, the electronic display <b>22</b> may facilitate providing visual representations of information by displaying a graphical user interface (GUI) of an operating system, an application interface, text, a still image, video content, or the like.</p><p id="p-0023" num="0022">As described above, the communication network <b>16</b> may enable data communication between the remote computing devices <b>12</b> and one or more client devices <b>14</b>. In other words, the communication network <b>16</b> may enable user inputs to be communicated from a client device <b>14</b> to a remote computing device <b>12</b>. Additionally or alternatively, the communication network <b>16</b> may enable results of operations performed by the remote computing device <b>12</b> based on the user inputs to be communicated back to the client device <b>14</b>, for example, as image data to be displayed on its electronic display <b>22</b>.</p><p id="p-0024" num="0023">In fact, in some embodiments, data communication provided by the communication network <b>16</b> may be leveraged to make centralized hardware available to multiple users, such that hardware at client devices <b>14</b> may be reduced. For example, the remote computing devices <b>12</b> may provide data storage for multiple different client devices <b>14</b>, thereby enabling data storage (e.g., memory) provided locally at the client devices <b>14</b> to be reduced. Additionally or alternatively, the remote computing devices <b>12</b> may provide processing for multiple different client devices <b>14</b>, thereby enabling processing power provided locally at the client devices <b>14</b> to be reduced.</p><p id="p-0025" num="0024">Thus, in addition to the network interface <b>18</b>, the remote computing devices <b>12</b> may include processing circuitry <b>24</b> and one or more memory modules <b>26</b> (e.g., sub-systems) communicatively coupled via a data bus <b>28</b>. In some embodiments, the processing circuitry <b>24</b> and/or the memory modules <b>26</b> may be implemented across multiple remote computing devices <b>12</b>, for example, such that a first remote computing device <b>12</b> includes a portion of the processing circuitry <b>24</b> and the first memory module <b>26</b>A, while an Mth remote computing device <b>12</b> includes another portion of the processing circuitry <b>24</b> and the Mth memory module <b>26</b>M. Additionally or alternatively, the processing circuitry <b>24</b> and the memory modules <b>26</b> may be implemented in a single remote computing device <b>12</b>.</p><p id="p-0026" num="0025">In any case, the processing circuitry <b>24</b> may generally execute instructions to perform operations, for example, indicated by user inputs received from a client device <b>14</b>. Thus, the processing circuitry <b>24</b> may include one or more central processing units (CPUs), one or more graphics processing units (GPUs), one or more processor cores, or any combination thereof. In some embodiments, the processing circuitry <b>24</b> may additionally perform operations based on circuit connections formed (e.g., programmed) in the processing circuitry <b>24</b>. Thus, in such embodiments, the processing circuitry <b>24</b> may additionally include one or more application specific integrated circuits (ASICs), one or more field programmable logic arrays (FPGAs), or any combination of suitable processing devices.</p><p id="p-0027" num="0026">Additionally, a memory module <b>26</b> may provide data storage accessible to the processing circuitry <b>24</b>. For example, a memory module <b>26</b> may store data received from a client device <b>14</b>, data resulting from an operation performed by the processing circuitry <b>24</b>, data to be input to the operation performed by the processing circuitry <b>24</b>, instructions executable by the processing circuitry <b>24</b> to perform the operation, or any combination thereof. To facilitate providing data storage, a memory module <b>26</b> may include one or more memory devices <b>30</b> (e.g., chips or integrated circuits). In other words, the memory devices <b>30</b> may each include a tangible, non-transitory, computer-readable medium that stores data accessible to the processing circuitry <b>24</b>.</p><p id="p-0028" num="0027">Since hardware of the remote computing devices <b>12</b> may be utilized by multiple client devices <b>14</b>, at least in some instances, a memory module <b>26</b> may store data corresponding to different client devices <b>14</b>. In some embodiments, the data may be grouped and stored as data blocks <b>32</b>. In fact, in some embodiments, data corresponding with each client device <b>14</b> may be stored as a separate data block <b>32</b>. For example, the memory devices <b>30</b> in the first memory module <b>26</b>A may store a first data block <b>32</b>A corresponding with the first client device <b>14</b>A and an Nth data block <b>32</b>N corresponding with the Nth client device <b>14</b>N. One or more data blocks <b>32</b> may be stored within a memory die of the memory device <b>30</b>.</p><p id="p-0029" num="0028">Additionally or alternatively, in some embodiments, a data block <b>32</b> may correspond to a virtual machine (VM) provided to a client device <b>14</b>. In other words, as an illustrative example, a remote computing device <b>12</b> may provide the first client device <b>14</b>A a first virtual machine via the first data block <b>32</b>A and provide the Nth client device <b>14</b>N an Nth virtual machine via the Nth data block <b>32</b>N. Thus, when the first client device <b>14</b>A receives user inputs intended for the first virtual machine, the first client device <b>14</b>A may communicate the user inputs to the remote computing devices <b>12</b> via the communication network <b>16</b>. Based at least in part on the user inputs, the remote computing device <b>12</b> may retrieve the first data block <b>32</b>A, execute instructions to perform corresponding operations, and communicate the results of the operations back to the first client device <b>14</b>A via the communication network <b>16</b>.</p><p id="p-0030" num="0029">Similarly, when the Nth client device <b>14</b>N receives user inputs intended for the Nth virtual machine, the Nth client device <b>14</b>N may communicate the user inputs to the remote computing devices <b>12</b> via the communication network <b>16</b>. Based at least in part on the user inputs, the remote computing device <b>12</b> may retrieve the Nth data block <b>32</b>N, execute instructions to perform corresponding operations, and communicate the results of the operations back to the Nth client device <b>14</b>N via the communication network <b>16</b>. Thus, the remote computing devices <b>12</b> may access (e.g., read and/or write) various data blocks <b>32</b> stored in a memory module <b>26</b>.</p><p id="p-0031" num="0030">To facilitate improving access to stored data blocks <b>32</b>, a memory module <b>26</b> may include a memory controller <b>34</b> that controls storage of data in its memory devices <b>30</b>. In some embodiments, the memory controller <b>34</b> may operate based on circuit connections formed (e.g., programmed) in the memory controller <b>34</b>. Thus, in such embodiments, the memory controller <b>34</b> may include one or more application-specific integrated circuits (ASICs), one or more field programmable logic gate arrays (FPGAs), or any combination of suitable processing devices. In any case, as described above, a memory module <b>26</b> may include memory devices <b>30</b> that use different memory types, for example, which provide varying tradeoffs between data access speed and data storage density. Thus, in such embodiments, the memory controller <b>34</b> may control data storage across multiple memory devices <b>30</b> to facilitate leveraging the various tradeoffs, for example, such that the memory module <b>26</b> provides fast data access speed as well as high data storage capacity.</p><p id="p-0032" num="0031">To help illustrate, <figref idref="DRAWINGS">FIG. <b>2</b></figref> depicts an example of a memory module <b>26</b> including different types of memory devices <b>30</b>. In particular, the memory module <b>26</b> may include one or more non-volatile memory devices <b>30</b> and one or more volatile memory devices <b>30</b>. In some embodiments, the volatile memory devices <b>30</b> may be dynamic random-access memory (DRAM) and/or static random-access memory (SRAM). In other words, in such embodiments, the memory module <b>26</b> may include one or more DRAM devices (e.g., chips or integrated circuits), one or more SRAM devices (e.g., chips or integrated circuits), or any combination of suitable memory devices.</p><p id="p-0033" num="0032">Additionally, in some embodiments, the non-volatile memory devices <b>30</b> may be flash (e.g., NAND) memory, phase-change (e.g., 3D XPoint&#x2122;) memory, and/or FeRAM. In other words, in such embodiments, the memory module <b>26</b> may include one or more NAND memory devices, one or more 3D XPoint&#x2122; memory devices, one or more FeRAM memory devices, or any combination of suitable memory devices. In fact, in some embodiments, the non-volatile memory devices <b>30</b> may provide storage class memory (SCM), which, at least in some instance, may facilitate reducing implementation associated cost, for example, by obviating other non-volatile data storage devices in the computing system <b>10</b>.</p><p id="p-0034" num="0033">In any case, in some embodiments, the memory module <b>26</b> may include the memory devices <b>30</b> on a flat (e.g., front and/or back) surface of a printed circuit board (PCB). To facilitate data communication via the data bus <b>28</b>, the memory module <b>26</b> may include a bus interface <b>44</b> (bus I/F). For example, the bus interface <b>44</b> may include data pins (e.g., contacts) formed along an (e.g., bottom) edge of the printed circuit board. Thus, in some embodiments, the memory module <b>26</b> may be a single in-line memory module (SIMM), a dual in-line memory module (DIMM), or the like.</p><p id="p-0035" num="0034">Additionally, in some embodiments, the bus interface <b>44</b> may include logic that enables the memory module <b>26</b> to communicate via a communication protocol of the data bus <b>28</b>. For example, the bus interface <b>44</b> may control timing of data output from the memory module <b>26</b> to the data bus <b>28</b> and/or interpret data input to the memory module <b>26</b> from the data bus <b>28</b> in accordance with the communication protocol. Thus, in some embodiments, the bus interface <b>44</b> may be a double data rate fourth-generation (DDR4) interface, a double data rate fifth-generation (DDR5) interface, a peripheral component interconnect express (PCIe) interface, a non-volatile dual in-line memory module (e.g., NVDIMM-P) interface, a cache coherent interconnect for accelerators (CCIX) interface, or the like.</p><p id="p-0036" num="0035">In any case, as described above, a memory controller <b>34</b> may control data storage within the memory module <b>26</b>, for example, to facilitate improving data access speed and/or data storage efficiency by leveraging the various tradeoffs provided by memory types of the memory module <b>26</b>. Thus, as in the depicted example, the memory controller <b>34</b> may be coupled between the bus interface <b>44</b> and the memory devices <b>30</b> via one or more internal buses <b>37</b>, for example, provided as conductive traces formed on the printed circuit board. For example, the memory controller <b>34</b> may control whether a data block <b>32</b> is stored in the memory devices <b>30</b>. In other words, the memory controller <b>34</b> may transfer a data block <b>32</b> from a first memory device <b>30</b> into a second memory device <b>30</b> or vice versa.</p><p id="p-0037" num="0036">To facilitate data transfers, the memory controller <b>34</b> may include buffer memory <b>46</b>, for example, to provide temporary data storage. In some embodiments, the buffer memory <b>46</b> may include static random-access memory (SRAM) and, thus, may provide faster data access speed compared to the volatile memory devices <b>30</b> and the non-volatile memory devices <b>30</b>. The buffer memory <b>46</b> may be DRAM or FeRAM in some cases. Additionally, to facilitate accessing stored data blocks <b>32</b>, the memory module <b>26</b> may include an logical-to-physical address translation table (L2P table) and/or other parameters stored in the buffer memory <b>46</b>, a non-volatile memory device (e.g., a portion of memory devices <b>30</b>), a volatile memory device (e.g., a portion of memory devices <b>30</b>), a dedicated address map memory device (e.g., a portion of memory devices <b>30</b>), or any combination thereof. The other parameters may include a physical experience table that stores parameters and/or data related to operation of the memory module <b>26</b> and/or one or more components of the computing system <b>10</b>.</p><p id="p-0038" num="0037">In addition, the remote computing device <b>12</b> may communicate with a service processor and/or a service bus included in or separate from the processing circuitry <b>24</b> and/or the data bus <b>28</b>. The service processor, processing circuitry <b>24</b>, and/or the memory controller <b>34</b> may perform error detection operations and/or error correction operations (ECC), and may be disposed external from the remote computing device <b>12</b> such that error detection and error correction operations may continue if power to the remote computing device <b>12</b> is lost. For simplicity of description, the operations of the service processor are described as being included in and performed by the memory controller <b>34</b>, but it should be noted that in some embodiments the error correction operations or data recovery operations may be employed as functions performed by the service processor, processing circuitry <b>24</b>, or additional processing circuitry located internal or external to the remote computing device <b>12</b> or the client device <b>14</b>.</p><p id="p-0039" num="0038">The memory module <b>26</b> is depicted in <figref idref="DRAWINGS">FIG. <b>2</b></figref> as a single device that includes various components or submodules. In some examples, a remote computing device <b>12</b> may include one or several discrete components equivalent to the various devices, modules, and components that make up the memory module <b>26</b>. For instance, a remote computing device <b>12</b> may include non-volatile memory, volatile memory, and a controller positioned on one or several different chips or substrates. In other words, the features and functions of the memory module <b>26</b> need not be employed in a single module to achieve the benefits described herein.</p><p id="p-0040" num="0039">As described above, the memory module <b>26</b> may store information as data in the data blocks <b>32</b>. Die <b>48</b> of the memory module <b>26</b> may store the data blocks <b>32</b>. The data blocks <b>32</b> may be stored in one portion <b>50</b> of the die <b>48</b> or across multiple portions <b>50</b>. The portions <b>50</b> may store any amount of bits, and thus may be designed for a particular application of the memory module <b>26</b>. As an example, a portion <b>50</b> of memory may store 512 megabits (MB). In this way, a portion <b>50</b> of memory may be considered a memory cell, a memory bank, a memory partition, a portion of a memory module <b>26</b>, an entire memory module <b>26</b>, or the like. As depicted, however, for ease of discussion, the portion <b>50</b> may be a portion of memory that is considered smaller than a portion of memory allocated as the die <b>30</b>.</p><p id="p-0041" num="0040">When the processing circuitry <b>24</b> requests access to data stored in the memory module <b>26</b>, the processing circuitry <b>24</b> may issue a command. The command may include an instruction to perform a read operation, such as to operate the memory controller <b>34</b> to facilitate the retrieval of information stored in one of the portions <b>50</b>. Sometimes, the command includes an instruction to perform a write operation, such as to operate the memory controller <b>34</b> to facilitate the storage of information in one of the portions <b>50</b>. Other commands may be used to instruct the memory controller <b>34</b> to perform other operations.</p><p id="p-0042" num="0041">Over time, some physical addresses of the memory may be accessed more often than other physical addresses of the memory which may age some portions of the memory at a relatively faster rate than other portions of the memory. To elaborate, <figref idref="DRAWINGS">FIG. <b>3</b></figref> is a block diagram of the memory module of <figref idref="DRAWINGS">FIG. <b>2</b></figref> after a duration of time. The memory module <b>26</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref> may be affected by relatively uneven aging and/or uneven memory accesses, which is illustrated by the use of different shading to emphasize relatively more or relatively less accesses to the portions <b>50</b>. For example, the memory device <b>30</b>B was accessed relatively more than the memory device <b>30</b>D, and thus the portions <b>50</b> of the memory device <b>30</b>B may experience component aging at a faster rate than the portions <b>50</b> of the memory device <b>30</b>D.</p><p id="p-0043" num="0042">To compensate for under-optimized memory accesses, the memory controller <b>34</b> may adjust the L2P table based on commands issued by the host device (e.g., historical datasets that are indicative of traffic patterns associated with accesses of the memory controller <b>34</b>). For example, the memory controller <b>34</b> may adjust the L2P table with consideration for physical addresses that are accessed relatively more often than other physical addresses and/or for consideration for physical addresses accessed subsequent to other physical addresses relatively more often. By adjusting the L2P table based on the most frequently accessed physical addresses, the memory controller <b>34</b> may preemptively reduce or eliminate uneven aging and/or uneven memory accesses since the adjusted L2P table may make the traffic patterns between portions of memory relatively more even or equal. Additionally or alternatively, by adjusting the L2P table based on frequent subsequently accessed physical addresses, addresses that are expected to be accessed subsequent to each other may be used to address physical locations of memory that are independent from each other, such as different memory banks and/or different portions of memory.</p><p id="p-0044" num="0043">An example of the controller <b>34</b> operating to compensate for under-optimized memory access patterns is shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>. <figref idref="DRAWINGS">FIG. <b>4</b></figref> is an illustration of the memory controller <b>34</b> operating to adjust an original logical-to-physical table (L2P table) to preemptively compensate for frequent sequentially accessed addresses (e.g., memory access patterns). The memory controller <b>34</b> may receive as inputs various traffic datasets <b>60</b> (<b>60</b>A, <b>60</b>B, <b>60</b>C) and a current L2P table <b>62</b>. The current L2P table <b>62</b> may be an original L2P table or may be a previously adjusted L2P table that the memory controller <b>34</b> is currently referencing for memory operations (e.g., read operations, write operations).</p><p id="p-0045" num="0044">The memory controller <b>34</b> may use the traffic datasets <b>60</b> to dynamically alter the current L2P table <b>62</b> into a new L2P table <b>64</b>. To do so, the memory controller <b>34</b> may analyze one or more of the traffic datasets <b>60</b>. From the analysis, the memory controller <b>34</b> may learn which portions of the memory module <b>26</b> are frequency accessed sequential to each other. For example, the memory controller <b>34</b> may analyze one of the traffic datasets <b>60</b> to determine that a first portion <b>50</b> is frequently accessed right before a second portion <b>50</b> is accessed (e.g., sequentially accessed a threshold amount of times). In response to the memory controller <b>34</b> identifying portions of the memory module <b>26</b> that are accessed more often by a threshold amount of accesses and/or accessed sequentially by a threshold amount of accesses, the memory controller <b>34</b> may generate the new L2P table <b>64</b> to compensate for these access patterns.</p><p id="p-0046" num="0045">The memory controller <b>34</b> may alter L2P mapping of the memory module <b>26</b> to compensate for any undesired access patterns. For example, the memory controller <b>34</b> may change physical locations addressed by subsequently accessed logical address to reference locations on independently operating portions of memory (e.g., different memory banks, portions of memory on different memory die <b>50</b>). The memory controller <b>34</b> may interchange memory addresses, such that one or more frequently accessed addresses are replaced by less frequently accessed addresses, for example, the most frequently accessed address may be replaced by the least frequently accessed address, the second most accessed address may be replaced by the second least frequently accessed address, and so on.</p><p id="p-0047" num="0046">Portions <b>50</b> may be interchanged in some cases, but it should be understood that undesired memory access patterns may be compensated for at any suitable granularity of memory access, such as at the memory die <b>48</b> level. In some cases, the memory controller <b>34</b> may not be the controller that adjusts the memory access patterns in response to traffic datasets <b>60</b>. When the controller adjusting the memory access patterns is the processing circuitry <b>24</b>, or some other system-level controller (e.g., as opposed to memory module-level memory controller <b>34</b>), the current L2P table <b>62</b> may be adjusted to compensate for undesired access patterns between memory modules <b>26</b>.</p><p id="p-0048" num="0047">Each of the traffic datasets <b>60</b> may include real-time traffic data, test traffic data, historical traffic data, or the like. In this way, each of the traffic datasets <b>60</b> may be representative traffic samples for a given workload. Real-time traffic data may be information associated with memory read and write operations that is stored and analyzed by the memory controller <b>34</b> in real-time, or while the memory read and write operations are ongoing. Memory transactions (e.g., individual read or write operation occurrences) may be recorded by the memory controller <b>34</b> over time until a particular amount of memory transaction data is recorded to form a traffic dataset <b>60</b> (e.g., <b>60</b>A, <b>60</b>B, <b>60</b>C). The particular amount of memory transaction data may be defined by a threshold, such that the memory controller <b>34</b> monitors and records the memory transactions until a number of memory transactions is greater than or equal to a threshold amount of memory transactions. In response to the number of memory transactions being greater than or equal to the threshold amount, the memory controller <b>34</b> may associate the memory transactions as part of a traffic dataset <b>60</b>. In this way, the traffic dataset <b>60</b> may indicate real memory operations. When using test traffic data, memory transactions may be simulated or sample sets of data based on real memory transactions or typically expected memory traffic patterns may be used. Furthermore, in some cases, data values which may or may not mimic or represent real memory traffic patterns may be used as the test traffic data, or as typically expected memory traffic patterns. Furthermore, in some cases, the memory controller <b>34</b> may store memory transaction data over time, and use the stored memory transaction data as the traffic datasets <b>60</b> at a later time, for example several days or months later.</p><p id="p-0049" num="0048">In some embodiments, the memory controller <b>34</b> may also consider performance when adjusting the L2P table to improve physical address access distributions (e.g., reduce an amount of sequential accesses to a same portion <b>50</b> of memory). For example, the memory controller <b>34</b> may use a deep learning operation that uses read or write operation times as a cost (e.g., input) and the new L2P table <b>64</b> as a knob (e.g., variable) to adjust to optimize the cost. An example of the deep learning operation may include use of a long short-term memory (LSTM) artificial recurrent neural network. In this way, the memory controller <b>34</b> may test various eligible address assignments before selecting a final address assignment combination to be output as the final L2P table <b>64</b>. The memory controller <b>34</b> may determine an arrangement of address assignments that minimizes the cost while maximizing the reassignment of some addresses (in particular, the addresses that relatively more frequently access one-after-another or a duration of time sequentially such that processing of the second command waits until processing of the first command finishes). In this way, the memory controller <b>34</b> may consider memory access latencies (e.g., cost defined as read or write operations times) and reassignment percentages (e.g., a percentage of overused or relatively more sequentially-accessed portions of memory reassigned to relatively less sequentially-accessed portions of memory) when reassigning a physical address to a logical address. For example, the memory controller <b>34</b> may consider a comparison between a total duration of time used to perform one or more read and/or write operations for a first L2P table adjustment option and for a second L2P table adjustment option to determine which resulting L2P table corresponds to a more suitable adjustment and/or optimization.</p><p id="p-0050" num="0049">When the memory controller <b>34</b> uses the deep learning operation, the memory controller <b>34</b> may train the final L2P table <b>64</b> on one or more traffic datasets <b>60</b>. A subset of the traffic data of the traffic datasets <b>60</b> may be reserved for testing of the trained L2P table <b>64</b>, such as to verify performance of the adjusted logical-to-physical address assignments. Performance of the trained L2P table <b>64</b> may be tested to see how access speeds or access distributions changed after the training or adjustment. For example, the memory controller <b>34</b> may verify performance of the new L2P table <b>64</b> (e.g., trained L2P table <b>64</b>) by comparing performance results of the new L2P table <b>64</b> to previous performance results of the current L2P table <b>62</b> or to a default setting of the logical-to-physical assignments (e.g., an original L2P table for the memory controller <b>34</b>).</p><p id="p-0051" num="0050">Changes may be applied to the new L2P table <b>64</b> over time and/or as part of an iterative process, such as by adjusting a subset of logical addresses from a set of logical addresses to be adjusted. In this way, the memory controller <b>34</b> may perform one or more rounds of improvement to the current L2P table <b>62</b> such that the new L2P table <b>64</b> becomes incrementally improved over time. For example, a first current L2P table <b>62</b> may be adjusted and output as a new L2P table <b>64</b>, which is used at a next iteration as a second current L2P table <b>62</b>, adjusted, and output as a subsequent new L2P table <b>64</b>. Any number of iterations may be performed by the memory controller <b>34</b> to adjust the current L2P table <b>62</b> to compensate for sequential access patterns. In some cases, a threshold number of iterations may be defined and used to control a maximum number of iterations to be performed by the memory controller <b>34</b>.</p><p id="p-0052" num="0051">Since the memory controller <b>34</b> is monitoring accesses and access patterns, the memory controller <b>34</b> may preemptively compensate for memory access patterns before the memory access patterns affect components of the computing system <b>10</b>. For example, when the memory controller <b>34</b> adjusts the L2P table before the undesired access patterns affect the memory module <b>26</b>, sequential accesses may be preemptively (e.g., proactively) prevented since access to the portions of memory is proactively compensated. Preemptive adjustment of the L2P table may occur in response to the memory controller <b>34</b> determining that queued commands correspond to non-independent portions of memory and determining to adjust the L2P table to change, for example, a location in memory where to write data as to be able to be performed simultaneous to another memory access.</p><p id="p-0053" num="0052">To elaborate on example operations of the memory controller <b>34</b>, <figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flowchart of a process <b>76</b> to preemptively compensate for memory access patterns. The memory controller <b>34</b> is described below as performing the process <b>76</b>, but it should be understood that any suitable processing circuitry may additionally or alternatively perform the process <b>76</b>. Furthermore, although the process <b>76</b> is described below as being performed in a particular order, it should be understood that any suitable order may be used to perform individual operations of the process <b>76</b>.</p><p id="p-0054" num="0053">At block <b>78</b>, the memory controller <b>34</b> may receive a training dataset. The training dataset may include one or more traffic datasets <b>60</b> and/or one or more portions of one or more traffic datasets <b>60</b>. As described above, the traffic datasets <b>60</b> may include real-time traffic data, test traffic data, historical traffic data, or the like. In some cases, the memory controller <b>34</b> may divide the traffic datasets <b>60</b> and/or portions of data of the traffic datasets <b>60</b> into training datasets and into testing datasets.</p><p id="p-0055" num="0054">At block <b>80</b>, the memory controller <b>34</b> may use the training dataset and/or the traffic datasets <b>60</b> to determine one or more sequentially accessed logical addresses. The memory controller <b>34</b> may use thresholds to identify a trend of expected sequentially accessed logical addresses. For example, the memory controller <b>34</b> may use a threshold amount of memory accesses to determine when a sequential access pattern occurs enough times to correspond to an expected (e.g., preemptively anticipated) sequential access pattern since a relatively few amount (e.g., less than the threshold amount of occurrences) of sequential accesses of two or more logical addresses may not necessarily benefit from a reassignment or adjustment to the L2P table. The threshold may define a threshold number of memory accesses relative to other amounts of memory accesses. In this way, the memory controller <b>34</b> may identify a portion of the memory module <b>26</b> that is accessed a number of times greater than a threshold amount relative to a different portion of memory, and thus determine that a first portion of memory (e.g., first portion <b>50</b> on same die <b>48</b>) is accessed sequential to an access of a second portion of memory (e.g., second portion <b>50</b> on same die <b>48</b>).</p><p id="p-0056" num="0055">At block <b>82</b>, the memory controller <b>34</b> may generate a new L2P table <b>64</b> to compensate for the sequentially accessed logical addresses. In this way, the memory controller <b>34</b> may adjust the logical address to physical address assignments to cause sequentially referenced logical addresses to translate to physical addresses associated with independent portions of memory (e.g., different memory banks, different die <b>48</b>, different portions <b>50</b>). As discussed above, the memory controller <b>34</b> may interchange physical addresses assigned to logical addresses via the L2P table such that portions <b>50</b>, die <b>48</b>, and/or memory devices <b>30</b> are accessed in a different pattern according to the same logical addressing. The memory controller <b>34</b> may adjust the current L2P table <b>62</b> to generate the new L2P table <b>64</b> and/or generate a new L2P table <b>64</b> independent of an existing data structure storing the current L2P table <b>62</b>. The memory controller <b>34</b>, in some cases, may generate a set of eligible new L2P tables <b>64</b> and use operations of block <b>84</b> to evaluate the set of eligible new L2P tables <b>64</b> for selection at block <b>86</b>. To generate each of the set of eligible new L2P tables <b>64</b>, the memory controller <b>34</b> may systemically change one or more aspects (e.g., variables) of a first new L2P table <b>64</b> to test different options for the new L2P table <b>64</b>. In this way, the memory controller <b>34</b> may determine a suitable arrangement of the L2P table that minimizes read or write latencies while improving distributions of logical addresses relatively more frequently accessed of the memory (e.g., reassigning to physical addresses corresponding to independent portions of memory). Thus, the memory controller <b>34</b> may adjust the current L2P table <b>62</b> to test various eligible address assignments (e.g., set of eligible new L2P tables <b>64</b>) before selecting a final address assignment combination to be output as the final L2P table <b>64</b>. For example, in response to determining that the first portion <b>50</b> of memory is accessed sequential to the access of the second portion <b>50</b> of memory, the memory controller <b>34</b> may generate a multiple logical-to-physical tables that each include an assignment of a logical address originally corresponding to a physical address of the first portion <b>50</b> of memory to now correspond to a physical address of a third portion <b>50</b> of memory (e.g., a portion of memory independent from the second portion <b>50</b> of memory).</p><p id="p-0057" num="0056">At block <b>84</b>, the memory controller <b>34</b> may evaluate performance of the set of eligible new L2P tables <b>64</b>. The memory controller <b>34</b> may test each of the set of eligible new L2P tables <b>64</b> using a testing dataset (e.g., one or more portions of the traffic datasets <b>60</b>) to obtain performance metrics. Each performance metric for each of the set of eligible new L2P tables <b>64</b> may be compared to a corresponding performance metric for the current L2P table <b>62</b> or a default setting for the memory controller <b>34</b>. The comparison between the performance metrics may yield performance improvement metrics that indicate whether the performance did improve or did not improve (e.g., yielded faster memory accesses, yielded equal memory access speeds with relatively more even access distributions, yielded reduced read or write latencies). A performance improvement metric may indicate an improvement in performance of a particular L2P table of the set of eligible new L2P tables <b>64</b>. In this way, the memory controller <b>34</b> may evaluate the performances of the set of eligible new L2P tables <b>64</b> based at least in part on the performance improvement metrics to determine a suitable combination of changes to the current L2P table <b>62</b> that yield desirable or maximized performances. In some cases, the memory controller <b>34</b> may use a deep learning operation (e.g., LSTM) that uses read or write operation times (e.g., read or write latencies) as a cost and the new L2P table <b>64</b> as a knob (e.g., variable) to adjust to optimize the cost.</p><p id="p-0058" num="0057">At block <b>86</b>, the memory controller <b>34</b> may select a final new L2P table <b>64</b> from the set of eligible new L2P tables <b>64</b> and may use the final new L2P table <b>64</b> in the buffer memory <b>46</b>. Thus, a L2P table may be selected from the multiple L2P tables generated and tested at block <b>84</b> to evaluate memory access latencies for each of the multiple L2P tables. The current L2P table <b>62</b>, in some cases, may be retained in memory as a backup L2P table and/or for future reference. The new L2P table <b>64</b> may be written to the buffer memory <b>46</b> to overwrite the current L2P table <b>62</b>. In this way, the memory controller <b>34</b> replaces the current L2P table <b>62</b> and uses the new L2P table <b>64</b> for future memory accesses (e.g., memory controller <b>34</b> uses the new L2P table <b>64</b>). For example, the memory controller <b>34</b> may store a first L2P table (e.g., current L2P table <b>62</b>) as an original logical-to-physical table in the buffer memory <b>46</b>, generate a second L2P table (e.g., new L2P table <b>64</b>) using the first L2P table after adjusting the first L2P table, such as at block <b>82</b>, and may write over the first L2P table in the buffer memory <b>46</b> with the second L2P table such that the memory controller <b>34</b> uses the second L2P table when performing logical-to-physical address translations.</p><p id="p-0059" num="0058">It is noted that although, in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the client devices <b>14</b> are depicted as communicatively coupled to the remote computing devices <b>12</b>, in some embodiments, the systems and methods described above may be used in a memory controller <b>34</b> of the client device <b>14</b>. The techniques described herein may be used in combination with a variety of memory types and computing structures to achieve the benefits described herein.</p><p id="p-0060" num="0059">In some cases, the memory controller <b>34</b> may use traffic datasets <b>60</b> that include an indication of a workload type. In this way, resulting new L2P tables <b>64</b> may be selected based at least in part on the workload type that operates the memory controller <b>34</b>. For example, some memory (e.g., memory chips) have a number of memory banks able to operate in parallel, such as sixteen memory banks that sometimes operate in parallel. The current L2P tables <b>62</b> for different workloads may be optimized, or designed, to increase the probability that successive commands are for different memory banks, and thus may be executed in parallel, yielding performance improvements of the memory (e.g., faster speed of completing memory transactions). Examples of workflows may include different software applications used by a same computing device and that access, during execution of the software application, the memory module <b>26</b>. When designing L2P tables for a particular workload, logical addresses may originally reference a physical address of a portion <b>50</b> of memory disposed or located within a same memory die <b>48</b> or a same memory device <b>30</b>. During an example workflow, a memory controller <b>34</b> may sequentially access logical addresses that reference portions <b>50</b> of memory not able to be processed in parallel (e.g., same portions of memory, same memory die <b>48</b>, same memory device <b>30</b>. These sequential access patterns involving the logical addresses may be identified, and the logical addresses may be reassigned physical addresses corresponding to portions <b>50</b> disposed on or within different memory die <b>48</b> or different memory devices <b>30</b>, such that the logical addresses are assigned to physical addresses referencing independent portions of memory. In this way, the workload that sequentially accesses the logical addresses may cause the memory controller <b>34</b> to access the different portions <b>50</b> of memory in parallel since the physical addresses reference portions <b>50</b> of memory able to be processed in parallel (e.g., since the physical addresses reference portions <b>50</b> of memory that operate independent). In some embodiments, a workload type may be identified to the memory controller <b>34</b>, which is able to preload the selected new L2P table <b>64</b> in response to the type of workload. For example, L2P tables <b>64</b> may be stored and loaded by a computing device (e.g., memory controller <b>34</b> of the computing device) in response to a corresponding software application being executed on the computing device.</p><p id="p-0061" num="0060">In some embodiments, the refinement of a current L2P table <b>62</b> (e.g., to generate a new L2P table <b>64</b>) may be performed after a time of manufacturing during operation of the computing system <b>10</b>. In this way, the current L2P table <b>62</b> may update over time and over the device life of the computing system <b>10</b>. In some cases, the refinement of the L2P table may be performed while the computing system <b>10</b> is in a reduced power mode, is idle, is offline, or is otherwise not accessing the memory module <b>26</b>, such that the new L2P table <b>64</b> may be used by the memory controller <b>34</b> without interruption to an ongoing memory process. For example, operations of <figref idref="DRAWINGS">FIG. <b>5</b></figref> may be performed by the memory controller <b>34</b> when the memory controller <b>34</b> is not in the middle of performing another memory operation (e.g., read operation, write operation, refresh operation).</p><p id="p-0062" num="0061">In some embodiments, determining relatively more accessed portions of the memory module <b>26</b> and/or determining a final adjustment to the L2P table may include the memory controller <b>34</b> performing a difference analysis to identify differences in access amounts that are greater than a threshold amount of accesses. For example, a first portion <b>50</b> may have been accessed 10 times, a second portion <b>50</b> may have been accessed 50 times, and the threshold amount of accesses may be a difference equaling 15 accesses. Since the difference between accesses of the first portion <b>50</b> and the second portion <b>50</b> is 40 and the threshold amount of accesses corresponds to a difference equaling 15 accesses, the memory controller <b>34</b> may identify the second portion <b>50</b> as relatively more overused. In this way, the memory controller <b>34</b> may sometimes adjust the logical address to physical address assignments to cause the less accessed portions of the memory module <b>26</b> to be accessed relatively more frequent.</p><p id="p-0063" num="0062">Additionally or alternatively, in some embodiments, identifying overused portions of the memory module <b>26</b>, at block <b>80</b> of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, may include identifying physical addresses of the overused portions and/or relatively less used portions of the memory module <b>26</b>. For example, the memory controller <b>34</b> may determine that the first portion <b>50</b> is more frequently accessed than the second portion <b>50</b> at least in part by analyzing a first number of memory transactions (e.g., memory read operation, memory write operation, memory refresh operation) involving the first portion <b>50</b> during a time period, analyzing a second number of memory transactions involving the second portion <b>50</b> during the same time period, and determining that the first number is greater than the second number. The time period may be for the same duration of time, such that the time period shares a starting and ending time, and/or the time period may be a same duration of time relative to a start time (e.g., equal durations periods of time that do not necessarily start at a same start time). The time periods may be a monitoring period to use when comparing accesses to portions of memory and/or analyzing traffic patterns associated with memory accesses. It is noted that in some embodiments, counters may be used to count physical address accesses. A counter may count up to a threshold value of accesses or may count down from the threshold value of accesses in response to a particular physical address being accessed. Any suitable type and number of counter may be used, for example, each address may correspond to a counter and/or sets of addresses may correspond to a counter.</p><p id="p-0064" num="0063">Technical effects of the present disclosure may include improving memory operations by compensating for memory access patterns (e.g., uneven access patterns, sequential logical address access patterns) of a memory. A memory controller may identify logical addresses that are frequently accessed sequential to each other and reassign, for the logical address corresponding to the identified sequentially accessed logical addresses, to a different physical address. This may cause the logical address to be reassigned a physical address corresponding to a portion of memory that is less used and/or that correspond to independent portions of memory, thereby permitting parallel memory access operation of the independent portions of memory. The reassignment of logical addresses to new or adjusted physical addresses may be stored in a new and/or updated L2P table. A memory controller managing accesses to memory according to the new and/or updated L2P table may improve performance of the memory system.</p><p id="p-0065" num="0064">With these technical effects in mind, multiple memory devices may be included on a memory module, thereby enabling the memory devices to be communicatively coupled to the processing circuitry as a unit. For example, a dual in-line memory module (DIMM) may include a printed circuit board (PCB) and multiple memory devices. Memory modules respond to commands from a memory controller communicatively coupled to a client device or a host device via a communication network. Or in some cases, a memory controller may be used on the host-side of a memory-host interface; for example, a processor, microcontroller, FPGA, ASIC, or the like may each include a memory controller. This communication network may enable data communication there between and, thus, the client device to utilize hardware resources accessible through the memory controller. Based at least in part on user input to the client device, processing circuitry of the memory controller may perform one or more operations to facilitate the retrieval or transmission of data between the client device and the memory devices. Data communicated between the client device and the memory devices may be used for a variety of purposes including, but not limited to, presentation of a visualization to a user through a graphical user interface (GUI) at the client device, processing operations, calculations, or the like. Thus, with this in mind, the above-described improvements to memory controller operations and memory writing operations may manifest as improvements in visualization quality (e.g., speed of rendering, quality of rendering), improvements in processing operations, improvements in calculations, or the like.</p><p id="p-0066" num="0065">The specific embodiments described above have been shown by way of example, and it should be understood that these embodiments may be susceptible to various modifications and alternative forms. It should be further understood that the claims are not intended to be limited to the particular forms disclosed, but rather to cover all modifications, equivalents, and alternatives falling within the spirit and scope of this disclosure.</p><p id="p-0067" num="0066">The techniques presented and claimed herein are referenced and applied to material objects and concrete examples of a practical nature that demonstrably improve the present technical field and, as such, are not abstract, intangible or purely theoretical. Further, if any claims appended to the end of this specification contain one or more elements designated as &#x201c;means for [perform]ing [a function] . . . &#x201d; or &#x201c;step for [perform]ing [a function] . . . &#x201d;, it is intended that such elements are to be interpreted under 35 U.S.C. 112(f). However, for any claims containing elements designated in any other manner, it is intended that such elements are not to be interpreted under 35 U.S.C. 112(f).</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method, comprising:<claim-text>determining, via a memory controller, a first assignment that causes a first logical address to correspond to a first physical address for a first memory portion;</claim-text><claim-text>determining, via the memory controller, a first memory access latency associated with the first assignment;</claim-text><claim-text>generating, via the memory controller, a second assignment that causes the first logical address to correspond to a second physical address for a second memory portion and a third assignment that causes the first logical address to correspond to a third physical address for a third memory portion; and</claim-text><claim-text>selecting the second assignment or the third assignment based on the first memory access latency, a second memory access latency associated with the second assignment, and a third memory access latency associated with the third assignment.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first memory access latency is determined based on a training dataset generated based on the first assignment.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, comprising determining memory access patterns based on the training dataset, wherein the memory access patterns comprise sequential memory access patterns.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the training dataset comprises at least a portion of one or more traffic datasets.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the at least a portion of the one or more traffic datasets comprise real-time traffic datasets, test traffic datasets, historical traffic datasets, or any combination thereof.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein selecting the second assignment or the third assignment comprises determining, via the memory controller, whether the training dataset indicates that a first number of memory accesses associated with the second assignment, a second number of memory accesses associated with the third assignment, or both, exceed a threshold number of memory accesses.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, comprising:<claim-text>in response to determining that the first number of memory accesses exceeds the threshold number, selecting the third assignment;</claim-text><claim-text>in response to determining that the second number of memory accesses exceeds the threshold number, selecting the second assignment; or</claim-text><claim-text>in response to determining that the first number of memory accesses and the second number of memory accesses both exceed the threshold number, generating and selecting a fourth assignment.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. A device, comprising:<claim-text>a memory comprising a first portion, a second portion, and a third portion; and</claim-text><claim-text>a memory controller configured to:<claim-text>determine, via the memory controller, a first memory access latency associated with a first assignment, wherein the first assignment causes a first logical address to correspond to a first physical address of the first portion;</claim-text><claim-text>generate, via the memory controller, a second assignment that causes the first logical address to correspond to a second physical address of the second portion, and a third assignment that causes the first logical address to correspond to a third physical address of the third portion; and</claim-text><claim-text>select the second assignment or the third assignment based on the first memory access latency, a second memory access latency associated with the second assignment, and a third memory access latency associated with the third assignment.</claim-text></claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The device of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the first assignment corresponds to a first logical-to-physical table, the second assignment corresponds to a second logical-to-physical table, and the third assignment corresponds to a third logical-to-physical table.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The device of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the memory controller comprises a buffer memory configured to store the first logical-to-physical table, the second logical-to-physical table, and the third logical-to-physical table.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The device of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein selecting the second assignment or the third assignment comprises causing the second logical-to-physical table or the third logical-to-physical table to overwrite the first logical-to-physical table in the buffer memory.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The device of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the memory controller is configured to:<claim-text>select the second assignment based at least in part on determining that the first portion is accessed more frequently than the second portion; or</claim-text><claim-text>select the third assignment based at least in part on determining that the first portion is accessed more frequently than the third portion.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The device of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the first memory access latency, the second memory access latency, the third memory access latency, or any combination thereof is based on address access patterns of the first portion, the second portion, and the third portion.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The device of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the memory controller is configured to determine the address access patterns based on at least a portion of a traffic dataset.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The device of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the memory controller is configured to utilize a deep learning algorithm to obtain the address access patterns.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The device of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the memory comprises a first die comprising the first portion and a second die comprising the second portion.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. Tangible, non-transitory, computer-readable media comprising instructions configured to cause one or more processors to:<claim-text>determine a first memory access latency associated with first logical-to-physical table;</claim-text><claim-text>generate a plurality of logical-to-physical tables;</claim-text><claim-text>determine a plurality of memory access latencies associated with at least a portion of the plurality of logical-to-physical tables;</claim-text><claim-text>select a second logical-to-physical table from the plurality of logical-to-physical tables based on a second memory access latency associated with the second logical-to-physical table; and</claim-text><claim-text>store, in a memory buffer, the second logical-to-physical table to overwrite the first logical-to-physical table.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The tangible, non-transitory, computer-readable media of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein causing the one or more processors to select the second logical-to-physical table comprises determining that the first memory access latency is greater than the second memory access latency.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The tangible, non-transitory, computer-readable media of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein causing the one or more processors to generate and select the plurality of logical-to-physical tables comprises repeating the generation and selection of the plurality of logical-to-physical tables until a number of iterations corresponding to a number of repetitions is greater than or equal to a threshold number of iterations.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The tangible, non-transitory, computer-readable media of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein causing the one or more processors to overwrite the first logical-to-physical table comprises:<claim-text>determining, in the first logical-to-physical table, that a first logical address corresponds to a first physical address for a first memory portion;</claim-text><claim-text>determining, in the first logical-to-physical table, that a second logical address corresponds to a second physical address for a second memory portion; and</claim-text><claim-text>generating the second logical-to-physical table at least in part by reassigning the first logical address to correspond to the second physical address and by reassigning the second logical address to correspond to the first physical address.</claim-text></claim-text></claim></claims></us-patent-application>