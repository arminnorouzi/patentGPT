<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230000301A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230000301</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17364580</doc-number><date>20210630</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>47</class><subclass>L</subclass><main-group>11</main-group><subgroup>40</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>05</class><subclass>D</subclass><main-group>1</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>47</class><subclass>L</subclass><main-group>11</main-group><subgroup>4011</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>05</class><subclass>D</subclass><main-group>1</main-group><subgroup>0016</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>05</class><subclass>D</subclass><main-group>1</main-group><subgroup>0088</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>47</class><subclass>L</subclass><main-group>2201</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>05</class><subclass>D</subclass><main-group>2201</main-group><subgroup>0203</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">USER INTERFACES FOR AUTONOMOUS CLEANING ROBOTS</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="obligated-assignee"><addressbook><orgname>iRobot Corporation</orgname><address><city>Bedford</city><state>MA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Arora</last-name><first-name>Shipra</first-name><address><city>Cambridge</city><state>MA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Stern</last-name><first-name>Lauren D.</first-name><address><city>Medford</city><state>MA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Schriesheim</last-name><first-name>Benjamin H.</first-name><address><city>Watertown</city><state>MA</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>White</last-name><first-name>Cory</first-name><address><city>Newburyport</city><state>MA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Some autonomous cleaning robots include a drive configured to maneuver the autonomous cleaning robot about a floor surface. The robots include a cleaning system to clean the floor surface as the autonomous cleaning robot is maneuvered about the floor surface. The robots include a robot button positioned on the autonomous cleaning robot. The robots include a controller in electrical communication with the drive and the robot button. The controller is configured to perform operations including selecting a behavior of the autonomous cleaning robot from a plurality of behaviors of the autonomous cleaning robot responsive to a duration of actuation of the robot button and causing the autonomous mobile robot to initiate the behavior.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="128.95mm" wi="149.27mm" file="US20230000301A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="142.58mm" wi="151.30mm" file="US20230000301A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="178.90mm" wi="163.58mm" file="US20230000301A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="234.53mm" wi="118.03mm" orientation="landscape" file="US20230000301A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="206.84mm" wi="189.48mm" file="US20230000301A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="213.95mm" wi="158.16mm" file="US20230000301A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="190.75mm" wi="160.95mm" file="US20230000301A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="220.90mm" wi="184.32mm" file="US20230000301A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="240.79mm" wi="182.80mm" orientation="landscape" file="US20230000301A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="210.57mm" wi="143.93mm" file="US20230000301A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="169.84mm" wi="157.23mm" orientation="landscape" file="US20230000301A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="187.88mm" wi="139.11mm" orientation="landscape" file="US20230000301A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="184.07mm" wi="178.73mm" orientation="landscape" file="US20230000301A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="178.65mm" wi="147.66mm" file="US20230000301A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0002" num="0001">This specification relates to user interfaces for autonomous cleaning robots.</p><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">Autonomous mobile robots include autonomous cleaning robots that autonomously perform cleaning tasks within an environment, e.g., a home. Many kinds of cleaning robots are autonomous to some degree and in different ways. A cleaning robot can include a controller configured to autonomously navigate the robot about an environment such that the robot can ingest debris as it moves.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0004" num="0003">An autonomous mobile robot can clean a home during a cleaning mission, e.g., by vacuuming, mopping, or performing some other cleaning operation in the home. The robots described herein include a one-button user interface to allow users to interact with the robot and cause the robot to initiate different behaviors (e.g., begin cleaning, return to a docking station, etc.). In particular, the one-button user interface provides a multi-function button that can be operated in different ways to cause the robot to initiate different behaviors. For example, by distinguishing between different button states (e.g., on vs. off), a duration of time that the button is held in a particular state (e.g., by a user pressing the button), and/or a sequence of button actuations, the robot can select a desired behavior of the robot. In some examples, if a user presses and releases the button within 2 seconds, a first behavior is performed by the robot (e.g., resume a cleaning job), while if the user presses and releases the button for 2-5 seconds a second, different, behavior is performed by the robot (e.g., cancel the cleaning job). In these examples, the robot selects the behavior based on the duration of the actuation of the button and initiates the behavior.</p><p id="p-0005" num="0004">Advantages of the implementations described in this disclosure may include, but are not limited to, those described below and elsewhere in this disclosure.</p><p id="p-0006" num="0005">A one-button user interface can enable a simplified user experience compared to robots with 2, 3, or more buttons. For example, a user may be able to more intuitively operate a robot with a one-button user interface that includes one button as opposed to a user interface with multiple buttons.</p><p id="p-0007" num="0006">A robot that selects a behavior of the robot based on a duration of the actuation of the button enables the robot to distinguish between short taps (e.g., less than 2 seconds), short holds (e.g., between 2-5 seconds), holds (e.g., at 7 seconds), and long holds (e.g., at 9 seconds). This feature can be combined with the state of the robot to distinguish between when the button is held for a certain period of time (e.g., regardless of whether the button is released) and when the button is held for a certain period of time and then released. This allows for a robot to select from multiple behaviors from operation of a single button and further allows for an increased number of behaviors that can be selected by the robot compared to user interfaces that do not account for both state of the actuation and duration of the actuation.</p><p id="p-0008" num="0007">A robot that selects a behavior of the robot based on sensors of the robot enables the robot to distinguish from more behaviors. In addition to enabling the single button of the robot to be operated to initiate different behaviors, selecting the behavior based on the sensors also enables the robot to utilize sensors that are already on the robot for multiple purposes. This reduces weight of the robot and design complexity.</p><p id="p-0009" num="0008">A robot that selects a behavior of the robot based on a current state of the robot further increases the number of behaviors that can uniquely be selected by the robot. By accounting for the current state of the robot to select the behavior to be initiated, the robot can be operated by a user in a more intuitive fashion. For example, the current state of the robot can be indicative of a condition of the robot that is observable (e.g., a docked state, an idle state, a moving state, or other user-observable state, etc.), and the selected behavior can intuitively correspond to the observed condition so that the behavior of the robot in response to user actuation of the button is intuitive to the user. This can also be expanded such that the robot selects a behavior based on whether the robot is docked on a docking station and/or a current battery state of the robot.</p><p id="p-0010" num="0009">A one-button user interface combined with a light ring of the robot that provides visual feedback to the user enables the robot to indicate to the user when a certain threshold is about to be reached. For example, as a proportion of the light ring is illuminates, the user may be more inclined to continue holding the button until the entire light ring is illuminated. This provides a visual indication to a user of the different timing thresholds for the button duration (e.g., 2 seconds, 3 seconds, 5 seconds, 7 seconds, etc.) so that, for example, the user knows when they are providing a short tap, a short hold, a hold, or a long hold.</p><p id="p-0011" num="0010">In one aspect, an autonomous cleaning robot includes a drive configured to maneuver the autonomous cleaning robot about a floor surface. The autonomous cleaning robot includes a cleaning system to clean the floor surface as the autonomous cleaning robot is maneuvered about the floor surface. The autonomous cleaning robot includes a robot button positioned on the autonomous cleaning robot. The autonomous cleaning robot includes a controller in electrical communication with the drive and the robot button. The controller is configured to perform operations including selecting a behavior of the autonomous cleaning robot from a plurality of behaviors of the autonomous cleaning robot responsive to a duration of actuation of the robot button. The controller is configured to perform operations comprising causing the autonomous mobile robot to initiate the behavior.</p><p id="p-0012" num="0011">In another aspect, a method of operating an autonomous cleaning robot includes receiving, by a processor of an autonomous cleaning robot, information generated from one or more sensors of the autonomous cleaning robot. The method includes selecting, by the processor, a behavior of the autonomous cleaning robot from a plurality of behaviors of the autonomous cleaning robot based on at least a duration of actuation of a robot button of the autonomous cleaning robot and the information. The method includes generating, by the processor, a control signal based on the behavior. The method includes causing the autonomous mobile robot to initiate the behavior using the control signal.</p><p id="p-0013" num="0012">In another aspect, a non-transitory computer-readable storage medium includes at least one program for execution by a processor of an autonomous cleaning robot. The at least one program includes instructions which, when executed by the processor, cause the autonomous cleaning robot to perform operations including selecting, by the processor, a behavior of the autonomous cleaning robot from a plurality of behaviors of the autonomous cleaning robot based on at least a duration of actuation of a robot button of the autonomous cleaning robot. The at least one program includes instructions which, when executed by the processor, cause the autonomous cleaning robot to perform operations including generating, by the processor, a control signal based on the behavior. The at least one program includes instructions which, when executed by the processor, cause the autonomous cleaning robot to perform operations including causing the autonomous cleaning robot to initiate the behavior using the control signal.</p><p id="p-0014" num="0013">In some implementations, selecting the behavior from the plurality of behaviors of the autonomous cleaning robot comprises selecting among at least three different behaviors of the autonomous cleaning robot.</p><p id="p-0015" num="0014">In some implementations, selecting the behavior from the plurality of behaviors of the autonomous cleaning robot is further based on sensor information generated from one or more sensors of the autonomous cleaning robot, the sensor information being independent from the actuation of the robot button. In some cases, the sensor information represents one or more of: an actuation of a bumper of the autonomous cleaning robot, a position of a wheel of the drive of the autonomous cleaning robot, a presence of a bin attached to the autonomous cleaning robot, or whether the autonomous cleaning robot is docked with a docking station.</p><p id="p-0016" num="0015">In some implementations, selecting the behavior from the plurality of behaviors of the autonomous cleaning robot is further based on one or more current states of the autonomous cleaning robot, the one or more current states comprising one or more of the following states: a sleep state, an awake state, an idle state, a cleaning state, a docking state, a child-lock behavior state, or a reboot state.</p><p id="p-0017" num="0016">In some implementations, selecting the behavior from the plurality of behaviors of the autonomous cleaning robot is further based on a sequence of the actuation of the robot button, the sequence comprising a number of times the robot button is actuated within a time interval.</p><p id="p-0018" num="0017">In some implementations, the plurality of behaviors comprise a first behavior of the autonomous cleaning robot and a second behavior of the autonomous cleaning robot, wherein selecting the behavior comprises selecting the behavior based on the duration such that the selected behavior corresponds to the first behavior if the robot button is actuated for a first duration and the selected behavior corresponds to the second behavior if the robot button is actuated for a second duration longer than the first duration.</p><p id="p-0019" num="0018">In some implementations, the robot button comprises a light ring configured to illuminate a portion of a top surface of the autonomous cleaning robot, wherein the controller is configured to control a length of the illuminated portion based on the duration of the actuation of the robot button.</p><p id="p-0020" num="0019">In some implementations, the controller is further configured to perform operations including receiving information generated from one or more sensors of the autonomous cleaning robot. In some cases, selecting the behavior from the plurality of behaviors of the autonomous cleaning robot can be further based on the information. In some cases, the information represents at least one of: an actuation of a bumper of the autonomous cleaning robot, a position of a wheel of the drive of the autonomous cleaning robot, a presence of a bin attached to the autonomous cleaning robot, or whether the autonomous cleaning robot is docked with a docking station.</p><p id="p-0021" num="0020">In some implementations, the plurality of behaviors include behaviors associated with one or more of the following states: a sleep state, an awake state, a cleaning state, a docking state, a child-lock behavior, and a reboot state.</p><p id="p-0022" num="0021">In some implementations, the selected behavior corresponds to a first behavior if the robot button is actuated for a first duration, and the selected behavior corresponds to a second behavior if the robot button is actuated for a second duration longer than the first duration. In some cases, the first duration is less than 2 seconds and/or the second duration is between 2 and 5 seconds.</p><p id="p-0023" num="0022">In some implementations, selecting the behavior from the plurality of behaviors of the autonomous cleaning robot is further based on one or more current states of the autonomous cleaning robot. In some cases, the one or more current states include at least one of: a sleep state, an awake state, an idle state, a cleaning state, a docking state, a child-lock behavior, or a reboot state.</p><p id="p-0024" num="0023">In some cases, when the one or more current states include an awake state and a docked state, the first behavior is a cleaning behavior and the second behavior is an evacuation behavior.</p><p id="p-0025" num="0024">In some cases, the cleaning behavior is associated with starting a cleaning job and/or the evacuation behavior associated with evacuating a bin of the autonomous cleaning robot.</p><p id="p-0026" num="0025">In some cases, when the one or more current states include an awake state and an idle state, the first behavior is a cleaning behavior and the second behavior is a docking behavior, the docking behavior associated with a return of the autonomous cleaning robot to a docking station.</p><p id="p-0027" num="0026">In some implementations, selecting the behavior from the plurality of behaviors of the autonomous cleaning robot is further based on a number of times the robot button is actuated within a time interval.</p><p id="p-0028" num="0027">In some implementations, selecting the behavior from the plurality of behaviors of the autonomous cleaning robot is further based on a sequence of actuation of the robot button.</p><p id="p-0029" num="0028">In some implementations, the autonomous cleaning robot includes a bumper and a bumper sensor configured to generate a signal in response to actuation of the bumper. In some cases, selecting the behavior from the plurality of behaviors of the autonomous cleaning robot is further based on the signal. In some cases, the plurality of behaviors includes a first behavior and a second behavior. In some cases, the selected behavior corresponds to the first behavior if the robot button is actuated for a first duration and the selected behavior corresponds to the second behavior if the robot button is actuated for a second duration.</p><p id="p-0030" num="0029">In some cases, the first behavior is associated with a sleep state of the autonomous cleaning robot and the second behavior is associated with a docked state of the autonomous cleaning robot.</p><p id="p-0031" num="0030">In some implementations, the drive includes a wheel movable between a first position in which the wheel is extended relative to a bottom surface of the autonomous cleaning robot and a second position in which the wheel is retracted relative to the bottom surface of the autonomous cleaning robot. In some cases, selecting the behavior from the plurality of behaviors of the autonomous cleaning robot is further based on a position of the wheel.</p><p id="p-0032" num="0031">In some cases, only when the position of the wheel corresponds to the first position, the plurality of behaviors includes a first behavior and a second behavior. In some cases, the selected behavior corresponds to the first behavior if the robot button is actuated for a first duration once within a time interval. In some cases, the first duration is 3 seconds and/or the time interval is 5 seconds. In some cases, the selected behavior corresponds to the second behavior if the robot button is actuated a predetermined number of times within the time interval. In some cases, the predetermined number of times is 3 times.</p><p id="p-0033" num="0032">In some cases, the first behavior is associated with powered-off behavior and the second behavior is associated with child-lock behavior.</p><p id="p-0034" num="0033">In some implementations, the autonomous cleaning robot includes a bin removably attached to the autonomous cleaning robot and a bin sensor configured to generate a signal in response to a presence of the bin attached the autonomous cleaning robot. In some cases, selecting the behavior from the plurality of behaviors of the autonomous cleaning robot is further based on the signal.</p><p id="p-0035" num="0034">In some cases, the selected behavior is a reset behavior if a bin is attached to the autonomous cleaning robot and the robot button is actuated for a first duration followed by a second actuation of the robot button. In some cases, the selected reset behavior is associated with a factory reset of the autonomous cleaning robot. In some cases, the first duration is 7 seconds.</p><p id="p-0036" num="0035">In some implementations, the controller is further configured to perform operations including receiving a signal generated in response to the autonomous cleaning robot being docked in a docking station. In some cases, selecting the behavior from the plurality of behaviors of the autonomous cleaning robot is further based on the signal. In some cases, the behavior is associated with evacuating a bin of the autonomous cleaning robot. In some cases, the duration is between 2 and 5 seconds.</p><p id="p-0037" num="0036">In some implementations, initiating the behavior includes initiating the behavior in response to a release of the actuation of the robot button.</p><p id="p-0038" num="0037">In some implementations, initiating the behavior includes initiating the behavior prior to a release of the actuation of the robot button.</p><p id="p-0039" num="0038">In some implementations, initiating the behavior includes setting a current state of the autonomous cleaning robot to be an awake state in response to the current state of the autonomous cleaning robot being a sleep state and in response to the actuation of the robot button.</p><p id="p-0040" num="0039">In some implementations, the controller is further configured to perform operations including receiving information regarding a current state of the autonomous cleaning robot. In some cases, selecting the behavior of the autonomous cleaning robot is further based on the current state.</p><p id="p-0041" num="0040">In some implementations, the autonomous cleaning robot has a single button corresponding to the robot button.</p><p id="p-0042" num="0041">In some implementations, the robot button includes a light ring configured to illuminate a portion of a top surface of the autonomous cleaning robot. In some cases, the controller is configured to control a length of the illuminated portion based on the duration of the actuation of the robot button.</p><p id="p-0043" num="0042">In some implementations, the autonomous cleaning robot includes a light ring configured to illuminate a portion of a continuous annular surface positioned around the robot button. In some cases, the controller is configured to control a length of the illuminated portion based on the duration of the actuation of the robot button.</p><p id="p-0044" num="0043">The details of one or more implementations of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other potential features, aspects, and advantages will become apparent from the description, the drawings, and the claims.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. <b>1</b>A</figref> is a perspective view of a user interacting with a button on an autonomous cleaning robot.</p><p id="p-0046" num="0045"><figref idref="DRAWINGS">FIGS. <b>1</b>B-<b>1</b>D</figref> are top, side, and bottom views of the autonomous cleaning robot of <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a schematic of the autonomous cleaning robot of <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0048" num="0047"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flowchart of a process of selecting behavior of the autonomous cleaning robot of <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flowchart of examples of selecting behavior of the autonomous cleaning robot of <figref idref="DRAWINGS">FIG. <b>1</b></figref> based on a release of the button and a duration of the button actuation.</p><p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flowchart of examples of selecting behavior of the autonomous cleaning robot of <figref idref="DRAWINGS">FIG. <b>1</b></figref> based on a press of the button and a duration of the button actuation.</p><p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flowchart of examples of selecting behavior of the autonomous cleaning robot of <figref idref="DRAWINGS">FIG. <b>1</b></figref> based on a sequence of button actuations.</p><p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. <b>7</b>A</figref> is a schematic of a light ring of the autonomous cleaning robot of <figref idref="DRAWINGS">FIG. <b>1</b></figref> transitioning from an awake state to initiating a mission.</p><p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. <b>7</b>B</figref> is a schematic of a light ring of the autonomous cleaning robot of <figref idref="DRAWINGS">FIG. <b>1</b></figref> transitioning from a mission state to returning to a docking station.</p><p id="p-0054" num="0053"><figref idref="DRAWINGS">FIGS. <b>8</b>A and <b>8</b>B</figref> are schematics of a light ring of the autonomous cleaning robot of <figref idref="DRAWINGS">FIG. <b>1</b></figref> during a transition from an active state to returning to a docking station.</p><p id="p-0055" num="0054"><figref idref="DRAWINGS">FIGS. <b>9</b>A-<b>9</b>C</figref> are schematics of a light ring of the autonomous cleaning robot of <figref idref="DRAWINGS">FIG. <b>1</b></figref> during a rebooting transition.</p><p id="p-0056" num="0055"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a perspective view of a docking station for the autonomous cleaning robot of <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0057" num="0056">An autonomous cleaning robot can autonomously perform cleaning operations to clean a floor surface. The cleaning robot can initiate multiple different behaviors. For example, the behaviors can include a cleaning behavior in which the robot autonomously moves about the floor surface to clean the floor surface, a docking behavior in which the robot moves back to its docking station, an idle behavior in which electronics of the robot are active but the robot is stationary, and other behaviors described in this disclosure. The robot can include a button that can be operated to initiate multiple different behaviors. As described herein, in response to one or more actuations of the button (e.g., by a user), the robot can identify which behavior should be initiated based on a characteristic of an actuation (e.g., a duration of time, whether the button is depressed or released, etc.) of the one or more actuations, a sequence of actuations (e.g., a number of times the button is actuated within a time interval) of the one or more actuations, and/or information collected from sensors on the robot.</p><p id="p-0058" num="0057">Referring to <figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref>, a robot <b>100</b> (e.g., an autonomous cleaning robot) includes a body with a one-button user interface (e.g., a button <b>102</b>). In some implementations, the button <b>102</b> is configured as a toggle switch with two states (e.g., an actuated (e.g., depressed state) and an unactuated (e.g., released state)). In other implementations, the button <b>102</b> has more than two states (e.g., a soft press state, a hard press state, a released state, etc.). In this way, the button <b>102</b> can be pressed by a user <b>150</b> to cause the robot <b>100</b> to initiate different behavior. Details about actuating the button <b>102</b> to cause the robot <b>100</b> to initiate behavior is described with reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref> below. In the implementation shown, the robot <b>100</b> has a single button corresponding to button <b>102</b>. In this way, a single button is operated to initiate multiple different behaviors of the robot <b>100</b>, as described in further detail below.</p><p id="p-0059" num="0058">Generally, the robot <b>100</b> has a front side <b>106</b>, a back side <b>108</b>, a left-hand-side <b>110</b>, a right-hand-side <b>112</b>, a top <b>114</b>, and a bottom <b>116</b> (or underside). The body of the robot <b>100</b> is circular-shaped such that an outer perimeter of the robot <b>100</b> is circular. However, square-shaped or other shaped robot bodies are used in other implementations.</p><p id="p-0060" num="0059">In the implementations represented in <figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref>, the button <b>102</b> is positioned on a top surface <b>104</b> of the robot <b>100</b>. In particular, the button <b>102</b> is positioned within a central portion <b>130</b> of the top surface <b>104</b>. The button <b>102</b> is oriented vertically such that a central axis of the button <b>102</b> is perpendicular to the top surface <b>104</b>. The button <b>102</b> is also mounted flush with the central portion <b>130</b>. In other implementations, the button <b>102</b> can be positioned on a different face of the robot <b>100</b> (e.g., the side of the robot <b>100</b>).</p><p id="p-0061" num="0060">The robot <b>100</b> represented in <figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref> also includes a light indicator <b>210</b>. The light indicator <b>210</b> includes a housing <b>124</b> that is mounted flush with the button <b>102</b> and the central portion <b>130</b>. The housing <b>124</b> is shaped as a ring around the button <b>102</b> and spans an entire circumference (e.g., 360&#xb0;) of the button <b>102</b>. A plurality of lights are mounted inside the housing <b>124</b> and are individually controllable by a controller of the robot <b>100</b> to individually illuminate and transmit light through a translucent portion of the housing <b>124</b>. In this way, the light indicator <b>210</b> is configured as a light ring and illuminates to provide visual indication to the user <b>150</b> of the robot <b>100</b>.</p><p id="p-0062" num="0061">In some implementations, the plurality of lights of the light indicator <b>210</b> define a ring of equally spaced 8 multi-color light emitting diodes (LEDs). In this scenario, the LEDs are mounted on a circuit board that is mounted inside the housing <b>124</b>. In some examples, the LEDs are individually controllable such that the robot <b>100</b> can illuminate a portion of the light indicator <b>210</b> based on a current state of the robot <b>100</b> and/or the actuation of the button <b>102</b>. In some examples, the light indicator <b>210</b> is configured to illuminate white, blue, red, and green. Further details about the light indicator <b>210</b> and coordination of illumination of the light indicator <b>210</b> with operation of the button <b>102</b> are described with reference to <figref idref="DRAWINGS">FIGS. <b>7</b>A-<b>9</b>C</figref> below.</p><p id="p-0063" num="0062">In some implementations, the robot <b>100</b> includes a camera <b>118</b> positioned near the front side <b>106</b> of the robot <b>100</b> that allows the robot <b>100</b> to image the environment of the robot <b>100</b>. In some implementations, the robot <b>100</b> processes the image information and determines control for the robot <b>100</b> based on the image information (e.g., obstacle avoidance). In some implementations, the camera <b>118</b> includes a range sensor to generate information indicative of a distance from the robot <b>100</b> to one or more obstacles in an environment of the robot.</p><p id="p-0064" num="0063">In some implementations, the robot <b>100</b> includes a bumper <b>120</b> that can be actuated by a force of a collision when the robot <b>100</b> encounters an obstacle. In some scenarios, a user can actuate the bumper <b>120</b> manually. The bumper <b>120</b> can include a bumper sensor configured to generate a bumper actuation signal in response to actuation of the bumper <b>120</b>. In some examples, the bumper sensor includes a switch and in other implementations, the bumper sensor includes a capacitance, inductance, a Hall effect, or other sensor.</p><p id="p-0065" num="0064">Referring to <figref idref="DRAWINGS">FIGS. <b>1</b>C and <b>1</b>D</figref>, the robot <b>100</b> includes a drive system <b>204</b> configured to maneuver the robot <b>100</b> about a floor surface. The drive system <b>204</b> includes two wheels <b>122</b><i>a</i>, <b>122</b><i>b </i>that are both rotatable to maneuver the robot <b>100</b> and also movable between a first position in which the wheels <b>122</b><i>a</i>, <b>122</b><i>b </i>are extended relative to a bottom surface of the robot <b>100</b> and a second position in which the wheels <b>122</b><i>a</i>, <b>122</b><i>b </i>are retracted relative to the bottom surface of the robot <b>100</b>. In some implementations, wheel <b>122</b><i>a </i>and wheel <b>122</b><i>b </i>are independently movable between the first position and the second position. The robot <b>100</b> also includes a third wheel <b>122</b><i>c </i>which is an undriven caster wheel.</p><p id="p-0066" num="0065">The drive system <b>204</b> includes drive wheel sensors to measure whether the wheels <b>122</b> are in the first position, the second position, or an in-between position. In some examples, the drive system <b>204</b> includes a drive wheel sensor associated with each wheel <b>122</b>. In some examples, the drive wheel sensors include a switch or IR break beam sensor. In some examples, the wheels <b>122</b> are biased away from the robot <b>100</b> using one or more springs. Additionally, as described in further detail below, information generated by the drive wheel sensors can be used by a controller of the robot <b>100</b> to determine that a wheel is &#x201c;off the ground&#x201d; when the respective wheel is fully extended from the robot <b>100</b>. Similarly, information generated by the drive wheel sensors can be used by the controller of the robot <b>100</b> to determine that a wheel is &#x201c;on the ground&#x201d; when the respective wheel is less than fully extended from the robot <b>100</b>.</p><p id="p-0067" num="0066">The robot <b>100</b> includes a cleaning system <b>212</b> to clean a floor surface underneath the robot <b>100</b>. The cleaning system <b>212</b> cleans the floor as the robot <b>100</b> is maneuvered about the floor surface using the wheels <b>122</b><i>a</i>, <b>122</b><i>b </i>of the drive system <b>204</b> of the robot <b>100</b>. In the implementations represented in <figref idref="DRAWINGS">FIGS. <b>1</b>C and <b>1</b>D</figref>, the cleaning system <b>212</b> includes rotatable rollers <b>126</b><i>a</i>, <b>126</b><i>b </i>and a vacuum system including a motor and an impeller to generate an airflow at a cleaning inlet between the rollers <b>126</b><i>a</i>, <b>126</b><i>b</i>. However, in other implementations, the cleaning system <b>212</b> can be a mopping system with a mop that faces the floor surface and scrubs the floor surface as the robot <b>100</b> is maneuvered about the floor surface.</p><p id="p-0068" num="0067">In some implementations, the robot <b>100</b> includes a bin <b>132</b> removably attached to the robot <b>100</b>. The bin <b>132</b> defines a volume where debris can be collected by the rollers <b>126</b><i>a</i>, <b>126</b><i>b </i>and the vacuum system of the cleaning system <b>212</b> and deposited into the bin <b>132</b>. The robot <b>100</b> also includes a bin sensor configured to generate a bin attachment signal in response to a presence of the bin <b>132</b> being attached the robot <b>100</b>.</p><p id="p-0069" num="0068">In some implementations, the robot <b>100</b> can be docked in a docking station to evacuate the bin and to recharge the battery of the robot <b>100</b>. For example, <figref idref="DRAWINGS">FIG. <b>10</b></figref> shows an example of a docking station <b>1000</b> to which the robot <b>100</b> can dock. The docking station <b>1000</b> includes charging contacts <b>1002</b><i>a</i>, <b>1002</b><i>b </i>to mate with corresponding contacts of the robot <b>100</b> to charge the battery of the robot <b>100</b> and an evacuation port <b>1004</b> through which debris in the bin <b>132</b> of the robot <b>100</b> is evacuated into the docking station <b>1000</b>.</p><p id="p-0070" num="0069">Referring to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the robot <b>100</b> includes a controller <b>200</b> in electrical communication with the drive system <b>204</b>, the sensors <b>206</b> (e.g., the bin attachment sensors, the wheel sensors, the bumper sensors, etc.), an audio output system <b>208</b> (e.g., one or more speakers of the robot <b>100</b>), the button <b>102</b>, the light indicator <b>210</b> (e.g., each of the plurality of lights of the light indicator <b>210</b>), the cleaning system <b>212</b>, non-transitory memory <b>214</b>, a wireless-communication system <b>216</b> (e.g., transceivers of the robot <b>100</b>), and a power system <b>218</b> (e.g., the batteries of the robot <b>100</b>).</p><p id="p-0071" num="0070">In some implementations, the controller <b>200</b> includes one or more processors configured to perform operations of the robot <b>100</b>. For example, the controller <b>200</b> can perform the operations described with reference to method <b>300</b> below. In general, the non-transitory memory <b>214</b> is a non-transitory computer-readable storage medium that includes at least one program for execution by the processor of robot such that, when executed by the processor, cause the robot <b>100</b> to perform the operations described with reference to method <b>300</b> below.</p><p id="p-0072" num="0071">Referring to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the method <b>300</b> of operating the robot <b>100</b> can include selecting, at step <b>302</b>, by a processor, a behavior of the autonomous cleaning robot from a plurality of behaviors of the autonomous cleaning robot based on at least a duration of actuation of a robot button of the autonomous cleaning robot. For example, the processor of the controller <b>200</b> of the robot <b>100</b> can select a behavior of the robot <b>100</b> (e.g., waking up the robot from a sleep state) from a plurality of behaviors of the robot <b>100</b> (e.g., waking up the robot from a sleep state, driving the robot to the docking station, initiating a cleaning mission, etc.) based on at least a duration of actuation of a button <b>102</b> of the robot <b>100</b>.</p><p id="p-0073" num="0072"><figref idref="DRAWINGS">FIGS. <b>4</b>-<b>6</b></figref> illustrate example scenarios of method <b>300</b>. Referring to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, a flowchart <b>400</b> of an example scenario for determining the behavior to initiate in response to actuation of the button <b>102</b> is presented. In this scenario, at step <b>402</b>, a user actuates (e.g., presses) the button <b>102</b> for a duration of time (e.g., 2 seconds, 4 seconds, etc.). In some examples, the user <b>150</b> actuates the button <b>102</b> according to short taps (e.g., less than 2 seconds), short holds (e.g., between 2-5 seconds), holds (e.g., between 5-7 seconds), or long holds (e.g., more than 7 seconds). However, in other examples, the specific times associated with taps, short holds, holds, and long holds can vary. Additionally, in some examples, more than four (or less than four) categories of user actuation can exist.</p><p id="p-0074" num="0073">Depending on the duration of time the button <b>102</b> is actuated, the controller <b>200</b> proceeds to one of two different processes illustrated in the flowchart <b>400</b>. In a scenario where the user <b>150</b> actuates the button <b>102</b> for a duration of time between 0-2 seconds (e.g., within 2 seconds) and then releases the button <b>102</b>, the robot <b>100</b> proceeds to process <b>404</b>. In a scenario where the user <b>150</b> actuates the button <b>102</b> for a duration of time between 2-5 seconds and then releases the button <b>102</b>, the robot <b>100</b> proceeds to process <b>406</b>.</p><p id="p-0075" num="0074">The controller <b>200</b> then proceeds to select the behavior of the robot <b>100</b> based on one or more conditions of the robot <b>100</b>. In some implementations, the one or more conditions include a current state of the robot <b>100</b> (e.g., an awake state, a sleeping state, a cleaning state, etc.), signals from one or more sensors of the robot <b>100</b> (e.g., sensors of the bumpers <b>120</b>, sensors of the drive wheels <b>122</b><i>a</i>, <b>122</b><i>b</i>, sensors of the bin <b>132</b>, etc.), and whether the robot <b>100</b> is currently docked in a docking station (e.g., the docking station <b>1000</b> described with reference to <figref idref="DRAWINGS">FIG. <b>10</b></figref> above). In some examples, information reflecting whether the robot <b>100</b> is docked is included in a state (e.g., a docked state) and other times information can be received from a sensor of the docking station.</p><p id="p-0076" num="0075">In some examples, the robot <b>100</b> is required to be in an awake state before the controller <b>200</b> can select the behavior of the robot <b>100</b>. For example, if the robot <b>100</b> is asleep, then the first behavior selected will be to awake the robot <b>100</b>. In particular, if the robot <b>100</b> is in a sleep state and the button <b>102</b> is actuated and released within 2 seconds of being actuated (e.g., the duration is less than 2 seconds), the controller <b>200</b> selects the behavior to be an awake behavior. In this scenario, the awake behavior can be associated with control signals that cause the robot <b>100</b> to awake from sleep. In this way, the controller <b>200</b> can select the behavior based on a current state of the robot <b>100</b> and the duration of actuation of the button <b>102</b>.</p><p id="p-0077" num="0076">In another example, if the robot <b>100</b> is in an awake state and also in an idle state, and the button <b>102</b> is actuated and released within 2 seconds of being actuated (e.g., the duration is less than 2 seconds), the controller <b>200</b> selects the behavior to be a cleaning behavior. In this scenario, the cleaning behavior can be associated with control signals that cause the robot <b>100</b> to begin cleaning. In this way, the controller <b>200</b> can select the behavior based on more than one current state of the robot <b>100</b> and the duration of actuation of the button <b>102</b>.</p><p id="p-0078" num="0077">In another example, if the robot <b>100</b> is in a cleaning state and the button <b>102</b> is actuated and released (e.g., within 2 seconds of being actuated) while the robot <b>100</b> is not docked in a docking station, the controller <b>200</b> selects the behavior to be a pause behavior. In this scenario, the pause behavior can be associated with control signals that cause the robot <b>100</b> to pause the cleaning behavior. In this way, the controller <b>200</b> can select the behavior based on the current state of the robot <b>100</b>, whether the robot <b>100</b> is docked, and the duration of actuation of the button <b>102</b>.</p><p id="p-0079" num="0078">Referring to process <b>404</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>, if the button <b>102</b> is actuated for a short duration (e.g., a &#x201c;short tap&#x201d; of less than 2 seconds) and released, the controller <b>200</b> proceeds to determine, at step <b>408</b>, if the robot <b>100</b> is in an awake state. If the robot <b>100</b> is not in an awake state, the controller <b>200</b> selects, at step <b>410</b>, the behavior to be awake behavior. In this scenario, the awake behavior can be associated with control signals that cause the robot <b>100</b> to awake from sleep.</p><p id="p-0080" num="0079">In some implementations, the controller <b>200</b> can also select the awake behavior immediately when the button <b>102</b> is actuated. For example, the controller <b>200</b> would first proceed to process <b>404</b> when the button <b>102</b> is actuated. If a behavior is associated with an &#x201c;on actuation&#x201d; state, then the controller <b>200</b> would immediately select that behavior. Otherwise, the controller <b>200</b> would wait until the user <b>150</b> has released the button <b>102</b> to determine the duration that the button <b>102</b> was held in the actuated state. In the example shown, the controller <b>200</b> selects the awake behavior (at step <b>410</b>) when the button <b>102</b> is actuated regardless of the duration that the button <b>102</b> is held in an actuated state.</p><p id="p-0081" num="0080">Continuing on, if the robot <b>100</b> is in an awake state, the controller <b>200</b> determines, at step <b>412</b>, if the robot <b>100</b> is in an idle state. If the robot <b>100</b> is in an idle state, the controller <b>200</b> selects, at step <b>414</b>, the behavior to be a resume behavior. In this scenario, the resume behavior can be associated with control signals that cause the robot <b>100</b> to resume a behavior. For example, the resume behavior can be associated with control signals that cause the robot <b>100</b> to resume a cleaning behavior (e.g., drive around and clean the floor) or resume a docking behavior (e.g., return to the docking station without cleaning the floor).</p><p id="p-0082" num="0081">If the robot is not in an idle state, the controller <b>200</b> determines, at step <b>416</b>, if the robot <b>100</b> is in a cleaning or driving state. If the robot <b>100</b> is not in a cleaning or driving state, the controller <b>200</b> selects, at step <b>418</b>, the behavior to be cleaning behavior. In this scenario, the cleaning behavior can be associated with control signals that cause the robot <b>100</b> to begin a cleaning job. In other examples, selecting the cleaning behavior at step <b>418</b> can be replaced with selecting a docking behavior. In this scenario, the docking behavior can be associated with control signals that cause the robot <b>100</b> to drive to a docking station (e.g., to recharge, evacuate the bin <b>132</b>, await further commands, etc.).</p><p id="p-0083" num="0082">If the robot <b>100</b> is in a cleaning or driving state, the controller <b>200</b> selects, at step <b>420</b>, the behavior to be a pause behavior. In this scenario, the pause behavior can be associated with control signals that cause the robot <b>100</b> to temporarily pause the cleaning or driving behavior and await further instructions from the controller <b>200</b>. In this example, the controller <b>200</b> selects the pause behavior (at step <b>420</b>) when the button <b>102</b> is actuated regardless of the duration that the button is held in an actuated state.</p><p id="p-0084" num="0083">As reflected in the above examples, the controller <b>200</b> selects the behavior from a plurality of behaviors. These plurality of behaviors can include an awake behavior, a resume behavior, a pause behavior, a cleaning behavior, and additional behaviors as described below. In this way, the controller <b>200</b> can select the behavior among at least three different behaviors.</p><p id="p-0085" num="0084">Still referring to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, if the button <b>102</b> is actuated for a different duration of time, the controller <b>200</b> can select a different behavior based on the duration. In the example shown, the controller <b>200</b> proceeds to process <b>406</b> when the button <b>102</b> is actuated for a short hold duration (e.g., a &#x201c;short hold&#x201d; between 2-5 seconds) and released. In this scenario, the controller <b>200</b> proceeds to determine, at step <b>422</b>, if the robot <b>100</b> is in a docked state.</p><p id="p-0086" num="0085">If the robot <b>100</b> is not in a docked state, at step <b>424</b>, the controller <b>200</b> cancels the current cleaning mission and then initiates an evacuation process. In this scenario, the cancel job and evacuate behavior can be associated with control signals that cause the robot <b>100</b> to cancel the current cleaning job and cause a docking station to evacuate debris from the bin <b>132</b> of the robot <b>100</b> into the docking station.</p><p id="p-0087" num="0086">If the robot <b>100</b> is in a docked state, the controller <b>200</b> determines, at step <b>426</b>, if the bumper <b>120</b> of the robot <b>100</b> is currently in an actuated state. If the bumper <b>120</b> is actuated, the controller <b>200</b> selects, at step <b>428</b>, the behavior to be a sleep behavior. In this scenario, the sleep behavior can be associated with control signals that cause the robot <b>100</b> to enter a sleep state (e.g., from an awake state). For example, the sleep state can represent a low-power consumption state to preserve battery power of the robot <b>100</b>. In such a state, the controller <b>200</b> can disable all sensors and systems of the robot (e.g., the drive system <b>204</b> can be disabled, the audio output system <b>208</b> can be disabled, the cleaning system <b>212</b> can be disabled, etc.) and instead only process signals generated by the button <b>102</b> to determine if robot <b>100</b> should be awoken from sleep. In some examples, the controller <b>200</b> controls the light indicator <b>210</b> while the robot <b>100</b> is in the sleep state as an indication to the user <b>150</b>. Conversely, the awake state can represent a full-power state of the robot <b>100</b>. In such a state, the controller <b>200</b> can enable all sensors and systems of the robot <b>100</b> (e.g., the drive system <b>204</b> is enabled, the audio output system <b>208</b> is enabled, the cleaning system <b>212</b> is enabled, etc.)</p><p id="p-0088" num="0087">Continuing on, if the bumper <b>120</b> of the robot <b>100</b> is currently not in an actuated state, the controller <b>200</b> determines, at step <b>430</b>, if the robot <b>100</b> is currently in an idle state. If the robot <b>100</b> is currently in an idle state, the controller <b>200</b> selects, at step <b>432</b>, the behavior to be a docking behavior. In this scenario, the docking behavior can be associated with control signals that cause the robot <b>100</b> to return home to a docking station.</p><p id="p-0089" num="0088">If the robot <b>100</b> is currently not in an idle state, the controller <b>200</b> determines, at step <b>434</b>, if the robot <b>100</b> is currently in a paused state. If the robot <b>100</b> is currently in a paused state, the controller <b>200</b> selects, at step <b>436</b>, the behavior to be a cancel job and dock behavior. In this scenario, the cancel job and dock behavior can be associated with control signals that cause the robot <b>100</b> to cancel any current cleaning jobs and return home to a docking station.</p><p id="p-0090" num="0089">As noted above, the controller <b>200</b> can select the behavior of the robot <b>100</b> based on sensor information generated from one or more sensors <b>206</b> of the robot <b>100</b>. In these scenarios, the sensor information can be independent from the actuation of the button <b>102</b>.</p><p id="p-0091" num="0090">In the examples described with reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the controller <b>200</b> can select a different behavior based on the duration of the button <b>102</b>. For example, the controller <b>200</b> selects the behavior to be a resume behavior (at step <b>414</b>) when the button <b>102</b> is actuated for less than 2 seconds and released. Conversely, the controller <b>200</b> selects the behavior to be a docking behavior (at step <b>432</b>) when the button <b>102</b> is actuated for between 2 and 5 seconds and released. In this way, the controller <b>200</b> can select either a first behavior (e.g., resume) or a second behavior (e.g., docking) based on the duration of the button <b>102</b> actuation.</p><p id="p-0092" num="0091"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flowchart <b>500</b> of additional example scenarios. In particular, <figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates further examples of &#x201c;on actuation&#x201d; events described above with reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>. As described above, the controller <b>200</b> can immediately select these behaviors when certain conditions are satisfied regardless of whether the user is holding the button <b>102</b> in an actuated state.</p><p id="p-0093" num="0092">The first scenario described with reference to <figref idref="DRAWINGS">FIG. <b>5</b></figref> is an immediate &#x201c;on actuation&#x201d; event, as described above with reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>. In this scenario, at step <b>502</b>, as soon as a user <b>150</b> actuates (e.g., presses) the button <b>102</b>, the controller <b>200</b> proceeds to process <b>504</b> and determines, at step <b>506</b>, if the robot <b>100</b> is in an asleep state. If the robot <b>100</b> is in an asleep state, the controller <b>200</b> immediately selects, at step <b>508</b>, the behavior to be an awake behavior. If the robot <b>100</b> is not in an asleep state, the controller <b>200</b> determines, at step <b>510</b>, if the robot <b>100</b> is cleaning and/or driving. If the robot <b>100</b> is cleaning and/or driving, the controller <b>200</b> proceeds to determine, at step <b>512</b>, if the robot <b>100</b> is docked. If the robot <b>100</b> is docked, the controller <b>200</b> immediately selects, at step <b>514</b>, the behavior to be a pause behavior.</p><p id="p-0094" num="0093">The second scenario described with reference to <figref idref="DRAWINGS">FIG. <b>5</b></figref> is performed as soon as the button <b>102</b> is actuated for a duration of 3 seconds. In this scenario, at step <b>502</b>, when the user <b>150</b> actuates and holds the button <b>102</b> for 3 seconds, the controller <b>200</b> proceeds to process <b>516</b>. While the controller <b>200</b> immediately proceeds to process <b>516</b> as soon as the button <b>102</b> is maintained in the actuated state for 3 seconds, the controller <b>200</b> continues to reevaluate the conditions associated with process <b>516</b> during a 3-7 second window. For example, conditions at steps <b>518</b>, <b>520</b>, <b>522</b> (described below) are continuously reevaluated during the 3-7 second window of time if the button <b>102</b> is maintained in an actuated state during this same time. Once the button <b>102</b> is maintained in the actuated state for more than 7 seconds, the controller <b>200</b> proceeds to process <b>526</b> (described below) instead of process <b>516</b>.</p><p id="p-0095" num="0094">Referring back to process <b>516</b>, the controller <b>200</b> determines, at step <b>518</b>, if the robot <b>100</b> is currently in one or more states. These states can include an awake state, an asleep state, a booting-up state, and an updating software state. If the robot <b>100</b> is in one or more of these states, the controller <b>200</b> determines, at step <b>520</b>, if the robot <b>100</b> is in a docked state.</p><p id="p-0096" num="0095">If the robot <b>100</b> is not in a docked state, the controller <b>200</b> determines, at step <b>522</b>, if exactly one wheel is off the ground. For example, the controller <b>200</b> can determine if exactly one wheel is off the ground based on the sensor information from the drive system <b>204</b>. In this example, the controller <b>200</b> determines that one wheel (e.g., either wheel <b>122</b><i>a </i>or wheel <b>122</b><i>b</i>) of the robot <b>100</b> is off the ground when the drive wheel sensors generate information indicative of only one wheel being fully (e.g., up to the movement limit of the respective wheel <b>122</b><i>a</i>, <b>122</b><i>b</i>) extended relative to a bottom surface of the robot <b>100</b> while the remaining wheel (e.g., the other of wheel <b>122</b><i>a </i>or <b>122</b><i>b</i>) is not fully extended (e.g., retracted relative to the bottom surface of the robot <b>100</b>). As noted above, in some examples, the movement of the third wheel <b>128</b> is included in this determination.</p><p id="p-0097" num="0096">If the robot <b>100</b> has exactly one wheel off the ground, the controller <b>200</b> selects, at step <b>524</b>, the behavior as a power-off behavior. In this scenario, the power-off behavior can be associated with control signals that cause the robot <b>100</b> to completely power down. In some cases, this power-off behavior is also referred to as a &#x201c;ship&#x201d; behavior since it is important to completely power-down the robot <b>100</b> for shipping. In this way, selecting the behavior is based on a position of at least one wheel <b>122</b><i>a</i>, <b>122</b><i>b </i>of the drive system <b>204</b> of the robot <b>100</b>.</p><p id="p-0098" num="0097">The third scenario described with reference to <figref idref="DRAWINGS">FIG. <b>5</b></figref> is performed as soon as the button <b>102</b> is maintained in an actuated state for a hold duration (e.g., a &#x201c;hold&#x201d; of at least 7 seconds). In this scenario, at step <b>502</b>, when the user <b>150</b> actuates and holds the button <b>102</b> for least 7 seconds, the controller <b>200</b> proceeds to process <b>526</b>. While the controller <b>200</b> immediately proceeds to process <b>526</b> as soon as the button <b>102</b> is maintained in the actuated state for 7 seconds, the controller <b>200</b> continues to reevaluate the conditions associated with process <b>526</b> during a 7-9 second window. For example, the conditions at steps <b>528</b>, <b>530</b>, and <b>532</b> (described below) are continuously reevaluated during the 7-9 second window of time if the button <b>102</b> is maintained in an actuated state during this same time. Once the button <b>102</b> is maintained in the actuated state for more than 9 seconds, the controller <b>200</b> proceeds to process <b>534</b> (described below) instead of process <b>526</b>.</p><p id="p-0099" num="0098">Referring back to process <b>526</b>, the controller <b>200</b> determines, at step <b>528</b>, if the robot <b>100</b> currently has a bin or tank removed. For example, the controller <b>200</b> can determine whether the robot <b>100</b> currently has a bin <b>132</b> (or tank) removed based on the bin sensors associated with the bin <b>132</b> of the robot <b>100</b>. If the robot <b>100</b> determines that the bin <b>132</b> is removed (e.g., the bin <b>132</b> is not attached to the robot <b>100</b>), the controller <b>200</b> determines, at step <b>530</b>, if the button <b>102</b> is actuated a second time. In order for a second actuation to occur, the user <b>150</b> would have to release the actuation of the button <b>102</b> after maintaining the button <b>102</b> in the actuated state for at least 7 seconds and then actuate the button <b>102</b> a second time within the 7-9 second window. If the controller <b>200</b> determines, at step <b>530</b>, that a second actuation of the button <b>102</b> has occurred, the controller <b>200</b> selects, at step <b>532</b>, the behavior as a factory reset behavior. In this scenario, the factory reset behavior can be associated with control signals that cause the robot <b>100</b> to perform a factory reset to original factory conditions (e.g., memory is cleared, user-defined settings are erased, etc.)</p><p id="p-0100" num="0099">In the example above, the controller <b>200</b> selects the behavior based on a number of times the button <b>102</b> is actuated. This feature (e.g., generally using a sequence of button actuations) is further described with reference to <figref idref="DRAWINGS">FIG. <b>6</b></figref> below. Furthermore, the controller <b>200</b> selects the behavior based on a bin attachment signal associated with the bin <b>132</b> of the robot <b>100</b>. Additionally, as described in this example, the selected behavior can be a factory reset behavior if the bin <b>132</b> is removed from the robot <b>100</b> and the button <b>102</b> is actuated for a first duration (e.g., 7 seconds) followed by a second actuation of the button <b>102</b> within a time interval (e.g., within the 7-9 second time window).</p><p id="p-0101" num="0100">The fourth scenario described with reference to <figref idref="DRAWINGS">FIG. <b>5</b></figref> is performed as soon as the button <b>102</b> is maintained in the actuated state for a long hold duration (e.g., a &#x201c;long hold&#x201d; of at least 9 seconds). In this scenario, at step <b>502</b>, when the user <b>150</b> actuates and holds the button <b>102</b> for at least 9 seconds, the controller <b>200</b> proceeds to process <b>534</b>. While the controller <b>200</b> immediately proceeds to process <b>534</b> as soon as the button <b>102</b> is maintained in the actuated state for 9 seconds, the controller <b>200</b> continues to reevaluate the conditions associated with process <b>534</b> as long as the actuation of the button <b>102</b> is maintained. For example, the condition at step <b>536</b> (described below) is continuously reevaluated after 9 seconds as long as the button <b>102</b> is maintained in an actuated state during this same time. For example, process <b>534</b> will reevaluate the condition at step <b>536</b> when the button <b>102</b> has been maintained in the actuated state for 10 seconds, 15 seconds, 20 seconds, etc.</p><p id="p-0102" num="0101">Referring back to process <b>534</b>, the controller <b>200</b> determines, at step <b>536</b>, if the robot <b>100</b> is currently in one or more states. These states include an awake state, an asleep state, and an error state. For example, the robot <b>100</b> can be in an error state when the controller <b>200</b> determines that the robot <b>100</b> is not working as required (e.g., not moving but should be moving, etc.).</p><p id="p-0103" num="0102">If the robot <b>100</b> is in one or more of these states, the controller <b>200</b> selects, at step <b>538</b>, the behavior as a reboot behavior. In this scenario, the reboot behavior can be associated with control signals that cause the robot <b>100</b> to perform a device reset (but not to original factory conditions). This allows the robot <b>100</b> to retain information in memory (e.g., maps) and user-configured settings.</p><p id="p-0104" num="0103">While <figref idref="DRAWINGS">FIG. <b>5</b></figref> represents four categories of actuation durations (e.g., on actuation, 3-7 seconds, 7-9 seconds, and greater than 9 seconds), in some implementations, more (or less) than four categories are used. In some implementations, the durations and time intervals associated with each process <b>504</b>, <b>516</b>, <b>526</b>, <b>534</b> can be different as well.</p><p id="p-0105" num="0104">Referring back to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the method <b>300</b> can also include generating, at step <b>304</b>, by the processor of the controller <b>200</b>, a control signal based on the behavior. The control signals can be used to cause the robot <b>100</b> to initiate the behavior. For example, the control signals can be transmitted to the drive system <b>204</b> so that the drive system <b>204</b> can control the drive wheels <b>122</b><i>a</i>, <b>122</b><i>b </i>of the robot <b>100</b> to maneuver as part of a particular behavior (e.g., drive to docking station, clean the floor, etc.)</p><p id="p-0106" num="0105">The method <b>300</b> can also include causing, at step <b>306</b>, the autonomous cleaning robot to initiate the behavior using the control signal. As described in the previous example, the controller <b>200</b> can cause the drive wheels <b>122</b><i>a</i>, <b>122</b><i>b </i>of robot <b>100</b> to rotate to maneuver the robot <b>100</b> to the docking station if the behavior is a docking behavior.</p><p id="p-0107" num="0106">In some implementations, initiating the behavior includes setting a current state of the robot <b>100</b>. For example, the controller <b>200</b> can cause the robot <b>100</b> to enter an awake state when the selected behavior is an awake behavior. As another example, the controller <b>200</b> can cause the robot <b>100</b> to enter a docking state when the selected behavior is a docking behavior. And so on.</p><p id="p-0108" num="0107">Referring to <figref idref="DRAWINGS">FIG. <b>6</b></figref>, a flowchart <b>600</b> presents additional example scenarios. In particular, <figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates how a sequence of actuations can be used by the controller <b>200</b> to select the behavior.</p><p id="p-0109" num="0108">The first scenario described with reference to <figref idref="DRAWINGS">FIG. <b>6</b></figref> is performed when the button <b>102</b> is actuated 3 times. In this scenario, at step <b>602</b>, as soon as a user actuates (e.g., presses) the button <b>102</b> at least 3 times within a time interval (e.g., within 2 seconds), the controller <b>200</b> proceeds to process <b>604</b> and determines, at step <b>606</b>, if the bin <b>132</b> is currently removed from the robot <b>100</b>. If the bin <b>132</b> is currently removed from the robot <b>100</b>, the controller <b>200</b> determines, at step <b>608</b>, if robot <b>100</b> is currently docked at a docking station (e.g., docking station <b>1000</b>). If the robot <b>100</b> is not docked, the controller <b>200</b> determines, at step <b>610</b>, if exactly one wheel is off the ground. Details regarding the determination of whether the wheels are off the ground are described herein.</p><p id="p-0110" num="0109">If the controller <b>200</b> determines that exactly one wheel is off the ground, the controller <b>200</b> selects, at step <b>612</b>, the behavior as a child-lock behavior. In this scenario, the child-lock behavior can be associated with control signals that cause the robot <b>100</b> to enter a child-lock state. In some examples, the child-lock behavior causes the robot <b>100</b> to ignore actuations of the button <b>102</b>. In this mode, for example, a user would need to use a software application on a mobile computing device (e.g., a smartphone, a personal computer, a tablet computer, etc.) to cause the robot <b>100</b> to leave the child-lock state. In some examples, this mode is also referred to as a pet-lock mode.</p><p id="p-0111" num="0110">The second scenario described with reference to <figref idref="DRAWINGS">FIG. <b>6</b></figref> is performed when the button <b>102</b> is actuated 5 times. In this scenario, at step <b>602</b>, as soon as a user actuates (e.g., presses) the button <b>102</b> at least 5 times within a time interval (e.g., within 2 seconds), the controller <b>200</b> proceeds to process <b>614</b> and determines, at step <b>616</b>, if the bin <b>132</b> is currently removed from the robot <b>100</b>. If the bin <b>132</b> is currently removed from the robot <b>100</b> (e.g., not attached to the robot <b>100</b>), the controller <b>200</b> determines, at step <b>618</b>, if the robot <b>100</b> is in an energy savings mode. If the robot <b>100</b> is not in an energy savings mode, the controller <b>200</b> selects, at step <b>620</b>, the behavior as an Enter energy savings behavior. In this scenario, the Enter energy savings behavior can be associated with control signals that cause the robot <b>100</b> to enter an energy savings mode. In some examples, an energy savings mode configures the robot <b>100</b> by turning off all functionality (e.g., network functionality) except a charging functionality and functionality associated with the button <b>102</b>. An energy savings mode compliant mode can be useful when the robot <b>100</b> is not being used for an extended period of time (e.g., weeks, months, etc.) since the robot <b>100</b> consumes less power in an energy savings mode compared to a non-energy savings mode.</p><p id="p-0112" num="0111">If the robot <b>100</b> is currently in an energy savings mode, the controller <b>200</b> determines, at step <b>622</b>, if the robot <b>100</b> is currently docked in a docking station. If the robot <b>100</b> is not currently docked, the controller <b>200</b> selects, at step <b>624</b>, the behavior as an Exit energy savings behavior. In this scenario, the Exit energy savings behavior can be associated with control signals that cause the robot <b>100</b> to exit an energy savings mode.</p><p id="p-0113" num="0112">In the examples described with reference to <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the controller <b>200</b> can select the behavior based on a sequence of the actuation of the button <b>102</b>. In some implementations, the sequence includes a number of times (e.g., 3 times, 5 times, 7 times, etc.) the button <b>102</b> is actuated within a time interval (e.g., within 2 seconds, within 3 seconds, within 5 seconds, etc.).</p><p id="p-0114" num="0113">In some implementations, the sequence of actuations of the button <b>102</b> can include a sequence of different types of actuation (tap, short hold, hold, etc.). A sequence of actuations can include multiple sequentially-occurring actuations. In some implementations, at least some of the actuations have types that are different from one another. A particular sequence of actuations can be associated with a particular behavior for the robot <b>100</b> to initiate. The number of actuations in the sequence of actuations can vary in implementations. For example, the number of actuations can be 2, 3, 4, or more actuations. The types of the actuations in the sequence of actuations can also vary in implementations. The various types of actuations that could be used in the sequence of actuations are described herein. For example, a user <b>150</b> can actuate the button <b>102</b> with a short tap (e.g., actuated and release within 2 seconds) followed by a short hold (e.g., actuate and release within 2-5 seconds) followed by another short tap (e.g., actuated and release within 2 seconds). In turn, the controller <b>200</b> can determine the behavior of the robot <b>100</b> based on a sequence of different types of actuation.</p><p id="p-0115" num="0114"><figref idref="DRAWINGS">FIG. <b>7</b>A</figref> is a schematic of the light indicator <b>210</b> of the robot <b>100</b> described with reference to <figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref>. In particular, the top of the light indicator <b>210</b>, as represented in <figref idref="DRAWINGS">FIGS. <b>7</b>A-<b>9</b>C</figref>, represents a forward side of the light indicator <b>210</b> and corresponds to a forward direction of the robot (e.g., toward front side <b>106</b> as represented in <figref idref="DRAWINGS">FIGS. <b>1</b>A-<b>1</b>D</figref>). Similarly, the bottom of the light indicator <b>210</b>, as represented in <figref idref="DRAWINGS">FIGS. <b>7</b>A-<b>9</b>C</figref>, represents a back side of the light indicator <b>210</b> and corresponds to a backwards direction of the robot (e.g., toward back side <b>108</b> as represented in <figref idref="DRAWINGS">FIGS. <b>1</b>A-<b>1</b>D</figref>).</p><p id="p-0116" num="0115">As represented in <figref idref="DRAWINGS">FIG. <b>7</b>A</figref>, the light indicator <b>210</b> is transitioning from an awake state to initiating a mission. In particular, <figref idref="DRAWINGS">FIG. <b>7</b>A</figref> illustrates different illuminations of the light indicator <b>210</b> of the robot <b>100</b> under different conditions. As previously described with reference to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the button <b>102</b> is positioned on a top surface <b>104</b> of the robot <b>100</b> and the light indicator <b>210</b> is configured as a light ring around the circumference of the button <b>102</b>.</p><p id="p-0117" num="0116">When the user <b>150</b> interacts with the button <b>102</b> by causing the button <b>102</b> to change from an actuated state to an unactuated state, and vice versa, the controller <b>200</b> of the robot <b>100</b> can cause the light indicator <b>210</b> to illuminate in different manners to illustrate different behaviors of the robot <b>100</b>. For example, the controller <b>200</b> of the robot <b>100</b> can illuminate a portion of a continuous annular surface positioned around the button <b>102</b>. In some examples, the light indicator <b>210</b> illuminates in different manners to indicate what type of actuation (tap, short hold, hold, etc.) is being performed, and to indicate a progress of a certain actuation. In this way, the length of illumination is based on and/or proportional to the duration of actuation of the button <b>102</b>.</p><p id="p-0118" num="0117">As represented by button state <b>702</b>, the light indicator <b>210</b> on the robot <b>100</b> does not emit light when the robot <b>100</b> is asleep. As used herein, a thin boundary line <b>752</b> of the light indicator <b>210</b> is used to represent an off-state. In some examples, an absence of light emission can correspond to a user interface that is not active on the surface of the robot <b>100</b>. For example, the user <b>150</b> can tap the button <b>102</b> during the robot's asleep state to awaken the robot <b>100</b>. In response, the controller <b>200</b> can transition the light indicator <b>210</b> around the button <b>102</b> from an off state (button state <b>702</b>) to an illuminated state (button state <b>704</b>), and the controller <b>200</b> can cause the light indicator <b>210</b> to illuminate with a particular color and a particular illumination pattern. As used herein, a thick boundary line <b>754</b> of the light indicator <b>210</b> is used to represent an on-state. In this example, the controller <b>200</b> causes the light indicator <b>210</b> to illuminate with a white light. However, in other examples, the color can be blue, red, green, or another color. In this way, any color light can be used by the light indicator <b>210</b>.</p><p id="p-0119" num="0118">In this example, the controller <b>200</b> causes the light indicator <b>210</b> to illuminate with an illumination pattern representing a constant illumination throughout the entire circumference of the light indicator <b>210</b>. For example, while the actual implementation of the light indicator <b>210</b> can be a discrete number of LEDs (e.g., 8 LEDs), as described with reference to <figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref> above, these LEDs combine to illuminate the entire light indicator <b>210</b>. In some examples, illuminating the entire light indicator <b>210</b> is referred to as a &#x201c;solid illumination.&#x201d; In this way, the light indicator <b>210</b> is perceived by the user <b>150</b> to be completely illuminated with light. However, in other examples, the illumination pattern can be spatially discontinuous (e.g., off vs. on states, bright vs. dim states, etc.), include pulsed illuminations (e.g., illuminations that vary over time, etc.). These illumination patterns are described in further detail below.</p><p id="p-0120" num="0119">For example, a solid illumination of a white light (as represented by button state <b>704</b>) can indicate that the robot <b>100</b> is awake. Additionally, when the user <b>150</b> actuates the button <b>102</b> again to transition the robot <b>100</b> from the awake state to a start (or resume) mission state (e.g., by actuating the button <b>102</b> with a second tap), the robot <b>100</b> transitions from an awake state to the start mission state. In turn, the controller <b>200</b> causes the light indicator <b>210</b> to transition from a solid illumination of white light (button state <b>704</b>) to an illumination pattern indicative of the mission being performed (button state <b>706</b>). As used herein, a thick dashed boundary line <b>756</b> of the light indicator <b>210</b> is used to represent a mission specific illumination pattern.</p><p id="p-0121" num="0120">In some examples, an illumination pattern of a periodic sweep sequence of white light symmetrically across the button <b>102</b> is associated with a cleaning mission. In some examples, an illumination pattern of a counterclockwise swirl sequence of blue light around the circumference of the button <b>102</b> is associated with a spot cleaning mission. In some examples, an illumination pattern of a marching forward sequence of blue light symmetrically across the button <b>102</b> is associated with a docking mission (e.g., returning to a docking station). In this example, the marching forward sequence can mean that light travels from a back side of the light indicator <b>210</b> to a front side of the light indicator <b>210</b>. In this way, light travels from the back side <b>108</b> of the robot <b>100</b> to the front side <b>106</b> of the robot <b>100</b>.</p><p id="p-0122" num="0121">As described above, the controller <b>200</b> can select behaviors associated with these states based on the actuation of the button <b>102</b>. In this way, the controller <b>200</b> causes the light indicator <b>210</b> to illuminate with a particular intensity, color, pattern, and pulse based on the actuation of the button <b>102</b>.</p><p id="p-0123" num="0122"><figref idref="DRAWINGS">FIG. <b>7</b>B</figref> is a schematic of the light indicator <b>210</b> of the robot <b>100</b> transitioning from a mission state (e.g., where the robot <b>100</b> is performing a cleaning behavior) to a pause state and then to a docking state (e.g., where the robot <b>100</b> is returning to a docking station (e.g., docking station <b>1000</b>)). In particular, <figref idref="DRAWINGS">FIG. <b>7</b>B</figref> represents different illuminations of the light indicator <b>210</b> of the robot <b>100</b> based on these different states. As described above with reference to <figref idref="DRAWINGS">FIG. <b>7</b>A</figref>, the controller <b>200</b> can select behaviors associated with these states based on the actuation of the button <b>102</b>. In this way, the controller <b>200</b> causes the light indicator <b>210</b> to illuminate with a particular intensity, color, pattern, and pulse based on the actuation of the button <b>102</b>.</p><p id="p-0124" num="0123">As represented in <figref idref="DRAWINGS">FIG. <b>7</b>B</figref>, and in particular by button states <b>712</b>, <b>716</b>, the light indicator <b>210</b> can be illuminated in a percentage varying from 0 to 100% around the circumference of the light indicator <b>210</b>. In some implementations, the illumination percentage of the light indicator <b>210</b> indicates a level of completion of a mission of the robot <b>100</b>. For example, the level of completion can represent whether the robot <b>100</b> is starting a mission, is mid-mission, is ending a mission, is returning to a docking station, and is docked at the docking station. In this way, the controller <b>200</b> causes the light indication <b>210</b> to illuminate with a different percentage based on a level of completion of the mission. In the example represented in <figref idref="DRAWINGS">FIG. <b>7</b>B</figref>, button state <b>712</b> represents a cleaning mission.</p><p id="p-0125" num="0124">In some examples, the percentage varies over time to give an indication of motion. For example, the controller <b>200</b> causes the light indicator <b>210</b> to progressively &#x201c;fill&#x201d; with light during a duration of the cleaning mission. In this scenario, the controller <b>200</b> controls the light indicator <b>210</b> to begin filling with light at a first location and symmetrically fills the light indicator <b>210</b> from the first location to a second location located diametrically opposite of the first location in proportion to the duration of the mission. In some examples, the first location is associated with a back side of the light indicator <b>210</b> (e.g., the side closest to the back side <b>108</b> of the robot <b>100</b> as shown in <figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref>) and the second location is associated with a front side of the light indicator <b>210</b> (e.g., the side closest to the front side <b>106</b> of the robot <b>100</b> as shown in <figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref>). In this way, the light indicator <b>210</b> progressively and symmetrically fills from a back side of the light indicator <b>210</b> to a front side of the light indicator <b>210</b>. The controller <b>200</b> causes the light indication <b>210</b> to illuminate with a percentage in proportion to a duration of the mission and/or the actuation of the button <b>102</b>.</p><p id="p-0126" num="0125">While the light indicator <b>210</b> is describes as progressively and symmetrically filling from a back side to a front side, other implementations are possible. For example, the controller <b>200</b> can control the light indicator <b>210</b> to fill asymmetrically (e.g., to perform a 360&#xb0; sweep around the circumference). In another example, the controller <b>200</b> can control the light indicator <b>210</b> to progressively and symmetrically fill from a front side to a back side, from a left side to a right side, or from a right side to a left side. Further still, the controller <b>200</b> can control the light indicator <b>210</b> to begin fill and end fill at different locations based on the behavior of the robot <b>100</b>.</p><p id="p-0127" num="0126">If a user <b>150</b> actuates the button <b>102</b> of the robot <b>100</b> when the robot <b>100</b> is mid-mission (e.g., by actuating the button <b>102</b> with a 0-2 second tap), the controller <b>200</b> temporarily pauses the robot <b>100</b>. For example, the controller <b>200</b> of the robot <b>100</b> can cause the drive system <b>204</b> to halt the movement of the robot <b>100</b>. The controller <b>200</b> also causes the light indicator <b>210</b> to illuminate with a pulsed solid illumination of white light, as represented by button state <b>714</b>. The pulsed light can indicate to the user <b>150</b> that the robot <b>100</b> has paused while mid-mission. In some examples, the pulsed light is pulsed with a frequency of 1 pulse per second (e.g., 1 Hz), but other frequencies can also be used (e.g., 0.1 Hz, 2 Hz, etc.). In some examples, a pulsed illumination is where the intensity of the light varies based on time. In some examples, the pulsed illumination sinusoidally transitions from an off-state to an on-state as a function of time. The controller <b>200</b> can control both solid illuminations and discontinuous illuminations to pulse.</p><p id="p-0128" num="0127">Then, if the user <b>150</b> holds the button <b>102</b> in the actuated state for a short hold duration (e.g., between 3-5 seconds), to indicate that the robot <b>100</b> should return to a docking station. For example, a similar scenario is described above with reference to step <b>432</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>. In turn, the controller <b>200</b> can control the robot <b>100</b> to drive home to a docking station and cause the light indicator <b>210</b> to illuminate according to button state <b>716</b> to indicate that the robot <b>100</b> is returning home. As used herein, a gray line <b>758</b> representing the boundary of the light indicator <b>210</b> is used to represent a color different from white. In this example, a thick gray line refers to a blue color while a thick black line refers to a white color, but other colors can be used in other examples.</p><p id="p-0129" num="0128">In this example, the controller <b>200</b> causes the light indicator <b>210</b> to illuminate with a back to front fill of blue light. In some examples, this is perceived by the user <b>150</b> as representing a marching sequence of light in a direction in front of the robot. In this way, the illumination pattern can represent a sequence of light indicating a direction of travel of the robot <b>100</b>. When the robot <b>100</b> arrives at the docking station and successfully docks (e.g., successfully makes an electrical connection with the contacts <b>1002</b><i>a</i>, <b>1002</b><i>b </i>of the docking station <b>1000</b> shown in <figref idref="DRAWINGS">FIG. <b>10</b></figref>), the controller <b>200</b> can cause the light indicator <b>210</b> to change the illumination again. For example, the controller <b>200</b> can cause the light indicator <b>210</b> to change from the marching blue sequence represented in button state <b>716</b> to a pulsing sequence of white light to indicate to the user <b>150</b> that the robot <b>100</b> is charging.</p><p id="p-0130" num="0129">In the examples of <figref idref="DRAWINGS">FIGS. <b>7</b>A and <b>7</b>B</figref>, a user <b>150</b> can interact with the button <b>102</b> to cause the robot to awaken (button state <b>704</b>), start (or resume) a mission (button state <b>706</b>), pause a mission (button state <b>714</b>), and return to a docking station (button state <b>716</b>). In particular, once the robot <b>100</b> is awake and paused (e.g., at button state <b>704</b> or <b>714</b>), a tap can be used to start a mission (button state <b>706</b>) and a short hold can be used to cause the robot <b>100</b> to return home to a docking station (button state <b>716</b>).</p><p id="p-0131" num="0130"><figref idref="DRAWINGS">FIGS. <b>8</b>A and <b>8</b>B</figref> are schematics of the light indicator <b>210</b> of the robot <b>100</b> during a transition from an active state to returning to a docking station. For example, the controller <b>200</b> of the robot <b>100</b> can cause the light indicator <b>210</b> to illuminate to indicate that the robot <b>100</b> is active (e.g., running), as represented by button state <b>802</b>. For example, a robot <b>100</b> in an active state is different from a robot <b>100</b> moving in a mission state. For example, a robot <b>100</b> in the running or active state can be associated with a robot <b>100</b> that is driving to (or from) a docking station and to (or from) a mission. During the robot's running state, the controller <b>200</b> can cause the light indicator <b>210</b> to illuminate according to various illumination patterns such as the illumination patterns previously described.</p><p id="p-0132" num="0131">During the robot's running state, a user <b>150</b> can actuate the button <b>102</b> of the robot <b>100</b> (e.g., by a 0-2 second tap), and the controller <b>200</b> can cause the light indicator <b>210</b> to transition from emitting illumination pattern corresponding to the particular mission being performed (button state <b>802</b>) to emitting a solid (or continuous) white light, as represented by button state <b>804</b>. However, while button state <b>804</b> is associated with white light, other colored light can be used in other examples (e.g., blue, red, green, or another color). Additionally, when the user <b>150</b> actuates the button <b>102</b> of the robot <b>100</b> while the robot <b>100</b> is in the running state, the controller <b>200</b> pauses the movement of the robot <b>100</b>. At this stage, the user <b>150</b> can provide additional interaction to the button <b>102</b> to indicate the next action of the robot <b>100</b>.</p><p id="p-0133" num="0132">For example, the controller can control the light indicator <b>210</b> based on the duration of the actuation (as previously described). For example, while the light indicator <b>210</b> is illuminating the solid white light according to button state <b>804</b>, the user <b>150</b> can still interact with the button <b>102</b> to cause additional behavior of the robot <b>100</b>. In this example, the user <b>150</b> actuates the button <b>102</b> a second time (e.g., after pressing the button <b>102</b> the first time to pause the robot <b>100</b>) and continues to hold down the button <b>102</b> for at least 2 seconds.</p><p id="p-0134" num="0133">Once the user <b>150</b> has maintained the actuation of the button <b>102</b> for 2 seconds, the controller <b>200</b> selects the behavior. In this example, the controller <b>200</b> selects the behavior to be docking behavior (e.g., to return home to the docking station). At this point, the controller <b>200</b> continues to control the light indication <b>210</b> to provide further indication to the user <b>150</b> before causing the robot <b>100</b> to initiate the selected behavior.</p><p id="p-0135" num="0134">In this example, once 2 seconds has been reached, the controller <b>200</b> controls the light indicator <b>210</b> to illuminate according to the illumination pattern represented by button states <b>806</b>. In particular, once the actuation has been maintained for 2 seconds, the controller <b>200</b> causes the light indicator <b>210</b> to begin to fill in with a second color. In this example, the light indicator <b>210</b> was initially filled with white light (e.g., representing a paused state of the robot) and is now beginning to fill with blue light because the robot <b>100</b> is providing an indication that the user <b>150</b> is about to instruct the robot <b>100</b> to return home to the docking station. In this way, the second color can represent the color of light associated with an illumination pattern of the behavior that would be selected if the user <b>150</b> continues to maintain the actuation of the button <b>102</b>.</p><p id="p-0136" num="0135">In this example, the controller <b>200</b> controls the light indicator <b>210</b> to fill 25% with blue light and 75% with white light once the actuation has been maintained for 2.25 seconds, as represented by button state <b>806</b><i>a</i>. Furthermore, the controller <b>200</b> controls the light indicator <b>210</b> to fill 50% with blue light and 50% with white light once the actuation has been maintained for 2.5 seconds, as represented by button state <b>806</b><i>b</i>. Similarly, the controller <b>200</b> controls the light indicator <b>210</b> to fill 75% with blue light and 25% with white light once the actuation has been maintained for 2.75 seconds, as represented by button state <b>806</b><i>c</i>. Then the controller <b>200</b> controls the light indicator <b>210</b> to fill 100% with blue light once the actuation has been maintained for 3 seconds, as represented by button state <b>806</b><i>d. </i></p><p id="p-0137" num="0136">In this way, the controller <b>200</b> of the robot <b>100</b> can illuminate a portion of the light indicator <b>210</b> in proportion to the duration the button <b>102</b> is actuated. In particular, the controller <b>200</b> can progress the filling of the light indicator <b>210</b> from 0% to 10% to 25%, for example, until 100% is reached.</p><p id="p-0138" num="0137">In this scenario, the controller <b>200</b> controls the light indicator <b>210</b> to begin filling with the blue light at a first location and symmetrically fills the light indicator <b>210</b> from the first location to a second location located diametrically opposite of the first location in proportion to the duration of the mission. In some examples, the first location is associated with a back side of the light indicator <b>210</b> (e.g., the side closest to the back side <b>108</b> of the robot <b>100</b> as shown in <figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref>) and the second location is associated with a front side of the light indicator <b>210</b> (e.g., the side closest to the front side <b>106</b> of the robot <b>100</b> as shown in <figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref>). However, the controller <b>200</b> can control the light indicator <b>210</b> to progressively and symmetrically fill from a front side to a back side, from a left side to a right side, or from a right side to a left side. Furthermore, the controller <b>200</b> can control the light indicator <b>210</b> to begin fill and end fill at different locations based on the behavior of the robot <b>100</b> and/or the actuation of the button <b>102</b>. In this way, the controller <b>200</b> can control a length of the illuminated portion of the light indicator <b>210</b> based on the duration of the actuation of the button <b>102</b>.</p><p id="p-0139" num="0138">In some implementations, the controller <b>200</b> can cause the audio output system <b>208</b> (as represented in the schematic of <figref idref="DRAWINGS">FIG. <b>2</b></figref>) to provide music associated with the illumination sequence of the light indicator <b>210</b> and/or associated with the behavior of the robot <b>100</b>. For example, during the progression of the light indicator <b>210</b> through the sequence of button states <b>806</b>, the audio output system <b>208</b> may provide noises, audible sounds, voices, or songs, to indicate the progression of the light indicator <b>210</b>. For example, a speaker of the audio output system <b>208</b> can produce an audible tone to indicate to the user when button state <b>806</b><i>d </i>is reached. As described above, in this example the controller <b>200</b> selected the behavior to be docking behavior at the 2 minute mark and continues to provide an indication to the user <b>150</b> (e.g., both visual and audible). Furthermore, the controller <b>200</b> controls the light indication <b>210</b> according to a raceback behavior, as represented by button states <b>806</b><i>e</i>-<b>806</b><i>h</i>. The raceback behavior provides an indication to the user <b>150</b> that it is okay to remove their finger from the button <b>102</b> to release the actuation.</p><p id="p-0140" num="0139">In this example, the light indicator <b>210</b> begins completely filled with blue light (button state <b>806</b><i>d</i>) and the controller <b>200</b> controls the light indicator <b>210</b> to progressively decrease the amount of blue light emitted by the light indicator over time. In particular, the controller <b>200</b> controls the light indicator <b>210</b> to fill 75% with blue light and 25% off (e.g., no light) 0.5 seconds after the 3 second mark, as represented by button state <b>806</b><i>e</i>. Furthermore, the controller <b>200</b> controls the light indicator <b>210</b> to fill 50% with blue light and 50% off 1 second after the 3 second mark, as represented by button state <b>806</b><i>f</i>. Similarly, the controller <b>200</b> controls the light indicator <b>210</b> to fill 25% with blue light and 75% off 1.5 seconds after the 3 second mark, as represented by button state <b>806</b><i>g</i>. Then the controller <b>200</b> controls the light indicator <b>210</b> to completely turn off 2 seconds after the 3 second mark (i.e., 5 seconds), as represented by button state <b>806</b><i>h</i>. Once the 5 second mark has been reached, the controller <b>200</b> causes the robot <b>100</b> to initiate the selected behavior. In this example, the robot <b>100</b> is instructed to return to the docking station and will proceed to drive to the docking station after the light indicator <b>210</b> is completely off at the 4 second mark.</p><p id="p-0141" num="0140">In this way, the button states <b>806</b> provide an indicator of a time buffer to the user <b>150</b>. For example, users <b>150</b> generally will not know when 2 seconds has been reached so the button states <b>206</b><i>a</i>-<b>206</b><i>d </i>are intended to let the user <b>150</b> know that is it ok to release the actuation of the button <b>102</b>. If the user <b>150</b> continues to maintain the actuation of the button <b>102</b>, the light indicator <b>210</b> continues to provide an indication to the user <b>150</b> until 5 seconds has been reached. In this way, the buffer is a window between 2-5 seconds.</p><p id="p-0142" num="0141">Additionally, the audio output system <b>208</b> may provide or output different noises during the raceback period so the user <b>150</b> can differentiate between the raceback period and a time during which the light indicator <b>210</b> is progressing to full illumination.</p><p id="p-0143" num="0142">While specific fill percentages and times were described with reference to button states <b>806</b>, other times and percentages can be used. Additionally, button states <b>806</b> can include more or less than 8 states (e.g., 6 states, 10 states, etc.).</p><p id="p-0144" num="0143">In some implementations, the controller <b>200</b> causes the robot <b>100</b> to initiate the selected behavior as soon as the user <b>150</b> releases the actuation of the button <b>102</b> (i.e., instead of waiting for the raceback sequence to complete). For example, if the user <b>150</b> releases actuation of the button <b>102</b> while the light indicator <b>210</b> is illuminated according to button state <b>806</b><i>f</i>, the controller <b>200</b> can immediately cause the robot <b>100</b> to begin the selected behavior. In this case, the selected behavior is a docking behavior so the controller <b>200</b> would cause the robot <b>100</b> to begin driving to the docking station.</p><p id="p-0145" num="0144">Additionally, the controller <b>200</b> can instruct the audio output system <b>208</b> to broadcast audio from one or more speakers during this time to illustrate that the robot <b>100</b> is transitioning to another state or that the robot <b>100</b> has just completed a mission, to name a few examples. The audio can correspond to music, noises, voices, or another sound to indicate the transition or a completed state.</p><p id="p-0146" num="0145"><figref idref="DRAWINGS">FIGS. <b>9</b>A-<b>9</b>C</figref> are schematics of a light indicator <b>210</b> of the robot <b>100</b> during a rebooting transition. <figref idref="DRAWINGS">FIGS. <b>9</b>A and <b>9</b>B</figref> include similar depictions and functions corresponding to <figref idref="DRAWINGS">FIGS. <b>8</b>A and <b>8</b>B</figref>, and are not explicitly repeated. In particular, button states <b>902</b>, <b>904</b>, and <b>906</b> are the same as button states <b>802</b>, <b>804</b>, and <b>806</b> described with reference to <figref idref="DRAWINGS">FIGS. <b>8</b>A and <b>8</b>B</figref>. However, in this example, the button <b>102</b> is maintained in the actuated state after the 5 second mark associated with button state <b>806</b><i>h. </i></p><p id="p-0147" num="0146">If a user <b>150</b> maintains the actuation of the button <b>102</b> for more than 5 seconds, then the controller <b>200</b> selects different behavior. In this example, the controller <b>200</b> selects the behavior as a reboot behavior. For example, the robot <b>100</b> can be rebooted for software upgrades, maintenance, or some other function. In this example, the controller <b>200</b> causes the light indicator <b>210</b> to illustrate different colors, brightness, patterns, and style of the lights to provide an indication that the robot <b>100</b> is progressing to perform a reboot. For example, as represented in <figref idref="DRAWINGS">FIG. <b>9</b>C</figref>, the user <b>150</b> can continue maintaining the actuation of the button <b>102</b> after the raceback period of 5 seconds and continuing into the 5-7 second period. During this time, the controller <b>200</b> controls the light indicator <b>210</b> to remain off (e.g., no light is emitted). In this way, the controller <b>200</b> can turn off the user interface of the robot <b>100</b> based on the duration of the actuation.</p><p id="p-0148" num="0147">If the user <b>150</b> continues to maintain actuation of the button <b>102</b> for 7 seconds, then the controller <b>200</b> changes the selected behavior from docking behavior to reboot behavior, as noted above. The controller <b>200</b> also causes the light indicator <b>210</b> to begin another raceback sequence as represented by button states <b>908</b><i>b</i>, <b>908</b><i>c</i>, <b>908</b><i>d</i>, <b>908</b><i>e</i>, and <b>908</b><i>f</i>. The user <b>150</b> can release actuation of the button <b>102</b> at any point during the raceback sequence and the robot <b>100</b> will still initiate the reboot sequence.</p><p id="p-0149" num="0148">In this example, the controller <b>200</b> controls the light indicator <b>210</b> to fill with white light, as represented by button state <b>908</b><i>b</i>. Then the controller <b>200</b> controls the light indicator <b>210</b> to progressively decrease the light fill percentage from 100% to 80% to 60% and so on, until 0% is reached, as represented by button states <b>908</b><i>b</i>-<b>908</b><i>f</i>. In this example, the controller <b>200</b> controls the light indicator <b>210</b> to progressively decrease the light fill percentage from 100% to 0% over a duration of 2 seconds.</p><p id="p-0150" num="0149">The robots and techniques described herein, or portions thereof, can be controlled by a computer program product that includes instructions that are stored on one or more non-transitory machine-readable storage media, and that are executable on one or more processing devices to control (e.g., to coordinate) the operations described herein. The robots described herein, or portions thereof, can be implemented as all or part of an apparatus or electronic system that can include one or more processing devices and memory to store executable instructions to implement various operations.</p><p id="p-0151" num="0150">Operations associated with implementing all or part of the robot operation and control described herein can be performed by one or more programmable processors executing one or more computer programs to perform the functions described herein. For example, the mobile device, a cloud computing system configured to communicate with the mobile device and the autonomous cleaning robot, and the robot's controller may all include processors programmed with computer programs for executing functions such as transmitting signals, computing estimates, or interpreting signals. A computer program can be written in any form of programming language, including compiled or interpreted languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment.</p><p id="p-0152" num="0151">The controllers and mobile devices described herein can include one or more processors. Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor will receive instructions and data from a read-only storage area or a random access storage area or both. Elements of a computer include one or more processors for executing instructions and one or more storage area devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from, or transfer data to, or both, one or more machine-readable storage media, such as mass PCBs for storing data, e.g., magnetic, magneto-optical disks, or optical disks. Machine-readable storage media suitable for embodying computer program instructions and data include all forms of non-volatile storage area, including by way of example, semiconductor storage area devices, e.g., EPROM, EEPROM, and flash storage area devices; magnetic disks, e.g., internal hard disks or removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks.</p><p id="p-0153" num="0152">The robot control and operating techniques described herein may be applicable to controlling other mobile robots aside from cleaning robots. For example, a lawn mowing robot or a space-monitoring robot may be trained to perform operations in specific portions of a lawn or space as described herein.</p><p id="p-0154" num="0153">Elements of different implementations described herein may be combined to form other implementations not specifically set forth above. Elements may be left out of the structures described herein without adversely affecting their operation. Furthermore, various separate elements may be combined into one or more individual elements to perform the functions described herein.</p><p id="p-0155" num="0154">A number of implementations have been described. Nevertheless, it will be understood that various modifications may be made.</p><p id="p-0156" num="0155">For example, the implementations described herein involve behaviors such as a cleaning behavior, a docking behavior, a sleep behavior, an awake behavior, a resume behavior, a pause behavior, a cancel behavior, an evacuation behavior, a reboot behavior, a power-off (ship mode) behavior, a factory reset behavior, etc.</p><p id="p-0157" num="0156">In other implementations, the behaviors initiated by user actuation of the button can include different types of cleaning behaviors. For example, the cleaning behavior can represent a full-home cleaning behavior (e.g., to clean each and every room of the home), a single-room cleaning behavior (e.g., to clean a single room of the home), an edge-clean-only cleaning behavior (e.g., such that the robot only cleans edges of an environment), a predefined duration cleaning behavior (e.g., less than 60 minutes, less than 30 minutes, etc.), a low-power cleaning behavior (e.g., to clean using low vacuum power to reduce power consumption), a high-powering behavior cleaning behavior (e.g., to clean using high vacuum power for improved vacuum performance), and/or a spot cleaning behavior (e.g., where the robot cleans a bounded area).</p><p id="p-0158" num="0157">In other implementations, the behavior includes a map training behavior. For example, the robot drives around the home to map the environment as part of a map training process. The cleaning systems are turned off during a map training behavior.</p><p id="p-0159" num="0158">In other implementations, the behaviors can be user assigned. For example, a user can use an application that runs on a mobile device to assign a particular behavior to a particular type of user actuation. In this example, the user can assign a cleaning behavior with a short hold (e.g., 2-5 second) actuation of the button instead of a tap (e.g., less than 2 second actuation). Similarly, the user can redefine the conditions described above with reference to the flows charts in <figref idref="DRAWINGS">FIGS. <b>4</b>-<b>6</b></figref> such that the controller of the robot selects different behavior than what is represented in those flow charts. In this way, the controller can select the behavior based on user assigned conditions which associate the behaviors to the various actuation durations (e.g., tap, short hold, hold, long hold, etc.), actuation sequences (e.g., a number of times the button is actuated within a time interval), sensors (e.g., bin sensors, docking sensors, wheel sensors, bumper sensors, etc.), and current states of the robot (e.g., awake state, sleep state, cleaning state, etc.)</p><p id="p-0160" num="0159">While robot <b>100</b> is described as having a single button (e.g., button <b>102</b>), some robots can have multiple buttons. For example, some of the buttons can be dedicated function buttons (e.g., a dedicated button to return to a docking station) while one of the buttons is a multi-function button described herein (e.g., button <b>102</b>).</p><p id="p-0161" num="0160">In some implementations, the button <b>102</b> is a mechanical switch, a capacitance switch, and/or a touchscreen display. For example, if the button <b>102</b> is implemented as part of a touchscreen display, the light indicator <b>210</b> can be part of the touchscreen display as well. In this example, the entire surface of the button <b>102</b> can be used for illumination and also serve as a surface for the button.</p><p id="p-0162" num="0161">Accordingly, other implementations are within the scope of the claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An autonomous cleaning robot comprising:<claim-text>a drive configured to maneuver the autonomous cleaning robot about a floor surface;</claim-text><claim-text>a cleaning system to clean the floor surface as the autonomous cleaning robot is maneuvered about the floor surface;</claim-text><claim-text>a robot button positioned on the autonomous cleaning robot; and</claim-text><claim-text>a controller in electrical communication with the drive and the robot button, the controller configured to perform operations comprising:<claim-text>selecting a behavior of the autonomous cleaning robot from a plurality of behaviors of the autonomous cleaning robot responsive to a duration of actuation of the robot button; and</claim-text><claim-text>causing the autonomous cleaning robot to initiate the behavior.</claim-text></claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The autonomous cleaning robot of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein selecting the behavior from the plurality of behaviors of the autonomous cleaning robot comprises selecting among at least three different behaviors of the autonomous cleaning robot.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The autonomous cleaning robot of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein selecting the behavior from the plurality of behaviors of the autonomous cleaning robot is further based on sensor information generated from one or more sensors of the autonomous cleaning robot, the sensor information being independent from the actuation of the robot button.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The autonomous cleaning robot of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the sensor information represents one or more of: an actuation of a bumper of the autonomous cleaning robot, a position of a wheel of the drive of the autonomous cleaning robot, a presence of a bin attached to the autonomous cleaning robot, or whether the autonomous cleaning robot is docked with a docking station.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The autonomous cleaning robot of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein selecting the behavior from the plurality of behaviors of the autonomous cleaning robot is further based on one or more current states of the autonomous cleaning robot, the one or more current states comprising one or more of the following states: a sleep state, an awake state, an idle state, a cleaning state, a docking state, a child-lock behavior state, or a reboot state.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The autonomous cleaning robot of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein selecting the behavior from the plurality of behaviors of the autonomous cleaning robot is further based on a sequence of the actuation of the robot button, the sequence comprising a number of times the robot button is actuated within a time interval.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The autonomous cleaning robot of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the plurality of behaviors comprise a first behavior of the autonomous cleaning robot and a second behavior of the autonomous cleaning robot, wherein selecting the behavior comprises selecting the behavior based on the duration such that the selected behavior corresponds to the first behavior if the robot button is actuated for a first duration and the selected behavior corresponds to the second behavior if the robot button is actuated for a second duration longer than the first duration.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The autonomous cleaning robot of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the robot button comprises a light ring configured to illuminate a portion of a top surface of the autonomous cleaning robot, wherein the controller is configured to control a length of the illuminated portion based on the duration of the actuation of the robot button.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. A method of operating an autonomous cleaning robot, the method comprising:<claim-text>selecting, by a processor, a behavior of the autonomous cleaning robot from a plurality of behaviors of the autonomous cleaning robot based on at least a duration of actuation of a robot button of the autonomous cleaning robot;</claim-text><claim-text>generating, by the processor, a control signal based on the behavior; and</claim-text><claim-text>causing the autonomous cleaning robot to initiate the behavior using the control signal.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein selecting the behavior from the plurality of behaviors of the autonomous cleaning robot comprises selecting among at least three different behaviors.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein selecting the behavior from the plurality of behaviors of the autonomous cleaning robot is further based on sensor information generated from one or more sensors of the autonomous cleaning robot, the sensor information representing one or more of: an actuation of a bumper of the autonomous cleaning robot, a position of a wheel of a drive of the autonomous cleaning robot, a presence of a bin attached to the autonomous cleaning robot, or whether the autonomous cleaning robot is docked with a docking station.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein selecting the behavior from the plurality of behaviors of the autonomous cleaning robot is further based on one or more current states of the autonomous cleaning robot, the one or more current states comprising one or more of the following states: a sleep state, an awake state, an idle state, a cleaning state, a docking state, a child-lock behavior state, or a reboot state.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein selecting the behavior from the plurality of behaviors of the autonomous cleaning robot is further based on a sequence of the actuation of the robot button, the sequence comprising a number of times the robot button is actuated within a time interval.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the plurality of behaviors comprise a first behavior of the autonomous cleaning robot and a second behavior of the autonomous cleaning robot, wherein selecting the behavior comprises selecting the behavior based on the duration such that the selected behavior corresponds to the first behavior if the robot button is actuated for a first duration and the selected behavior corresponds to the second behavior if the robot button is actuated for a second duration longer than the first duration.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, further comprising:<claim-text>illuminating a portion of a light ring of the autonomous cleaning robot; and</claim-text><claim-text>controlling a length of the illuminated portion based on the duration of the actuation of the robot button.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. A non-transitory computer-readable storage medium comprising at least one program for execution by a processor of an autonomous cleaning robot, the at least one program including instructions which, when executed by the processor, cause the autonomous cleaning robot to perform operations comprising.<claim-text>selecting, by the processor, a behavior of the autonomous cleaning robot from a plurality of behaviors of the autonomous cleaning robot based on at least a duration of actuation of a robot button of the autonomous cleaning robot;</claim-text><claim-text>generating, by the processor, a control signal based on the behavior; and</claim-text><claim-text>causing the autonomous cleaning robot to initiate the behavior using the control signal.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein selecting the behavior from the plurality of behaviors of the autonomous cleaning robot comprises selecting among at least three different behaviors and selecting the behavior from the plurality of behaviors of the autonomous cleaning robot is further based on sensor information generated from one or more sensors of the autonomous cleaning robot, the sensor information being independent from the actuation of the robot button.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein selecting the behavior from the plurality of behaviors of the autonomous cleaning robot is further based on one or more current states of the autonomous cleaning robot, the one or more current states comprising one or more of the following states: a sleep state, an awake state, an idle state, a cleaning state, a docking state, a child-lock behavior state, or a reboot state.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein selecting the behavior from the plurality of behaviors of the autonomous cleaning robot is further based on a sequence of the actuation of the robot button, the sequence comprising a number of times the robot button is actuated within a time interval.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the plurality of behaviors comprise a first behavior of the autonomous cleaning robot and a second behavior of the autonomous cleaning robot, wherein selecting the behavior comprises selecting the behavior based on the duration such that the selected behavior corresponds to the first behavior if the robot button is actuated for a first duration and the selected behavior corresponds to the second behavior if the robot button is actuated for a second duration longer than the first duration.</claim-text></claim></claims></us-patent-application>