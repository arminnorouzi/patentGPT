<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230007277A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230007277</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17631298</doc-number><date>20200930</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>169</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>132</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20141101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>169</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20141101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20141101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>132</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">IMMERSIVE VIDEO CODING USING OBJECT METADATA</invention-title><us-related-documents><us-provisional-application><document-id><country>US</country><doc-number>62908983</doc-number><date>20191001</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>INTEL CORPORATION</orgname><address><city>Santa Clara</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Salahieh</last-name><first-name>Basel</first-name><address><city>Santa Clara</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Yeung</last-name><first-name>Fai</first-name><address><city>Palo Alto</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Boyce</last-name><first-name>Jill</first-name><address><city>Portland</city><state>OR</state><country>US</country></address></addressbook></inventor></inventors></us-parties><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/US20/53635</doc-number><date>20200930</date></document-id><us-371c12-date><date>20220128</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Methods, apparatus, systems and articles of manufacture for video coding using object metadata are disclosed. An example apparatus includes an object separator to separate input views into layers associated with respective objects to generate object layers for geometry data and texture data of the input views, a pruner to project the first object layer of a first basic view of the at least one basic views against the first object layer of a first additional view of the at least one additional views to generate a first pruned view and a first pruning mask, a patch packer to tag a patch with an object identifier of the first object, the patch corresponding to the first pruning mask, and an atlas generator to generate at least one atlas to include in encoded video data, the atlas including the patch.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="97.87mm" wi="158.75mm" file="US20230007277A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="243.33mm" wi="172.97mm" orientation="landscape" file="US20230007277A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="247.06mm" wi="168.83mm" orientation="landscape" file="US20230007277A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="241.72mm" wi="175.18mm" orientation="landscape" file="US20230007277A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="244.69mm" wi="173.99mm" orientation="landscape" file="US20230007277A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="247.57mm" wi="175.51mm" orientation="landscape" file="US20230007277A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="241.30mm" wi="168.83mm" orientation="landscape" file="US20230007277A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="246.55mm" wi="160.61mm" orientation="landscape" file="US20230007277A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="247.82mm" wi="175.18mm" orientation="landscape" file="US20230007277A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="239.10mm" wi="166.29mm" orientation="landscape" file="US20230007277A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="241.89mm" wi="173.06mm" file="US20230007277A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="241.89mm" wi="174.67mm" file="US20230007277A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="138.94mm" wi="149.01mm" file="US20230007277A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="235.54mm" wi="175.18mm" orientation="landscape" file="US20230007277A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="237.15mm" wi="173.23mm" file="US20230007277A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="236.14mm" wi="166.54mm" file="US20230007277A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="213.19mm" wi="152.91mm" file="US20230007277A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="213.19mm" wi="152.91mm" file="US20230007277A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="171.20mm" wi="137.84mm" file="US20230007277A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">RELATED APPLICATION</heading><p id="p-0002" num="0001">This patent claims the benefit of U.S. Provisional Patent Application Ser. No. 62/908,983, which was filed on Oct. 1, 2019. U.S. Provisional Patent Application Ser. No. 62/908,983 is hereby incorporated herein by reference in its entirety. Priority to U.S. Provisional Patent Application Ser. No. 62/908,983 is hereby claimed.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">FIELD OF THE DISCLOSURE</heading><p id="p-0003" num="0002">This disclosure relates generally to video coding, and, more particularly, to immersive video coding using object metadata.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">Video codecs are electronic circuits or software that compress or decompress digital video. For example, video codecs may be used to convert uncompressed video to a compressed format, and/or vice versa.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0005" num="0004"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a block diagram of an example Moving Picture Experts Group (MPEG) Immersive Video (MIV) encoder.</p><p id="p-0006" num="0005"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates a block diagram of an example MPEG Immersive Video (MIV) decoder.</p><p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates a block diagram of an example object-based MIV coding system implemented in accordance with teachings of this disclosure.</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates an example block diagram of an example implementation of the example object-based MIV encoder included in the example object-based MIV coding system of <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates an example block diagram of an example implementation of the example MIV decoder and the example renderer included in the example object-based MIV coding system of <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates example immersive data.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates example input views and patches.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates an example Video-Based Volumetric Video Coding (V3C) sample stream with MIV extensions.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>9</b></figref> illustrates an example pruning graph for example basic views and example additional views.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>10</b></figref> illustrates an example object-based pruning process.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>11</b></figref> illustrates an example object-view pruning scheme.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>12</b></figref> illustrates an example 3D bounding box.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>13</b></figref> illustrates an example object-based synthesized scene.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a flowchart representative of machine readable instructions which may be executed to implement the object-based MIV encoder of <figref idref="DRAWINGS">FIGS. <b>3</b> and/or <b>4</b></figref>.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a flowchart representative of machine readable instructions which may be executed to implement the example MIV decoder and the example renderer of <figref idref="DRAWINGS">FIGS. <b>3</b> and/or <b>5</b></figref>.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>16</b></figref> is a block diagram of an example processing platform structured to execute the instructions of <figref idref="DRAWINGS">FIG. <b>14</b></figref> to implement the object-based MIV encoder of <figref idref="DRAWINGS">FIGS. <b>3</b> and/or <b>4</b></figref>.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is a block diagram of an example processing platform structured to execute the instructions of <figref idref="DRAWINGS">FIG. <b>15</b></figref> to implement the example MIV decoder and the example renderer of <figref idref="DRAWINGS">FIGS. <b>3</b> and/or <b>5</b></figref>.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>18</b></figref> is a block diagram of an example software distribution platform to distribute software (e.g., software corresponding to the example computer readable instructions of <figref idref="DRAWINGS">FIGS. <b>14</b>-<b>15</b></figref>) to client devices such as consumers (e.g., for license, sale and/or use), retailers (e.g., for sale, re-sale, license, and/or sub-license), and/or original equipment manufacturers (OEMs) (e.g., for inclusion in products to be distributed to, for example, retailers and/or to direct buy customers).</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0023" num="0022">The figures are not to scale. In general, the same reference numbers will be used throughout the drawing(s) and accompanying written description to refer to the same or like parts.</p><p id="p-0024" num="0023">Unless specifically stated otherwise, descriptors such as &#x201c;first,&#x201d; &#x201c;second,&#x201d; &#x201c;third,&#x201d; etc. are used herein without imputing or otherwise indicating any meaning of priority, physical order, arrangement in a list, and/or ordering in any way, but are merely used as labels and/or arbitrary names to distinguish elements for ease of understanding the disclosed examples. In some examples, the descriptor &#x201c;first&#x201d; may be used to refer to an element in the detailed description, while the same element may be referred to in a claim with a different descriptor such as &#x201c;second&#x201d; or &#x201c;third.&#x201d; In such instances, it should be understood that such descriptors are used merely for identifying those elements distinctly that might, for example, otherwise share a same name. As used herein, &#x201c;approximately&#x201d; and &#x201c;about&#x201d; refer to dimensions that may not be exact due to manufacturing tolerances and/or other real world imperfections. As used herein &#x201c;substantially real time&#x201d; refers to occurrence in a near instantaneous manner recognizing there may be real world delays for computing time, transmission, etc. Thus, unless otherwise specified, &#x201c;substantially real time&#x201d; refers to real time +/&#x2212;1 second.</p><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0025" num="0024">Advancements in digital media technology are enabling delivering of compelling immersive experiences with new media formats. The Moving Picture Experts Group (MPEG) is one standardization group that is developing standards to support immersive media access and delivery. For example, the coded representation of immersive media (MPEG-I) is a set of immersive media industry standards directed to immersive media formats, such as panoramic 360&#xb0; video, volumetric point-clouds, and immersive video.</p><p id="p-0026" num="0025">MPEG is developing an immersive video coding standard called MPEG immersive video (MIV). The MIV draft standard codes texture and depth video for multiple input views, each at a particular position and orientation, using the existing High Efficiency Video Coding (HEVC) video codec. The MIV decoder does not specify the reference renderer but supplies it with the required metadata and decoded streams. The intended output of the reference renderer is a perspective viewport of the texture, with the viewport selected based upon a viewer's position and orientation, and generated using the outputs of the immersive media decoder. The MIV standard enables the viewer to dynamically move with 6 Degrees of Freedom (6DoF), adjust position (x, y, z) and orientation (yaw, pitch, roll) within a limited range (e.g., as supported by a head mounted display, a two-dimensional (2D) monitor with positional inputs as examples, etc.).</p><p id="p-0027" num="0026">However, the MIV draft standard does not include representation of objects in a video scene. As used herein, the terms &#x201c;object&#x201d; or &#x201c;entity&#x201d; are used interchangeably and refer to actors, things, properties, etc., in images. For example, an object can correspond to a physical object (e.g., a person, a piece of furniture, etc.) and/or a part of a physical object (e.g., a hand of a person, a head of a person, a t-shirt a person is wearing, etc.) represented in the scene. Additionally or alternatively, an object can correspond to a segmentation of the scene based on one or more aspects of the scene, such as reflectance properties, material definitions, etc. For example, an object can also be materials (e.g., skin of a person, hair of a person, grass, etc.), colors (e.g., green, blue, etc.), etc. represented in the scene. Presently, the MIV draft standard lacks the capability of distinguishing between different objects in a video scene, separating the background or floor from other objects in a video scene, etc., and using such information to customize the transmitted stream and synthesized scene based on network conditions and visual experiences desired by the end user.</p><p id="p-0028" num="0027">Methods, apparatus, and systems for object-based immersive video coding are disclosed herein. For example, objects within the object-based immersive videos, as disclosed herein, can be indexed, filtered, and coded in better fidelity while being able to control bandwidth utilization and better support objects of interest. The techniques described herein thus enable object-based operations including identifying and separation of objects and/or separation of objects from the background, thereby enabling network adaptive, attention-based, and personal scene synthesis in immersive video coding standards. Example techniques disclosed herein can also improve quality and performance while enabling new immersive use cases.</p><p id="p-0029" num="0028">Example encoding techniques disclosed herein include receiving input views and assigning the input views as a basic view or an additional view, with the input views including texture maps, depth maps, and/or object maps. Disclosed example techniques also include generating object layers based on the object maps, the respective object layers each to include pixels associated with one corresponding object, with each object being associated with one or multiple corresponding object layers. Disclosed example techniques further include pruning input views by projecting object layers of basic views onto object layers of additional views and/or object layers of previously pruned views to generate pruning masks. Disclosed example techniques also include aggregating the pruning masks to form patches, where each patch includes data of a corresponding single object. Disclosed examples further include packing the patches into atlases and generating an encoded bitstream including the atlases and metadata. In some examples, the metadata includes a camera parameters list, an atlas parameters list, etc.</p><p id="p-0030" num="0029">Example decoding techniques disclosed herein include decoding an encoded bitstream into atlases (e.g., texture data, depth data) and metadata. Disclosed example techniques further include generating an occupancy map indicating the position of the patch in the atlas. Disclosed example techniques also include object filtering of the atlas data (e.g., to identify objects, to remove objects, etc.). Disclosed example techniques further include synthesizing a view/port based on the filtered atlas data, the metadata, and the desired viewing position and orientation.</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a block diagram of an example MEG Immersive Video (MIV) encoder <b>100</b>. The example MIV encoder <b>100</b> includes an example view labeler <b>102</b>, an example atlas constructor <b>104</b>, an example encoder <b>118</b>, and an example metadata composer <b>120</b>. The example MIV encoder <b>100</b> receives example input views <b>101</b>. For example, the input views <b>101</b> include texture data (e.g., texture bitstreams) and depth data (e.g., depth bitstreams) of the captured scene. As used herein, &#x201c;texture&#x201d; and &#x201c;attribute&#x201d; are used interchangeably and refer to visible aspects of a pixel, such as the color component (e.g., red-green-blue (RGB) components) of the pixel. As used herein, &#x201c;depth&#x201d; and &#x201c;geometry&#x201d; are used interchangeably unless noted otherwise as the geometry of a pixel typically includes the pixel's depth, which refers to the distance of the pixel from a reference plane. For example, the input views <b>101</b> can be source views generated by video capturers, virtual views generated by a computer, etc. In the illustrated example, the input views <b>101</b> are represented as a video sequence. The example MIV encoder <b>100</b> further receives an example source camera parameter list <b>103</b>. For example, the source camera parameter list <b>103</b> can include where source cameras are positioned, the angle of source cameras, etc.</p><p id="p-0032" num="0031">The example view labeler <b>102</b> selects views to encode. For example, the view labeler <b>102</b> analyzes the input views <b>101</b> and selects which views to encode. In some examples, the view labeler <b>102</b> labels views of the input views <b>101</b> as a basic view or an additional view. As used herein, a basic view is an input view that is to be packed in an atlas as a single patch. As used herein, an additional view is an input view that is to be pruned and packed in one or more patches. For example, the view labeler <b>102</b> can determine how many basic views there can be in the input views based on criteria such as pixel rate constraints, a sample count per input view, etc.</p><p id="p-0033" num="0032">The example atlas constructor <b>104</b> receives basic and/or additional views determined by the example view labeler <b>102</b>. The example atlas constructor <b>104</b> includes an example pruner <b>106</b>, an example aggregator <b>108</b>, and an example patch packer <b>110</b>. The example pruner <b>106</b> prunes the additional views. For example, the pruner <b>106</b> projects the depth and/or texture data of the basic views onto the respective depth and/or texture data of the additional views and/or the previously pruned views to extract non-redundant occluded regions.</p><p id="p-0034" num="0033">The example aggregator <b>108</b> aggregates the pruning results generated by the example pruner <b>106</b> to generate patches. For example, the aggregator <b>108</b> accumulates the pruning results (e.g., pruning masks) over an intra-period (e.g., a predetermined collection of frames) to account for motion. In the illustrated example, the patches include an example texture component <b>112</b> and an example depth component <b>114</b>. The example patch packer <b>110</b> packs the patches into one or more example atlases <b>116</b>. For example, the patch packer <b>110</b> performs clustering (e.g., combining pixels in a pruning mask to form patches) to extract and pack rectangular patches into atlases. In some examples, the patch packer <b>110</b> updates the content per frame across the processed intra-period.</p><p id="p-0035" num="0034">The patch packer <b>110</b> tags the patch with a patch identifier (e.g., a patch ID). The patch ID identifies the patch index. For example, the patch IDs can range from 0 to one less than the number of patches generated. The patch packer <b>110</b> can generate a block to patch map (e.g., a patch ID map). The block to patch map is a 2D array (e.g., representative of point positions/locations in an atlas) indicating the patch ID associated with a given block of one or more pixels. Additionally or alternatively, the patch packer <b>110</b> generates an occupancy map to indicate whether a pixel of the patch is occupied (e.g., valid) or unoccupied (e.g., invalid). That is, the occupancy map is a binary map. In some examples, the occupancy map has a value of 1 to indicate a pixel is occupied and a value of 0 to indicate a pixel is not occupied.</p><p id="p-0036" num="0035">The example video encoder <b>118</b> generates encoded atlases. That is, the video encoder <b>118</b> encodes the example texture component <b>112</b> and the example depth component <b>114</b>. For example, the video encoder <b>118</b> receives the example atlases <b>116</b> and encodes the texture component <b>112</b> and the depth component <b>114</b> using an HEVC video encoder. However, the example video encoder <b>118</b> can additionally or alternatively use an Advanced Video Coding (AVC) video encoder, etc. In some examples, the video encoder <b>118</b> includes a video texture encoder and a video depth encoder.</p><p id="p-0037" num="0036">The example metadata composer <b>120</b> generates a metadata bitstream. For example, the metadata composer <b>120</b> receives an example camera parameters list <b>122</b> and an example atlas parameters list <b>124</b>. The example camera parameters list <b>122</b> includes how views (e.g., the basic views and/or the additional views) are placed and oriented in space. The example atlas parameters list <b>124</b> includes how patches are mapped between the atlases <b>116</b> and the views. For example, the atlas parameters list <b>124</b> can include the occupancy map and/or the block to patch map. The example MIV encoder <b>100</b> generates an encoded bitstream <b>126</b>. For example, the WV encoder <b>100</b> multiplexes the encoded atlases and the metadata bitstream together.</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates a block diagram of an example MIV decoder <b>200</b>. The example MIV decoder <b>200</b> includes an example video decoder <b>202</b>, an example metadata parser <b>204</b>, an example atlas patch occupancy map generator <b>206</b>, and an example reference renderer <b>208</b>.</p><p id="p-0039" num="0038">The example MIV decoder <b>200</b> receives the encoded bitstream <b>126</b> (<figref idref="DRAWINGS">FIG. <b>1</b></figref>). The example video decoder <b>202</b> generates a sequence of decoded picture pairs of example decoded texture pictures <b>210</b> and example decoded depth pictures <b>212</b>. In the illustrated example, the example decoded texture pictures <b>210</b> and the example decoded depth pictures <b>212</b> represent an example atlas <b>214</b>. For example, the video decoder <b>202</b> can be an HEVC decoder. In some examples, the decoder <b>202</b> includes a video texture decoder and a video depth decoder.</p><p id="p-0040" num="0039">The example metadata parser <b>204</b> parses the encoded bitstream <b>126</b> to generate an example atlas parameters list <b>216</b> and an example camera parameters list <b>218</b>. For example, the metadata parser <b>204</b> parses the encoded bitstream <b>126</b> for the example atlas parameters list <b>124</b> (<figref idref="DRAWINGS">FIG. <b>1</b></figref>) to generate the atlas parameters list <b>216</b>. Additionally or alternatively, the example metadata parser <b>204</b> parses the encoded bitstream <b>126</b> for the example camera parameters list <b>122</b> (<figref idref="DRAWINGS">FIG. <b>1</b></figref>) to generate the camera parameters list <b>218</b>.</p><p id="p-0041" num="0040">The example atlas patch occupancy map generator <b>206</b> generates an example atlas patch occupancy map <b>220</b>. In the illustrated example, the atlas patch occupancy map <b>220</b> is the same size as the atlas <b>214</b> and indicates whether the pixel of the atlas <b>214</b> is occupied. For example, the atlas patch occupancy map generator <b>206</b> accesses the example decoded depth pictures <b>212</b> and the example atlas parameters list <b>216</b> to generate a decoded occupancy map (e.g., the atlas patch occupancy map <b>220</b>).</p><p id="p-0042" num="0041"><b>100411</b> The example reference renderer <b>208</b> generates an example viewport <b>222</b>. For example, the reference renderer <b>208</b> accesses one or more of the decoded atlases <b>214</b>, the atlas parameters list <b>216</b>, the camera parameters list <b>218</b>, the atlas patch occupancy map <b>220</b>, and the viewer position and orientation <b>224</b>. That is, the example reference renderer <b>208</b> outputs a perspective viewport of the texture images, selected based on the example viewer position and orientation <b>224</b>.</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates a block diagram of an example object-based MIV coding system <b>300</b> implemented in accordance with teachings of this disclosure. The example object-based MIV coding system <b>300</b> includes an example server side immersive video encoding system <b>302</b> and an example client side immersive video decoding system <b>350</b>. The example server side immersive encoding system <b>302</b> includes one or more example video capturers <b>304</b>, an example segmenter <b>305</b>, an example virtual camera renderer <b>306</b>, example object and depth information storage <b>308</b>, an example object-based MIV encoder <b>310</b>, an example video encoder <b>312</b>, an example stream packager <b>314</b>, an example content management system <b>315</b>, and an example content distribution network <b>316</b>.</p><p id="p-0044" num="0043">The example video capturer(s) <b>304</b> capture image data. For example, the video capturer(s) <b>304</b> can include and/or interface with an image sensor (e.g., a camera) to capture frames of images. In some examples, the video capturer(s) <b>304</b> implement an array of image sensors to capture frames of images at different angles. The example virtual camera renderer <b>306</b> obtains data from the example video capturer(s) <b>304</b> (e.g., frames of images). For example, the virtual camera renderer <b>306</b> determines depth information and/or object segmentation information of the data captured. The server side immersive video encoding system <b>302</b> receives the virtual camera data.</p><p id="p-0045" num="0044">The example object and depth information storage <b>308</b> stores object and depth data. For example, the object and depth information storage <b>308</b> stores object maps generated by the example segmenter <b>305</b>. In some examples, the segmenter <b>305</b> generates object identifiers (e.g. object IDs) for each pixel of immersive data (e.g., the texture data and the depth data). For example, the segmenter <b>305</b> uses a machine-learning classifier, a conventional classifier with or without pattern recognition, and/or a segmentation algorithm to analyze pixels of the immersive data (e.g., texture and depth data) to (i) identify one or more different objects represented in the views and (ii) assign object ID(s) to pixels of the views such that the pixels representing the same object in different views are assigned a same object ID. That is, pixels of different views belonging to the same object will have the same object ID. The segmenter <b>305</b> generates one or more object maps to store the object IDs for each pixel of immersive data. Additionally or alternatively, the example video capturer(s) <b>304</b> can capture and assign an object ID to individual objects in a similar manner and then populate the objects in the same scene. In some examples, the object IDs are provided by a computer-generated graphics program. That is, object IDs can be assigned to objects during graphics generation. For example, a video game can generate a video labeled with object identifiers (e.g., the video game provides object identifiers for each object in a video sequence).</p><p id="p-0046" num="0045">In examples disclosed herein, the immersive data generated by the virtual camera renderer <b>306</b> includes texture data, geometry data, and object IDs. In some examples, the immersive data includes texture maps, geometry maps, and object maps. For example, the (X, Y, Z) values of a pixel represent the geometry (e.g., location) of the pixel in the depth map with respect to a reference 3D global coordinate system (e.g., having 3 orthogonal dimensions labeled the X, Y and Z dimensions). The (R, G, B) values of the pixel represent texture components (e.g., three color components, such as red, green and blue color components) of the pixel in the texture map. The object ID value of the pixel identifies the object represented by the corresponding pixel in the texture and depth maps.</p><p id="p-0047" num="0046">In examples disclosed herein, an object ID identifies an object so that there are no duplicated object IDs for different immersive data objects in a scene within a finite time period. In some examples, the number of bits used to represent the object ID depends on the number of objects supported in the object-based MIV coding system <b>300</b>. For example, the number of bits to encode object identifiers can have a fixed length or a variable length depending on the total number of objects in a scene. For example, the object maps can be the same resolution as the texture and depth maps, but the bitdepth of the object map (e.g., the number of bits used to represent each entry of the object map) depends on the number of objects in the scene. For example, the object ID can be represented by a byte, with the last bit reserved to indicate an extension. In some such examples, the extendibility of the object ID is to support usages with a large number (e.g., thousands) of possible objects. For example, if the images captured by the video capturer(s) <b>304</b> include six objects, the bitdepth of the object maps may include at least three bits to store the six object IDs. In another example, if the images captured at the video capturer(s) <b>304</b> include 20,000 objects, the bitdepth of the object maps may include at least 16 bits to store 20,000 object IDs. In some examples, the number of bits used to store object IDs in the object maps changes based on the number of objects in a scene, thereby reducing the number of unused bits. Additionally or alternatively, a normalized format of object IDs can use eight bits to store up to 256 objects, 16 bits to store 256-65536 objects, etc.</p><p id="p-0048" num="0047">The example object-based MIV encoder <b>310</b> receives immersive video data (e.g., generated by the example virtual camera renderer <b>306</b>) and/or the object and depth information storage <b>308</b>. The object-based MIV encoder <b>310</b> performs object-based encoding of the immersive data. For example, the object-based MIV encoder <b>310</b> prunes the plurality of views and generates patches. In examples disclosed herein, the object-based MIV encoder <b>310</b> generates patches such that each patch is associated with a single object, and different patches can be associated with the same or different objects. The object-based MIV encoder <b>310</b> packs the generated patches in atlases. In some examples, the object-based MIV encoder <b>310</b> generates occupancy map bitstreams including an occupancy map. For example, the occupancy map indicates whether pixels of the atlases are occupied.</p><p id="p-0049" num="0048">The example object-based MIV encoder <b>310</b> enables object-based network scalability. That is, the object-based MIV encoder <b>310</b> identifies and processes immersive objects and/or backgrounds of various views separately, enabling object-based scalability for adaptive streamlining over different network conditions. For example, patches belonging to objects not of interest can be entirely dropped (e.g., not included in the encoded bitstream) or encoded at a lower visual quality than objects of interest (e.g., relatively less bandwidth is allocated to objects not of interest).</p><p id="p-0050" num="0049">The example object-based MIV encoder <b>310</b> enables an attention-based user experience. An attention-based user experience allows viewers to focus on objects of interest when certain actions occur in the scene. For example, the object-based MIV encoder <b>310</b> can use motion activity (e.g., the depth data over a plurality of frames) to determine the location of action. For example, an action in a sports game can be the motion of a ball. The object-based MIV encoder <b>310</b> allocates a relatively higher hit budget to the patches of the immersive objects belonging to the action collection. Thus, users can zoom into the higher resolution patches to see a high fidelity rendering of the object of interest.</p><p id="p-0051" num="0050">The example object-based MIV encoder <b>310</b> enables object-based variable patch rate encoding. For example, patches of background and/or static objects (e.g., inferred from the object labels) can be signaled once per intra-period, while patches of moving objects can be signaled every frame. The example pruners included in the example object-based MIV encoder <b>310</b> determine whether to extract object patches once per intra-period, once per frame, etc., based on object labels within the bounding box Supplemental Enhanced Information (SEI) message of the object.</p><p id="p-0052" num="0051">The example object-based MIV encoder <b>310</b> enables object tracking. For example, the object-based MIV encoder <b>310</b> can synthesize a viewport video to track the movement of a specific immersive object using bounding box information associated with the object (e.g., &#x201c;follow object&#x201d; navigation path).</p><p id="p-0053" num="0052">The example video encoder <b>312</b> performs video encoding. For example, the video encoder <b>312</b> encodes frames captured from virtual cameras (e.g., 360&#xb0; videos) in separate channels to support backward compatibility. For example, the video encoder <b>312</b> encodes data compatible with decoders that do not support MIV (e.g., the consumer device at the client side immersive video decoding system <b>350</b> does not support a MIV decoder). In some examples, the server side immersive video encoding system <b>302</b> does not include the video encoder <b>312</b>.</p><p id="p-0054" num="0053">The example stream packager <b>314</b> combines the encoded bitstreams together. That is, the example stream packager <b>314</b> produces a multiplexed bitstream. For example, the stream packager <b>314</b> combines the encoded bitstream of the object-based MIV encoder <b>310</b> and the encoded bitstream of the video encoder <b>312</b>. In some examples, the stream packager <b>314</b> adds metadata indicating various assets in the scene. For example, the stream packager <b>314</b> adds the frame rate of the encoded bitstream, the resolution of the encoded bitstream, the amount of bandwidth available, etc.</p><p id="p-0055" num="0054">The example content management system <b>315</b> controls imaging modalities streamed to the client side immersive video decoding system <b>350</b>. For example, the server side immersive video encoding system <b>302</b> generates view from different modalities of imaging (e.g., point cloud images, 360&#xb0; video, etc.) capturing the same scene. The content management system <b>315</b> determines whether to selects subsets of the modalities. For example, if a user is not viewing 360&#xb0; content, the content management system <b>315</b> extracts viewport corresponding to the viewer position and orientation.</p><p id="p-0056" num="0055">The example content distribution network <b>316</b> is a network used to transmit the multiplexed bitstream to the example client side immersive video decoding system <b>350</b>. In some examples, the content distribution network <b>316</b> can be the Internet or any other suitable external network. Additionally or alternatively, any other suitable means of transmitting the multiplexed bitstream to the client side immersive video decoding system <b>350</b> can be used.</p><p id="p-0057" num="0056">The example client side immersive video decoding system <b>350</b> includes an example client player <b>352</b>. In the illustrated example of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the client player <b>352</b> includes an example de-packager <b>354</b>, an example MIV decoder <b>356</b>, an example presentation engine <b>358</b>, an example video decoder <b>360</b>, and an example video renderer <b>362</b>.</p><p id="p-0058" num="0057">The example de-packager <b>354</b> demultiplexes the multiplexed bitstream. That is, the example de-packager <b>354</b> receives the multiplexed bitstream from the example server side immersive video encoding system <b>302</b> via the example content distribution network <b>316</b> and demultiplexes the bitstream to generate substreams (e.g., the video bitstream, the MIV bitstream, etc.).</p><p id="p-0059" num="0058">The example MIV decoder <b>356</b> decodes the MIV bitstream. For example, the MIV decoder <b>356</b> decompresses object map bitstreams and/or patch bitstreams to reconstruct geometry data. An example implementation of the MIV decoder <b>356</b> is described below in conjunction with <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0060" num="0059">The example presentation engine <b>358</b> maps the decoded atlases and metadata generated by the MIV decoder <b>356</b> to the projection format of the client player <b>352</b>. For example, the presentation engine <b>358</b> reconstructs the input views. For example, the presentation engine <b>358</b> reconstructs the input views based on the projection format (e.g., rectangular projection, perspective projection, orthographic projection, etc.).</p><p id="p-0061" num="0060">The example video decoder <b>360</b> decodes the video bitstream. For example, the video decoder <b>360</b> decompresses the texture and geometry bitstreams. In some examples, the video decoder <b>360</b> matches the video encoder <b>312</b> (e.g., uses the same video coding format). For example, the video decoder <b>360</b> can be an HEVC decoder.</p><p id="p-0062" num="0061">The example video renderer <b>362</b> generates viewport (e.g., volumetric) content. For example, the video renderer <b>362</b> receives the reconstructed input views from the presentation engine <b>358</b> and the decoded video bitstream from the video decoder <b>360</b> and generates volumetric content. Additionally or alternatively, the video renderer <b>362</b> performs object filtering, occupancy reconstruction, pruned view reconstruction, and synthesizes a view. For example, the video renderer <b>362</b> identifies an object ID corresponding to a selected object to be removed from the viewport. The video renderer <b>362</b> removes the patches corresponding to the identified object ID. The volumetric content is displayed to a user of the client player <b>352</b>. An example implementation of the video renderer <b>362</b> is described below in conjunction with <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0063" num="0062">In examples disclosed herein, the video renderer <b>362</b> performs priority rendering. For example, the video renderer <b>362</b> updates background objects and/or static objects (e.g., determined based on the object IDs) once per intra-period. As used herein, a static object is an object that has moved an imperceptible amount by a user (e.g., as determined by one or more criteria, such as one or more thresholds, percentages, etc.). For example, depending on the application, a static object may not have a displacement greater than one foot, one inch, etc., with respect to a previous frame. That is, the background objects and static objects are assumed to not change substantially over an intra-period. For example, the video renderer <b>362</b> can identify object IDs of objects of interest to synthesize and output those objects first (e.g., objects that should be rendered in real-time). The video renderer <b>362</b> obtains the patches corresponding to the identified object IDs and synthesizes those patches. Thus, the video renderer <b>362</b> synthesizes background objects and/or static objects at a relatively later time (e.g., once per intra-period). This saves computing resources and bandwidth, allowing for greater bandwidth allocation to objects of interest.</p><p id="p-0064" num="0063">In some examples, the video renderer <b>362</b> performs object-based averaging. For example, the video renderer <b>362</b> blends pixels belonging to the same object based on object IDs in object maps. That is, the video renderer <b>362</b> identities pixels corresponding to the same object ID. Thus, the sharpness of objects' edges and the fidelity of the rendered views is increased. In examples in which there are pixels belonging to more than one object on the same rendered pixels (e.g., overlapping objects), the video renderer <b>362</b> averages pixels of the dominant object (e.g., the object with the maximum number of samples across various views on the same pixel location) and excludes the pixels of the non-dominant objects.</p><p id="p-0065" num="0064">In some examples, the video renderer <b>362</b> identifies objects of interest. For example, the video renderer <b>362</b> identifies a bounding box tagged with the object ID of the object of interest (e.g., the bounding box includes objects of interest) that was identified by the example object-based MIV encoder <b>310</b>. In some examples, the video renderer <b>362</b> blurs the pixels outside of the bounding box (e.g., the background, objects outside the bounding box, etc.) and synthesizes a view of the bounding box. In some examples, the video renderer <b>362</b> renders the bounding box at a higher resolution (e.g., for a zooming application, etc.).</p><p id="p-0066" num="0065">In some examples, the example video renderer <b>362</b> performs background rendering. For example, the video renderer <b>362</b> identifies the object ID corresponding to the background and renders patches associated with the object ID of the background separately from the objects of the scene. In some examples, the patches generated by the object-based MIV encoder <b>310</b> include regions in the background that are missing information (e.g., not visible) in any of the input views due to occlusions. The example video capturer(s) <b>304</b> (<figref idref="DRAWINGS">FIG. <b>3</b></figref>) can capture the scene without objects (e.g., before a sports game begins, etc.) and stream within the metadata a single image per intra-period to be used by the video renderer <b>362</b> to render the background. Additionally or alternatively, the video renderer <b>362</b> can filter out the captured background object (e.g., the patches tagged with the object ID of the background) and instead synthesize virtual content (e.g., render a background, render prerendered content, etc.). For example, the video renderer <b>362</b> can render a customized advertisement banner on a wall, replace a synthetic background with a custom design, change the color of different objects within the scene, etc.</p><p id="p-0067" num="0066">In some examples, the example video renderer <b>362</b> utilizes object-based varying patch rates. The example video renderer <b>362</b> determines whether to obtain samples from the patches within the current frame or the intra-period based on the dominant object ID of the to-be-rendered pixel. For example, the video renderer <b>362</b> determines to obtain patches tagged with the object ID of the background from the intra-period instead of the current frame in order to save bandwidth.</p><p id="p-0068" num="0067">In some examples, the example video renderer <b>362</b> performs object-based scaling. For example, the video renderer <b>362</b> synthesize patches tagged with the object ID of an object of interest at a higher resolution in response to the object-based MIV encoder <b>310</b> forming patches of the object of interest at a larger resolution.</p><p id="p-0069" num="0068">The example video renderer <b>362</b> allows for a personalized 6DoF user experience. For example, viewers of object-based immersive video can request to filter out uninteresting and/or unimportant (e.g., user preference) objects and keep only relevant or interesting objects. In some examples, the object-based MIV encoder <b>310</b> signals objects of interest based on bounding box attributes (e.g., object labels, location, size, etc.) streamed in an SEI message (e.g., all data and/or objects have been streamed to the client side immersive video decoding system <b>350</b>). The video renderer <b>362</b> removes patches tagged with object IDs that don't correspond to objects of interest (e.g., the video renderer <b>362</b> does not render the objects not of interest), allowing for a personalized experience for viewers to choose only content that matters to them.</p><p id="p-0070" num="0069"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates an example block diagram of an example implementation of the example object-based MIV encoder <b>310</b> included in the example object-based MIV coding system <b>300</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>. The example object-based MIV encoder <b>310</b> includes an example view labeler <b>402</b>, an example object-based atlas constructor <b>404</b>, an example depth occupancy coder <b>424</b>, an example metadata composer <b>430</b>, and an example bitstream encoder <b>438</b>.</p><p id="p-0071" num="0070">The example view labeler <b>402</b> selects views to encode. For example, the view labeler <b>402</b> analyzes example input views <b>401</b> and selects which views to encode. The example input views <b>401</b> include texture images and depth images. Additionally or alternatively, the input views <b>401</b> include object maps corresponding to the texture and depth images. For example, the pixels of the object maps include an object ID identifying the object represented in the corresponding pixels of the texture and depth images. In some examples, the view labeler <b>402</b> labels views of the input views <b>401</b> as a basic view or an additional view. In some examples, the view labeler <b>402</b> outputs the input views as either a basic view or an additional view. In such examples, the example object-based MIV encoder <b>310</b> generates one or more atlases with complete views and patches taken from the additional views. As used herein, complete views correspond to the basic views. Additionally or alternatively, the example view labeler <b>402</b> outputs only basic views (e.g., the view labeler <b>402</b> filters out additional views). In such examples, the example object-based MIV encoder <b>310</b> generates one or more atlases with only complete views.</p><p id="p-0072" num="0071">The example object-based atlas constructor generates atlases. The example object-based atlas constructor <b>404</b> includes an example object loader <b>406</b>, an example object separator <b>408</b><i>a, </i>an example object separator <b>408</b><i>b, </i>an example pruner <b>410</b><i>a, </i>an example pruner <b>410</b><i>b, </i>an example object masker <b>412</b>, an example aggregator <b>414</b>, an example object clusterer <b>416</b>, an example patch packer <b>418</b>, and an example atlas generator <b>420</b>. For example, the object loader <b>406</b>, the object separators <b>408</b><i>a, </i><b>408</b><i>b, </i>the example pruners <b>410</b><i>a, </i><b>410</b><i>b, </i>and the example object masker <b>412</b> perform frame level operations. The example object clusterer <b>416</b>, the example patch packer <b>418</b>, and the example atlas generator <b>420</b> perform intra-period level operations.</p><p id="p-0073" num="0072">The example object loader <b>406</b> obtains object maps. For example, the object loader <b>406</b> obtains object maps included in the input views <b>401</b>. In examples disclosed herein, the input views <b>401</b> include texture maps, depth maps, and object maps. An object map indicates the pixels related to different objects in the scene, such that pixels from different views belonging to the same object have the same object identifier (e.g., object ID). As used herein, the terms &#x201c;object ID&#x201d; and &#x201c;object index&#x201d; are used interchangeably. The example object loader <b>406</b> obtains the associated object map of the frame being processed (e.g., a basic view, an additional view, etc.).</p><p id="p-0074" num="0073">The example object separators <b>408</b><i>a, </i><b>408</b><i>b </i>convert the texture and depth maps into object layers based on the associated object map of the view. As used herein, an object layer includes data (e.g., texture and/or depth) corresponding to a singular object. For example, the object separator <b>408</b><i>a </i>generates object layers of texture and depth maps corresponding to object A and the example object separator <b>408</b><i>b </i>generates object layers of texture and depth maps corresponding to object B. In some examples, the object separator <b>408</b><i>a </i>tags the object layer with the object ID of the object A and the object separator <b>408</b><i>b </i>tags the object layer with the object ID of the object B. While the illustrated example of <figref idref="DRAWINGS">FIG. <b>4</b></figref> includes the object separators <b>408</b><i>a, </i><b>408</b><i>b, </i>examples disclosed herein can include a fewer or greater number of object separators <b>408</b>. For example, the object-based atlas constructor <b>404</b> can include k number of object separators <b>408</b>, wherein k is the number of objects in the scene.</p><p id="p-0075" num="0074">In some examples, the object separators <b>408</b><i>a, </i><b>408</b><i>b </i>associate each object with a corresponding bounding box. That is, the example MIV decoder <b>356</b> and the example video renderer <b>362</b> can efficiently identify, label, localize, process, and render objects in corresponding bounding boxes rather than accessing all pixels across the atlases and/or patches. The object-based MIV encoder <b>310</b> includes a single 3D bounding box per object and updates the bounding box as needed. For example, the object separators <b>408</b><i>a, </i><b>408</b><i>b </i>tag the hounding box with the object ID that is included in the bounding box. In examples disclosed herein, the geometry of the bounding box is specified by a starting point. In some examples, the starting point is the top left corner of the bounding box and is defined as (x, y, z) with respect to a 3D reference global coordinate system (e.g., the same coordinate system that the camera parameters list within the metadata is referenced to). The dimensions of the bounding box are defined by width (w), height (h), and depth (d) (e.g., (w, h, d)). In some examples, the dimensions of the bounding box are stored as floating point values.</p><p id="p-0076" num="0075">The example bounding box can be signaled within a SEI message. For example, the SEI metadata indicates a rectangular cuboid per object. In some examples, the SEI message lists object labels (e.g., object IDs) where each SEI message can be used by multiple objects. For example, the object labels are indicated by the variable ar_label[ar_label_idx[i]] within the SEI message, wherein the variable ar_label_idx is the index of object parameters (e.g., bounding box, label, etc.) to be signaled and indicates the object ID of the related patches signaled in the variable atlas_params. That is, the annotated regions of SEI messages can be modified to account for 3D bounding boxes by adding front and depth components to the bounding box structure. For example, the variables ar_bounding_box_top[ar_object_idx [i]], ar_bounding_box_left[ar_object_idx [i]], ar_bounding_box_front[ar_object_idx [i]], ar_bounding_box_width[ar_object_idx [i]], ar_bounding_box_height [ar_object_idx [i]], and ar_bounding_box_depth[ar_object_idx [i]] specify the coordinates of the top-left-front corner (e.g., the starting point) and the width, height, and depth of the 3D bounding box of the ar_object_idx[i]<sup>th </sup>object relative to the 3D reference coordinate system.</p><p id="p-0077" num="0076">The example pruners <b>410</b><i>a, </i><b>410</b><i>b </i>prune the texture and depth maps to generate binary masks. As used herein, a binary mask indicates one of two values (e.g., occluded, non-occluded) for each pixel of the mask. For example, a binary mask can include a pixel value of 1 to indicate pixels to keep and a pixel value of 0 to indicate pixels to remove. That is, the example pruners <b>410</b><i>a, </i><b>410</b><i>b </i>project the object layers of the basic views onto the object layers of the additional views and/or object layers of the previously pruned views to extract non-redundant occluded regions. For example, the pruners <b>410</b><i>a, </i><b>410</b><i>b </i>perform pruning based on object ID. In such examples, the pruners <b>410</b><i>a, </i><b>410</b><i>b </i>project depth maps of the object layers of the basic views onto depth maps of the object layers of the additional views to generate patches for their respective objects. The pruners <b>410</b><i>a</i>, <b>410</b><i>b </i>perform the same patching process on the associated texture maps and object maps for their respective objects. While the illustrated example of <figref idref="DRAWINGS">FIG. <b>4</b></figref> includes the pruners <b>410</b><i>a, </i><b>410</b><i>b, </i>examples disclosed herein can include a fewer or greater number of pruners <b>410</b>. For example, the object-based atlas constructor <b>404</b> can include k number of pruners <b>410</b>, wherein k is the number of objects in the scene. An example illustration of the pruning process based on object ID is described in further detail below in connection with <figref idref="DRAWINGS">FIG. <b>9</b></figref>.</p><p id="p-0078" num="0077">The example pruners <b>410</b><i>a, </i><b>410</b><i>b </i>can additionally or alternatively perform object-based pruning. In such examples, the view labeler <b>402</b> identifies, synthesizes, and labels basic views and additional views from the input views <b>401</b>. The object separators <b>408</b><i>a, </i><b>408</b><i>b </i>divide the basic and additional views (e.g., texture and depth maps) based on the associated object maps into multiple layers where each layer corresponds to one object. That is, the object separator <b>408</b><i>a </i>generates an object layer by identifying pixels corresponding to the object ID of the object A, the object separator <b>408</b><i>b </i>generates an object layer by identifying pixels corresponding to the object ID of the object B, etc. The example pruner <b>410</b><i>a </i>projects the object layers of the basic views onto the object layers of the additional views of object A and/or the object layers of the previously pruned object layers. The example pruner <b>410</b><i>b </i>projects the object layers of the basic views onto the object layers of the additional views of object B and/or the object layers of the previously pruned object layers. That is, each of the pruners <b>410</b> project the object layers of the basic views onto the object layers of the additional views and/or object layers of the previously pruned views corresponding to a respective single object. Thus, the example pruner <b>410</b><i>a </i>generates masks corresponding to object A and the example pruner <b>410</b><i>b </i>generates masks corresponding to object B. In some examples, the pruners <b>410</b> perform object-based pruning for each object layer of the additional views, for some object layers of the additional views, etc. An example illustration of the example object-based pruning process is described in further detail below in connection with <figref idref="DRAWINGS">FIG. <b>10</b></figref>.</p><p id="p-0079" num="0078">The example pruners <b>410</b><i>a, </i><b>410</b><i>b </i>can additionally or alternatively perform object-view pruning. In such examples, the example view labeler <b>402</b> is implemented by an example object selector. The example object selector selects one projection of each object across all views based on the object maps, texture maps, and depth maps and labels the identified projection as the basic view. That is, the object selector labels only one input view as a basic view. In examples disclosed herein, the object selector selects the projection based one or more criteria such as the pixel count from object maps, the depth range from depth maps, and/or the number of features from texture maps. For example, the object selector selects the view having the largest pixel count from object maps (e.g., the view including the largest number of objects), the widest depth range from depth maps, and/or the maximum number of features from the texture maps in comparison to the remaining projections. The object selector labels the remaining views as additional views. The example pruner <b>410</b><i>a </i>projects the object layer of the basic view onto the object layers of the additional views and/or object layers of the previously pruned views of object A. The example pruner <b>410</b><i>b </i>projects the object layer of the basic view onto the object layers of the additional views and/or object layers of the previously pruned views of object B. That is, the pruners <b>410</b> project the object layer of the basic view onto the object layers of the additional views and/or object layers of the previously pruned views corresponding to a single object. Thus, the example pruner <b>410</b><i>a </i>generates masks corresponding to object A and the example pruner <b>410</b><i>b </i>generates masks corresponding to object B. In some examples, the pruners <b>410</b> perform object-view pruning for each object layer of the additional views, for some object layers of the additional views, etc. An example illustration of the example object-view pruning process is described in further detail below in connection with <figref idref="DRAWINGS">FIG. <b>11</b></figref>.</p><p id="p-0080" num="0079">The example object masker <b>412</b> obtains pruning masks (e.g., the masks generated by the example pruners <b>410</b><i>a, </i><b>410</b><i>b</i>). In examples disclosed herein, the pixels of the basic views' masks by default are turned on (e.g., the pixels of the basic views' masks are set to the maximum grey level). That is, the basic views' masks denote that the entire basic views are streamed in whole patches. However, in object-based coding, no patch can have pixels that correspond to more than one object (e.g., the patches each correspond to a respective singular object). Thus, the example object masker <b>412</b> refines the basic views' masks based on the object maps such that only the pixels corresponding to the object of the patch are turned on. Additionally or alternatively, the example object masker <b>412</b> refines the additional views' masks based on the object maps such that only the pixels corresponding to the object of the patch area are turned on. That is, the example object masker <b>412</b> generate refined masks for basic views and/or additional views.</p><p id="p-0081" num="0080">The example object masker <b>412</b> generates object masks based on the refined masks. That is, the example object masker <b>412</b> assigns the object ID of the patch to the pixels of the refined masks that are turned on. The example object masker <b>412</b> merges the object masks with object masks of the previous iteration (e.g., previous frame). This allows the example object-based atlas constructor <b>404</b> to track object IDs and tag patches with the corresponding object ID at a relatively later time. Additionally or alternatively, the object masker <b>412</b> merges the object layers generated by the example object separators <b>408</b><i>a, </i><b>408</b><i>b. </i></p><p id="p-0082" num="0081">The example aggregator <b>414</b> obtains the masks generated by the example pruners <b>410</b><i>a, </i><b>410</b><i>b. </i>For example, the aggregator <b>414</b> terminates the accumulation of masks after the object loader <b>406</b>, the object separators <b>408</b><i>a, </i><b>408</b><i>b, </i>the pruners <b>410</b><i>a, </i><b>410</b><i>b, </i>and the object masker <b>412</b> have processed all frames within an intra-period. That is, the example aggregator <b>414</b> aggregates pruning binary masks (e.g., generated by the example pruners <b>410</b><i>a, </i><b>410</b><i>b</i>) and/or merged multi-level object masks (e.g., generated by the object masker <b>412</b>) to account for motion across frames within the intra-period. For example, the aggregator <b>414</b> accumulates pruning masks per object layer (e.g., per object represented in the input views) per view (e.g., the basic views, the additional views, etc.) over a number of frames (e.g., frames of an intra-period).</p><p id="p-0083" num="0082">The example object clusterer <b>416</b> generates clusters of masks to generate patches. That is, the example object clusterer <b>416</b> filters and removes binary masks based on object masks. For example, the object clusterer <b>416</b> clusters and extracts adjacent (e.g., neighboring, etc.) pixels to generate patches based on the aggregated pruning masks per object layer per view (e.g., generated by the aggregator <b>414</b>). For example, the object clusterer <b>416</b> performs clustering per object and tags the resulting clusters with the associated object ID. Thus, each patch corresponds to a single object. In some examples, the patches are made of multiple blocks. As used herein, a block is a group of one or more pixels. For example, a block can be a group of eight pixels. However, a block can include a greater or fewer number of pixels. The example object clusterer <b>416</b> orders the clusters based on how many active pixels are included, unless the dusters belong to basic views. Additionally or alternatively, the object clusterer <b>416</b> can order the clusters based on patch area, etc.</p><p id="p-0084" num="0083">The example patch packer <b>418</b> generates patch metadata. For example, the patch packer <b>418</b> obtains the dusters (e.g., patches) from the object clusterer <b>416</b>. In some examples, the patch packer <b>418</b> identifies the location of each patch generated by the object clusterer <b>416</b> in an atlas. For example, the patch packer <b>418</b> generates a block to patch map to be included in the metadata. For example, the block to patch map maps blocks to the patches they belong to. In examples disclosed herein, the patch packer <b>418</b> tags the patch with the same object ID as the cluster. That is, each patch is tagged with an object ID of the object it is associated with.</p><p id="p-0085" num="0084">The example atlas generator <b>420</b> packs the patches into atlases. For example, the atlas generator <b>420</b> obtains the patches generated by the object clusterer <b>416</b> and/or patch metadata generated by the patch packer <b>418</b> and writes the texture and depth content of the patches. For example, the atlas generator <b>420</b> receives object views generated by the example object masker <b>412</b> to extract the texture and depth content of the patch. In some examples, the atlas generator <b>420</b> adds the corresponding object ID of the patch in the associated atlas patch parameters list. In examples in which patches area extracted per object (e.g., the pruners <b>410</b><i>a, </i><b>410</b><i>b </i>perform object-based pruning and/or object-selected pruning), the atlas generator <b>420</b> packs the patches next to each other and/or within the same atlas, reducing the required bandwidth for coding and streaming these atlases.</p><p id="p-0086" num="0085">The example object-based atlas constructor <b>404</b> generates an example encoded texture component <b>422</b>. The example depth occupancy coder <b>424</b> encodes depth content to generate an example encoded depth component <b>426</b>. The example encoded texture component <b>422</b> and the example encoded depth component <b>426</b> form an example atlas <b>428</b>.</p><p id="p-0087" num="0086">The example metadata, composer <b>430</b> generates example metadata <b>436</b>. For example, the metadata composer <b>430</b> obtains example immersive video (IV) sequence parameters <b>432</b> and example IV access unit parameters <b>434</b>. For example, the IV sequence parameters <b>432</b> include view parameters (e.g., the location, orientation, depth range, etc. of the video capturer(s) <b>304</b>). The example IV access unit parameters <b>434</b> include frame parameters. For example, the IV access unit parameters <b>434</b> include information per atlas (e.g., atlas data, patch information associated with atlases, etc.) and metadata (atlas size, atlas depth threshold, atlas scaling, etc.). The example metadata composer <b>430</b> multiplexes the IV sequence parameters <b>432</b> and the IV access unit parameters <b>434</b> into the metadata <b>436</b>.</p><p id="p-0088" num="0087">In examples disclosed herein, the metadata composer <b>430</b> includes the object ID per patch and/or the number of objects in the IV access unit parameters <b>434</b> (e.g., the atlas (patch) parameters list metadata). In some examples, the number of objects is represented using the variable num_objects. The number of bits used to represent the object ID is based on the number of bits required to signal the number of objects in the scene (e.g., ceil(log2(num_objects))). The ceil(n) function returns the smallest possible integer value which is greater than or equal to the input, n. For example, ceil(2.8) returns the integer 3. The log2(x) function returns the base-2 logarithm of the input, x. For example, log2(2) returns the value 1. Additionally or alternatively, the number of bits used to signal the object ID can be a fixed length based on the number of objects supported by the example object-based MIV encoder <b>310</b>.</p><p id="p-0089" num="0088">As illustrated in Table 1 below, the num_objects can be added in a data structure (e.g., atlas_params_list) that defines atlas parameters (e.g., atlas_params).</p><p id="p-0090" num="0000"><tables id="TABLE-US-00001" num="00001"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="offset" colwidth="14pt" align="left"/><colspec colname="1" colwidth="154pt" align="left"/><colspec colname="2" colwidth="49pt" align="left"/><thead><row><entry/><entry namest="offset" nameend="2" rowsep="1">TABLE 1</entry></row><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/><entry/><entry>Descriptor</entry></row><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row><row><entry/><entry>atlas_params_list( ){</entry></row><row><entry/><entry>&#x2003;num_atlases_minus1</entry><entry>ue(v)</entry></row><row><entry/><entry>&#x2003;num_objects</entry><entry>u(16)</entry></row><row><entry/><entry>&#x2003;omaf_v1_compatible_flag</entry><entry>u(1)</entry></row><row><entry/><entry>&#x2003;for ( i = 0; I &#x3c;= num_atlases_minus1; i++){</entry></row><row><entry/><entry>&#x2003;&#x2003;atlas_id[ i ];</entry><entry>u(8)</entry></row><row><entry/><entry>&#x2003;&#x2003;atlas_params( atlas_id[ i ] )</entry></row><row><entry/><entry>&#x2003;}</entry></row><row><entry/><entry>}</entry></row><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row><row><entry/><entry/><entry>Descriptor</entry></row><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row><row><entry/><entry>atlas_params( a ){</entry></row><row><entry/><entry>&#x2003;num_patches_minus1[ a ]</entry><entry>u(16)</entry></row><row><entry/><entry>&#x2003;atlas_width[ a ]</entry><entry>u(16)</entry></row><row><entry/><entry>&#x2003;atlas_height[ a ]</entry><entry>u(16)</entry></row><row><entry/><entry>&#x2003;for ( i = 0; i &#x3c;= num_patches_minus1; i++){</entry></row><row><entry/><entry>&#x2003;&#x2003;view_id[ a ][ i ]</entry><entry>u(v)</entry></row><row><entry/><entry>&#x2003;&#x2003;object_id[ a ][ i ]</entry><entry>u(v)</entry></row><row><entry/><entry>&#x2003;&#x2003;patch_width_in_view[ a ][ i ]</entry><entry>u(v)</entry></row><row><entry/><entry>&#x2003;&#x2003;patch_height_in_view[ a ][ i ]</entry><entry>u(v)</entry></row><row><entry/><entry>&#x2003;&#x2003;patch_pos_in_atlas_x[ a ][ i ]</entry><entry>u(v)</entry></row><row><entry/><entry>&#x2003;&#x2003;patch_pos_in_atlas_y[ a ][ i ]</entry><entry>u(v)</entry></row><row><entry/><entry>&#x2003;&#x2003;patch_pos_in_view_x[ a ][ i ]</entry><entry>u(v)</entry></row><row><entry/><entry>&#x2003;&#x2003;patch_pos_in_view_y[ a ][ i ]</entry><entry>u(v)</entry></row><row><entry/><entry>&#x2003;&#x2003;patch_rotation[ a ][ i ]</entry><entry>u(3)</entry></row><row><entry/><entry>&#x2003;}</entry></row><row><entry/><entry>}</entry></row><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup></table></tables><br/>In the example Table 1, the variable num_objects indicates the total number of objects indexed in the object maps across all views of the immersive video content and the variable object_id[a][i] indicates the object ID of the i<sup>th </sup>patch of the a<sup>th </sup>atlas within the view_id[a ][i]<sup>th </sup>view. For example, the atlas parameters list includes the number of objects (e.g., num_objects) in the scene. The patch parameters list (e.g., illustrated in atlas_params(a)) includes the object ID of each patch (e.g., objectID[a][i]).</p><p id="p-0091" num="0089">In some examples, the pruners <b>410</b><i>a, </i><b>410</b><i>b </i>do not perform a pruning process that generates patches containing only a single object. In such examples, the metadata composer <b>430</b> signals the object ID as an additional component that is mapped into atlases and coded with the object-based MIV encoder <b>310</b>. For example, for each pixel position, the pixel value is set equal to the object ID. In some examples, the number of allowable objects is limited by the number of bits used in the video coding (e.g., 2<sup>number of bits</sup>).</p><p id="p-0092" num="0090">In some examples, the metadata composer <b>430</b> includes a patch rate parameter per patch in the parameters e.g., the IV access unit parameters <b>434</b>). The patch rate parameter enables varying patch rates (e.g., the rate a patch is generated and/or encoded). For example, the object-based MIV encoder <b>310</b> determines to encode objects of interest at a higher patch rate and, thus, the metadata composer <b>430</b> stores the patch rate in the metadata. Additionally or alternatively, the metadata composer <b>430</b> stores a scaling parameter for each object (e.g., in the bounding box SEI message). That is, the scaling parameter identifies the resolution used to form patches. In some examples, the metadata composer <b>430</b> signals the scaling parameter for each atlas, for each object ID, for each patch, etc. The example metadata composer <b>430</b> combines the example IV sequence parameters <b>432</b> and the example IV access unit parameters <b>434</b> to generate the example metadata <b>436</b>. The example bitstream encoder <b>438</b> multiplexes the example atlas <b>428</b> and the example metadata <b>436</b> to form an example encoded bitstream <b>440</b>. For example, the encoded bitstream <b>440</b> is a Visual Volumetric Video-based Coding (V3C) sample stream with MIV extensions and related SEI messages. In some examples, the object ID for each patch is included in the encoded bitstream <b>440</b> (e.g., the patch is tagged with the object ID), the object ID is stored in the related SEI messages, etc.</p><p id="p-0093" num="0091"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates an example block diagram of an example implementation of the example MIV decoder <b>356</b> and the example video renderer <b>362</b> included in the example object-based MIV coding system <b>300</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>. The example MIV decoder <b>356</b> includes an example video decoder <b>502</b>, an example metadata parser <b>504</b>, and an example atlas patch occupancy map generator <b>506</b>.</p><p id="p-0094" num="0092">The example MIV decoder <b>356</b> receives an example encoded bitstream <b>501</b> (e.g., the encoded bitstream <b>440</b> generated by the example object-based MIV encoder <b>310</b>). The example video decoder <b>502</b> generates a sequence of decoded picture pairs of example decoded texture pictures <b>510</b> and example decoded depth pictures <b>512</b>. In examples disclosed herein, the example decoded texture pictures <b>510</b> and the example decoded depth pictures <b>512</b> represent an example atlas <b>514</b>. For example, the video decoder <b>502</b> can be an HEVC decoder. In some examples, the video decoder <b>502</b> includes a video texture decoder and a video depth decoder.</p><p id="p-0095" num="0093">The example metadata parser <b>504</b> parses the example encoded bitstream <b>501</b> to generate example IV access unit parameters <b>516</b> and example IV sequence parameters <b>518</b>. For example, the metadata parser <b>504</b> parses the encoded bitstream <b>501</b> for the example IV access unit parameters <b>434</b> (<figref idref="DRAWINGS">FIG. <b>4</b></figref>) to generate the IV access unit parameters <b>516</b>. Additionally or alternatively, the example metadata parser <b>504</b> parses the encoded bitstream <b>501</b> for the example IV sequence parameters <b>432</b> (<figref idref="DRAWINGS">FIG. <b>4</b></figref>) to generate the example IV sequence parameters <b>518</b>.</p><p id="p-0096" num="0094">The example atlas patch occupancy map generator <b>506</b> generates an example atlas patch occupancy map <b>520</b>. In examples disclosed herein, the atlas patch occupancy map <b>520</b> is the same size as the atlas <b>514</b> and indicates whether pixels of the atlas are occupied. For example, the atlas patch occupancy map generator <b>506</b> accesses the example decoded depth pictures <b>512</b>, the IV access unit parameters <b>516</b>, and the example IV sequence parameters <b>518</b> to associate the pixels of the decoded depth pictures <b>512</b> with the atlas <b>514</b>. In examples disclosed herein, the atlas patch occupancy map generator <b>506</b> filters objects based on object IDs. For example, the atlas patch occupancy map generator <b>506</b> sets the occupancy of pixels corresponding to patches that are to be filtered out to unoccupied.</p><p id="p-0097" num="0095">The example video renderer <b>362</b> includes an example object filter <b>522</b>, an example patch culler <b>523</b>, an example synthesizer <b>524</b>, and an example inpainter <b>526</b>. The example object filter <b>522</b> filters pixels based on the occupancy map <b>520</b>. For example, the object filter <b>522</b> determines which pixels to render based on whether corresponding pixels of the occupancy map <b>520</b> are occupied (e.g., the occupied pixels of the occupancy map correspond to objects that are to be rendered). In some examples, the object filter <b>522</b> identifies the associated bounding box using the object ID of the selected object. That is, the object filter <b>522</b> performs object filtering. For example, the object filter <b>522</b> removes patches of the objects to be filtered out from the atlas data. For example, the object filter <b>522</b> identifies the object IDs of the objects to be removed and removes the patches corresponding to the identified object IDs. In some examples, the object filter <b>522</b> removes the patch by removing the one or more blocks that make up the patch. For example, the object filter <b>522</b> identifies and removes block(s) corresponding to the identified patches from the block to patch map (e.g., generated by the example metadata parser <b>504</b>). That is, the object filter <b>522</b> modifies the block to patch map such that pixels (e.g., blocks) of patches that correspond to the objects of interest (e.g., objects to be rendered) are identified (e.g., using the object IDs associated with their patch metadata) and kept in the block to patch map while pixels of patches that correspond to objects not of interest are filtered out (e.g., pixels set to unoccupied values).</p><p id="p-0098" num="0096">The example patch culler <b>523</b> performs patch culling based on an example viewing position and viewing orientation <b>530</b> (e.g., a target view). The patch culler <b>523</b> can receive the viewing position and viewing orientation <b>530</b> from a user. The patch culler <b>523</b> filters out blocks from the atlas data based on the block to patch map that are not visible based on the target viewing position and viewing orientation <b>530</b>.</p><p id="p-0099" num="0097">The example synthesizer <b>524</b> reconstructs the pruned views. For example, the synthesizer <b>524</b> copies the patches from the atlases (e.g., the patches that were not filtered out by the object filter <b>522</b> and/or the patches that were not culled by the patch culler <b>523</b>) to images corresponding to each input view. The example synthesizer <b>524</b> synthesizes a view based on the patches in the input view. For example, the synthesizer <b>524</b> can be implemented by a Reference View Synthesizer (RVS). Additionally or alternatively, the synthesizer <b>524</b> can be implemented by a View Weighting Synthesizer (VWS).</p><p id="p-0100" num="0098">The example inpainter <b>526</b> fills inactive pixels. That is, the example object filter <b>522</b> filters out pixels corresponding to objects that are not to be rendered. Thus, there may be missing information between pixels of the selected objects (e.g., corresponding to objects that were filtered out). The example inpainter <b>526</b> converts the inactive pixels to a neutral (e.g., a grey color). The use of a neutral filling reduces stretched artifacts (e.g., if the example inpainter <b>526</b> generates information for the inactive pixels instead of using a neutral filling).</p><p id="p-0101" num="0099">The example video renderer <b>362</b> generates an example viewport <b>528</b>. For example, the video renderer <b>362</b> accesses one or more of the decoded atlases <b>514</b>, the IV access unit parameters <b>516</b>, and the example IV sequence parameters <b>518</b>, the atlas patch occupancy map <b>520</b>, and the viewer position and orientation <b>530</b>. That is, the example video renderer <b>362</b> outputs a perspective viewport of the texture images of the selected objects based on the example viewer position and orientation <b>230</b>.</p><p id="p-0102" num="0100"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates example immersive data <b>600</b>. The example immersive data <b>600</b> includes an example first view <b>602</b>, an example second view <b>604</b>, and an example third view <b>606</b>. For example, the views <b>602</b>, <b>604</b>, <b>606</b> are captured from three different cameras at three different angles relative to each other. For example, the first view <b>602</b> is the view <b>0</b>, the second view <b>604</b> is the view <b>1</b>, and the third view <b>606</b> is the view <b>2</b>. The example immersive data <b>600</b> includes example texture maps <b>608</b>. That is, there is an example texture map <b>608</b> for each of the views <b>602</b>, <b>604</b>, <b>606</b>. The example immersive data <b>600</b> includes example depth maps <b>610</b>. That is, there is an example depth map <b>610</b> for each of the views <b>602</b>, <b>604</b>, <b>606</b>.</p><p id="p-0103" num="0101">The example immersive data <b>600</b> includes example object maps <b>612</b>. That is, there is an example object map <b>612</b> for each of the views <b>602</b>, <b>604</b>, <b>606</b>. For example, the object maps <b>612</b> include an example first object ID <b>614</b>, an example second object ID <b>616</b>, an example third object ID <b>618</b>, and an example fourth object ID <b>620</b>. For example, the first object ID <b>614</b> has a value of 0 and corresponds to the background of the views <b>602</b>, <b>604</b>, <b>606</b>, the second object ID <b>616</b> has a value of 1 and corresponds to the first person of the views <b>602</b>, <b>604</b>, <b>606</b>, the third object ID <b>618</b> has a value of 2 and corresponds to the second person of the views <b>602</b>, <b>604</b>, <b>606</b>, and the fourth object ID <b>620</b> has a value of 3 and corresponds to the third person in the views <b>602</b>, <b>604</b>, <b>606</b>. That is, the same four objects captured in the views <b>602</b>, <b>604</b>, <b>606</b> are tagged with a corresponding object ID, such that the same object has the same object ID in each object map <b>612</b> of the views <b>602</b>, <b>604</b>, <b>606</b>. Each pixel of the object maps <b>612</b> corresponds to the object ID of the respective object. For example, the views <b>602</b>, <b>604</b>, <b>606</b> include the first person. Thus, the object maps <b>612</b> of the views <b>602</b>, <b>604</b>, <b>606</b> include the first object ID at the pixel location representative of the first person.</p><p id="p-0104" num="0102"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates example input views and patches. The input views include an example first <b>702</b>, an example second <b>704</b>, and an example third <b>706</b>. For example, the first view <b>702</b> is the view <b>0</b>, the second view <b>704</b> is the view <b>1</b>, and the third view <b>706</b> is the view <b>2</b>. For example, the views <b>702</b>, <b>704</b>, <b>706</b> include view representations <b>708</b> (e.g., texture maps, depth maps, object maps). In the illustrated example of <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the view representations <b>708</b> of the views <b>702</b>, <b>704</b>, <b>706</b> include three people. That is, the views <b>702</b>, <b>704</b>, <b>706</b> are captured from three different cameras at three different angles with respect to the three people.</p><p id="p-0105" num="0103">The example pruners <b>410</b><i>a, </i><b>410</b><i>b </i>(<figref idref="DRAWINGS">FIG. <b>4</b></figref>) prune the views <b>702</b>, <b>704</b>, <b>706</b> to generate patches. For example, the first view <b>702</b> includes an example first patch <b>710</b> and an example second patch <b>712</b>, the second view <b>704</b> includes an example third patch <b>714</b>, and the third view <b>706</b> includes an example fourth patch <b>716</b> and an example fifth patch <b>718</b>. In examples disclosed herein, each patch corresponds to one respective object. For example, the first patch <b>710</b> corresponds to a first person's head, the second patch <b>712</b> corresponds to a second person's head, the third patch <b>714</b> corresponds to the second person's arm, the fourth patch <b>716</b> corresponds to a third person's head, and the fifth patch corresponds to the second person's leg.</p><p id="p-0106" num="0104">In some examples, the patch packer <b>418</b> (<figref idref="DRAWINGS">FIG. <b>4</b></figref>) tags the patches with patch IDs. For example the patch packer <b>418</b> tags the first patch <b>710</b> with a patch ID of <b>0</b>, the second patch <b>712</b> with a patch ID of <b>1</b>, the third patch <b>714</b> with a patch ID of <b>2</b>, the fourth patch <b>716</b> with a patch ID of <b>3</b>, and the fifth patch <b>718</b> with a patch ID of <b>4</b>. Additionally or alternatively, the patch packer <b>418</b> tags the patches with object IDs. For example, the patch packer <b>418</b> tags the patches with the object ID of the object represented in the views <b>702</b>, <b>704</b>, <b>706</b>. For example, the first patch <b>710</b> is tagged with an object ID of <b>0</b>, the fourth patch <b>716</b> is tagged with an object ID of <b>1</b>, and the second patch <b>712</b>, the third patch <b>714</b>, and the fifth patch <b>718</b> are tagged with an object ID of <b>2</b>.</p><p id="p-0107" num="0105">The example atlas generator <b>420</b> (<figref idref="DRAWINGS">FIG. <b>4</b></figref>) generates an example first atlas <b>720</b> and an example second atlas <b>722</b>. The atlases <b>720</b>, <b>722</b> include a texture map and a depth map. The example first atlas <b>720</b> includes the example first patch <b>710</b>, the example second patch <b>712</b>, and the example third patch <b>714</b>. The example second atlas <b>722</b> includes the example fourth patch <b>716</b> and the example fifth patch <b>718</b>.</p><p id="p-0108" num="0106"><figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates an example V3C sample stream with MIV extensions <b>800</b> (e.g., an example V3C sample stream <b>800</b>). For example, the object-based MIV encoder <b>310</b> generates the V3C sample stream <b>800</b>. The example V3C sample stream <b>800</b> includes an example V3C parameter set <b>802</b>, example common atlas data <b>804</b>, example atlas data <b>806</b>, example geometry video data <b>808</b>, example attribute video data <b>810</b>, and example occupancy video data <b>811</b>.</p><p id="p-0109" num="0107">For example, the V3C parameter set <b>802</b> includes example IV access unit parameters <b>516</b> and example IV sequence parameters <b>518</b>. The example common atlas data <b>804</b> includes an example view parameters list <b>812</b>. For example, the common atlas data <b>804</b> contains an atlas sub bitstream, but the main Network Abstraction Layer (NAL) unit is the common atlas frame (CAF) that contains the view parameters list <b>812</b> or updates thereof. The example atlas data <b>806</b> is a NAL sample stream, which includes example SEI messages. For example, the atlas data <b>806</b> includes an atlas tile layer (ATL) that carries a list of patch data units (PDU). In examples disclosed herein, each PDU describes the relation between a patch in an atlas and the same patch in a hypothetical input view. The example atlas data <b>806</b> includes example atlas tile layers <b>814</b> (e.g., patch data <b>814</b>). For example, the patch data <b>814</b> is sent only for example intra-periods <b>816</b> (e.g., once per intra-period <b>816</b>). In some examples, a frame order count NAL unit is used to skip all interframes at once.</p><p id="p-0110" num="0108"><figref idref="DRAWINGS">FIG. <b>9</b></figref> illustrates an example pruning graph <b>900</b> for example basic views <b>902</b> and example additional views <b>904</b>. For example, the basic views <b>902</b> include an example first view v<b>0</b> and an example second view v<b>1</b>. The example additional views <b>904</b> include an example third view v<b>2</b>, an example fourth view v<b>3</b>, and an example fifth view v<b>4</b>. In some examples, the view labeler <b>402</b> (<figref idref="DRAWINGS">FIG. <b>4</b></figref>) labels the first view v<b>0</b> and the second view v<b>1</b> as basic views and labels the third view v<b>2</b>, the fourth view v<b>3</b>, and the fifth view v<b>4</b> as additional views. In the illustrated example of <figref idref="DRAWINGS">FIG. <b>9</b></figref>, the basic views <b>902</b> and the additional views <b>904</b> are the corresponding depth maps of the views v<b>0</b>, v<b>1</b>, v<b>2</b>, v<b>3</b>, v<b>4</b>.</p><p id="p-0111" num="0109">The example pruning graph <b>900</b> includes a plurality of example pruning blocks <b>906</b>. For example, the pruners <b>410</b><i>a, </i><b>410</b><i>b </i>(<figref idref="DRAWINGS">FIG. <b>4</b></figref>) receive an example first view vi <b>908</b>, an example second view vj <b>910</b>, and example camera parameters <b>912</b>. At block <b>914</b>, the example pruners <b>410</b><i>a, </i><b>410</b><i>b </i>project the depth maps of the first view vi <b>908</b> onto the depth maps of the second view vj <b>910</b>. At block <b>916</b>, the example pruners <b>410</b><i>a, </i><b>410</b><i>b </i>compare the views <b>908</b>, <b>910</b> and determines whether to prune the views (e.g., generate a mask). The example pruners <b>410</b><i>a, </i><b>410</b><i>b </i>generate an example first mask <b>918</b>, an example second mask <b>920</b>, and an example third mask <b>922</b>. For example, the first mask <b>918</b> is a mask of the pruned third view v<b>2</b>, the second mask <b>920</b> is a mask of the pruned fourth view v<b>3</b>, and the third mask <b>922</b> is a mask of the pruned fourth view v<b>4</b>.</p><p id="p-0112" num="0110"><figref idref="DRAWINGS">FIG. <b>10</b></figref> illustrates an example object-based pruning process. The illustrated example of <figref idref="DRAWINGS">FIG. <b>10</b></figref> includes an example first pruning graph <b>1000</b> and an example second pruning graph <b>1050</b>. The example first pruning graph <b>1000</b> includes example basic views <b>1002</b> and example additional views <b>1004</b>. For example, the basic views <b>1002</b> include an example first view v<b>0</b> and an example second view v<b>1</b>. The example additional views <b>1004</b> include an example third view v<b>2</b>, an example fourth view v<b>3</b>, and an example fifth view v<b>4</b>. In some examples, the view labeler <b>402</b> (<figref idref="DRAWINGS">FIG. <b>4</b></figref>) labels the first view v<b>0</b> and the second view v<b>1</b> as basic views and labels the third view v<b>2</b>, the fourth view v<b>3</b>, and the fifth view v<b>4</b> as additional views. In the illustrated example of <figref idref="DRAWINGS">FIG. <b>10</b></figref>, the basic views <b>1002</b> and the additional views <b>1004</b> are the corresponding depth maps of the views v<b>0</b>, v<b>1</b>, v<b>2</b>, v<b>3</b>, v<b>4</b>.</p><p id="p-0113" num="0111">The example first pruning graph <b>1000</b> corresponds to a first object. For example, the object separator <b>408</b><i>a </i>(<figref idref="DRAWINGS">FIG. <b>4</b></figref>) identifies the object ID of the first object based on an object map (not illustrated) and generates object layers for each view corresponding to the object ID of the first object. The example pruner <b>410</b><i>a </i>performs pruning (e.g., block <b>906</b> of <figref idref="DRAWINGS">FIG. <b>9</b></figref>). For example, the pruner <b>410</b><i>a </i>projects the object layers of the basic views <b>1002</b> onto the object layers of the additional views <b>1004</b> to generate an example first mask <b>1006</b>, an example second mask <b>1008</b>, and an example third mask <b>1010</b>. For example, the first mask <b>1006</b> is a mask of the pruned third view v<b>2</b> of the first object, the second mask <b>1008</b> is a mask of the pruned fourth view v<b>3</b> of the first object, and the third mask <b>1010</b> is a mask of the pruned fifth view v<b>4</b> of the first object.</p><p id="p-0114" num="0112">The example second pruning graph <b>1050</b> includes example basic views <b>1052</b> and example additional views <b>1054</b>. For example, the basic views <b>1052</b> include an example first view v<b>0</b> and an example second view v<b>1</b>. The example additional views <b>1054</b> include an example third view v<b>2</b>, an example fourth view v<b>3</b>, and an example fifth view v<b>4</b>. In the illustrated example of <figref idref="DRAWINGS">FIG. <b>10</b></figref>. The example second pruning graph <b>1050</b> corresponds to the k<sup>th </sup>object. That is, the basic views <b>1052</b> and the additional views <b>1054</b> are the corresponding depth maps of the views v<b>0</b>, v<b>1</b>, v<b>2</b>, v<b>3</b>, v<b>4</b> corresponding to the k<sup>th </sup>object. That is, the example views <b>1052</b>, <b>1054</b> represent the object layers of the k<sup>th </sup>object. For example, the object separator <b>408</b><i>b </i>(<figref idref="DRAWINGS">FIG. <b>4</b></figref>) identifies the object ID of the k<sup>th </sup>object based on the object map (not illustrated). The example pruner <b>410</b><i>b </i>performs pruning (e.g., block <b>906</b> of <figref idref="DRAWINGS">FIG. <b>9</b></figref>). For example, the pruner <b>410</b><i>b </i>projects the object layers of the basic views <b>1052</b> onto the object layers of the additional views <b>1054</b> of the k<sup>th </sup>object to generate an example first mask <b>1056</b>, an example second mask <b>1058</b>, and an example third mask <b>1060</b>. For example, the first mask <b>1056</b> is a mask of the pruned third view v<b>2</b> of the k<sup>th </sup>object, the second mask <b>1058</b> is a mask of the pruned fourth view v<b>3</b> of the k<sup>th </sup>object, and the third mask <b>1010</b> is a mask of the pruned fifth view v<b>4</b> of the k<sup>th </sup>object.</p><p id="p-0115" num="0113"><figref idref="DRAWINGS">FIG. <b>11</b></figref> illustrates an example object-view pruning scheme. The illustrated example of <figref idref="DRAWINGS">FIG. <b>11</b></figref> includes an example first pruning graph <b>1100</b> and an example second pruning graph <b>1150</b>. The example first pruning graph <b>1100</b> includes an example basic view <b>1102</b> and example additional views <b>1104</b>. For example, the basic view <b>1102</b> is an example second view v<b>1</b>. The example additional views <b>1104</b> include an example first view v<b>0</b>, an example third view v<b>2</b>, an example fourth view v<b>3</b>, and an example fifth view v<b>4</b>. In some examples, the view labeler <b>402</b> (<figref idref="DRAWINGS">FIG. <b>4</b></figref>) labels the second view v<b>1</b> as the basic view and labels the first view v<b>0</b>, the third view v<b>2</b>, the fourth view v<b>3</b>, and the fifth view v<b>4</b> as additional views. In the illustrated example of <figref idref="DRAWINGS">FIG. <b>11</b></figref>, the basic views <b>1102</b> and the additional views <b>1104</b> are the corresponding depth maps of the views v<b>0</b>, v<b>1</b>, v<b>2</b>, v<b>3</b>, v<b>4</b>. The example first pruning graph <b>1100</b> corresponds to a first object. For example, the object separator <b>408</b><i>a </i>(<figref idref="DRAWINGS">FIG. <b>4</b></figref>) identifies the object ID of the first object based on an object map (not illustrated). The example pruner <b>410</b><i>a </i>performs pruning (e.g., block <b>906</b> of <figref idref="DRAWINGS">FIG. <b>9</b></figref>). For example, the pruner <b>410</b><i>a </i>projects the object layer of the basic view <b>1102</b> onto the object layers of the additional views <b>1104</b> of the first object to generate an example first mask <b>1106</b>, an example second mask <b>1108</b>, an example third mask <b>1110</b>, and an example fourth mask <b>1112</b>. For example, the first mask <b>1106</b> is a mask of the pruned first pruned view v<b>0</b> of the first object, the second mask <b>1108</b> is a mask of the pruned third view v<b>2</b> of the first object, the third mask <b>1110</b> is a mask of the pruned fourth view v<b>3</b> of the first object, and the fourth mask <b>1112</b> is a mask of the pruned fifth view v<b>4</b> of the first object.</p><p id="p-0116" num="0114">The example second pruning graph <b>1150</b> includes an example basic view <b>1152</b> and example additional views <b>1154</b>. For example, the basic view <b>1152</b> is an example fifth view v<b>4</b>. The example additional views <b>1154</b> include an example first view v<b>0</b>, an example second view v<b>1</b>, an example third view v<b>2</b>, and an example fourth view v<b>3</b>. In some examples, the view labeler <b>402</b> labels the fifth view v<b>4</b> as the basic view and labels the first view v<b>0</b>, the second view v<b>1</b>, the third view v<b>2</b>, and the fourth view v<b>3</b> as additional views. In the illustrated example of <figref idref="DRAWINGS">FIG. <b>11</b></figref>, the basic view <b>1152</b> and the additional views <b>1154</b> are the corresponding depth maps of the views v<b>0</b>, v<b>1</b>, v<b>2</b>, v<b>3</b>, v<b>4</b>. The example second pruning graph <b>1150</b> corresponds to the k<sup>th </sup>object. That is, the example views <b>1152</b>, <b>1154</b> represent the object layers of the k<sup>th </sup>object. For example, the object separator <b>408</b><i>b </i>(<figref idref="DRAWINGS">FIG. <b>4</b></figref>) identifies the object ID of the k<sup>th </sup>object based on the object map (not illustrated). The example pruner <b>410</b><i>b </i>performs pruning (e.g., block <b>906</b> of <figref idref="DRAWINGS">FIG. <b>9</b></figref>). For example, the pruner <b>410</b><i>b </i>projects the object layer of the basic view <b>1152</b> onto the object layers of the additional views <b>1154</b> of the k<sup>th </sup>object to generate an example first mask <b>1156</b>, an example second mask <b>1158</b>, an example third mask <b>1160</b>, and an example fourth mask <b>1162</b>. For example, the first mask <b>1156</b> is a mask of the pruned first pruned view v<b>0</b> of the k<sup>th </sup>object, the second mask <b>1158</b> is a mask of the pruned second view v<b>1</b> of the first object, the third mask <b>1160</b> is a mask of the pruned third view v<b>2</b> of the k<sup>th </sup>object, and the fourth mask <b>1112</b> is a mask of the pruned fourth view v<b>3</b> of the k<sup>th </sup>object.</p><p id="p-0117" num="0115"><figref idref="DRAWINGS">FIG. <b>12</b></figref> illustrates an example 3D bounding box <b>1200</b>. In examples disclosed herein, the bounding box <b>1200</b> is associated with an object (not illustrated). The geometry of the bounding box <b>1200</b> is specified by a starting point <b>1202</b>. For example, the starting point <b>1202</b> corresponds to the coordinates (x, y, z). The bounding box <b>1200</b> has an example width <b>1204</b>, an example height <b>1206</b>, and an example depth <b>1208</b>. That is, the width <b>1204</b>, the height <b>1206</b>, and the depth <b>1208</b> are measured with respect to the starting point <b>1202</b>. In some examples, the width <b>1204</b>, the height <b>1206</b>, and the depth <b>1208</b> are measured in floating point values.</p><p id="p-0118" num="0116"><figref idref="DRAWINGS">FIG. <b>13</b></figref> illustrates an example object-based synthesized scene <b>1300</b>. For example, the object-based synthesized scene <b>1300</b> includes an example first view <b>1302</b>, an example second view <b>1304</b>, and an example third view <b>1306</b>. For example, the views <b>1302</b>, <b>1304</b>, <b>1306</b> are captured from three different cameras at three different angles relative to each other. The illustrated example of <figref idref="DRAWINGS">FIG. <b>13</b></figref> includes example original views <b>1308</b>. For example, the original views <b>1308</b> are texture maps. The example original views <b>1308</b> can additionally or alternatively include depth maps and/or object maps. For example, the views <b>1302</b>, <b>1304</b>, <b>1306</b> include an example first person <b>1310</b>, an example second person <b>1312</b>, and an example third person <b>1314</b>. That is, the example first person <b>1310</b> corresponds to a first object ID, the example second person <b>1312</b> corresponds to a second object ID, and the example third person <b>1314</b> corresponds to a third object ID.</p><p id="p-0119" num="0117">The example object-based synthesized scene <b>1300</b> includes example synthesized views <b>1316</b>. For example, the example object-based MIV encoder <b>310</b> encodes the views <b>1302</b>, <b>1304</b>, <b>1306</b> and the example video renderer <b>362</b> (<figref idref="DRAWINGS">FIG. <b>3</b></figref>) generates the synthesized views <b>1316</b>. In the illustrated example of <figref idref="DRAWINGS">FIG. <b>13</b></figref>, the example video renderer <b>362</b> filtered out the first person <b>1310</b> and the third person <b>1314</b>. That is, the example synthesized views <b>1316</b> include the example second person <b>1312</b> and not the example first person <b>1310</b> or the example third person <b>1314</b>.</p><p id="p-0120" num="0118">While an example manner of implementing the server side immersive video encoding system <b>302</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref> is illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, one or more of the elements, processes and/or devices illustrated in <figref idref="DRAWINGS">FIGS. <b>3</b> and/or <b>4</b></figref> may be combined, divided, re-arranged, omitted, eliminated and/or implemented in any other way. Further, the example video capturer(s) <b>304</b>, the example segmenter <b>305</b>, the example virtual camera renderer <b>306</b>, the example object and depth information storage <b>308</b>, the example object-based MIV encoder <b>310</b>, the example video encoder <b>312</b>, the example stream packager <b>314</b>, the example content management system <b>315</b>, the example content distribution network <b>316</b>, the example view labeler <b>402</b>, the example object-based atlas constructor <b>404</b>, the example depth occupancy coder <b>424</b>, the example metadata composer <b>430</b>, the example bitstream encoder <b>438</b> and/or, more generally, the server side immersive video encoding system <b>302</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref> may be implemented by hardware, software, firmware and/or any combination of hardware, software and/or firmware. Thus, for example, any of the example video capturer(s) <b>304</b>, the example segmenter <b>305</b>, the example virtual camera renderer <b>306</b>, the example object and depth information storage <b>308</b>, the example object-based MIV encoder <b>310</b>, the example video encoder <b>312</b>, the example stream packager <b>314</b>, the example content management system <b>315</b>. the example content distribution network <b>316</b>, the example view labeler <b>402</b>, the example object-based atlas constructor <b>404</b>, the example depth occupancy coder <b>424</b>, the example metadata composer <b>430</b>, the example bitstream encoder <b>438</b> and/or, more generally, the example server side immersive video encoding system <b>302</b> could be implemented by one or more analog or digital circuit(s), logic circuits, programmable processor(s), programmable controller(s), graphics processing units) (GPU(s)), digital signal processor(s) (DSP(s)), application specific integrated circuit(s) (ASIC(s)), programmable logic device(s) (PLD(s)) and/or field programmable logic device(s) (FPLD(s)). When reading any of the apparatus or system claims of this patent to cover a purely software and/or firmware implementation, at least one of the example video capturer(s) <b>304</b>, the example segmenter <b>305</b>, the example virtual camera renderer <b>306</b>, the example object and depth information storage <b>308</b>, the example object-based MIV encoder <b>310</b>, the example video encoder <b>312</b>, the example stream packager <b>314</b>, the example content management system <b>315</b>, the example content distribution network <b>316</b>, the example view labeler <b>402</b>, the example object-based atlas constructor <b>404</b>, the example depth occupancy coder <b>424</b>, the example metadata composer <b>430</b>, and/or the example bitstream encoder <b>438</b> is/are hereby expressly defined to include a non-transitory computer readable storage device or storage disk such as a memory, a digital versatile disk (DVD), a compact disk (CD), a Blu-ray disk, etc. including the software and/or firmware. Further still, the example server side immersive video encoding system <b>302</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref> may include one or more elements, processes and/or devices in addition to, or instead of, those illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, and/or may include more than one of any or all of the illustrated elements, processes and devices. As used herein, the phrase &#x201c;in communication,&#x201d; including variations thereof, encompasses direct communication and/or indirect communication through one or more intermediary components, and does not require direct physical (e.g., wired) communication and/or constant communication, but rather additionally includes selective communication at periodic intervals, scheduled intervals, aperiodic intervals, and/or one-time events.</p><p id="p-0121" num="0119">While an example manner of implementing the client side immersive video decoding system <b>350</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref> is illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, one or more of the elements, processes and/or devices illustrated in <figref idref="DRAWINGS">FIGS. <b>3</b> and/or <b>5</b></figref> may be combined, divided, re-arranged, omitted, eliminated and/or implemented in any other way. Further, the example de-packager <b>354</b>, the example MIV decoder <b>356</b>, the example presentation engine <b>358</b>, the example video decoder <b>360</b>, the example video renderer <b>362</b>, the example video decoder <b>502</b>, the example metadata parser <b>504</b>, the example atlas patch occupancy map generator <b>506</b>, the example object filter <b>522</b>, the example synthesizer <b>524</b>, the example inpainter <b>526</b> and/or, more generally, the example client side immersive video decoding system <b>350</b> of <figref idref="DRAWINGS">FIG. <b>5</b></figref> may be implemented by hardware, software, firmware and/or any combination of hardware, software and/or firmware. Thus, for example, any of the example de-packager <b>354</b>, the example MIV decoder <b>356</b>, the example presentation engine <b>358</b>, the example video decoder <b>360</b>, the example video renderer <b>362</b>, the example video decoder <b>502</b>, the example metadata parser <b>504</b>, the example atlas patch occupancy map generator <b>506</b>, the example object filter <b>522</b>, the example synthesizer <b>524</b>, the example inpainter <b>526</b> and/or, more generally, the example client side immersive video decoding system <b>350</b> could be implemented by one or more analog or digital circuit(s), logic circuits, programmable processor(s), programmable controller(s), graphics processing unit(s) (GPU(s)), digital signal processor(s) (DSP(s)), application specific integrated circuit(s) (ASIC(s)), programmable logic device(s) (PLD(s)) and/or field programmable logic device(s) (FPLD(s)). When reading any of the apparatus or system claims of this patent to cover a purely software and/or firmware implementation, at least one of the example, de-packager <b>354</b>, the example MIV decoder <b>356</b>, the example presentation engine <b>358</b>, the example video decoder <b>360</b>, the example video renderer <b>362</b>, the example video decoder <b>502</b>, the example metadata parser <b>504</b>, the example atlas patch occupancy map generator <b>506</b>, the example object filter <b>522</b>, the example synthesizer <b>524</b>, and/or the example inpainter <b>526</b> is/are hereby expressly defined to include a non-transitory computer readable storage device or storage disk such as a memory, a digital versatile disk (DVD), a compact disk (CD), a Blu-ray disk, etc. including the software and/or firmware. Further still, the example client side immersive video decoding system <b>350</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref> may include one or more elements, processes and/or devices in addition to, or instead of, those illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, and/or may include more than one of any or all of the illustrated elements, processes and devices. As used herein, the phrase &#x201c;in communication,&#x201d; including variations thereof, encompasses direct communication and/or indirect communication through one or more intermediary components, and does not require direct physical (e.g., wired) communication and/or constant communication, but rather additionally includes selective communication at periodic intervals, scheduled intervals, aperiodic intervals, and/or one-time events.</p><p id="p-0122" num="0120">A flowchart representative of example hardware logic, machine readable instructions, hardware implemented state machines, and/or any combination thereof for implementing the example server side immersive video encoding system <b>302</b> of <figref idref="DRAWINGS">FIGS. <b>3</b> and/or <b>4</b></figref> is shown in <figref idref="DRAWINGS">FIG. <b>14</b></figref>. The machine readable instructions may be one or more executable programs or portion(s) of an executable program for execution by a computer processor and/or processor circuitry, such as the processor <b>1612</b> shown in the example processor platform <b>1600</b> discussed below in connection with <figref idref="DRAWINGS">FIG. <b>16</b></figref>. The program may he embodied in software stored on a non-transitory computer readable storage medium such as a CD-ROM, a floppy disk, a hard drive, a DVD, a Blu-ray disk, or a memory associated with the processor <b>1612</b>, but the entire program and/or parts thereof could alternatively be executed by a device other than the processor <b>1612</b> and/or embodied in firmware or dedicated hardware. Further, although the example program is described with reference to the flowchart illustrated in <figref idref="DRAWINGS">FIG. <b>14</b></figref>, many other methods of implementing the example server side immersive video encoding system <b>302</b> may alternatively be used. For example, the order of execution of the blocks may be changed, and/or some of the blocks described may be changed, eliminated, or combined. Additionally or alternatively, any or all of the blocks may be implemented by one or more hardware circuits (e.g., discrete and/or integrated analog and/or digital circuitry, an FPGA, an ASIC, a comparator, an operational-amplifier (op-amp), a logic circuit, etc.) structured to perform the corresponding operation without executing software or firmware. The processor circuitry may be distributed in different network locations and/or local to one or more devices (e.g., a multi-core processor in a single machine, multiple processors distributed across a server rack, etc.).</p><p id="p-0123" num="0121">A flowchart representative of example hardware logic, machine readable instructions, hardware implemented state machines, and/or any combination thereof for implementing the example client side immersive video decoding system <b>350</b> of <figref idref="DRAWINGS">FIGS. <b>3</b> and/or <b>5</b></figref> is shown in <figref idref="DRAWINGS">FIG. <b>15</b></figref>. The machine readable instructions may be one or more executable programs or portion(s) of an executable program for execution by a computer processor and/or processor circuitry, such as the processor <b>1712</b> shown in the example processor platform <b>1700</b> discussed below in connection with <figref idref="DRAWINGS">FIG. <b>17</b></figref>. The program may be embodied in software stored on a non-transitory computer readable storage medium such as a CD-ROM, a floppy disk, a hard drive, a DVD, a Blu-ray disk, or a memory associated with the processor <b>1712</b>, but the entire program and/or parts thereof could alternatively be executed by a device other than the processor <b>1712</b> and/or embodied in firmware or dedicated hardware. Further, although the example program is described with reference to the flowchart illustrated in <figref idref="DRAWINGS">FIG. <b>15</b></figref>, many other methods of implementing the example client side immersive video decoding system <b>350</b> may alternatively be used. For example, the order of execution of the blocks may be changed, and/or some of the blocks described may be changed, eliminated, or combined. Additionally or alternatively, any or all of the blocks may be implemented by one or more hardware circuits (e.g., discrete and/or integrated analog and/or digital circuitry, an FPGA, an ASIC, a comparator, an operational-amplifier (op-amp), a logic circuit, etc.) structured to perform the corresponding operation without executing software or firmware. The processor circuitry may be distributed in different network locations and/or local to one or more devices (e.g., a multi-core processor in a single machine, multiple processors distributed across a server rack, etc.).</p><p id="p-0124" num="0122">The machine readable instructions described herein may be stored in one or more of a compressed format, an encrypted format, a fragmented format, a compiled format, an executable format, a packaged format, etc. Machine readable instructions as described herein may be stored as data or a data structure (e.g., portions of instructions, code, representations of code, etc.) that may be utilized to create, manufacture, and/or produce machine executable instructions. For example, the machine readable instructions may be fragmented and stored on one or more storage devices and/or computing devices (e.g., servers) located at the same or different locations of a network or collection of networks (e.g., in the cloud, in edge devices, etc.). The machine readable instructions may require one or more of installation, modification, adaptation, updating, combining, supplementing, configuring, decryption, decompression, unpacking, distribution, reassignment, compilation, etc. in order to make them directly readable, interpretable, and/or executable by a computing device and/or other machine. For example, the machine readable instructions may be stored in multiple parts, which are individually compressed, encrypted, and stored on separate computing devices, wherein the parts when decrypted, decompressed, and combined form a set of executable instructions that implement one or more functions that may together form a program such as that described herein.</p><p id="p-0125" num="0123">In another example, the machine readable instructions may be stored in a state in which they may be read by processor circuitry, but require addition of a library (e.g., a dynamic link library (DLL)), a software development kit (SDK), an application programming interface (API), etc. in order to execute the instructions on a particular computing device or other device. In another example, the machine readable instructions may need to be configured (e.g., settings stored, data input, network addresses recorded, etc.) before the machine readable instructions and/or the corresponding program(s) can be executed in whole or in part. Thus, machine readable media, as used herein, may include machine readable instructions and/or program(s) regardless of the particular format or state of the machine readable instructions and/or program(s) when stored or otherwise at rest or in transit.</p><p id="p-0126" num="0124">The machine readable instructions described herein can be represented by any past, present, or future instruction language, scripting language, programming language, etc. For example, the machine readable instructions may be represented using any of the following languages: C, C++, Java, C#, Perl, Python, JavaScript, HyperText Markup Language (HTML), Structured Query Language (SQL), Swift, etc.</p><p id="p-0127" num="0125">As mentioned above, the example processes of <figref idref="DRAWINGS">FIGS. <b>14</b>-<b>15</b></figref> may be implemented using executable instructions (e.g., computer and/or machine readable instructions) stored on a non-transitory computer and/or machine readable medium such as a hard disk drive, a flash memory, a read-only memory, a compact disk, a digital versatile disk, a cache, a random-access memory and/or any other storage device or storage disk in which information is stored for any duration (e.g., for extended time periods, permanently, for brief instances, for temporarily buffering, and/or for caching of the information). As used herein, the term non-transitory computer readable medium is expressly defined to include any type of computer readable storage device and/or storage disk and to exclude propagating signals and to exclude transmission media.</p><p id="p-0128" num="0126">&#x201c;Including&#x201d; and &#x201c;comprising&#x201d; (and all forms and tenses thereof) are used herein to be open ended terms. Thus, whenever a claim employs any form of &#x201c;include&#x201d; or &#x201c;comprise&#x201d; (e.g., comprises, includes, comprising, including, having, etc.) as a preamble or within a claim recitation of any kind, it is to be understood that additional elements, terms, etc. may be present without falling outside the scope of the corresponding claim or recitation. As used herein, when the phrase &#x201c;at least&#x201d; is used as the transition term in, for example, a preamble of a claim, it is open-ended in the same manner as the term &#x201c;comprising&#x201d; and &#x201c;including&#x201d; are open ended. The term &#x201c;and/or&#x201d; when used, for example, in a form such as A, B, and/or C refers to any combination or subset of A, B, C such as (1) A alone, (2) B alone, (3) C alone, (4) A with B, (5) A with C. (6) B with C, and (7) A with B and with C. As used herein in the context of describing structures, components, items, objects and/or things, the phrase &#x201c;at least one of A and B&#x201d; is intended to refer to implementations including any of (1) at least one A, (2) at least one B, and (3) at least one A and at least one B. Similarly, as used herein in the context of describing structures, components, items, objects and/or things, the phrase &#x201c;at least one of A or B&#x201d; is intended to refer to implementations including any of (1) at least one A, (2) at least one B, and (3) at least one A and at least one B. As used herein in the context of describing the performance or execution of processes, instructions, actions, activities and/or steps, the phrase &#x201c;at least one of A and B&#x201d; is intended to refer to implementations including any of (1) at least one A, (2) at least one B, and (3) at least one A and at least one B. Similarly, as used herein in the context of describing the performance or execution of processes, instructions, actions, activities and/or steps, the phrase &#x201c;at least one of A or B&#x201d; is intended to refer to implementations including any of (1) at least one A, (2) at least one B, and (3) at least one A and at least one B.</p><p id="p-0129" num="0127">As used herein, singular references (e.g., &#x201c;a&#x201d;, &#x201c;an&#x201d;, &#x201c;first&#x201d;, &#x201c;second&#x201d;, etc.) do not exclude a plurality. The term &#x201c;a&#x201d; or &#x201c;an&#x201d; entity, as used herein, refers to one or more of that entity. The terms &#x201c;a&#x201d; (or &#x201c;an&#x201d;), &#x201c;one or more&#x201d;, and &#x201c;at least one&#x201d; can be used interchangeably herein. Furthermore, although individually listed, a plurality of means, elements or method actions may be implemented by, e.g., a single unit or processor. Additionally, although individual features may be included in different examples or claims, these may possibly be combined, and the inclusion in different examples or claims does not imply that a combination of features is not feasible and/or advantageous.</p><p id="p-0130" num="0128">The program <b>1400</b> of <figref idref="DRAWINGS">FIG. <b>14</b></figref> includes block <b>1402</b>. At block <b>1402</b>, the example view labeler <b>402</b> (<figref idref="DRAWINGS">FIG. <b>4</b></figref>) receives video data of a scene. For example, the view labeler <b>402</b> receives input views including texture maps, depth maps, and/or object maps. At block <b>1404</b>, the example view labeler <b>402</b> identifies, synthesizes, and labels basic views and additional views from the input views. For example, the view labeler <b>402</b> identifies and labels multiple basic and additional views. Additionally or alternatively, the view labeler <b>402</b> identifies one basic view, no additional views, etc.</p><p id="p-0131" num="0129">At block <b>1406</b>, the example object loader <b>406</b> (<figref idref="DRAWINGS">FIG. <b>4</b></figref>) loads and/or generates an object map per view. For example, the object loader <b>406</b> loads the object maps of the image data of the scene. At block <b>1408</b>, the example pruners <b>410</b><i>a, </i><b>410</b><i>b </i>(<figref idref="DRAWINGS">FIG. <b>4</b></figref>) perform pruning on the basic views and the additional views. That is, the example pruners <b>410</b><i>a, </i><b>410</b><i>b </i>generate pruning masks. In some examples, the pruners <b>410</b><i>a, </i><b>410</b><i>b </i>perform object-based pruning. For example, the example object separators <b>408</b><i>a, </i><b>408</b><i>b </i>(<figref idref="DRAWINGS">FIG. <b>4</b></figref>) generate object layers based on the object map. The example pruner <b>410</b><i>a </i>projects the object layers of the basic views onto the object layers of the additional views of object A, the example pruner <b>410</b><i>b </i>projects the object layers of the basic views onto the object layers of the additional views of an object B, etc. Additionally or alternatively, the example pruners <b>410</b><i>a, </i><b>410</b><i>b </i>perform object-selected pruning. For example, the pruner <b>410</b><i>a </i>projects the object layer of the basic view onto the object layers of the additional views of object A, the example pruner <b>410</b><i>b </i>projects the object layer of the basic view onto the object layers of the additional views of object B, etc.</p><p id="p-0132" num="0130">At block <b>1410</b>, the example pruners <b>410</b><i>a, </i><b>410</b><i>b </i>determine whether to analyze additional objects. For example, the pruners <b>410</b><i>a, </i><b>410</b><i>b </i>determine if all of the object indices of the object maps have been analyzed. If the example pruners <b>410</b><i>a, </i><b>410</b><i>b </i>determine to analyze additional objects, control returns to block <b>1408</b>. If the example pruners <b>410</b><i>a, </i><b>410</b><i>b </i>determine to not analyze additional objects, at block <b>1412</b>, the example object-based atlas constructor <b>404</b> (<figref idref="DRAWINGS">FIG. <b>4</b></figref>) determines whether to analyze additional frames. For example, the object-based atlas constructor <b>404</b> determines whether there are remaining frames of the input data in an intra-period. If the example object-based atlas constructor <b>404</b> determines to analyze additional frames, control returns to block <b>1404</b>.</p><p id="p-0133" num="0131">If the example object-based atlas constructor <b>404</b> determines to not analyze additional frames, at block <b>1414</b>, the example <b>414</b> (<figref idref="DRAWINGS">FIG. <b>4</b></figref>) aggregates pruning masks. For example, the aggregator <b>414</b> aggregates the pruning masks of the frames of the intra-period. At block <b>1416</b>, the example patch packer <b>418</b> (<figref idref="DRAWINGS">FIG. <b>4</b></figref>) generates patches. For example, the patch packer <b>418</b> generates patches based on the aggregated masks. In examples disclosed herein, each patch is associated with a single object. Thus, the example patch packer <b>418</b> tags the patch with the corresponding object ID, adds the object ID per patch within an associated atlas patch parameters list, etc.</p><p id="p-0134" num="0132">At block <b>1418</b>, the example atlas generator <b>420</b> generates atlases. For example, the atlas generator <b>420</b> packs the patches generated by the patch packer <b>418</b> into atlases. At block <b>1420</b>, the example object-based MIV encoder <b>310</b> generates an encoded bitstream. For example, the object-based MIV encoder <b>310</b> multiplexes the atlases and parameters to form a V3C sample stream.</p><p id="p-0135" num="0133">The program <b>1500</b> of <figref idref="DRAWINGS">FIG. <b>15</b></figref> includes block <b>1502</b>. At block <b>1502</b>, the example MIV decoder <b>356</b> receives encoded data. For example, the MIV decoder <b>356</b> receives the encoded bitstream generated by the example object-based MIV encoder <b>310</b> (<figref idref="DRAWINGS">FIG. <b>3</b></figref>). At block <b>1504</b>, the example MIV decoder <b>356</b> demultiplexes and decodes the encoded data. For example, the video decoder <b>502</b> (<figref idref="DRAWINGS">FIG. <b>5</b></figref>) decodes the encoded atlases to produce decoded atlases including texture and depth data (e.g., the atlases <b>514</b> of <figref idref="DRAWINGS">FIG. <b>5</b></figref>).</p><p id="p-0136" num="0134">At block <b>1506</b>, the example metadata parser <b>504</b> determiners parameter lists. For example, the metadata parser <b>504</b> parses the encoded data to determine example IV access unit parameters <b>516</b> (<figref idref="DRAWINGS">FIG. <b>5</b></figref>) and example IV sequence parameters <b>518</b> (<figref idref="DRAWINGS">FIG. <b>5</b></figref>). At block <b>1508</b>, the example atlas patch occupancy map generator <b>506</b> (<figref idref="DRAWINGS">FIG. <b>5</b></figref>) generates an atlas patch occupancy map. For example, the atlas patch occupancy map generator <b>506</b> generates the example occupancy map <b>520</b> (<figref idref="DRAWINGS">FIG. <b>5</b></figref>).</p><p id="p-0137" num="0135">At block <b>1510</b>, the example object filter <b>522</b> (<figref idref="DRAWINGS">FIG. <b>5</b></figref>) determines whether to perform object filtering. For example, the object filter <b>522</b> determines whether to filter out objects. In some examples, the object filter <b>522</b> receives user input and/or user preference (e.g., filter out one sports team, filter out background, etc.). If the example object filter <b>522</b> determines to perform object filtering, at block <b>1512</b>, the example object filter <b>522</b> performs object filtering. For example, the object filter <b>522</b> sets pixels corresponding to the object ID of filtered objects of the atlases to unoccupied, removes patches corresponding to the object ID of the filtered objects from the atlases, etc.</p><p id="p-0138" num="0136">Returning to block <b>1510</b>, if the example object filter <b>522</b> determines to not perform object filtering, at block <b>1514</b>, the example video renderer <b>362</b> (<figref idref="DRAWINGS">FIG. <b>3</b></figref>) synthesizes a view. For example, the synthesizer <b>524</b> (<figref idref="DRAWINGS">FIG. <b>4</b></figref>) synthesizes a scene based on the atlases, the occupancy map, and/or the parameters.</p><p id="p-0139" num="0137"><figref idref="DRAWINGS">FIG. <b>16</b></figref> is a block diagram of an example processor platform <b>1600</b> structured to execute the instructions of <figref idref="DRAWINGS">FIG. <b>14</b></figref> to implement the server side immersive video encoding system <b>302</b> of <figref idref="DRAWINGS">FIGS. <b>3</b> and/or <b>4</b></figref>. The processor platform <b>1600</b> can be, for example, a server, a personal computer, a workstation, a self-learning machine (e.g., a neural network), a mobile device (e.g., a cell phone, a smart phone, a tablet such as an iPad&#x2122;), a personal digital assistant (PDA), an Internet appliance, a DVD player, a CD player, a digital video recorder, a Blu-ray player, a gaming console, a personal video recorder, a set top box, a headset or other wearable device, or any other type of computing device.</p><p id="p-0140" num="0138">The processor platform <b>1600</b> of the illustrated example includes a processor <b>1612</b>. The processor <b>1612</b> of the illustrated example is hardware. For example, the processor <b>1612</b> can be implemented by one or more integrated circuits, logic circuits, microprocessors, GPUs, DSPs, or controllers from any desired family or manufacturer. The hardware processor may be a semiconductor based (e.g., silicon based) device. In this example, the processor implements the example video capturer(s) <b>304</b>, the example segmenter <b>305</b>, the example virtual camera. renderer <b>306</b>, the example object-based MIV encoder <b>310</b>, the example video encoder <b>312</b>. The example stream packager <b>314</b>, and the example content management system <b>315</b>.</p><p id="p-0141" num="0139">The processor <b>1612</b> of the illustrated example includes a local memory <b>1613</b> (e.g., a cache). The processor <b>1612</b> of the illustrated example is in communication with a main memory including a volatile memory <b>1614</b> and anon-volatile memory <b>1616</b> via a bus <b>1618</b>. The volatile memory <b>1614</b> may be implemented by Synchronous Dynamic Random Access Memory (SDRAM), Dynamic Random Access Memory (DRAM), RAMBUS&#xae; Dynamic Random Access Memory (RDRAM) and/or any other type of random access memory device. The non-volatile memory <b>1616</b> may be implemented by flash memory and/or any other desired type of memory device. Access to the main memory <b>1614</b>, <b>1616</b> is controlled by a memory controller.</p><p id="p-0142" num="0140">The processor platform <b>1600</b> of the illustrated example also includes an interface circuit <b>1620</b>. The interface circuit <b>1620</b> may be implemented by any type of interface standard, such as an Ethernet interface, a universal serial bus (USB), a Bluetooth&#xae; interface, a near field communication (NFC) interface, and/or a PCI express interface.</p><p id="p-0143" num="0141">In the illustrated example, one or more input devices <b>1622</b> are connected to the interface circuit <b>1620</b>. The input device(s) <b>1622</b> permit(s) a user to enter data and/or commands into the processor <b>1612</b>. The input device(s) can be implemented by, for example, an audio sensor, a microphone, a camera (still or video), a keyboard, a button, a mouse, a touchscreen, a track-pad, a trackball, isopoint and/or a voice recognition system.</p><p id="p-0144" num="0142">One or more output devices <b>1624</b> are also connected to the interface circuit <b>1620</b> of the illustrated example. The output devices <b>1624</b> can be implemented, for example, by display devices (e.g., a light emitting diode (LED), an organic light emitting diode (OLED), a liquid crystal display (LCD), a cathode ray tube display (CRT), an in-place switching (IPS) display, a touchscreen, etc.), a tactile output device, a printer and/or speaker. The interface circuit <b>1620</b> of the illustrated example, thus, typically includes a graphics driver card, a graphics driver chip and/or a graphics driver processor.</p><p id="p-0145" num="0143">The interface circuit <b>1620</b> of the illustrated example also includes a communication device such as a transmitter, a receiver, a transceiver, a modem, a residential gateway, a wireless access point, and/or a network interface to facilitate exchange of data with external machines (e.g., computing devices of any kind) via a network <b>1626</b>. The communication can be via, for example, an Ethernet connection, a digital subscriber line (DSL) connection, a telephone line connection, a coaxial cable system, a satellite system, a line-of-site wireless system, a cellular telephone system, etc.</p><p id="p-0146" num="0144">The processor platform <b>1600</b> of the illustrated example also includes one or more mass storage devices <b>1628</b> for storing software and/or data. Examples of such mass storage devices <b>1628</b> include floppy disk drives, hard drive disks, compact disk drives, Blu-ray disk drives, redundant array of independent disks (RAID) systems, and digital versatile disk (DVD) drives.</p><p id="p-0147" num="0145">The machine executable instructions <b>1632</b> of <figref idref="DRAWINGS">FIG. <b>14</b></figref> may be stored in the mass storage device <b>1628</b>, in the volatile memory <b>1614</b>, in the non-volatile memory <b>1616</b>, and/or on a removable non-transitory computer readable storage medium such as a CD or DVD.</p><p id="p-0148" num="0146"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is a block diagram of an example processor platform <b>1700</b> structured to execute the instructions of <figref idref="DRAWINGS">FIG. <b>15</b></figref> to implement the client side immersive video decoding system <b>350</b> of <figref idref="DRAWINGS">FIGS. <b>3</b> and/or <b>5</b></figref>. The processor platform <b>1700</b> can be, for example, a server, a personal computer, a workstation, a self-learning machine (e.g., a neural network), a mobile device (e.g., a cell phone, a smart phone, a tablet such as an iPad&#x2122;), a personal digital assistant (PDA), an Internet appliance, a DVD player, a CD player, a digital video recorder, a Blu-ray player, a gaming console, a personal video recorder, a set top box, a headset or other wearable device, or any other type of computing device.</p><p id="p-0149" num="0147">The processor platform <b>1700</b> of the illustrated example includes a processor <b>1712</b>. The processor <b>1712</b> of the illustrated example is hardware. For example, the processor <b>1712</b> can be implemented by one or more integrated circuits, logic circuits, microprocessors, GPUs, DSPs, or controllers from any desired family or manufacturer. The hardware processor may be a semiconductor based (e.g., silicon based) device. In this example, the processor implements the example de-packager <b>354</b>, the example MIV decoder <b>356</b>, the example presentation engine <b>358</b>, the example video decoder <b>360</b>, and the example video renderer <b>362</b>.</p><p id="p-0150" num="0148">The processor <b>1712</b> of the illustrated example includes a local memory <b>1713</b> (e.g., a cache). The processor <b>1712</b> of the illustrated example is in communication with a main memory including a volatile memory <b>1714</b> and a non-volatile memory <b>1716</b> via a bus <b>1718</b>. The volatile memory <b>1714</b> may be implemented by Synchronous Dynamic Random Access Memory (SDRAM), Dynamic Random Access Memory (DRAM), RAMBUS&#xae; Dynamic Random Access Memory (RDRAM&#xae;) and/or any other type of random access memory device. The non-volatile memory <b>1716</b> may be implemented by flash memory and/or any other desired type of memory device. Access to the main memory <b>1714</b>, <b>1716</b> is controlled by a memory controller.</p><p id="p-0151" num="0149">The processor platform <b>1700</b> of the illustrated example also includes an interface circuit <b>1720</b>. The interface circuit <b>1720</b> may be implemented by any type of interface standard, such as an Ethernet interface, a universal serial bus (USB), a Bluetooth&#xae; interface, a near field communication (NEC) interface, and/or a PCI express interface.</p><p id="p-0152" num="0150">In the illustrated example, one or more input devices <b>1722</b> are connected to the interface circuit <b>1720</b>. The input device(s) <b>1722</b> permit(s) a user to enter data and/or commands into the processor <b>1712</b>. The input device(s) can be implemented by, for example, an audio sensor, a microphone, a camera (still or video), a keyboard, a button, a mouse, a touchscreen, a track-pad, a trackball, isopoint and/or a voice recognition system.</p><p id="p-0153" num="0151">One or more output devices <b>1724</b> are also connected to the interface circuit <b>1720</b> of the illustrated example. The output devices <b>1724</b> can be implemented, for example, by display devices (e.g., a light emitting diode (LED), an organic light emitting diode (OLED), a liquid crystal display (LCD), a cathode ray tube display (CRT), an in-place switching (IPS) display, a touchscreen, etc.), a tactile output device, a printer and/or speaker. The interface circuit <b>1720</b> of the illustrated example, thus, typically includes a graphics driver card, a graphics driver chip and/or a graphics driver processor.</p><p id="p-0154" num="0152">The interface circuit <b>1720</b> of the illustrated example also includes a communication device such as a transmitter, a receiver, a transceiver, a modem, a residential gateway, a wireless access point, and/or a network interface to facilitate exchange of data with external machines (e.g., computing devices of any kind) via a network <b>1726</b>. The communication can be via, for example, an Ethernet connection, a digital subscriber line (DSL) connection, a telephone line connection, a coaxial cable system, a satellite system, a line-of-site wireless system, a cellular telephone system, etc.</p><p id="p-0155" num="0153">The processor platform <b>1700</b> of the illustrated example also includes one or more mass storage devices <b>1728</b> for storing software and/or data. Examples of such mass storage devices <b>1728</b> include floppy disk drives, hard drive disks, compact disk drives, Blu-ray disk drives, redundant array of independent disks (RAID) systems, and digital versatile disk (DVD) drives.</p><p id="p-0156" num="0154">The machine executable instructions <b>1732</b> of <figref idref="DRAWINGS">FIG. <b>15</b></figref> may be stored in the mass storage device <b>1728</b>, in the volatile memory <b>1714</b>, in the non-volatile memory <b>1716</b>, and/or on a removable non-transitory computer readable storage medium such as a CD or DVD.</p><p id="p-0157" num="0155">A block diagram illustrating an example software distribution platform <b>1805</b> to distribute software such as the example computer readable instructions <b>1632</b> of <figref idref="DRAWINGS">FIG. <b>16</b></figref> and/or the example computer readable instructions <b>1732</b> of <figref idref="DRAWINGS">FIG. <b>17</b></figref> to third parties is illustrated in <figref idref="DRAWINGS">FIG. <b>18</b></figref>. The example software distribution platform <b>1805</b> may be implemented by any computer server, data facility, cloud service, etc., capable of storing and transmitting software to other computing devices. The third parties may be customers of the entity owning and/or operating the software distribution platform. For example, the entity that owns and/or operates the software distribution platform may be a developer, a seller, and/or a licensor of software such as the example computer readable instructions <b>1632</b> of <figref idref="DRAWINGS">FIG. <b>16</b></figref> and/or the example computer readable instructions <b>1732</b> of <figref idref="DRAWINGS">FIG. <b>17</b></figref>. The third parties may be consumers, users, retailers, OEMs, etc., who purchase and/or license the software for use and/or re-sale and/or sub-licensing. In the illustrated. example, the software distribution platform <b>1805</b> includes one or more servers and one or more storage devices. The storage devices store the computer readable instructions <b>1632</b> and/or <b>1732</b>. which may correspond to the example computer readable instructions <b>1632</b> of <figref idref="DRAWINGS">FIG. <b>16</b></figref> and/or the example computer readable instructions <b>1732</b> of <figref idref="DRAWINGS">FIG. <b>17</b></figref>, as described above. The one or more servers of the example software distribution platform <b>1805</b> are in communication with a network <b>1810</b>, which may correspond to any one or more of the Internet and/or any of the example networks <b>1626</b>, <b>1726</b> described above. In some examples, the one or more servers are responsive to requests to transmit the software to a requesting party as part of a commercial transaction. Payment for the delivery, sale and/or license of the software may be handled by the one or more servers of the software distribution platform and/or via a third party payment entity. The servers enable purchasers and/or licensors to download the computer readable instructions <b>1632</b> and/or <b>1732</b> from the software distribution platform <b>1805</b>. For example, the software, which may correspond to the example computer readable instructions <b>1632</b> of <figref idref="DRAWINGS">FIG. <b>16</b></figref>, may be downloaded to the example processor platform <b>1600</b>, which is to execute the computer readable instructions <b>1632</b> to implement the server side immersive video encoding system <b>302</b>. For example, the software, which may correspond to the example computer readable instructions <b>1732</b> of <figref idref="DRAWINGS">FIG. <b>17</b></figref>, may be downloaded to the example processor platform <b>1700</b>, which is to execute the computer readable instructions <b>1732</b> to implement the client side immersive video decoding system <b>350</b>. In some example, one or more servers of the software distribution platform <b>1805</b> periodically offer, transmit, and/or force updates to the software (e.g., the example computer readable instructions <b>1632</b> of <figref idref="DRAWINGS">FIG. <b>16</b></figref> and/or the example computer readable instructions <b>1732</b> of <figref idref="DRAWINGS">FIG. <b>17</b></figref>) to ensure improvements, patches, updates, etc. are distributed and applied to the software at the end user devices.</p><p id="p-0158" num="0156">From the foregoing, it will be appreciated that example methods, apparatus and articles of manufacture have been disclosed that enable object-based immersive video coding. The disclosed methods, apparatus and articles of manufacture improve the efficiency of using a computing device by assigning object IDs. For example, an object-based encoder varies the number of bits used to store object IDs based on the total number of objects in a scene. The object-based encoder identifies objects of interest and allocates relatively more bandwidth to encode the objects of interest and relatively less bandwidth to background objects, static objects, etc. The disclosed methods, apparatus and articles of manufacture are accordingly directed to one or more improvement(s) in the functioning of a computer.</p><p id="p-0159" num="0157">Example methods, apparatus, systems, and articles of manufacture for immersive video coding using object metadata are disclosed herein. Further examples and combinations thereof include the following:</p><p id="p-0160" num="0158">Example 1 includes a video encoder comprising an object separator to separate input views into layers associated with respective objects to generate object layers for geometry data and texture data of the input views, a first object layer associated with a first object, the input views including at least one basic view and at least one additional view, a pruner to project the first object layer of a first basic view of the at least one basic views against the first object layer of a first additional view of the at least one additional views to generate a first pruned view and a first pruning mask, a patch packer to tag a patch with an object identifier of the first object, the patch corresponding to the first pruning mask, and an atlas generator to generate at least one atlas to include in encoded video data, the atlas including the patch.</p><p id="p-0161" num="0159">Example 2 includes the video encoder of example 1, wherein the pruner is to project the first object layer of the first basic view and the first object layer of the first pruned view against the first object layer of a second additional view of the at least one additional view to generate a second pruning mask.</p><p id="p-0162" num="0160">Example 3 includes the video encoder of example 2, further including an aggregator to aggregate the first pruning mask and the second pruning mask.</p><p id="p-0163" num="0161">Example 4 includes the video encoder of example 3, further including a clusterer to cluster pixels of the first pruning mask and the second pruning mask to generate the patch.</p><p id="p-0164" num="0162">Example 5 includes the video encoder of example 1, wherein the patch packer is to identify a location of the patch in the at least one atlas, and generate metadata corresponding to the patch.</p><p id="p-0165" num="0163">Example 6 includes the video encoder of example 5, wherein the at least one atlas includes at least one of texture maps, depth maps, or occupancy maps.</p><p id="p-0166" num="0164">Example 7 includes the video encoder of example 1, wherein the input views include texture maps, depth maps, and object maps.</p><p id="p-0167" num="0165">Example 8 includes the video encoder of example 7, further including a segmenter to generate object maps, wherein data locations of the object maps store object identifiers identifying objects represented in the texture maps and the depth maps.</p><p id="p-0168" num="0166">Example 9 includes the video encoder of example 7, wherein the object separator is to generate the object layers based on the object maps, the object layers corresponding to object identifiers and the object maps including the object identifiers.</p><p id="p-0169" num="0167">Example 10 includes the video encoder of example 1, wherein the input views include a first view corresponding to a first object map, the first view includes the first object, and the object identifier of the first object is stored in the first object map.</p><p id="p-0170" num="0168">Example 11 includes the video encoder of example 10, wherein the input views include a second view corresponding to a second object map, the second view includes the first object, and the object identifier of the first object is stored in the second object map.</p><p id="p-0171" num="0169">Example 12 includes the video encoder of example 1, further including a view labeler to select, from the input views, a view including the first object based on at least one criteria, the at least one criteria including at least one of pixel count of object maps, depth range of depth data, or a number of features of texture data.</p><p id="p-0172" num="0170">Example 13 includes the video encoder of example 12, wherein the pruner is to project the first object layer of the selected view onto the object layers of the additional views to generate pruning masks associated with the first object.</p><p id="p-0173" num="0171">Example 14 includes the video encoder of example 1, wherein the first object is associated with a bounding box, the bounding box having a depth, a height, and a width.</p><p id="p-0174" num="0172">Example 15 includes the video encoder of example 1, wherein the first object is static, and the pruner is to update the patch once per intra-period, an intra-period including a plurality of frames.</p><p id="p-0175" num="0173">Example 16 includes the video encoder of example 15, wherein the input views includes a second object, the object separator is to generate a second object layer associated with the second object and further including a clusterer to generate a second patch of the second object once per frame.</p><p id="p-0176" num="0174">Example 17 includes the video encoder of example 16, wherein the input views include a third object, the object separator is to generate a third object layer associated with the third object and the clusterer is to generate a third patch of the third object at a lower resolution than the second patch.</p><p id="p-0177" num="0175">Example 18 includes the video encoder of example 1, wherein the object identifier is stored in a supplemental enhancement information (SET) message.</p><p id="p-0178" num="0176">Example 19 includes a non-transitory computer readable medium comprising instructions which, when executed, cause a machine to at least separate input views into layers associated with respective objects to generate object layers for geometry data and texture data of the input views, a first object layer associated with a first object, the input views including at least one basic view and at least one additional view, project the first object layer of a first basic view of the at least one basic views against the first object layer of a first additional view of the at least one additional views to generate a first pruned view and a first pruning mask, tag a patch with an object identifier of the first object, the patch corresponding to the first pruning mask, and generate at least one atlas to include in encoded video data, the atlas including the patch.</p><p id="p-0179" num="0177">Example 20 includes the non-transitory computer readable medium of example 19, wherein the instructions cause the machine to project the first object layer of the first basic view and the first object layer of the first pruned view against the first object layer of a second additional view of the at least one additional view to generate a second pruning mask.</p><p id="p-0180" num="0178">Example 21 includes the non-transitory computer readable medium of example 20, wherein the instructions cause the machine to aggregate the first pruning mask and the second pruning mask.</p><p id="p-0181" num="0179">Example 22 includes the non-transitory computer readable medium of example 21, wherein the instructions cause the machine to cluster pixels of the first pruning mask and the second pruning mask to generate the patch.</p><p id="p-0182" num="0180">Example 23 includes the non-transitory computer readable medium of example 19, wherein the instructions cause the machine to identify a location of the patch in the at least one atlas, and generate metadata corresponding to the patch.</p><p id="p-0183" num="0181">Example 24 includes the non-transitory computer readable medium of example 23, wherein the at least one atlas includes at least one of texture maps, depth maps, or occupancy maps.</p><p id="p-0184" num="0182">Example 25 includes the non-transitory computer readable medium of example 19, wherein the input views include texture maps, depth maps, and object maps.</p><p id="p-0185" num="0183">Example 26 includes the non-transitory computer readable medium of example 25, wherein the instructions cause the machine to generate object maps, wherein data locations of the object maps store object identifiers identifying objects represented in the texture maps and the depth maps.</p><p id="p-0186" num="0184">Example 27 includes the non-transitory computer readable medium of example 25, wherein the instructions cause the machine to generate the object layers based on the object maps, the object layers corresponding to object identifiers and the object maps including the object identifiers.</p><p id="p-0187" num="0185">Example 28 includes the non-transitory computer readable medium of example 19, wherein the input views include a first view corresponding to a first object map, the first view includes the first object, and the object identifier of the first object is stored in the first object map.</p><p id="p-0188" num="0186">Example 29 includes the non-transitory computer readable medium of example 28, wherein the input views include a second view corresponding to a second object map, the second view includes the first object, and the object identifier of the first object is stored in the second object map.</p><p id="p-0189" num="0187">Example 30 includes the non-transitory computer readable medium of example 19, wherein the instructions cause the machine to select, from the input views, a view including the first object based on at least one criteria, the at least one criteria including at least one of pixel count of object maps, depth range of depth data, or a number of features of texture data.</p><p id="p-0190" num="0188">Example 31 includes the non-transitory computer readable medium of example 30, wherein the instructions cause the machine to project the first object layer of the selected view onto the object layers of the additional views to generate pruning masks associated with the first object.</p><p id="p-0191" num="0189">Example 32 includes the non-transitory computer readable medium of example 19, wherein the first object is associated with a bounding box, the bounding box having a depth, a height, and a width.</p><p id="p-0192" num="0190">Example 33 includes the non-transitory computer readable medium of example 19, wherein the first object is static, and the instructions cause the machine to update the patch once per intra-period, an intra-period including a plurality of frames.</p><p id="p-0193" num="0191">Example 34 includes the non-transitory computer readable medium of example 33, wherein the input views includes a second object, the instructions cause the machine to generate a second object layer associated with the second object and generate a second patch of the second object once per frame.</p><p id="p-0194" num="0192">Example 35 includes the non-transitory computer readable medium of example 34, wherein the input views include a third object, the instructions cause the machine to generate a third object layer associated with the third object and generate a third patch of the third object at a lower resolution than the second patch.</p><p id="p-0195" num="0193">Example 36 includes the non-transitory computer readable medium of example 19, wherein the object identifier is stored in a supplemental enhancement information (SEI) message.</p><p id="p-0196" num="0194">Example 37 includes a method comprising separating input views into layers associated with respective objects to generate object layers for geometry data and texture data of the input views, a first object layer associated with a first object, the input views including at least one basic view and at least one additional view, projecting the first object layer of a first basic view of the at least one basic views against the first object layer of a first additional view of the at least one additional views to generate a first pruned view and a first pruning mask, tagging a patch with an object identifier of the first object, the patch corresponding to the first pruning mask, and generating at least one atlas to include in encoded video data, the atlas including the patch.</p><p id="p-0197" num="0195">Example 38 includes the method of example 37, further including projecting the first object layer of the first basic view and the first object layer of the first pruned view against the first object layer of a second additional view of the at least one additional view to generate a second pruning mask.</p><p id="p-0198" num="0196">Example 39 includes the method of example 38, further including aggregating the first pruning mask and the second pruning mask.</p><p id="p-0199" num="0197">Example 40 includes the method of example 39, further including clustering pixels of the first pruning mask and the second pruning mask to generate the patch.</p><p id="p-0200" num="0198">Example 41 includes the method of example 37. further including identifying a location of the patch in the at least one atlas, and generating metadata corresponding to the patch.</p><p id="p-0201" num="0199">Example 42 includes the method of example 41, wherein the at least one atlas includes at least one of texture maps, depth maps, or occupancy maps.</p><p id="p-0202" num="0200">Example 43 includes the method of example 37, wherein the input views include texture maps, depth maps, and object maps.</p><p id="p-0203" num="0201">Example 44 includes the method of example 43, further including generating object maps, wherein data locations of the object maps store object identifiers identifying objects represented in the texture maps and the depth maps.</p><p id="p-0204" num="0202">Example 45 includes the method of example 43, further including generating the object layers based on the object maps, the object layers corresponding to object identifiers and the object maps including the object identifiers.</p><p id="p-0205" num="0203">Example 46 includes the method of example 37, wherein the input views include a first view corresponding to a first object map, the first view include the first object, and the object identifier of the first object is stored in the first object map.</p><p id="p-0206" num="0204">Example 47 includes the method of example 46, wherein the input views include a second view corresponding to a second object map, the second view includes the first object, and the object identifier of the first object is stored in the second object map.</p><p id="p-0207" num="0205">Example 48 includes the method of example 37, further including selecting, from the input views, a view including the first object based on at least one criteria, the at least one criteria including at least one of pixel count of object maps, depth range of depth data, or a number of features of texture data.</p><p id="p-0208" num="0206">Example 49 includes the method of example 48, further including projecting the first object layer of the selected view onto the object layers of the additional views to generate pruning masks associated with the first object.</p><p id="p-0209" num="0207">Example 50 includes the method of example 37, wherein the first object is associated with a bounding box, the bounding box having a depth, a height, and a width.</p><p id="p-0210" num="0208">Example 51 includes the method of example 37, wherein the first object is static, and further including updating the patch once per intra-period, an intra-period including a plurality of frames.</p><p id="p-0211" num="0209">Example 52 includes the method of example 51, wherein the input views includes a second object, further including generating a second object layer associated with the second object and generating a second patch of the second object once per frame.</p><p id="p-0212" num="0210">Example 53 includes the method of example 52, wherein the input views include a third object, further including generating a third object layer associated with the third object and generating a third patch of the third object at a lower resolution than the second patch.</p><p id="p-0213" num="0211">Example 54 includes the method of example 37, wherein the object identifier is stored in a supplemental enhancement information (SEI) message.</p><p id="p-0214" num="0212">Example 55 includes a video decoder, comprising a bitstream decoder to decode encoded video data to obtain a decoded atlas and metadata, the decoded atlas including patches tagged with an object identifier, the metadata including a map that maps blocks to patches, an object filter to remove a first patch associated with a first object identifier from the map to generate a modified map, and a renderer to map a second patch associated with a second object identifier from the decoded atlas based on the modified map, and render the second patch to synthesize a target view.</p><p id="p-0215" num="0213">Example 56 includes the video decoder of example 55, wherein the first patch associated with the first object identifier corresponds to a first object and the second patch associated with the second object identifier corresponds to a second object.</p><p id="p-0216" num="0214">Example 57 includes the video decoder of example 55, wherein the decoded video data includes a bounding box associated with the second object identifier.</p><p id="p-0217" num="0215">Example 58 includes the video decoder of example 57, wherein the renderer is to identify the bounding box to generate a viewport video that tracks movement of a second object based on the bounding box.</p><p id="p-0218" num="0216">Example 59 includes the video decoder of example 57, wherein the renderer is to render a third patch associated with a third object identifier to synthesize the target view, and blur the first patch associated with the first object identifier and the third patch associated with the third object identifier, the third object identifier corresponding to a background object of the target view.</p><p id="p-0219" num="0217">Example 60 includes the video decoder of example 59, wherein the renderer is to render the third patch associated with the third object identifier at a lower resolution than the second patch associated with the second object identifier.</p><p id="p-0220" num="0218">Example 61 includes the video decoder of example 59, wherein the renderer is to replace the third patch associated with the third object identifier with a synthesized object.</p><p id="p-0221" num="0219">Example 62 includes a non-transitory computer readable medium comprising instructions which, when executed, cause a machine to at least decode encoded video data to obtain a decoded atlas and metadata, the decoded atlas including patches tagged with an object identifier, the metadata including a map that maps blocks to patches, remove a first patch associated with a first object identifier from the map to generate a modified map, map a second patch associated with a second object identifier from the decoded atlas based on the modified map, and render the second patch to synthesize a target view.</p><p id="p-0222" num="0220">Example 63 includes the non-transitory computer readable medium of example 62, wherein the first patch associated with the first object identifier corresponds to a first object and the second patch associated with the second object identifier corresponds to a second object.</p><p id="p-0223" num="0221">Example 64 includes the non-transitory computer readable medium of example 62, wherein the decoded video data includes a bounding box associated with the second object identifier.</p><p id="p-0224" num="0222">Example 65 includes the non-transitory computer readable medium of example 64, wherein the instructions cause the machine to identify the bounding box to generate a viewport video that tracks movement of a second object based on the bounding box.</p><p id="p-0225" num="0223">Example 66 includes the non-transitory computer readable medium of example 64, wherein the instructions cause the machine to render a third patch associated with a third object identifier to synthesize the target view, and blur the first patch associated with the first object identifier and the third patch associated with the third object identifier, the third object identifier corresponding to a background object of the target view.</p><p id="p-0226" num="0224">Example 67 includes the non-transitory computer readable medium of example 66, wherein the instructions cause the machine to render the third patch associated with the third object identifier at a lower resolution than the second patch associated with the second object identifier.</p><p id="p-0227" num="0225">Example 68 includes the non-transitory computer readable medium of example 66, wherein instructions cause the machine to replace the third patch associated with the third object identifier with a synthesized object.</p><p id="p-0228" num="0226">Example 69 includes a method comprising decoding encoded video data to obtain a decoded atlas and metadata, the decoded atlas including patches tagged with an object identifier, the metadata including a map that maps blocks to patches, removing a first patch associated with a first object identifier from the map to generate a modified map, mapping a second patch associated with a second object identifier from the decoded atlas based on the modified map, and rendering the second patch to synthesize a target view.</p><p id="p-0229" num="0227">Example 70 includes the method of example 69, wherein the first patch associated with the first object identifier corresponds to a first object and the second patch associated with the second object identifier corresponds to a second object.</p><p id="p-0230" num="0228">Example 71 includes the method of example 69, wherein the decoded video data includes a bounding box associated with the second object identifier.</p><p id="p-0231" num="0229">Example 72 includes the method of example 71, further including identifying the bounding box to generate a viewport video that tracks movement of a second object based on the bounding box.</p><p id="p-0232" num="0230">Example 73 includes the method of example 71, further including rendering a third patch associated with a third object identifier to synthesize the target view, and blurring the first patch associated with the first object identifier and the third patch associated with the third object identifier, the third object identifier corresponding to a background object of the target view.</p><p id="p-0233" num="0231">Example 74 includes the method of example 73, further including rendering the third patch associated with the third object identifier at a lower resolution than the second patch associated with the second object identifier.</p><p id="p-0234" num="0232">Example 75 includes the method of example 73, further including replacing the third patch associated with the third object identifier with a synthesized object.</p><p id="p-0235" num="0233">Although certain example methods, apparatus and articles of manufacture have been disclosed herein, the scope of coverage of this patent is not limited thereto. On the contrary, this patent covers all methods, apparatus and articles of manufacture fairly falling within the scope of the claims of this patent.</p><p id="p-0236" num="0234">The following claims are hereby incorporated into this Detailed Description by this reference, with each claim standing on its own as a separate embodiment of the present disclosure.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A video encoder comprising:<claim-text>at least one memory;</claim-text><claim-text>instructions; and</claim-text><claim-text>processor circuitry to execute the instructions to:</claim-text><claim-text>separate input views into layers associated with respective objects to generate object layers for geometry data and texture data of the input views, a first object layer associated with a first object, the input views including at least one basic view and at least one additional view;</claim-text><claim-text>project the first object layer of a first basic view of the at least one basic views against the first object layer of a first additional view of the at least one additional views to generate a first pruned view and a first pruning mask;</claim-text><claim-text>tag a patch with an object identifier of the first object, the patch corresponding to the first pruning mask; and</claim-text><claim-text>generate at least one atlas to include in encoded video data, the atlas including the patch.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The video encoder of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the processor circuitry is to execute the instructions to project the first object layer of the first basic view and the first object layer of the first pruned view against the first object layer of a second additional view of the at least one additional view to generate a second pruning mask.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The video encoder of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the processor circuitry is to aggregate the first pruning mask and the second pruning mask.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The video encoder of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the processor circuitry is to cluster pixels of the first pruning mask and the second pruning mask to generate the patch.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The video encoder of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the processor circuitry is to:<claim-text>identify a location of the patch in the at least one atlas; and</claim-text><claim-text>generate metadata corresponding to the patch.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The video encoder of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the at least one atlas includes at least one of texture maps, depth maps, or occupancy maps.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The video encoder of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the input views include texture maps, depth maps, and object maps.</claim-text></claim><claim id="CLM-08-18" num="08-18"><claim-text><b>8</b>-<b>18</b>. (canceled)</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. A non-transitory computer readable medium comprising instructions which, when executed, cause processor circuitry to at least:<claim-text>separate input views into layers associated with respective objects to generate object layers for geometry data and texture data of the input views, a first object layer associated with a first object, the input views including at least one basic view and at least one additional view;</claim-text><claim-text>project the first object layer of a first basic view of the at least one basic views against the first object layer of a first additional view of the at least one additional views to generate a first pruned view and a first pruning mask;</claim-text><claim-text>tag a patch with an object identifier of the first object, the patch corresponding to the first pruning mask; and</claim-text><claim-text>generate at least one atlas to include in encoded video data, the atlas including the patch.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The non-transitory computer readable medium of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein the instructions cause the processor circuitry to project the first object layer of the first basic view and the first object layer of the first pruned view against the first object layer of a second additional view of the at least one additional view to generate a second pruning mask.</claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. The non-transitory computer readable medium of <claim-ref idref="CLM-00020">claim 20</claim-ref>, wherein the instructions cause the processor circuitry to aggregate the first pruning mask and the second pruning mask.</claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. The non-transitory computer readable medium of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the instructions cause the processor circuitry to cluster pixels of the first pruning mask and the second pruning mask to generate the patch.</claim-text></claim><claim id="CLM-00023" num="00023"><claim-text><b>23</b>. The non-transitory computer readable medium of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein the instructions cause the processor circuitry to:<claim-text>identify a location of the patch in the at least one atlas; and</claim-text><claim-text>generate metadata corresponding to the patch.</claim-text></claim-text></claim><claim id="CLM-00024" num="00024"><claim-text><b>24</b>. The non-transitory computer readable medium of <claim-ref idref="CLM-00023">claim 23</claim-ref>, wherein the at least one atlas includes at least one of texture maps, depth maps, or occupancy maps.</claim-text></claim><claim id="CLM-00025" num="00025"><claim-text><b>25</b>. The non-transitory computer readable medium of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein the input views include texture maps, depth maps, and object maps.</claim-text></claim><claim id="CLM-26-54" num="26-54"><claim-text><b>26</b>-<b>54</b>. (canceled)</claim-text></claim><claim id="CLM-00055" num="00055"><claim-text><b>55</b>. A video decoder, comprising:<claim-text>at least one memory;</claim-text><claim-text>instructions; and</claim-text><claim-text>processor circuitry to execute the instructions to:</claim-text><claim-text>decode encoded video data to obtain a decoded atlas and metadata, the decoded atlas including patches tagged with an object identifier, the metadata including a map that maps blocks to patches;</claim-text><claim-text>remove a first patch associated with a first object identifier from the map to generate a modified map; and</claim-text><claim-text>map a second patch associated with a second object identifier from the decoded atlas based on the modified map; and</claim-text><claim-text>render the second patch to synthesize a target view.</claim-text></claim-text></claim><claim id="CLM-00056" num="00056"><claim-text><b>56</b>. The video decoder of <claim-ref idref="CLM-00055">claim 55</claim-ref>, wherein the first patch associated with the first object identifier corresponds to a first object and the second patch associated with the second object identifier corresponds to a second object.</claim-text></claim><claim id="CLM-00057" num="00057"><claim-text><b>57</b>. The video decoder of <claim-ref idref="CLM-00055">claim 55</claim-ref>, wherein the decoded video data includes a bounding box associated with the second object identifier.</claim-text></claim><claim id="CLM-00058" num="00058"><claim-text><b>58</b>. The video decoder of <claim-ref idref="CLM-00057">claim 57</claim-ref>, wherein the processor circuitry is to identify the bounding box to generate a viewport video that tracks movement of a second object based on the bounding box.</claim-text></claim><claim id="CLM-00059" num="00059"><claim-text><b>59</b>. The video decoder of <claim-ref idref="CLM-00057">claim 57</claim-ref>, wherein the processor circuitry is to:<claim-text>render a third patch associated with a third object identifier to synthesize the target view; and</claim-text><claim-text>blur the first patch associated with the first object identifier and the third patch associated with the third object identifier, the third object identifier corresponding to a background object of the target view.</claim-text></claim-text></claim><claim id="CLM-00060" num="00060"><claim-text><b>60</b>. The video decoder of <claim-ref idref="CLM-00059">claim 59</claim-ref>, wherein the processor circuitry is to render the third patch associated with the third object identifier at a lower resolution than the second patch associated with the second object identifier.</claim-text></claim><claim id="CLM-61-75" num="61-75"><claim-text><b>61</b>-<b>75</b>. (canceled)</claim-text></claim></claims></us-patent-application>