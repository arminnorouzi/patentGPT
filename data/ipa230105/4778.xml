<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004779A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004779</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17942232</doc-number><date>20220912</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>04</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>62</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>04</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>6298</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">STORAGE MEDIUM, ESTIMATION METHOD, AND INFORMATION PROCESSING APPARATUS</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>PCT/JP2020/016212</doc-number><date>20200410</date></document-id><parent-status>PENDING</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>17942232</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>FUJITSU LIMITED</orgname><address><city>Kawasaki-shi</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Kamata</last-name><first-name>Yuichi</first-name><address><city>Isehara</city><country>JP</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>NAKAGAWA</last-name><first-name>Akira</first-name><address><city>Sagamihara</city><country>JP</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Kato</last-name><first-name>Keizo</first-name><address><city>Kawasaki</city><country>JP</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>FUJITSU LIMITED</orgname><role>03</role><address><city>Kawasaki-shi</city><country>JP</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A non-transitory computer-readable storage medium storing an estimation program that causes at least one computer to execute a process, the process includes inputting an input data into a trained variational autoencoder that includes an encoder and a decoder; converting, into a first probability distribution, a probability distribution of a latent variable that is generated by the trained variational autoencoder according to the input based on a magnitude of a standard deviation output from the encoder; converting the first probability distribution into a second probability distribution based on an output error of the decoder regarding the input data; and outputting the second probability distribution as an estimated value of a probability distribution of the input data.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="98.55mm" wi="158.75mm" file="US20230004779A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="218.78mm" wi="154.35mm" orientation="landscape" file="US20230004779A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="121.75mm" wi="168.99mm" file="US20230004779A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="253.24mm" wi="139.45mm" orientation="landscape" file="US20230004779A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="247.06mm" wi="134.37mm" orientation="landscape" file="US20230004779A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="166.62mm" wi="109.30mm" orientation="landscape" file="US20230004779A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="186.69mm" wi="115.74mm" file="US20230004779A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="174.33mm" wi="115.74mm" file="US20230004779A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="147.32mm" wi="89.07mm" orientation="landscape" file="US20230004779A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="131.23mm" wi="128.86mm" file="US20230004779A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="128.86mm" wi="138.01mm" file="US20230004779A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="244.94mm" wi="136.74mm" orientation="landscape" file="US20230004779A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="165.95mm" wi="111.34mm" orientation="landscape" file="US20230004779A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION</heading><p id="p-0002" num="0001">This application is a continuation application of International Application PCT/JP2020/016212 filed on Apr. 10, 2020 and designated the U.S., the entire contents of which are incorporated herein by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">FIELD</heading><p id="p-0003" num="0002">The present invention relates to a storage medium, an estimation method, and an information processing apparatus.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">In data analysis, by using autoencoders or the like, low-dimensional features are extracted from complex multidimensional data, and data analysis is performed using the features. For example, features of an image of a product flowing on a belt conveyor are extracted, and a defective product is detected from among the flowing products.</p><p id="p-0005" num="0004">In recent years, data analysis using a variational autoencoder (VAE) that trains a latent variable as a probability distribution has been used. For example, the VAE includes an encoder and a decoder, and parameters of the encoder and the decoder are machine-learned so as to minimize an expected value of a reconstruction error calculated using an output of the decoder to which the latent variable is input and a normalization error of a probability distribution of a latent variable calculated using an output of the encoder to which the features are input. Anomaly data is detected by inputting a plurality of pieces of detection target data into the VAE that has been trained (trained) in this way.<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0005">Non-Patent Document 1: Diederik P. Kingma, Max Welling, &#x201c;Auto-Encoding Variational Bayes&#x201d;, ICLR 2014.</li></ul></p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0006" num="0006">According to an aspect of the embodiments, a non-transitory computer-readable storage medium storing an estimation program that causes at least one computer to execute a process, the process includes inputting an input data into a trained variational autoencoder that includes an encoder and a decoder; converting, into a first probability distribution, a probability distribution of a latent variable that is generated by the trained variational autoencoder according to the input based on a magnitude of a standard deviation output from the encoder; converting the first probability distribution into a second probability distribution based on an output error of the decoder regarding the input data; and outputting the second probability distribution as an estimated value of a probability distribution of the input data.</p><p id="p-0007" num="0007">The object and advantages of the invention will be realized and attained by means of the elements and combinations particularly pointed out in the claims.</p><p id="p-0008" num="0008">It is to be understood that both the foregoing general description and the following detailed description are exemplary and explanatory and are not restrictive of the invention.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0009" num="0009"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram for explaining an information processing apparatus according to a first embodiment;</p><p id="p-0010" num="0010"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a functional block diagram illustrating a functional configuration of the information processing apparatus according to the first embodiment;</p><p id="p-0011" num="0011"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram for explaining a configuration of a VAE and machine learning;</p><p id="p-0012" num="0012"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram for explaining calculation of a probability distribution of input data;</p><p id="p-0013" num="0013"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram for explaining a correspondence between the input data and each variable;</p><p id="p-0014" num="0014"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flowchart illustrating a flow of training processing;</p><p id="p-0015" num="0015"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart illustrating a flow of detection processing;</p><p id="p-0016" num="0016"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a diagram for explaining input data that is artificially generated for verification;</p><p id="p-0017" num="0017"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a diagram for explaining an anomaly detection result using a reference technique;</p><p id="p-0018" num="0018"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a diagram for explaining an anomaly detection result using the first embodiment;</p><p id="p-0019" num="0019"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a diagram for explaining another example of the VAE; and</p><p id="p-0020" num="0020"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a diagram for explaining a hardware configuration example.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DESCRIPTION OF EMBODIMENTS</heading><p id="p-0021" num="0021">In the VAE described above, a single-variable-independent normal distribution is assumed, and there is no guarantee that the obtained probability distribution of the latent space reflects a distribution of a real space. Therefore, in a case where determination target data is input to the trained VAE and a probability distribution of the input data is estimated using the output of the encoder so as to detect the anomaly data, it is not possible to guarantee an estimation result, anomaly detection accuracy is not high.</p><p id="p-0022" num="0022">Note that it is also considered that an autoencoder that applies the Rate-Distortion theory for minimizing an information entropy of a latent variable is used. In a case where such an autoencoder is used, the probability distribution of the latent space is substantially the same as the probability distribution of the real space data. However, in a case where a shape of the probability distribution of the real space is complicated, it is needed to express the probability distribution of the latent space designed to be the same as that as a complicated shape, for example, by mixing a plurality of parametric probability distributions. Therefore, cost increases in order to improve accuracy, and this is not realistic.</p><p id="p-0023" num="0023">In one aspect, an object is to provide an estimation program, an estimation method, and an information processing apparatus that can improve accuracy of input data anomaly detection.</p><p id="p-0024" num="0024">In one aspect, accuracy of input data anomaly detection can be improved.</p><p id="p-0025" num="0025">Hereinafter, an embodiment of an estimation program, an estimation method, and an information processing apparatus according to the present invention will be described in detail with reference to the drawings. Note that the present invention is not limited by this embodiment. Furthermore, each of the embodiments may be appropriately combined within a range without inconsistency.</p><p id="p-0026" num="0026"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram for explaining an information processing apparatus <b>10</b> according to a first embodiment. The information processing apparatus <b>10</b> illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref> inputs input data of a real space into a model generated using a VAE and corrects a prior probability to be substantially the same as a generation probability of the input data using a posterior distribution parameter of a latent variable estimated by an encoder of the VAE. In this way, the information processing apparatus <b>10</b> is a computer device that estimates a probability distribution of the input data from the probability distribution of the latent space extracted by the VAE and improves anomaly detection accuracy of the input data.</p><p id="p-0027" num="0027">Specifically, the information processing apparatus <b>10</b> performs machine learning of the VAE that includes an encoder and a decoder using training data and generates a model to which the trained VAE is applied. Then, the information processing apparatus <b>10</b> inputs input data of the same domain as the training data into an encoder of the model and acquires restored input data from a decoder of the model.</p><p id="p-0028" num="0028">Here, the information processing apparatus <b>10</b> converts the probability distribution of the latent variable output from the encoder of the VAE into a first probability distribution on the basis of a magnitude of a standard deviation of an output of the encoder. Moreover, the information processing apparatus <b>10</b> converts the first probability distribution into a second probability distribution on the basis of an output error of the decoder of the VAE and outputs the second probability distribution as an estimated value of the probability distribution of the input data.</p><p id="p-0029" num="0029">In this way, the information processing apparatus <b>10</b> detects data, having a lower probability, that occupies a specific ratio as anomaly data on the basis of the generated second probability distribution. Furthermore, the information processing apparatus <b>10</b> may detect data having a probability equal to or less than a threshold, from among the plurality of pieces of input data, as the anomaly data, on the basis of the second probability distribution.</p><p id="p-0030" num="0030"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a functional block diagram illustrating a functional configuration of the information processing apparatus <b>10</b> according to the first embodiment. As illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the information processing apparatus <b>10</b> includes a communication unit <b>11</b>, a display unit <b>12</b>, a storage unit <b>13</b>, and a control unit <b>20</b>.</p><p id="p-0031" num="0031">The communication unit <b>11</b> controls communication with another device. For example, the communication unit <b>11</b> receives a machine learning start instruction and various types of data from an administrator's terminal and transmits a result of machine learning, a result of anomaly detection, or the like to the administrator's terminal.</p><p id="p-0032" num="0032">The storage unit <b>13</b> stores various types of data, programs executed by the control unit <b>20</b>, or the like. For example, the storage unit <b>13</b> stores training data <b>14</b>, input data <b>15</b>, a model <b>16</b>, or the like.</p><p id="p-0033" num="0033">The training data <b>14</b> is training data that is used for machine learning of the VAE and is data belonging to the same domain. For example, in a case where a model that detects a defective product from among products flowing on a belt conveyor is generated, the training data <b>14</b> corresponds to image data of the product or the like.</p><p id="p-0034" num="0034">The input data <b>15</b> is each piece of data to be input to the generated model and is data to be determined whether or not the data is abnormal. Explaining with reference to the above example, in a case where machine learning of the VAE is performed using the image data of the product as the training data <b>14</b>, the input data <b>15</b> corresponds to an image of the product flowing on the belt conveyor or the like.</p><p id="p-0035" num="0035">The model <b>16</b> is a model that is generated by the control unit <b>20</b>. Specifically, the model <b>16</b> is a model to which the VAE that is trained through machine learning using the training data <b>14</b> is applied.</p><p id="p-0036" num="0036">The control unit <b>20</b> is a processing unit that controls the entire information processing apparatus <b>10</b> and includes a training unit <b>21</b> and a detection unit <b>22</b>. The training unit <b>21</b> is a processing unit that performs machine learning of the VAE using the training data <b>14</b> and generates the model <b>16</b>. This training unit <b>21</b> generates the model <b>16</b> to which the VAE trained through machine learning illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref> to be described later and stores the model <b>16</b> in the storage unit <b>13</b>.</p><p id="p-0037" num="0037"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram for explaining a configuration of the VAE and machine learning. As illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the VAE includes an encoder <b>21</b><i>a </i>(f<sub>&#x3c6;</sub>(x)), a noise generation unit <b>21</b><i>b</i>, a decoder <b>21</b><i>c </i>(g<sub>&#x3c6;</sub>(z)), an estimation unit <b>21</b><i>d </i>(R), and an optimization unit <b>21</b><i>e </i>(&#x3b8;, &#x3c6;).</p><p id="p-0038" num="0038">Here, machine learning of the VAE will be described. When training data x belonging to a domain D is input, the encoder <b>21</b><i>a </i>compresses features of the training data x and outputs a mean &#x3bc;<sub>(x) </sub>and a standard deviation &#x3c3;<sub>(x) </sub>of an N-dimensional normal distribution. Then, the noise generation unit <b>21</b><i>b </i>generates an N-dimensional noise &#x3b5; according a mean 0 and a standard deviation I.</p><p id="p-0039" num="0039">By mixing a value obtained by multiplying the noise &#x3b5; generated by the noise generation unit <b>21</b><i>b </i>by the standard deviation &#x3c3;<sub>(x) </sub>into the mean &#x3bc;<sub>(x)</sub>, a latent variable z to be input to the decoder <b>21</b><i>c </i>through sampling is determined from the normal distribution according to the standard deviation &#x3c3;<sub>(x) </sub>and the mean &#x3bc;<sub>(x)</sub>. Then, the decoder <b>21</b><i>c </i>generates reconstructed data obtained by decoding the training data x, using the latent variable z corresponding to a feature vector of the training data x.</p><p id="p-0040" num="0040">Thereafter, the estimation unit <b>21</b><i>d </i>estimates a normalization error R that is an error between a probability distribution of the latent variable z calculated from the training data x and a prior probability distribution of the latent variable z, using the mean &#x3bc;<sub>(x) </sub>and the standard deviation &#x3c3;<sub>(x) </sub>output from the encoder <b>21</b><i>a</i>. Then, the optimization unit <b>21</b><i>e </i>adjusts (machine learning) each parameter of the encoder <b>21</b><i>a </i>and each parameter of the decoder <b>21</b><i>c </i>so as to minimize the normalization error R estimated by the estimation unit <b>21</b><i>d </i>and minimize a reconstruction error that is an error between the training data x and the reconstructed data.</p><p id="p-0041" num="0041">Returning to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the detection unit <b>22</b> detects anomaly data from the input data, using the model <b>16</b>. Specifically, the detection unit <b>22</b> converts input data of the domain into a parameter of the probability distribution of the latent variable using the encoder <b>21</b><i>a </i>that has trained training data of the domain by the VAE and calculates a generation probability of the input data using the converted parameter. In other words, the detection unit <b>22</b> estimates a probability distribution of each piece of the input data in the domain from the probability distribution of the latent variable extracted by the trained VAE and detects anomaly data using an estimation result.</p><p id="p-0042" num="0042">First, the detection unit <b>22</b> calculates a probability distribution p(X) of input data (x) from the probability distribution of the latent variable identified by the VAE. <figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram for explaining a probability distribution of input data. As illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, both at the time of training and detection, the input data (x) is converted unspecified principal component coordinates y once, and thereafter, it can be assumed that a scale be appropriately changed and the principal component coordinates y be converted into the latent variable z.</p><p id="p-0043" num="0043">Specifically, by performing Karhunen-Loeve expansion (KLT) as orthonormal transformation and principal component analysis (PCA) for the input data (x), a probability distribution (p(y<sub>m</sub>)) of a principal component with a small variance is generated from a probability distribution (p(y<sub>1</sub>)) of a principal component with a large variance. Thereafter, it is assumed that the scale be adjusted so as to equalize the variances when the probability distribution corresponding to each variance is converted into a latent variable of a normal distribution. For example, regarding a probability distribution (p(z<sub>1</sub>)) of the latent variable converted from the probability distribution (p(y<sub>1</sub>)) of the principal component with the large variance and a probability distribution (p(z<sub>m</sub>)) of the latent variable converted from the probability distribution (p(y<sub>m</sub>)) of the principal component with the small variance, the scale is adjusted so that the variances become the same.</p><p id="p-0044" num="0044">According to such an assumption, as illustrated in (a) of <figref idref="DRAWINGS">FIG. <b>4</b></figref>, by inversely converting a probability distribution p(z) of the latent variable, a probability distribution p(y) of a principal component can be generated. Then, as illustrated in (b) of <figref idref="DRAWINGS">FIG. <b>4</b></figref>, by further inversely converting the probability distribution p(y) of the principal component, the probability distribution p(X) of the input data (x) can be generated.</p><p id="p-0045" num="0045">Moreover, the detection unit <b>22</b> corrects a probability density of the latent variable estimated from the input data with the standard deviation output from the encoder <b>21</b><i>a </i>of the VAE. In a case where the scale changes at the time when the probability distribution (p(y)) of the assumed principal component is converted into the probability distribution (p(z)) of the latent variable z, the converted probability density changes in proportion to the scale.</p><p id="p-0046" num="0046"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram for explaining a correspondence between input data and each variable. <figref idref="DRAWINGS">FIG. <b>5</b></figref> selects and displays one of conversion processes illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref> and illustrates conversion from the input data into the probability distribution (p(z)) of the latent variable z. As illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, although the features (mean &#x3bc;<sub>(x)</sub>, standard deviation &#x3c3;<sub>(x)</sub>) are generated from the input data (x), a probability distribution (p(&#x3bc;<sub>(x)</sub>)) in a latent space of the mean &#x3bc;<sub>(x) </sub>of these is associated on the normal distribution, in the conversion process.</p><p id="p-0047" num="0047">On the other hand, since the standard deviation &#x3c3;<sub>(x) </sub>indicates a magnitude of the noise mixed into data, the standard deviation &#x3c3;<sub>(x) </sub>indicating the noise changes if the scale changes. That is, when it is assumed that the noise mixed into the input data is known (certain distribution), a change rate of the scale appears in the magnitude of the standard deviation &#x3c3;<sub>(x) </sub>output from the encoder <b>21</b><i>a </i>of the VAE. Therefore, the detection unit <b>22</b> defines a conversion scale as in the formula (1), using a coefficient &#x3b2; of a normalization term of an optimization equation. As a result of these, the probability distribution p(X) of the input data (x) can be expressed by the formula (2). That is, the probability distribution p(X) is an example of a sampling probability and is a distribution of a generation probability that each piece of the input data (x) follows.</p><p id="p-0048" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mrow>  <mo>[</mo>  <mrow>   <mi>Math</mi>   <mo>.</mo>   <mtext>   </mtext>   <mn>1</mn>  </mrow>  <mo>]</mo> </mrow></math></maths><maths id="MATH-US-00001-2" num="00001.2"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mfrac>       <msub>        <mi>dz</mi>        <mi>j</mi>       </msub>       <msub>        <mi>dy</mi>        <mi>j</mi>       </msub>      </mfrac>      <msub>       <mo>&#x2758;</mo>       <mrow>        <msub>         <mi>z</mi>         <mi>j</mi>        </msub>        <mo>=</mo>        <msub>         <mi>&#x3bc;</mi>         <mrow>          <mi>j</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <mi>x</mi>          <mo>)</mo>         </mrow>        </msub>       </mrow>      </msub>     </mrow>     <mo>=</mo>     <mrow>      <msqrt>       <mrow>        <mn>2</mn>        <mo>/</mo>        <mi>&#x3b2;</mi>       </mrow>      </msqrt>      <mo>&#x2062;</mo>      <msub>       <mi>&#x3c3;</mi>       <mrow>        <mi>j</mi>        <mo>&#x2061;</mo>        <mo>(</mo>        <mi>x</mi>        <mo>)</mo>       </mrow>      </msub>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mi>Formula</mi>     <mo>&#x2062;</mo>     <mtext>   </mtext>     <mrow>      <mo>(</mo>      <mn>1</mn>      <mo>)</mo>     </mrow>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0049" num="0048">(&#x3b2;: coefficient of normalization term of optimization equation)</p><p id="p-0050" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mrow>  <mo>[</mo>  <mrow>   <mi>Math</mi>   <mo>.</mo>   <mtext>   </mtext>   <mn>2</mn>  </mrow>  <mo>]</mo> </mrow></math></maths><maths id="MATH-US-00002-2" num="00002.2"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mi>p</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mi>X</mi>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mrow>      <mrow>       <mrow>        <semantics definitionURL="">         <mo>&#x2758;</mo>         <annotation encoding="Mathematica">"\[LeftBracketingBar]"</annotation>        </semantics>        <mfrac>         <mrow>          <mo>&#x2202;</mo>          <mtext> </mtext>          <mi>y</mi>         </mrow>         <mrow>          <mo>&#x2202;</mo>          <mtext> </mtext>          <mi>x</mi>         </mrow>        </mfrac>        <semantics definitionURL="">         <mo>&#x2758;</mo>         <annotation encoding="Mathematica">"\[RightBracketingBar]"</annotation>        </semantics>       </mrow>       <mo>&#x2062;</mo>       <mrow>        <munderover>         <mo>&#x220f;</mo>         <mrow>          <mi>j</mi>          <mo>=</mo>          <mn>1</mn>         </mrow>         <mi>m</mi>        </munderover>        <mrow>         <mi>p</mi>         <mo>&#x2061;</mo>         <mo>(</mo>         <msub>          <mi>y</mi>          <mi>j</mi>         </msub>         <mo>)</mo>        </mrow>       </mrow>      </mrow>      <mo>=</mo>      <mtext></mtext>      <mrow>       <mrow>        <mrow>         <semantics definitionURL="">          <mo>&#x2758;</mo>          <annotation encoding="Mathematica">"\[LeftBracketingBar]"</annotation>         </semantics>         <mfrac>          <mrow>           <mo>&#x2202;</mo>           <mtext> </mtext>           <mi>y</mi>          </mrow>          <mrow>           <mo>&#x2202;</mo>           <mtext> </mtext>           <mi>x</mi>          </mrow>         </mfrac>         <semantics definitionURL="">          <mo>&#x2758;</mo>          <annotation encoding="Mathematica">"\[RightBracketingBar]"</annotation>         </semantics>        </mrow>        <mo>&#x2062;</mo>        <mrow>         <munderover>          <mo>&#x220f;</mo>          <mrow>           <mi>j</mi>           <mo>=</mo>           <mn>1</mn>          </mrow>          <mi>m</mi>         </munderover>         <mfrac>          <mrow>           <mo>(</mo>           <mrow>            <mfrac>             <msub>              <mi>dz</mi>              <mi>j</mi>             </msub>             <msub>              <mi>dy</mi>              <mi>j</mi>             </msub>            </mfrac>            <mo>&#x2062;</mo>            <mrow>             <mi>p</mi>             <mo>&#x2061;</mo>             <mo>(</mo>             <msub>              <mi>z</mi>              <mi>j</mi>             </msub>             <mo>)</mo>            </mrow>           </mrow>           <mo>)</mo>          </mrow>          <mrow>           <msub>            <mi>z</mi>            <mi>j</mi>           </msub>           <mo>=</mo>           <msub>            <mi>&#x3bc;</mi>            <mrow>             <mi>j</mi>             <mo>&#x2061;</mo>             <mo>(</mo>             <mi>x</mi>             <mo>)</mo>            </mrow>           </msub>          </mrow>         </mfrac>        </mrow>       </mrow>       <mo>=</mo>       <mrow>        <msup>         <mrow>          <mo>(</mo>          <mrow>           <mn>2</mn>           <mo>/</mo>           <mi>&#x3b2;</mi>          </mrow>          <mo>)</mo>         </mrow>         <mfrac>          <mi>m</mi>          <mn>2</mn>         </mfrac>        </msup>        <mo>&#x2062;</mo>        <munder>         <munder>          <msup>           <mrow>            <mo>(</mo>            <mrow>             <mn>2</mn>             <mo>&#x2062;</mo>             <mi>&#x3c0;</mi>            </mrow>            <mo>)</mo>           </mrow>           <mfrac>            <mrow>             <mi>n</mi>             <mo>-</mo>             <mi>m</mi>            </mrow>            <mn>2</mn>           </mfrac>          </msup>          <mo>&#xfe38;</mo>         </munder>         <mi>A</mi>        </munder>        <mo>&#x2062;</mo>        <msup>         <mrow>          <semantics definitionURL="">           <mo>&#x2758;</mo>           <annotation encoding="Mathematica">"\[LeftBracketingBar]"</annotation>          </semantics>          <mi>G</mi>          <semantics definitionURL="">           <mo>&#x2758;</mo>           <annotation encoding="Mathematica">"\[RightBracketingBar]"</annotation>          </semantics>         </mrow>         <mfrac>          <mn>1</mn>          <mn>2</mn>         </mfrac>        </msup>        <mo>&#x2062;</mo>        <mrow>         <munderover>          <mo>&#x220f;</mo>          <mrow>           <mi>j</mi>           <mo>=</mo>           <mn>1</mn>          </mrow>          <mi>n</mi>         </munderover>         <munder>          <munder>           <mrow>            <mo>(</mo>            <mrow>             <msub>              <mi>&#x3c3;</mi>              <mrow>               <mi>j</mi>               <mo>&#x2061;</mo>               <mo>(</mo>               <mi>x</mi>               <mo>)</mo>              </mrow>             </msub>             <mo>&#x2062;</mo>             <mfrac>              <mrow>               <mi>p</mi>               <mo>&#x2061;</mo>               <mo>(</mo>               <msub>                <mi>&#x3bc;</mi>                <mrow>                 <mi>j</mi>                 <mo>&#x2061;</mo>                 <mo>(</mo>                 <mi>x</mi>                 <mo>)</mo>                </mrow>               </msub>               <mo>)</mo>              </mrow>              <mrow>               <mi>N</mi>               <mo>&#x2061;</mo>               <mo>(</mo>               <mrow>                <mn>0</mn>                <mo>,</mo>                <mn>1</mn>               </mrow>               <mo>)</mo>              </mrow>             </mfrac>            </mrow>            <mo>)</mo>           </mrow>           <mo>&#xfe38;</mo>          </munder>          <mi>B</mi>         </munder>        </mrow>       </mrow>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mi>Formula</mi>     <mo>&#x2062;</mo>     <mtext>   </mtext>     <mrow>      <mo>(</mo>      <mn>2</mn>      <mo>)</mo>     </mrow>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0051" num="0049">Note that, although a formula using m principal components is indicated in the formula (2), since an average value of the minus square of the standard deviation &#x3c3;<sub>(x) </sub>&#x201c;bar of &#x3c3;<sub>(x)</sub><sup>&#x2212;2</sup>&#x201d; corresponds to a data variance (unique value) in each principal component, an order indicating a variance of which principal component has the larger variance can be specified. Therefore, when compression to a predetermined dimension is performed, a principal component with a higher compression effect can be selected.</p><p id="p-0052" num="0050">Here, an item A in the formula (2) indicates a probability other than the principal component and is a constant value (mean &#x3bc;<sub>(x)</sub>=0, standard deviation &#x3c3;<sub>(x)</sub>=1), and an item B corresponds to a probability of the principal component. That is, in the assumption of the conversion illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, for example, a probability distribution of a principal component having a small variance, such as p(y<sub>m</sub>), is treated as a constant because the principal component is not dispersed.</p><p id="p-0053" num="0051">Therefore, as indicated in the formula (3), the probability distribution p(X) of the input data (x) can be expressed as proportional to the scale. Note that an item C in the formula (3) is a scale of the reconstruction error and can be defined by the formula (4) in a case where the normal distribution is assumed for the reconstruction error. That is, the probability distribution p(X) of the input data (x) can be defined with an item D of the formula (3), and the probability distribution of the latent variable can be corrected so as to reflect the generation probability of the input data with &#x3c3;<sub>(x) </sub>of the latent variable.</p><p id="p-0054" num="0000"><maths id="MATH-US-00003" num="00003"><math overflow="scroll"> <mrow>  <mo>[</mo>  <mrow>   <mi>Math</mi>   <mo>.</mo>   <mtext>   </mtext>   <mn>3</mn>  </mrow>  <mo>]</mo> </mrow></math></maths><maths id="MATH-US-00003-2" num="00003.2"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mi>p</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mi>X</mi>      <mo>)</mo>     </mrow>     <mo>&#x221d;</mo>     <mrow>      <munder>       <munder>        <msup>         <mrow>          <semantics definitionURL="">           <mo>&#x2758;</mo>           <annotation encoding="Mathematica">"\[LeftBracketingBar]"</annotation>          </semantics>          <msub>           <mi>G</mi>           <mi>x</mi>          </msub>          <semantics definitionURL="">           <mo>&#x2758;</mo>           <annotation encoding="Mathematica">"\[RightBracketingBar]"</annotation>          </semantics>         </mrow>         <mfrac>          <mn>1</mn>          <mn>2</mn>         </mfrac>        </msup>        <mo>&#xfe38;</mo>       </munder>       <mi>C</mi>      </munder>      <mo>&#x2062;</mo>      <munder>       <munder>        <mrow>         <mfrac>          <mrow>           <mi>p</mi>           <mo>&#x2061;</mo>           <mo>(</mo>           <msub>            <mi>&#x3bc;</mi>            <mrow>             <mo>(</mo>             <mi>x</mi>             <mo>)</mo>            </mrow>           </msub>           <mo>)</mo>          </mrow>          <mrow>           <msub>            <mi>N</mi>            <mi>n</mi>           </msub>           <mo>(</mo>           <mrow>            <mn>0</mn>            <mo>,</mo>            <msub>             <mi>I</mi>             <mi>n</mi>            </msub>           </mrow>           <mo>)</mo>          </mrow>         </mfrac>         <mo>&#x2062;</mo>         <mrow>          <munderover>           <mo>&#x220f;</mo>           <mrow>            <mi>j</mi>            <mo>=</mo>            <mn>1</mn>           </mrow>           <mi>n</mi>          </munderover>          <msub>           <mi>&#x3c3;</mi>           <mrow>            <mi>j</mi>            <mo>&#x2061;</mo>            <mo>(</mo>            <mi>x</mi>            <mo>)</mo>           </mrow>          </msub>         </mrow>        </mrow>        <mo>&#xfe38;</mo>       </munder>       <mi>D</mi>      </munder>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mi>Formula</mi>     <mo>&#x2062;</mo>     <mtext>   </mtext>     <mrow>      <mo>(</mo>      <mn>3</mn>      <mo>)</mo>     </mrow>    </mrow>   </mtd>  </mtr> </mtable></math></maths><maths id="MATH-US-00003-3" num="00003.3"><math overflow="scroll"> <mrow>  <mo>[</mo>  <mrow>   <mi>Math</mi>   <mo>.</mo>   <mtext>   </mtext>   <mn>4</mn>  </mrow>  <mo>]</mo> </mrow></math></maths><maths id="MATH-US-00003-4" num="00003.4"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <semantics definitionURL="">       <mo>&#x2758;</mo>       <annotation encoding="Mathematica">"\[LeftBracketingBar]"</annotation>      </semantics>      <msub>       <mi>G</mi>       <mi>x</mi>      </msub>      <semantics definitionURL="">       <mo>&#x2758;</mo>       <annotation encoding="Mathematica">"\[RightBracketingBar]"</annotation>      </semantics>     </mrow>     <mo>=</mo>     <msup>      <mrow>       <mo>(</mo>       <mfrac>        <mn>1</mn>        <mrow>         <mn>2</mn>         <mo>&#x2062;</mo>         <msup>          <mi>&#x3c3;</mi>          <mn>2</mn>         </msup>        </mrow>       </mfrac>       <mo>)</mo>      </mrow>      <mi>m</mi>     </msup>    </mrow>   </mtd>   <mtd>    <mrow>     <mi>Formula</mi>     <mo>&#x2062;</mo>     <mtext>   </mtext>     <mrow>      <mo>(</mo>      <mn>4</mn>      <mo>)</mo>     </mrow>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0055" num="0052">Here, processing for calculating the probability distribution of the input data described above will be described in detail. Specifically, similarly to the VAE, a probability distribution q<sub>&#x3c6;</sub>(z|x) of the latent variable z with respect to input data x of an m-dimensional domain is assumed as an m-dimensional Gaussian distribution N (&#x3bc;<sub>(x)</sub>, &#x3c3;<sub>(x)</sub>), and parameters &#x3bc;(x) and &#x3c3;(x) thereof are identified by the encoder <b>21</b> (f<sub>&#x3c6;</sub>(x)). Then, using the latent variable z indicated in the formula (5) sampled from the identified distribution, the reconstructed data is estimated by the decoder <b>21</b><i>c </i>(g<sub>&#x3c6;</sub>(z)) as indicated in the formula (6). Furthermore, the respective parameters of the encoder <b>21</b><i>a </i>and the decoder <b>21</b><i>c </i>are optimized through machine learning that minimizes the formula (7).</p><p id="p-0056" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>[Math. 5]<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0057" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>z=&#x3bc;</i><sub>(x)</sub>+&#x3c3;<sub>(x)</sub><i>&#x2299;&#x2208;,&#x2208;&#x2dc;N</i>(0,<i>I</i>)&#x2003;&#x2003;Formula (5)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0058" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>[Math. 6]<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0059" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>Reconstructed data <i>{circumflex over (x)}&#x2192;{circumflex over (x)}=g</i><sub>&#x3b8;</sub>(<i>z</i>)&#x2003;&#x2003;Formula (6)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0060" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>[Math. 7]<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0061" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>&#x3b2;&#xb7;<i>R+E&#x2dc;&#x3b2;&#xb7;D</i><sub>KL</sub>(<i>q</i><sub>&#x3c6;</sub>(<i>z|x</i>)&#x2225;<i>p</i>(<i>z</i>))+<i>E</i><sub>&#x2dc;q</sub><sub><sub2>&#x3c6;</sub2></sub><sub>(z|x)</sub>[&#x2212;log p<sub>&#x3b8;</sub>(<i>x|z</i>)]&#x2003;&#x2003;Formula (7)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0062" num="0053">Thereafter, for the acquired z, the detection unit <b>22</b> calculates n (n-dimensional) z that satisfy the formula (8) or the average value of the minus square of the standard deviation &#x3c3;<sub>(x) </sub>of the output of the encoder <b>21</b><i>a </i>regarding the trained input data and extracts n values in a descending order. Then, the detection unit <b>22</b> converts the input data (x) of the domain into distribution parameters (&#x3bc;<sub>(x)</sub>, &#x3c3;<sub>(x)</sub>) by the encoder <b>21</b><i>a </i>(f<sub>&#x3c6;</sub>(x)), and estimates a generation probability p(x) of the input data (x) according to the formula (9). That is, the generation probability p(x) is a generation probability for each piece of the input data (x), each piece of the input data (x) can be defined according to the probability distribution p(X) in the formula (2), and as result, the generation probability p(x) of each piece of the input data can be defined by the formula (9). Therefore, one generation probability p(x) is calculated for one piece of the input data, and the probability distribution p(X) is configured by collecting the plurality of generation probabilities p(x).</p><p id="p-0063" num="0000"><maths id="MATH-US-00004" num="00004"><math overflow="scroll"> <mrow>  <mo>[</mo>  <mrow>   <mi>Math</mi>   <mo>.</mo>   <mtext>   </mtext>   <mn>8</mn>  </mrow>  <mo>]</mo> </mrow></math></maths><maths id="MATH-US-00004-2" num="00004.2"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mrow>       <mrow>        <mrow>         <mrow>          <mo>{</mo>          <mrow>           <msub>            <mi>z</mi>            <mi>j</mi>           </msub>           <mo>&#x2208;</mo>           <mrow>            <mi>z</mi>            <mo>&#x2062;</mo>            <mrow>             <semantics definitionURL="">              <mo>&#x2758;</mo>              <annotation encoding="Mathematica">"\[LeftBracketingBar]"</annotation>             </semantics>             <mrow>              <msub>               <mi>D</mi>               <mi>KL</mi>              </msub>              <mo>(</mo>              <mrow>               <msub>                <mi>q</mi>                <mi>&#x3c6;</mi>               </msub>               <mo>(</mo>               <mrow>                <msub>                 <mi>z</mi>                 <mi>j</mi>                </msub>                <mo>&#x2758;</mo>                <mi>x</mi>               </mrow>               <mo>)</mo>              </mrow>             </mrow>            </mrow>           </mrow>          </mrow>         </mrow>         <mo>&#xf606;</mo>        </mrow>        <mo>&#x2062;</mo>        <mrow>         <mi>p</mi>         <mo>&#x2061;</mo>         <mo>(</mo>         <msub>          <mi>z</mi>          <mi>j</mi>         </msub>         <mo>)</mo>        </mrow>       </mrow>       <mo>)</mo>      </mrow>      <mo>&#x2260;</mo>      <mn>0</mn>     </mrow>     <mo>}</mo>    </mrow>   </mtd>   <mtd>    <mrow>     <mi>Formula</mi>     <mo>&#x2062;</mo>     <mtext>   </mtext>     <mrow>      <mo>(</mo>      <mn>8</mn>      <mo>)</mo>     </mrow>    </mrow>   </mtd>  </mtr> </mtable></math></maths><maths id="MATH-US-00004-3" num="00004.3"><math overflow="scroll"> <mrow>  <mo>[</mo>  <mrow>   <mi>Math</mi>   <mo>.</mo>   <mtext>   </mtext>   <mn>9</mn>  </mrow>  <mo>]</mo> </mrow></math></maths><maths id="MATH-US-00004-4" num="00004.4"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mi>p</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mi>x</mi>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mrow>      <munderover>       <mo>&#x220f;</mo>       <mrow>        <mi>j</mi>        <mo>=</mo>        <mn>1</mn>       </mrow>       <mi>n</mi>      </munderover>      <mrow>       <mo>(</mo>       <mrow>        <msub>         <mi>&#x3c3;</mi>         <mrow>          <mi>j</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <mi>x</mi>          <mo>)</mo>         </mrow>        </msub>        <mo>&#x2062;</mo>        <mfrac>         <mrow>          <mi>p</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <msub>           <mi>&#x3bc;</mi>           <mrow>            <mi>j</mi>            <mo>&#x2061;</mo>            <mo>(</mo>            <mi>x</mi>            <mo>)</mo>           </mrow>          </msub>          <mo>)</mo>         </mrow>         <mrow>          <mi>N</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <mrow>           <mn>0</mn>           <mo>,</mo>           <mn>1</mn>          </mrow>          <mo>)</mo>         </mrow>        </mfrac>       </mrow>       <mo>)</mo>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mi>Formula</mi>     <mo>&#x2062;</mo>     <mtext>   </mtext>     <mrow>      <mo>(</mo>      <mn>9</mn>      <mo>)</mo>     </mrow>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0064" num="0054">Then, the detection unit <b>22</b> detects a certain percentage, for example, 10% of the total, of lower data as the anomaly data, from data of the generation probabilities calculated using the formula (9) for the plurality of pieces of input data <b>15</b>.</p><p id="p-0065" num="0055">Next, a flow of training processing will be described. Here, the training data <b>14</b> will be described as the training data x. <figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flowchart illustrating a flow of the training processing. As illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the training unit <b>21</b> inputs the training data x into the encoder <b>21</b><i>a</i>, encodes the training data x by the encoder <b>21</b><i>a</i>, and acquires the distribution parameters (&#x3bc;<sub>(x)</sub>, &#x3c3;<sub>(x)</sub>) of the latent variable z (S<b>101</b>).</p><p id="p-0066" num="0056">Subsequently, the training unit <b>21</b> generates N-dimensional data by sampling a predetermined number of latent variables z (S<b>102</b>). Then, the training unit <b>21</b> acquires data obtained by inputting the N-dimensional data into the decoder <b>21</b><i>c </i>(g<sub>&#x3c6;</sub>(z)) and decoding the training data x (S<b>103</b>).</p><p id="p-0067" num="0057">Thereafter, the training unit <b>21</b> calculates a training cost using the normalization error R estimated by the estimation unit <b>21</b><i>d </i>and a reconstruction error E that is an error between the training data x and the reconstructed data (S<b>104</b>) and updates respective parameters (&#x3b8;, &#x3c6;) of the encoder <b>21</b><i>a </i>and the decoder <b>21</b><i>c </i>so as to minimize the training cost (S<b>105</b>).</p><p id="p-0068" num="0058">Thereafter, in a case where machine learning is not converged (S<b>106</b>: No), S<b>101</b> and subsequent steps are repeated for a next piece of the training data <b>14</b>. On the other hand, in a case where machine learning is converged (S<b>106</b>: Yes), the training unit <b>21</b> generates the model <b>16</b> to which the VAE that has completed machine learning is applied. Note that, in a case where the number of times of training is equal to or more than a threshold or a restoration error is equal to or less than a threshold, the training unit <b>21</b> can determine that machine learning is converged.</p><p id="p-0069" num="0059">Next, a flow of detection processing will be described. Here, the input data <b>15</b> will be described as the input data (x). <figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart illustrating a flowchart of detection processing. As illustrated in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the detection unit <b>22</b> reads the input data (x) (S<b>201</b>), inputs the input data (x) into the encoder <b>21</b><i>a </i>of the trained VAE, encodes the input data (x), and acquires the distribution parameters (&#x3bc;<sub>(x)</sub>, &#x3c3;<sub>(x)</sub>) of the latent variable z (S<b>201</b>).</p><p id="p-0070" num="0060">Subsequently, the detection unit <b>22</b> calculates the generation probability p(x) of the input data (x) using the formula (9), on the basis of the acquired distribution parameters (&#x3bc;<sub>(x)</sub>, &#x3c3;<sub>(x)</sub>) (S<b>202</b>). Here, in a case where there is unprocessed input data (x) (S<b>204</b>: No), the detection unit <b>22</b> repeats S<b>201</b> and subsequent steps for the next piece of the input data (x).</p><p id="p-0071" num="0061">Then, when completing the processing for all the pieces of the input data (x) (S<b>204</b>: Yes), the detection unit <b>22</b> detects a certain percentage of the input data (x), in ascending order of the generation probability p(x), as the anomaly data (S<b>205</b>).</p><p id="p-0072" num="0062">As described above, the information processing apparatus <b>10</b> can estimate the probability distribution of the input data, using the standard deviation output from the encoder <b>21</b><i>a </i>of the VAE. Therefore, the information processing apparatus <b>10</b> can guarantee that the probability distribution of the latent space reflects a distribution of the real space and can perform highly accurate anomaly detection using the output of the encoder <b>21</b><i>a </i>of the VAE. Furthermore, in a case where data of a real domain can be latently represented by a single-variable Gaussian distribution in a task using a probability distribution of the real domain, such as anomaly detection, the information processing apparatus <b>10</b> can perform input data anomaly detection without using a complicated distribution and reduce a calculation cost. The input data may be, for example, image data or audio data.</p><p id="p-0073" num="0063">Here, verification using input data that is artificially generated will be described. Here, respective verification results of the first embodiment in which the generation probability p(x) of the input data is estimated according to the formula (8) and the reference technique in which the generation probability (p(&#x3bc;<sub>(x)</sub>) of the input data is estimated using the latent space of the VAE will be described. <figref idref="DRAWINGS">FIG. <b>8</b></figref> is a diagram for explaining the input data that is artificially generated for verification, <figref idref="DRAWINGS">FIG. <b>9</b></figref> is a diagram for explaining an anomaly detection result using the reference technique, and <figref idref="DRAWINGS">FIG. <b>10</b></figref> is a diagram for explaining an anomaly detection result using the first embodiment.</p><p id="p-0074" num="0064">As illustrated in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, input data is generated using three pieces of data according to a probability density function (PDF). For example, three-dimensional input data is generated in which a variable of which a PDF belongs to p(S<sub>1</sub>) to be a range of &#x201c;0 to 1&#x201d;, a variable of which a PDF belongs to p(S<sub>2</sub>) to be &#x201c;(root 2)/4&#x201d;, and a variable of which a PDF belongs to p(S<sub>3</sub>) to be &#x201c;(root 3)/6 to 0) are multiplied.</p><p id="p-0075" num="0065">Then, a result of estimating the generation probability of the input data using the reference technique, for the plurality of pieces of input data, is illustrated in <figref idref="DRAWINGS">FIG. <b>9</b></figref>, and a result of estimating the generation probability of the input data using the first embodiment is illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref>. The horizontal axis of <figref idref="DRAWINGS">FIG. <b>9</b></figref> indicates the generation probability p(x), the vertical axis indicates an estimated probability (p(&#x3bc;<sub>(x)</sub>)). The horizontal axis of <figref idref="DRAWINGS">FIG. <b>10</b></figref> indicates the generation probability p(x) calculated by the formula (9), and the vertical axis indicates the estimated probability calculated by the formula (3). As illustrated in <figref idref="DRAWINGS">FIG. <b>9</b></figref>, with the reference technique, the generation probability of the input data is dispersed, and it is difficult to determine a range detected as abnormal, and anomaly detection accuracy is not high. On the other hand, as illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, with the first embodiment, the generation probability of the input data is linear, and a certain percentage with a low generation probability can be accurately specified and detected as abnormal. Therefore, the anomaly detection accuracy is improved.</p><p id="p-0076" num="0066">Numerical values, data, the number of dimensions, or the like used in the embodiment described above are merely examples and can be arbitrarily changed. Furthermore, a device that performs machine learning of the VAE and a device that performs anomaly detection may be implemented as separate devices.</p><p id="p-0077" num="0067">Furthermore, in addition to the configuration illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the VAE can adopt, for example, a configuration of a VAE that applies the Rate-Distortion theory or the like. <figref idref="DRAWINGS">FIG. <b>11</b></figref> is a diagram for explaining another example of the VAE. As illustrated in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, the VAE applying the Rate-Distortion theory includes the encoder <b>21</b><i>a</i>, the noise generation unit <b>21</b><i>b</i>, a decoder <b>21</b><i>c</i>-<b>1</b>, a decoder <b>21</b><i>c</i>-<b>2</b>, the estimation unit <b>21</b><i>d</i>, and the optimization unit <b>21</b><i>e. </i></p><p id="p-0078" num="0068">When the training data x belonging to the domain D is input, the encoder <b>21</b><i>a </i>compresses features of the training data x and outputs the mean &#x3bc;<sub>(x) </sub>and the standard deviation &#x3c3;<sub>(x) </sub>of the N-dimensional normal distribution. The decoder <b>21</b><i>c</i>-<b>1</b> generates reconstructed data obtained by decoding the input data using the mean &#x3bc;<sub>(x) </sub>output from the encoder <b>21</b><i>a</i>. After the noise &#x3b5; generated by the noise generation unit <b>21</b><i>b </i>is mixed into the standard deviation &#x3c3;<sub>(x)</sub>, the decoder <b>21</b><i>c</i>-<b>2</b> generates the reconstructed data obtained by decoding the input data, using the standard deviation &#x3c3;<sub>(x) </sub>including the noise &#x3b5; and the mean &#x3bc;<sub>(x)</sub>.</p><p id="p-0079" num="0069">Thereafter, the estimation unit <b>21</b><i>d </i>estimates a normalization error R between a probability distribution of training data (x) and the probability distribution of the latent variable z using the mean &#x3bc;<sub>(x) </sub>and the standard deviation &#x3c3;<sub>(x) </sub>output from the encoder <b>21</b><i>a</i>. Then, the optimization unit <b>21</b><i>e </i>adjusts (machine learning) each parameter of the encoder <b>21</b><i>a </i>and each decoder so as to minimize the normalization error R estimated by the estimation unit <b>21</b><i>d </i>and to minimize a reconstruction error D<b>1</b> that is an error between the reconstructed data generated by the decoder <b>21</b><i>c</i>-<b>1</b> and the input data and a reconstruction error D<b>2</b> that is an error between the reconstructed data generated by the decoder <b>21</b><i>c</i>-<b>1</b> and the reconstructed data generated by the decoder <b>21</b><i>c</i>-<b>2</b>.</p><p id="p-0080" num="0070">Note that, after machine learning is completed, with a method as in the first embodiment, the probability distribution of the input data can be estimated using the trained VAE.</p><p id="p-0081" num="0071">Pieces of information including a processing procedure, a control procedure, a specific name, various types of data, and parameters described above or illustrated in the drawings may be optionally changed unless otherwise specified.</p><p id="p-0082" num="0072">Furthermore, each component of each device illustrated in the drawings is functionally conceptual, and does not necessarily have to be physically configured as illustrated in the drawings. In other words, specific forms of distribution and integration of individual devices are not limited to those illustrated in the drawings. That is, all or a part thereof may be configured by being functionally or physically distributed or integrated in optional units according to various types of loads, usage situations, or the like.</p><p id="p-0083" num="0073">Moreover, all or an optional part of individual processing functions performed in each device may be realized by a CPU and a program analyzed and executed by the CPU, or may be realized as hardware by wired logic.</p><p id="p-0084" num="0074">Next, a hardware configuration example of the information processing apparatus <b>10</b> will be described. <figref idref="DRAWINGS">FIG. <b>12</b></figref> is a diagram for explaining a hardware configuration example. As illustrated in <figref idref="DRAWINGS">FIG. <b>12</b></figref>, the information processing apparatus <b>10</b> includes a communication device <b>10</b><i>a</i>, a display device <b>10</b><i>b</i>, a hard disk drive (HDD) <b>10</b><i>c</i>, a memory <b>10</b><i>d</i>, and a processor <b>10</b><i>e</i>. Furthermore, each of the units illustrated in <figref idref="DRAWINGS">FIG. <b>12</b></figref> is mutually connected by a bus or the like.</p><p id="p-0085" num="0075">The communication device <b>10</b><i>a </i>is a network interface card or the like, and communicates with another server. The display device <b>10</b><i>b </i>is a device that displays a training result, a detection result, or the like and is, for example, a touch panel, a display, or the like. The HDD <b>10</b><i>c </i>stores programs that operate the functions illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref> and DBs.</p><p id="p-0086" num="0076">The processor <b>10</b><i>e </i>reads a program that executes processing similar to that of each processing unit illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref> from the HDD <b>10</b><i>c </i>or the like, and develops the read program in the memory <b>10</b><i>d</i>, thereby activating a process that executes each function described with reference to <figref idref="DRAWINGS">FIG. <b>2</b></figref> or the like. For example, this process executes a function similar to that of each processing unit included in the information processing apparatus <b>10</b>. Specifically, the processor <b>10</b><i>e </i>reads programs having functions similar to the training unit <b>21</b>, the detection unit <b>22</b>, or the like from the HDD <b>10</b><i>c </i>or the like. Then, the processor <b>10</b><i>e </i>executes a process for executing processing similar to the training unit <b>21</b>, the detection unit <b>22</b>, or the like.</p><p id="p-0087" num="0077">In this way, the information processing apparatus <b>10</b> operates as an information processing apparatus that executes an estimation method by reading and executing programs. Furthermore, the information processing apparatus <b>10</b> may realize functions similar to the functions of the embodiment described above by reading the program described above from a recording medium by a medium reading device and executing the read program described above. Note that the program referred to in another embodiment is not limited to being executed by the information processing apparatus <b>10</b>. For example, the present invention may be similarly applied to a case where another computer or server executes a program, or a case where such a computer and server cooperatively execute a program.</p><p id="p-0088" num="0078">All examples and conditional language provided herein are intended for the pedagogical purposes of aiding the reader in understanding the invention and the concepts contributed by the inventor to further the art, and are not to be construed as limitations to such specifically recited examples and conditions, nor does the organization of such examples in the specification relate to a showing of the superiority and inferiority of the invention. Although one or more embodiments of the present invention have been described in detail, it should be understood that the various changes, substitutions, and alterations could be made hereto without departing from the spirit and scope of the invention.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001 MATH-US-00001-2" nb-file="US20230004779A1-20230105-M00001.NB"><img id="EMI-M00001" he="10.92mm" wi="76.20mm" file="US20230004779A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002 MATH-US-00002-2" nb-file="US20230004779A1-20230105-M00002.NB"><img id="EMI-M00002" he="27.86mm" wi="76.20mm" file="US20230004779A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00003 MATH-US-00003-2 MATH-US-00003-3 MATH-US-00003-4" nb-file="US20230004779A1-20230105-M00003.NB"><img id="EMI-M00003" he="26.42mm" wi="76.20mm" file="US20230004779A1-20230105-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00004 MATH-US-00004-2 MATH-US-00004-3 MATH-US-00004-4" nb-file="US20230004779A1-20230105-M00004.NB"><img id="EMI-M00004" he="20.83mm" wi="76.20mm" file="US20230004779A1-20230105-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A non-transitory computer-readable storage medium storing an estimation program that causes at least one computer to execute a process, the process comprising:<claim-text>inputting an input data into a trained variational autoencoder that includes an encoder and a decoder;</claim-text><claim-text>converting, into a first probability distribution, a probability distribution of a latent variable that is generated by the trained variational autoencoder according to the input based on a magnitude of a standard deviation output from the encoder;</claim-text><claim-text>converting the first probability distribution into a second probability distribution based on an output error of the decoder regarding the input data; and</claim-text><claim-text>outputting the second probability distribution as an estimated value of a probability distribution of the input data.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The non-transitory computer-readable storage medium according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the converting the probability distribution of the latent variable into the first probability distribution includes converting the probability distribution of the latent variable into the first probability distribution according to conversion processing from the latent variable into principal component coordinates, based on the magnitude of the standard deviation.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The non-transitory computer-readable storage medium according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the conversion processing includes:<claim-text>acquiring the standard deviation and a mean that are distribution parameters of the probability distribution of the latent variable from the encoder,</claim-text><claim-text>acquiring a change rate of a scale between the principal component coordinates and the latent variable, by using the magnitude of the standard deviation, and</claim-text><claim-text>converting the latent variable into the principal component coordinates by using the standard deviation, the mean, and the change rate.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The non-transitory computer-readable storage medium according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein converting the first probability distribution into the second probability distribution includes:<claim-text>setting a probability distribution other than a principal component in the first probability distribution as a constant; and</claim-text><claim-text>converting the first probability distribution into the second probability distribution by using the output error according to a normal distribution to which the scale of the input data is set.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The non-transitory computer-readable storage medium according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, the process further comprising<claim-text>detecting data, with a lower probability, that occupies a certain ratio as anomaly data based on the second probability distribution.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The non-transitory computer-readable storage medium according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, the process further comprising<claim-text>detecting data with a probability equal to or less than a threshold as anomaly data based on the second probability distribution.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. An estimation method for a computer to execute a process comprising:<claim-text>inputting an input data into a trained variational autoencoder that includes an encoder and a decoder;</claim-text><claim-text>converting, into a first probability distribution, a probability distribution of a latent variable that is generated by the trained variational autoencoder according to the input based on a magnitude of a standard deviation output from the encoder;</claim-text><claim-text>converting the first probability distribution into a second probability distribution based on an output error of the decoder regarding the input data; and</claim-text><claim-text>outputting the second probability distribution as an estimated value of a probability distribution of the input data.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The estimation method according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the converting the probability distribution of the latent variable into the first probability distribution includes converting the probability distribution of the latent variable into the first probability distribution according to conversion processing from the latent variable into principal component coordinates, based on the magnitude of the standard deviation.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The estimation method according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the conversion processing includes:<claim-text>acquiring the standard deviation and a mean that are distribution parameters of the probability distribution of the latent variable from the encoder,</claim-text><claim-text>acquiring a change rate of a scale between the principal component coordinates and the latent variable, by using the magnitude of the standard deviation, and</claim-text><claim-text>converting the latent variable into the principal component coordinates by using the standard deviation, the mean, and the change rate.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The estimation method according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein converting the first probability distribution into the second probability distribution includes:<claim-text>setting a probability distribution other than a principal component in the first probability distribution as a constant; and</claim-text><claim-text>converting the first probability distribution into the second probability distribution by using the output error according to a normal distribution to which the scale of the input data is set.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. An information processing apparatus comprising:<claim-text>one or more memories; and</claim-text><claim-text>one or more processors coupled to the one or more memories and the one or more processors configured to:</claim-text><claim-text>input an input data into a trained variational autoencoder that includes an encoder and a decoder,</claim-text><claim-text>convert, into a first probability distribution, a probability distribution of a latent variable that is generated by the trained variational autoencoder according to the input based on a magnitude of a standard deviation output from the encoder,</claim-text><claim-text>convert the first probability distribution into a second probability distribution based on an output error of the decoder regarding the input data, and</claim-text><claim-text>output the second probability distribution as an estimated value of a probability distribution of the input data.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The information processing apparatus according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the one or more processors are further configured to<claim-text>convert the probability distribution of the latent variable into the first probability distribution according to conversion processing from the latent variable into principal component coordinates, based on the magnitude of the standard deviation.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The information processing apparatus according to <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the conversion processing includes:<claim-text>acquiring the standard deviation and a mean that are distribution parameters of the probability distribution of the latent variable from the encoder,</claim-text><claim-text>acquiring a change rate of a scale between the principal component coordinates and the latent variable, by using the magnitude of the standard deviation, and</claim-text><claim-text>converting the latent variable into the principal component coordinates by using the standard deviation, the mean, and the change rate.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The information processing apparatus according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the one or more processors are further configured to:<claim-text>set a probability distribution other than a principal component in the first probability distribution as a constant, and</claim-text><claim-text>convert the first probability distribution into the second probability distribution by using the output error according to a normal distribution to which the scale of the input data is set.</claim-text></claim-text></claim></claims></us-patent-application>