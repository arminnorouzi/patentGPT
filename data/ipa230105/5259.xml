<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005260A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005260</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17780029</doc-number><date>20200915</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>CN</country><doc-number>202010922400.3</doc-number><date>20200904</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>10</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>56</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>22</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>188</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>56</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>23</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">METHOD FOR DETECTING FIELD NAVIGATION LINE AFTER RIDGE SEALING OF CROPS</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>ZHEJIANG UNIVERSITY</orgname><address><city>ZHEJIANG</city><country>CN</country></address></addressbook><residence><country>CN</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>RAO</last-name><first-name>Xiuqin</first-name><address><city>Zhejiang</city><country>CN</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>LIN</last-name><first-name>Yangyang</first-name><address><city>Zhejiang</city><country>CN</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>ZHANG</last-name><first-name>Yanning</first-name><address><city>Zhejiang</city><country>CN</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>ZHANG</last-name><first-name>Xiaomin</first-name><address><city>Zhejiang</city><country>CN</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>YING</last-name><first-name>Yibin</first-name><address><city>Zhejiang</city><country>CN</country></address></addressbook></inventor><inventor sequence="05" designation="us-only"><addressbook><last-name>YANG</last-name><first-name>Haitao</first-name><address><city>Zhejiang</city><country>CN</country></address></addressbook></inventor><inventor sequence="06" designation="us-only"><addressbook><last-name>JIANG</last-name><first-name>Haiyi</first-name><address><city>Zhejiang</city><country>CN</country></address></addressbook></inventor><inventor sequence="07" designation="us-only"><addressbook><last-name>ZHU</last-name><first-name>Yihang</first-name><address><city>Zhejiang</city><country>CN</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>ZHEJIANG UNIVERSITY</orgname><role>03</role><address><city>ZHEJIANG</city><country>CN</country></address></addressbook></assignee></assignees><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/CN2020/115253</doc-number><date>20200915</date></document-id><us-371c12-date><date>20220526</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A method for detecting a field navigation line after ridge sealing of crops includes the following steps. A field crop image is acquired. Image color space transformation, image binaryzation, longitudinal integration, neighborhood setting and region integration calculation are sequentially performed on the field crop image to obtain a crop row image. Detections of an initial middle ridge, a left ridge and a right ridge are performed on the crop row image to obtain center lines of the initial middle ridge, left ridge and right ridge. Center lines of a left (right) crop row are established by using an area 1 between the center lines of the left (right) ridge and the initial middle ridge. A center line model of a middle ridge is established by using an area 0 between the center lines of the left and right crop rows, namely a navigation line of a field operation machine.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="81.70mm" wi="148.08mm" file="US20230005260A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="219.88mm" wi="150.11mm" file="US20230005260A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="211.50mm" wi="150.11mm" file="US20230005260A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="196.17mm" wi="137.41mm" orientation="landscape" file="US20230005260A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="228.09mm" wi="169.93mm" file="US20230005260A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="226.14mm" wi="169.93mm" file="US20230005260A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="117.94mm" wi="172.97mm" file="US20230005260A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNOLOGY FIELD</heading><p id="p-0002" num="0001">The invention relates to a method for automatic field navigation of agricultural machinery, and particularly to a method for detecting a field navigation line after a ridge sealing of crops.</p><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">The automatic operation of intelligent agricultural machinery in the field requires navigation control. As a general navigation technology, the satellite positioning system is applicable to the situation where a walking path is determined. However, in a field, due to factors such as changes in crop types and crop growth conditions, it is difficult to maintain a stable advance path in the field, and it needs to be adjusted according to the actual situation. Therefore, it is particularly important to identify field crops and provide navigation information for intelligent operating machines.</p><p id="p-0004" num="0003">To implement the acquisition of field navigation information, scholars have carried out a lot of research.</p><p id="p-0005" num="0004">Hough transform method: Jiang et al. (2016) used the over-green <b>2</b>G-R-B feature combined with the Otsu threshold segmentation method and the moving window method to extract feature points of candidate crop rows. For the candidate straight lines extracted by Hough transform method, after processing based on a vanishing point and K-means clustering method, the real crop rows are obtained (Jiang G, Wang X, Wang Z, et al. wheat rows detection at the early growth stage based on Hough transform and vanishing point[J]. Computers &#x26; Electronics in Agriculture, 2016, 123:211-223).</p><p id="p-0006" num="0005">Template matching method: Hague et al. (2001) matched wheat rows with a bandpass filter, which can effectively solve the image shadow problem. One function of the bandpass filter is to block some high-frequency signals to attenuate the effect of spurious features such as weeds and internal structural details of crop rows, and the second function is to block some low-frequency signals to suppress the effect of light changes. However, the adaptability of the method to different natural conditions needs to be further improved. (Hague T, Tillett ND. A bandpass filter-based approach to crop row location and tracking[J]. Mechatronics, 2001, 11(1):1-12). Zhang Fangming (2006) proposed an algorithm for locating crop rows by a trapezoidal model. Based on the grayscale features of the line scan lines of the image, a grayscale curve model representing crop characteristics was constructed, and the wavelet analysis method was used to extract the trend curve. Fast algorithms for target features from rough positioning to precise positioning are constructed. This rough-to-precise strategy based on wavelet decomposition can ensure the real-time performance of image processing algorithms, reliable detection, and fast calculation speed. However, when the vehicle deviates from the road greatly, if a certain row moves out of the field of view, a matching failure may occur accordingly. (Zhang Fangming. Research on field path recognition algorithm and vehicle autonomous navigation method based on stereo vision [D]. Zhejiang University, 2006).</p><p id="p-0007" num="0006">Linear regression method: Montalvo et al. (2012) proposed the &#x201c;Otsu and linear regression (OLR) method&#x201d;, i.e., crop rows are detected by least squares method (Montalvo M, Pajares G, Guerrero J M, et al. Automatic detection of crop rows in maize fields with high weeds pressure[J]. Expert Systems with Applications, An International Journal, 2012, 39(15):11889-11897). Garc&#xed;a-Santill&#xe1;n et al. (2017) proposed the detection based on micro-ROIs (DBMR) method, and based on multiple regions of interest, the Hough transform and the least squares method are combined. The Hough transform is used to locate the starting point of each crop row, then the ROI is divided into multiple horizontal bars, the candidate points are extracted each time by using the micro-ROI, and finally the least squares method is used to fit the crop row straight line (Garc&#xed;a-Santill&#xe1;n, Iv&#xe1;n D, Montalvo, Mart&#xed;n, Guerrero, Jos&#xe9; M, et al. Automatic detection of curved and straight crop rows from images in maize fields[J]. Biosystems Engineering, 2017, 156:61-79).</p><p id="p-0008" num="0007">The foregoing methods mostly extract navigation information based on the larger spacing among crops. However, in the late stage of growth of crops such as corns, cotton, and sugarcanes, the branches and leaves of two adjacent rows of the crops are overlapped with each other, that is, the rows of crops are closed (closed rows or closed ridges), and these methods are no longer applicable.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0009" num="0008">To meet the requirements and solve the problems in the technology of the related art, the invention proposes a method for extracting navigation information by using regional integral difference, so as to implement the detection between the rows of closed crops.</p><p id="p-0010" num="0009">The technical scheme of the invention is as follows.</p><p id="p-0011" num="0010">The method includes the following steps:</p><p id="p-0012" num="0011">1) Crop image acquisition: a camera is used to acquire a field crop image, recorded as an original image S<b>1</b>.</p><p id="p-0013" num="0012">The optical axis of the camera takes pictures in the direction of the field ridge.</p><p id="p-0014" num="0013">2) Image color space transformation: the original image S<b>1</b> is converted to HSI color space to obtain an HSI image S<b>2</b>.</p><p id="p-0015" num="0014">3) Image binarization: the pixel value of the pixel whose hue component value H is between 0.2 and 0.583 in the HSI image S<b>2</b> is set to be 1, and the pixel values of the remaining pixels are set to be 0 to obtain a binary image S<b>3</b>.</p><p id="p-0016" num="0015">4) Longitudinal integration: the binary image S<b>3</b> is duplicated as a longitudinal integral image S<b>4</b>, and each column on the longitudinal integral image S<b>4</b> is traversed. In each column, each pixel is traversed downward from the pixel of the second row. When traversing, the pixel values of the pixels of the previous row are added, and the result is covered with the pixel value of the current pixel, so as to obtain the longitudinal integral image S<b>4</b>.</p><p id="p-0017" num="0016">5) Neighborhood setting: the neighborhood of the current pixel is set, and the neighborhood is 1/48 of the image width of the original image S<b>1</b>. A 3-row two-dimensional matrix R is used to represent the neighborhood, and a column in the two-dimensional matrix R represents a column in the neighborhood. Each element of the first row in the two-dimensional matrix R represents the column offset of each column in the neighborhood relative to the current pixel, each element of the second row represents the abscissa offset of the start row of each column in the neighborhood, and each element of the third row represents the abscissa offset of the end row of each column in the neighborhood.</p><p id="p-0018" num="0017">6) Region integration calculation: a blank image having a size the same as that of the longitudinal integral image S<b>4</b> is constructed as a region integration image S<b>5</b>, and each pixel is traversed on the longitudinal integral image S<b>4</b>, which is processed in the way as follows. When traversing, the current pixel coordinates are marked as (x, y), an accumulator C is set, and the initial value of the accumulator C is set to be 0. When traversing each column of the two-dimensional matrix R and traversing the two-dimensional matrix R, the elements of the first row to the third row of the current j-th column are R<sub>1j</sub>, R<sub>2j</sub>, and R<sub>3j</sub>, the difference is obtained by subtracting the pixel value of the pixel with coordinates (x+R<sub>3j</sub>&#x2212;1, y+R<sub>1j</sub>) from the pixel value of the pixel with coordinates (x+R<sub>3j</sub>, y+R<sub>1j</sub>) on the longitudinal integral image S<b>4</b>, and the difference is accumulated into the accumulator C. After traversing the two-dimensional matrix R is completed, the value in the accumulator C is taken as the regional integral value M of the current pixel, and the regional integral value M is assigned to the pixel having coordinates the same as those of the current pixel in the region integration image S<b>5</b>.</p><p id="p-0019" num="0018">7) Detections of crop rows: each row is traversed in the region integration image S<b>5</b>, the average value of the regional integral value M of all pixels in each row is calculated, the pixel whose regional integral value M is greater than the average value is set to be 1, the remaining pixels are set to be 0, and a crop row image S<b>6</b> is obtained.</p><p id="p-0020" num="0019">8) Detections of the initial middle ridge, the left ridge, and the right ridge:</p><p id="p-0021" num="0020">8.1) The crop row image S<b>6</b> is divided into N crop row sub-images S<b>7</b> having a width the same as the width of the crop row image S<b>6</b> and a height 1/N of the height of the crop row image S<b>6</b>.</p><p id="p-0022" num="0021">8.2) The i-th crop row sub-image S<b>7</b> is taken, and a longitudinal projection vector S<b>8</b> of the i-th crop row sub-image S<b>7</b> is calculated.</p><p id="p-0023" num="0022">8.3) Detection of the left boundary of the initial middle ridge: an initial middle ridge start detection template ML<b>0</b> is constructed, and the initial middle ridge start detection template ML<b>0</b> is a vector whose length is &#x2159; of the width of the original image S<b>1</b>, the first half is 1, and the second half is &#x2212;1. The longitudinal projection vector S<b>8</b> is convolved with the initial middle ridge start detection template ML<b>0</b>, and the column number of the position of the point with the maximum convolution value is taken as an initial middle ridge left boundary p0L0<sub>i </sub>of the i-th crop row sub-image S<b>7</b>.</p><p id="p-0024" num="0023">8.4) Detection of the right boundary of the initial middle ridge: an initial middle ridge termination detection template MR<b>0</b> is constructed, and the initial middle ridge termination detection template MR<b>0</b> is a vector whose length is &#x2159; of the width of the original image S<b>1</b>, the first half is &#x2212;1, and the second half is 1. The longitudinal projection vector S<b>8</b> is convolved with the initial middle ridge termination detection template MR<b>0</b>, and the column number of the position of the point with the maximum convolution value is taken as an initial middle ridge right boundary p0R0<sub>i </sub>of the i-th crop row sub-image S<b>7</b>.</p><p id="p-0025" num="0024">8.5) An initial middle ridge center p0M0<sub>i </sub>of the i-th crop row sub-image S<b>7</b> is calculated by the following formula: p0M0<sub>i</sub>=(p0L0<sub>i</sub>+p0R0<sub>i</sub>)/2.</p><p id="p-0026" num="0025">8.6) Detection of the left boundary of the initial left row: an initial left row start detection template MR<b>1</b> is constructed, and the initial left row start detection template MR<b>1</b> is a vector whose length is &#xbd; of the length of the initial middle ridge termination detection template MR<b>0</b>, the first half is &#x2212;1, and the second half is 1. The initial left row start detection template MR<b>1</b> is used to be convolved with the data of the longitudinal projection vector S<b>8</b> on the left side of the initial middle ridge left boundary p0L0<sub>i</sub>. The column number of the position of the point with the maximum convolution value is taken as an initial left row left boundary CL0<sub>i </sub>of the i-th crop row sub-image S<b>7</b>.</p><p id="p-0027" num="0026">8.7) Detection of the right boundary of the initial right row: an initial right row termination detection template ML<b>1</b> is constructed, and the initial right row termination detection template ML<b>1</b> is a vector whose length is &#xbd; of the length of the initial middle ridge start detection template ML<b>0</b>, the first half is 1, and the second half is &#x2212;1. The initial right row termination detection template ML<b>1</b> is used to be convolved with the data of the longitudinal projection vector S<b>8</b> on the right side of the initial middle ridge right boundary p0R0<sub>i</sub>. The column number of the position of the point with the maximum convolution value is taken as an initial right row right boundary CR0<sub>i </sub>of the i-th crop row sub-image S<b>7</b>.</p><p id="p-0028" num="0027">8.8) Estimation of the center point of the left ridge: an initial left row horizontal center column CLM0<sub>i </sub>of the i-th crop row sub-image S<b>7</b> is calculated by the following formula: CLM0<sub>i</sub>=(CL0<sub>i</sub>+p0L0<sub>i</sub>)/2. Then, a column pLM0<sub>i </sub>where the center point of the left ridge of the i-th crop row sub-image S<b>7</b> is located is calculated by the following formula: pLM0<sub>i</sub>=2&#xd7;CLM0<sub>i</sub>&#x2212;p0M0<sub>i</sub>.</p><p id="p-0029" num="0028">8.9) Estimation of the center point of the right ridge: an initial right row horizontal center column CRM0<sub>i </sub>of the i-th crop row sub-image S<b>7</b> is calculated by the following formula: CRM0<sub>i</sub>=(CR0<sub>i</sub>+p0R0<sub>i</sub>)/2. Then, a column pRM0<sub>i </sub>where the center point of the right ridge of the i-th crop row sub-image S<b>7</b> is located is calculated by the following formula: pRM0<sub>i</sub>=2&#xd7;CRM0<sub>i</sub>&#x2212;p0M0<sub>i</sub>.</p><p id="p-0030" num="0029">8.10) Calculation of the ordinate of the crop row sub-image S<b>7</b>: the ordinate of the position of the center point of the crop row sub-image S<b>7</b> on the crop row image S<b>6</b> is taken as an ordinate S<b>7</b><i>y</i><sub>i </sub>of the crop row sub-image S<b>7</b>.</p><p id="p-0031" num="0030">8.11) Determining the center lines of the initial middle ridge, the left ridge, and the right ridge.</p><p id="p-0032" num="0031">Step 8.2) to step 8.11) are repeated, the N crop row sub-images S<b>7</b> of the crop row image S<b>6</b> are sequentially traversed. Each crop row sub-image S<b>7</b> obtains an initial middle ridge center p0M0<sub>i</sub>, an initial left row horizontal center column CLM0<sub>i</sub>, an initial right row horizontal center column CRM0<sub>i</sub>, and an ordinate S<b>7</b><i>y</i><sub>i </sub>of the crop row sub-image S<b>7</b>, and therefore the results of all N crop row sub-images S<b>7</b> are composed to obtain a set of an initial middle ridge center set p0M0, an initial left row horizontal center column set CLM0, an initial right row horizontal center column set CRM0, and an ordinate set S<b>7</b><i>y </i>of the crop row sub-image S<b>7</b>.</p><p id="p-0033" num="0032">The ordinate S<b>7</b><i>y</i><sub>i </sub>of the crop row sub-image S<b>7</b> serves as an independent variable, the initial middle ridge center p0M0<sub>i</sub>, the initial left row horizontal center column CLM0<sub>i</sub>, and the initial right row horizontal center column CRM0<sub>i </sub>serve as dependent variables, respectively, and the univariate regression models pM, pL and pR are constructed between the initial middle ridge center p0M0<sub>i </sub>and the ordinate S<b>7</b><i>y</i><sub>i </sub>of the crop row sub-image S<b>7</b>, between the initial left row horizontal center column CLM0<sub>i </sub>and the ordinate S<b>7</b><i>y</i><sub>i </sub>of the crop row sub-image S<b>7</b>, and between the initial right row horizontal center column CRM0<sub>i </sub>and the ordinate S<b>7</b><i>y</i><sub>i </sub>of the crop row sub-image S<b>7</b>, respectively. The univariate regression models pM, pL, and pR are actually a fitted straight line.</p><p id="p-0034" num="0033">9) Detections of the left crop row and the right crop row:</p><p id="p-0035" num="0034">9.1) A blank right crop row point set SCR and a blank left crop row point set SCL are constructed.</p><p id="p-0036" num="0035">9.2) The k-th row is taken on the crop row image S<b>6</b> as a row image S<b>9</b>, the ordinate of the row image S<b>9</b> as an independent variable is substituted into the univariate regression models pM, pL, and pR to obtain a crop middle ridge center column p0M1<sub>k</sub>, a crop left ridge horizontal center column CLM1<sub>k</sub>, and a crop left ridge horizontal center column CRM1<sub>k </sub>on the current row image S<b>9</b>.</p><p id="p-0037" num="0036">9.3) The blank left crop row point set SCL is constructed. On the current row image S<b>9</b>, the coordinates of the pixel with a pixel value of 1 between the crop middle ridge center column p0M1<sub>k </sub>and the crop left ridge horizontal center column CLM1<sub>k </sub>corresponding to the crop row image S<b>6</b> is added to the left crop row point set SCL.</p><p id="p-0038" num="0037">9.4) The blank right crop row point set SCR is constructed. On the current row image S<b>9</b>, the coordinates of the pixel with a pixel value of 1 between the crop middle ridge center column p0M1<sub>k </sub>and the crop left ridge horizontal center column CRM1<sub>k </sub>corresponding to the crop row image S<b>6</b> is added to the right crop row point set SCR.</p><p id="p-0039" num="0038">9.5) Step 9.2) to step 9.4) are repeated. Each row of the crop row image S<b>6</b> is traversed to obtain the complete left crop row point set SCL and the right crop row point set SCR.</p><p id="p-0040" num="0039">9.6) The ordinates of the pixels in the left crop row point set SCL serve as independent variables, the abscissas of the pixels in the left crop row point set SCL serve as dependent variables, and a univariate regression model for the left crop row point set SCL is constructed, and a left crop row centerline model CL is obtained.</p><p id="p-0041" num="0040">9.7) The ordinates of the pixels in the right crop row point set SCR serve as independent variables, the abscissas of the pixels in the right crop row point set SCR serve as dependent variables, and a univariate regression model for the right crop row point set SCR is constructed, and a right crop row centerline model CR is obtained.</p><p id="p-0042" num="0041">The left crop row centerline model CL and the right crop row centerline model CR are actually a fitted straight line.</p><p id="p-0043" num="0042">10) Detection of the middle ridge:</p><p id="p-0044" num="0043">10.1) A blank middle ridge point set Spath is constructed.</p><p id="p-0045" num="0044">10.2) The q-th row on the crop row image S<b>6</b> is taken as a row image S<b>10</b>, and the ordinate of the line image S<b>10</b> as an independent variable is substituted into the left crop row centerline model CL and the right crop row centerline model CR to obtain a left row center point CL1<sub>q </sub>and a right row center point CR1<sub>q </sub>on the current line image S<b>10</b>.</p><p id="p-0046" num="0045">10.3) On the current line image S<b>10</b>, the coordinates of the pixel with a pixel value of 0 between the left row center point CL1<sub>q </sub>and the right row center point CR1<sub>q </sub>corresponding to the crop row image S<b>6</b> is added into the middle ridge point set SPath.</p><p id="p-0047" num="0046">10.4) Step 10.2) to step 10.3) are repeated. Each row image S<b>10</b> of the crop row image S<b>6</b> is traversed to obtain the complete middle ridge point set Spath.</p><p id="p-0048" num="0047">10.5) The ordinates of the pixels in the middle ridge point set SPath serve as independent variables, and the abscissas of the pixels in the middle ridge point set SPath serve as dependent variables. A univariate regression model is constructed for the middle ridge point set SPath, and a middle ridge centerline model pPath is obtained. The straight line where the middle ridge centerline model pPath is located is the navigation line for the field machinery.</p><heading id="h-0004" level="1">Beneficial Effects of the Invention</heading><p id="p-0049" num="0048">The invention utilizes the difference of crop region integration to obtain an initial crop row. By constructing a regression model, an initial ridge is determined to be left and right crop rows, and then the left and right crop rows are used to construct the middle ridge centerline model, which overcomes the defect of which the previous methods cannot be applied to the extraction of navigation information of closed crops, implements the acquisition of navigation information between closed crop rows, and improves the adaptability of field machinery.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWING</heading><p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic view of an original image S<b>1</b> of an embodiment.</p><p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a schematic view of an HSI image S<b>2</b> of an embodiment.</p><p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a schematic view of a binary image S<b>3</b> of an embodiment.</p><p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a schematic view of a longitudinal integral image S<b>4</b> of the embodiment.</p><p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a schematic view of a neighborhood of an embodiment.</p><p id="p-0055" num="0054"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a schematic view of a crop row image S<b>6</b> of the embodiment.</p><p id="p-0056" num="0055"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a schematic view of an initial middle ridge start detection template ML of an embodiment.</p><p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a schematic view of an initial middle ridge termination detection template MR of an embodiment.</p><p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a schematic view of a left ridge centerline, an initial middle ridge centerline, and a right ridge centerline of the embodiment.</p><p id="p-0059" num="0058"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a schematic view of a left crop row centerline, a middle row centerline, and a right crop row centerline of the embodiment.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DESCRIPTION OF THE EMBODIMENTS</heading><p id="p-0060" num="0059">The invention is further illustrated with reference to the accompanying drawings and embodiments in the subsequent paragraphs.</p><p id="p-0061" num="0060">The invention includes steps as follows.</p><p id="p-0062" num="0061">1) Crop image acquisition: a camera is used to acquire a field crop image, recorded as an original image S<b>1</b>, as shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0063" num="0062">The optical axis of the camera takes pictures in the direction of the field ridge.</p><p id="p-0064" num="0063">2) Image color space transformation: the original image S<b>1</b> is converted to HSI color space to obtain an HSI image S<b>2</b>, as shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0065" num="0064">3) Image binarization: the pixel value of the pixel whose hue component value H is between 0.2 and 0.583 in the HSI image S<b>2</b> is set to be 1, and the pixel values of the remaining pixels are set to be 0 to obtain a binary image S<b>3</b>, as shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0066" num="0065">4) Longitudinal integration:</p><p id="p-0067" num="0066">The binary image S<b>3</b> is duplicated as a longitudinal integral image S<b>4</b>, and each column on the longitudinal integral image S<b>4</b> is traversed. In each column, each pixel is traversed downward from the pixel of the second row. When traversing, the pixel values of the pixels of the previous row are added, and the result is covered with the pixel value of the current pixel, so as to obtain the longitudinal integral image S<b>4</b>, as shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0068" num="0067">5) Neighborhood setting:</p><p id="p-0069" num="0068">The neighborhood of the current pixel is set, and the neighborhood is 1/48 of the image width of the original image S<b>1</b>, as shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>. A 3-row two-dimensional matrix R is used to represent the neighborhood, and a column in the two-dimensional matrix R represents a column in the neighborhood. Each element of the first row in the two-dimensional matrix R represents the column offset of each column in the neighborhood relative to the current pixel, each element of the second row represents the abscissa offset of the start row of each column in the neighborhood, and each element of the third row represents the abscissa offset of the end row of each column in the neighborhood.</p><p id="p-0070" num="0069">6) Region integration calculation:</p><p id="p-0071" num="0070">A blank image having a size the same as that of the longitudinal integral image S<b>4</b> is constructed as a region integration image S<b>5</b>, and each pixel is traversed on the longitudinal integral image S<b>4</b>, which is processed in the way as follows. When traversing, the current pixel coordinates are marked as (x, y), an accumulator C is set, and the initial value of the accumulator C is set to be 0. When traversing each column of the two-dimensional matrix R and traversing the two-dimensional matrix R, the elements of the first row to the third row of the current j-th column are R<sub>1j</sub>, R<sub>2j</sub>, and R<sub>3j</sub>, the difference is obtained by subtracting the pixel value of the pixel with coordinates (x+R<sub>3j</sub>&#x2212;1, y+R<sub>1j</sub>) from the pixel value of the pixel with coordinates (x+R<sub>3j</sub>, y+R<sub>1j</sub>) on the longitudinal integral image S<b>4</b>, and the difference is accumulated into the accumulator C. After traversing the two-dimensional matrix R is completed, the value in the accumulator C is taken as the regional integral value M of the current pixel, and the regional integral value M is assigned to the pixel having coordinates the same as those of the current pixel in the region integration image S<b>5</b>.</p><p id="p-0072" num="0071">After traversing each pixel of the longitudinal integral image S<b>4</b> is completed, the region integration image S<b>5</b> is obtained, as shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0073" num="0072">7) Detections of crop rows: Each row is traversed in the region integration image S<b>5</b>, the average value of the regional integral value M of all pixels in each row is calculated, the pixel whose regional integral value M is greater than the average value is set to be 1, the remaining pixels are set to be 0, and a crop row image S<b>6</b> is obtained, as shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>.</p><p id="p-0074" num="0073">8) Detections of the initial middle ridge, the left ridge, and the right ridge Step 8) is as follows specifically.</p><p id="p-0075" num="0074">8.1) The crop row image S<b>6</b> is divided into N crop row sub-images S<b>7</b> having a width the same as the width of the crop row image S<b>6</b> and a height 1/N of the height of the crop row image S<b>6</b>.</p><p id="p-0076" num="0075">8.2) The i-th crop row sub-image S<b>7</b> is taken, and a longitudinal projection vector S<b>8</b> of the i-th crop row sub-image S<b>7</b> is calculated.</p><p id="p-0077" num="0076">8.3) Detection of the left boundary of the initial middle ridge: an initial middle ridge start detection template ML<b>0</b> is constructed, and the initial middle ridge start detection template ML<b>0</b> is a vector whose length is &#x2159; of the width of the original image S<b>1</b>, the first half is 1, and the second half is &#x2212;1, as shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>. The longitudinal projection vector S<b>8</b> is convolved with the initial middle ridge start detection template ML<b>0</b>, and the column number of the position of the point with the maximum convolution value is taken as an initial middle ridge left boundary p0L0<sub>i </sub>of the i-th crop row sub-image S<b>7</b>.</p><p id="p-0078" num="0077">8.4) Detection of the right boundary of the initial middle ridge: an initial middle ridge termination detection template MR<b>0</b> is constructed, and the initial middle ridge termination detection template MR<b>0</b> is a vector whose length is &#x2159; of the width of the original image S<b>1</b>, the first half is &#x2212;1, and the second half is 1, as shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>. The longitudinal projection vector S<b>8</b> is convolved with the initial middle ridge termination detection template MR<b>0</b>, and the column number of the position of the point with the maximum convolution value is taken as an initial middle ridge right boundary p0R0<sub>i </sub>of the i-th crop row sub-image S<b>7</b>.</p><p id="p-0079" num="0078">8.5) An initial middle ridge center p0M0<sub>i </sub>of the i-th crop row sub-image S<b>7</b> is calculated by the following formula: p0M0<sub>i</sub>=(p0L0<sub>i</sub>+p0R0<sub>i</sub>)/2.</p><p id="p-0080" num="0079">8.6) Detection of the left boundary of the initial left row: an initial left row start detection template MR<b>1</b> is constructed, and the initial left row start detection template MR<b>1</b> is a vector whose length is &#xbd; of the length of the initial middle ridge termination detection template MR<b>0</b>, the first half is &#x2212;1, and the second half is 1. The initial left row start detection template MR<b>1</b> is used to be convolved with the data of the longitudinal projection vector S<b>8</b> on the left side of the initial middle ridge left boundary p0L0<sub>i</sub>. The column number of the position of the point with the maximum convolution value is taken as an initial left row left boundary CL0<sub>i </sub>of the i-th crop row sub-image S<b>7</b>.</p><p id="p-0081" num="0080">8.7) Detection of the right boundary of the initial right row: an initial right row termination detection template ML<b>1</b> is constructed, and the initial right row termination detection template ML<b>1</b> is a vector whose length is &#xbd; of the length of the initial middle ridge start detection template ML<b>0</b>, the first half is 1, and the second half is &#x2212;1. The initial right row termination detection template ML<b>1</b> is used to be convolved with the data of the longitudinal projection vector S<b>8</b> on the right side of the initial middle ridge right boundary p0R0<sub>i</sub>. The column number of the position of the point with the maximum convolution value is taken as an initial right row right boundary CR0<sub>i </sub>of the i-th crop row sub-image S<b>7</b>.</p><p id="p-0082" num="0081">8.8) Estimation of the center point of the left ridge: an initial left row horizontal center column CLM0<sub>i </sub>of the i-th crop row sub-image S<b>7</b> is calculated by the following formula: CLM0<sub>i</sub>=(CL0<sub>i</sub>+p0L0<sub>i</sub>)/2. Then, a column pLM0<sub>i </sub>where the center point of the left ridge of the i-th crop row sub-image S<b>7</b> is located is calculated by the following formula: pLM0<sub>i</sub>=2&#xd7;CLM0<sub>i</sub>&#x2212;p0M0<sub>i</sub>.</p><p id="p-0083" num="0082">8.9) Estimation of the center point of the right ridge: an initial right row horizontal center column CRM0<sub>i </sub>of the i-th crop row sub-image S<b>7</b> is calculated by the following formula: CRM0<sub>i</sub>=(CR0<sub>i</sub>+p0R0<sub>i</sub>)/2. Then, a column pRM0<sub>i </sub>where the center point of the right ridge of the i-th crop row sub-image S<b>7</b> is located is calculated by the following formula: pRM0<sub>i</sub>=2&#xd7;CRM0<sub>i</sub>&#x2212;p0M0<sub>i</sub>.</p><p id="p-0084" num="0083">8.10) Calculation of the ordinate of the crop row sub-image S<b>7</b>: The ordinate of the position of the center point of the crop row sub-image S<b>7</b> on the crop row image S<b>6</b> is taken as an ordinate S<b>7</b><i>y</i><sub>i </sub>of the crop row sub-image S<b>7</b>.</p><p id="p-0085" num="0084">8.11) Determining the center lines of the initial middle ridge, the left ridge, and the right ridge.</p><p id="p-0086" num="0085">Step 8.2) to step 8.11) are repeated, the N crop row sub-images S<b>7</b> of the crop row image S<b>6</b> are sequentially traversed. Each crop row sub-image S<b>7</b> obtains an initial middle ridge center p0M0<sub>i</sub>, an initial left row horizontal center column CLM0<sub>i</sub>, an initial right row horizontal center column CRM0<sub>i</sub>, and an ordinate S<b>7</b><i>y</i><sub>i </sub>of the crop row sub-image S<b>7</b>, and therefore the results of all N crop row sub-images S<b>7</b> are composed to obtain a set of an initial middle ridge center set p0M0, an initial left row horizontal center column set CLM0, an initial right row horizontal center column set CRM0, and an ordinate set S<b>7</b><i>y </i>of the crop row sub-image S<b>7</b>.</p><p id="p-0087" num="0086">As shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref>, the ordinate S<b>7</b><i>y</i><sub>i </sub>of the crop row sub-image S<b>7</b> serves as an independent variable, the initial middle ridge center p0M0<sub>i</sub>, the initial left row horizontal center column CLM0<sub>i</sub>, and the initial right row horizontal center column CRM0<sub>i </sub>serve as dependent variables, respectively, and the univariate regression models pM, pL and pR are constructed between the initial middle ridge center p0M0<sub>i </sub>and the ordinate S<b>7</b><i>y</i><sub>i </sub>of the crop row sub-image S<b>7</b>, between the initial left row horizontal center column CLM0<sub>i </sub>and the ordinate S<b>7</b><i>y</i><sub>i </sub>of the crop row sub-image S<b>7</b>, and between the initial right row horizontal center column CRM0<sub>i </sub>and the ordinate S<b>7</b><i>y</i><sub>i </sub>of the crop row sub-image S<b>7</b>, respectively. The univariate regression models pM, pL, and pR are actually a fitted straight line.</p><p id="p-0088" num="0087">As shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref>, the three straight lines from right to right in the drawing are pM, pL, and pR, respectively.</p><p id="p-0089" num="0088">9) Detections of the left crop row and the right crop row:</p><p id="p-0090" num="0089">The implementation is as follows.</p><p id="p-0091" num="0090">9.1) A blank right crop row point set SCR and a blank left crop row point set SCL are constructed.</p><p id="p-0092" num="0091">9.2) The k-th row is taken on the crop row image S<b>6</b> as a row image S<b>9</b>, the ordinate of the row image S<b>9</b> as an independent variable is substituted into the univariate regression model pM, pL, and pR to obtain a crop middle ridge center column p0M1<sub>k</sub>, a crop left ridge horizontal center column CLM1<sub>k</sub>, and a crop left ridge horizontal center column CRM1<sub>k </sub>on the current row image S<b>9</b>.</p><p id="p-0093" num="0092">9.3) The blank left crop row point set SCL is constructed. On the current row image S<b>9</b>, the coordinates of the pixel with a pixel value of 1 between the crop middle ridge center column p0M1<sub>k </sub>and the crop left ridge horizontal center column CLM1<sub>k </sub>corresponding to the crop row image S<b>6</b> is added to the left crop row point set SCL.</p><p id="p-0094" num="0093">9.4) The blank right crop row point set SCR is constructed. On the current row image S<b>9</b>, the coordinates of the pixel with a pixel value of 1 between the crop middle ridge center column p0M1<sub>k </sub>and the crop left ridge horizontal center column CRM1<sub>k </sub>corresponding to the crop row image S<b>6</b> is added to the right crop row point set SCR.</p><p id="p-0095" num="0094">9.5) Step 9.2) to step 9.4) are repeated. Each row of the crop row image S<b>6</b> is traversed to obtain the complete left crop row point set SCL and the right crop row point set SCR.</p><p id="p-0096" num="0095">9.6) The ordinates of the pixels in the left crop row point set SCL serve as independent variables, the abscissas serve as dependent variables, a univariate regression model for the left crop row point set SCL is constructed, and a left crop row centerline model CL is obtained.</p><p id="p-0097" num="0096">9.7) The ordinates of the pixels in the right crop row point set SCR serve as independent variables, the abscissas serve as dependent variables, a univariate regression model for the right crop row point set SCR is constructed, and a right crop row centerline model CR is obtained.</p><p id="p-0098" num="0097">The left crop row centerline model CL and the right crop row centerline model CR are actually a fitted straight line.</p><p id="p-0099" num="0098">10) Detection of the middle ridge:</p><p id="p-0100" num="0099">The implementation is as follows.</p><p id="p-0101" num="0100">10.1) A blank middle ridge point set Spath is constructed.</p><p id="p-0102" num="0101">10.2) The q-th row on the crop row image S<b>6</b> is taken as a row image S<b>10</b>, and the ordinate of the line image S<b>10</b> as an independent variable is substituted into the left crop row centerline model CL and the right crop row centerline model CR to obtain a left row center point CL1<sub>q </sub>and a right row center point CR1<sub>q </sub>on the current line image S<b>10</b>.</p><p id="p-0103" num="0102">10.3) On the current line image S<b>10</b>, the pixel with a pixel value of 0 between the left row center point CL1<sub>q </sub>and the right row center point CR1<sub>q </sub>corresponding to the coordinates of the crop row image S<b>6</b> is added into the middle ridge point set SPath.</p><p id="p-0104" num="0103">10.4) Step 10.2) to step 10.3) are repeated. Each row image S<b>10</b> of the crop row image S<b>6</b> is traversed to obtain the complete middle ridge point set Spath.</p><p id="p-0105" num="0104">10.5) The ordinates of the pixels in the middle ridge point set SPath serve as independent variables, the abscissas serve as dependent variables, a univariate regression model is constructed for the middle ridge point set SPath, and a middle ridge centerline model pPath is obtained. The straight line where the middle ridge centerline model pPath is located is the navigation line for the field machinery. As shown in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, the three straight lines from right to right in the drawing are the left crop row centerline model CL, the middle ridge centerline model pPath, and the right crop row centerline model CR, respectively.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method for detecting a field navigation line after a ridge sealing of crops, characterized in that the method comprises steps as follows:<claim-text>1) performing crop image acquisition, wherein a camera is used to acquire a field crop image, recorded as an original image S<b>1</b>;</claim-text><claim-text>2) performing image color space transformation, wherein the original image S<b>1</b> is converted to HSI color space to obtain an HSI image S<b>2</b>;</claim-text><claim-text>3) performing image binarization to obtain a binary image S<b>3</b>;</claim-text><claim-text>4) processing longitudinal integration of the binary image S<b>3</b> to obtain a longitudinal integral image S<b>4</b>;</claim-text><claim-text>5) performing neighborhood setting,</claim-text><claim-text>wherein a neighborhood of a current pixel is set, the neighborhood is 1/48 of an image width of the original image S<b>1</b>, a 3-row two-dimensional matrix R is used to represent the neighborhood, a column in the two-dimensional matrix R represents a column in the neighborhood, each element of a first row in the two-dimensional matrix R represents a column offset of each column in the neighborhood relative to the current pixel, each element of a second row represents an abscissa offset of a start row of each column in the neighborhood, and each element of a third row represents an abscissa offset of an end row of each column in the neighborhood;</claim-text><claim-text>6) performing region integration calculation,</claim-text><claim-text>wherein a blank image having a size the same as that of the longitudinal integral image S<b>4</b> is constructed as a region integration image S<b>5</b>, and each pixel is traversed on the longitudinal integral image S<b>4</b>, which is processed in the way as follows: when traversing, current pixel coordinates are marked as (x, y), an accumulator C is set, and an initial value of the accumulator C is set to be 0; when traversing each column of the two-dimensional matrix R and traversing the two-dimensional matrix R, elements of the first row to the third row of a current j-th column are R<sub>1j</sub>, R<sub>2j</sub>, and R<sub>3j</sub>, a difference is obtained by subtracting a pixel value of a pixel with coordinates (x+R<sub>3j</sub>&#x2212;1, y+R<sub>1j</sub>) from a pixel value of a pixel with coordinates (x+R<sub>3j</sub>, y+R<sub>1j</sub>) on the longitudinal integral image S<b>4</b>, the difference is accumulated into the accumulator C, after traversing the two-dimensional matrix R is completed, a value in the accumulator C is taken as a regional integral value M of the current pixel, the regional integral value M is assigned to the pixel having coordinates the same as those of the current pixel in the region integration image S<b>5</b>, and after traversing each pixel of the longitudinal integral image S<b>4</b> is completed, the region integration image S<b>5</b> is obtained;</claim-text><claim-text>7) performing detections of crop rows, wherein each row is traversed in the region integration image S<b>5</b>, an average value of the regional integral value M of all pixels in each row is calculated, a pixel whose regional integral value M is greater than the average value is set to be 1, the remaining pixels are set to be 0, and a crop row image S<b>6</b> is obtained;</claim-text><claim-text>8) performing detections of an initial middle ridge, a left ridge, and a right ridge;</claim-text><claim-text>9) performing detections of left crop row and right crop row;</claim-text><claim-text>10) performing detection of a middle ridge.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method for detecting the field navigation line after the ridge sealing of the crops according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the step 3) specifically comprises: setting a pixel value of the pixel whose hue component value H is between 0.2 and 0.583 in the HSI image S<b>2</b> to 1, and setting a pixel value of remaining pixels to 0 to obtain the binary image S<b>3</b>.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method for detecting the field navigation line after the ridge sealing of the crops according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the step 4) specifically comprises: duplicating the binary image S<b>3</b> as the longitudinal integral image S<b>4</b>; traversing each column on the longitudinal integral image S<b>4</b>; in each column, traversing each pixel downward from a pixel of the second row; when traversing, pixel values of pixels of the previous row are added; and covering a result with a pixel value of the current pixel, so as to obtain the longitudinal integral image S<b>4</b>.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method for detecting the field navigation line after the ridge sealing of the crops according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the step 8) specifically comprises:<claim-text>8.1) dividing the crop row image S<b>6</b> into N crop row sub-images S<b>7</b> having a width the same as a width of the crop row image S<b>6</b> and a height 1/N of a height of the crop row image S<b>6</b>;</claim-text><claim-text>8.2) taking an i-th crop row sub-image S<b>7</b>, and calculating a longitudinal projection vector S<b>8</b> of the i-th crop row sub-image S<b>7</b>;</claim-text><claim-text>8.3) performing detection of the left boundary of the initial middle ridge, wherein an initial middle ridge start detection template ML<b>0</b> is constructed, the initial middle ridge start detection template ML<b>0</b> is a vector whose length is &#x2159; of a width of the original image S<b>1</b>, a first half is 1, a second half is &#x2212;1, the longitudinal projection vector S<b>8</b> is convolved with the initial middle ridge start detection template ML<b>0</b>, and a column number of a position of a point with a maximum convolution value is taken as an initial middle ridge left boundary p0L0<sub>i </sub>of the i-th crop row sub-image S<b>7</b>;</claim-text><claim-text>8.4) performing detection of a right boundary of the initial middle ridge, wherein an initial middle ridge termination detection template MR<b>0</b> is constructed, the initial middle ridge termination detection template MR<b>0</b> is a vector whose length is &#x2159; of the width of the original image S<b>1</b>, a first half is &#x2212;1, a second half is 1, the longitudinal projection vector S<b>8</b> is convolved with the initial middle ridge termination detection template MR<b>0</b>, and a column number of a position of a point with a maximum convolution value is taken as an initial middle ridge right boundary p0R0<sub>i </sub>of the i-th crop row sub-image S<b>7</b>;</claim-text><claim-text>8.5) calculating an initial middle ridge center p0M0<sub>i </sub>of the i-th crop row sub-image S<b>7</b> by a formula as follows: p0M0<sub>i</sub>=(p0L0<sub>i</sub>+p0R0<sub>i</sub>)/2;</claim-text><claim-text>8.6) performing detection of a left boundary of an initial left row, wherein an initial left row start detection template MR<b>1</b> is constructed, the initial left row start detection template MR<b>1</b> is a vector whose length is &#xbd; of a length of the initial middle ridge termination detection template MR<b>0</b>, a first half is &#x2212;1, a second half is 1, the initial left row start detection template MR<b>1</b> is used to be convolved with data of the longitudinal projection vector S<b>8</b> on the left side of the initial middle ridge left boundary p0L0<sub>i</sub>, and a column number of a position of a point with the maximum convolution value is taken as an initial left row left boundary CL0<sub>i </sub>of the i-th crop row sub-image S<b>7</b>;</claim-text><claim-text>8.7) performing detection of a right boundary of an initial right row, wherein an initial right row termination detection template ML<b>1</b> is constructed, wherein the initial right row termination detection template ML<b>1</b> is a vector whose length is &#xbd; of a length of the initial middle ridge start detection template ML<b>0</b>, a first half is 1, a second half is &#x2212;1, the initial right row termination detection template ML<b>1</b> is used to be convolved with data of the longitudinal projection vector S<b>8</b> on the right side of the initial middle ridge right boundary p0R0<sub>i</sub>, and a column number of a position of a point with the maximum convolution value is taken as an initial right row right boundary CR0<sub>i </sub>of the i-th crop row sub-image S<b>7</b>;</claim-text><claim-text>8.8) performing estimation of a center point of the left ridge, wherein an initial left row horizontal center column CLM0<sub>i </sub>of the i-th crop row sub-image S<b>7</b> is calculated by the following formula: CLM0<sub>i</sub>=(CL0<sub>i</sub>+p0L0<sub>i</sub>)/2, and then a column pLM0<sub>i </sub>where the center point of the left ridge of the i-th crop row sub-image S<b>7</b> is located is calculated by the following formula: pLM0<sub>i</sub>=2&#xd7;CLM0<sub>i</sub>&#x2212;p0M0<sub>i</sub>;</claim-text><claim-text>8.9) performing estimation of a center point of the right ridge, wherein an initial right row horizontal center column CRM0<sub>i </sub>of the i-th crop row sub-image S<b>7</b> is calculated by the following formula: CRM0<sub>i</sub>=(CR0<sub>i</sub>+p0R0<sub>i</sub>)/2, and then a column pRM0<sub>i </sub>where the center point of the right ridge of the i-th crop row sub-image S<b>7</b> is located is calculated by the following formula: pRM0<sub>i</sub>=2&#xd7;CRM0<sub>i</sub>&#x2212;p0M0<sub>i</sub>;</claim-text><claim-text>8.10) performing calculation of an ordinate of the crop row sub-image S<b>7</b>, wherein an ordinate of a position of a center point of the crop row sub-image S<b>7</b> on the crop row image S<b>6</b> is taken as an ordinate S<b>7</b><i>y</i><sub>i </sub>of the crop row sub-image S<b>7</b>;</claim-text><claim-text>8.11) determining center lines of the initial middle ridge, the left ridge, and the right ridge,</claim-text><claim-text>wherein step 8.2) to step 8.11) are repeated, the N crop row sub-images S<b>7</b> of the crop row image S<b>6</b> are sequentially traversed, each crop row sub-image S<b>7</b> obtains an initial middle ridge center p0M0<sub>i</sub>, an initial left row horizontal center column CLM0<sub>i </sub>an initial right row horizontal center column CRM0<sub>i</sub>, and the ordinate S<b>7</b><i>y</i><sub>i </sub>of the crop row sub-image S<b>7</b>, and therefore results of all N crop row sub-images S<b>7</b> are composed to obtain a set of an initial middle ridge center set p0M0, an initial left row horizontal center column set CLM0, an initial right row horizontal center column set CRM0, and an ordinate set S<b>7</b><i>y </i>of the crop row sub-image S<b>7</b>;</claim-text><claim-text>wherein the ordinate S<b>7</b><i>y</i><sub>i </sub>of the crop row sub-image S<b>7</b> serves as an independent variable, the initial middle ridge center p0M0<sub>i</sub>, the initial left row horizontal center column CLM0<sub>i</sub>, the initial right row horizontal center column CRM0<sub>i </sub>serve as dependent variables, respectively, univariate regression models pM, pL and pR are constructed between the initial middle ridge center p0M0<sub>i </sub>and the ordinate S<b>7</b><i>y</i><sub>i </sub>of the crop row sub-image S<b>7</b>, between the initial left row horizontal center column CLM0<sub>i </sub>and the ordinate S<b>7</b><i>y</i><sub>i </sub>of the crop row sub-image S<b>7</b>, and between the initial right row horizontal center column CRM0<sub>i </sub>and the ordinate S<b>7</b><i>y</i><sub>i </sub>of the crop row sub-image S<b>7</b>, respectively.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method for detecting the field navigation line after the ridge sealing of the crops according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the step 9) specifically comprising:<claim-text>9.1) constructing a blank right crop row point set SCR and a blank left crop row point set SCL;</claim-text><claim-text>9.2) taking a k-th row on the crop row image S<b>6</b> as a row image S<b>9</b>, wherein an ordinate of the row image S<b>9</b> as an independent variable is substituted into univariate regression models pM, pL, and pR to obtain a crop middle ridge center column p0M1<sub>k</sub>, a crop left ridge horizontal center column CLM1<sub>k</sub>, and a crop right ridge horizontal center column CRM1<sub>k </sub>on the current row image S<b>9</b>;</claim-text><claim-text>9.3) on the current row image S<b>9</b>, adding the coordinates of a pixel with a pixel value of 1 between the crop middle ridge center column p0M1<sub>k </sub>and the crop left ridge horizontal center column CLM1<sub>k </sub>corresponding to the crop row image S<b>6</b> to the left crop row point set SCL;</claim-text><claim-text>9.4) on the current row image S<b>9</b>, adding the coordinates of a pixel with a pixel value of 1 between the crop middle ridge center column p0M1<sub>k </sub>and the crop right ridge horizontal center column CRM1<sub>k </sub>corresponding to the crop row image S<b>6</b> to the right crop row point set SCR;</claim-text><claim-text>9.5) repeating step 9.2) to step 9.4), wherein each row of the crop row image S<b>6</b> is traversed to obtain the complete left crop row point set SCL and the right crop row point set SCR;</claim-text><claim-text>9.6) wherein ordinates of the pixels in the left crop row point set SCL serve as independent variables, abscissas of the pixels in the left crop row point set SCL serve as dependent variables, a univariate regression model for the left crop row point set SCL is constructed, and a left crop row centerline model CL is obtained;</claim-text><claim-text>9.7) wherein ordinates of the pixels in the right crop row point set SCR serve as independent variables, abscissas of the pixels in the right crop row point set SCR serve as dependent variables, a univariate regression model for the right crop row point set SCR is constructed, and a right crop row centerline model CR is obtained.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method for detecting the field navigation line after the ridge sealing of the crops according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the step 10) specifically comprises:<claim-text>10.1) constructing a blank middle ridge point set Spath;</claim-text><claim-text>10.2) taking a q-th row on the crop row image S<b>6</b> as a row image S<b>10</b>, wherein an ordinate of the line image S<b>10</b> as an independent variable is substituted into a left crop row centerline model CL and a right crop row centerline model CR to obtain a left row center point CL1<sub>q </sub>and a right row center point CR1<sub>q </sub>on the current line image S<b>10</b>;</claim-text><claim-text>10.3) on the current line image S<b>10</b>, adding the coordinates of a pixel with a pixel value of 0 between the left row center point CL1<sub>q </sub>and the right row center point CR1<sub>q </sub>corresponding to the crop row image S<b>6</b> into the middle ridge point set SPath;</claim-text><claim-text>10.4) repeating step 10.2) to step 10.3), wherein each row image S<b>10</b> of the crop row image S<b>6</b> is traversed to obtain the complete middle ridge point set Spath;</claim-text><claim-text>10.5) wherein the ordinates of the pixels in the middle ridge point set SPath serve as independent variables, abscissas of the pixels in the middle ridge point set SPath serve as dependent variables, a univariate regression model is constructed for the middle ridge point set SPath, a middle ridge centerline model pPath is obtained, and the straight line where the middle ridge centerline model pPath is located is the navigation line for the field machinery.</claim-text></claim-text></claim></claims></us-patent-application>