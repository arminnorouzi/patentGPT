<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004762A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004762</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17942898</doc-number><date>20220912</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>62</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>7</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>521</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>15</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>20</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>28</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>50</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>56</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>58</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>64</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>6268</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>7</main-group><subgroup>005</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>6256</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>521</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>15</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20190101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>20</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>6282</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>28</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>50</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>56</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>58</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>64</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>584</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>05</class><subclass>D</subclass><main-group>2201</main-group><subgroup>0213</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20081</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2210</main-group><subgroup>12</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30261</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>05</class><subclass>D</subclass><main-group>1</main-group><subgroup>0238</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">Multiple Stage Image Based Object Detection and Recognition</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>17007969</doc-number><date>20200831</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11443148</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17942898</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>15972566</doc-number><date>20180507</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>10762396</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17007969</doc-number></document-id></child-doc></relation></continuation><us-provisional-application><document-id><country>US</country><doc-number>62594631</doc-number><date>20171205</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>UATC, LLC</orgname><address><city>Mountain View</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Vallespi-Gonzalez</last-name><first-name>Carlos</first-name><address><city>Pittsburgh</city><state>PA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Amato</last-name><first-name>Joseph Lawrence</first-name><address><city>Pittsburgh</city><state>PA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Totolos, JR.</last-name><first-name>George</first-name><address><city>Cranberry Township</city><state>PA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Systems, methods, tangible non-transitory computer-readable media, and devices for autonomous vehicle operation are provided. For example, a computing system can receive object data that includes portions of sensor data. The computing system can determine, in a first stage of a multiple stage classification using hardware components, one or more first stage characteristics of the portions of sensor data based on a first machine-learned model. In a second stage of the multiple stage classification, the computing system can determine second stage characteristics of the portions of sensor data based on a second machine-learned model. The computing system can generate an object output based on the first stage characteristics and the second stage characteristics. The object output can include indications associated with detection of objects in the portions of sensor data.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="137.67mm" wi="135.47mm" file="US20230004762A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="212.94mm" wi="180.51mm" orientation="landscape" file="US20230004762A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="222.25mm" wi="151.05mm" file="US20230004762A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="202.10mm" wi="137.50mm" file="US20230004762A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="216.41mm" wi="162.90mm" file="US20230004762A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="218.78mm" wi="172.13mm" file="US20230004762A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="226.57mm" wi="127.68mm" file="US20230004762A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="226.99mm" wi="127.68mm" file="US20230004762A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="242.91mm" wi="166.62mm" orientation="landscape" file="US20230004762A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">RELATED APPLICATION</heading><p id="p-0002" num="0001">The present application is based on and claims benefit of U.S. Provisional Patent Application No. 62/594,631 having a filing date of Dec. 5, 2017, which is incorporated by reference herein.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">FIELD</heading><p id="p-0003" num="0002">The present disclosure relates generally to operation of an autonomous vehicle including the detection and recognition of one or more characteristics of an object using multiple stage classification.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">Vehicles, including autonomous vehicles, can receive data based on the state of the environment around the vehicle including the state of objects in the environment. This data can be used by the autonomous vehicle to perform various functions related to the movement of those objects through the environment. Further, as the vehicle travels through the environment the set of objects in the environment and the state of those objects can also change. As such, the safe operation of an autonomous vehicle in the environment relies on an accurate determination of the state of the environment. Accordingly, there exists a need for a computing system that more effectively determines the state of objects in an environment.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0005" num="0004">Aspects and advantages of embodiments of the present disclosure will be set forth in part in the following description, or may be learned from the description, or may be learned through practice of the embodiments.</p><p id="p-0006" num="0005">An example aspect of the present disclosure is directed to a computer-implemented method of autonomous vehicle operation. The computer-implemented method of autonomous vehicle operation can include receiving, by a computing system comprising one or more computing devices, object data including one or more portions of sensor data. The method can include determining, by the computing system, in a first stage of a multiple stage classification using one or more hardware components, one or more first stage characteristics of the one or more portions of sensor data based in part on a first machine-learned model. Further, the method can include determining, by the computing system, in a second stage of the multiple stage classification, one or more second stage characteristics of the one or more portions of sensor data based in part on a second machine-learned model. The method can include generating, by the computing system, an object output based in part on the one or more first stage characteristics and the one or more second stage characteristics. The object output can include one or more indications associated with detection of one or more objects in the one or more portions of sensor data.</p><p id="p-0007" num="0006">Another example aspect of the present disclosure is directed to one or more tangible, non-transitory computer-readable media storing computer-readable instructions that when executed by one or more processors cause the one or more processors to perform operations. The operations can include receiving object data including one or more portions of sensor data. The operations can include determining, in a first stage of a multiple stage classification using one or more hardware components, one or more first stage characteristics of the one or more portions of sensor data based in part on a first machine-learned model. Further, the operations can include determining, in a second stage of the multiple stage classification, one or more second stage characteristics of the one or more portions of sensor data based in part on a second machine-learned model. The operations can include generating an object output based in part on the one or more first stage characteristics and the one or more second stage characteristics. The object output can include one or more indications associated with detection of one or more objects in the one or more portions of sensor data.</p><p id="p-0008" num="0007">Another example aspect of the present disclosure is directed to an autonomous vehicle comprising one or more processors and one or more non-transitory computer-readable media storing instructions that when executed by the one or more processors cause the one or more processors to perform operations. The operations can include receiving object data including one or more portions of sensor data. The operations can include determining, in a first stage of a multiple stage classification using one or more hardware components, one or more first stage characteristics of the one or more portions of sensor data based in part on a first machine-learned model. Further, the operations can include determining, in a second stage of the multiple stage classification, one or more second stage characteristics of the one or more portions of sensor data based in part on a second machine-learned model. The operations can include generating an object output based in part on the one or more first stage characteristics and the one or more second stage characteristics. The object output can include one or more indications associated with detection of one or more objects in the one or more portions of sensor data.</p><p id="p-0009" num="0008">Other example aspects of the present disclosure are directed to other systems, methods, vehicles, apparatuses, tangible non-transitory computer-readable media, and devices for autonomous vehicle operation including the detection and recognition of one or more characteristics of an object using multiple stage classification.</p><p id="p-0010" num="0009">These and other features, aspects and advantages of various embodiments will become better understood with reference to the following description and appended claims. The accompanying drawings, which are incorporated in and constitute a part of this specification, illustrate embodiments of the present disclosure and, together with the description, serve to explain the related principles.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0011" num="0010">Detailed discussion of embodiments directed to one of ordinary skill in the art are set forth in the specification, which makes reference to the appended figures, in which:</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>1</b></figref> depicts a diagram of an example system according to example embodiments of the present disclosure;</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>2</b></figref> depicts a diagram of an example multiple stage classification system according to example embodiments of the present disclosure;</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>3</b></figref> depicts an example of object detection from an overhead view using a multiple stage classifier according to example embodiments of the present disclosure;</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>4</b></figref> depicts an example of object detection and use of decision trees by a multiple stage classifier according to example embodiments of the present disclosure;</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>5</b></figref> depicts a second example of object detection and use of a decision tree by a multiple stage classifier according to example embodiments of the present disclosure;</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>6</b></figref> depicts a flow diagram of an example method of object detection and recognition according to example embodiments of the present disclosure;</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>7</b></figref> depicts a second flow diagram of an example method of object detection and recognition according to example embodiments of the present disclosure;</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>8</b></figref> depicts a third flow diagram of an example method of object detection and recognition according to example embodiments of the present disclosure; and</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>9</b></figref> depicts a second diagram of an example system according to example embodiments of the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0021" num="0020">Example aspects of the present disclosure are directed to image classification or object detection (e.g., detecting, identifying, and/or recognizing objects represented in one or more images) using multiple stage classification that can be applied to vehicle technologies (e.g., autonomous vehicles, manually operated vehicles, and/or semi-autonomous vehicles). In particular, aspects of the present disclosure include receiving object data that is associated with one or more images (e.g., object data based on images captured by one or more cameras), determining, in a first stage of a multiple stage classification, one or more first stage characteristics of the object data using a first machine-learned model, determining, in a second stage of the multiple stage classification, one or more second stage characteristics of the object data using a second machine-learned model, and generating indications associated with detection of one or more objects in the one or more images.</p><p id="p-0022" num="0021">By way of example, the disclosed technology can receive object data that is based in part on sensor data that can include images of an environment (e.g., an urban street with vehicles and pedestrians). The images can be based in part on output from one or more sensors including one or more light detection and ranging devices (LIDAR). The object data can be processed in a first stage of a multiple stage classification process that uses a first machine-learned model (e.g., a first decision tree model or a first neural network model) to determine one or more first stage characteristics including the portions of the one or more images that are background (e.g., the portions of the one or more images that are less likely to include objects of interest) and the portions of the one or more images that are foreground (e.g., the portions of the one or more images that are more likely to include objects of interest). For example, the first machine-learned model can be stored in, and implemented by, a hardware system including one or more programmable logic devices (e.g., a field programmable gate array (FPGA) device or an application specific integrated circuit (ASIC) device).</p><p id="p-0023" num="0022">In the second stage of the multiple stage classification process, the disclosed technology can use a second machine-learned model (e.g., a second decision tree model or a second neural network model) to determine one or more second stage characteristics including the identity and location of objects (e.g., vehicles and/or pedestrians) with a greater level of confidence. In some implementations, the second machine-learned model can be stored in, and implemented by, a software system including one or more processing units, processor cores, microprocessors, and/or central processing units (CPUs)). The disclosed technology can then generate one or more indications that can be used in various ways, for example by an autonomous vehicle, to perform actions including detecting and/or tracking objects; activating vehicle systems based on the detection of the detected objects (e.g., vehicle notification systems); and/or modifying the path of the vehicle to avoid the detected objects (e.g., vehicle motion planning and/or autonomy systems).</p><p id="p-0024" num="0023">As such, the disclosed technology can more effectively (e.g., more rapidly and accurately) detect and/or identify one or more objects in one or more portions of sensor data. In particular, the disclosed technology can achieve superior results by leveraging the capabilities of different types of hardware at different stages of the classification process, including using hardware that is specially configured in a first stage of the process and a more software driven approach using specially adapted software in a second stage of the process.</p><p id="p-0025" num="0024">The disclosed technology can include an object detection system (e.g., a computing system including one or more computing devices with one or more processors and a memory) that can detect or identify one or more objects and/or a vehicle computing system that can control a variety of vehicle systems and communicate with the object detection system. The object detection system can process, generate, or exchange (e.g., send or receive) signals or data, including signals or data exchanged with various computing systems including the vehicle computing system, vehicle systems (e.g., vehicle engines, vehicle motors, vehicle electrical systems, and/or vehicle notification systems), and/or remote computing systems (e.g., computing devices at a remote location).</p><p id="p-0026" num="0025">For example, the object detection system can exchange signals (e.g., electronic signals) or data with vehicle components or vehicle computing system including sensor systems (e.g., sensors that generate output based on the state of the physical environment in range of the sensors including LIDAR, cameras, microphones, radar, or sonar); communication systems (e.g., wired or wireless communication systems that can exchange signals or data with other devices); navigation systems (e.g., devices that can receive signals from GPS, GLONASS, or other systems used to determine a vehicle's geographical location); notification systems (e.g., devices used to provide notifications to pedestrians, cyclists, and vehicles, including electronic communication devices, display devices, status indicator lights, and/or audio output systems); braking systems (e.g., brakes of the vehicle including mechanical and/or electric brakes); propulsion systems (e.g., motors or engines including electric engines or internal combustion engines); and/or steering systems used to change the path, course, or direction of travel of the vehicle.</p><p id="p-0027" num="0026">The object detection system can receive object data that is associated with one or more portions of sensor data (e.g., data output from one or more sensors including one or more LIDAR devices, one or more cameras, one or more RADAR devices, one or more sonar devices, and/or one or more thermal imaging devices). For example, the sensor data can include two-dimensional images including images captured by one or more cameras and/or three-dimensional point clouds captured by a LIDAR device. The one or more portions of sensor data can be analyzed to detect one or more objects including one or more pedestrians (e.g., one or more persons laying down, sitting, crouching, standing, walking, or running); one or more other vehicles (e.g., automobiles, trucks, buses, trolleys, motorcycles, mopeds, aircraft, boats, amphibious vehicles, and/or trains); one or more cyclists (e.g., a person sitting and/or riding on a bicycle); and/or one or more buildings (e.g., houses and/or apartment buildings). Further, the object data can include a set of three-dimensional points (e.g., x, y, and z coordinates) associated with one or more physical dimensions (e.g., the length, width, and/or height) of the one or more objects in the one or more images. The portions of sensor data and/or the associated object data can be used to determine physical properties or characteristics (e.g., visual properties or characteristics) of the one or more objects including the shape, texture, brightness, saturation, and/or physical dimensions (e.g., length, width, and/or height).</p><p id="p-0028" num="0027">In some embodiments the one or more portions of sensor data, which can be associated with other data including the object data, can be based in part on sensor output from one or more sensors including one or more LIDAR devices, one or more cameras, one or more radar devices, one or more sonar devices, or one or more thermal imaging devices.</p><p id="p-0029" num="0028">The object detection system can determine, in a first stage of a multiple stage classification, one or more first stage characteristics of the one or more portions of sensor data based in part on traversal of a first decision tree of a first machine-learned model (e.g., a first machine-learned model associated with data which can include the object data). In some embodiments, the first machine-learned model used by the object detection system can be based in part on one or more classification techniques including a random forest classifier, gradient boosting, a support vector machine, a logistic regression classifier, and/or a boosted forest classifier.</p><p id="p-0030" num="0029">In some embodiments, the first stage of the multiple stage classification can include traversal of a first decision tree that includes a first plurality of nodes associated with a plurality of classifier labels. Each of the first plurality of nodes in the first decision tree can be associated with a classifier label that is used to classify, categorize and/or determine the one or more first stage characteristics of the one or more portions of sensor data. For example, the first stage of the multiple stage classification can include a determination of the one or more first stage characteristics including the portions of the one or more portions of sensor data that are background and the portions of the one or more portions of sensor data that are foreground. As such, the object detection system can provide a first stage output (i.e., the one or more first stage characteristics) that allows the second stage of the multiple stage classification to more efficiently process the one or more portions of sensor data by determining in advance the areas of the one or more portions of sensor data to focus on for purposes of object detection.</p><p id="p-0031" num="0030">In some embodiments, the first stage of the multiple stage classification can be performed by one or more hardware components of the one or more computing devices including an FPGA, a digital signal processor (DSP), an application specific integrated circuit (ASIC), or a graphics processing unit (GPU). By way of example, using an FPGA can allow for an improvement in processing effectiveness through processing of the one or more images in parallel, which can result in image processing that exceeds the speed of other techniques that do not process the images in parallel (e.g., serial processing of the one or more images).</p><p id="p-0032" num="0031">The object detection system can determine, in a second stage of the multiple stage classification, one or more second stage characteristics of the one or more objects based in part on traversal of a second decision tree of a second machine-learned model (e.g., a second machine-learned model associated with data which can include the object data). In some embodiments, the second machine-learned model can include, or be the same as, the first machine-learned model.</p><p id="p-0033" num="0032">The second stage of the multiple stage classification can include traversal of a second decision tree that includes a second plurality of nodes associated with the second plurality of classifier labels. For example, each of the plurality of nodes in the second decision tree can be associated with a classifier label that is used to classify, categorize and/or determine the one or more first stage characteristics of the one or images. For example, the second stage of the multiple stage classification can include a determination of the one or more second stage characteristics of the one or more images including the location and identity of one or more objects (e.g., the location of pedestrians in the one or more images).</p><p id="p-0034" num="0033">The second stage of the multiple stage classification can include determining one or more second stage characteristics of the one or more images that are based in part on the output of the first stage. The one or more second stage characteristics can include the one or more first stage characteristics (e.g., if a background characteristic is determined in the first stage a background characteristic can be further determined, to a greater level of confidence, in the second stage). Further, the one or more second stage characteristics can include characteristics that were not determined in the first stage. For example, if one or more objects (e.g., pedestrians, vehicles, and/or cyclists) were not determined in the first stage, the one or more objects can be determined in the second stage.</p><p id="p-0035" num="0034">In some embodiments, the second decision tree can include an equal or greater number of nodes than the first plurality of nodes. For example, the first decision tree can include one-thousand nodes and the second decision tree can include five-thousand nodes which can allow for a deeper decision tree that can include more classifier labels and can be traversed for greater accuracy in detecting and/or identifying the one or more objects in the one or more images.</p><p id="p-0036" num="0035">In some embodiments, parts of the second stage of the multiple stage classification can be performed by one or more software components of the one or more computing devices including one or more software components that operate or are executed on one or more central processing units. The use of software components can allow for more flexible adjustment and customization of the second machine-learned model.</p><p id="p-0037" num="0036">The object detection system can determine, based in part on the object data and a machine-learned model that can be the first machine-learned model or the second machine-learned model, an amount (e.g., a number of occurrences) of false positive determinations of the one or more first stage characteristics (when the first machine-learned model is used) or the one or more second stage characteristics (when the second machine-learned model is used) of the one or more objects that has occurred. For example, a false positive determination of the one or more first stage characteristics can include a determination that a portion of the background (e.g., the Sun in the sky) is part of the foreground. Further, the object detection system can terminate traversal of a decision tree including the first decision tree or the second decision tree when the amount of false positive determinations exceeds a predetermined threshold level.</p><p id="p-0038" num="0037">In some embodiments, the determination of the predetermined threshold level to terminate traversal of the decision tree can be based on performance (e.g., false positive rate) of the first machine-learned model or the second machine-learned model on a known data set (e.g., training data in which all of the objects have been correctly identified) at the various depths of the decision tree. For example, the predetermined threshold level to terminate traversal of the decision tree can be based in part on the depth of the first decision tree when the amount of false positives exceeds a predetermined percentage of identified objects (e.g., ten percent of the identified objects) or a predetermined number of objects per image (e.g., two objects per image).</p><p id="p-0039" num="0038">The object detection system can include generating, based in part on the object data, visual descriptor output associated with the one or more images. When the one or more portions of sensor data include images, the visual descriptor output can include various properties or characteristics of the one or more images including color hue information, color saturation information, brightness information, or histogram of oriented gradients information. In some embodiments, the one or more first stage characteristics can be based in part on the visual descriptor output.</p><p id="p-0040" num="0039">The object detection system can generate, based in part on the visual descriptor output, a heat map associated with the one or more images. The heat map can include a plurality of areas associated with a probability of at least one of the one or more objects being within the respective one of the plurality of areas. For example, the object detection system can segment the one or images in the plurality of areas and, for each of the plurality of areas, determine a probability of an object being within that area. In some embodiments, the one or more second stage characteristics can be based in part on the heat map.</p><p id="p-0041" num="0040">The object detection system can determine, based in part on the visual descriptor output or the heat map, one or more portions of the one or more images that are associated with one or more background images (i.e., the portions of the one or more images that are background in contrast with a foreground of the one or more images that contains the one or more objects). In some embodiments, the second stage of the multiple stage classification can exclude the one or more portions of the one or more images that are associated with the one or more background images. In this way, the second stage of the multiple stage classification can focus more resources on a smaller portion of the object data (e.g., the foreground images of the one or more images) and conserve resources by not analyzing portions of the one or more images that are part of the background.</p><p id="p-0042" num="0041">In some embodiments, at least one node of the second plurality of nodes in the second decision tree is a terminal node (e.g., the last node/leaf of the decision tree) of the first plurality of nodes in the first decision tree. For example, the first node in the second decision tree can be the terminal node in the first decision tree. In this way, the second decision tree can be a continuation of the first decision tree and leverage the benefits of building upon the first decision tree based in part on the outputs of the first decision tree.</p><p id="p-0043" num="0042">In some embodiments, the second decision tree of the second machine-learned model can include an equal number of nodes as the first plurality of nodes or a greater number of nodes than the first plurality of nodes. In some embodiments, the first stage of the multiple stage classification can be performed on a customized device (e.g., an FPGA) that operates in parallel and can rapidly determine one or more first stage characteristics of the one or more portions of sensor data including whether a portion of sensor data (e.g., a portion of an image) is foreground or a background. After determining one or more first stage characteristics, the second stage of the multiple stage classification can use a decision tree that is deeper (i.e., has more nodes along the path from a root node to a terminal node) to determine one or more second stage characteristics that can, with a higher level of confidence, determine or identify one or more objects including vehicles, pedestrians, and/or cyclists.</p><p id="p-0044" num="0043">The object detection system can generate an object output based in part on the one or more first stage characteristics and/or the one or more second stage characteristics. The object output can include one or more indications associated with detection of one or more objects in the one or more portions of sensor data. For example, the object output can be exchanged with vehicle systems or remote computing devices and can include one or more indications of whether objects were detected; the type of objects that were detected; the location of the objects detected; the physical characteristics of the objects detected; the velocity and/or acceleration of the objects detected; and/or a probability associated with an estimated accuracy of the object detection.</p><p id="p-0045" num="0044">In some embodiments, the object output can be used by one or more vehicle systems to perform one or more actions including activating vehicle systems based on detection of the one or more objects (e.g., activating a headlight when an object is detected at night); modifying the path of the vehicle (e.g., to maneuver the vehicle around objects); and/or exchange the object output with one or more vehicle systems or remote computing systems.</p><p id="p-0046" num="0045">The object detection system can determine, based in part on the object output, locations for one or more bounding shapes (e.g., two-dimensional or three-dimensional bounding boxes and/or bounding polygons) associated with the one or more objects in the one or more portions of sensor data. The object detection system can use the first machine-learned model and/or the second machine-learned model to determine the one or more locations or areas of the sensor data that are more likely to contain an object or a certain type of object (e.g., a pedestrian is more likely to be in the ground portion of an image than the sky portion of an image).</p><p id="p-0047" num="0046">The object detection system can select a set of the locations for the one or more bounding shapes. An image processing technique (e.g., a filter including non-maximum suppression) can then be used to select a location including an optimal location from the set of locations for the one or more bounding shapes. For example, by analyzing the image gradient direction, pixels that are not part of the local maxima for the portion of the sensor data corresponding to each of the set of locations can be suppressed. The object detection system can, based on the set of locations for the one or more bounding shapes, generate the one or more bounding shapes in the selected locations.</p><p id="p-0048" num="0047">The systems, methods, devices, and tangible, non-transitory computer-readable media in the disclosed technology can provide a variety of technical effects and benefits to the overall operation of autonomous vehicles including vehicle computing systems that use machine-learned models for the detection of objects. In particular, the disclosed technology leverages the advantages of a multi-stage classifier to reduce the time to create an output while maintaining a high level of accuracy with respect to object detection and identification. For example, a first stage of classification that uses hardware components that can process inputs (e.g., object data based on sensor outputs from one or more sensors including LIDAR and/or cameras) in parallel and can rapidly identify portions of images for further processing in subsequent stages. Then, in a second stage of classification, the disclosed technology can more thoroughly process the one or more images using a deeper decision tree. As a result, the disclosed technology can output highly accurate results in less time. Additionally, the use of hardware components including an FPGA can result in lower latency and greater energy efficiency in comparison to general usage processors.</p><p id="p-0049" num="0048">Furthermore, the disclosed technology can apply early termination of traversing the decision tree at any of the multiple stages of the classification process. Early termination allows the disclosed technology to conserve computing resources by not continuing to traverse a decision tree when the estimated probability that the output of a decision tree (e.g., detection of an object) is correct has reached a predetermined threshold level. For example, if after traversing five hundred nodes along a thousand node long path of a decision tree, the probability of correct object detection is ninety-nine point nine (<b>99</b>.<b>9</b>) percent, computational resources could be conserved by terminating early and not traversing the remaining five hundred nodes of the decision tree.</p><p id="p-0050" num="0049">The disclosed technology can more effectively determine one or more characteristics of one or more images including shapes, physical dimensions, colors, and/or textures of objects through use of one or more machine-learned models that allows such object characteristics to be determined more rapidly and with greater precision, speed, and accuracy.</p><p id="p-0051" num="0050">As a result of more effective determinations of one or more characteristics of sensor data (e.g., background characteristics, foreground characteristics, object shapes, and/or object physical dimensions) the disclosed technology can enable improvements in safety through earlier and more accurate object detection. Further, when paired with vehicle systems including steering, propulsion, braking, or notification systems the disclosed technology can respectively change course, increase velocity, reduce velocity, or provide notifications to other vehicles, pedestrians, and/or cyclists.</p><p id="p-0052" num="0051">Accordingly, the disclosed technology provides more effective detection or identification of objects in one or more images by leveraging the operational benefits of a multiple stage classifier. In this way, various technologies including autonomous vehicles can benefit from the improved object detection.</p><p id="p-0053" num="0052">With reference now to <figref idref="DRAWINGS">FIGS. <b>1</b>-<b>9</b></figref>, example embodiments of the present disclosure will be discussed in further detail. <figref idref="DRAWINGS">FIG. <b>1</b></figref> depicts a diagram of an example system according to example embodiments of the present disclosure. As illustrated, a system <b>100</b> can include a plurality of vehicles <b>102</b>; a vehicle <b>104</b>; a vehicle computing system <b>108</b> that includes one or more computing devices <b>110</b>; one or more data acquisition systems <b>112</b>; an autonomy system <b>114</b>; one or more control systems <b>116</b>; one or more human machine interface systems <b>118</b>; other vehicle systems <b>120</b>; a communications system <b>122</b>; a network <b>124</b>; one or more image capture devices <b>126</b>; one or more sensors <b>128</b>; one or more remote computing devices <b>130</b>; a communications network <b>140</b>; and an operations computing system <b>150</b>.</p><p id="p-0054" num="0053">The operations computing system <b>150</b> can be associated with a service provider that provides one or more vehicle services to a plurality of users via a fleet of vehicles that includes, for example, the vehicle <b>104</b>. The vehicle services can include transportation services (e.g., rideshare services), courier services, delivery services, and/or other types of services.</p><p id="p-0055" num="0054">The operations computing system <b>150</b> can include multiple components for performing various operations and functions. For example, the operations computing system <b>150</b> can include and/or otherwise be associated with one or more remote computing devices that are remote from the vehicle <b>104</b>. The one or more remote computing devices can include one or more processors and one or more memory devices. The one or more memory devices can store instructions that when executed by the one or more processors cause the one or more processors to perform operations and functions associated with operation of the vehicle including: receiving object data including portions of sensor data; determining, in a first stage of a multiple stage classification, first stage characteristics of the portions of sensor data based in part on a first machine-learned model; determining, in a second stage of the multiple stage classification, second stage characteristics of the portions of sensor data based in part on a second machine-learned model; and generating, an object output based in part on the first stage characteristics and the second stage characteristics, the object output including indications associated with detection of objects in the portions of sensor data.</p><p id="p-0056" num="0055">For example, the operations computing system <b>150</b> can be configured to monitor and communicate with the vehicle <b>104</b> and/or its users to coordinate a vehicle service provided by the vehicle <b>104</b>. To do so, the operations computing system <b>150</b> can manage a database that includes data including vehicle status data associated with the status of vehicles including the vehicle <b>104</b>. The vehicle status data can include a location of the plurality of vehicles <b>102</b> (e.g., a latitude and longitude of a vehicle), the availability of a vehicle (e.g., whether a vehicle is available to pick-up or drop-off passengers or cargo), or the state of objects external to the vehicle (e.g., the physical dimensions, velocity, acceleration, and/or orientation of objects external to the vehicle).</p><p id="p-0057" num="0056">An indication, record, and/or other data indicative of the state of the one or more objects, including the state (e.g., physical dimensions, velocity, acceleration, color, location, and/or orientation) of the one or more objects, can be stored locally in one or more memory devices of the vehicle <b>104</b>. Furthermore, the vehicle <b>104</b> can provide data indicative of the state of the one or more objects (e.g., objects external to the vehicle) within a predefined distance of the vehicle <b>104</b> to the operations computing system <b>150</b>, which can store an indication, record, and/or other data indicative of the state of the one or more objects within a predefined distance of the vehicle <b>104</b> in one or more memory devices associated with the operations computing system <b>150</b>.</p><p id="p-0058" num="0057">The operations computing system <b>150</b> can communicate with the vehicle <b>104</b> via one or more communications networks including the communications network <b>140</b>. The communications network <b>140</b> can exchange (send or receive) signals (e.g., electronic signals) or data (e.g., data from a computing device) and include any combination of various wired (e.g., twisted pair cable) and/or wireless communication mechanisms (e.g., cellular, wireless, satellite, microwave, and radio frequency) and/or any desired network topology (or topologies). For example, the communications network <b>140</b> can include a local area network (e.g. intranet), wide area network (e.g. Internet), wireless LAN network (e.g., via Wi-Fi), cellular network, a SATCOM network, VHF network, a HF network, a WiMAX based network, and/or any other suitable communications network (or combination thereof) for transmitting data to and/or from the vehicle <b>104</b>.</p><p id="p-0059" num="0058">The vehicle <b>104</b> can be a ground-based vehicle (e.g., an automobile, and/or a truck), an aircraft, a watercraft, and/or another type of vehicle. The vehicle <b>104</b> can be an autonomous vehicle that can perform various actions including driving, navigating, and/or operating, with minimal and/or no interaction from a human driver. The autonomous vehicle <b>104</b> can be configured to operate in one or more modes including, for example, a fully autonomous operational mode, a semi-autonomous operational mode, a park mode, and/or a sleep mode. A fully autonomous (e.g., self-driving) operational mode can be one in which the vehicle <b>104</b> can provide driving and navigational operation with minimal and/or no interaction from a human driver present in the vehicle. A semi-autonomous operational mode can be one in which the vehicle <b>104</b> can operate with some interaction from a human driver present in the vehicle. Park and/or sleep modes can be used between operational modes while the vehicle <b>104</b> performs various actions including waiting to provide a subsequent vehicle service, and/or recharging between operational modes.</p><p id="p-0060" num="0059">The vehicle <b>104</b> can include or be associated with a vehicle computing system <b>108</b>. The vehicle computing system <b>108</b> can include various components for performing various operations and functions. For example, the vehicle computing system <b>108</b> can include one or more computing devices <b>110</b> on-board the vehicle <b>104</b>. The one or more computing devices <b>110</b> can include one or more processors and one or more memory devices, each of which are on-board the vehicle <b>104</b>. The one or more memory devices can store instructions that when executed by the one or more processors cause the one or more processors to perform operations and functions, including taking the vehicle <b>104</b> out-of-service, stopping the motion of the vehicle <b>104</b>, determining the state of one or more objects within a predefined distance of the vehicle <b>104</b>, or generating indications associated with the state of one or more objects within a predefined distance of the vehicle <b>104</b>, as described herein. Further, the vehicle computing system <b>108</b> can perform one or more operations including: receiving object data including portions of sensor data; determining, in a first stage of a multiple stage classification, first stage characteristics of the portions of sensor data based in part on a first machine-learned model; determining, in a second stage of the multiple stage classification, second stage characteristics of the portions of sensor data based in part on a second machine-learned model; and generating, an object output based in part on the first stage characteristics and the second stage characteristics, the object output including indications associated with detection of objects in the portions of sensor data.</p><p id="p-0061" num="0060">The one or more computing devices <b>110</b> can implement, include, and/or otherwise be associated with various other systems on-board the vehicle <b>104</b>. The one or more computing devices <b>110</b> can be configured to communicate with these other on-board systems of the vehicle <b>104</b>. For instance, the one or more computing devices <b>110</b> can be configured to communicate with one or more data acquisition systems <b>112</b>, an autonomy system <b>114</b> (e.g., including a navigation system), one or more control systems <b>116</b>, one or more human machine interface systems <b>118</b>, other vehicle systems <b>120</b>, and/or a communications system <b>122</b>. The one or more computing devices <b>110</b> can be configured to communicate with these systems via a network <b>124</b>. The network <b>124</b> can include one or more data buses (e.g., controller area network (CAN)), on-board diagnostics connector (e.g., OBD-II), and/or a combination of wired and/or wireless communication links. The one or more computing devices <b>110</b> and/or the other on-board systems can send and/or receive data, messages, and/or signals, amongst one another via the network <b>124</b>.</p><p id="p-0062" num="0061">The one or more data acquisition systems <b>112</b> can include various devices configured to acquire data associated with the vehicle <b>104</b>. This can include data associated with the vehicle including one or more of the vehicle's systems (e.g., health data), the vehicle's interior, the vehicle's exterior, the vehicle's surroundings, and/or the vehicle users. The one or more data acquisition systems <b>112</b> can include, for example, one or more image capture devices <b>126</b>. The one or more image capture devices <b>126</b> can include one or more cameras, LIDAR systems), two-dimensional image capture devices, three-dimensional image capture devices, static image capture devices, dynamic (e.g., rotating) image capture devices, video capture devices (e.g., video recorders), lane detectors, scanners, optical readers, electric eyes, and/or other suitable types of image capture devices. The one or more image capture devices <b>126</b> can be located in the interior and/or on the exterior of the vehicle <b>104</b>. The one or more image capture devices <b>126</b> can be configured to acquire image data to be used for operation of the vehicle <b>104</b> in an autonomous mode. For example, the one or more image capture devices <b>126</b> can acquire image data to allow the vehicle <b>104</b> to implement one or more machine vision techniques (e.g., to detect objects in the surrounding environment).</p><p id="p-0063" num="0062">Additionally, or alternatively, the one or more data acquisition systems <b>112</b> can include one or more sensors <b>128</b>. The one or more sensors <b>128</b> can include impact sensors, motion sensors, pressure sensors, mass sensors, weight sensors, volume sensors (e.g., sensors that can determine the volume of an object in liters), temperature sensors, humidity sensors, RADAR, sonar, radios, medium-range and long-range sensors (e.g., for obtaining information associated with the vehicle's surroundings), global positioning system (GPS) equipment, proximity sensors, and/or any other types of sensors for obtaining data indicative of parameters associated with the vehicle <b>104</b> and/or relevant to the operation of the vehicle <b>104</b>. The one or more data acquisition systems <b>112</b> can include the one or more sensors <b>128</b> dedicated to obtaining data associated with a particular aspect of the vehicle <b>104</b>, including, the vehicle's fuel tank, engine, oil compartment, and/or wipers. The one or more sensors <b>128</b> can also, or alternatively, include sensors associated with one or more mechanical and/or electrical components of the vehicle <b>104</b>. For example, the one or more sensors <b>128</b> can be configured to detect whether a vehicle door, trunk, and/or gas cap, is in an open or closed position. In some implementations, the data acquired by the one or more sensors <b>128</b> can help detect other vehicles and/or objects, road conditions (e.g., curves, potholes, dips, bumps, and/or changes in grade), measure a distance between the vehicle <b>104</b> and other vehicles and/or objects.</p><p id="p-0064" num="0063">The vehicle computing system <b>108</b> can also be configured to obtain map data. For instance, a computing device of the vehicle (e.g., within the autonomy system <b>114</b>) can be configured to receive map data from one or more remote computing device including the operations computing system <b>150</b> or the one or more remote computing devices <b>130</b> (e.g., associated with a geographic mapping service provider). The map data can include any combination of two-dimensional or three-dimensional geographic map data associated with the area in which the vehicle can travel including areas the vehicle is currently traveling, has previously traveled, or will travel to in the future.</p><p id="p-0065" num="0064">The data acquired from the one or more data acquisition systems <b>112</b>, the map data, and/or other data can be stored in one or more memory devices on-board the vehicle <b>104</b>. The on-board memory devices can have limited storage capacity. As such, the data stored in the one or more memory devices may need to be periodically removed, deleted, and/or downloaded to another memory device (e.g., a database of the service provider). The one or more computing devices <b>110</b> can be configured to monitor the memory devices, and/or otherwise communicate with an associated processor, to determine how much available data storage is in the one or more memory devices. Further, one or more of the other on-board systems (e.g., the autonomy system <b>114</b>) can be configured to access the data stored in the one or more memory devices.</p><p id="p-0066" num="0065">The autonomy system <b>114</b> can be configured to allow the vehicle <b>104</b> to operate in an autonomous mode. For instance, the autonomy system <b>114</b> can obtain the data associated with the vehicle <b>104</b> (e.g., acquired by the one or more data acquisition systems <b>112</b>). The autonomy system <b>114</b> can also obtain the map data. The autonomy system <b>114</b> can control various functions of the vehicle <b>104</b> based, at least in part, on the acquired data associated with the vehicle <b>104</b> and/or the map data to implement the autonomous mode. For example, the autonomy system <b>114</b> can include various models to perceive road features, signage, and/or objects, people, animals, based on the data acquired by the one or more data acquisition systems <b>112</b>, map data, and/or other data. In some implementations, the autonomy system <b>114</b> can include machine-learned models that use the data acquired by the one or more data acquisition systems <b>112</b>, the map data, and/or other data to help operate the autonomous vehicle. Moreover, the acquired data can help detect other vehicles and/or objects, road conditions (e.g., curves, potholes, dips, bumps, changes in grade, or the like), measure a distance between the vehicle <b>104</b> and other vehicles and/or objects. The autonomy system <b>114</b> can be configured to predict the position and/or movement (or lack thereof) of such elements (e.g., using one or more odometry techniques). The autonomy system <b>114</b> can be configured to plan the motion of the vehicle <b>104</b> based, at least in part, on such predictions. The autonomy system <b>114</b> can implement the planned motion to appropriately navigate the vehicle <b>104</b> with minimal or no human intervention. For instance, the autonomy system <b>114</b> can include a navigation system configured to direct the vehicle <b>104</b> to a destination location. The autonomy system <b>114</b> can regulate vehicle speed, acceleration, deceleration, steering, and/or operation of other components to operate in an autonomous mode to travel to such a destination location.</p><p id="p-0067" num="0066">The autonomy system <b>114</b> can determine a position and/or route for the vehicle <b>104</b> in real-time and/or near real-time. For instance, using acquired data, the autonomy system <b>114</b> can calculate one or more different potential routes (e.g., every fraction of a second). The autonomy system <b>114</b> can then select which route to take and cause the vehicle <b>104</b> to navigate accordingly. By way of example, the autonomy system <b>114</b> can calculate one or more different straight paths (e.g., including some in different parts of a current lane), one or more lane-change paths, one or more turning paths, and/or one or more stopping paths. The vehicle <b>104</b> can select a path based, at last in part, on acquired data, current traffic factors, traveling conditions associated with the vehicle <b>104</b>. In some implementations, different weights can be applied to different criteria when selecting a path. Once selected, the autonomy system <b>114</b> can cause the vehicle <b>104</b> to travel according to the selected path.</p><p id="p-0068" num="0067">The one or more control systems <b>116</b> of the vehicle <b>104</b> can be configured to control one or more aspects of the vehicle <b>104</b>. For example, the one or more control systems <b>116</b> can control one or more access points of the vehicle <b>104</b>. The one or more access points can include features including the vehicle's door locks, trunk lock, hood lock, fuel tank access, latches, and/or other mechanical access features that can be adjusted between one or more states, positions, and/or locations. For example, the one or more control systems <b>116</b> can be configured to control an access point (e.g., door lock) to adjust the access point between a first state (e.g., lock position) and a second state (e.g., unlocked position). Additionally, or alternatively, the one or more control systems <b>116</b> can be configured to control one or more other electrical features of the vehicle <b>104</b> that can be adjusted between one or more states. For example, the one or more control systems <b>116</b> can be configured to control one or more electrical features (e.g., hazard lights, microphone) to adjust the feature between a first state (e.g., off) and a second state (e.g., on).</p><p id="p-0069" num="0068">The one or more human machine interface systems <b>118</b> can be configured to allow interaction between a user (e.g., human), the vehicle <b>104</b> (e.g., the vehicle computing system <b>108</b>), and/or a third party (e.g., an operator associated with the service provider). The one or more human machine interface systems <b>118</b> can include a variety of interfaces for the user to input and/or receive information from the vehicle computing system <b>108</b>. For example, the one or more human machine interface systems <b>118</b> can include a graphical user interface, direct manipulation interface, web-based user interface, touch user interface, attentive user interface, conversational and/or voice interfaces (e.g., via text messages, chatter robot), conversational interface agent, interactive voice response (IVR) system, gesture interface, and/or other types of interfaces. The one or more human machine interface systems <b>118</b> can include one or more input devices (e.g., touchscreens, keypad, touchpad, knobs, buttons, sliders, switches, mouse, gyroscope, microphone, other hardware interfaces) configured to receive user input. The one or more human machine interfaces <b>118</b> can also include one or more output devices (e.g., display devices, speakers, lights) to receive and output data associated with the interfaces.</p><p id="p-0070" num="0069">The other vehicle systems <b>120</b> can be configured to control and/or monitor other aspects of the vehicle <b>104</b>. For instance, the other vehicle systems <b>120</b> can include software update monitors, an engine control unit, transmission control unit, and/or on-board memory devices. The one or more computing devices <b>110</b> can be configured to communicate with the other vehicle systems <b>120</b> to receive data and/or to send to one or more signals. By way of example, the software update monitors can provide, to the one or more computing devices <b>110</b>, data indicative of a current status of the software running on one or more of the on-board systems and/or whether the respective system requires a software update.</p><p id="p-0071" num="0070">The communications system <b>122</b> can be configured to allow the vehicle computing system <b>108</b> (and its one or more computing devices <b>110</b>) to communicate with other computing devices. In some implementations, the vehicle computing system <b>108</b> can use the communications system <b>122</b> to communicate with one or more user devices over the networks. In some implementations, the communications system <b>122</b> can allow the one or more computing devices <b>110</b> to communicate with one or more of the systems on-board the vehicle <b>104</b>. The vehicle computing system <b>108</b> can use the communications system <b>122</b> to communicate with the operations computing system <b>150</b> and/or the one or more remote computing devices <b>130</b> over the networks (e.g., via one or more wireless signal connections). The communications system <b>122</b> can include any suitable components for interfacing with one or more networks, including for example, transmitters, receivers, ports, controllers, antennas, or other suitable components that can help facilitate communication with one or more remote computing devices that are remote from the vehicle <b>104</b>.</p><p id="p-0072" num="0071">In some implementations, the one or more computing devices <b>110</b> on-board the vehicle <b>104</b> can obtain vehicle data indicative of one or more parameters associated with the vehicle <b>104</b>. The one or more parameters can include information, including health and maintenance information, associated with the vehicle <b>104</b>, the vehicle computing system <b>108</b>, and/or one or more of the on-board systems. For example, the one or more parameters can include fuel level, engine conditions, tire pressure, conditions associated with the vehicle's interior, conditions associated with the vehicle's exterior, mileage, time until next maintenance, time since last maintenance, available data storage in the on-board memory devices, a charge level of an energy storage device in the vehicle <b>104</b>, current software status, needed software updates, and/or other heath and maintenance data of the vehicle <b>104</b>.</p><p id="p-0073" num="0072">At least a portion of the vehicle data indicative of the parameters can be provided via one or more of the systems on-board the vehicle <b>104</b>. The one or more computing devices <b>110</b> can be configured to request the vehicle data from the on-board systems on a scheduled and/or as-needed basis. In some implementations, one or more of the on-board systems can be configured to provide vehicle data indicative of one or more parameters to the one or more computing devices <b>110</b> (e.g., periodically, continuously, as-needed, as requested). By way of example, the one or more data acquisitions systems <b>112</b> can provide a parameter indicative of the vehicle's fuel level and/or the charge level in a vehicle energy storage device. In some implementations, one or more of the parameters can be indicative of user input. For example, the one or more human machine interfaces <b>118</b> can receive user input (e.g., via a user interface displayed on a display device in the vehicle's interior). The one or more human machine interfaces <b>118</b> can provide data indicative of the user input to the one or more computing devices <b>110</b>. In some implementations, the one or more remote computing devices <b>130</b> can receive input and can provide data indicative of the user input to the one or more computing devices <b>110</b>. The one or more computing devices <b>110</b> can obtain the data indicative of the user input from the one or more remote computing devices <b>130</b> (e.g., via a wireless communication).</p><p id="p-0074" num="0073">The one or more computing devices <b>110</b> can be configured to determine the state of the vehicle <b>104</b> and the environment around the vehicle <b>104</b> including the state of one or more objects external to the vehicle including pedestrians, cyclists, motor vehicles (e.g., trucks, and/or automobiles), roads, waterways, and/or buildings. Further, the one or more computing devices <b>110</b> can be configured to determine one or more physical characteristics of the one or more objects including physical dimensions of the one or more objects (e.g., shape, length, width, and/or height of the one or more objects). The one or more computing devices <b>110</b> can determine an estimated set of physical dimensions and/or orientations of the one or more objects, including portions of the one or more objects that are not detected by the one or more sensors <b>128</b>, through use of one or more machine-learned models. Further, the one or more computing devices <b>110</b> can perform multiple-stage detection and/or recognition of objects based in part on use of the one or more machine-learned models.</p><p id="p-0075" num="0074"><figref idref="DRAWINGS">FIG. <b>2</b></figref> depicts a diagram of an example multiple stage classifier system according to example embodiments of the present disclosure. As illustrated, a multiple stage classifier system <b>200</b> can include one or more sensor devices <b>202</b>; an interconnect <b>204</b>/<b>206</b>/<b>208</b>; a first stage computing system <b>210</b>; a field programmable gate array (FPGA) <b>212</b>; one or more programmable logic blocks and interconnects <b>214</b>; a memory <b>216</b>; data <b>218</b>; instructions <b>220</b>; a communication interface <b>222</b>; a second stage computing system <b>230</b>; a central processing unit (CPU) <b>232</b>; a memory <b>236</b>; data <b>238</b>; instructions <b>240</b>; a communication interface <b>242</b>; and one or more output devices <b>250</b>. Further, the multiple stage classifier system <b>200</b> can perform one or more functions including receiving object data including portions of sensor data; determining, in a first stage of a multiple stage classification, first stage characteristics of the portions of sensor data based in part on a first machine-learned model; determining, in a second stage of the multiple stage classification, second stage characteristics of the portions of sensor data based in part on a second machine-learned model; and generating, an object output based in part on the first stage characteristics and the second stage characteristics, the object output including indications associated with detection of objects in the portions of sensor data.</p><p id="p-0076" num="0075">In this example, the multiple stage classifier system <b>200</b> includes two computing systems, the first stage computing system <b>210</b> and the second stage computing system <b>230</b>. However, in other embodiments the multiple stage classifier system <b>200</b> can include three or more computing systems, which can include any of the features, components, and/or devices included in the first stage computing system <b>210</b> and the second stage computing system <b>230</b>.</p><p id="p-0077" num="0076">The multiple stage classifier system <b>200</b> can include one or more sensor devices <b>202</b>. The one or more sensor devices <b>202</b> can include one or more physical sensors that can generate one or more sensor outputs (e.g., sensor data) based in part on the detection of an environment including one or more objects. The one or more physical sensors can include one or more LIDAR devices, one or more cameras, one or more RADAR devices, one or more sonar devices, one or more image sensors, and/or one or more thermal imaging devices. Further, the one or more sensor devices <b>202</b> can include one or more simulated sensors that can generate one or more simulated sensor outputs (e.g., simulated sensor data) based in part on one or more simulated objects (e.g., simulated objects based in part on data structures generated by a computing device including the vehicle computing system <b>108</b>, the one or more remote computing devices <b>130</b>, and/or the operations computing system <b>150</b>).</p><p id="p-0078" num="0077">The object data output by the one or more sensor devices <b>202</b> can be used in the detection and/or recognition of one or more objects including one or more pedestrians (e.g., one or more persons standing, laying down, sitting, squatting, crouching, climbing, running, and/or walking); one or more other vehicles (e.g., motor vehicles including automobiles, trucks, buses, trolleys, trams, motorcycles, mopeds, aircraft, helicopters, boats, amphibious vehicles, and/or trains); one or more cyclists (e.g., one or more persons sitting and/or riding on a bicycle); transportation infrastructure (e.g., roads, streets, railroads, sidewalks, high-ways, parking lots, and/or pavement); and/or one or more buildings (e.g., houses, office buildings, stadia, and/or apartment buildings).</p><p id="p-0079" num="0078">Further, the object data output by the one or more sensor devices <b>202</b> can include a set of three-dimensional points (e.g., x, y, and z coordinates) associated with one or more physical dimensions (e.g., the length, width, and/or height) of the one or more objects in the one or more images. One or more portions of the sensor data and/or the associated object data can be used to determine physical properties, attributes, and/or characteristics (e.g., visual properties and/or characteristics) of the one or more objects including the shape, texture, brightness, saturation, and/or physical dimensions (e.g., length, width, and/or height), of the one or more objects.</p><p id="p-0080" num="0079">The object data generated by the one or more sensor devices <b>202</b> can be output as object data that includes one or more portions of the sensor data and/or one or more portions of the simulated sensor data. The one or more sensor devices <b>202</b> can generate object data that includes one or more two-dimensional images (e.g., two-dimensional images captured by one or more cameras) and/or three-dimensional images (e.g., three-dimensional point clouds captured by a LIDAR device). One or more portions of the object data can be sent to one or more computing devices and/or computing systems via one or more communication networks and/or interconnects including the interconnect <b>204</b> which can be used to exchange (e.g., send and/or receive) one or more signals and/or data including signals and/or data between the one or more sensor devices <b>202</b>, the first stage computing system <b>210</b>, and/or the second stage computing system <b>230</b>.</p><p id="p-0081" num="0080">The first stage computing system <b>210</b> can perform various operations and/or functions including sending, receiving, analyzing, detecting, recognizing, and/or processing one or more signals and/or data including the object data. For example, the first stage computing system <b>210</b> can receive (e.g., receive via the interconnect <b>204</b>) object data from the one or more sensor devices <b>202</b>, perform one or more operations (e.g., detect and/or recognize one or more objects) based on the object data, and send (e.g., send via the interconnect <b>206</b>) the object data to the second stage computing device <b>230</b>.</p><p id="p-0082" num="0081">The first stage computing system <b>210</b> can include one or more computing devices including the one or more FPGAs <b>212</b> and the memory <b>216</b>. The one or more FPGAs <b>212</b> can include any suitable processing device (e.g., a processor core, a microprocessor, an ASIC, a FPGA, a graphics processing unit, a digital signal processor, a controller, and/or a microcontroller) and can include one processor or a plurality of processors that are operatively connected. In this example, the one or more FPGAs <b>212</b> can include the one or more programmable logic blocks and interconnects <b>214</b> which can be configured according to the type of operations (e.g., processing data including the detection and/or recognition of objects) to be performed by the one or more FPGAs <b>212</b>. In other embodiments, the one or more processors <b>212</b> can be configured not to include or not to use the one or more programmable logic blocks and interconnects <b>214</b>.</p><p id="p-0083" num="0082">The memory <b>216</b> can include one or more non-transitory computer-readable storage media, including RAM, ROM, EEPROM, EPROM, NVRAM, one or more memory devices, flash memory devices, and/or combinations thereof. The memory <b>216</b> can store information that can be accessed by the one or more FPGAs <b>212</b>. For instance, the memory <b>216</b> (e.g., one or more non-transitory computer-readable storage mediums, memory devices) can store the data <b>218</b> that can be obtained, received, accessed, written, manipulated, created, and/or stored. The data <b>218</b> can include the object data from the one or more sensor devices <b>202</b>, data identifying detected and/or classified objects including current object states and predicted object locations and/or trajectories, motion plans, classification models, and/or rules, as described herein. In some implementations, the first stage computing system <b>210</b> can obtain data from one or more memory devices that are remote from the first stage computing system <b>210</b> including, for example, the one or more sensor devices <b>202</b>, and/or the second stage computing system <b>230</b>.</p><p id="p-0084" num="0083">The memory <b>216</b> can also store computer-readable instructions <b>220</b> that can be executed by the one or more processors <b>212</b>. The instructions <b>220</b> can be software written in any suitable programming language or can be implemented in hardware. Additionally, or alternatively, the instructions <b>220</b> can be executed in logically and/or virtually separate threads on the one or more processors <b>212</b>.</p><p id="p-0085" num="0084">Further, the data <b>218</b> and/or the instructions <b>220</b> stored in the memory <b>216</b> can include one or more machine-learned models including one or more machine-learned models that can be used to generate classified object labels based on the object data. In some embodiments, the classified object labels associated with the one or more objects can be generated in the same format as the classified object labels generated by the machine-learned model.</p><p id="p-0086" num="0085">For example, the first stage computing system <b>210</b> can include, use, and/or operate a machine-learned object detection and recognition model stored in the memory <b>216</b>. The machine-learned object detection and recognition model can include one or more models including, neural networks (e.g., deep neural networks), or other multi-layer non-linear models.</p><p id="p-0087" num="0086">Neural networks can include convolutional neural networks, recurrent neural networks (e.g., long short-term memory recurrent neural networks), feed-forward neural networks, and/or other forms of neural networks. Supervised training techniques can be performed to train the machine-learned object detection and recognition model to detect, recognize, and/or classify one or more objects in the object data. In some implementations, training data for the machine-learned object detection and recognition model can be based at least in part on the predicted detection outcomes determined using a rules-based model that can be used to train the machine-learned object detection and recognition model to detect, recognize, and/or classify one or more objects associated with the object data. Further, the training data can be used to train the machine-learned object detection and recognition model offline.</p><p id="p-0088" num="0087">In some embodiments, the first stage computing system <b>210</b> can input data into the machine-learned object detection and recognition model and receive an output. For instance, the first stage computing system <b>210</b> can obtain data indicative of a machine-learned object detection and recognition model from the one or more remote computing devices that store various machine-learned object detection and recognition models. The input data can include the data associated with the one or more objects including one or more vehicles, pedestrians, cyclists, buildings, and/or environments associated with the one or more objects (e.g., roads, bodies of water, mountains, hills, and/or foliage). In some embodiments, the input data can include data associated with a visual descriptor including color, brightness, and/or saturation information associated with the one or more objects.</p><p id="p-0089" num="0088">Further, the input data can include the object data, prediction data (e.g., data predicting the state and/or location of the one or more objects), a motion plan (e.g., the motion plan for a vehicle to navigate relative to the one or more objects), and/or map data associated with the one or more objects.</p><p id="p-0090" num="0089">The machine-learned object detection and recognition model can process the input data to detect, recognize, and/or classify the one or more objects. Moreover, the machine-learned object detection and recognition model can predict one or more interactions for the one or more objects. Further, the first stage computing system <b>210</b> can obtain an output from the machine-learned object detection and recognition model. The output from the machine-learned object detection and recognition model can be indicative of the one or more predicted detections, recognitions, and/or classifications of the one or more objects. In some implementations, the output can also be indicative of a probability associated with each respective detection, recognition, and/or classification. Further, the machine-learned object detection and recognition model can process the input data to determine which of the one or more portions of an input image in the input data is background (e.g., an area in an input image included in the input data that does not include one or more objects of interest) or foreground (e.g., an area in an input image included in the input data that includes one or more objects that are of interest). For example, the machine-learned object detection and recognition model can determine, in a first stage of processing the input data performed by the first stage computing system <b>210</b>, that an area of an input image is determined to be background (e.g., a portion of the input image that includes an object that is the sky or the ground) and the machine-learned object detection and recognition model can determine in a second stage of processing the input data performed by the second stage computing system <b>230</b>, that an area of an input image is determined to be foreground (e.g., a portion of the input image that includes an object that is a vehicle or a pedestrian).The first stage computing system <b>210</b> can also include a communication interface <b>222</b> that can be used to communicate with one or more systems or devices, including systems or devices that are remote from the first stage computing system <b>210</b>. The communication interface <b>222</b> can include any circuits, components, and/or software, for communicating with one or more networks. In some implementations, the communication interface <b>222</b> can include, for example, one or more of a communications controller, receiver, transceiver, transmitter, port, conductors, software and/or hardware for communicating data. Further, the first stage computing system <b>210</b> can send one or more signals and/or data (e.g., one or more signals associated with the object data) to one or more computing systems including the second stage computing system <b>230</b> and/or the one or more output devices <b>250</b>.</p><p id="p-0091" num="0090">The second stage computing system <b>230</b> can perform various operations and/or functions including sending, receiving, analyzing, detecting, recognizing, and/or processing one or more signals and/or data including the object data. For example, the second stage computing system <b>230</b> can receive (e.g., receive via the interconnect <b>204</b>) object data from the first stage computing system <b>210</b>, perform one or more operations (e.g., detect and/or recognize one or more objects) based on the object data, and send (e.g., send via the interconnect <b>208</b>) one or more signals associated with the object data to the one or more output devices <b>250</b>.</p><p id="p-0092" num="0091">The second stage computing system <b>230</b> can include one or more computing devices including the one or more processors <b>232</b> and the memory <b>236</b>. The one or more processors <b>232</b> can include any suitable processing device (e.g., a processor core, a microprocessor, an ASIC, a FPGA, a graphics processing unit, a digital signal processor, a controller, and/or a microcontroller) and can include one processor or a plurality of processors that are operatively connected. In some embodiments, the one or more processors <b>232</b> can include one or more programmable logic blocks and interconnects (not shown) which can be configured according to the type of operations (e.g., processing data including the detection and/or recognition of objects) to be performed by the one or more processors <b>232</b>.</p><p id="p-0093" num="0092">The memory <b>236</b> can include one or more non-transitory computer-readable storage media, including RAM, ROM, EEPROM, EPROM, NVRAM, one or more memory devices, flash memory devices, and/or combinations thereof. The memory <b>236</b> can store information that can be accessed by the one or more processors <b>232</b>. For instance, the memory <b>236</b> (e.g., one or more non-transitory computer-readable storage mediums, memory devices) can store the data <b>238</b> that can be obtained, received, accessed, written, manipulated, created, and/or stored. The data <b>238</b> can include the object data from the one or more sensor devices <b>202</b>, the first stage computing system <b>210</b>, data identifying detected and/or classified objects including current object states and predicted object locations and/or trajectories, motion plans, classification models, rules, as described herein. In some implementations, the second stage computing system <b>230</b> can obtain data from one or more memory devices that are remote from the second stage computing system <b>230</b> including, for example, the one or more sensor devices <b>202</b>, and/or the first stage computing system <b>210</b>.</p><p id="p-0094" num="0093">The memory <b>236</b> can also store computer-readable instructions <b>240</b> that can be executed by the one or more processors <b>232</b>. The instructions <b>240</b> can be software written in any suitable programming language or can be implemented in hardware. Additionally, or alternatively, the instructions <b>240</b> can be executed in logically and/or virtually separate threads on the one or more processors <b>232</b>.</p><p id="p-0095" num="0094">Further, the data <b>238</b> and/or the instructions <b>240</b> stored in the memory <b>236</b> can include one or more machine-learned models including one or more machine-learned models that can be used to generate classified object labels based on the object data and/or data associated with the object data (e.g., data received from the first stage computing system <b>210</b>). In some embodiments, the classified object labels associated with the one or more objects can be generated in the same format as the classified object labels generated by the machine-learned model.</p><p id="p-0096" num="0095">For example, the second stage computing system <b>230</b> can include, use, and/or operate a machine-learned object detection and recognition model stored in the memory <b>236</b>. The machine-learned object detection and recognition model can include one or more models including, neural networks (e.g., deep neural networks), or other multi-layer non-linear models.</p><p id="p-0097" num="0096">Neural networks can include convolutional neural networks, recurrent neural networks (e.g., long short-term memory recurrent neural networks), feed-forward neural networks, and/or other forms of neural networks. Supervised training techniques can be performed to train the machine-learned object detection and recognition model to detect, recognize, and/or classify one or more objects in the object data. In some implementations, training data for the machine-learned object detection and recognition model can be based at least in part on the predicted detection outcomes determined using a rules-based model that can be used to train the machine-learned object detection and recognition model to detect, recognize, and/or classify one or more objects associated with the object data. Further, the training data can be used to train the machine-learned object detection and recognition model offline.</p><p id="p-0098" num="0097">In some embodiments, the second stage computing system <b>230</b> can input data into the machine-learned object detection and recognition model and receive an output. For instance, the second stage computing system <b>230</b> can obtain data indicative of a machine-learned object detection and recognition model from the one or more remote computing devices that store various machine-learned object detection and recognition models. The input data can include the data associated with the one or more objects including one or more vehicles, pedestrians, cyclists, buildings, and/or environments associated with the one or more objects (e.g., roads, bodies of water, mountains, hills, and/or foliage). Further, the input data can include the object data, prediction data (e.g., data predicting the state and/or location of the one or more objects), a motion plan (e.g., the motion plan for the one or more object), and/or map data associated with the one or more objects.</p><p id="p-0099" num="0098">The machine-learned object detection and recognition model can process the input data to detect, recognize, and/or classify the one or more objects. Moreover, the machine-learned object detection and recognition model can predict one or more interactions for the one or more objects. Further, the second stage computing system <b>230</b> can obtain an output from the machine-learned object detection and recognition model. The output from the machine-learned object detection and recognition model can be indicative of the one or more predicted detections, recognitions, and/or classifications of the one or more objects. In some implementations, the output can also be indicative of a probability associated with each respective detection, recognition, and/or classification.</p><p id="p-0100" num="0099">The second stage computing system <b>230</b> can also include a communication interface <b>242</b> that can be used to communicate with one or more systems or devices, including systems or devices that are remote from the second stage computing system <b>230</b>. The communication interface <b>242</b> can include any circuits, components, and/or software, for communicating with one or more networks. In some implementations, the communication interface <b>242</b> can include, for example, one or more of a communications controller, receiver, transceiver, transmitter, port, conductors, software and/or hardware for communicating data. Further, the second stage computing system <b>230</b> can send one or more signals and/or data (e.g., one or more signals associated with the object data) to one or more computing systems including the first stage computing system <b>210</b> and/or the one or more output devices <b>250</b>.</p><p id="p-0101" num="0100">The one or more output devices <b>250</b> can receive one or more signals or data from one or more computing devices or computing systems including the first stage computing system <b>210</b> and/or the second stage computing system <b>230</b>. The one or more output devices <b>250</b> can receive (e.g., receive one or more signals from the second stage computing system <b>230</b> via the interconnect <b>208</b>) one or more signals associated with the object data including one or more signals that are used to produce one or more visual images associated with the object data and/or output associated with the output data (e.g., the results of processing the object data by the first stage computing system <b>210</b> and/or the second stage computing system <b>230</b>). For example, the one or more output devices <b>250</b> can, based on one or more signals from the second stage computing system <b>230</b>, generate visual output including an image and indications of portions of the image that include one or more objects.</p><p id="p-0102" num="0101">The one or more output devices <b>250</b> can include one or more devices that are used to generate a representation associated with one or more signals and/or data received by the one or more output devices <b>250</b>. For example, the one or more output devices <b>250</b> can include one or more display devices (e.g., organic light emitting diode display devices, liquid crystal display devices, cathode ray tube display devices, and/or plasma display devices); one or more audio output devices (e.g., loud speakers); and/or one or more haptic output devices (e.g., piezoelectric devices that can produce one or more vibrations).</p><p id="p-0103" num="0102"><figref idref="DRAWINGS">FIG. <b>3</b></figref> depicts an example of object detection from an overhead view using a multiple stage classifier according to example embodiments of the present disclosure. The output can be based in part on the detection, recognition, and/or processing of one or more portions of an environment by one or more devices (e.g., one or more computing devices) or systems including, for example, the vehicle <b>104</b>, the vehicle computing system <b>108</b>, or the operations computing system <b>150</b>, shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>; or the multiple stage classifier system <b>200</b> shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. Moreover, the detection, recognition, and/or processing of one or more portions of an environment can be implemented as an algorithm on the hardware components of one or more devices or systems (e.g., the vehicle <b>104</b>, the vehicle computing system <b>108</b>, and/or the operations computing system <b>150</b>, shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref> or the multiple stage classifier system <b>200</b> shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>). Further, the multiple stage classifier in <figref idref="DRAWINGS">FIG. <b>3</b></figref> can perform one or more operations including receiving object data including portions of sensor data; determining, in a first stage of a multiple stage classification, first stage characteristics of the portions of sensor data based in part on a first machine-learned model; determining, in a second stage of the multiple stage classification, second stage characteristics of the portions of sensor data based in part on a second machine-learned model; and generating, an object output based in part on the first stage characteristics and the second stage characteristics, the object output including indications associated with detection of objects in the portions of sensor data. As illustrated, <figref idref="DRAWINGS">FIG. <b>3</b></figref> shows an output image <b>300</b>; an object <b>310</b>; a bounding shape <b>312</b>; an object <b>314</b> (e.g., a pedestrian); an object <b>320</b> (e.g., a road); an object <b>322</b> (e.g., a sidewalk); and a segment <b>324</b>.</p><p id="p-0104" num="0103">The output image <b>300</b> depicts an image which can be based on, or associated with object data (e.g., object data from the multiple stage classifier system <b>200</b>). For example, the output image <b>300</b> can be based in part on sensor outputs from one or more image sensors (e.g., one or more cameras) including the one or more sensor devices <b>202</b>. As shown, the output image <b>300</b> includes an object <b>310</b> (e.g., an autonomous vehicle), a bounding shape <b>312</b> (e.g., a bounding shape around the object <b>310</b>), an object <b>314</b> (e.g., a pedestrian), an object <b>320</b> (e.g., a road), and an object <b>322</b> (e.g., a sidewalk).</p><p id="p-0105" num="0104">As shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the output image <b>300</b> can be divided into a plurality of segments including the segment <b>324</b>, although in other embodiments the output image <b>300</b> can include more segments, fewer segments, or no segments at all. In some embodiments, different segments within the output image <b>300</b> can be obtained using a sliding window having a predetermined segment size. In some embodiments, the output image can be resized into a plurality of representations of data having different scales. By analyzing multiple data representations using a sliding window of fixed size, objects partially captured by a sliding window in some image representations can be fully captured by a sliding window in one or more other image representations.</p><p id="p-0106" num="0105">Each of the plurality of segments can be analyzed (e.g., processed by using a machine-learned classification model including the machine-learned object detection and recognition model used by the multiple stage classifier system <b>200</b>). For example, first stage computing system <b>210</b> within a multiple stage classifier system <b>200</b> can process each of the plurality of segments within output image <b>300</b> to determine whether each segment corresponds to a portion of background (e.g., a segment that does not include one or more objects of interest) or foreground (e.g., a segment that includes one or more objects that are of interest). In this example, the segment <b>324</b> is empty to indicate that the segment <b>324</b> contains a background portion. In other embodiments, the segments that are foreground and/or background can be indicated by different patterns, shapes, or colors. Further, in some embodiments, determination of whether each of the plurality of segments is foreground or background can be based in part on map data which can indicate for instance, the portions of a map associated with an area that includes streets, buildings, and other areas that can be classified as background.</p><p id="p-0107" num="0106">Referring still to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, for segments of output image <b>300</b> that are determined by a first stage computing system (e.g., first stage computing system <b>210</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>) to correspond to foreground portions, such segments can be provided to a second stage computing system (e.g., second stage computing system <b>230</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>) for further processing. Second stage computing system <b>230</b> within multiple stage classifier system <b>200</b> can process each of the plurality of segments determined to correspond to foreground portions to more particularly detect and classify particular objects of interest (e.g., object <b>310</b> (e.g., an autonomous vehicle), object <b>314</b> (e.g., a pedestrian)</p><p id="p-0108" num="0107"><figref idref="DRAWINGS">FIG. <b>4</b></figref> depicts an example of object detection and use of decision trees by a multiple stage classifier according to example embodiments of the present disclosure. The output can be based in part on the detection and/or processing of one or more portions of an environment by one or more devices (e.g., one or more computing devices) or systems including, for example, the vehicle <b>104</b>, the vehicle computing system <b>108</b>, and/or the operations computing system <b>150</b>, shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>; or the multiple stage classifier system <b>200</b> shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. Moreover, the detection and processing of one or more portions of an environment can be implemented as an algorithm on the hardware components of one or more devices or systems (e.g., the vehicle <b>104</b>, the vehicle computing system <b>108</b>, and/or the operations computing system <b>150</b>, shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>) to, for example, determine the physical dimensions, position, shape, and/or orientation of objects. Further, the multiple stage classifier in <figref idref="DRAWINGS">FIG. <b>4</b></figref> can perform one or more operations including receiving object data including portions of sensor data; determining, in a first stage of a multiple stage classification, first stage characteristics of the portions of sensor data based in part on a first machine-learned model; determining, in a second stage of the multiple stage classification, second stage characteristics of the portions of sensor data based in part on a second machine-learned model; and generating, an object output based in part on the first stage characteristics and the second stage characteristics, the object output including indications associated with detection of objects in the portions of sensor data. As illustrated, <figref idref="DRAWINGS">FIG. <b>4</b></figref> shows an output image <b>400</b>; an object <b>410</b>; a bounding area <b>412</b>; a classification model <b>414</b>; an object <b>420</b>; a bounding area <b>422</b>; a classification model <b>424</b>, and a sidewalk area <b>430</b>.</p><p id="p-0109" num="0108">The output image <b>400</b> depicts an image which can be based on, or associated with object data (e.g., object data from the multiple stage classifier system <b>200</b>). As shown, the output image includes an object <b>410</b> (e.g., a vehicle), a bounding area <b>412</b> (e.g., an area surrounding the object <b>410</b>), a classification model <b>414</b> (e.g., a classification model including one or more of the machine-learned object detection and recognition models used by the first stage computing system <b>210</b> of the multiple stage classifier system <b>200</b>), an object <b>420</b> (e.g., an object that is not a vehicle), a bounding area <b>422</b> (e.g., an area surrounding the object <b>420</b>), and a classification model <b>424</b> (e.g., a classification model including one or more of the machine-learned object detection and recognition models used by the second stage computing system of the multiple stage classifier system <b>200</b>).</p><p id="p-0110" num="0109">When the portion of the output image <b>400</b> within the bounding area <b>412</b> is provided as input to the classification model <b>414</b>, the classification model <b>414</b> can generate an output indicating that the object detected within the boundary area <b>412</b> is not a background (e.g., the object detected within the boundary area <b>412</b> is of interest). When the portion of the output image <b>400</b> within the bounding area <b>412</b> is provided as input to the classification model <b>424</b>, the classification model <b>424</b> can generate an output indicating that the object detected within the boundary area <b>412</b> is a vehicle.</p><p id="p-0111" num="0110">In this example, the object <b>420</b> (e.g., the vehicle) is located on the sidewalk area <b>430</b>, which according to a heat map associated with the output image <b>400</b> is a portion of the output image <b>400</b> that is less likely to be occupied by a vehicle. When the portion of the output image <b>400</b> within the bounding area <b>422</b> is provided as input to the classification model <b>414</b>, the classification model <b>414</b> can generate an output indicating that the object detected within the boundary area <b>422</b> is not background (e.g., the object detected within the boundary area <b>422</b> is of interest). When the portion of the output image <b>400</b> within the bounding area <b>422</b> is provided as input to the classification model <b>424</b>, the classification model <b>424</b> can generate an output indicating that the object detected within the boundary area <b>422</b> is a vehicle. In this example, the heat map decreased the probability of a vehicle being within the bounding area <b>422</b>, however, the other characteristics of the object <b>420</b> determined by the multiple stage classifier system <b>200</b> can result in the determination that the object <b>420</b> is a vehicle.</p><p id="p-0112" num="0111"><figref idref="DRAWINGS">FIG. <b>5</b></figref> depicts a second example of object detection and use of a decision tree by a multiple stage classifier according to example embodiments of the present disclosure. The output can be based in part on the detection and/or processing of one or more portions of an environment by one or more devices (e.g., one or more computing devices) or systems including, for example, the vehicle <b>104</b>, the vehicle computing system <b>108</b>, and/or the operations computing system <b>150</b>, shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>; or the multiple stage classifier system <b>200</b> shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. Moreover, the detection and processing of one or more portions of an environment can be implemented as an algorithm on the hardware components of one or more devices or systems (e.g., the vehicle <b>104</b>, the vehicle computing system <b>108</b>, and/or the operations computing system <b>150</b>, shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>) to, for example, determine the physical dimensions, position, shape, and/or orientation of objects. Further, the multiple stage classifier in <figref idref="DRAWINGS">FIG. <b>5</b></figref> can perform one or more operations including receiving object data including portions of sensor data; determining, in a first stage of a multiple stage classification, first stage characteristics of the portions of sensor data based in part on a first machine-learned model; determining, in a second stage of the multiple stage classification, second stage characteristics of the portions of sensor data based in part on a second machine-learned model; and generating, an object output based in part on the first stage characteristics and the second stage characteristics, the object output including indications associated with detection of objects in the portions of sensor data. As illustrated, <figref idref="DRAWINGS">FIG. <b>5</b></figref> shows an output image <b>500</b>, an object <b>510</b>; a bounding area <b>512</b>; a classification model <b>514</b>; an object <b>520</b>; a bounding area <b>522</b>; and a classification model <b>524</b>.</p><p id="p-0113" num="0112">The output image <b>500</b> depicts an image which can be based on, or associated with object data (e.g., object data from the multiple stage classifier system <b>200</b>). As shown, the output image includes an object <b>510</b> (e.g., a pedestrian), a bounding area <b>512</b> (e.g., an area surrounding the object <b>510</b>), a classification model <b>514</b> (e.g., a classification model including one or more of the machine-learned object detection and recognition models used by first stage computing system <b>210</b> of the multiple stage classifier system <b>200</b>), an object <b>520</b> (e.g., a pedestrian), a bounding area <b>522</b> (e.g., an area surrounding the object <b>520</b>), and a classification model <b>524</b> (e.g., a classification model including one or more of the machine-learned object detection and recognition models used by the second stage computing system <b>230</b> of the multiple stage classifier system <b>200</b>).</p><p id="p-0114" num="0113">When the portion of the output image <b>500</b> within the bounding area <b>512</b> is provided as input to the classification model <b>514</b>, the classification model <b>514</b> can generate an output indicating that the object detected within the boundary area <b>512</b> is foreground. Further, when the portion of the output image <b>500</b> within the bounding area <b>512</b> is provided as input to the classification model <b>524</b>, the classification model <b>524</b> can generate an output indicating that the object detected within the boundary area <b>512</b> is a pedestrian.</p><p id="p-0115" num="0114">Furthermore, in this example, the object <b>520</b> is located on a portion of the output image, that according to a map (e.g., a map of the geographical area associated with the output image <b>500</b>) is a portion (e.g., a sidewalk portion) of the output image <b>500</b> that is more likely to be occupied by a pedestrian. When the portion of the output image <b>500</b> within the bounding area <b>522</b> is provided as input to the classification model <b>514</b>, the classification model <b>514</b> can generate an output indicating that the object detected within the boundary area <b>522</b> is foreground. Further, when the portion of the output image <b>500</b> within the bounding area <b>522</b> is provided as input to the classification model <b>524</b>, the classification model <b>524</b> can generate an output indicating that the object detected within the boundary area <b>522</b> is a pedestrian. In this example, the map associated with the output image <b>500</b> increased the probability of a pedestrian being within the bounding area <b>522</b>, and, in conjunction with the other characteristics of the object <b>520</b> determined by the multiple stage classifier system <b>200</b>, the classification model <b>514</b> has output the result that the object <b>520</b> is foreground and the classification model <b>524</b> has output the result that the object <b>520</b> is a pedestrian.</p><p id="p-0116" num="0115"><figref idref="DRAWINGS">FIG. <b>6</b></figref> depicts a flow diagram of an example method of object detection and recognition according to example embodiments of the present disclosure. One or more portions of the method <b>600</b>, illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, can be implemented by one or more devices (e.g., one or more computing devices) or systems including, for example, the vehicle <b>104</b>, the vehicle computing system <b>108</b>, or the operations computing system <b>150</b>, shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>; or the multiple stage classifier system <b>200</b>, shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. Moreover, one or more portions of the method <b>600</b> can be implemented as an algorithm on the hardware components of the devices described herein (e.g., as in <figref idref="DRAWINGS">FIG. <b>1</b></figref>) to, for example, perform multiple-stage detection and/or recognition of objects including receiving object data, determining characteristics of one or more objects, and generating object output associated with detection of one or more objects. <figref idref="DRAWINGS">FIG. <b>6</b></figref> depicts elements performed in a particular order for purposes of illustration and discussion. Those of ordinary skill in the art, using the disclosures provided herein, will understand that the elements of any of the methods discussed herein can be adapted, rearranged, expanded, omitted, combined, and/or modified in various ways without deviating from the scope of the present disclosure.</p><p id="p-0117" num="0116">At <b>602</b>, the method <b>600</b> can include receiving object data including one or more portions of sensor data. For example, the first stage computing system <b>210</b> of the multiple stage classifier system <b>200</b> can receive object data from one or more computing devices and/or one or more sensor devices including the one or more sensor devices <b>202</b>.</p><p id="p-0118" num="0117">In some embodiments, the one or more portions of sensor data (e.g., one or more portions or segments of one or more images associated with the sensor data) can be based in part on sensor output from one or more sensors (e.g., physical sensors that detect actual objects and/or phenomena) and/or one or more simulated sensors (e.g., simulated sensor outputs generated by one or more computing devices). The one or more sensors can include one or more light detection and ranging devices (LIDAR), one or more cameras, one or more radar devices, one or more sonar devices, and/or one or more thermal imaging devices.</p><p id="p-0119" num="0118">For example, the one or more portions of sensor data and/or the object data can include two-dimensional images including images captured by one or more cameras and/or three-dimensional point clouds captured by a LIDAR device. The one or more portions of sensor data can be analyzed to detect and/or recognize one or more objects including one or more pedestrians; one or more other vehicles; transportation infrastructure; one or more cyclists; and/or one or more buildings. Further, the object data can include a set of three-dimensional points (e.g., x, y, and z coordinates) associated with one or more physical dimensions (e.g., the length, width, and/or height) of the one or more objects in the one or more images.</p><p id="p-0120" num="0119">At <b>604</b>, the method <b>600</b> can include determining one or more characteristics of the one or more portions of sensor data (e.g., the one or more portions of sensor data in <b>602</b>). In particular, the method <b>600</b> can include determining, in a first stage of a multiple stage classification (e.g., classification including two or more stages) which can include the use of one or more hardware components (e.g., a configurable hardware component which can include a FPGA), one or more first stage characteristics of the one or more portions of sensor data based in part on a first machine-learned model (e.g., the machine-learned object detection and recognition model used by the first stage computing system <b>210</b>). For example, the first stage computing system <b>210</b> can determine one or more characteristics of one or more portions of sensor data received from the one or more sensor devices <b>202</b>, and can perform the determination using a machine-learned object detection and recognition model that has been trained to detect and/or recognize one or more objects including streets, buildings, the sky, vehicles, pedestrians, and/or cyclists.</p><p id="p-0121" num="0120">In some embodiments, the object detection system can determine, in a first stage of a multiple stage classification, one or more first stage characteristics of the one or more portions of sensor data based in part on traversal of a first portion of a first machine-learned model (e.g., a first machine-learned model associated with data which can include the object data). In some embodiments, the first machine-learned model used by the object detection system can be based in part on one or more classification techniques including a random forest classifier, neural network, gradient boosting, a support vector machine, a logistic regression classifier, and/or a boosted forest classifier.</p><p id="p-0122" num="0121">In some embodiments, the first stage of the multiple stage classification can include traversal of a first portion that includes a first plurality of nodes associated with a plurality of classifier labels (e.g., machine-learned model classifier labels). Each of the first plurality of nodes in the first portion can be associated with a classifier label that is used to classify, categorize and/or determine the one or more first stage characteristics of the one or more portions of sensor data. For example, the first stage of the multiple stage classification can include a determination of the one or more first stage characteristics including the portions of the one or more portions of sensor data that are background (e.g., the one or more portions of the sensor data that are associated with objects that are not of interest including a street surface and/or a sky) and the portions of the one or more portions of sensor data that are foreground (e.g., the one or more portions of the sensor data that are associated with objects that are of interest including a vehicle, a pedestrian, and/or a cyclist). Accordingly, the object detection system can provide a first stage output (i.e., data associated with the one or more first stage characteristics) that allows the second stage of the multiple stage classification to more efficiently process the one or more portions of sensor data by determining in advance (e.g., in the first or earlier stages of the multiple stage classification) the areas of the one or more portions of sensor data to focus on for purposes of object detection and/or recognition.</p><p id="p-0123" num="0122">In some embodiments, the first stage of the multiple stage classification can be performed by one or more hardware components of the one or more computing devices including an FPGA, a digital signal processor (DSP), an application specific integrated circuit (ASIC), or a graphics processing unit (GPU). By way of example, using a GPU can leverage the parallel processing capabilities of the GPU, which can improve processing effectiveness and result in object detection, recognition, and/or image processing that exceeds the speed of other techniques that do not process the images in parallel (e.g., serial processing of the one or more images).</p><p id="p-0124" num="0123">At <b>606</b>, the method <b>600</b> can include determining, characteristics of the one or more portions of sensor data. In some embodiments, the method <b>600</b> can include determining characteristics of the one or more portions of sensor data (e.g., the one or more portions of sensor data in <b>602</b> and/or <b>604</b>) in a second stage of the multiple stage classification, one or more second stage characteristics of the one or more portions of sensor data based in part on a second machine-learned model. For example, the second stage computing system <b>230</b> can determine one or more characteristics of one or more portions of sensor data received from the one or more sensor devices <b>202</b> and/or the first stage computing system <b>210</b>, and can perform the determination using a machine-learned object detection and recognition model that has been trained to detect and/or recognize one or more objects including streets, buildings, the sky, vehicles, pedestrians, and/or cyclists.</p><p id="p-0125" num="0124">In some embodiments, the one or more first stage characteristics and/or the one or more second stage characteristics can be determined using a first machine-learned model and a second machine-learned model respectively. The first machine-learned model can include a first plurality of nodes associated with a first plurality of classifier labels and the second machine-learned model can include a second plurality of nodes associated with a second plurality of classifier labels.</p><p id="p-0126" num="0125">In some embodiments, the first machine-learned model (the machine-learned object detection and recognition model in <b>604</b>) and/or the second machine-learned model (the machine-learned object detection and recognition model in <b>606</b>) can be based in part on one or more classification techniques including a neural network, a random forest classifier, gradient boosting, a support vector machine, a logistic regression classifier, or a boosted forest classifier.</p><p id="p-0127" num="0126">In some embodiments, the one or more first stage characteristics determined at <b>604</b> can include an indication of when or whether a portion of the one or more portions of sensor data is foreground or background and the one or more second stage characteristics determined at <b>606</b> can include an object classification of a foreground portion of the one or more portions of sensor data. For example, the first stage computing system <b>210</b> can determine the one or more portions of sensor data that include background (e.g., the sky and/or street surfaces). These background portions can be excluded from subsequent analysis by the second stage computing system. When the first stage computing system determines one or more portions of sensor data to include foreground, then the second stage computing system <b>230</b> can further analyze the one or more foreground portions of sensor data to detect and/or classify objects therein (e.g., vehicles and/or pedestrians or other objects of interest).</p><p id="p-0128" num="0127">The second stage of the multiple stage classification can include implementation of a second machine-learned model that includes a second plurality of nodes associated with the second plurality of classifier labels. For example, each of the plurality of nodes in the second machine-learned model can be associated with a classifier label that is used to classify, categorize and/or determine the one or more first stage characteristics of the one or images. For example, the second stage of the multiple stage classification can include a determination of the one or more second stage characteristics of the one or more images including the location and identity of one or more objects (e.g., the location of pedestrians in the one or more images).</p><p id="p-0129" num="0128">The second stage of the multiple stage classification can include determining one or more second stage characteristics of the one or more images that are based in part on the output of the first stage. The one or more second stage characteristics can include the one or more first stage characteristics (e.g., if a background characteristic is determined in the first stage a background characteristic can be further determined, to a greater level of confidence, in the second stage). Further, the one or more second stage characteristics can include characteristics that were not determined in the first stage. For example, if one or more objects (e.g., pedestrians, vehicles, and/or cyclists) were not determined in the first stage, the one or more objects can be determined in the second stage.</p><p id="p-0130" num="0129">In some embodiments, the second machine-learned model can include a second plurality of nodes that is equal or greater in number than the first plurality of nodes in the first machine-learned model. For example, the first machine-learned model can include five-hundred (500) nodes, while the second machine-learned model can include one-thousand five-hundred (1500) nodes. Because a subset of the object data received at <b>602</b> is analyzed by both the first machine-learned model and the second machine-learned model (e.g., second stage characteristics determined at <b>606</b> can be determined for foreground portions of object data), then data processing using separate first and second models can be significantly faster as compared with a single model that includes two-thousand (2000) nodes of similar nature. In addition, the second machine-learned model can allow for a deeper second-model analysis that can include more classifier labels and can be traversed for greater accuracy in detecting and/or identifying the one or more objects after the first model analysis.</p><p id="p-0131" num="0130">In some embodiments, parts of the second stage of the multiple stage classification can be performed by one or more software components (e.g., software applications that execute one or more program instructions) of the one or more computing devices including one or more software components that operate or are executed on one or more central processing units.</p><p id="p-0132" num="0131">At <b>608</b>, the method <b>600</b> can include generating an object output based in part on the one or more first stage characteristics and the one or more second stage characteristics. The object output can include one or more indications associated with detection of one or more objects in the one or more portions of sensor data.</p><p id="p-0133" num="0132">For example, the multiple stage classifier system <b>200</b> can generate an object output based in part on data associated with the one or more first stage characteristics (e.g., data generated by the first stage computing system <b>210</b>) and/or data associated with the one or more second stage characteristics (e.g., data generated by the second stage computing system <b>230</b>). The object output can include one or more indications or signs associated with detection of one or more objects in the one or more portions of sensor data. In some embodiments, the object output can include, for each of the one or more objects, one or more indications of whether an object was detected; the type of object that was detected; the location of the object detected; the physical characteristics of the object detected; the velocity and/or acceleration of the object detected; and/or a probability associated with an estimated accuracy of the object detection.</p><p id="p-0134" num="0133">In some embodiments, the object output generated at <b>608</b> can be used by one or more vehicle systems (e.g., vehicle systems used to control the operation of a vehicle including an autonomous vehicle) to perform one or more actions including activating vehicle systems based on detection of the one or more objects (e.g., activating brakes when an object is within a predetermined proximity of the vehicle); modifying the path of the vehicle (e.g., maneuver the vehicle around objects including buildings, vehicles, and/or pedestrians); and/or exchange the object output with one or more vehicle systems or remote computing systems (e.g., the object output can be sent to other vehicles to improve object detection by other vehicles that may have reduced sensor coverage or capacity).</p><p id="p-0135" num="0134">At <b>610</b>, the method <b>600</b> can include determining, based in part on the object output generated at <b>608</b>, locations for one or more bounding shapes associated with the one or more objects in the one or more images. For example, the multiple stage classifier system <b>200</b> can determine, based in part on the object output, locations for one or more bounding shapes (e.g., two-dimensional or three-dimensional bounding polygons and/or bounding ellipses) associated with the one or more objects in the one or more portions of sensor data. The object detection system can use the first machine-learned model and/or the second machine-learned model to determine the one or more locations or areas of the sensor data that are more likely to contain an object or a certain type of object (e.g., in an image in which the ground is part of the lower half of the image and the sky is part of the upper half of the image, a motor vehicle or cyclist is more likely to be in the lower half of the image than the upper half of an image).</p><p id="p-0136" num="0135">At <b>612</b>, the method <b>600</b> can include selecting, based in part on an image processing technique including non-maximum suppression, a set of the locations for the one or more bounding shapes. For example, the multiple stage classifier system <b>200</b> can select a set of the locations in which to generate the one or more bounding shapes. For example, by analyzing the image gradient direction, pixels that are not part of the local maxima for the portion of the sensor data corresponding to each of the set of locations can be suppressed.</p><p id="p-0137" num="0136">At <b>614</b>, the method <b>600</b> can include generating the one or more bounding shapes in the set of the locations for the one or more bounding shapes. For example, the multiple stage classifier system <b>200</b> can, based on the set of locations for the one or more bounding shapes, generate the one or more bounding shapes in the selected locations. In some embodiments, the one or more bounding shapes can be represented on a display device (e.g., an LCD display) as polygons (e.g., one or more squares and/or rectangles) and/or ellipses (e.g., one or more circles and/or ovals) generated to fully enclose or partly cover a portion of the display output in which an object is detected.</p><p id="p-0138" num="0137"><figref idref="DRAWINGS">FIG. <b>7</b></figref> depicts a second flow diagram of an example method of object detection and recognition according to example embodiments of the present disclosure. One or more portions of the method <b>700</b>, illustrated in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, can be implemented by one or more devices (e.g., one or more computing devices) or systems including, for example, the vehicle <b>104</b>, the vehicle computing system <b>108</b>, or the operations computing system <b>150</b>, shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>; or the multiple stage classifier system <b>200</b> shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. Moreover, one or more portions of the method <b>700</b> can be implemented as an algorithm on the hardware components of the devices described herein (e.g., as in <figref idref="DRAWINGS">FIG. <b>1</b></figref>) to, for example, perform multiple-stage detection and/or recognition of objects including receiving object data, determining characteristics of one or more objects, and generating object output associated with detection of one or more objects. <figref idref="DRAWINGS">FIG. <b>7</b></figref> depicts elements performed in a particular order for purposes of illustration and discussion. Those of ordinary skill in the art, using the disclosures provided herein, will understand that the elements of any of the methods discussed herein can be adapted, rearranged, expanded, omitted, combined, and/or modified in various ways without deviating from the scope of the present disclosure.</p><p id="p-0139" num="0138">At <b>702</b>, the method <b>700</b> can include generating, in the first stage of a multiple stage classification (e.g., the first stage of the multiple stage classification in the method <b>600</b>) and based in part on the object data (e.g., the object data in the method <b>600</b>), visual descriptor output associated with the one or more images (e.g., the one or more images in the method <b>600</b>), the visual descriptor output can include color hue information, color saturation information, brightness information, and/or histogram of oriented gradients information. In some embodiments, the one or more first stage characteristics (e.g., the one or more first stage characteristics of the method <b>600</b>) can be determined based in part on the visual descriptor output. For example, the first stage computing system <b>210</b> of the multiple stage classifier system <b>200</b> can generate, in the first stage of a multiple stage classification, and based in part on the object data, data including visual descriptor output associated with the one or more images (e.g., visual images). In some embodiments, the one or more first stage characteristics can be based in part on the visual descriptor output (e.g., the multiple stage classifier system <b>200</b> can use the visual descriptor output to determine the one or more first stage characteristics of the one or more portions of the sensor data). For example, the multiple stage classifier system <b>200</b> can use brightness information to determine the one or more first stage characteristics associated with the sky (e.g., the sky will tend to be brighter than the ground).</p><p id="p-0140" num="0139">At <b>704</b>, the method <b>700</b> can include generating, in the second stage of the multiple stage classification (e.g., the second stage of the multiple stage classification in the method <b>600</b>) and based in part on the visual descriptor output from the first stage, a heat map associated with the one or more images (e.g., the one or more images in the method <b>600</b>). The heat map can include a plurality of areas associated with a probability of at least one of the one or more objects being within the respective one of the plurality of areas. For example, the multiple stage classifier system <b>200</b> can generate a heat map indicating that the probability of a vehicle object being in the sky is a very low probability. Further, the multiple stage classifier system <b>200</b> can segment the one or images into a plurality of areas and, for each of the plurality of areas, determine a probability of an object being within that area. In some embodiments, the one or more second stage characteristics (e.g., the one or more second stage characteristics in the method <b>600</b>) can be determined based in part on the heat map.</p><p id="p-0141" num="0140">At <b>706</b>, the method <b>700</b> can include determining, based in part on the visual descriptor output from the first stage, one or more portions of the one or more images that are associated with a background portion. In some embodiments, determining the one or more second stage characteristics in the second stage can include determining the one or more second stage characteristics in the second stage and excluding (e.g., not using) the one or more portions of the one or more images that are associated with the one or more background images (e.g., the one or more portions of the one or more images that are determined to not be of interest). Accordingly, the second stage of the multiple stage classification can perform object detection and recognition more rapidly by concentrating computational resources on a smaller subset of the object data (e.g., the foreground images of the one or more images) and avoiding the waste of resources that results from the analysis and/or processing of the one or more images that are part of the background.</p><p id="p-0142" num="0141"><figref idref="DRAWINGS">FIG. <b>8</b></figref> depicts a third flow diagram of an example method of object detection and recognition according to example embodiments of the present disclosure. One or more portions of the method <b>800</b>, illustrated in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, can be implemented by one or more devices (e.g., one or more computing devices) or systems including, for example, the vehicle <b>104</b>, the vehicle computing system <b>108</b>, or the operations computing system <b>150</b>, shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>; or the multiple stage classifier system <b>200</b> shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. Moreover, one or more portions of the method <b>800</b> can be implemented as an algorithm on the hardware components of the devices described herein (e.g., as in <figref idref="DRAWINGS">FIG. <b>1</b></figref>) to, for example, perform multiple-stage detection and/or recognition of objects including receiving object data, determining characteristics of one or more objects, and/or generating object output associated with detection of one or more objects. <figref idref="DRAWINGS">FIG. <b>8</b></figref> depicts elements performed in a particular order for purposes of illustration and discussion. Those of ordinary skill in the art, using the disclosures provided herein, will understand that the elements of any of the methods discussed herein can be adapted, rearranged, expanded, omitted, combined, and/or modified in various ways without deviating from the scope of the present disclosure.</p><p id="p-0143" num="0142">At <b>802</b>, the method <b>800</b> can include determining, based in part on the object data (e.g., the object data in the method <b>600</b>) and the second machine-learned model (e.g., the second machine-learned model in the method <b>600</b>), an amount (e.g., a number of occurrences) of false positive determinations of the one or more second stage characteristics (e.g., the one or more second stage characteristics in the method <b>600</b>) of the one or more objects (e.g., the one or more objects in the method <b>600</b>) that has occurred. For example, the second stage computing system <b>230</b> can determine, based in part on the object data received from the first stage computing system <b>210</b>, an amount of false positive determinations (e.g., the determination of the number of the one or more second stage characteristics that were detected but were not actually present) of the one or more second stage characteristics of the one or more objects that has occurred.</p><p id="p-0144" num="0143">In some embodiments, the object detection system can determine, based in part on the object data and a second machine-learned model subsequent to the first machine-learned model, an amount of false positive determinations of the one or more second stage characteristics (when the second machine-learned model is used) of the one or more objects that has occurred. For example, a false positive determination of the one or more second stage characteristics can include a false determination that a portion of the object data that is foreground (e.g., a pedestrian laying down) is part of the background. The second stage of the multiple stage classification can subsequently correctly determine that the portion of the object data is actually foreground and not background. Accordingly, the number of times that false determinations occur can be determined (e.g., determined by the multiple stage classifier system <b>200</b>).</p><p id="p-0145" num="0144">At <b>804</b>, the method <b>800</b> can include terminating the traversal of a portion of the second machine-learned model (e.g., terminating traversal of a second decision tree, including terminating traversal of the second plurality of nodes in the second classification model in the method <b>600</b>) when the amount of the false positives determined to have occurred exceeds a predetermined threshold level. For example, the second stage computing system <b>230</b> can use the portion of the second machine-learned model (e.g., the second decision tree). Further, the multiple stage classifier system <b>200</b> can terminate traversal of a portion of the second machine-learned model (e.g., the second decision tree) by the second stage computing system <b>230</b> when the amount of false positive determinations by the second stage computing system <b>230</b> exceeds a predetermined amount (e.g., a number of false positive determinations and/or a proportion of false positive determinations of the one or more second stage characteristics with respect to the total number of the one or more second stage characteristics).</p><p id="p-0146" num="0145">In some embodiments, at least one node of the second plurality of nodes in the second classification model is a terminal node of the first plurality of nodes (e.g., the first plurality of nodes in the method <b>600</b>) in the first classification model (e.g., the first machine-learned model in the method <b>600</b>); the second classification model can include an equal number of nodes as the first plurality of nodes; and/or the second classification model includes a greater number of nodes than the first plurality of nodes. For example, the first node in the second classification model can be the terminal node in the first classification model. As such, the second classification model can be a continuation of the first classification model and build upon the first classification model without starting classification analysis anew.</p><p id="p-0147" num="0146">In some embodiments, the determination of the predetermined threshold level to terminate traversal of the machine-learned model can be based on performance (e.g., false positive rate) of the first machine-learned model or the second machine-learned model on a previously established data set (e.g., a training dataset in which all of the objects have been correctly identified) at the various depths of the portion of the second machine learned model (e.g., the decision tree). For example, the predetermined threshold level to terminate traversal of the portion of the second machine-learned model (e.g., decision tree) can be based in part on the depth of the portion of the first machine-learned model (e.g., the first decision tree) when the amount of false positives exceeds a predetermined percentage of detected objects (e.g., one percent of the detected objects) or a predetermined number of objects per image (e.g., four objects per image).</p><p id="p-0148" num="0147">In some embodiments, the first stage of the multiple stage classification can be performed on a customized device (e.g., a customized FPGA) that operates in parallel and can rapidly determine one or more first stage characteristics of the one or more portions of sensor data including whether a portion of sensor data (e.g., a portion of an image) is foreground or a background. After determining one or more first stage characteristics, the second stage of the multiple stage classification can use a classification model with greater depth (i.e., has more nodes along the path from a root node to a terminal node) to determine one or more second stage characteristics that can, with a higher level of confidence, detect, recognize, and/or identify one or more objects including vehicles, pedestrians, streets, buildings, the sky, and/or cyclists.</p><p id="p-0149" num="0148"><figref idref="DRAWINGS">FIG. <b>9</b></figref> depicts a diagram of a second example system according to example embodiments of the present disclosure. As illustrated, an example system <b>900</b> includes a computing system <b>902</b> and a machine learning computing system <b>930</b> that are communicatively coupled (e.g., configured to send and/or receive signals and/or data) over one or more networks <b>980</b>. Further, the example system <b>900</b> can perform one or more operations including receiving object data including portions of sensor data; determining, in a first stage of a multiple stage classification, first stage characteristics of the portions of sensor data based in part on a first machine-learned model; determining, in a second stage of the multiple stage classification, second stage characteristics of the portions of sensor data based in part on a second machine-learned model; and generating, an object output based in part on the first stage characteristics and the second stage characteristics, the object output including indications associated with detection of objects in the portions of sensor data.</p><p id="p-0150" num="0149">In some implementations, the computing system <b>902</b> can perform various operations including multiple-stage detection and/or recognition of objects. In some implementations, the computing system <b>902</b> can be included in an autonomous vehicle. For example, the computing system <b>902</b> can be on-board the autonomous vehicle. In other implementations, the computing system <b>902</b> is not located on-board the autonomous vehicle. For example, the computing system <b>902</b> can operate offline to perform multiple-stage detection and/or recognition of objects. The computing system <b>902</b> can include one or more distinct physical computing devices.</p><p id="p-0151" num="0150">The computing system <b>902</b> includes one or more processors <b>912</b> and a memory <b>914</b>. The one or more processors <b>912</b> can be any suitable processing device (e.g., a processor core, a microprocessor, an ASIC, a FPGA, a controller, and/or a microcontroller) and can be one processor or a plurality of processors that are operatively connected. The memory <b>914</b> can include one or more non-transitory computer-readable storage media, including RAM, ROM, EEPROM, EPROM, one or more memory devices, and/or flash memory devices.</p><p id="p-0152" num="0151">The memory <b>914</b> can store information that can be accessed by the one or more processors <b>912</b>. For instance, the memory <b>914</b> (e.g., one or more non-transitory computer-readable storage mediums, and/or memory devices) can store data <b>916</b> that can be obtained, received, accessed, written, manipulated, created, and/or stored. The data <b>916</b> can include, for instance, include examples as described herein. In some implementations, the computing system <b>902</b> can obtain data from one or more memory devices that are remote from the computing system <b>902</b>.</p><p id="p-0153" num="0152">The memory <b>914</b> can also store computer-readable instructions <b>918</b> that can be executed by the one or more processors <b>912</b>. The instructions <b>918</b> can be software written in any suitable programming language or can be implemented in hardware. Additionally, or alternatively, the instructions <b>918</b> can be executed in logically and/or virtually separate threads on the one or more processors <b>912</b>.</p><p id="p-0154" num="0153">For example, the memory <b>914</b> can store instructions <b>918</b> that when executed by the one or more processors <b>912</b> cause the one or more processors <b>912</b> to perform any of the operations and/or functions described herein, including, for example, performing multiple-stage detection and/or recognition of objects.</p><p id="p-0155" num="0154">According to an aspect of the present disclosure, the computing system <b>902</b> can store or include one or more machine-learned models <b>910</b>. As examples, the one or more machine-learned models <b>910</b> can include various machine-learned models including, for example, neural networks (e.g., deep neural networks), support vector machines, decision trees, ensemble models, k-nearest neighbors models, Bayesian networks, logistic regression classification, boosted forest classification, or other types of models including linear models and/or non-linear models. Example neural networks include feed-forward neural networks, recurrent neural networks (e.g., long short-term memory recurrent neural networks), or other forms of neural networks. The one or more machine-learned models <b>910</b> can include, for example, a first machine-learned model associated with first stage computing system <b>210</b> and/or a second machine-learned model associated with second stage computing system <b>230</b> within the multiple stage classifier system <b>200</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0156" num="0155">In some implementations, the computing system <b>902</b> can receive the one or more machine-learned models <b>910</b> from the machine learning computing system <b>930</b> over the one or more networks <b>980</b> and can store the one or more machine-learned models <b>910</b> in the memory <b>914</b>. The computing system <b>902</b> can then use or otherwise implement the one or more machine-learned models <b>910</b> (e.g., by the one or more processors <b>912</b>). In particular, the computing system <b>902</b> can implement the one or more machine-learned models <b>910</b> to perform multiple-stage detection and/or recognition of objects.</p><p id="p-0157" num="0156">The machine learning computing system <b>930</b> includes one or more processors <b>932</b> and a memory <b>934</b>. The one or more processors <b>932</b> can be any suitable processing device (e.g., a processor core, a microprocessor, an ASIC, a FPGA, a controller, and/or a microcontroller) and can be one processor or a plurality of processors that are operatively connected. The memory <b>934</b> can include one or more non-transitory computer-readable storage media, including RAM, ROM, EEPROM, EPROM, one or more memory devices, and/or flash memory devices.</p><p id="p-0158" num="0157">The memory <b>934</b> can store information that can be accessed by the one or more processors <b>932</b>. For instance, the memory <b>934</b> (e.g., one or more non-transitory computer-readable storage mediums, memory devices) can store data <b>936</b> that can be obtained, received, accessed, written, manipulated, created, and/or stored. The data <b>936</b> can, for instance, include examples as described herein. In some implementations, the machine learning computing system <b>930</b> can obtain data from one or more memory devices that are remote from the machine learning computing system <b>930</b>.</p><p id="p-0159" num="0158">The memory <b>934</b> can also store computer-readable instructions <b>938</b> that can be executed by the one or more processors <b>932</b>. The instructions <b>938</b> can be software written in any suitable programming language or can be implemented in hardware. Additionally, or alternatively, the instructions <b>938</b> can be executed in logically and/or virtually separate threads on the one or more processors <b>932</b>.</p><p id="p-0160" num="0159">For example, the memory <b>934</b> can store instructions <b>938</b> that when executed by the one or more processors <b>932</b> cause the one or more processors <b>932</b> to perform any of the operations and/or functions described herein, including, for example, performing multiple-stage detection and/or recognition of objects.</p><p id="p-0161" num="0160">In some implementations, the machine learning computing system <b>930</b> includes one or more server computing devices. If the machine learning computing system <b>930</b> includes multiple server computing devices, such server computing devices can operate according to various computing architectures, including, for example, sequential computing architectures, parallel computing architectures, or some combination thereof.</p><p id="p-0162" num="0161">In addition or alternatively to the one or more machine-learned models <b>910</b> at the computing system <b>902</b>, the machine learning computing system <b>930</b> can include one or more machine-learned models <b>940</b>. As examples, the one or more machine-learned models <b>940</b> can include various machine-learned models including, for example, neural networks (e.g., deep neural networks), support vector machines, decision trees, ensemble models, k-nearest neighbors models, Bayesian networks, logistic regression classification, boosted forest classification, or other types of models including linear models and/or non-linear models. Example neural networks include feed-forward neural networks, recurrent neural networks (e.g., long short-term memory recurrent neural networks, or other forms of neural networks).</p><p id="p-0163" num="0162">As an example, the machine learning computing system <b>930</b> can communicate with the computing system <b>902</b> according to a client-server relationship. For example, the machine learning computing system <b>930</b> can implement the one or more machine-learned models <b>940</b> to provide a web service to the computing system <b>902</b>. For example, the web service can provide results including the type, identity, and/or class of objects that have been detected and/or recognized.</p><p id="p-0164" num="0163">Thus, one or more machine-learned models <b>910</b> can be located and used at the computing system <b>902</b> and/or the one or more machine-learned models <b>940</b> can be located and used at the machine learning computing system <b>930</b>.</p><p id="p-0165" num="0164">In some implementations, the machine learning computing system <b>930</b> and/or the computing system <b>902</b> can train the one or more machine-learned models <b>910</b> and/or the one or more machine-learned models <b>940</b> through use of a model trainer <b>960</b>. The model trainer <b>960</b> can train the one or more machine-learned models <b>910</b> and/or the one or more machine-learned models <b>940</b> using one or more training or learning algorithms. One example training technique is backwards propagation of errors. In some implementations, the model trainer <b>960</b> can perform supervised training techniques using a set of labeled training data. In other implementations, the model trainer <b>960</b> can perform unsupervised training techniques using a set of unlabeled training data. The model trainer <b>960</b> can perform a number of generalization techniques to improve the generalization capability of the models being trained. Generalization techniques include weight decays, dropouts, or other techniques.</p><p id="p-0166" num="0165">In particular, the model trainer <b>960</b> can train one or more machine-learned models <b>910</b> and/or one or more machine-learned models <b>940</b> based on a set of training data <b>962</b>. The training data <b>962</b> can include, for example, various features of one or more objects. The model trainer <b>960</b> can be implemented in hardware, firmware, and/or software controlling one or more processors.</p><p id="p-0167" num="0166">The computing system <b>902</b> can also include a network interface <b>924</b> used to communicate with one or more systems or devices, including systems or devices that are remotely located from the computing system <b>902</b>. The network interface <b>924</b> can include any circuits, components, software, for communicating with one or more networks (e.g., the one or more networks <b>980</b>). In some implementations, the network interface <b>924</b> can include, for example, one or more of a communications controller, receiver, transceiver, transmitter, port, conductors, software and/or hardware for communicating data. Further, the machine learning computing system <b>930</b> can include a network interface <b>964</b>.</p><p id="p-0168" num="0167">The one or more networks <b>980</b> can include any type of network or combination of networks that allows for communication between devices. In some embodiments, the one or more networks <b>980</b> can include one or more of a local area network, wide area network, the Internet, secure network, cellular network, mesh network, peer-to-peer communication link and/or some combination thereof and can include any number of wired or wireless links. Communication over the one or more networks <b>980</b> can be accomplished, for instance, via a network interface using any type of protocol, protection scheme, encoding, format, and/or packaging.</p><p id="p-0169" num="0168"><figref idref="DRAWINGS">FIG. <b>9</b></figref> illustrates one example computing system <b>900</b> that can be used to implement the present disclosure. Other computing systems can be used as well. For example, in some implementations, the computing system <b>902</b> can include the model trainer <b>960</b> and the training data <b>962</b>. In such implementations, the one or more machine-learned models <b>910</b> can be both trained and used locally at the computing system <b>902</b>. As another example, in some implementations, the computing system <b>902</b> is not connected to other computing systems.</p><p id="p-0170" num="0169">In addition, components illustrated and/or discussed as being included in one of the computing systems <b>902</b> or <b>930</b> can instead be included in another of the computing systems <b>902</b> or <b>930</b>. Such configurations can be implemented without deviating from the scope of the present disclosure. The use of computer-based systems allows for a great variety of possible configurations, combinations, and divisions of tasks and functionality between and among components. Computer-implemented operations can be performed on a single component or across multiple components. Computer-implemented tasks and/or operations can be performed sequentially or in parallel. Data and instructions can be stored in a single memory device or across multiple memory devices.</p><p id="p-0171" num="0170">While the present subject matter has been described in detail with respect to specific example embodiments and methods thereof, it will be appreciated that those skilled in the art, upon attaining an understanding of the foregoing can readily produce alterations to, variations of, and equivalents to such embodiments. Accordingly, the scope of the present disclosure is by way of example rather than by way of limitation, and the subject disclosure does not preclude inclusion of such modifications, variations and/or additions to the present subject matter as would be readily apparent to one of ordinary skill in the art.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-01-20" num="01-20"><claim-text><b>1</b>.-<b>20</b>. (canceled)</claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. An autonomous vehicle control system for an autonomous vehicle, the autonomous vehicle control system comprising:<claim-text>one or more processors; and</claim-text><claim-text>one or more non-transitory, computer-readable media storing instructions that are executable to cause the one or more processors to perform operations comprising:<claim-text>receiving sensor data descriptive of an environment of the autonomous vehicle;</claim-text><claim-text>determining in a first stage of a multiple stage classification, one or more first stage characteristics of the sensor data based in part on a first machine-learned model, wherein the first stage characteristics are determined by the first machine-learned model with a first level of confidence;</claim-text><claim-text>determining in a second stage of the multiple stage classification, one or more second stage characteristics of the sensor data based in part on a second machine-learned model, wherein the second stage characteristics are determined by the second machine-learned model with a second level of confidence that is higher than the first level of confidence; and</claim-text><claim-text>generating an object output based in part on the second stage characteristics, the object output indicating detection of one or more objects in the sensor data.</claim-text></claim-text></claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. The autonomous vehicle control system of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the one or more first stage characteristics of the sensor data determined in the first stage of the multiple stage classification are indicative of a likelihood that the sensor data contains objects.</claim-text></claim><claim id="CLM-00023" num="00023"><claim-text><b>23</b>. The autonomous vehicle control system of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the one or more first stage characteristics of the sensor data determined in the first stage of the multiple stage classification are indicative of one or more portions of the sensor data being classified as background or foreground.</claim-text></claim><claim id="CLM-00024" num="00024"><claim-text><b>24</b>. The autonomous vehicle control system of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the one or more second stage characteristics of the sensor data determined in the second stage of the multiple stage classification are indicative of an object classification for a type of object detected in the sensor data.</claim-text></claim><claim id="CLM-00025" num="00025"><claim-text><b>25</b>. The autonomous vehicle control system of <claim-ref idref="CLM-00021">claim 21</claim-ref>, the operations further comprising:<claim-text>generating in the first stage, a heat map associated with the sensor data, the heat map describing a probability of an object being contained within a respective area of the sensor data.</claim-text></claim-text></claim><claim id="CLM-00026" num="00026"><claim-text><b>26</b>. The autonomous vehicle control system of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein an input to the second stage of the multiple stage classification is associated with one or more foreground portions of the sensor data.</claim-text></claim><claim id="CLM-00027" num="00027"><claim-text><b>27</b>. The autonomous vehicle control system of <claim-ref idref="CLM-00021">claim 21</claim-ref>, the operations further comprising:<claim-text>generating, in the first stage and based in part on the sensor data, visual descriptor output associated with the sensor data, the visual descriptor output comprising color hue information, color saturation information, brightness information, or histogram of oriented gradients information, wherein the one or more first stage characteristics are determined based in part on the visual descriptor output.</claim-text></claim-text></claim><claim id="CLM-00028" num="00028"><claim-text><b>28</b>. The autonomous vehicle control system of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the sensor data comprises one or more LIDAR features and one or more camera features.</claim-text></claim><claim id="CLM-00029" num="00029"><claim-text><b>29</b>. The autonomous vehicle control system of <claim-ref idref="CLM-00021">claim 21</claim-ref>, the operations further comprising:<claim-text>controlling a motion of the autonomous vehicle based in part on the object output.</claim-text></claim-text></claim><claim id="CLM-00030" num="00030"><claim-text><b>30</b>. A method comprising:<claim-text>receiving sensor data descriptive of an environment of an autonomous vehicle;</claim-text><claim-text>determining in a first stage of a multiple stage classification, one or more first stage characteristics of the sensor data based in part on a first machine-learned model, wherein the first stage characteristics are determined by the first machine-learned model with a first level of confidence;</claim-text><claim-text>determining in a second stage of the multiple stage classification, one or more second stage characteristics of the sensor data based in part on a second machine-learned model, wherein the second stage characteristics are determined by the second machine-learned model with a second level of confidence that is higher than the first level of confidence; and</claim-text><claim-text>generating an object output based in part on the second stage characteristics, the object output indicating detection of one or more objects in the sensor data.</claim-text></claim-text></claim><claim id="CLM-00031" num="00031"><claim-text><b>31</b>. The method of <claim-ref idref="CLM-00030">claim 30</claim-ref>, wherein the one or more first stage characteristics of the sensor data determined in the first stage of the multiple stage classification are indicative of a likelihood that the sensor data contains objects.</claim-text></claim><claim id="CLM-00032" num="00032"><claim-text><b>32</b>. The method of <claim-ref idref="CLM-00030">claim 30</claim-ref>, wherein the one or more first stage characteristics of the sensor data determined in the first stage of the multiple stage classification are indicative of one or more portions of the sensor data being background or foreground.</claim-text></claim><claim id="CLM-00033" num="00033"><claim-text><b>33</b>. The method of <claim-ref idref="CLM-00030">claim 30</claim-ref>, wherein the one or more second stage characteristics of the sensor data determined in the second stage of the multiple stage classification are indicative of an object classification for a type of object detected in the sensor data.</claim-text></claim><claim id="CLM-00034" num="00034"><claim-text><b>34</b>. The method of <claim-ref idref="CLM-00030">claim 30</claim-ref>, further comprising:<claim-text>generating in the first stage, a heat map associated with the sensor data, the heat map describing a probability of an object being contained within a respective area of the sensor data.</claim-text></claim-text></claim><claim id="CLM-00035" num="00035"><claim-text><b>35</b>. The method of <claim-ref idref="CLM-00030">claim 30</claim-ref>, wherein an input to the second stage of the multiple stage classification is associated with one or more foreground portions of the sensor data.</claim-text></claim><claim id="CLM-00036" num="00036"><claim-text><b>36</b>. The method of <claim-ref idref="CLM-00030">claim 30</claim-ref>, further comprising:<claim-text>generating, in the first stage and based in part on the sensor data, visual descriptor output associated with the sensor data, the visual descriptor output comprising color hue information, color saturation information, brightness information, or histogram of oriented gradients information, wherein the one or more first stage characteristics are determined based in part on the visual descriptor output.</claim-text></claim-text></claim><claim id="CLM-00037" num="00037"><claim-text><b>37</b>. The method of <claim-ref idref="CLM-00030">claim 30</claim-ref>, wherein the sensor data comprises one or more LIDAR features and one or more camera features.</claim-text></claim><claim id="CLM-00038" num="00038"><claim-text><b>38</b>. The method of <claim-ref idref="CLM-00030">claim 30</claim-ref>, further comprising:<claim-text>controlling a motion of the autonomous vehicle based in part on the object output.</claim-text></claim-text></claim><claim id="CLM-00039" num="00039"><claim-text><b>39</b>. One or more tangible, non-transitory computer-readable media storing computer-readable instructions that when executed by one or more processors cause the one or more processors to perform operations, the operations comprising:<claim-text>receiving sensor data descriptive of an environment of an autonomous vehicle;</claim-text><claim-text>determining in a first stage of a multiple stage classification, one or more first stage characteristics of the sensor data based in part on a first machine-learned model, wherein the first stage characteristics are determined by the first machine-learned model with a first level of confidence;</claim-text><claim-text>determining in a second stage of the multiple stage classification, one or more second stage characteristics of the sensor data based in part on a second machine-learned model, wherein the second stage characteristics are determined by the second machine-learned model with a second level of confidence that is higher than the first level of confidence; and</claim-text><claim-text>generating an object output based in part on the second stage characteristics, the object output indicating detection of one or more objects in the sensor data.</claim-text></claim-text></claim><claim id="CLM-00040" num="00040"><claim-text><b>40</b>. The one or more tangible, non-transitory computer-readable media of <claim-ref idref="CLM-00039">claim 39</claim-ref>, wherein:<claim-text>the one or more first stage characteristics of the sensor data determined in the first stage of the multiple stage classification are indicative of one or more portions of the sensor data being background or foreground; and</claim-text><claim-text>the one or more second stage characteristics of the sensor data determined in the second stage of the multiple stage classification are indicative of an object classification for a type of object detected in the sensor data.</claim-text></claim-text></claim></claims></us-patent-application>