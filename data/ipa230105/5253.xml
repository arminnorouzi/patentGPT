<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005254A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005254</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17778469</doc-number><date>20201113</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>CN</country><doc-number>201911159693.8</doc-number><date>20191122</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>98</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>60</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>25</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>98</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>60</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>25</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">IMAGE DETECTION METHOD AND APPARATUS, AND ELECTRONIC DEVICE</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>HUAWEI TECHNOLOGIES CO., LTD.</orgname><address><city>Shenzhen, Guangdong</city><country>CN</country></address></addressbook><residence><country>CN</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>ZHOU</last-name><first-name>Deyu</first-name><address><city>Shenzhen</city><country>CN</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>LU</last-name><first-name>Yuewan</first-name><address><city>Shenzhen</city><country>CN</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>DONG</last-name><first-name>Chen</first-name><address><city>Shenzhen</city><country>CN</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>JIANG</last-name><first-name>Yongtao</first-name><address><city>Shenzhen</city><country>CN</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>GAO</last-name><first-name>Wenmei</first-name><address><city>Shenzhen</city><country>CN</country></address></addressbook></inventor></inventors></us-parties><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/CN2020/128786</doc-number><date>20201113</date></document-id><us-371c12-date><date>20220520</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">The technology of this application relates to an artificial intelligence terminal-based image detection method and apparatus, and an electronic device. The method includes obtaining a to-be-detected image, determining a light source region of the to-be-detected image and a foreground region of the to-be-detected image, and determining a blurring degree of the to-be-detected image based on the light source region and the foreground region. Impact of a light source on clarity of the to-be-detected image is determined by performing light source region detection and foreground region detection on the to-be-detected image, to effectively detect whether an image shot in a backlight condition is blurred.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="30.99mm" wi="41.74mm" file="US20230005254A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="167.89mm" wi="46.91mm" file="US20230005254A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="147.83mm" wi="152.48mm" file="US20230005254A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="185.67mm" wi="154.86mm" file="US20230005254A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="191.52mm" wi="112.18mm" file="US20230005254A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="164.34mm" wi="93.98mm" file="US20230005254A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="193.80mm" wi="151.55mm" file="US20230005254A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="207.09mm" wi="100.50mm" file="US20230005254A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="183.81mm" wi="95.00mm" file="US20230005254A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="183.81mm" wi="121.24mm" file="US20230005254A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION(S)</heading><p id="p-0002" num="0001">This application is a National Stage of International Application No. PCT/CN2020/128786, filed on Nov. 13, 2020, which claims priority to Chinese Patent Application No. 201911159693.8, filed on Nov. 22, 2019, both of which are hereby incorporated by reference in their entireties.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">This application pertains to the field of electronic technologies, and in particular, to an artificial intelligence (AI) terminal-based image detection method and apparatus, and an electronic device.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">During photographing, an existing mobile terminal (for example, a mobile phone, a tablet, or a camera) can only detect whether a backlight source exists in a photographing process, but cannot detect whether backlight blur exists in a shot image.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0005" num="0004">Embodiments of this application provide an artificial intelligence terminal-based image detection method and apparatus, and an electronic device, to effectively detect whether an image shot in a backlight condition is blurred.</p><p id="p-0006" num="0005">According to a first aspect, an embodiment of this application provides an image detection method, including: obtaining a to-be-detected image; determining a light source region of the to-be-detected image and a foreground region of the to-be-detected image; and determining a blurring degree of the to-be-detected image based on the light source region and the foreground region.</p><p id="p-0007" num="0006">In the first aspect, impact of a light source on clarity of the to-be-detected image is determined by performing light source region detection and foreground region detection on the to-be-detected image, to effectively detect whether an image shot in a backlight condition is blurred.</p><p id="p-0008" num="0007">In a possible implementation of the first aspect, the determining a light source region of the to-be-detected image includes: performing color space conversion on the to-be-detected image, and obtaining a brightness value of each pixel of the image obtained through the color space conversion; and determining a region of a pixel whose brightness value is greater than a preset brightness threshold as the light source region of the to-be-detected image.</p><p id="p-0009" num="0008">For example, the to-be-detected image is converted to an HSV (Hue, Saturation, Value) color space or an LAB (CIELAB color model) color space through the color space conversion.</p><p id="p-0010" num="0009">It should be understood that the to-be-detected image may also be converted to another color space to determine the brightness value of each pixel of the to-be-detected image.</p><p id="p-0011" num="0010">The color space conversion is performed on the to-be-detected image to determine brightness of each pixel, and the light source region of the to-be-detected image can be determined according to a threshold segmentation method, so that the light source region can be accurately and quickly determined, to improve image detection efficiency.</p><p id="p-0012" num="0011">In a possible implementation of the first aspect, the determining a foreground region of the to-be-detected image includes: detecting a foreground target of the to-be-detected image; and determining a location of the foreground target in the to-be-detected image, and determining the location of the foreground target in the to-be-detected image as the foreground region of the to-be-detected image.</p><p id="p-0013" num="0012">It should be understood that the foreground target may be a target having a dynamic feature in the to-be-detected image, for example, a person or an animal, or the foreground target may be a scene that is close to a viewer and that has a static feature, for example, a flower or food.</p><p id="p-0014" num="0013">In a possible implementation of the first aspect, the determining a blurring degree of the to-be-detected image based on the light source region and the foreground region includes: determining the blurring degree of the to-be-detected image based on a quantity of all pixels in the light source region and a quantity of all pixels in the foreground region.</p><p id="p-0015" num="0014">With reference to the previous possible implementation, the determining the blurring degree of the to-be-detected image based on a quantity of all pixels in the light source region and a quantity of all pixels in the foreground region includes: determining whether a function value of an increasing function of a ratio of the quantity of all pixels in the light source region to the quantity of all pixels in the foreground region is greater than a predetermined threshold; and if the function value of the increasing function of the ratio of the quantity of all pixels in the light source region to the quantity of all pixels in the foreground region is greater than the predetermined threshold, determining that the to-be-detected image is a blurred image.</p><p id="p-0016" num="0015">In a possible implementation of the first aspect, the determining a blurring degree of the to-be-detected image based on the light source region and the foreground region includes: determining the blurring degree of the to-be-detected image based on a quantity of all pixels in the light source region and an area of the foreground region.</p><p id="p-0017" num="0016">In a possible implementation of the first aspect, the obtaining a to-be-detected image includes: obtaining a preview frame image in a first shooting mode; and correspondingly, the determining a blurring degree of the to-be-detected image based on the light source region and the foreground region includes: if it is determined, based on the light source region and the foreground region, that the preview frame image is a blurred image, switching a current shooting mode from the first shooting mode to a second shooting mode, where the first shooting mode is different from the second shooting mode.</p><p id="p-0018" num="0017">For example, the first shooting mode is a Torch mode, and the second shooting mode is a Flash mode. The switching a current shooting mode from the first shooting mode to a second shooting mode includes an electronic device sending a control instruction to a flashlight module of the electronic device, to switch a flashlight from a steady-on mode to a mode in which the flashlight flashes once during photographing.</p><p id="p-0019" num="0018">According to a second aspect, an embodiment of this application provides an image detection apparatus, including: an image obtaining module, configured to obtain a to-be-detected image; a first determining module, configured to determine a light source region of the to-be-detected image and a foreground region of the to-be-detected image; and a second determining module, configured to determine a blurring degree of the image based on the light source region and the foreground region.</p><p id="p-0020" num="0019">According to a third aspect, an embodiment of this application provides a terminal device, including a memory, a processor, and a computer program that is stored in the memory and that can be run on the processor. When executing the computer program, the processor implements steps in the image detection method according to the first aspect.</p><p id="p-0021" num="0020">According to a fourth aspect, an embodiment of this application provides a computer-readable storage medium. The computer-readable storage medium stores a computer program. When the computer program is executed by a processor, steps in the image detection method according to the first aspect are implemented.</p><p id="p-0022" num="0021">According to a fifth aspect, an embodiment of this application provides a computer program product. When the computer program product runs on a terminal device, the terminal device is enabled to perform the image detection method according to any one of the first aspect or the possible implementations of the first aspect.</p><p id="p-0023" num="0022">It may be understood that, for beneficial effects of the second aspect to the fifth aspect, refer to related descriptions in the first aspect.</p><p id="p-0024" num="0023">Compared with the conventional technology, embodiments of this application have the following beneficial effects: Impact of a light source on clarity of the to-be-detected image is determined by performing light source region detection and foreground region detection on the to-be-detected image, to effectively detect whether an image shot in a backlight condition is blurred.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>1</b><i>a </i></figref>is an example schematic diagram of an image shot by an existing electronic device in a Torch mode when there is no backlight source;</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>1</b><i>b </i></figref>is an example schematic diagram of an image shot by an existing electronic device in a Flash mode under the condition that a luminous intensity of a backlight source is 23 lux and a luminous intensity area of the backlight source is 50%;</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>1</b><i>c </i></figref>is an example schematic diagram of an image shot by an existing electronic device in a Torch mode when there is no backlight source;</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>1</b><i>d </i></figref>is an example schematic diagram of an image shot by an existing electronic device in a Flash mode under the condition that a luminous intensity of a backlight source is 23 lux and a luminous intensity area of the backlight source is 50%;</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is an example schematic diagram of a structure of an electronic device to which an image detection method is applicable according to an embodiment of this application;</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is an example schematic diagram of a software architecture of an electronic device to which an image detection method is applicable according to an embodiment of this application;</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>4</b><i>a </i></figref>is an example schematic diagram of a group of display interfaces according to an embodiment of this application;</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>4</b><i>b </i></figref>is an example schematic diagram of another group of display interfaces according to an embodiment of this application;</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is an example schematic flowchart of an image detection method according to an embodiment of this application;</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>6</b><i>a </i></figref>is an example schematic diagram of an image of a to-be-detected image in a current color space;</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>6</b><i>b </i></figref>is an example schematic diagram of an image obtained by performing light source segmentation on a to-be-detected image;</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is an example schematic diagram of a foreground target according to an embodiment of this application;</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>8</b><i>a </i></figref>is an example schematic diagram of an image in which a foreground target is a foam box according to this application;</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>8</b><i>b </i></figref>is another example schematic diagram of an image in which a foreground target is a foam box according to this application;</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is an example schematic diagram of a to-be-detected image in an image detection method according to an embodiment of this application;</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is an example schematic diagram of a structure of an image detection apparatus according to an embodiment of this application; and</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is an example schematic diagram of a structure of an electronic device according to an embodiment of this application.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DESCRIPTION OF EMBODIMENTS</heading><p id="p-0042" num="0041">In the following descriptions, to illustrate rather than limit, specific details such as a particular system structure and a technology are provided to make a thorough understanding of embodiments of this application. However, persons skilled in the art should know that this application may also be implemented in other embodiments without these specific details. In other cases, detailed descriptions of well-known systems, apparatuses, circuits, and methods are omitted, so that this application is described without being obscured by unnecessary details.</p><p id="p-0043" num="0042">It should be understood that, when used in the specification and claims of this application, the term &#x201c;include&#x201d; indicates presence of described features, entireties, steps, operations, elements, and/or components, but does not exclude presence or addition of one or more other features, entireties, steps, operations, elements, components, and/or collections thereof.</p><p id="p-0044" num="0043">It should be further understood that the term &#x201c;and/or&#x201d; used in the specification and claims of this application indicates any combination and all possible combinations of one or more items listed in association, and includes the combinations.</p><p id="p-0045" num="0044">As used in the specification and claims of this application, the term &#x201c;if&#x201d; may be interpreted as &#x201c;when&#x201d;, &#x201c;once&#x201d;, &#x201c;in response to determining&#x201d;, or &#x201c;in response to detecting&#x201d; depending on the context. Similarly, the phrase &#x201c;if it is determined&#x201d; or &#x201c;if the [described condition or event] is detected&#x201d; may be interpreted as meaning &#x201c;once determined&#x201d; or &#x201c;in response to determining&#x201d; or &#x201c;once the [described condition or event] is detected&#x201d; or &#x201c;in response to detecting the [described condition or event]&#x201d; depending on the context.</p><p id="p-0046" num="0045">In addition, in the descriptions of the specification and claims of this application, the terms &#x201c;first&#x201d;, &#x201c;second&#x201d;, &#x201c;third&#x201d;, and the like are merely intended for a purpose of differentiated description, but shall not be understood as an indication or an implication of relative importance.</p><p id="p-0047" num="0046">Referring to &#x201c;an embodiment&#x201d; or &#x201c;some embodiments&#x201d; or the like in the specification of this application means that one or more embodiments of this application include a specific feature, structure, or characteristic described with reference to the embodiment. Therefore, statements such as &#x201c;in an embodiment&#x201d;, &#x201c;in some embodiments&#x201d;, &#x201c;in some other embodiments&#x201d;, and &#x201c;in other embodiments&#x201d; that appear at different places in this specification do not necessarily mean referring to a same embodiment. Instead, the statements mean &#x201c;one or more but not all of the embodiments&#x201d;, unless otherwise specifically emphasized in another manner. The terms &#x201c;include&#x201d;, &#x201c;have&#x201d;, and their variants all mean &#x201c;include but are not limited to&#x201d;, unless otherwise specifically emphasized in another manner.</p><p id="p-0048" num="0047">It should be noted that backlight in this application refers to a case in which a photographed subject is just located between a light source and a camera. In this case, the photographed subject is very likely to be underexposed. As a result, a shot image cannot clearly reflect facial skin details, for example, detail features such as fine lines, nasolabial folds, dark eye circles, red regions, acne, pores, color spots, and blackheads. Images shot in different shooting modes present different clarity due to existence of a backlight source. The shooting modes include a first shooting mode (e.g., Torch mode) in which a flashlight is steady on and a second shooting mode (e.g., Flash mode) in which a flashlight flashes during photographing. <figref idref="DRAWINGS">FIG. <b>1</b><i>a </i></figref>and <figref idref="DRAWINGS">FIG. <b>1</b><i>c </i></figref>respectively show images shot by an existing electronic device in a Torch mode and a Flash mode when there is no backlight source. <figref idref="DRAWINGS">FIG. <b>1</b><i>b </i></figref>and <figref idref="DRAWINGS">FIG. <b>1</b><i>d </i></figref>respectively show images shot by an existing electronic device in a Torch mode and a Flash mode with backlight sources of a same luminous intensity (23 lux) and a same luminous intensity area (50%). It can be learned from <figref idref="DRAWINGS">FIG. <b>1</b><i>a </i></figref>and <figref idref="DRAWINGS">FIG. <b>1</b><i>c </i></figref>that, when there is no backlight source, a detail feature (e.g., acne) can be clearly photographed in both the Torch mode and the Flash mode. It can be learned from comparison between <figref idref="DRAWINGS">FIG. <b>1</b><i>c </i></figref>and <figref idref="DRAWINGS">FIG. <b>1</b><i>d </i></figref>that existence of the backlight source does not greatly affect the image shot in the Flash mode, and a detail feature (e.g., acne) can also be photographed. It can be learned from comparison between <figref idref="DRAWINGS">FIG. <b>1</b><i>a </i></figref>and <figref idref="DRAWINGS">FIG. <b>1</b><i>b </i></figref>that existence of the backlight source significantly affects clarity of the image shot in the Torch mode, and causes blurring of a detail feature (e.g., acne). Therefore, an image detection method in embodiments of this application is mainly used to detect an image shot in a Torch mode, and can be used to switch a current shooting mode from the Torch mode to a Flash mode when it is detected that a to-be-detected image is a blurred image. In actual application, photographing is performed based on different luminous intensities and different light source areas by using a dichotomy, to determine that a luminous intensity of 23 lux and a light source area of 25% are used as critical values for determining whether the image shot in the Torch mode is blurred. The light source area is a most important factor that affects whether a foreground is blurred. For a same foreground (a same face in different photos can be approximately considered as the same foreground), if a light source area is larger, impact on clarity of the foreground is greater. If the foreground is large enough and the light source area is small enough, the impact of the light source area on the clarity of the foreground can be ignored. Therefore, a relationship between a light source area and a foreground area is an important parameter for detecting whether feature details of a to-be-detected image are blurred. Whether backlight blur exists in the to-be-detected image can be determined based on light source region detection and foreground region detection. If it is detected that backlight blur exists in the to-be-detected image, the image is not used as an object for evaluating a facial skin status, to improve accuracy of evaluating the facial skin status. It should be noted that the foreground is a person or an object that is in front of a subject or near a front edge in a lens. The foreground in embodiments of this application may include a plurality of object types, for example, objects such as a person, a vehicle, a plant, an animal, a building, a ground, a sky, a table, a chair, and a door frame.</p><p id="p-0049" num="0048">The image detection method provided in embodiments of this application may be applied to a terminal device such as an electronic device, a tablet computer, a wearable device, a vehicle-mounted device, an augmented reality (AR) device/a virtual reality (VR) device, a notebook computer, an ultra-mobile personal computer (UMPC), a netbook, or a personal digital assistant (PDA). A specific type of the terminal device is not limited in embodiments of this application.</p><p id="p-0050" num="0049">By way of example but not limitation, when the terminal device is a wearable device, the wearable device may alternatively be a generic term for wearable devices such as glasses, gloves, watches, clothes, and shoes that are developed based on intelligent design of daily wearing by using wearable technologies. The wearable device is a portable device that is directly worn on a body or integrated into clothes or an accessory of a user. The wearable device is not only a hardware device, but also implements a powerful function through software support, data exchange, and cloud interaction. In a broad sense, the wearable intelligent device includes full-featured and large-sized devices that can implement all or some of functions without depending on smart electronic devices, for example, smart watches or smart glasses, and devices that focus on only one type of application function and need to be used with other devices such as smart electronic devices, for example, various smart bands or smart jewelry for monitoring physical signs.</p><p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram of a partial structure of an electronic device according to an embodiment of this application. Refer to <figref idref="DRAWINGS">FIG. <b>2</b></figref>. The electronic device includes components such as a radio frequency (RF) circuit <b>110</b>, a memory <b>120</b>, an input unit <b>130</b>, a display unit <b>140</b>, a sensor <b>150</b>, an audio circuit <b>160</b>, a wireless fidelity (Wi-Fi) module <b>170</b>, a processor <b>180</b>, a power supply <b>190</b>, and a camera <b>191</b>. Persons skilled in the art may understand that the structure of the electronic device shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref> constitutes no limitation on the electronic device, and the electronic device may include more or fewer components than those shown in the figure, or combine some components, or have different component arrangements.</p><p id="p-0052" num="0051">The following describes the components of the electronic device in detail with reference to <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0053" num="0052">The RF circuit <b>110</b> may be configured to: receive and send a signal in an information receiving or sending process or a call process. Particularly, after receiving downlink information from a base station, the RF circuit <b>110</b> sends the downlink information to the processor <b>180</b> for processing, and sends related uplink data to the base station. Usually, the RF circuit includes but is not limited to an antenna, at least one amplifier, a transceiver, a coupler, a low noise amplifier (LNA), a duplexer, and the like. In addition, the RF circuit <b>110</b> may further communicate with a network and another device through wireless communication. The foregoing wireless communication may be any communications standard or protocol, which includes but is not limited to a global system for mobile communications (GSM), a general packet radio service (GPRS), code division multiple access (CDMA), wideband code division multiple access (WCDMA), long term evolution (LTE), an email, a short message service (SMS), and the like.</p><p id="p-0054" num="0053">The memory <b>120</b> may be configured to store a software program and a module. The processor <b>180</b> performs various function applications of the electronic device and data processing by running the software program and the module that are stored in the memory <b>120</b>. The memory <b>120</b> may mainly include a program storage region and a data storage region. The program storage region may store an operating system, an application required by at least one function (such as a sound playback function or an image playback function), and the like. The data storage region may store data (such as audio data or an address book) created based on use of the electronic device, and the like. In addition, the memory <b>120</b> may include a high-speed random access memory, or may include a nonvolatile memory, such as at least one magnetic disk storage device, a flash memory device, or another volatile solid-state storage device.</p><p id="p-0055" num="0054">The input unit <b>130</b> may be configured to: receive entered digit or character information, and generate a key signal input related to a user setting and function control of the electronic device <b>100</b>. Specifically, the input unit <b>130</b> may include a touch panel <b>131</b> and another input device <b>132</b>. The touch panel <b>131</b>, also referred to as a touchscreen, can collect a touch operation performed by the user on or near the touch panel <b>131</b> (for example, an operation performed by the user on or near the touch panel <b>131</b> by using any proper object or accessory such as a finger or a stylus), and drive a corresponding connection apparatus based on a preset program. Optionally, the touch panel <b>131</b> may include two parts: a touch detection apparatus and a touch controller. The touch detection apparatus detects a touch location of the user, detects a signal generated by the touch operation, and transfers the signal to the touch controller. The touch controller receives touch information from the touch detection apparatus, converts the touch information into touch point coordinates, then sends the touch point coordinates to the processor <b>180</b>, and can receive and execute a command sent from the processor <b>180</b>. In addition, the touch panel <b>131</b> may be implemented by using a plurality of types, such as a resistive type, a capacitive type, an infrared ray type, and a surface acoustic wave type. In addition to the touch panel <b>131</b>, the input unit <b>130</b> may further include the another input device <b>132</b>. Specifically, the another input device <b>132</b> may include but is not limited to one or more of a physical keyboard, a function key (such as a volume control key or an on/off key), a trackball, a mouse, and a joystick.</p><p id="p-0056" num="0055">The display unit <b>140</b> may be configured to display information entered by the user or information provided for the user, and various menus of the electronic device. The display unit <b>140</b> may include a display panel <b>141</b>. Optionally, the display panel <b>141</b> may be configured in a form of a liquid crystal display (LCD), an organic light-emitting diode (OLED), or the like. Further, the touch panel <b>131</b> may cover the display panel <b>141</b>. When detecting the touch operation on or near the touch panel <b>131</b>, the touch panel <b>131</b> transfers the touch operation to the processor <b>180</b> to determine a type of a touch event, and then the processor <b>180</b> provides a corresponding visual output on the display panel <b>141</b> based on the type of the touch event. Although the touch panel <b>131</b> and the display panel <b>141</b> are used as two independent parts in <figref idref="DRAWINGS">FIG. <b>1</b></figref> to implement input and input functions of the electronic device, in some embodiments, the touch panel <b>131</b> and the display panel <b>141</b> may be integrated to implement the input and output functions of the electronic device.</p><p id="p-0057" num="0056">The electronic device <b>100</b> may further include at least one sensor <b>150</b> such as a light sensor, a motion sensor, and another sensor. Specifically, the light sensor may include an ambient light sensor and a proximity sensor. The ambient light sensor may adjust luminance of the display panel <b>141</b> based on brightness of ambient light. The proximity sensor may turn off the display panel <b>141</b> and/or backlight when the electronic device moves to an ear. As a motion sensor, an accelerometer sensor may detect values of acceleration in all directions (usually, three axes), may detect a value and a direction of gravity when the accelerometer sensor is static, and may be used in an application for recognizing a posture (such as switching between landscape mode and portrait mode, a related game, or magnetometer posture calibration) of the electronic device, a function related to vibration recognition (such as a pedometer or a knock), or the like. Other sensors may include a gyro, a barometer, a hygrometer, a thermometer, or an infrared sensor that may be further disposed on the electronic device.</p><p id="p-0058" num="0057">The audio circuit <b>160</b>, a speaker <b>161</b>, and a microphone <b>162</b> may provide an audio interface between the user and the electronic device. The audio circuit <b>160</b> may convert received audio data into an electrical signal and then transmit the electrical signal to the speaker <b>161</b>, and the speaker <b>161</b> converts the electrical signal into a sound signal for output. In addition, the microphone <b>162</b> converts a collected sound signal into an electrical signal. The audio circuit <b>160</b> receives the electrical signal, converts the electrical signal into audio data, and then outputs the audio data to the processor <b>180</b> for processing, to send the audio data to, for example, another electronic device through the RF circuit <b>110</b>, or outputs the audio data to the memory <b>120</b> for further processing.</p><p id="p-0059" num="0058">Wi-Fi belongs to a short distance wireless transmission technology. The electronic device may help, by using the Wi-Fi module <b>170</b>, the user receive and send emails, browse a web page, access streaming media, and the like. The Wi-Fi module <b>170</b> provides wireless broadband internet access for the user. Although <figref idref="DRAWINGS">FIG. <b>1</b></figref> shows the Wi-Fi module <b>170</b>, it may be understood that the Wi-Fi module <b>170</b> is not a mandatory part of the electronic device <b>100</b>, and may be omitted based on a requirement without changing the essence of the present technology.</p><p id="p-0060" num="0059">The processor <b>180</b> is a control center of the electronic device. The processor <b>180</b> is connected to all parts of the entire electronic device by using various interfaces and lines, and performs various functions of the electronic device and data processing by running or executing the software program and/or the module stored in the memory <b>120</b> and invoking data stored in the memory <b>120</b>, to perform overall monitoring on the electronic device. Optionally, the processor <b>180</b> may include one or more processing units. Preferably, the processor <b>180</b> may integrate an application processor and a modem processor. The application processor mainly processes an operating system, a user interface, an application, and the like. The modem processor mainly processes wireless communication. It may be understood that the modem processor may not be integrated into the processor <b>180</b>.</p><p id="p-0061" num="0060">The electronic device <b>100</b> further includes the power supply <b>190</b> (for example, a battery) supplying power to all parts. Preferably, the power supply may be logically connected to the processor <b>180</b> by using a power management system, to implement functions such as charging management, discharging management, and power consumption management by using the power management system.</p><p id="p-0062" num="0061">The electronic device <b>100</b> may further include a camera <b>191</b>. The camera <b>191</b> is configured to capture a static image or a video. An optical image of an object is generated through the lens, and is projected onto a photosensitive element. The photosensitive element may be a charge coupled device (CCD) or a complementary metal-oxide-semiconductor (CMOS) phototransistor. The photosensitive element converts an optical signal into an electrical signal, and then transmits the electrical signal to an ISP for converting the electrical signal into a digital image signal. The ISP outputs the digital image signal to a DSP for processing. The DSP converts the digital image signal into a standard image signal in an RGB format, a YUV format, or the like. Optionally, the camera may be disposed in front or rear of the electronic device <b>100</b>. This is not limited in this embodiment of this application.</p><p id="p-0063" num="0062">Optionally, the electronic device <b>100</b> may include one camera, two cameras, three cameras, and the like. This is not limited in this embodiment of this application.</p><p id="p-0064" num="0063">For example, the electronic device <b>100</b> may include three cameras: one main camera, one wide-angle camera, and one long-focus camera.</p><p id="p-0065" num="0064">Optionally, when the electronic device <b>100</b> includes a plurality of cameras, the plurality of cameras may all be disposed in the front, or may all be disposed in the rear, or some of the cameras may be disposed in the front, and others may be disposed in the rear. This is not limited in this embodiment of this application.</p><p id="p-0066" num="0065">In addition, although not shown in the figure, the electronic device <b>100</b> may further include a flashlight module and the like.</p><p id="p-0067" num="0066">In addition, although not shown in the figure, the electronic device <b>100</b> may further include a Bluetooth module and the like.</p><p id="p-0068" num="0067">In addition, although not shown in the figure, the electronic device <b>100</b> may further include a neural-network (NN) processing unit (NPU). The NPU quickly processes input information by referring to a structure of a biological neural network, for example, by referring to a transfer mode between human brain neurons, and may further continuously perform self-learning. Applications such as intelligent cognition of the electronic device <b>100</b>, for example, image recognition, facial recognition, speech recognition, and text understanding, may be implemented through the NPU.</p><p id="p-0069" num="0068">A software system of the electronic device <b>100</b> may use a layered architecture, an event-driven architecture, a microkernel architecture, a micro service architecture, or a cloud architecture. <figref idref="DRAWINGS">FIG. <b>3</b></figref> is a schematic diagram of a software structure of an electronic device <b>100</b> according to an embodiment of this application. For example, an operating system of the electronic device <b>100</b> is an Android system. In some embodiments, the Android system is divided into four layers: an application layer, an application framework (FWK) layer, a system layer, and a hardware abstraction layer. The layers communicate with each other through a software interface.</p><p id="p-0070" num="0069">As shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the application layer may include a series of application packages, and the application packages may include applications such as Messages, Calendar, Camera, Videos, Navigation, Gallery, and Phone.</p><p id="p-0071" num="0070">The application framework layer provides an application programming interface (API) and a programming framework for an application at the application layer. The application framework layer may include some predefined functions, such as a function for receiving an event sent by the application framework layer.</p><p id="p-0072" num="0071">As shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the application framework layer may include a window manager, a resource manager, a notification manager, and the like.</p><p id="p-0073" num="0072">The window manager is configured to manage a window program. The window manager may obtain a size of a display, determine whether there is a status bar, perform screen locking, take a screenshot, and the like. The content provider is configured to: store and obtain data, and enable the data to be accessed by an application. The data may include a video, an image, audio, calls that are made and received, a browsing history and a browsing bookmark, an address book, and the like.</p><p id="p-0074" num="0073">The resource manager provides various resources for an application such as a localized character string, an icon, a picture, a layout file, and a video file.</p><p id="p-0075" num="0074">The notification manager enables an application to display notification information in a status bar, and may be configured to convey a notification type message. The displayed notification information may automatically disappear after a short pause without user interaction. For example, the notification manager is configured to notify download completion, provide a message notification, and the like. The notification manager may alternatively be a notification that appears in a top status bar of the system in a form of a graph or a scroll bar text, for example, a notification of an application running on the background or a notification that appears on a screen in a form of a dialog window. For example, text information is displayed in the status bar, an announcement is produced, the electronic device vibrates, or an indicator light blinks.</p><p id="p-0076" num="0075">The application framework layer may further include: a view system, where the view system includes visual controls such as a control for displaying a text and a control for displaying a picture. The view system may be configured to construct an application. A display interface may include one or more views. For example, a display interface including an SMS message notification icon may include a view for displaying a text and a view for displaying a picture.</p><p id="p-0077" num="0076">The phone manager is configured to provide a communication function of the electronic device <b>100</b>, for example, management of a call status (including answering, declining, or the like).</p><p id="p-0078" num="0077">The system layer may include a plurality of functional modules, for example, a sensor service module, a physical status identification module, and a three-dimensional graphics processing library (for example, OpenGL ES).</p><p id="p-0079" num="0078">The sensor service module is configured to monitor sensor data uploaded by various sensors at a hardware layer, to determine a physical status of the electronic device <b>100</b>.</p><p id="p-0080" num="0079">The physical status identification module is configured to: analyze and identify a user gesture, a face, and the like.</p><p id="p-0081" num="0080">The three-dimensional graphics processing library is configured to implement three-dimensional graphics drawing, image rendering, composition, layer processing, and the like.</p><p id="p-0082" num="0081">The system layer may further include: a surface manager, configured to manage a display subsystem and provide fusion of 2D and 3D layers for a plurality of applications.</p><p id="p-0083" num="0082">A media library supports playback and recording in a plurality of commonly used audio and video formats, static image files, and the like. The media library may support a plurality of audio and video coding formats, for example, MPEG-4, H.264, MP3, AAC, AMR, JPG, and PNG.</p><p id="p-0084" num="0083">The hardware abstraction layer is a layer between hardware and software. The hardware abstraction layer may include a display driver, a camera driver, a sensor driver, and the like, and is configured to drive related hardware, for example, a display, a camera, and a sensor, at the hardware layer.</p><p id="p-0085" num="0084">The system layer may further include an image processing library. After a camera application is opened, the camera application may obtain a to-be-detected image collected by the camera of the electronic device, and determine impact of a light source on clarity of the to-be-detected image by performing light source region detection and foreground region detection on the to-be-detected image, to determine whether backlight blur exists in the to-be-detected image.</p><p id="p-0086" num="0085">The following embodiments may be implemented in the electronic device <b>100</b> having the foregoing hardware structure/software structure. In the following embodiments, the electronic device <b>100</b> is used as an example to describe the image detection method provided in embodiments of this application.</p><p id="p-0087" num="0086"><figref idref="DRAWINGS">FIG. <b>4</b><i>a </i></figref>shows a graphical user interface (GUI) of the electronic device (for example, a mobile phone) shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref> and <figref idref="DRAWINGS">FIG. <b>3</b></figref>. The GUI may be a home screen <b>401</b> of the electronic device. After detecting an operation that a user taps an icon <b>402</b> of a facial skin status evaluation application (APP) on the home screen <b>401</b>, the electronic device may open the facial skin status evaluation application. <figref idref="DRAWINGS">FIG. <b>4</b><i>b </i></figref>shows another GUI. The GUI may be referred to as a processing interface <b>403</b> of a facial skin status evaluation app. After the electronic device detects that a user taps an icon <b>404</b> for obtaining a photo in the processing interface <b>403</b>, the electronic device may open an album, and the user selects a picture from the album and loads the picture onto an image box <b>405</b>. Alternatively, after tapping an icon <b>404</b>, a user starts a camera and performs shooting by using the camera in a first shooting mode, and then loads a shot image onto an image box <b>405</b>. A preview image that exists in the first shooting mode after the camera is started may be further displayed in the image box <b>405</b>.</p><p id="p-0088" num="0087">After a to-be-detected image is loaded onto the image box <b>405</b>, the electronic device may automatically start to detect the to-be-detected image in the image box <b>405</b>, to detect whether the to-be-detected image is blurred.</p><p id="p-0089" num="0088">In a possible implementation, the processing interface <b>403</b> includes a first control that is used to indicate to perform blur detection. When the electronic device <b>100</b> detects that the user taps the first control, the electronic device detects the to-be-detected image in the image box <b>405</b>, to detect whether the to-be-detected image is blurred. Specifically, a light source region of the to-be-detected image and a foreground region of the to-be-detected image may be determined, a blurring degree of the image is determined based on the light source region and the foreground region, and whether the image is blurred is determined based on the blurring degree of the image.</p><p id="p-0090" num="0089">Specifically, during blur detection, the electronic device <b>100</b> performs color space conversion on the to-be-detected image to obtain a brightness value of each pixel of the image obtained through the color space conversion, and determines, by comparing the brightness value of each pixel with a preset brightness threshold, a quantity of pixels that exceeds the preset brightness threshold, to determine the light source region. Then, the electronic device determines a foreground target in the to-be-detected image by using a feature point detection method, and determines the foreground region by determining a location of the foreground target in the to-be-detected image. The electronic device calculates the blurring degree of the to-be-detected image based on a quantity of all pixels in the light source region and a quantity of all pixels in the foreground region, and compares the blurring degree of the to-be-detected image with a predetermined threshold. If the blurring degree of the image is greater than or equal to the predetermined threshold, the electronic device determines that backlight blur exists in the to-be-detected image. If the blurring degree of the image is less than the predetermined threshold, the electronic device determines that backlight blur does not exist in the to-be-detected image.</p><p id="p-0091" num="0090">If the electronic device detects that an image shot in a backlight condition or a frame of image in a preview image in a backlight condition is blurred, the electronic device switches a current shooting mode from the first shooting mode to a second shooting mode, where the first shooting mode is different from the second shooting mode. Alternatively, if the electronic device detects that the to-be-detected image obtained from the album is blurred in a backlight condition, the electronic device prompts the user to select another image, or re-shoot an image.</p><p id="p-0092" num="0091"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a schematic flowchart of an implementation of an image detection method according to an embodiment of this application. The method may be implemented in the electronic device (for example, a mobile phone or a tablet computer) shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref> or <figref idref="DRAWINGS">FIG. <b>3</b></figref>. As shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the method may include the following steps.</p><p id="p-0093" num="0092">S<b>101</b>: Obtain a to-be-detected image.</p><p id="p-0094" num="0093">The to-be-detected image may be an image currently shot, or may be a frame of image that exists in a preview image after a camera is started, or may be an image in an album, or the like.</p><p id="p-0095" num="0094">For example, after the user taps the icon <b>404</b> shown in <figref idref="DRAWINGS">FIG. <b>4</b><i>b</i></figref>, the electronic device opens an album for the user to select a shot photo to obtain the to-be-detected image, and may load the obtained to-be-detected image onto the image box <b>405</b>. Alternatively, after tapping the icon <b>404</b>, the user starts the camera and shoots an image by using the camera in the first shooting mode to obtain the to-be-detected image, and then loads the shot to-be-detected image onto the image box <b>405</b>. A preview image that exists in the first shooting mode after the camera is started may be further displayed in the image box <b>405</b>, and a frame of image in the preview image is used as the to-be-detected image.</p><p id="p-0096" num="0095">Specifically, to evaluate a facial skin status, the obtained image needs to include a face image. After the image including the face image is obtained, a part of the image including only the face image may be captured as the to-be-detected image, that is, the to-be-detected image obtained in this embodiment may be a to-be-detected image including the face image.</p><p id="p-0097" num="0096">S<b>102</b>: Determine a light source region of the to-be-detected image and a foreground region of the to-be-detected image.</p><p id="p-0098" num="0097">Specifically, light source region detection is performed on the to-be-detected image to determine the light source region of the to-be-detected image. Color space conversion is performed on the to-be-detected image to determine brightness of all pixels of the image obtained through the color space conversion, and the light source region is determined based on the brightness of all the pixels. Then, the image obtained through the color space conversion is re-converted to an original color space, a foreground target of the to-be-detected image is detected based on a feature point detection mode, and a location of the foreground target in the to-be-detected image is determined, to determine the foreground region.</p><p id="p-0099" num="0098">For example, after detecting that the to-be-detected image is loaded onto the image box <b>405</b>, the electronic device <b>100</b> automatically performs blur detection on the to-be-detected image loaded onto the image box <b>405</b>.</p><p id="p-0100" num="0099">For example, when the user taps a first control, blur detection on the to-be-detected image loaded onto the image box <b>405</b> may be triggered. In this case, the electronic device <b>100</b> performs light source region detection on the to-be-detected image to determine the light source region. The light source region detection may be: performing color space conversion on the image to obtain a brightness value of each pixel, and determining the light source region according to a threshold segmentation method; and then re-converting the image obtained through the color space conversion to an original color space, detecting a foreground target in the to-be-detected image based on a feature point detection mode, and determining a location of the foreground target in the to-be-detected image, to determine the foreground region.</p><p id="p-0101" num="0100">In an embodiment, the determining a light source region of the to-be-detected image includes: performing color space conversion on the to-be-detected image, and obtaining a brightness value of each pixel of the image obtained through the color space conversion; and determining a region of a pixel whose brightness value is greater than a preset brightness threshold as the light source region of the to-be-detected image.</p><p id="p-0102" num="0101">Specifically, the light source region can be determined based on the brightness value of each pixel of the to-be-detected image by using a threshold segmentation method. The image is first converted to an HSV (Hue, Saturation, Value) color space or an LAB (CIELAB color model) color space through color space conversion, so that the brightness value of each pixel of the image can be obtained. It may be understood that the image may also be converted to another color space to determine the brightness value of each pixel of the image. This is not limited herein.</p><p id="p-0103" num="0102">Specifically, if the image is converted to the HSV color space, a V value (Value) of each pixel is the brightness value of the pixel. If the image is converted to the LAB color space, an L value of each pixel is the brightness value of the pixel. It should be noted that the brightness value may also be obtained by performing mathematical transformation on an original brightness value in a monotonically increasing/decreasing manner, for example, L&#x2032;=2L.</p><p id="p-0104" num="0103">Specifically, the preset brightness threshold may be determined by viewing, by using a mask generation method, a light source image obtained through segmentation. <figref idref="DRAWINGS">FIG. <b>6</b><i>a </i></figref>shows an image in a current color space. <figref idref="DRAWINGS">FIG. <b>6</b><i>b </i></figref>shows an image obtained through light source segmentation. A brightness value of each pixel of the image obtained through the light source segmentation is compared with the preset brightness threshold. If the brightness value of the pixel is greater than the preset brightness threshold, it is determined that the pixel is a pixel in the light source region. Therefore, only a region of a pixel whose brightness value is greater than the preset brightness threshold needs to be determined as the light source region of the to-be-detected image.</p><p id="p-0105" num="0104">In an embodiment, the determining a foreground region of the to-be-detected image includes: detecting a foreground target of the to-be-detected image; and determining a location of the foreground target in the to-be-detected image, and determining the location of the foreground target in the to-be-detected image as the foreground region of the to-be-detected image.</p><p id="p-0106" num="0105">It should be noted that the foreground target may be a target having a dynamic feature in the to-be-detected image, for example, a person or an animal, or the foreground target may be a scene that is close to a viewer and that has a static feature, for example, a flower or food.</p><p id="p-0107" num="0106">In this embodiment, a trained foreground detection model may be used to detect the foreground target in the to-be-detected image. For example, the foreground detection model may be a model having a foreground target detection function, such as a single shot multibox detector (SSD). Certainly, another foreground detection manner may also be used. For example, whether a foreground target (for example, a face) exists in the to-be-detected image is detected by using a mode recognition algorithm, and after it is detected that the foreground target exists, a location of the foreground target in the to-be-detected image is determined by using a target positioning algorithm or a target tracking algorithm.</p><p id="p-0108" num="0107">It should be noted that other foreground target detection solutions that can be easily figured out by persons skilled in the art within the technical scope disclosed in the present technology shall also fall within the protection scope of the present technology.</p><p id="p-0109" num="0108">An example in which the trained foreground detection model is used to detect the foreground target in the to-be-detected image is used to describe a specific training process of the foreground detection model: pre-obtaining a sample picture and a detection result corresponding to the sample picture, where the detection result corresponding to the sample picture includes a category and a location of each foreground target in the sample picture; detecting the foreground target in the sample picture by using an initial foreground detection model, and calculating detection accuracy of the initial foreground detection model based on the pre-obtained detection result corresponding to the sample picture; and if the detection accuracy is less than a preset first detection threshold, adjusting a parameter of the initial foreground detection model, detecting the sample picture by using a foreground detection model whose parameter is adjusted until detection accuracy of the adjusted foreground detection model is greater than or equal to the first detection threshold, and using the foreground detection model as the trained foreground detection model, where a parameter adjustment method includes but is not limited to a stochastic gradient descent algorithm, a dynamic update algorithm, and the like.</p><p id="p-0110" num="0109">The foreground target is determined by performing, based on the foreground detection model, foreground target detection on the to-be-detected image that is converted to the original color space, and then the foreground region is determined based on the location of the foreground target in the to-be-detected image.</p><p id="p-0111" num="0110">For example, the foreground target is a face image region. The foreground detection model may be used to perform face feature detection according to the HIA276 feature point detection method, and then select a foreground region including a face. In this embodiment, the foreground region detection may be implemented by the neural-network (NN) processing unit (NPU) of the electronic device <b>100</b>. The NPU performs facial recognition on the to-be-detected image, selects, by using a rectangle, the foreground region including the face (the foreground target), and automatically outputs an area of the target rectangular region.</p><p id="p-0112" num="0111">For example, a rectangular region shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref> is a foreground region, and a specific location of the foreground region can be determined by determining location coordinates of pixels of four vertices of the rectangle in the to-be-detected image. It should be understood that the foreground region including the face (the foreground target) may alternatively be selected by using a circle, may be selected by using a triangle, or may be selected by using a plurality of shapes such as a pentagon.</p><p id="p-0113" num="0112">An area of the foreground region can be determined by calculating the area of the rectangular region. The location coordinates of the four vertices of the rectangular region are determined, so that a length and a width of the rectangular region are determined, and the area of the rectangular region is determined, that is, the area of the foreground region is obtained.</p><p id="p-0114" num="0113">It should be noted that the foreground target may alternatively be another object. For example, as shown in <figref idref="DRAWINGS">FIG. <b>8</b><i>a </i></figref>and <figref idref="DRAWINGS">FIG. <b>8</b><i>b</i></figref>, the foreground target is a foam box.</p><p id="p-0115" num="0114">S<b>103</b>: Determine a blurring degree of the to-be-detected image based on the light source region and the foreground region.</p><p id="p-0116" num="0115">Specifically, the blurring degree of the to-be-detected image is determined based on a quantity of all pixels in the light source region and a quantity of all pixels in the foreground region.</p><p id="p-0117" num="0116">Specifically, a size of the light source region is a most important factor that affects whether the to-be-detected image is blurred, and a size of the foreground region is also an important factor that affects whether the to-be-detected image is blurred. Therefore, to determine the blurring degree of the to-be-detected image, a function value of a strict increasing function of a ratio of a quantity of all pixels in the light source region to a quantity of all pixels in the foreground region may be used as an impact degree of whether a backlight source causes blurring of the to-be-detected image, that is, a function value of a strict increasing function of a ratio of a quantity of pixels whose brightness values are greater than a preset brightness threshold to the quantity of all pixels in the foreground region is used as the blurring degree S of the to-be-detected image.</p><p id="p-0118" num="0117">In an embodiment, a calculation formula of the blurring degree S is as follows: S=f(a, b)=a/b, where a is the quantity of all pixels in the light source region, b is the quantity of all pixels in the foreground region, and S is the blurring degree.</p><p id="p-0119" num="0118">In another embodiment, a calculation formula of the blurring degree S is as follows: S=f(a, b)=log(a/b), where a is the quantity of all pixels in the light source region, b is the quantity of all pixels in the foreground region, and S is the blurring degree.</p><p id="p-0120" num="0119">It should be noted that impact of the area of the light source region and the area of the foreground region on the blurring degree can be reflected provided that the blurring degree is a strict increasing function of a/b. Therefore, f(a, b) has infinite variations, for example, f(a, b)=log 2(a&#x2212;b).</p><p id="p-0121" num="0120">In another embodiment, the blurring degree of the to-be-detected image is determined based on a quantity of all pixels in the light source region and an area of the foreground region. A function value of a strict increasing function of a ratio of the quantity of all pixels in the light source region to the area of the foreground region is used as an impact degree of whether a backlight source causes blurring of the to-be-detected image, that is, a function value of a strict increasing function of a ratio of a quantity of pixels whose brightness values are greater than a preset brightness threshold to the area of the foreground region is used as the blurring degree S of the to-be-detected image.</p><p id="p-0122" num="0121">A calculation formula of the blurring degree S is as follows: S=f(a, b)=a/b, where a is the quantity of all pixels in the light source region, b is the area of the foreground region, and S is the blurring degree.</p><p id="p-0123" num="0122">In another embodiment, a calculation formula of the blurring degree S is as follows: S=f(a, b)=log(a/b), where a is the quantity of all pixels in the light source region, b is the area of the foreground region, and S is the blurring degree.</p><p id="p-0124" num="0123">In an embodiment, S<b>103</b> includes: determining whether a function value of an increasing function of a ratio of the quantity of all pixels in the light source region to the quantity of all pixels in the foreground region is greater than a predetermined threshold; and if the function value of the increasing function of the ratio of the quantity of all pixels in the light source region to the quantity of all pixels in the foreground region is greater than the predetermined threshold, determining that the to-be-detected image is a blurred image.</p><p id="p-0125" num="0124">Specifically, the blurring degree of the to-be-detected image (that is, the function value of the increasing function of the ratio of the quantity of all pixels in the light source region to the quantity of all pixels in the foreground region) is compared with the predetermined threshold. If the blurring degree of the image is greater than or equal to the predetermined threshold, it is determined that a detection result is that the to-be-detected image is the blurred image. If the blurring degree of the image is less than the predetermined threshold, it is determined that a detection result is that the to-be-detected image is a clear image.</p><p id="p-0126" num="0125">It should be noted that, the predetermined threshold may be a predetermined threshold obtained by performing parameter adjustment after training and testing are performed on a large quantity of sample images, that is, if the blurring degree of the to-be-detected image is greater than or equal to the predetermined threshold, it may be determined that the to-be-detected image is the blurred image.</p><p id="p-0127" num="0126">Specifically, an image classifier may be constructed based on a KNN (K-Nearest Neighbor) classifier. A principle of the KNN classifier is to compare an image in a test set with each image in a training set, and assign a label value of a most similar image in the training set to the image. For example, in CIFAR-10, 32&#xd7;32&#xd7;3 pixel blocks are compared. A simplest method is to compare pixels one by one, finally add all difference values to obtain a value of a difference between two images, and determine an image category according to a classification decision principle (for example, majority voting). When the image classifier is constructed, an initial model parameter and an initial predetermined threshold are pre-assigned to the image classifier. Training and testing the image classifier is a process of performing parameter adjustment on the initial model parameter and the predetermined threshold, so that an image classifier that meets a classification accuracy requirement can be obtained.</p><p id="p-0128" num="0127">Specifically, traversal optimization may be further performed based on sample images in a training set to search for a trainer or a combination of trainers with a best classification effect, and the trainer or the combination of trainers is used as the image classifier in this embodiment.</p><p id="p-0129" num="0128">Specifically, the image classifier is trained and tested based on the sample images to determine the predetermined threshold.</p><p id="p-0130" num="0129">Specifically, the sample images include sample images in a training set and sample images in a test set, the sample images in the training set include a sample image in a first training set and a sample image in a second training set, and the sample images in the test set include a sample image in a first test set and a sample image in a second test set. The sample image in the first training set is a sample image that is in the training set and that is labeled &#x201c;clear&#x201d;, the sample image in the second training set is a sample image that is in the training set and that is labeled &#x201c;blurred&#x201d;, the sample image in the first test set is a sample image that is in the test set and that is labeled &#x201c;clear&#x201d;, and the sample image in the second test set is a sample image that is in the test set and that is labeled &#x201c;blurred&#x201d;. For example, 100 sample images are grouped into two groups, a first group includes 50 sample images labeled &#x201c;blurred&#x201d;, and a second group includes 50 sample images labeled &#x201c;clear&#x201d;. Then, 30 sample images in the 50 sample images labeled &#x201c;blurred&#x201d; are used as sample images in the first training set, and 20 sample images in the 50 sample images labeled &#x201c;blurred&#x201d; are used as sample images in the first test set; and 30 sample images in the 50 sample images labeled &#x201c;clear&#x201d; are used as sample images in the second training set, and 20 sample images in the 50 sample images labeled &#x201c;clear&#x201d; are used as sample images in the second test set.</p><p id="p-0131" num="0130">There are many methods for training the image classifier, for example, an AadaBboost method based on a Hhaar feature and an SVM (support vector machine) method. Specifically, sample images in a training set and sample images in a test set are first constructed. Then, traversal optimization is performed based on the sample images in the training set to search for a trainer or a combination of trainers with a best classification effect, and the trainer or the combination of trainers is used as the image classifier in this embodiment. Finally, classification accuracy of the found image classifier is verified based on the sample images in the test set, and the image classifier can be used after an accuracy requirement (for example, accuracy of 70%) is met. If the requirement is not met, the initial model parameter and the predetermined threshold are adjusted, and the entire process is continuously repeated cyclically until the accuracy requirement is finally met, so that the classifier can determine the predetermined threshold.</p><p id="p-0132" num="0131">In this embodiment, after the foregoing parameter adjustment process, when the blurring degree S is equal to a/b, it is determined that the predetermined threshold may be 0.12; or when the blurring degree S is equal to log(a/b), it is determined that the preset blurring threshold is &#x2212;0.3269.</p><p id="p-0133" num="0132">It should be noted that the image detection method provided in this embodiment can be used not only for backlight blur detection of a face, but also for backlight blur detection of another object.</p><p id="p-0134" num="0133">For example, as shown in <figref idref="DRAWINGS">FIG. <b>8</b><i>a </i></figref>and <figref idref="DRAWINGS">FIG. <b>8</b><i>b</i></figref>, after images in <figref idref="DRAWINGS">FIG. <b>8</b><i>a </i></figref>and <figref idref="DRAWINGS">FIG. <b>8</b><i>b </i></figref>are respectively loaded onto the image box <b>405</b>, the first control is tapped to trigger a detection operation on the images. In this case, a detection result corresponding to <figref idref="DRAWINGS">FIG. <b>8</b><i>a </i></figref>is that the image is clear (backlight blur does not exist in the image); and a detection result corresponding to <figref idref="DRAWINGS">FIG. <b>8</b><i>b </i></figref>is that the image is blurred (backlight blur exists in the image).</p><p id="p-0135" num="0134">In this embodiment, after obtaining the to-be-detected image, the electronic device can automatically perform backlight blur detection on the obtained to-be-detected image, or may perform backlight blur detection after the user taps the first control, to output a corresponding detection result. If it is detected that backlight blur exists in the image, the user is prompted to re-obtain an image, for example, select another image or switch to a Flash mode for photographing again to obtain an image, so as to improve accuracy of facial skin evaluation.</p><p id="p-0136" num="0135">For example, as shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref>, after a to-be-detected image in <figref idref="DRAWINGS">FIG. <b>9</b></figref> is loaded onto the image box <b>405</b>, the first control is tapped to trigger a detection operation on the to-be-detected image. In this case, a detection result corresponding to <figref idref="DRAWINGS">FIG. <b>9</b></figref> is that the image is blurred (backlight blur exists in the image).</p><p id="p-0137" num="0136">In an embodiment, the obtaining a to-be-detected image includes: obtaining a preview frame image in a first shooting mode; and correspondingly, the determining a blurring degree of the to-be-detected image based on the light source region and the foreground region includes: if it is determined, based on the light source region and the foreground region, that the preview frame image is a blurred image, switching a current shooting mode from the first shooting mode to a second shooting mode, where the first shooting mode is different from the second shooting mode.</p><p id="p-0138" num="0137">In this embodiment, the obtained to-be-detected image is the preview frame image in the first shooting mode. Because the preview frame image in the first shooting mode is the blurred image, if the image shot in the first shooting mode is definitely blurred, the current shooting mode needs to be switched from the first shooting mode to the second shooting mode.</p><p id="p-0139" num="0138">For example, the first shooting mode is a Torch mode, and the second shooting mode is a Flash mode. The switching a current shooting mode from the first shooting mode to a second shooting mode is specifically: An electronic device <b>100</b> sends a control instruction to a flashlight module of the electronic device, to switch a flashlight from a steady-on mode to a mode in which the flashlight flashes once during photographing.</p><p id="p-0140" num="0139">According to the image detection method provided in this embodiment of this application, an artificial intelligence terminal can effectively detect whether an image shot in a backlight condition is blurred, and determine impact of a light source on clarity of the image by performing light source region detection and foreground region detection on the image, to effectively detect whether the image shot in the backlight condition is blurred.</p><p id="p-0141" num="0140">It should be understood that sequence numbers of the steps do not mean an execution sequence in the foregoing embodiments. The execution sequence of the processes should be determined based on functions and internal logic of the processes, and should not constitute any limitation on the implementation processes of the embodiments of this application.</p><p id="p-0142" num="0141">Corresponding to the image detection method in the foregoing embodiment, <figref idref="DRAWINGS">FIG. <b>10</b></figref> is a block diagram of a structure of an image detection apparatus according to an embodiment of this application. For ease of description, only a part related to embodiments of this application are shown.</p><p id="p-0143" num="0142">Refer to <figref idref="DRAWINGS">FIG. <b>10</b></figref>. The image detection apparatus includes: an image obtaining module <b>11</b>, configured to obtain a to-be-detected image; a first determining module <b>12</b>, configured to determine a light source region of the to-be-detected image and a foreground region of the to-be-detected image; and a second determining module <b>13</b>, configured to determine a blurring degree of the image based on the light source region and the foreground region.</p><p id="p-0144" num="0143">Optionally, the first determining module <b>12</b> includes: a conversion unit, configured to: perform color space conversion on the to-be-detected image, and obtain a brightness value of each pixel of the image obtained through the color space conversion; and a first determining unit, configured to determine a region of a pixel whose brightness value is greater than a preset brightness threshold as the light source region of the to-be-detected image.</p><p id="p-0145" num="0144">Optionally, the first determining module <b>12</b> includes: a detection unit, configured to detect a foreground target in the to-be-detected image; and a second determining unit, configured to: determine a location of the foreground target in the to-be-detected image, and determine the location of the foreground target in the to-be-detected image as the foreground region of the to-be-detected image.</p><p id="p-0146" num="0145">Optionally, the second determining module <b>13</b> includes: a third determining unit, configured to determine a blurring degree of the to-be-detected image based on a quantity of all pixels in the light source region and a quantity of all pixels in the foreground region.</p><p id="p-0147" num="0146">Optionally, the third determining unit includes: a determining unit, configured to determine whether a function value of an increasing function of a ratio of the quantity of all pixels in the light source region to the quantity of all pixels in the foreground region is greater than a predetermined threshold; and a fourth determining unit, configured to: if the function value of the increasing function of the ratio of the quantity of all pixels in the light source region to the quantity of all pixels in the foreground region is greater than the predetermined threshold, determine that the to-be-detected image is a blurred image.</p><p id="p-0148" num="0147">Optionally, the third determining unit is further configured to determine the blurring degree of the to-be-detected image based on the quantity of all pixels in the light source region and an area of the foreground region.</p><p id="p-0149" num="0148">Optionally, the image obtaining module <b>11</b> includes: a first image obtaining unit, configured to obtain a preview frame image in a first shooting mode.</p><p id="p-0150" num="0149">Correspondingly, the second determining module <b>13</b> is further configured to: if it is determined, based on the light source region and the foreground region, that the preview frame image is a blurred image, switch a current shooting mode from the first shooting mode to a second shooting mode, where the first shooting mode is different from the second shooting mode.</p><p id="p-0151" num="0150">It should be noted that content such as information exchange and an execution process between the foregoing modules/units is based on a same concept as that in the method embodiments of this application. For details about specific functions and technical effects of the content, refer to the method embodiments.</p><p id="p-0152" num="0151">It may be clearly understood by persons skilled in the art that, for convenient and brief description, division into the foregoing functional units and modules is merely used as an example for illustration. In actual application, the foregoing functions can be allocated to different functional units and modules for implementation based on a requirement, that is, an inner structure of the apparatus is divided into different functional units or modules to implement all or some of the functions described above. Functional units and modules in the embodiments may be integrated into one processing unit, or each of the units may exist alone physically, or two or more units may be integrated into one unit. The integrated unit may be implemented in a form of hardware, or may be implemented in a form of a software functional unit. In addition, specific names of the functional units or modules are merely provided for distinguishing between the units or modules, but are not intended to limit the protection scope of this application. For a specific working process of the units or modules in the foregoing system, refer to a corresponding process in the foregoing method embodiments.</p><p id="p-0153" num="0152">Therefore, the image detection apparatus provided in this embodiment can also determine impact of a light source on clarity of the image by performing light source region detection and foreground region detection on the image, to determine whether backlight blur exists in the image, so as to improve accuracy of an evaluation result of a facial skin status.</p><p id="p-0154" num="0153"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a schematic diagram of a structure of an electronic device according to an embodiment of this application. As shown in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, the electronic device <b>11</b> in this embodiment includes at least one processor <b>110</b> (only one processor is shown in <figref idref="DRAWINGS">FIG. <b>11</b></figref>), a memory <b>111</b>, and a computer program <b>112</b> that is stored in the memory <b>111</b> and that can be run on the at least one processor <b>110</b>. When executing the computer program <b>112</b>, the processor <b>110</b> implements steps in any one of the foregoing embodiments of the image detection method.</p><p id="p-0155" num="0154">The electronic device <b>11</b> may be a computing device such as a desktop computer, a notebook computer, a palmtop computer, or a cloud server. The electronic device may include but is not limited to the processor <b>110</b> and the memory <b>111</b>. Persons skilled in the art may understand that <figref idref="DRAWINGS">FIG. <b>11</b></figref> is merely an example of the electronic device <b>11</b>, and does not constitute a limitation on the electronic device <b>11</b>. The electronic device may include more or fewer components than those shown in the figure, or may combine some components, or may have different components. For example, the electronic device may further include an input/output device, a network access device, or the like.</p><p id="p-0156" num="0155">The processor <b>110</b> may be a central processing unit (CPU). The processor <b>110</b> may alternatively be another general-purpose processor, a digital signal processor (DSP), an application-specific integrated circuit (ASIC), a field-programmable gate array (FPGA) or another programmable logic device, a discrete gate or a transistor logic device, or a discrete hardware component. The general-purpose processor may be a microprocessor, or the processor may be any conventional processor or the like.</p><p id="p-0157" num="0156">In some embodiments, the memory <b>111</b> may be an internal storage unit of the electronic device <b>11</b>, for example, a hard disk or memory of the electronic device <b>11</b>. In some other embodiments, the memory <b>111</b> may alternatively be an external storage device of the electronic device <b>11</b>, for example, a plug-connected hard disk, a smart media card (SMC), a secure digital (SD) card, or a flash card that is disposed on the electronic device <b>11</b>. Further, the memory <b>111</b> may alternatively include both an internal storage unit and an external storage device of the electronic device <b>11</b>. The memory <b>111</b> is configured to store an operating system, an application, a boot loader, data, another program, and the like, for example, program code of the computer program. The memory <b>111</b> may be further configured to temporarily store data that has been output or is to be output.</p><p id="p-0158" num="0157">An embodiment of this application further provides a network device. The network device includes at least one processor, a memory, and a computer program that is stored in the memory and that can run on the at least one processor. When executing the computer program, the processor implements steps in any one of the foregoing method embodiments.</p><p id="p-0159" num="0158">An embodiment of this application further provides a computer-readable storage medium. The computer-readable storage medium stores a computer program. When the computer program is executed by a processor, steps in the foregoing method embodiments can be implemented.</p><p id="p-0160" num="0159">An embodiment of this application provides a computer program product. When the computer program product runs on a mobile terminal, the mobile terminal is enabled to implement the steps in the foregoing method embodiments when executing the computer program product.</p><p id="p-0161" num="0160">When the integrated unit is implemented in a form of a software functional unit and sold or used as an independent product, the integrated unit may be stored in a computer-readable storage medium. Based on such an understanding, in this application, all or some of the procedures of the methods in the foregoing embodiments may be implemented by a computer program instructing related hardware. The computer program may be stored in a computer-readable storage medium. When the computer program is executed by a processor, the steps in the foregoing method embodiments can be implemented. The computer program includes computer program code, and the computer program code may be in a source code form, an object code form, an executable file form, some intermediate forms, or the like. The computer-readable medium may include at least any entity or apparatus that can carry computer program code to a photographing apparatus/terminal device, a recording medium, a computer memory, a read-only memory (ROM), a random access memory (RAM), an electrical carrier signal, a telecommunications signal, and a software distribution medium, for example, a USB flash drive, a removable hard disk, a magnetic disk, or an optical disk.</p><p id="p-0162" num="0161">In the foregoing embodiments, descriptions of all embodiments have respective focuses. For a part that is not described in detail or recorded in an embodiment, refer to related descriptions in other embodiments.</p><p id="p-0163" num="0162">Persons of ordinary skill in the art may be aware that, units and algorithm steps in the examples described with reference to embodiments disclosed in this specification can be implemented by electronic hardware or a combination of computer software and electronic hardware. Whether these functions are performed by hardware or software depends on particular applications and design constraints of the technical solutions. Persons skilled in the art may use different methods to implement the described functions for each particular application, but it should not be considered that the implementation goes beyond the scope of this application.</p><p id="p-0164" num="0163">In embodiments provided in this application, it should be understood that the disclosed apparatus/network device and method may be implemented in other manners. For example, the described apparatus/network device embodiment is merely an example. For example, division into the modules or units is merely logical function division and may be other division in an actual implementation. For example, a plurality of units or components may be combined or integrated into another system, or some features may be ignored or not performed. In addition, the displayed or discussed mutual couplings or direct couplings or communication connections may be implemented through some interfaces. The indirect couplings or communication connections between the apparatuses or units may be implemented in electronic, mechanical, or other forms.</p><p id="p-0165" num="0164">The units described as separate parts may or may not be physically separate, and parts displayed as units may or may not be physical units, and may be located in one position, or may be distributed on a plurality of network units. Some or all of the units may be selected based on an actual requirement to achieve the objectives of the solutions of embodiments.</p><p id="p-0166" num="0165">The foregoing embodiments are merely intended to describe the technical solutions of this application, but are not to limit this application. Although this application is described in detail with reference to the foregoing embodiments, persons of ordinary skill in the art should understand that they may still make modifications to the technical solutions described in the foregoing embodiments or make equivalent replacements to some technical features thereof, without departing from the spirit and scope of the technical solutions of the embodiments of this application.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An image detection method implemented via an electronic device, the image detection method comprising:<claim-text>obtaining a to-be-detected image;</claim-text><claim-text>determining a light source region of the to-be-detected image;</claim-text><claim-text>determining a foreground region of the to-be-detected image; and</claim-text><claim-text>determining a blurring degree of the to-be-detected image based on the light source region and the foreground region.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The image detection method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein determining the light source region of the to-be-detected image comprises:<claim-text>performing color space conversion on the to-be-detected image, and obtaining a brightness value of each pixel of an image obtained through the color space conversion; and</claim-text><claim-text>determining a region of a pixel whose brightness value is greater than a specified brightness threshold as the light source region of the to-be-detected image.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The image detection method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein determining the foreground region of the to-be-detected image comprises:<claim-text>detecting a foreground target of the to-be-detected image;</claim-text><claim-text>determining a location of the foreground target in the to-be-detected image; and</claim-text><claim-text>determining the location of the foreground target as the foreground region of the to-be-detected image.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The image detection method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein determining the blurring degree of the to-be-detected image based on the light source region and the foreground region comprises:<claim-text>determining the blurring degree of the to-be-detected image based on a quantity of all pixels in the light source region and a quantity of all pixels in the foreground region.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The image detection method according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein determining the blurring degree of the to-be-detected image based on the quantity of all pixels in the light source region and the quantity of all pixels in the foreground region comprises:<claim-text>determining whether a function value of an increasing function of a ratio of the quantity of all pixels in the light source region, to the quantity of all pixels in the foreground region, is greater than a threshold; and</claim-text><claim-text>if the function value of the increasing function of the ratio of the quantity of all pixels in the light source region, to the quantity of all pixels in the foreground region, is greater than the threshold, determining the to-be-detected image is a blurred image.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The image detection method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein determining the blurring degree of the to-be-detected image based on the light source region and the foreground region comprises:<claim-text>determining the blurring degree of the to-be-detected image based on a quantity of all pixels in the light source region and an area of the foreground region.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The image detection method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the obtaining the to-be-detected image comprises:<claim-text>obtaining a preview frame image in a first shooting mode, wherein</claim-text><claim-text>determining the blurring degree of the to-be-detected image based on the light source region and the foreground region comprises:<claim-text>if, based on the light source region and the foreground region, the preview frame image is a blurred image, switching a current shooting mode from the first shooting mode to a second shooting mode, wherein the first shooting mode is different from the second shooting mode.</claim-text></claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. (canceled)</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. An electronic device, comprising:<claim-text>a processor; and</claim-text><claim-text>a memory configured to store computer be run on the processor, wherein when executing the computer program, the processor readable instructions that, when executed by the processor, cause the electronic device to:<claim-text>obtain a to-be-detected image;</claim-text><claim-text>determine a light source region of the to-be-detected image;</claim-text><claim-text>determine a foreground region of the to-be-detected image; and</claim-text><claim-text>determine a blurring degree of the to-be-detected image based on the light source region and the foreground region.</claim-text></claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. A non-transitory computer-readable storage medium configured to store computer readable instructions that, when executed by a processor, cause the processor to provide execution comprising:<claim-text>obtaining a to-be-detected image;</claim-text><claim-text>determining a light source region of the to-be-detected image;</claim-text><claim-text>determining a foreground region of the to-be-detected image; and</claim-text><claim-text>determining a blurring degree of the to-be-detected image based on the light source region and the foreground region.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The electronic device according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the electronic device is further caused to:<claim-text>perform color space conversion on the to-be-detected image, and obtaining a brightness value of each pixel of an image obtained through the color space conversion; and</claim-text><claim-text>determine a region of a pixel whose brightness value is greater than a specified brightness threshold as the light source region of the to-be-detected image.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The electronic device according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the electronic device is further caused to:<claim-text>detect a foreground target of the to-be-detected image;</claim-text><claim-text>determine a location of the foreground target in the to-be-detected image; and</claim-text><claim-text>determine the location of the foreground target as the foreground region of the to-be-detected image.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The electronic device according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the electronic device is further caused to:<claim-text>determine the blurring degree of the to-be-detected image based on a quantity of all pixels in the light source region and a quantity of all pixels in the foreground region.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The electronic device according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the electronic device is further caused to:<claim-text>determine whether a function value of an increasing function of a ratio of the quantity of all pixels in the light source region, to the quantity of all pixels in the foreground region, is greater than a threshold; and<claim-text>if the function value of the increasing function of the ratio of the quantity of all pixels in the light source region, to the quantity of all pixels in the foreground region, is greater than the threshold, determine the to-be-detected image is a blurred image.</claim-text></claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The electronic device according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the electronic device is further caused to:<claim-text>determine the blurring degree of the to-be-detected image based on a quantity of all pixels in the light source region and an area of the foreground region.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The electronic device according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the electronic device is further caused to:<claim-text>obtain a preview frame image in a first shooting mode, wherein</claim-text><claim-text>determining the blurring degree of the to-be-detected image based on the light source region and the foreground region comprises:<claim-text>if, based on the light source region and the foreground region, the preview frame image is a blurred image, switching a current shooting mode from the first shooting mode to a second shooting mode, wherein the first shooting mode is different from the second shooting mode.</claim-text></claim-text></claim-text></claim></claims></us-patent-application>