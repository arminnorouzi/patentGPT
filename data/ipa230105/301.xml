<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230000302A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230000302</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17756869</doc-number><date>20201204</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>JP</country><doc-number>2019-225886</doc-number><date>20191213</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>47</class><subclass>L</subclass><main-group>11</main-group><subgroup>40</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>77</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>47</class><subclass>L</subclass><main-group>11</main-group><subgroup>4011</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>7715</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>0008</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>47</class><subclass>L</subclass><main-group>2201</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>2201</main-group><subgroup>07</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">CLEANING AREA ESTIMATION DEVICE AND METHOD FOR ESTIMATING CLEANING AREA</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>SONY GROUP CORPORATION</orgname><address><city>TOKYO</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>OYAIZU</last-name><first-name>HIDEKI</first-name><address><city>TOKYO</city><country>JP</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>SHINGYOUCHI</last-name><first-name>AKINORI</first-name><address><city>KANAGAWA</city><country>JP</country></address></addressbook></inventor></inventors></us-parties><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/JP2020/045153</doc-number><date>20201204</date></document-id><us-371c12-date><date>20220603</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A cleaning area estimation device (<b>30</b>) includes an estimation unit (<b>33</b>) that estimates dirt information (D<b>2</b>) about an inside of a cleaning area on the basis of image information (D<b>1</b>) obtained by imaging a cleaning area by an imaging device (<b>10</b>), and a generation unit (<b>34</b>) that generates map information (D<b>3</b>) indicating a map of the dirt information about the cleaning area on the basis of the estimated time-series dirt information (D<b>2</b>).</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="77.22mm" wi="158.75mm" file="US20230000302A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="239.27mm" wi="128.95mm" orientation="landscape" file="US20230000302A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="226.82mm" wi="89.24mm" orientation="landscape" file="US20230000302A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="197.70mm" wi="157.14mm" file="US20230000302A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="100.50mm" wi="138.94mm" file="US20230000302A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="222.08mm" wi="130.81mm" file="US20230000302A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="239.35mm" wi="128.86mm" orientation="landscape" file="US20230000302A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="113.28mm" wi="76.62mm" file="US20230000302A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="239.35mm" wi="128.86mm" orientation="landscape" file="US20230000302A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="200.91mm" wi="130.81mm" file="US20230000302A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="117.09mm" wi="153.25mm" file="US20230000302A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="239.35mm" wi="128.86mm" orientation="landscape" file="US20230000302A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="182.63mm" wi="130.81mm" file="US20230000302A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="131.49mm" wi="158.41mm" file="US20230000302A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="134.87mm" wi="149.01mm" file="US20230000302A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">FIELD</heading><p id="p-0002" num="0001">The present disclosure relates to a cleaning area estimation device and a method for estimating a cleaning area.</p><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">Patent Literature 1 discloses a technique for visualizing a cleaning state of an area to be cleaned by displaying an amount and a type of dirt in a room to be cleaned in a form of map and displaying them in augmented reality (AR) during cleaning.</p><heading id="h-0003" level="1">CITATION LIST</heading><heading id="h-0004" level="1">Patent Literature</heading><p id="p-0004" num="0000"><ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0003">Patent Literature 1: JP 2019-82807 A</li></ul></p><heading id="h-0005" level="1">SUMMARY</heading><heading id="h-0006" level="1">Technical Problem</heading><p id="p-0005" num="0004">In the above conventional technique, an automatic cleaner needs to be operated in advance in the cleaning area in order to acquire dirt information.</p><p id="p-0006" num="0005">In view of this, the present disclosure provides a cleaning area estimation device and a method for estimating a cleaning area, which are capable of supporting determination of necessity of cleaning by using image information obtained by imaging the cleaning area.</p><heading id="h-0007" level="1">Solution to Problem</heading><p id="p-0007" num="0006">To solve the problems described above, a cleaning area estimation device according to an embodiment of the present disclosure includes: an estimation unit configured to estimate dirt information about an inside of a cleaning area on a basis of image information obtained by imaging the cleaning area by an imaging device; and a generation unit configured to generate map information indicating a map of the dirt information about the cleaning area on a basis of the dirt information that is estimated and in a time series.</p><p id="p-0008" num="0007">Moreover, a method for estimating a cleaning area according to an embodiment of the present disclosure includes: estimating, by a computer, dirt information about an inside of a cleaning area on a basis of image information obtained by imaging a cleaning area by an imaging device; and generating, by the computer, map information indicating a map of the dirt information about the cleaning area on a basis of the dirt information that is estimated and in a time series.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0008" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram illustrating an example of a configuration of a cleaning area estimation system according to a first embodiment.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a diagram illustrating an example of the relationship between an imaging device and a cleaning area according to the first embodiment.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram for explaining an example of an area to be extracted from the cleaning area according to the first embodiment.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram illustrating an example of the relationship between time and an accumulated amount of dirt of the cleaning area according to the first embodiment.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram illustrating an example of map information according to the first embodiment.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flowchart illustrating an example of a processing procedure executed by a cleaning area estimation device according to the first embodiment.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a diagram illustrating an example of a configuration of a cleaning area estimation system according to a modification of the first embodiment.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a diagram for explaining the relationship between an object and the degree of ease with which dust accumulates.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a diagram illustrating an example of a configuration of a cleaning area estimation system according to a second embodiment.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a flowchart illustrating an example of a processing procedure executed by a cleaning area estimation device according to the second embodiment.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a diagram for explaining an example of dirt in the cleaning area.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a diagram illustrating an example of a configuration of a cleaning area estimation system according to a third embodiment.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a flowchart illustrating an example of a processing procedure executed by a cleaning area estimation device according to the third embodiment.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a diagram illustrating an example of map information generated by the cleaning area estimation device according to the third embodiment.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a hardware configuration diagram illustrating an example of a computer that implements functions of a cleaning area estimation device.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0009" level="1">DESCRIPTION OF EMBODIMENTS</heading><p id="p-0024" num="0023">Hereinafter, embodiments of the present disclosure will be described in detail with reference to the drawings. In the following embodiments, the same parts are denoted by the same reference signs, and redundant description will be omitted.</p><heading id="h-0010" level="1">First Embodiment</heading><heading id="h-0011" level="1">Outline of Cleaning Area Estimation System According to First Embodiment</heading><p id="p-0025" num="0024">Cleaning removes trash, dust, dirt, and the like. In cleaning periodically performed in an office building, a warehouse, or the like, each room is cleaned regardless of the presence or absence of dirt, which increases the cost required for cleaning. For example, if a place to be intensively cleaned and a place requiring only light cleaning can be discriminated, the time allocation for cleaning can be changed, and more efficient cleaning can be realized in the same time. However, an amount and a type of dirt needs to be actually checked and determined by a person who performs cleaning. For this reason, an operation occurs in which a cleaning person, a cleaning robot, or the like always visits and checks a room that does not actually need to be cleaned. There is hereby provided a cleaning map in which a cleaning area where cleaning is required can be estimated and the need for a check operation that entails going out to the cleaning area can be eliminated. The cleaning area includes, for example, a three-dimensional area and a planar area.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram illustrating an example of a configuration of a cleaning area estimation system according to a first embodiment. As illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, a cleaning area estimation system <b>1</b> includes an imaging device <b>10</b>, a sensor unit <b>20</b>, a cleaning area estimation device <b>30</b>, and a communication unit <b>40</b>. The cleaning area estimation device <b>30</b> is electrically connected to the imaging device <b>10</b>, the sensor unit <b>20</b>, and the communication unit <b>40</b>, and is configured to be able to transfer and receive various types of information. The cleaning area estimation device <b>30</b> estimates the state of dirt of the cleaning area on the basis of the image captured by the imaging device <b>10</b>, the detection result of the sensor unit <b>20</b>, and the like. The image includes, for example, a moving image, a still image, and the like.</p><p id="p-0027" num="0026">The imaging device <b>10</b> is provided to be able to image the cleaning area. The imaging device <b>10</b> includes, for example, a single or a plurality of cameras installed in the cleaning place. The imaging device <b>10</b> includes, for example, a far infrared camera, a visible light camera, a polarization camera, a time of flight (ToF) camera, an RGB camera, a stereo camera, a depth camera. The imaging device <b>10</b> may be configured to divide the cleaning area into a plurality of areas and to image each of the divided areas by a plurality of cameras. The imaging device <b>10</b> supplies the captured image information to the cleaning area estimation device <b>30</b>.</p><p id="p-0028" num="0027">For example, when sweat adheres to a table, a chair, a wall, or the like touched by a person due to temperature or humidity, the adhered portion becomes dirty. Similarly, for example, a portion of an object where an animal such as a pet has come in contact with becomes dirty. As such, an example in a case where the imaging device <b>10</b> is a far infrared camera will be described in the first embodiment in order to, for example, estimate motions of a person.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a diagram illustrating an example of the relationship between the imaging device <b>10</b> and a cleaning area <b>100</b> according to the first embodiment. As illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the imaging device <b>10</b> images the inside of a room <b>200</b> to be cleaned. The room <b>200</b> includes a table <b>201</b>, four chairs <b>202</b>, and a whiteboard <b>203</b>. The room <b>200</b> is used by two persons <b>300</b>. The imaging device <b>10</b> images an imaging area including part or all of the cleaning area <b>100</b>. The cleaning area <b>100</b> includes, for example, a floor of the room <b>200</b>, the table <b>201</b>, the chairs <b>202</b>, the whiteboard <b>203</b>, and the like and includes an area requiring determination whether or not to perform cleaning. The cleaning area <b>100</b> may be the entire area of the room <b>200</b> or a partial area of the room <b>200</b>. The imaging device <b>10</b> captures image information D<b>1</b> with which the temperature can be identified. The image information D<b>1</b> includes an infrared image. The image information D<b>1</b> includes, for example, an image indicating that the temperature of a portion where the person <b>300</b> is present is higher than the ambient temperature in the room <b>200</b>. In the example illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the image information D<b>1</b> indicates that the temperature of the areas of the person <b>300</b> seated on the chair <b>202</b> and the person <b>300</b> using the whiteboard <b>203</b> are higher than the ambient temperature. Further, the image information D<b>1</b> at different time indicates that the temperature of the areas of the two persons <b>300</b> seated on the chair <b>202</b> are higher than the ambient temperature. Accordingly, the image information D<b>1</b> can indicate the area where the person <b>300</b> is present in the cleaning area <b>100</b> in a time series.</p><p id="p-0030" num="0029">The sensor unit <b>20</b> is provided in or near the cleaning area <b>100</b>. The sensor unit <b>20</b> includes, for example, a sensor such as a temperature sensor, a humidity sensor, an ultrasonic sensor, a radar, a light detection and ranging or laser imaging detection and ranging (LiDAR), or a sonar. The sensor unit <b>20</b> supplies the measured sensor information to the cleaning area estimation device <b>30</b>. The sensor information includes, for example, temperature, humidity, distance to an object, measurement date and time, and the like.</p><p id="p-0031" num="0030">For example, it is known that the amount of sweat of the person <b>300</b> is obtained by the relationship of an environmental temperature, humidity, a sweating rate, and the like. As a method of obtaining the sweating rate, it is possible to refer to the description of Reference Literature 1 &#x201c;Wang, Shugang, et al. &#x2018;Hot environment-estimation of thermal comfort in deep underground mines.&#x2019; (2012)&#x201d;. As can be seen from the above, the amount of sweat discharged from the human body can be estimated if the room temperature and the humidity are provided. The sensor unit <b>20</b> supplies measurement information indicating the measured temperature, humidity, and the like of the cleaning area <b>100</b> to the cleaning area estimation device <b>30</b>, thereby enabling estimation of how much and in which area of the cleaning area <b>100</b> sweat is accumulated.</p><p id="p-0032" num="0031">Returning to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the communication unit <b>40</b> communicates with a cleaning robot <b>500</b>, an electronic device <b>600</b>, and the like outside the cleaning area estimation device <b>30</b>. The communication unit <b>40</b> transmits various types of information from the cleaning area estimation device <b>30</b> to the electronic device that is a transmission destination. The communication unit <b>40</b> supplies the various types of information received therefrom to the cleaning area estimation device <b>30</b>. In the example illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the cleaning robot <b>500</b> is an autonomous mobile cleaning robot. The cleaning robot <b>500</b> is, for example, a robot that includes a cleaning unit, avoids collision with an obstacle, and cleans while moving to a target point. The electronic device <b>600</b> includes, for example, a smartphone, a tablet terminal, a personal computer, a home appliance, and the like. The communication protocol supported by the communication unit <b>40</b> is not particularly limited, and the communication unit <b>40</b> can support a plurality of types of communication protocols. The communication unit <b>40</b> functions as a communication means of the cleaning area estimation device <b>30</b>.</p><heading id="h-0012" level="1">Configuration Example of Cleaning Area Estimation Device According to First Embodiment</heading><p id="p-0033" num="0032">Next, an example of a functional configuration of the cleaning area estimation device <b>30</b> according to the first embodiment will be described. The cleaning area estimation device <b>30</b> includes an extraction unit <b>31</b>, an acquisition unit <b>32</b>, an estimation unit <b>33</b>, a generation unit <b>34</b>, a storage unit <b>35</b>, and a management unit <b>36</b>. Each functional unit of the extraction unit <b>31</b>, the acquisition unit <b>32</b>, the estimation unit <b>33</b>, the generation unit <b>34</b>, and the management unit <b>36</b> is, for example, implemented by executing a program stored in the inside of the cleaning area estimation device <b>30</b> using a random access memory (RAM) or the like as a work area by a central processing unit (CPU), a micro control unit (MCU), or the like.</p><p id="p-0034" num="0033">Furthermore, each functional unit may be implemented by, for example, an integrated circuit such as an application specific integrated circuit (ASIC) or a field-programmable gate array (FPGA).</p><p id="p-0035" num="0034">The extraction unit <b>31</b> extracts an area satisfying an extraction condition from the image information D<b>1</b> obtained by shooting the cleaning area <b>100</b> by the imaging device <b>10</b>. The extraction condition includes, for example, a condition for extracting a feature area such as an area where the person <b>300</b> is present, an area where the person <b>300</b> is not present, and a used area in the cleaning area <b>100</b>. In other words, the feature area is an area inside the cleaning area <b>100</b>. The extraction unit <b>31</b> supplies, to the estimation unit <b>33</b>, area information D<b>11</b> indicating the feature area extracted from the image information D<b>1</b>.</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram for explaining an example of an area to be extracted from the cleaning area <b>100</b> according to the first embodiment. <figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates the relationship between the table <b>201</b> and the persons <b>300</b> and between the whiteboard <b>203</b> and the persons <b>300</b> in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, as a schematic diagram. In the example illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the extraction unit <b>31</b> extracts a feature area <b>110</b> such as an area <b>111</b>, an area <b>112</b>, and an area <b>113</b> from the image information D<b>1</b>. The area <b>111</b> is, for example, an area where the person <b>300</b> is present and that may be dirtied by sweat, sebum, or the like of the person <b>300</b>. The area <b>112</b> is an area where the person <b>300</b> does not enter, and is an area where dust easily accumulates. In other words, the area <b>112</b> is an area that may be dirtied with dust. The area <b>113</b> is an area that may be dirtied with both sweat and dust. The extraction condition includes a condition for extracting at least one of the area <b>111</b>, the area <b>112</b>, the area <b>113</b>, and the like. The extraction condition of the area <b>111</b> includes, for example, a condition for extracting the whole or part of the person <b>300</b> who is a living thing in the cleaning area <b>100</b>. The extraction condition of the area <b>112</b> includes, for example, a condition for extracting an area where the person <b>300</b> is not present or an area around the moving person <b>300</b> in the cleaning area <b>100</b>. The extraction condition of the area <b>113</b> includes, for example, a condition for extracting an area of an object used by the person <b>300</b> in the cleaning area <b>100</b>. Examples of the object include the table <b>201</b>, the chair <b>202</b>, the whiteboard <b>203</b>, a desk, a wall, a floor, and the like.</p><p id="p-0037" num="0036">Returning to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the acquisition unit <b>32</b> acquires feature information indicating a feature of the cleaning area <b>100</b> from the image information D<b>1</b>. For example, when the image information D<b>1</b> includes information indicating temperature, the acquisition unit <b>32</b> acquires additional information D<b>12</b> indicating the temperature of the cleaning area <b>100</b>. That is, the acquisition unit <b>32</b> acquires the additional information D<b>12</b> indicating the temperature of the person <b>300</b> from the image information D<b>1</b>. The acquisition unit <b>32</b> supplies, to the estimation unit <b>33</b>, the additional information D<b>12</b> acquired from the image information D<b>1</b>.</p><p id="p-0038" num="0037">The estimation unit <b>33</b> estimates dirt information D<b>2</b> about the inside of the cleaning area <b>100</b> on the basis of the area information D<b>11</b> and the additional information D<b>12</b> of the image information D<b>1</b>, and measurement information (humidity) of the sensor unit <b>20</b>. The estimation unit <b>33</b> identifies a feature of the feature area <b>110</b> on the basis of the area information D<b>11</b> and the additional information D<b>12</b>, and estimates the dirt information D<b>2</b> on the basis of the feature. In a case where the feature area <b>110</b> of the area information D<b>11</b> is the area <b>111</b>, when the area <b>111</b> is specified as a living thing, the estimation unit <b>33</b> estimates the dirt information D<b>2</b> corresponding to the living thing. For example, when the living thing is the person <b>300</b>, the estimation unit <b>33</b> estimates the amount of sweat of the person <b>300</b> on the basis of temperature, humidity, and the like of the feature area <b>110</b>, and stores the estimation result in the storage unit <b>35</b> as the dirt information D<b>2</b> about the feature area <b>110</b>. The dirt information D<b>2</b> includes, for example, information such as imaging date and time of the image information D<b>1</b>, a type of the feature area <b>110</b>, and an estimation result of dirt. For example, when the living thing is an animal, the estimation unit <b>33</b> estimates dirt due to the animal on the basis of temperature, humidity, a type of animal of the area <b>111</b>, and the like, and stores the estimation result in the storage unit <b>35</b> as the dirt information D<b>2</b> about the feature area <b>110</b>.</p><p id="p-0039" num="0038">When the feature area <b>110</b> of the area information D<b>11</b> is the area <b>112</b>, the estimation unit <b>33</b> estimates the accumulated amount of dust in the feature area <b>110</b>, and stores the estimation result in the storage unit <b>35</b> as the dirt information D<b>2</b> about the feature area <b>110</b>. When the feature area <b>110</b> of the area information D<b>11</b> is the area <b>113</b>, the estimation unit <b>33</b> estimates synthetic dirt in the feature area <b>110</b>, and stores the estimation result in the storage unit <b>35</b> as the dirt information D<b>2</b> about the feature area. The synthetic dirt is, for example, a combination of dirt due to a living thing and dirt due to dust.</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram illustrating an example of the relationship between time and an accumulated amount of dirt of the cleaning area <b>100</b> according to the first embodiment. In <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the vertical axis represents the accumulated amount of dirt, and the horizontal axis represents time t. Time to presented in <figref idref="DRAWINGS">FIG. <b>4</b></figref> indicates, for example, a clean state in which the cleaning of the cleaning area <b>100</b> is finished. The period from the time t<sub>0 </sub>to time t<sub>1 </sub>indicates that the accumulated amount of dust increases due to absence of the person <b>300</b>, and the accumulated amount of dirt due to sweat does not change. The time t<sub>1 </sub>indicates that the person <b>300</b> enters the cleaning area <b>100</b> thereby clearing the accumulated amount of dust. The period from the time t<sub>1 </sub>to time t<sub>2 </sub>indicates that the accumulated amount of dirt due to sweat increases since the person <b>300</b> continues to be present, and the accumulated amount of dust does not change. The time t<sub>2 </sub>indicates that the person <b>300</b> leaves the cleaning area <b>100</b> thereby stopping an increase of the accumulated amount of dirt due to sweat. The period from the time t<sub>2 </sub>to time t<sub>3 </sub>indicates that the accumulated amount of dirt due to sweat does not change due to absence of the person <b>300</b>, and the accumulated amount of dust increases. The time t<sub>3 </sub>indicates that the state in which the person <b>300</b> is not present continues, and the accumulated amount of dust exceeds the accumulated amount of dirt due to sweat. In the example illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the case where the use by the person <b>300</b> clear the accumulated amount of dust has been described, but the accumulated amount of dust may be accumulated without being cleared.</p><p id="p-0041" num="0040">Returning to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the estimation unit <b>33</b> estimates the dirt information D<b>2</b> from the image information D<b>1</b> captured at times different from each other, and stores the dirt information D<b>2</b> in the storage unit <b>35</b> for each time. Accordingly, the dirt information D<b>2</b> about the feature area <b>110</b> in the cleaning area <b>100</b> is stored (accumulated) in the storage unit <b>35</b> in a time-series manner.</p><p id="p-0042" num="0041">The generation unit <b>34</b> generates map information D<b>3</b> indicating a map of the dirt information D<b>2</b> about the cleaning area <b>100</b> on the basis of the estimated time-series dirt information D<b>2</b>. For example, the generation unit <b>34</b> collects, from the storage unit <b>35</b>, the dirt information D<b>2</b> from the date and time of the previous cleaning to the latest date and time, and generates the map information D<b>3</b> about the cleaning area <b>100</b> on the basis of the collected dirt information D<b>2</b>. The generation unit <b>34</b> generates the map information D<b>3</b> indicating a map of the time-series dirt information D<b>2</b> from the date and time of the previous cleaning to the present. The map information D<b>3</b> includes a map indicating the transition (accumulated amount) of dirt for each feature area <b>110</b> in the cleaning area <b>100</b>. After generating the map information D<b>3</b>, the generation unit <b>34</b> stores, in the storage unit <b>35</b>, the map information D<b>3</b> and the cleaning area <b>100</b> in association with each other.</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram illustrating an example of the map information D<b>3</b> according to the first embodiment. As illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the map information D<b>3</b> is a map indicating the relationship between a three-dimensional image of the room <b>200</b> and the feature area <b>110</b> extracted in a monitoring period. The monitoring period includes, for example, a period from the end of the previous cleaning to the present, and a set period. In the map information D<b>3</b>, the feature area <b>110</b> is an area where the feature is detected in the cleaning area <b>100</b>. Each of the feature areas <b>110</b> in the map information D<b>3</b> is associated with information indicating the relationship between the time and the accumulated amount of dirt illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>. As a result, the map information D<b>3</b> enables checking the use status of the room <b>200</b> with the feature area <b>110</b>, and checking the relationship between the time and the accumulated amount of dirt of the feature area <b>110</b>. Note that the map information D<b>3</b> may be a map indicating the feature area <b>110</b> in the planar image of the room <b>200</b>, a map in which the display mode of the feature area <b>110</b> is changed in accordance with the accumulated amount of dirt, or the like.</p><p id="p-0044" num="0043">Returning to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the storage unit <b>35</b> stores various data and programs. The storage unit <b>35</b> can store various types of information such as the dirt information D<b>2</b> and the map information D<b>3</b>. The storage unit <b>35</b> may store, for example, the image information D<b>1</b>, the measurement information of the sensor unit <b>20</b>, and the like. The storage unit <b>35</b> may store various types of information in association with the cleaning area <b>100</b>. The storage unit <b>35</b> is electrically connected to, for example, the estimation unit <b>33</b>, the generation unit <b>34</b>, the management unit <b>36</b>, and the like. The storage unit <b>35</b> is, for example, a semiconductor memory element such as a RAM or a flash memory, a hard disk, or an optical disk. Note that the storage unit <b>35</b> may be provided on a cloud server connected to the cleaning area estimation device <b>30</b> via the communication unit <b>40</b>.</p><p id="p-0045" num="0044">The management unit <b>36</b> manages the dirt information D<b>2</b>, the map information D<b>3</b>, and the like in the storage unit <b>35</b> for each cleaning area <b>100</b>. The management unit <b>36</b> provides, via the communication unit <b>40</b>, the map information D<b>3</b> generated by the generation unit <b>34</b> to the cleaning robot <b>500</b>, the electronic device <b>600</b>, and the like outside the cleaning area estimation device <b>30</b>. Upon receiving the instruction to output the map information D<b>3</b> via the communication unit <b>40</b>, the management unit <b>36</b> provides the map information D<b>3</b> about the corresponding cleaning area <b>100</b>. For example, the management unit <b>36</b> may cause the generation unit <b>34</b> to generate and update the map information D<b>3</b> in response to reception of the output instruction.</p><p id="p-0046" num="0045">The exemplary functional configuration of the cleaning area estimation device <b>30</b> according to the first embodiment has been described above. Note that the above-described configurations described with reference to <figref idref="DRAWINGS">FIG. <b>1</b></figref> is merely an example, and the functional configurations of the cleaning area estimation device <b>30</b> according to the first embodiment is not limited to the example. The functional configuration of the cleaning area estimation device <b>30</b> according to the first embodiment can be flexibly modified according to specifications and operations.</p><heading id="h-0013" level="1">Processing Procedure of Cleaning Area Estimation Device According to First Embodiment</heading><p id="p-0047" num="0046">Next, an example of a processing procedure of the cleaning area estimation device <b>30</b> according to the first embodiment will be described. <figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flowchart illustrating an example of a processing procedure executed by the cleaning area estimation device <b>30</b> according to the first embodiment. The processing procedure illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref> is realized by executing a program by the cleaning area estimation device <b>30</b>. The processing procedure illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref> is repeatedly executed by the cleaning area estimation device <b>30</b>.</p><p id="p-0048" num="0047">As illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the cleaning area estimation device <b>30</b> acquires the image information D<b>1</b> from the imaging device <b>10</b> (Step S<b>101</b>). The cleaning area estimation device <b>30</b> causes the extraction unit <b>31</b> to extract, from the image information D<b>1</b>, the area information D<b>11</b> indicating the feature area <b>110</b> satisfying the extraction condition (Step S<b>102</b>). For example, the cleaning area estimation device <b>30</b> extracts the area information D<b>11</b> such that the feature area <b>110</b> corresponds to the pixel of the image information D<b>1</b> on a one-to-one basis. In the present embodiment, the area information D<b>11</b> is a mask image in which the information about the feature area <b>110</b> corresponds to the pixel of the image information D<b>1</b> on a one-to-one basis. The cleaning area estimation device <b>30</b> causes the acquisition unit <b>32</b> to acquire the additional information D<b>12</b> from the image information D<b>1</b> (Step S<b>103</b>).</p><p id="p-0049" num="0048">The cleaning area estimation device <b>30</b> acquires the measurement information from the sensor unit <b>20</b> (Step S<b>104</b>). The cleaning area estimation device <b>30</b> causes the estimation unit <b>33</b> to estimate the dirt information D<b>2</b> on the basis of the area information D<b>11</b>, the additional information D<b>12</b>, and the measurement information, and stores the dirt information D<b>2</b> in the storage unit <b>35</b> (Step S<b>105</b>). For example, the cleaning area estimation device <b>30</b> generates the dirt information D<b>2</b> in which the feature value of the feature area <b>110</b> is made for each pixel of the image information D<b>1</b>. When there are a plurality of pieces of the area information D<b>11</b>, the cleaning area estimation device <b>30</b> estimates the dirt information D<b>2</b> for each of the plurality of pieces of the area information D<b>11</b>. When the dirt information D<b>2</b> is already stored in the storage unit <b>35</b>, the cleaning area estimation device <b>30</b> associates the estimated dirt information D<b>2</b> with the stored dirt information D<b>2</b> in the order of time series and stores the estimated dirt information D<b>2</b> in the storage unit <b>35</b>.</p><p id="p-0050" num="0049">The cleaning area estimation device <b>30</b> causes the generation unit <b>34</b> to generate the map information D<b>3</b> on the basis of the estimated time-series dirt information D<b>2</b> (Step S<b>106</b>). When the map information D<b>3</b> about the corresponding cleaning area <b>100</b> is stored in the storage unit <b>35</b>, the cleaning area estimation device <b>30</b> updates the map information D<b>3</b> in the storage unit <b>35</b> on the basis of the generated map information D<b>3</b>.</p><p id="p-0051" num="0050">The cleaning area estimation device <b>30</b> determines whether or not it is output timing (Step S<b>107</b>). For example, when it is the preset date and time or the like at which output is performed in accordance with an instruction received from the outside, the cleaning area estimation device <b>30</b> determines that it is the output timing. When determining that it is not the output timing (No in Step S<b>107</b>), the cleaning area estimation device <b>30</b> ends the processing procedure illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>.</p><p id="p-0052" num="0051">On the other hand, when determining that it is the output timing (Yes in Step S<b>107</b>), the cleaning area estimation device <b>30</b> advances the processing to Step S<b>108</b>. The cleaning area estimation device <b>30</b> causes the management unit <b>36</b> to provide the generated map information D<b>3</b> (Step S<b>108</b>). For example, the cleaning area estimation device <b>30</b> provides the map information D<b>3</b> to the cleaning robot <b>500</b>, the electronic device <b>600</b>, and the like via the communication unit <b>40</b>. For example, the cleaning robot <b>500</b> cleans the cleaning area <b>100</b> that requires cleaning, on the basis of the map information D<b>3</b> provided from the cleaning area estimation device <b>30</b>. For example, the electronic device <b>600</b> displays the map information D<b>3</b> provided from the cleaning area estimation device <b>30</b> on a display unit to support the determination of the user as to whether or not the place requires cleaning. After providing the map information D<b>3</b>, the cleaning area estimation device <b>30</b> ends the processing procedure illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>.</p><p id="p-0053" num="0052">As described above, the cleaning area estimation device <b>30</b> according to the first embodiment estimates the dirt information D<b>2</b> about the inside of the cleaning area <b>100</b> on the basis of the image information D<b>1</b> obtained by imaging the cleaning area <b>100</b> by the imaging device <b>10</b>. The cleaning area estimation device <b>30</b> generates the map information D<b>3</b> indicating a map of the dirt information D<b>2</b> about the cleaning area <b>100</b> on the basis of the time-series dirt information D<b>2</b>. Accordingly, the cleaning area estimation device <b>30</b> generates the map information D<b>3</b> using the image information D<b>1</b> obtained by imaging the cleaning area <b>100</b>, thereby making it possible to support the determination of necessity of cleaning in the cleaning area <b>100</b> by using the map information D<b>3</b>. Furthermore, the cleaning area estimation device <b>30</b> generates the map information D<b>3</b> on the basis of the image information D<b>1</b> obtained by imaging a plurality of the cleaning area <b>100</b>, thereby making it possible to support the determination of necessity of cleaning in the plurality of the cleaning area <b>100</b>. The cleaning area estimation device <b>30</b> can support determination of necessity of cleaning in the cleaning area <b>100</b> by using the image information D<b>1</b> of the installed imaging device <b>10</b>. As a result, the cleaning area estimation device <b>30</b> can suppress the time and cost required for the preliminary confirmation of cleaning and enables spending the remaining time to improve the quality of cleaning.</p><p id="p-0054" num="0053">The above-described first embodiment is described as an example, and various modifications and applications can be made.</p><heading id="h-0014" level="1">Modification of First Embodiment</heading><p id="p-0055" num="0054"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a diagram illustrating an example of a configuration of the cleaning area estimation system <b>1</b> according to a modification of the first embodiment. As illustrated in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the cleaning area estimation system <b>1</b> includes the imaging device <b>10</b>, the sensor unit <b>20</b>, the cleaning area estimation device <b>30</b>, and the communication unit <b>40</b>. The imaging device <b>10</b> is a visible light camera.</p><p id="p-0056" num="0055">The cleaning area estimation device <b>30</b> according to the modification of the first embodiment includes the extraction unit <b>31</b>, the estimation unit <b>33</b>, the generation unit <b>34</b>, the storage unit <b>35</b>, and the management unit <b>36</b>. That is, the cleaning area estimation device <b>30</b> does not include the acquisition unit <b>32</b> of the first embodiment.</p><p id="p-0057" num="0056">The extraction unit <b>31</b> extracts, as the feature area <b>110</b>, an area recognized as a human body by analyzing the visible-light image information D<b>1</b> captured by the visible light camera. The estimation unit <b>33</b> estimates the dirt information D<b>2</b> about the inside of the cleaning area <b>100</b> on the basis of the extracted area information D<b>11</b>, temperature, and humidity. For example, the estimation unit <b>33</b> estimates the amount of sweat according to the temperature and humidity measured by the sensor unit <b>20</b>, and stores the estimation result in the storage unit <b>35</b> as the dirt information D<b>2</b> about the feature area <b>110</b>.</p><p id="p-0058" num="0057">As described above, the cleaning area estimation device <b>30</b> according to the modification of the first embodiment extracts the area of a human body as the feature area <b>110</b> on the basis of the image information D<b>1</b> captured by the visible light camera. The cleaning area estimation device <b>30</b> estimates, as the dirt information D<b>2</b>, the amount of sweat in the feature area <b>110</b> in accordance with the temperature and humidity measured by the sensor unit <b>20</b>. The cleaning area estimation device <b>30</b> generates the map information D<b>3</b> indicating a map of the dirt information D<b>2</b> about the cleaning area <b>100</b> on the basis of the time-series dirt information D<b>2</b>. Accordingly, also in the case of using a visible light camera, the cleaning area estimation device <b>30</b> can generate the map information D<b>3</b> based on the image information D<b>1</b> obtained by imaging the cleaning area <b>100</b>; thus, the cleaning area estimation device <b>30</b> can support the determination of necessity of cleaning in the cleaning area <b>100</b> by using the map information D<b>3</b>. As a result, the cleaning area estimation device <b>30</b> can suppress the time and cost required for the preliminary confirmation of cleaning and enables spending the remaining time to improve the quality of cleaning.</p><p id="p-0059" num="0058">Note that the modification of the first embodiment may be applied to the cleaning area estimation device <b>30</b> of other embodiments or modifications.</p><heading id="h-0015" level="1">Second Embodiment</heading><heading id="h-0016" level="1">Configuration Example of Cleaning Area Estimation System According to Second Embodiment</heading><p id="p-0060" num="0059">Next, a second embodiment will be described. <figref idref="DRAWINGS">FIG. <b>8</b></figref> is a diagram for explaining the relationship between an object and the degree of ease with which dust accumulates. In the example illustrated in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the closer to the vertical direction a normal line <b>210</b>A of the surface of an object <b>210</b> is, the more easily dust accumulates. Further, the closer to the right angle the angle at which a normal line <b>210</b>B of the surface of a backrest <b>230</b> or the like intersects with the vertical direction is, the less easily dust accumulates. That is, in the object <b>210</b>, the surface <b>220</b> is a portion where dust easily accumulates. It is known that the normal line <b>210</b>A and the normal line <b>210</b>B of the object <b>210</b> are obtained by acquiring polarization information from reflected light from the object <b>210</b>. As a method of obtaining the normal line of the object <b>210</b>, for example, it is possible to refer to the description of Reference Literature 2 &#x201c;Daisuke Miyazaki and Katsushi Ikeuchi. &#x2018;Basic Theory of Polarization and Its Applications&#x2019; Information Processing Society of Japan Transactions on Computer Vision and Image Media (CVIM) 1.1 (2008)&#x201d;. In the second embodiment, an example of the cleaning area estimation system <b>1</b> using a polarization camera will be described.</p><p id="p-0061" num="0060"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a diagram illustrating an example of a configuration of the cleaning area estimation system according to the second embodiment. As illustrated in <figref idref="DRAWINGS">FIG. <b>9</b></figref>, a cleaning area estimation system <b>1</b>A includes an imaging device <b>10</b>A, the cleaning area estimation device <b>30</b>, and the communication unit <b>40</b>.</p><p id="p-0062" num="0061">The imaging device <b>10</b>A is a polarization camera. The imaging device <b>10</b>A supplies, to the cleaning area estimation device <b>30</b>, image information D<b>1</b>A including a polarized image obtained by shooting the cleaning area <b>100</b>. The image information D<b>1</b>A includes color information and polarization information. The imaging device <b>10</b>A supplies, to the cleaning area estimation device <b>30</b>, installation information D<b>1</b>S including an installation direction, an installation position, and the like.</p><p id="p-0063" num="0062">The cleaning area estimation device <b>30</b> according to the second embodiment includes the extraction unit <b>31</b>, the acquisition unit <b>32</b>, the estimation unit <b>33</b>, the generation unit <b>34</b>, the storage unit <b>35</b>, and the management unit <b>36</b>.</p><p id="p-0064" num="0063">The extraction unit <b>31</b> extracts an area satisfying the extraction condition from the image information D<b>1</b>A of the imaging device <b>10</b>A. The extraction unit <b>31</b> obtains a normal line from the polarization information of the image information D<b>1</b>A, and extracts an area of the surface of the object <b>210</b> for which the normal line has been obtained. That is, the extraction condition is an area of the surface of the object <b>210</b> for which the normal line has been obtained. The extraction unit <b>31</b> supplies, to the estimation unit <b>33</b>, the area information D<b>11</b> indicating the area extracted from the image information D<b>1</b>A and the normal line of the area.</p><p id="p-0065" num="0064">The acquisition unit <b>32</b> estimates the vertical direction in the image from the installation information D<b>1</b>S of the imaging device <b>10</b>A. That is, the acquisition unit <b>32</b> acquires the additional information D<b>12</b> indicating the vertical direction from the installation information D<b>1</b>S. The acquisition unit <b>32</b> supplies the acquired additional information D<b>12</b> to the estimation unit <b>33</b>.</p><p id="p-0066" num="0065">The estimation unit <b>33</b> estimates the dirt information D<b>2</b> about the inside of the cleaning area <b>100</b> on the basis of the area information D<b>11</b> and the additional information D<b>12</b> of the image information D<b>1</b>A. The estimation unit <b>33</b> estimates, for each feature area <b>110</b>, the dirt information D<b>2</b> indicating the degree of ease with which dust is deposited on the basis of the relationship between the normal line of the area indicated by the area information D<b>11</b> and the vertical direction of the additional information D<b>12</b>. For example, when the normal line of the area of the area information D<b>11</b> is close to the vertical direction, the estimation unit <b>33</b> estimates the area of the surface of the object <b>210</b> as the dirt information D<b>2</b> indicating that dust easily accumulates, and stores the dirt information D<b>2</b> in the storage unit <b>35</b>. For example, when the normal line of the area of the area information D<b>11</b> is intersects with the vertical direction, the estimation unit <b>33</b> estimates the area of the surface of the object <b>210</b> as the dirt information D<b>2</b> indicating that dust hardly accumulates, and stores the dirt information D<b>2</b> in the storage unit <b>35</b>.</p><p id="p-0067" num="0066">The generation unit <b>34</b> generates the map information D<b>3</b> with which deposition of dust on the object <b>210</b> in the cleaning area <b>100</b> can be identified for each feature area <b>110</b> on the basis of the estimated time-series dirt information D<b>2</b>. For example, the generation unit <b>34</b> calculates, for each feature area <b>110</b>, a rate of dust deposition on the basis of the presence or absence of use, an unused time, and the like, and generates the map information D<b>3</b> indicating a deposition state of dust based on the calculation result as a map for each feature area <b>110</b>. The map information D<b>3</b> includes, for example, a map indicating that dust is deposited in the area of the surface <b>220</b> of the object <b>210</b> illustrated in <figref idref="DRAWINGS">FIG. <b>8</b></figref> and dust is not deposited in the area other than the surface <b>220</b> of the object <b>210</b>. The map information D<b>3</b> may include, for example, information indicating a deposition amount (accumulated amount) of dust, an elapsed time from cleaning, an elapsed time from use of the object <b>210</b>, and the like in an area of the surface of the object <b>210</b>. After generating the map information D<b>3</b>, the generation unit <b>34</b> stores, in the storage unit <b>35</b>, the map information D<b>3</b> and the cleaning area <b>100</b> in association with each other.</p><p id="p-0068" num="0067">The exemplary functional configuration of the cleaning area estimation device <b>30</b> according to the second embodiment has been described above. Note that the above-described configurations described with reference to <figref idref="DRAWINGS">FIG. <b>9</b></figref> is merely an example, and the functional configurations of the cleaning area estimation device <b>30</b> according to the second embodiment is not limited to the example. The functional configuration of the cleaning area estimation device <b>30</b> according to the second embodiment can be flexibly modified according to specifications and operations.</p><heading id="h-0017" level="1">Processing Procedure of Cleaning Area Estimation Device According to Second Embodiment</heading><p id="p-0069" num="0068">Next, an example of a processing procedure of the cleaning area estimation device <b>30</b> according to the second embodiment will be described. <figref idref="DRAWINGS">FIG. <b>10</b></figref> is a flowchart illustrating an example of a processing procedure executed by the cleaning area estimation device <b>30</b> according to the second embodiment. The processing procedure illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref> is realized by executing a program by the cleaning area estimation device <b>30</b>. The processing procedure illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref> is repeatedly executed by the cleaning area estimation device <b>30</b>.</p><p id="p-0070" num="0069">As illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, the cleaning area estimation device <b>30</b> acquires the image information D<b>1</b>A from the imaging device <b>10</b>A (Step S<b>110</b>). The cleaning area estimation device <b>30</b> causes the extraction unit <b>31</b> to extract, from the image information D<b>1</b>A, the area information D<b>11</b> indicating the area satisfying the extraction condition (Step S<b>111</b>). For example, the cleaning area estimation device <b>30</b> extracts the area information D<b>11</b> such that the area corresponds to the pixel of the image information D<b>1</b>A on a one-to-one basis. In the present embodiment, the area information D<b>11</b> includes information indicating the normal line and a mask image in which the information about the area corresponds to the pixel of the image information D<b>1</b>A on a one-to-one basis. The cleaning area estimation device <b>30</b> causes the acquisition unit <b>32</b> to acquire the additional information D<b>12</b> indicating the vertical direction from the imaging device <b>10</b>A (Step S<b>112</b>).</p><p id="p-0071" num="0070">The cleaning area estimation device <b>30</b> causes the estimation unit <b>33</b> to estimate the dirt information D<b>2</b> on the basis of the normal line of the area and the vertical direction, and stores the dirt information D<b>2</b> in the storage unit <b>35</b> (Step S<b>113</b>). For example, the cleaning area estimation device <b>30</b> estimates the dirt information D<b>2</b> indicating the deposition state of dust in the area on the basis of the relationship between the normal line and the vertical direction. When the dirt information D<b>2</b> is already stored in the storage unit <b>35</b>, the cleaning area estimation device <b>30</b> associates the estimated dirt information D<b>2</b> with the stored dirt information D<b>2</b> in the order of time series and stores the estimated dirt information D<b>2</b> in the storage unit <b>35</b>.</p><p id="p-0072" num="0071">The cleaning area estimation device <b>30</b> causes the generation unit <b>34</b> to generate the map information D<b>3</b> on the basis of the estimated time-series dirt information D<b>2</b> (Step S<b>114</b>). When the map information D<b>3</b> about the corresponding area is stored in the storage unit <b>35</b>, the cleaning area estimation device <b>30</b> updates the map information D<b>3</b> in the storage unit <b>35</b> on the basis of the generated map information D<b>3</b>. After the processing of Step S<b>114</b> ends, the cleaning area estimation device <b>30</b> advances the processing to Step S<b>107</b> that has been already described.</p><p id="p-0073" num="0072">The cleaning area estimation device <b>30</b> determines whether or not it is output timing (Step S<b>107</b>). When determining that it is not the output timing (No in Step S<b>107</b>), the cleaning area estimation device <b>30</b> ends the processing procedure illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref>. On the other hand, when determining that it is the output timing (Yes in Step S<b>107</b>), the cleaning area estimation device <b>30</b> advances the processing to Step S<b>108</b>. The cleaning area estimation device <b>30</b> causes the management unit <b>36</b> to provide the generated map information D<b>3</b> (Step S<b>108</b>). After providing the map information D<b>3</b>, the cleaning area estimation device <b>30</b> ends the processing procedure illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref>.</p><p id="p-0074" num="0073">As described above, the cleaning area estimation device <b>30</b> according to the second embodiment extracts the feature area <b>110</b> of the surface of the object <b>210</b> from the image information D<b>1</b>A of the imaging device <b>10</b>A, and estimates the dirt information D<b>2</b> on the basis of the relationship between the normal line of the feature area <b>110</b> and the vertical direction. The cleaning area estimation device <b>30</b> generates the map information D<b>3</b> indicating a map of the dirt information D<b>2</b> about the cleaning area <b>100</b> on the basis of the time-series dirt information D<b>2</b>. Accordingly, the cleaning area estimation device <b>30</b> generates the map information D<b>3</b> using the image information D<b>1</b>A obtained by imaging the cleaning area <b>100</b>, thereby making it possible to support the determination of necessity of cleaning for the object <b>210</b> in the cleaning area <b>100</b> by using the map information D<b>3</b>. For example, the cleaning area estimation device <b>30</b> can support recognition of a portion of the object <b>210</b> where dust is easily deposited, by using the map information D<b>3</b>. As a result, the cleaning area estimation device <b>30</b> enables cleaning of a portion of the object <b>210</b> where dust is easily deposited, and thus, the cleaning area estimation device <b>30</b> can contribute to improvement of the quality of cleaning.</p><p id="p-0075" num="0074">The above-described second embodiment is described as an example, and various modifications and applications can be made. The cleaning area estimation device <b>30</b> according to the second embodiment may be applied to other embodiments and the like.</p><heading id="h-0018" level="1">Third Embodiment</heading><heading id="h-0019" level="1">Configuration Example of Cleaning Area Estimation System According to Third Embodiment</heading><p id="p-0076" num="0075">Next, a third embodiment will be described. <figref idref="DRAWINGS">FIG. <b>11</b></figref> is a diagram for explaining an example of dirt in the cleaning area <b>100</b>. In the example illustrated in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, in the cleaning area <b>100</b>, for example, dirt <b>121</b> of coffee exists on the table, dirt <b>122</b> of tea exists on the floor, and dirt <b>123</b> of ketchup exists on another table. As described above, in the cleaning area <b>100</b>, various dirt such as beverages, seasonings, and foods may exist, for example. It is known that a component of an observed substance can be analyzed from an image captured by a spectral camera. For example, it is possible to refer to the description of Reference Literature 3 &#x201c;Miyuki KONDO. &#x2018;Food Analysis by Near-Infrared Spectroscopy&#x2019;, Journal of Nagoya Bunri University 7 (2007)&#x201d;. In the third embodiment, an example of the cleaning area estimation system <b>1</b> using a spectral camera will be described.</p><p id="p-0077" num="0076"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a diagram illustrating an example of a configuration of a cleaning area estimation system according to the third embodiment. As illustrated in <figref idref="DRAWINGS">FIG. <b>12</b></figref>, a cleaning area estimation system <b>1</b>B includes an imaging device <b>10</b>B, the cleaning area estimation device <b>30</b>, and the communication unit <b>40</b>.</p><p id="p-0078" num="0077">The imaging device <b>10</b>B is a spectral camera. The imaging device <b>10</b>B spectrally disperses and detects light in the vertical direction as one horizontal line, using optical components such as a diffraction grating and a mirror. The imaging device <b>10</b>B captures a two-dimensional spectral image for each wavelength of light by performing the above-described spectral dispersion and detection in the horizontal direction. The imaging device <b>10</b>B supplies, to the cleaning area estimation device <b>30</b>, image information D<b>1</b>B indicating a spectral image obtained by shooting the cleaning area <b>100</b>. The image information D<b>1</b>B includes an image in a normal visible light band and a spectral image obtained by finely dividing the wavelength of light related to the cleaning area <b>100</b> into a plurality of wavelengths and detecting the plurality of wavelengths.</p><p id="p-0079" num="0078">The cleaning area estimation device <b>30</b> according to the third embodiment includes the extraction unit <b>31</b>, the estimation unit <b>33</b>, the generation unit <b>34</b>, the storage unit <b>35</b>, the management unit <b>36</b>, and an analysis unit <b>37</b>.</p><p id="p-0080" num="0079">The analysis unit <b>37</b> analyzes a component for each pixel from the spectral image of the imaging device <b>10</b>B and generates a component map. The analysis unit <b>37</b> estimates the type of dirt from the component map and generates dirt type information D<b>1</b>C. For example, the analysis unit <b>37</b> estimates the type of dirt from the component map on the basis of model data for recognizing machine-learned dirt (food). The model data includes, for example, data indicating the relationship between a component and food. In the example illustrated in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, the analysis unit <b>37</b> estimates that the dirt <b>121</b> is coffee, the dirt <b>122</b> is tea, and the dirt <b>122</b> is ketchup. The analysis unit <b>37</b> generates the dirt type information D<b>1</b>C indicating the estimated result and associated with the component map. The dirt type information D<b>1</b>C includes the image information D<b>1</b>B of the imaging device <b>10</b>B, but may not include the image information D<b>1</b>B. The analysis unit <b>37</b> supplies the generated dirt type information D<b>1</b>C to the extraction unit <b>31</b>.</p><p id="p-0081" num="0080">The extraction unit <b>31</b> extracts an area satisfying the extraction condition from the image information D<b>1</b>B of the imaging device <b>10</b>B on the basis of the dirt type information D<b>1</b>C of the analysis unit <b>37</b>. The extraction unit <b>31</b> extracts the area including dirt from the dirt type information D<b>1</b>C for each of the same or similar types. That is, the extraction condition is a condition for classifying the type of dirt. For example, in the cleaning area <b>100</b>, the extraction unit <b>31</b> extracts areas of each of the dirt <b>121</b>, the dirt <b>122</b>, and the dirt <b>123</b> illustrated in <figref idref="DRAWINGS">FIG. <b>11</b></figref>. The extraction unit <b>31</b> supplies, to the estimation unit <b>33</b>, the area information D<b>11</b> indicating the extracted area.</p><p id="p-0082" num="0081">The estimation unit <b>33</b> estimates the dirt information D<b>2</b> indicating the type of area and the area of dirt in the cleaning area <b>100</b> on the basis of the area information D<b>11</b> and the dirt type information D<b>1</b>C of the image information D<b>1</b>B. For example, the estimation unit <b>33</b> estimates the dirt information D<b>2</b> indicating the type and state of dirt for each area indicated by the area information D<b>11</b>, and stores the dirt information D<b>2</b> in the storage unit <b>35</b>.</p><p id="p-0083" num="0082">Based on the estimated time-series dirt information D<b>2</b>, the generation unit <b>34</b> generates the map information D<b>3</b> indicating a map by areas, with which the type and state of dirt in the cleaning area <b>100</b> can be identified. For example, the generation unit <b>34</b> estimates the degree of dryness of dirt by comparing the date and time when the dirt is attached with the current date and time, and estimates the amount and type of dirt by areas. For example, on the basis of the dirt information D<b>2</b> in the monitoring period, the generation unit <b>34</b> measures how much dirt of the same type is left. Then the generation unit <b>34</b> estimates the dry state of the dirt, and generates the map information D<b>3</b> indicating the type and state of the dirt on the map. The map information D<b>3</b> includes, for example, information for displaying information indicating an area of dirt, a type of dirt, and a dry state of dirt on a map of the cleaning area <b>100</b>. After generating the map information D<b>3</b>, the generation unit <b>34</b> stores, in the storage unit <b>35</b>, the map information D<b>3</b> and the cleaning area <b>100</b> in association with each other.</p><p id="p-0084" num="0083">The exemplary functional configuration of the cleaning area estimation device <b>30</b> according to the third embodiment has been described above. Note that the above-described configurations described with reference to <figref idref="DRAWINGS">FIG. <b>12</b></figref> is merely an example, and the functional configurations of the cleaning area estimation device <b>30</b> according to the third embodiment is not limited to the example. The functional configuration of the cleaning area estimation device <b>30</b> according to the third embodiment can be flexibly modified according to specifications and operations.</p><heading id="h-0020" level="1">Processing Procedure of Cleaning Area Estimation Device According to Third Embodiment</heading><p id="p-0085" num="0084">Next, an example of a processing procedure of the cleaning area estimation device <b>30</b> according to the third embodiment will be described. <figref idref="DRAWINGS">FIG. <b>13</b></figref> is a flowchart illustrating an example of a processing procedure executed by the cleaning area estimation device <b>30</b> according to the third embodiment. <figref idref="DRAWINGS">FIG. <b>14</b></figref> is a diagram illustrating an example of map information generated by the cleaning area estimation device <b>30</b> according to the third embodiment. The processing procedure illustrated in <figref idref="DRAWINGS">FIG. <b>13</b></figref> is realized by executing a program by the cleaning area estimation device <b>30</b>. The processing procedure illustrated in <figref idref="DRAWINGS">FIG. <b>13</b></figref> is repeatedly executed by the cleaning area estimation device <b>30</b>.</p><p id="p-0086" num="0085">As illustrated in <figref idref="DRAWINGS">FIG. <b>13</b></figref>, the cleaning area estimation device <b>30</b> causes the analysis unit <b>37</b> to analyze the image information D<b>1</b>B captured by the imaging device <b>10</b>B (Step S<b>120</b>). After estimating the type of dirt from the component map and generating the dirt type information D<b>1</b>C, the cleaning area estimation device <b>30</b> advances the processing to Step S<b>121</b>.</p><p id="p-0087" num="0086">The cleaning area estimation device <b>30</b> causes the extraction unit <b>31</b> to extract, from the analyzed image information D<b>1</b>B, the area information D<b>11</b> indicating the area satisfying the extraction condition (Step S<b>121</b>). For example, the cleaning area estimation device <b>30</b> extracts the area information D<b>11</b> such that the area corresponds to the pixel of the image information D<b>1</b>B on a one-to-one basis. In the present embodiment, the area information D<b>11</b> includes information indicating a mask image in which the information about the area corresponds to the pixel of the image information D<b>1</b>B on a one-to-one basis.</p><p id="p-0088" num="0087">The cleaning area estimation device <b>30</b> causes the estimation unit <b>33</b> to estimate the dirt information D<b>2</b> on the basis of the area information D<b>11</b> and the dirt type information D<b>1</b>C, and stores the dirt information D<b>2</b> in the storage unit <b>35</b> (Step S<b>122</b>). For example, the cleaning area estimation device <b>30</b> estimates the dirt information D<b>2</b> indicating the type of area and the state of dirt in the cleaning area <b>100</b>. When the dirt information D<b>2</b> is already stored in the storage unit <b>35</b>, the cleaning area estimation device <b>30</b> associates the estimated dirt information D<b>2</b> with the stored dirt information D<b>2</b> in the order of time series and stores the estimated dirt information D<b>2</b> in the storage unit <b>35</b>.</p><p id="p-0089" num="0088">The cleaning area estimation device <b>30</b> causes the generation unit <b>34</b> to generate the map information D<b>3</b> on the basis of the time-series dirt information D<b>2</b> estimated in Step S<b>122</b> (Step S<b>123</b>). For example, the cleaning area estimation device <b>30</b> estimates the dry state of the dirt for each area indicated by the time-series dirt information D<b>2</b> on the basis of the type of dirt, the time during which the dirt is left, and the model data. The model data includes, for example, data for estimating a state such as dryness, semi-wetting, or wetting on the basis of the type and the elapsed time. The model data includes, for example, a calculation formula for calculating the dry state on the basis of the type of dirt and the elapsed time, data such as a table for conversion, and a program. The cleaning area estimation device <b>30</b> generates the map information D<b>3</b> in which the state information indicating the type of dirt and the dry state is associated with the area.</p><p id="p-0090" num="0089">In the example illustrated in <figref idref="DRAWINGS">FIG. <b>14</b></figref>, the cleaning area estimation device <b>30</b> generates the map information D<b>3</b> including state information indicating that the type of a feature area <b>110</b>A is coffee and the state of dirt is dry. The cleaning area estimation device <b>30</b> generates the map information D<b>3</b> including state information indicating that the type of a feature area <b>110</b>B is tea and the state of dirt is wet. The cleaning area estimation device <b>30</b> generates the map information D<b>3</b> including state information indicating that the type of a feature area <b>110</b>C is ketchup and the state of dirt is semi-wet. As a result, the cleaning area estimation device <b>30</b> enables checking the type and state of dirt with the map information D<b>3</b> for each feature area <b>110</b>.</p><p id="p-0091" num="0090">Returning to <figref idref="DRAWINGS">FIG. <b>13</b></figref>, when the map information D<b>3</b> about the corresponding area is stored in the storage unit <b>35</b>, the cleaning area estimation device <b>30</b> updates the map information D<b>3</b> in the storage unit <b>35</b> on the basis of the generated map information D<b>3</b>. After the processing of Step S<b>123</b> ends, the cleaning area estimation device <b>30</b> advances the processing to Step S<b>107</b> that has been already described.</p><p id="p-0092" num="0091">The cleaning area estimation device <b>30</b> determines whether or not it is output timing (Step S<b>107</b>). When determining that it is not the output timing (No in Step S<b>107</b>), the cleaning area estimation device <b>30</b> ends the processing procedure illustrated in <figref idref="DRAWINGS">FIG. <b>13</b></figref>. On the other hand, when determining that it is the output timing (Yes in Step S<b>107</b>), the cleaning area estimation device <b>30</b> advances the processing to Step S<b>108</b>. The cleaning area estimation device <b>30</b> causes the management unit <b>36</b> to provide the generated map information D<b>3</b> (Step S<b>108</b>). After providing the map information D<b>3</b>, the cleaning area estimation device <b>30</b> ends the processing procedure illustrated in <figref idref="DRAWINGS">FIG. <b>13</b></figref>.</p><p id="p-0093" num="0092">As described above, the cleaning area estimation device <b>30</b> according to the third embodiment analyzes the image information D<b>1</b>B of the imaging device <b>10</b>B, and estimates the dirt information D<b>2</b> about the feature area extracted from the image information D<b>1</b>B. Based on the time-series dirt information D<b>2</b>, the cleaning area estimation device <b>30</b> generates the map information D<b>3</b> with which the type and state of dirt of the feature area <b>110</b> in the cleaning area <b>100</b> can be identified. Accordingly, the cleaning area estimation device <b>30</b> generates the map information D<b>3</b> using the image information D<b>1</b>B obtained by imaging the cleaning area <b>100</b>, thereby making it possible to support the determination of the type of cleaning based on the type and state of dirt of the cleaning area <b>100</b> by using the map information D<b>3</b>. As a result, the cleaning area estimation device <b>30</b> enables cleaning suitable for the feature area <b>110</b>, and thus, the cleaning area estimation device <b>30</b> can contribute to improvement of the work efficiency of cleaning.</p><p id="p-0094" num="0093">The above-described third embodiment is described as an example, and various modifications and applications can be made. The cleaning area estimation device <b>30</b> according to the third embodiment may be applied to other embodiments and the like.</p><p id="p-0095" num="0094">[Hardware Configuration]</p><p id="p-0096" num="0095">The cleaning area estimation device <b>30</b> according to the present embodiment described above may be implemented by a computer <b>1000</b> having a configuration as illustrated in <figref idref="DRAWINGS">FIG. <b>15</b></figref>, for example. Hereinafter, the cleaning area estimation device <b>30</b> according to the embodiments will be described as an example. <figref idref="DRAWINGS">FIG. <b>15</b></figref> is a hardware configuration diagram illustrating an example of the computer <b>1000</b> that implements functions of the cleaning area estimation device <b>30</b>. The computer <b>1000</b> includes a CPU <b>1100</b>, a RAM <b>1200</b>, a read only memory (ROM) <b>1300</b>, a hard disk drive (HDD) <b>1400</b>, a communication interface <b>1500</b>, and an input/output interface <b>1600</b>. Each unit of the computer <b>1000</b> is coupled through a bus <b>1050</b>.</p><p id="p-0097" num="0096">The CPU <b>1100</b> operates on the basis of a program stored in the ROM <b>1300</b> or the HDD <b>1400</b>, and controls each unit. For example, the CPU <b>1100</b> develops a program stored in the ROM <b>1300</b> or the HDD <b>1400</b> to the RAM <b>1200</b>, and executes processing corresponding to various programs.</p><p id="p-0098" num="0097">The ROM <b>1300</b> stores a boot program such as a basic input output system (BIOS) executed by the CPU <b>1100</b> when the computer <b>1000</b> is activated, and stores a program depending on hardware of the computer <b>1000</b>, and the like.</p><p id="p-0099" num="0098">The HDD <b>1400</b> is a computer-readable recording medium that non-transiently records a program executed by the CPU <b>1100</b>, data used by the program, and the like. Specifically, the HDD <b>1400</b> is a recording medium that records an information processing program according to the present disclosure, which is an example of program data <b>1450</b>.</p><p id="p-0100" num="0099">The communication interface <b>1500</b> is an interface for the computer <b>1000</b> to connect to an external network <b>1550</b> (for example, the Internet). For example, the CPU <b>1100</b> receives data from another device or transmits data generated by the CPU <b>1100</b> to another device via the communication interface <b>1500</b>.</p><p id="p-0101" num="0100">The input/output interface <b>1600</b> is an interface for coupling an input/output device <b>1650</b> and the computer <b>1000</b>. For example, the CPU <b>1100</b> receives data from an input device such as a keyboard or a mouse via the input/output interface <b>1600</b>. In addition, the CPU <b>1100</b> transmits data to an output device such as a display, a speaker, or a printer via the input/output interface <b>1600</b>. Furthermore, the input/output interface <b>1600</b> may function as a media interface that reads a program or the like recorded in a predetermined recording medium (medium). Examples of the medium include an optical recording medium such as a digital versatile disc (DVD); a magneto-optical recording medium such as a magneto-optical disk (MO); a tape medium; a magnetic recording medium; and a semiconductor memory.</p><p id="p-0102" num="0101">For example, when the computer <b>1000</b> functions as the cleaning area estimation device <b>30</b> according to the embodiment, the CPU <b>1100</b> of the computer <b>1000</b> implements the functions of the extraction unit <b>31</b>, the acquisition unit <b>32</b>, the estimation unit <b>33</b>, the generation unit <b>34</b>, the management unit <b>36</b>, the analysis unit <b>37</b>, and the like of the cleaning area estimation device <b>30</b>, by executing the program loaded on the RAM <b>1200</b>. In addition, the HDD <b>1400</b> stores the program according to the present disclosure and data in the storage unit <b>35</b>. Note that the CPU <b>1100</b> reads the program data <b>1450</b> from the HDD <b>1400</b> and executes the program data. As another example, these programs may be acquired from another device via the external network <b>1550</b>.</p><p id="p-0103" num="0102">Although the preferred embodiments of the present disclosure have been described in detail with reference to the accompanying drawings, the technical scope of the present disclosure is not limited to the foregoing examples. It is obvious that a person who has common knowledge in the technical field of the present disclosure may, within the scope of the technical idea recited in the claims, conceive various alterations or modifications, and it should be understood that they also naturally belong to the technical scope of the present disclosure.</p><p id="p-0104" num="0103">Furthermore, the effects described herein are merely illustrative or exemplary, and are not limitative. That is, the technology according to the present disclosure may, with or in lieu of the foregoing effects, exhibit other effects obvious to those skilled in the art from the description provided herein.</p><p id="p-0105" num="0104">In addition, it is also possible to create a program for causing hardware such as a CPU, a ROM, and a RAM built in a computer to exhibit functions equivalent to the configurations of the cleaning area estimation device <b>30</b>, and a computer-readable recording medium in which this program is recorded may also be provided.</p><p id="p-0106" num="0105">Furthermore, each step pertaining to the processing of the cleaning area estimation device <b>30</b> provided herein is not necessarily processed in a time-series manner in the order illustrated in the flowchart. For example, each step pertaining to the processing of the cleaning area estimation device <b>30</b> may be processed in an order different from the order illustrated in the flowchart, or may be processed in parallel.</p><p id="p-0107" num="0106">In the foregoing embodiments, the case where the cleaning area estimation device <b>30</b> is included in the cleaning area estimation systems <b>1</b>, <b>1</b>A, and <b>1</b>B has been described, but the present disclosure is not limited thereto. For example, the cleaning area estimation device <b>30</b> may be implemented by the cleaning robot <b>500</b>, the electronic device <b>600</b>, a monitoring device of a building, or the like. For example, when implemented by the cleaning robot <b>500</b>, the cleaning area estimation device <b>30</b> can be implemented by a control device of the cleaning robot <b>500</b>.</p><p id="p-0108" num="0107">(Effects)</p><p id="p-0109" num="0108">The cleaning area estimation device <b>30</b> includes the estimation unit <b>33</b> that estimates the dirt information D<b>2</b> about the inside of the cleaning area <b>100</b> on the basis of the image information D<b>1</b> obtained by imaging the cleaning area <b>100</b> by the imaging device <b>10</b>, and the generation unit <b>34</b> that generates the map information D<b>3</b> indicating a map of the dirt information D<b>2</b> about the cleaning area <b>100</b> on the basis of the estimated time-series dirt information D<b>2</b>.</p><p id="p-0110" num="0109">Accordingly, the cleaning area estimation device <b>30</b> generates the map information D<b>3</b> about the dirt information D<b>2</b> using the image information D<b>1</b> obtained by imaging the cleaning area <b>100</b>, thereby making it possible to support the determination of necessity of cleaning in the cleaning area <b>100</b> by using the map information D<b>3</b>. As a result, the cleaning area estimation device <b>30</b> can suppress the time and cost required for the preliminary confirmation of cleaning and enables spending the remaining time to improve the quality of cleaning.</p><p id="p-0111" num="0110">The cleaning area estimation device <b>30</b> further includes the extraction unit <b>31</b> that extracts the feature area <b>110</b> satisfying the extraction condition from the image information D<b>1</b>, and the estimation unit <b>33</b> estimates the dirt information D<b>2</b> on the basis of the feature of feature area <b>110</b>.</p><p id="p-0112" num="0111">Accordingly, the cleaning area estimation device <b>30</b> extracts the feature area <b>110</b> satisfying the extraction condition from the image information D<b>1</b>, thereby making it possible to generate the map information D<b>3</b> suitable for the determination of necessity of cleaning in the cleaning area <b>100</b>. As a result, the cleaning area estimation device <b>30</b> can improve accuracy in the determination of necessity of cleaning in the cleaning area <b>100</b>, and thus, the cleaning area estimation device <b>30</b> can contribute to improvement of the work efficiency of cleaning.</p><p id="p-0113" num="0112">In the cleaning area estimation device <b>30</b>, the extraction condition is a condition for extracting an area of a living thing in the cleaning area <b>100</b>.</p><p id="p-0114" num="0113">Accordingly, the cleaning area estimation device <b>30</b> extracts the area of a living thing in the cleaning area <b>100</b> from the image information D<b>1</b>, thereby making it possible to generate the map information D<b>3</b> suitable for the determination of necessity of cleaning in the feature area <b>110</b> that has been dirtied by the living thing. As a result, the cleaning area estimation device <b>30</b> can improve accuracy in the determination of necessity of cleaning with respect to dirt due to a living thing in the cleaning area <b>100</b>, and thus, the cleaning area estimation device <b>30</b> can contribute to improvement of the quality of cleaning.</p><p id="p-0115" num="0114">In the cleaning area estimation device <b>30</b>, the estimation unit <b>33</b> estimates the dirt information D<b>2</b> about dirt due to a living thing on the basis of the feature of the feature area <b>110</b> and at least one of the temperature and the humidity of the cleaning area <b>100</b> detected by the sensor unit <b>20</b>. The dirt information D<b>2</b> includes information indicating at least one of the type of dirt, the accumulated amount of dirt, and the state of dirt.</p><p id="p-0116" num="0115">As a result, the cleaning area estimation device <b>30</b> can generate the map information D<b>3</b> indicating the dirt information D<b>2</b> about dirt due to the living thing, which is estimated on the basis of the environment in the cleaning area <b>100</b>. As a result, the cleaning area estimation device <b>30</b> can improve accuracy in the determination of necessity of cleaning with respect to dirt due to a living thing on the basis of at least one of the type of dirt, the accumulated amount of dirt, and the state of dirt; thus, the cleaning area estimation device <b>30</b> can contribute to further improvement of the quality of cleaning.</p><p id="p-0117" num="0116">The cleaning area estimation device <b>30</b> further includes the extraction unit <b>31</b> that extracts the feature area <b>110</b> satisfying the extraction condition of the object <b>210</b> in the cleaning area <b>100</b> on the basis of the polarized image included in the image information D<b>1</b>A. The estimation unit <b>33</b> estimates the dirt information D<b>2</b> indicating the degree of ease with which dust is deposited on the object <b>210</b> on the basis of the relationship between the normal line of the feature area <b>110</b> and the vertical direction. The generation unit <b>34</b> generates the map information D<b>3</b> with which the estimated deposition state of dust on the object <b>210</b> in the feature area <b>110</b> can be identified.</p><p id="p-0118" num="0117">Accordingly, the cleaning area estimation device <b>30</b> generates the map information D<b>3</b> using the image information D<b>1</b>A obtained by imaging the cleaning area <b>100</b>, thereby making it possible to support the determination of necessity of cleaning for the object <b>210</b> in the cleaning area <b>100</b> by using the map information D<b>3</b>. As a result, the cleaning area estimation device <b>30</b> enables cleaning of a portion of the object <b>210</b> where dust is easily deposited, and thus, the cleaning area estimation device <b>30</b> can contribute to improvement of the quality of cleaning.</p><p id="p-0119" num="0118">The cleaning area estimation device <b>30</b> further includes the acquisition unit <b>32</b> that acquires the vertical direction in the image indicated by the image information D<b>1</b>A from the installation information D<b>1</b>S of the imaging device <b>10</b>A.</p><p id="p-0120" num="0119">Accordingly, the cleaning area estimation device <b>30</b> acquires the vertical direction in the image information D<b>1</b>A from the installation information D<b>1</b>S, thereby making it possible to improve the accuracy of the dirt information D<b>2</b> indicating the degree of ease with which dust is deposited on the object <b>210</b>. As a result, the cleaning area estimation device <b>30</b> can improve the accuracy in estimation of a portion of the object <b>210</b> where dust is easily deposited, and thus, the cleaning area estimation device <b>30</b> can contribute to further improvement of the quality of cleaning.</p><p id="p-0121" num="0120">The cleaning area estimation device <b>30</b> further includes the analysis unit <b>37</b> that analyzes a dirt component in the cleaning area <b>100</b> on the basis of a spectral image included in the image information D<b>1</b>B, and the extraction unit <b>31</b> that extracts the feature area <b>110</b> satisfying the extraction condition from the image information D<b>1</b>B on the basis of the analyzed dirt component. The estimation unit <b>33</b> estimates the dirt information D<b>2</b> indicating a type of the feature area <b>110</b> on the basis of the analyzed dirt component. The generation unit <b>34</b> generates the map information D<b>3</b> with which at least one of a type and a state of dirt in the cleaning area <b>100</b> can be identified.</p><p id="p-0122" num="0121">Accordingly, the cleaning area estimation device <b>30</b> generates the map information D<b>3</b> using the image information D<b>1</b>B obtained by imaging the cleaning area <b>100</b>, thereby making it possible to support the determination of the type of cleaning based on the type and state of dirt of the cleaning area <b>100</b> by using the map information D<b>3</b>. As a result, the cleaning area estimation device <b>30</b> enables cleaning suitable for the feature area <b>110</b>, and thus, the cleaning area estimation device <b>30</b> can contribute to improvement of the work efficiency of cleaning.</p><p id="p-0123" num="0122">In the cleaning area estimation device <b>30</b>, the generation unit <b>34</b> generates the map information D<b>3</b> indicating the dry state of dirt on the basis of the time-series dirt information D<b>2</b> after the dirt is generated.</p><p id="p-0124" num="0123">Accordingly, the cleaning area estimation device <b>30</b> generates the map information D<b>3</b> indicating the dry state of dirt, thereby making it possible to support the determination of the type of cleaning based on the dry state of dirt of the cleaning area <b>100</b> by using the map information D<b>3</b>. As a result, the cleaning area estimation device <b>30</b> enables cleaning suitable for the dry state of dirt of the feature area <b>110</b>, and thus, the cleaning area estimation device <b>30</b> can contribute to improvement of the quality of cleaning.</p><p id="p-0125" num="0124">The cleaning area estimation device <b>30</b> further includes the management unit <b>36</b> that manages provision of the map information D<b>3</b>.</p><p id="p-0126" num="0125">Accordingly, the cleaning area estimation device <b>30</b> can manage the timing of generation, output, and the like of the map information D<b>3</b> by managing the provision of the map information D<b>3</b>. As a result, the cleaning area estimation device <b>30</b> can provide the map information D<b>3</b> suitable for determination of cleaning, and thus, the cleaning area estimation device <b>30</b> can contribute to further improvement of the quality of cleaning.</p><p id="p-0127" num="0126">A method for estimating a cleaning area includes estimating, by a computer, the dirt information D<b>2</b> about the inside of the cleaning area <b>100</b> on the basis of the image information D<b>1</b> obtained by imaging the cleaning area <b>100</b> by the imaging device <b>10</b>, and generating, by the computer, the map information D<b>3</b> indicating a map of the dirt information D<b>2</b> about the cleaning area <b>100</b> on the basis of the estimated time-series dirt information D<b>2</b>.</p><p id="p-0128" num="0127">Accordingly, the method for estimating a cleaning area causes a computer to generate the map information D<b>3</b> about the dirt information D<b>2</b> using the image information D<b>1</b> obtained by imaging the cleaning area <b>100</b>, thereby making it possible to support the determination of necessity of cleaning in the cleaning area <b>100</b> by using the map information D<b>3</b>. As a result, the method for estimating a cleaning area can suppress the time and cost required for the preliminary confirmation of cleaning and enables spending the remaining time to improve the quality of cleaning.</p><p id="p-0129" num="0128">Note that the following configurations also belong to the technical scope of the present disclosure.</p><p id="p-0130" num="0000">(1)</p><p id="p-0131" num="0129">A cleaning area estimation device including:</p><p id="p-0132" num="0130">an estimation unit configured to estimate dirt information about an inside of a cleaning area on a basis of image information obtained by imaging the cleaning area by an imaging device; and</p><p id="p-0133" num="0131">a generation unit configured to generate map information indicating a map of the dirt information about the cleaning area on a basis of the dirt information that is estimated and in a time series.</p><p id="p-0134" num="0000">(2)</p><p id="p-0135" num="0132">The cleaning area estimation device according to (1), further including</p><p id="p-0136" num="0133">an extraction unit configured to extract a feature area satisfying an extraction condition from the image information,</p><p id="p-0137" num="0134">wherein the estimation unit estimates the dirt information on a basis of a feature of the feature area.</p><p id="p-0138" num="0000">(3)</p><p id="p-0139" num="0135">The cleaning area estimation device according to (2), wherein the extraction condition is a condition for extracting an area of a living thing in the cleaning area.</p><p id="p-0140" num="0000">(4)</p><p id="p-0141" num="0136">The cleaning area estimation device according to (2) or</p><p id="p-0142" num="0000">(3), wherein</p><p id="p-0143" num="0137">the estimation unit estimates the dirt information about dirt due to the living thing on a basis of the feature of the feature area and at least one of temperature and humidity of the cleaning area detected by a sensor unit, and</p><p id="p-0144" num="0138">the dirt information includes information indicating at least one of a type of dirt, an accumulated amount of dirt, and a state of dirt.</p><p id="p-0145" num="0000">(5)</p><p id="p-0146" num="0139">The cleaning area estimation device according to (1), further including</p><p id="p-0147" num="0140">an extraction unit configured to extract a feature area satisfying an extraction condition of an object in the cleaning area on a basis of a polarized image included in the image information,</p><p id="p-0148" num="0141">wherein the estimation unit estimates the dirt information indicating a degree of ease with which dust is deposited on the object on a basis of a relationship between a normal line of the feature area and a vertical direction, and</p><p id="p-0149" num="0142">the generation unit generates the map information with which an estimated deposition state of dust on the feature area of the object can be identified.</p><p id="p-0150" num="0000">(6)</p><p id="p-0151" num="0143">The cleaning area estimation device according to (5), further including</p><p id="p-0152" num="0144">an acquisition unit configured to acquire the vertical direction in an image indicated by the image information from installation information of the imaging device.</p><p id="p-0153" num="0000">(7)</p><p id="p-0154" num="0145">The cleaning area estimation device according to (1), further including:</p><p id="p-0155" num="0146">an analysis unit configured to analyze a dirt component in the cleaning area on a basis of a spectral image included in the image information; and</p><p id="p-0156" num="0147">an extraction unit configured to extract a feature area satisfying an extraction condition from the image information on a basis of the analyzed dirt component,</p><p id="p-0157" num="0148">wherein the estimation unit estimates the dirt information indicating a type of the feature area on a basis of the analyzed dirt component, and</p><p id="p-0158" num="0149">the generation unit generates the map information with which at least one of a type and a state of dirt in the cleaning area can be identified.</p><p id="p-0159" num="0000">(8)</p><p id="p-0160" num="0150">The cleaning area estimation device according to (7), wherein</p><p id="p-0161" num="0151">the generation unit generates the map information indicating a dry state of the dirt on a basis of the dirt information in a time series after the dirt is generated.</p><p id="p-0162" num="0000">(9)</p><p id="p-0163" num="0152">The cleaning area estimation device according to any one of (1) to (8), further including</p><p id="p-0164" num="0153">a management unit configured to manage provision of the map information.</p><p id="p-0165" num="0000">(10)</p><p id="p-0166" num="0154">A method for estimating a cleaning area including:</p><p id="p-0167" num="0155">estimating, by a computer, dirt information about an inside of a cleaning area on a basis of image information obtained by imaging a cleaning area by an imaging device; and</p><p id="p-0168" num="0156">generating, by the computer, map information indicating a map of the dirt information about the cleaning area on a basis of the dirt information that is estimated and in a time series.</p><p id="p-0169" num="0000">(11)</p><p id="p-0170" num="0157">A program causing a computer to perform: estimating dirt information about an inside of a cleaning area on a basis of image information obtained by imaging the cleaning area by an imaging device; and generating map information indicating a map of the dirt information about the cleaning area on a basis of the dirt information that is estimated and in a time series.</p><p id="p-0171" num="0000">(12)</p><p id="p-0172" num="0158">A cleaning area estimation system including: an imaging device configured to image a cleaning area; and a cleaning area estimation device, in which the cleaning area estimation device includes an estimation unit configured to estimate dirt information about an inside of the cleaning area on a basis of image information obtained by imaging the cleaning area by the imaging device, and a generation unit configured to generate map information indicating a map of the dirt information about the cleaning area on a basis of the dirt information that is estimated and in a time series.</p><heading id="h-0021" level="1">REFERENCE SIGNS LIST</heading><p id="p-0173" num="0000"><ul id="ul0002" list-style="none">    <li id="ul0002-0001" num="0159"><b>1</b>, <b>1</b>A, <b>1</b>B CLEANING AREA ESTIMATION SYSTEM</li>    <li id="ul0002-0002" num="0160"><b>10</b>, <b>10</b>A, <b>10</b>B IMAGING DEVICE</li>    <li id="ul0002-0003" num="0161"><b>20</b> SENSOR UNIT</li>    <li id="ul0002-0004" num="0162"><b>30</b> CLEANING AREA ESTIMATION DEVICE</li>    <li id="ul0002-0005" num="0163"><b>31</b> EXTRACTION UNIT</li>    <li id="ul0002-0006" num="0164"><b>32</b> ACQUISITION UNIT</li>    <li id="ul0002-0007" num="0165"><b>33</b> ESTIMATION UNIT</li>    <li id="ul0002-0008" num="0166"><b>34</b> GENERATION UNIT</li>    <li id="ul0002-0009" num="0167"><b>35</b> STORAGE UNIT</li>    <li id="ul0002-0010" num="0168"><b>36</b> MANAGEMENT UNIT</li>    <li id="ul0002-0011" num="0169"><b>37</b> ANALYSIS UNIT</li>    <li id="ul0002-0012" num="0170"><b>40</b> COMMUNICATION UNIT</li>    <li id="ul0002-0013" num="0171"><b>100</b> CLEANING AREA</li>    <li id="ul0002-0014" num="0172"><b>110</b> FEATURE AREA</li>    <li id="ul0002-0015" num="0173">D<b>1</b>, D<b>1</b>A, D<b>1</b>B IMAGE INFORMATION</li>    <li id="ul0002-0016" num="0174">D<b>2</b> DIRT INFORMATION</li>    <li id="ul0002-0017" num="0175">D<b>3</b> MAP INFORMATION</li>    <li id="ul0002-0018" num="0176">D<b>11</b> AREA INFORMATION</li>    <li id="ul0002-0019" num="0177">D<b>12</b> ADDITIONAL INFORMATION</li></ul></p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A cleaning area estimation device including:<claim-text>an estimation unit configured to estimate dirt information about an inside of a cleaning area on a basis of image information obtained by imaging the cleaning area by an imaging device; and</claim-text><claim-text>a generation unit configured to generate map information indicating a map of the dirt information about the cleaning area on a basis of the dirt information that is estimated and in a time series.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The cleaning area estimation device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further including<claim-text>an extraction unit configured to extract a feature area satisfying an extraction condition from the image information,</claim-text><claim-text>wherein the estimation unit estimates the dirt information on a basis of a feature of the feature area.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The cleaning area estimation device according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the extraction condition is a condition for extracting an area of a living thing in the cleaning area.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The cleaning area estimation device according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein<claim-text>the estimation unit estimates the dirt information about dirt due to the living thing on a basis of the feature of the feature area and at least one of temperature and humidity of the cleaning area detected by a sensor unit, and</claim-text><claim-text>the dirt information includes information indicating at least one of a type of dirt, an accumulated amount of dirt, and a state of dirt.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The cleaning area estimation device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further including<claim-text>an extraction unit configured to extract a feature area satisfying an extraction condition of an object in the cleaning area on a basis of a polarized image included in the image information,</claim-text><claim-text>wherein the estimation unit estimates the dirt information indicating a degree of ease with which dust is deposited on the object on a basis of a relationship between a normal line of the feature area and a vertical direction, and</claim-text><claim-text>the generation unit generates the map information with which an estimated deposition state of dust on the feature area of the object can be identified.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The cleaning area estimation device according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, further including<claim-text>an acquisition unit configured to acquire the vertical direction in an image indicated by the image information from installation information of the imaging device.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The cleaning area estimation device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further including:<claim-text>an analysis unit configured to analyze a dirt component in the cleaning area on a basis of a spectral image included in the image information; and</claim-text><claim-text>an extraction unit configured to extract a feature area satisfying an extraction condition from the image information on a basis of the analyzed dirt component,</claim-text><claim-text>wherein the estimation unit estimates the dirt information indicating a type of the feature area on a basis of the analyzed dirt component, and</claim-text><claim-text>the generation unit generates the map information with which at least one of a type and a state of dirt in the cleaning area can be identified.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The cleaning area estimation device according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein<claim-text>the generation unit generates the map information indicating a dry state of the dirt on a basis of the dirt information in a time series after the dirt is generated.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The cleaning area estimation device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further including<claim-text>a management unit configured to manage provision of the map information.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. A method for estimating a cleaning area including:<claim-text>estimating, by a computer, dirt information about an inside of a cleaning area on a basis of image information obtained by imaging a cleaning area by an imaging device; and</claim-text><claim-text>generating, by the computer, map information indicating a map of the dirt information about the cleaning area on a basis of the dirt information that is estimated and in a time series.</claim-text></claim-text></claim></claims></us-patent-application>