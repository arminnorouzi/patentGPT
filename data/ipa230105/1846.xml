<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230001847A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230001847</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17721146</doc-number><date>20220414</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>B</section><class>60</class><subclass>Q</subclass><main-group>1</main-group><subgroup>11</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>B</section><class>60</class><subclass>Q</subclass><main-group>1</main-group><subgroup>115</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>S</subclass><main-group>17</main-group><subgroup>89</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>60</class><subclass>Q</subclass><main-group>1</main-group><subgroup>11</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>60</class><subclass>Q</subclass><main-group>1</main-group><subgroup>115</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>S</subclass><main-group>17</main-group><subgroup>89</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>60</class><subclass>Q</subclass><main-group>2300</main-group><subgroup>322</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">Vehicles With Automatic Headlight Alignment</invention-title><us-related-documents><us-provisional-application><document-id><country>US</country><doc-number>63298365</doc-number><date>20220111</date></document-id></us-provisional-application><us-provisional-application><document-id><country>US</country><doc-number>63216780</doc-number><date>20210630</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Apple Inc.</orgname><address><city>Cupertino</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Child</last-name><first-name>Christopher P.</first-name><address><city>San Jose</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Mazuir</last-name><first-name>Clarisse</first-name><address><city>San Jose</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Stiehl</last-name><first-name>Kurt R.</first-name><address><city>Los Gatos</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Zurcher</last-name><first-name>Mark A.</first-name><address><city>Encinitas</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>Mannberg</last-name><first-name>Mikael B.</first-name><address><city>San Jose</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="05" designation="us-only"><addressbook><last-name>Garrone</last-name><first-name>Ryan J.</first-name><address><city>San Francisco</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="06" designation="us-only"><addressbook><last-name>Tang</last-name><first-name>Xiaofeng</first-name><address><city>San Jose</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A vehicle may have lights such as headlights. The lights may be moved using a positioner. Control circuitry in the vehicle may use sensor circuitry to monitor the environment surrounding the vehicle. The sensor circuitry may include one or more sensors such as a lidar sensor, radar sensor, image sensor, and/or other sensors to measure the shape of a surface in front of the vehicle and the location of the surface relative to the vehicle. These sensors and/or other sensors in the sensor circuitry also measure headlight illumination on the surface. Based on the known shape of the surface in front of the vehicle and the distance of the surface from the vehicle, the control circuitry can predict where a headlight should be aimed on the surface. By comparing predictions of headlight illumination on the surface to measurements of headlight illumination on the surface, the vehicle can determine how to move the headlight with the positioner to align the headlight.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="87.88mm" wi="95.84mm" file="US20230001847A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="83.40mm" wi="154.60mm" file="US20230001847A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="98.55mm" wi="97.87mm" file="US20230001847A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="97.54mm" wi="107.87mm" file="US20230001847A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="79.84mm" wi="132.25mm" file="US20230001847A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="126.24mm" wi="89.24mm" file="US20230001847A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="102.79mm" wi="155.19mm" file="US20230001847A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="155.45mm" wi="113.96mm" file="US20230001847A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><p id="p-0002" num="0001">This application claims the benefit of provisional patent application No. 63/298,365, filed Jan. 11, 2022, and provisional patent application No. 63/216,780, filed Jun. 30, 2021, which are hereby incorporated by reference herein in their entireties.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">FIELD</heading><p id="p-0003" num="0002">This relates generally to systems such as vehicles, and, more particularly, vehicles that have lights.</p><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0004" num="0003">Automobiles and other vehicles have lights such as headlights. To accommodate different driving conditions, headlights are sometimes provided with adjustable settings such as low beam and high beam settings. Some headlights can be steered during operation to accommodate road curvature.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0005" num="0004">A vehicle may have lights such as headlights. Sensor circuitry in the vehicle may be used to measure the shape and location of surfaces in front of the vehicle. The sensor circuitry can also be used to measure how the headlights illuminate the surfaces as light from the headlights is projected onto the surfaces. For example, the sensor circuitry may measure where the headlights are aimed on the surfaces and can measure the pattern of light from the headlights on the surface as the headlight illumination is projected onto the surface. Light intensity measurements from an image sensor or other sensor may be used to obtain a peak headlight intensity position, may be used to locate edges in an illumination pattern, and may be used to determine other illumination characteristics.</p><p id="p-0006" num="0005">Information on the three-dimensional shape of a surface in front of the vehicle can be used to predict where the headlights should be aimed and therefore the pattern of illumination from the headlights on the surface when the headlights are aligned relative to the vehicle. By comparing a prediction of headlight illumination intensity on the surface to measured headlight illumination intensity on the surface, the vehicle can determine how to move the headlight with the positioner to align the headlight. If desired, information on the three-dimensional shape of a surface in front of the vehicle may be obtained from a database. For example, a three-dimensional map of the environment may be stored in a navigation database. Information from satellite navigation system sensors and/or other navigation sensors may be used to determine vehicle location. The known vehicle location may then be used to retrieve corresponding three-dimensional surface shape information from the database.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a top view of an illustrative vehicle in accordance with an embodiment.</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a side view of an illustrative adjustable headlight in accordance with an embodiment.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a perspective view of an illustrative scene with a target being illuminated by headlights in accordance with an embodiment.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a graph showing how headlight performance can be monitored by measuring headlight illumination intensity as a function of position across an illuminated surface in accordance with an embodiment.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a cross-sectional side view of an illustrative headlight with multiple independently adjustable elements in accordance with an embodiment.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a graph showing how measurements may be made on illumination from the headlight of <figref idref="DRAWINGS">FIG. <b>5</b></figref> in accordance with an embodiment.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a cross-sectional side view of an illustrative vehicle with headlights and sensor circuitry in accordance with an embodiment.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a flow chart of illustrative operations involved in using a vehicle with headlights in accordance with an embodiment.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0015" num="0014">A system such as a vehicle or other system may have components that emit light such as headlights and other lights. Headlights may be used to illuminate roadways and other objects in the vicinity of a vehicle. The illumination provided by the headlights allows vehicle occupants to view the objects at night or in other dim ambient lighting conditions and facilitates the operation of sensors. For example, headlight illumination at visible and/or infrared wavelengths may be used to provide illumination for image sensors that are used by an autonomous driving system or driver's assistance system.</p><p id="p-0016" num="0015">The illumination that is emitted by the headlights in a vehicle may be adjustable. For example, the headlights may have adjustable components that allow the headlights to be operated in high-beam and low-beam modes and to be steered to the left and right (e.g., to accommodate curves in a road). If desired, headlight adjustments may be made to calibrate the headlights. In this way, unintended misalignment of the headlights over time may be prevented.</p><p id="p-0017" num="0016">To help ensure that headlights are properly aligned and therefore emit light beams in desired directions, a vehicle sensor such as a three-dimensional sensor may gather information on an object within range of the headlights. For example, a lidar sensor may be used to map the three-dimensional shape of a roadway and an object on the roadway in front of a vehicle. An image sensor in the vehicle can measure the pattern of illumination from the headlights that falls on the roadway and object. Measurements of headlight illumination reveal the direction in which a headlight is pointing. By comparing the expected illumination (e.g., the expected headlight illumination direction) with the measured illumination (e.g., the measured headlight illumination direction), variations in headlight performance can be detected and corrective action taken. If, as an example, it is determined that the headlights are pointed 5&#xb0; too high, a positioner coupled to the headlights may be directed to automatically tilt the headlights downward by 5&#xb0; to compensate for this measured misalignment. In this way, the headlights may be continually adjusted during use of the vehicle to ensure that the headlights operate satisfactorily. The headlights may also be adjusted based on measured and predicated changes in vehicle orientation relative to a roadway and other measured and predicated conditions.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a side view of a portion of an illustrative vehicle. In the example of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, vehicle <b>10</b> is the type of vehicle that may carry passengers (e.g., an automobile, truck, or other automotive vehicle). Configurations in which vehicle <b>10</b> is a robot (e.g., an autonomous robot) or other vehicle that does not carry human passengers may also be used. Vehicles such as automobiles may sometimes be described herein as an example. As shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, vehicle <b>10</b> may be operated on roads such as roadway <b>14</b>. Objects such as object <b>26</b> may be located on or near other structures in the vicinity of vehicle <b>10</b> such as roadway <b>14</b>.</p><p id="p-0019" num="0018">Vehicle <b>10</b> may be manually driven (e.g., by a human driver), may be operated via remote control, and/or may be autonomously operated (e.g., by an autonomous driving system or other autonomous propulsion system). Using vehicle sensors such as lidar, radar, visible and/or infrared cameras (e.g., two-dimensional and/or three-dimensional cameras), proximity (distance) sensors, and/or other sensors, an autonomous driving system and/or driver-assistance system in vehicle <b>10</b> may perform automatic braking, steering, and/or other operations to help avoid pedestrians, inanimate objects, and/or other external structures such as illustrative obstacle <b>26</b> on roadway <b>14</b>.</p><p id="p-0020" num="0019">Vehicle <b>10</b> may include a body such as vehicle body <b>12</b>. Body <b>12</b> may include vehicle structures such as body panels formed from metal and/or other materials, may include doors, a hood, a trunk, fenders, a chassis to which wheels are mounted, a roof, etc. Windows may be formed in doors <b>18</b> (e.g., on the sides of vehicle body <b>12</b>, on the roof of vehicle <b>10</b>, and/or in other portions of vehicle <b>10</b>). Windows, doors <b>18</b>, and other portions of body <b>12</b> may separate the interior of vehicle <b>10</b> from the exterior environment that is surrounding vehicle <b>10</b>. Doors <b>18</b> may be opened and closed to allow people to enter and exit vehicle <b>10</b>. Seats and other structures may be formed in the interior of vehicle body <b>12</b>.</p><p id="p-0021" num="0020">Vehicle <b>10</b> may have automotive lighting such as one or more headlights (sometimes referred to as headlamps), driving lights, fog lights, daytime running lights, turn signals, brake lights, and/or other lights. As shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, for example, vehicle <b>10</b> may have lights such as lights <b>16</b>. In general, lights <b>16</b> may be mounted on front F of vehicle <b>10</b>, on rear R of vehicle <b>10</b>, on left and/or right sides W of vehicle <b>10</b>, and/or other portions of body <b>12</b>. In an illustrative configuration, which may sometimes be described herein as an example, lights <b>16</b> are headlights and are mounted to front F of body <b>12</b>. There may be, as an example, left and right headlights <b>16</b> located respectively on the left and right of vehicle <b>10</b> to provide illumination <b>20</b> in the forward direction (e.g., in the +X direction in which vehicle <b>10</b> moves when driven forward in the example of <figref idref="DRAWINGS">FIG. <b>1</b></figref>). By shining headlights <b>16</b> on external surfaces <b>28</b> such as roadway <b>14</b> and object <b>26</b> in front of vehicle <b>10</b>, occupants of vehicle <b>10</b> may view surfaces <b>28</b> even in dim ambient lighting conditions (e.g., at night). The operation of sensors in vehicle <b>10</b> such as image sensors and other sensors that use light may also be supported by providing surfaces <b>28</b> with illumination.</p><p id="p-0022" num="0021">Vehicle <b>10</b> may have components <b>24</b>. Components <b>24</b> may include propulsion and steering systems (e.g., manually adjustable driving systems and/or autonomous driving systems having wheels coupled to body <b>12</b>, steering controls, one or more motors for driving the wheels, etc.), and other vehicle systems. Components <b>24</b> may include control circuitry and input-output devices. Control circuitry in components <b>24</b> may be configured to run an autonomous driving application, a navigation application (e.g., an application for displaying maps on a display), and software for controlling vehicle climate control devices, lighting, media playback, window movement, door operations, sensor operations, and/or other vehicle operations. For example, the control system may form part of an autonomous driving system that drives vehicle <b>10</b> on roadways such as roadway <b>14</b> autonomously using data such as sensor data. The control circuitry may include processing circuitry and storage and may be configured to perform operations in vehicle <b>10</b> using hardware (e.g., dedicated hardware or circuitry), firmware and/or software. Software code for performing operations in vehicle <b>10</b> and other data is stored on non-transitory computer readable storage media (e.g., tangible computer readable storage media) in the control circuitry. The software code may sometimes be referred to as software, data, program instructions, computer instructions, instructions, or code. The non-transitory computer readable storage media may include non-volatile memory such as non-volatile random-access memory, one or more hard drives (e.g., magnetic drives or solid state drives), one or more removable flash drives or other removable media, or other storage. Software stored on the non-transitory computer readable storage media may be executed on the processing circuitry of components <b>24</b>. The processing circuitry may include application-specific integrated circuits with processing circuitry, one or more microprocessors, a central processing unit (CPU) or other processing circuitry.</p><p id="p-0023" num="0022">The input-output devices of components <b>24</b> may include displays, sensors, buttons, light-emitting diodes and other light-emitting devices, haptic devices, speakers, and/or other devices for gathering environmental measurements, information on vehicle operations, and/or user input and for providing output. The sensors in components <b>24</b> may include ambient light sensors, touch sensors, force sensors, proximity sensors, optical sensors such as cameras operating at visible, infrared, and/or ultraviolet wavelengths (e.g., fisheye cameras, two-dimensional cameras, three-dimensional cameras, and/or other cameras), capacitive sensors, resistive sensors, ultrasonic sensors (e.g., ultrasonic distance sensors), microphones, radio-frequency sensors such as radar sensors, lidar (light detection and ranging) sensors, door open/close sensors, seat pressure sensors and other vehicle occupant sensors, window sensors, position sensors for monitoring location, orientation, and movement, speedometers, satellite positioning system sensors, and/or other sensors. Output devices in components <b>24</b> may be used to provide vehicle occupants and others with haptic output, audio output, visual output (e.g., displayed content, light, etc.), and/or other suitable output.</p><p id="p-0024" num="0023">Three-dimensional sensors in components <b>24</b> may be formed from pairs of two-dimensional image sensors operating together as a stereoscopic depth sensor (e.g., a binocular camera pair forming at three-dimensional camera). Three-dimensional sensors may also be formed using image sensor systems that emit structured light (e.g., arrays of dots, lines, grids, and/or other structured light patterns at infrared and/or visible wavelengths) and that capture images (e.g., two-dimensional images) for analysis. The captured images reveal how the structured light patterns have been distorted by the three-dimensional surfaces illuminated by the structured light patterns. By analyzing the distortion of the structured light, the three-dimensional shape of the surfaces can be reconstructed. If desired, three-dimensional sensors for vehicle <b>10</b> may include one or more time-of-flight sensors. For example, time-of-flight measurements may be made using light (e.g., lidar sensor measurements) and radio-frequency signals (e.g., three-dimensional radar).</p><p id="p-0025" num="0024">During operation, the control circuitry of components <b>24</b> may gather information from sensors and/or other input-output devices such as lidar data, camera data (e.g., two-dimensional images), radar data, and/or other sensor data. For example, three-dimensional image data may be captured using three-dimensional image sensor(s). Two-dimensional images (e.g., images of headlight illumination on one or more external surfaces such as external surface(s) <b>28</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>) may also be gathered.</p><p id="p-0026" num="0025">A vehicle occupant or other user of vehicle <b>10</b> may provide user input to the control circuitry of vehicle <b>10</b>. Cameras, touch sensors, physical controls, and other input devices may be used to gather the user input. Using wireless communications with vehicle <b>10</b>, remote data sources may provide the control circuitry of components <b>24</b> with database information. Displays, speakers, and other output devices may be used to provide users with content such as interactive on-screen menu options and audio. A user may interact with this interactive content by supplying touch input to a touch sensor in a display and/or by providing user input with other input devices. If desired, the control circuitry of vehicle <b>10</b> may use sensor data, user input, information from remote databases, and/or other information in providing a driver with driver assistance information (e.g., information on nearby obstacles on a roadway and/or other environment surrounding vehicle <b>10</b>) and/or in autonomously driving vehicle <b>10</b>.</p><p id="p-0027" num="0026">Components <b>24</b> may include forward-facing sensor circuitry, as shown by forward-facing sensor(s) <b>24</b>F of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. The forward-facing sensor circuitry may include one or more sensors facing a surface in front of vehicle <b>10</b> (e.g., one or more sensors that are directed in the +X direction of <figref idref="DRAWINGS">FIG. <b>1</b></figref> to detect surfaces <b>28</b> of structures in front of vehicle <b>10</b> such as obstacle <b>26</b> and roadway <b>14</b>). Sensors <b>24</b>F and/or other sensors in vehicle <b>10</b> may include lidar, radar, visible and/or infrared cameras, and/or other sensors. For example, sensors <b>24</b>F may include two-dimensional image sensors and/or three-dimensional image sensors operating using structured light, binocular vision, time-of-flight (e.g., lidar or radar), and/or other three-dimensional imaging arrangements. Sensors <b>24</b>F may include a three-dimensional sensor that measures the three-dimensional shape of surface(s) <b>28</b> and that optionally measures the pattern of headlight illumination from headlights <b>16</b> on surface(s) <b>28</b>. If desired, a two-dimensional image sensor may be used to measure the headlight illumination pattern on surface(s) <b>28</b> (e.g., the forward-facing sensor circuitry of vehicle <b>10</b> may use three-dimensional and two-dimensional sensors to respectively measure surface shapes and headlight illumination intensity or both of these sensors may be used in gathering information on surface shape and/or surface illumination).</p><p id="p-0028" num="0027">To ensure that surfaces <b>28</b> are sufficiently well illuminated to be visible to a user in vehicle <b>10</b> and to be visible to visible-light image sensors in sensors <b>26</b>F, headlights <b>16</b> may produce visible light illumination. To help ensure that optional infrared image sensors in forward-facing sensors <b>24</b>F receive sufficient reflected infrared light from the illuminated structures in front of vehicle <b>10</b>, headlights <b>16</b> may, if desired, produce infrared illumination. The forward-facing sensor circuitry of vehicle <b>10</b> that is used in measuring headlight illumination may be sensitive to visible light and, if desired, infrared light.</p><p id="p-0029" num="0028">To correct for misalignment of headlights <b>16</b> over time (e.g., misalignment due to shifts in the mounting structures for headlights <b>16</b>, changes in vehicle suspension components, etc.), the control circuitry of vehicle <b>10</b> may control positioners in headlights <b>16</b> dynamically based on sensor measurements (e.g., based on discrepancies between an expected pattern of headlight illumination and a measured pattern of illumination). If, as an example, headlights <b>16</b> are pointed too high, a positioner may be used to tilt headlights <b>16</b> downwards so that headlights <b>16</b> are aimed appropriately. In this way, headlights <b>16</b> may be automatically compensated for misalignment and may remain aligned during operation of vehicle <b>10</b>.</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a cross-sectional side view of an illustrative headlight showing how the headlight may be mounted to body <b>12</b>. Body <b>12</b> may have a cavity that receives headlight <b>16</b>, headlight <b>16</b> may be attached to an outer surface of body <b>12</b>, and/or headlight <b>16</b> may be otherwise supported by body <b>12</b>. As shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, headlight <b>16</b> may include headlight housing <b>30</b> and one or more lenses or other optical components such as headlight lens <b>32</b>. Housing <b>30</b> may include support structures and enclosure structures for supporting the components of headlight <b>16</b>. These structures may facilitate mounting of headlight <b>16</b> to body <b>12</b>. Housing <b>30</b> may include polymer, metal, carbon-fiber composites and other fiber composites, glass, ceramic, other materials, and/or combinations of these materials. Lens <b>32</b> may include polymer, glass, transparent ceramic, and/or other materials that are transparent to visible light and infrared light (e.g., near infrared light). Headlight <b>16</b> includes a light source such as light source <b>40</b> that emits light <b>20</b>. Light <b>20</b> may include visible light (e.g., light from 400 nm to 750 nm) and, if desired, may include infrared light (e.g., near infrared light at one or more wavelengths from 800 to 2500 nm or other suitable infrared light). Lens <b>32</b> may be formed from one or more lens elements and may be used to help collimate light <b>20</b> and direct light <b>20</b> from headlight <b>16</b> in desired directions (e.g., to produce a beam of illumination in the +X direction).</p><p id="p-0031" num="0030">Light source <b>40</b> may include one or more light-emitting devices such as light-emitting diodes, lasers, lamps, or other components that emit light. Optical elements such as reflectors, lenses, diffusers, colored elements, filters, adjustable shutters for adjusting the output of headlight <b>16</b> between low-beam and high-beam illumination patterns, and/or other optical components may be included in headlamp <b>16</b> (e.g., such optical elements may be included in housing <b>30</b>). Independently adjustable light-emitting diodes and electrically adjustable components such as adjustable shutters and/or other adjustable optical components associated with headlight <b>16</b> may be adjusted by the control circuitry of vehicle <b>10</b> to adjust the direction of light <b>20</b> and the shape of the area covered by light <b>20</b> (e.g., to adjust light <b>20</b> to produce a desired low-beam or high-beam illumination pattern and/or other illumination pattern(s), to steer light <b>20</b>, etc.).</p><p id="p-0032" num="0031">A positioner such as positioner <b>44</b> may be used to adjust the position and therefore the angular orientation of headlight <b>16</b> relative to body <b>12</b>. Positioner <b>44</b> may include one or more electrically adjustable actuators such as actuators <b>42</b> and may include optional manually adjusted positioning components (e.g., threaded members that can be rotated with a manual or motorized screwdriver to adjust the position of headlight <b>16</b>). Actuators <b>42</b> may include one or more motors, solenoids, and/or other actuators. In response to commands from the control circuitry of vehicle <b>10</b>, the positioner formed from actuator(s) <b>42</b> may be used to translate headlight <b>16</b> along the X, Y, and/or Z axes and/or other axes and/or may be used to rotate headlight <b>16</b> about the X, Y, and/or Z axes and/or other axes. As one example, actuators <b>42</b> may tilt headlight <b>16</b> up and down relative to the structures in front of vehicle <b>10</b> by rotating headlight <b>16</b> about the Y axis of <figref idref="DRAWINGS">FIG. <b>2</b></figref> and may rotate headlight <b>16</b> to the left and right about the Z axis of <figref idref="DRAWINGS">FIG. <b>2</b></figref>. If desired, the positioner for headlight <b>16</b> may be used to make different types of position adjustments (e.g., rotations about the X axis, translation and/or rotation relative to another axis, etc.). The use of a positioner such as positioner <b>44</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref> that is formed from one or more actuators <b>42</b> in vehicle <b>10</b> to tilt headlight <b>16</b> up/down and to rotate headlight <b>16</b> right/left is illustrative.</p><p id="p-0033" num="0032">During operation, vehicle <b>10</b> may adjust headlights <b>16</b> to accommodate different driving conditions. One or more adjustable shutters, adjustable light-emitting devices, and/or other adjustable components in headlights <b>16</b> may be controlled by the control circuitry of vehicle <b>10</b>. If desired, high-beams or low-beams may be selected based on user input and/or based on oncoming traffic detected using one or more sensors. As another example, when it is determined (from a steering system sensor, location sensor, lidar sensor, etc.) that the roadway on which vehicle <b>10</b> is traveling is starting to curve to the left, headlights <b>16</b> can automatically be turned to the left by the positioner to ensure that the roadway is satisfactorily illuminated by light <b>20</b>. Headlights <b>16</b> may also be turned on and off and/or otherwise adjusted based on measured ambient lighting conditions, weather, and other factors.</p><p id="p-0034" num="0033">Adjustments to the position of headlights <b>16</b> may also be made for calibration purposes. For example, to avoid risk that headlights <b>16</b> might become misaligned over time, vehicle <b>10</b> may monitor the alignment of headlights <b>16</b>. Vehicle <b>10</b> may, as an example, use forward-facing sensor circuitry to map the structures in front of vehicle <b>10</b> and to measure the pattern of illumination on these structures. From these measurements, the control circuitry of vehicle <b>10</b> may determine which (if any) corrective actions are to be taken. For example, vehicle <b>10</b> may determine how headlights <b>16</b> should be repositioned by positioner <b>44</b> to correct for detected changes in headlight alignment.</p><p id="p-0035" num="0034">To map the structures in front of vehicle <b>10</b>, vehicle <b>10</b> may use a three-dimensional sensor to gather a three-dimensional image of the structures. The three-dimensional sensor may be a lidar sensor, a radar sensor, a stereoscopic camera, a structured light sensor, or other three-dimensional image sensor that can gather three-dimensional images. Consider, as an example, the scenario of <figref idref="DRAWINGS">FIG. <b>3</b></figref>. In the example of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, vehicle <b>10</b> is traveling on roadway <b>14</b> (e.g., a public road, a driveway, etc.). A three-dimensional sensor in forward-facing sensor(s) <b>26</b>F is facing forward in the +X direction. Surfaces <b>28</b> are associated with the portion of roadway <b>14</b> in front of vehicle <b>10</b> and object <b>26</b> and are in the field of view of the three-dimensional sensor. The three-dimensional sensor may therefore capture a three-dimensional image of surfaces <b>28</b> to determine the shape (e.g., the location in three dimensions) of roadway <b>14</b> and the shape (e.g., the location in three-dimensions) of object <b>26</b>. The captured shape information includes information on the distance between vehicle <b>10</b> and surfaces <b>28</b>. Objects such as roadway <b>14</b> and object <b>26</b> may receive illumination from headlights <b>16</b> and may therefore sometimes be referred to as target objects or a target.</p><p id="p-0036" num="0035">Object <b>26</b> of surfaces <b>28</b> may be a test target that has a predetermined set of registration marks <b>50</b> (sometimes referred to as fiducials, optical targets, or alignment marks) or may be any other object (e.g., an everyday object such as a wall, garage door, vehicle, or other structure). As an example, object <b>26</b> may be an external object that contains detectable surface markings <b>54</b> (e.g., visually apparent markings or other characteristics that allow the three-dimensional sensor to sense the shape and appearance surfaces <b>28</b>). The presence of marks <b>50</b> and/or other markings <b>54</b> may assist vehicle <b>10</b> in accurately measuring the location surfaces <b>28</b>. For example, alignment marks <b>50</b> may be separated by known distances from each other, so analysis of an image that contains marks <b>50</b> may help determine the distance of object <b>26</b> to vehicle <b>10</b> and may help determine the angular orientation of object <b>26</b> relative to vehicle <b>10</b>. In three-dimensional sensors based on stereoscopic image sensors, the presence of marks <b>50</b> and/or markings <b>54</b> may help in the construction of three-dimensional images from stereoscopic pairs of two-dimensional images. If desired, sensor data from multiple sources in the forward-facing sensor circuitry of vehicle <b>10</b> may be combined to further enhance three-dimensional surface shape measurements. As an example, three-dimensional image data from a lidar sensor may be combined with three-dimensional data from a stereoscopic camera, three-dimensional radar data, and data from a two-dimensional sensor.</p><p id="p-0037" num="0036">Based on the three-dimensional image of surfaces <b>28</b> that is captured using the three-dimensional image sensor, vehicle <b>10</b> can determine the expected projection of headlight beams (illumination <b>20</b>) from headlights <b>16</b> onto surfaces <b>28</b>. A two-dimensional image sensor or other sensor(s) in sensor(s) <b>24</b>F may measure the actual pattern of illumination <b>20</b> projected onto surfaces <b>28</b>, so that the actual and expected projection patterns can be compared to identify discrepancies.</p><p id="p-0038" num="0037">Consider, as an example, a scenario in which object <b>26</b> is a planar surface that is 10 meters in front of vehicle <b>10</b> and that is orientated perpendicular to vehicle <b>10</b>. Using the three-dimensional image of surfaces <b>28</b>, vehicle <b>10</b> can determine the location and orientation of object <b>26</b> (e.g., 10 m in front of vehicle <b>10</b>) and can determine the tilt and/or other characteristics of roadway <b>14</b>. The three-dimensional image of roadway <b>14</b> may reveal, as an example, that roadway <b>14</b> is flat and horizontal. Based on the known shape of surfaces <b>28</b> (e.g., the known position of the surface of object <b>26</b> relative to vehicle <b>10</b> and roadway <b>14</b>), vehicle <b>10</b> (e.g., the control circuitry of components <b>24</b>) may determine the position of headlights <b>16</b> relative to surfaces <b>28</b> and thereby predict the locations on surfaces <b>28</b> of left and right headlight illumination center points <b>52</b> on object <b>26</b> that are to be produced by left and right headlights <b>16</b> in vehicle <b>10</b>, respectively. If desired, headlight operation may be characterized by making other headlight illumination intensity measurements (e.g., measurements that identify the edges of a headlight beam, or other headlight illumination measurements that determine the direction of the headlight illumination).</p><p id="p-0039" num="0038">Due to vibrations and normal aging in the mounting components for headlights <b>16</b> and/or other variations in vehicle <b>10</b> over time, there may be a tendency for headlights <b>16</b> to move out of perfect alignment. As an example, in the absence of intervention, the left and right headlights of vehicle <b>10</b> might slowly begin to aim higher than nominal. Knowing the distance of object <b>26</b> from headlights <b>16</b> and the nominal (correct) orientation of headlights <b>16</b>, vehicle <b>10</b> can predict the correct location of headlight aiming points <b>52</b>. By capturing an image of the projected output of headlights <b>16</b>, the actual orientation of headlights <b>16</b> (e.g., the actual direction in which headlights <b>16</b> are pointed) can be measured and compared with the expected orientation of headlights <b>16</b> when perfectly aligned (e.g., the expected direction in which headlights <b>16</b> should be pointed). For example, an image sensor in vehicle <b>10</b> may capture an image of surfaces <b>28</b> while surfaces <b>28</b> are under illumination from headlights <b>16</b>. The pattern of light <b>20</b> projected onto surfaces <b>28</b> (e.g., object <b>26</b> and roadway <b>14</b>) may reveal that headlights <b>16</b> are pointed 10 cm higher on the surface of object <b>26</b> than expected (e.g., points <b>52</b> may be 10 cm too high, in this example). Because the shape of surfaces <b>28</b> is known and the distance from headlights <b>16</b> to the surface of object <b>26</b> is known, vehicle <b>10</b> can determine from the measured 10 cm vertical offset of points <b>52</b> that headlights <b>16</b> are pointed 2&#xb0; too high (as an example). Based on this determination, positioner <b>44</b> can be directed to tilt headlights <b>16</b> downwards by 2&#xb0; to compensate for the measured 2&#xb0; of angular misalignment. This aligns headlights <b>16</b> so that they point were expected and so that points <b>52</b> on object <b>26</b> coincide with their expected positions. In this way, the overall pattern of illumination produced when light <b>20</b> strikes surfaces <b>28</b> will be as desired.</p><p id="p-0040" num="0039">In monitoring headlight performance, vehicle <b>10</b> may measure the peak intensity of headlight illumination <b>20</b>, may measure the edges of illumination <b>20</b> (e.g., the boundary of the illumination pattern), and/or may measure other headlight performance parameters to characterize the output of headlights <b>16</b>. One or more of these measured headlight performance parameters may then be compared to corresponding predicted headlight performance parameters.</p><p id="p-0041" num="0040">Consider, as an example, the headlight output shown in the graph of <figref idref="DRAWINGS">FIG. <b>4</b></figref>. In the example of <figref idref="DRAWINGS">FIG. <b>4</b></figref>, headlight output intensity I has been plotted as a function of DISTANCE (e.g., distance across surfaces <b>28</b> parallel to the X axis or Y axis of <figref idref="DRAWINGS">FIG. <b>3</b></figref>). Solid line <b>60</b> corresponds to the expected output of headlight <b>16</b> when headlight <b>16</b> is properly aligned (e.g., a prediction based on the measured shape of surfaces <b>28</b> and the known nominal operating characteristics of headlights <b>16</b> when aligned). Dashed line <b>62</b> corresponds to the measured output of headlight <b>16</b> (e.g., the output measured by capturing an image of surfaces <b>28</b> while illuminated by light <b>20</b>). To determine how much measured performance varies from expected performance, vehicle <b>10</b> may determine the location of the peak in intensity I for each curve, may determine the locations of the edges of each curve, and/or may otherwise measure the intensity and position of the light output from headlights <b>16</b>.</p><p id="p-0042" num="0041">As shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, for example, expected intensity curve <b>60</b> has an expected intensity peak <b>64</b>, whereas measured curve <b>62</b> has a measured intensity peak <b>66</b> that is shifted by a distance DP with respect to peak <b>64</b>. Vehicle <b>10</b> may compare points <b>64</b> and <b>66</b> to determine the value of DP and/or vehicle <b>10</b> may gather information on the expected and measured intensity patterns for headlights by comparing edge intensities (see, e.g., points <b>68</b>, which correspond to the positions of the edges of the headlight illumination pattern where expected intensity <b>60</b> has fallen to intensity threshold ITH and points <b>70</b> which correspond to the measured positions of these edges where measured intensity <b>62</b> has intensity threshold ITH). Using illumination pattern edges, peaks, and/or other illumination pattern characteristics, predicated and measured headlight information (e.g., curves <b>60</b> and <b>62</b>) can be compared by vehicle <b>10</b> to determine the amount by which positioner <b>44</b> should be adjusted to align headlights <b>16</b>. Headlights <b>16</b> may be aligned collectively (e.g., measurements may take place while left and right headlights are illuminated) or may be aligned individually (e.g., by making a first measurement while the left headlight is illuminated but not the right headlight and by making a second measurement while the right headlight is illuminated but not the left).</p><p id="p-0043" num="0042">If desired, headlights <b>16</b> may contain multiple individually adjustable headlight elements. As shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, for example, headlight <b>16</b> may have multiple headlight elements <b>72</b>, each of which is individually adjustable. Elements <b>72</b> may have independently adjustable light sources (e.g., each element <b>72</b> may correspond to a separate light-emitting diode) and/or elements <b>72</b> may have independently adjustable shutters or other light-adjusting devices. To enhance the accuracy of headlight output characterization measurements, one or more of elements <b>72</b> may be used to produce illumination while remaining elements <b>72</b> do not produce illumination. By cycling through each element <b>72</b> (or set of elements), different corresponding output intensity measurements corresponding to each element <b>72</b> (or set of elements) may be obtained. Consider, as an example, a scenario in which there are three separate light-emitting diodes in headlight <b>16</b> (e.g., elements <b>72</b> correspond to individually adjustable light sources). To determine whether headlight <b>16</b> needs to be aligned, each of the three light-emitting diodes may be turned on in sequence while corresponding images of surfaces <b>28</b> under the resulting illumination are captured. In this way, more detailed headlight illumination measurements may be made than if all elements <b>72</b> were turned on at the same time.</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. <b>6</b></figref> shows how this type of approach may produce multiple partially activated headlight output curves, each of which corresponding to activation of a separate respective element <b>72</b>. For each element <b>72</b>, vehicle <b>10</b> may produce a corresponding expected output curve <b>74</b> and may measure a corresponding actual output intensity (curve <b>76</b>). By gathering headlight performance data using more granular measurements such as these, headlight performance can be gauged more accurately then when all elements <b>72</b> are activated together. Following characterization of each separate element <b>72</b> (e.g., by measuring how much expected curves <b>74</b> are shifted relative to measured curves <b>76</b>) any headlight misalignment may be accurately determined. Positioner <b>44</b> may then be used to move headlight <b>16</b> (e.g., to adjust the angular orientation of headlight <b>16</b>) and/or the relative intensities of each element <b>72</b> may be adjusted to align headlights <b>16</b> and to help ensure that headlights <b>16</b> provide illumination in a desired pattern.</p><p id="p-0045" num="0044">Vehicle <b>10</b> may make measurements on surfaces <b>28</b> and the projected headlight illumination on surfaces <b>28</b> when parked next to a calibration target (e.g., a screen or other object with registration marks <b>50</b>), when parked next to a wall, garage door, or other structure, or during normal operation traveling on a road (e.g., when vehicle <b>10</b> is being driven autonomously or manually through traffic).</p><p id="p-0046" num="0045">Depending on the operating conditions for vehicle <b>10</b>, vehicle <b>10</b> may tilt or otherwise changes its orientation relative to roadway <b>14</b>. As shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, for example, vehicle <b>10</b> may tilt forward when decelerating. This tilt may be detected by a three-dimensional sensor in forward-facing sensors <b>24</b>F and, if desired, may be detected using sensors such as sensors <b>24</b>T (e.g., suspension displacement sensors that sense how much wheels <b>78</b> are protruding from vehicle body <b>12</b> to determine the orientation of vehicle body <b>12</b> relative to roadway <b>14</b>). By measuring the orientation of vehicle <b>10</b> relative to roadway <b>14</b>, the expected location of the headlight illumination on surfaces <b>28</b> may be determined. If, for example, it is determined that vehicle <b>10</b> is tilting downwards, the expected location of points <b>52</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref> will be lower than if it is determined that vehicle <b>10</b> is tilting upwards. Accordingly, sensor information such as vehicle suspension sensor information and/or other tilt sensor information may be taken into account when predicting the location of headlight output on a target.</p><p id="p-0047" num="0046">If desired, positioner <b>44</b> may be controlled, one or more light sources and/or light modulating components may be controlled (see, e.g., elements <b>72</b> of <figref idref="DRAWINGS">FIG. <b>5</b></figref>), and/or other adjustable components associated with headlights <b>16</b> may be controlled to adjust illumination <b>20</b> (e.g., while vehicle <b>10</b> is being driven). These adjustments may be made based on sensor measurements that reveal vehicle tilt, road characteristics such as the presence or predicted presence of speed bumps in roadway <b>14</b> (see, e.g., bump <b>14</b>B), weather (e.g., whether rain or other precipitation is present or is not present), ambient lighting conditions, predicated or detected turns in roadway <b>14</b>, geographic vehicle location, and/or other conditions of vehicle <b>10</b> when parked, when being driven, etc. If desired, vehicle <b>10</b> may have sensors such a sensors <b>241</b>. Sensors <b>241</b> may be, for example, inertial measurement units containing compasses, accelerometers, and/or gyroscopes and may be used to measure the orientation of vehicle body <b>12</b>, forward-facing sensors <b>24</b>F, and/or headlights <b>16</b> with respect to gravity.</p><p id="p-0048" num="0047">Consider, as an example, a scenario in which the control circuitry of vehicle <b>10</b> uses a sensor or other data source to determine that vehicle <b>10</b> is starting to turn to the left along roadway <b>14</b>. Vehicle <b>10</b> may obtain information on the left turn in roadway <b>14</b> from a map database or other external databased, from lidar measurements or other forward-facing sensor measurements, from inertial measurement unit measurements, from steering system components (e.g., steering position sensors), and/or from other sources. In response to detecting that a left-hand bend is present or is upcoming, vehicle <b>10</b> may use positioner <b>44</b> to turn headlights <b>16</b> to the left. This helps ensure that illumination <b>20</b> will be present on roadway <b>14</b>. As another example, if an upcoming bump such as bump <b>14</b>B is detected, vehicle <b>10</b> can automatically adjust the position of headlight <b>16</b> as vehicle <b>10</b> travels over bump <b>14</b>B to help maintain a desired direction for headlight illumination <b>20</b> (e.g., to help ensure that headlight illumination <b>20</b> is directed straight forward, even as vehicle <b>10</b> tilts due to movement of wheels <b>78</b> over bump <b>14</b>B. If desired, headlights <b>16</b> may support low-beam and high-beam modes. Vehicle <b>10</b> may switched between these modes based on sensor data from sensors in vehicle <b>10</b> such as rain sensors (e.g., moisture sensors), ambient light sensors, oncoming headlight sensors, traffic sensors, and/or other sensors. Headlight movements such as movements to accommodate bends in a road may be may be taken into account during automatic alignment operations. For example, if headlights <b>16</b> have been turned to the left due to the presence of a left turn in roadway <b>14</b>, vehicle <b>10</b> will expect that headlight illumination <b>20</b> will likewise be moved to the left on surfaces <b>28</b> and can therefore take this information into account when measuring headlight output to assess headlight alignment.</p><p id="p-0049" num="0048">Illustrative operations involved in using vehicle <b>10</b> are shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>.</p><p id="p-0050" num="0049">During the operations of block <b>80</b>, headlights <b>16</b> may be used to illuminate object <b>26</b>, roadway <b>14</b> (e.g., surfaces <b>28</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>). Left and right headlights <b>16</b> may be illuminated simultaneously or separately. In headlight configurations in which each headlight has multiple adjustable elements such as elements <b>72</b> of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, these elements may, if desired, be individually adjusted during the operations of block <b>80</b> (e.g., to provide information during headlight characterization on the individual contributions of these elements to different portions of the headlight illumination supplied by headlights <b>16</b>).</p><p id="p-0051" num="0050">During the operations of block <b>82</b>, a three-dimensional sensor in forward-facing sensors <b>24</b>F may be used to capture images of surfaces <b>28</b> (e.g., three-dimensional images may be captured). The presence of registration marks <b>50</b> on a target surface such as the surface of object <b>26</b> and/or other detectable features such as markings <b>54</b> may facilitate the capturing of satisfactory three-dimensional image data from a target. In addition to obtaining a three-dimensional map (shape) for surfaces <b>28</b>, vehicle <b>10</b> may capture an image of the headlight illumination from headlight(s) <b>16</b> that is present on surfaces <b>28</b>. For example, a visible light image and/or an infrared image from a three-dimensional image sensor, a separate two-dimensional image sensor, or other sensor may be captured that shows the location of the peak intensity of headlight illumination and/or that shows other headlight illumination features (e.g., the locations of the edges of the headlight illumination pattern).</p><p id="p-0052" num="0051">If desired, information on the three-dimensional shapes of surfaces in front of the vehicle may be obtained from a local (in-vehicle) and/or remote navigation system database in addition to or instead of obtaining three-dimensional shape information. For example, a three-dimensional map of the environment may be stored in a navigation database for use in driving assistance functions and/or autonomous driving functions. Information from navigation system sensors (e.g., Global Positioning System circuitry and/or other satellite navigation system circuitry, inertial measurement units, lidar, image recognition systems, and/or other navigation sensors) may be used to determine vehicle location (position and orientation). The vehicle location information that is obtained from the navigation system sensors in this way may be used to retrieve corresponding three-dimensional surface shape information from the database (e.g., the three-dimensional shapes of surfaces at the determined vehicle location.</p><p id="p-0053" num="0052">After measuring the shape of surfaces <b>28</b> and/or otherwise determining the shape of surfaces <b>28</b> (e.g., by obtaining information from a database) and measuring the pattern of headlight illumination <b>20</b> that is illuminating surfaces <b>28</b>, vehicle <b>10</b> may, during the operations of block <b>84</b>, determine the expected pattern of the headlight illumination on surfaces <b>28</b> (e.g., the expected peak intensity position of headlight output, the expected location of headlight beam edges, and other characteristics associated with the direction in which the headlight illumination is expected to be pointing). The expected headlight illumination pattern is determined based on the known shape of surfaces <b>28</b> (e.g., the location in three dimensions of surfaces <b>28</b> relative to vehicle <b>10</b>), and the known nominal performance characteristics of headlights <b>16</b> (e.g., the known size and shape of the beam of light emitted by each headlight. During block <b>84</b>, vehicle <b>10</b> measures the actual headlight illumination pattern produced on surfaces <b>28</b> by headlights <b>16</b> and compares the measured headlight illumination information to the expected headlight illumination information.</p><p id="p-0054" num="0053">If the expected and measured illumination patterns (center position, edge position, etc.) do not match, corrective action may be taken based on the results of the comparison to align headlights <b>16</b>. For example, during the operations of block <b>86</b>, the control circuitry of vehicle <b>10</b> may direct positioner <b>44</b> to tilt headlight <b>16</b> downwards by 3&#xb0; in response to detection of an undesired 3&#xb0; upward tilt. As shown by line <b>88</b>, the automatic alignment operations of <figref idref="DRAWINGS">FIG. <b>8</b></figref> may be performed repeatedly (e.g., whenever vehicle <b>10</b> is parked, periodically according to a schedule, whenever satisfactory surfaces <b>28</b> are available in front of vehicle <b>10</b>, in response to a user input command, and/or in response to determining that other headlight calibration criteria have been satisfied).</p><p id="p-0055" num="0054">Although sometimes described in the context of headlights, any suitable lights in vehicle <b>10</b> may be aligned using the approach of <figref idref="DRAWINGS">FIG. <b>8</b></figref> (e.g., fog lights, tail lights, parking lights, supplemental side lighting, etc.). In addition to performing headlight alignment operations, the control circuitry of vehicle <b>10</b> may, if desired, use sensor measurements to calibrate actuators such as positioner <b>44</b>. As an example, when vehicle <b>10</b> is parked, positioner <b>44</b> may be calibrated by directing positioner <b>44</b> to move while making corresponding sensor measurements to evaluate the accuracy of these movements. Examples of sensors that may be used in gauging actuator performance so that compensating calibration operations may be performed include light sensors (e.g., an image sensor that measures whether light output from headlight <b>16</b> moves by 4.5&#xb0; when positioner <b>44</b> is directed to move by 4.5&#xb0;) and inertial measurement units (e.g., an inertial measurement unit coupled to positioner <b>44</b> that measures the angular movement of positioner <b>44</b> during calibration). Calibrating positioner <b>44</b> while vehicle <b>10</b> is stationary (parked), enables vehicle <b>10</b> to more accurately perform open loop control of the aim of positioner <b>44</b> when driving based on navigation system information (inertial measurement unit data and satellite navigation system data) and other data.</p><p id="p-0056" num="0055">The foregoing is merely illustrative and various modifications can be made to the described embodiments. The foregoing embodiments may be implemented individually or in any combination.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A vehicle, comprising:<claim-text>a vehicle body;</claim-text><claim-text>a headlight supported by the vehicle body that is configured to produce headlight illumination;</claim-text><claim-text>control circuitry configured to detect a difference between an expected direction of the headlight illumination and a measured direction of the headlight illumination; and</claim-text><claim-text>an electrically adjustable positioner configured to align the headlight in response to the detected difference.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The vehicle defined in <claim-ref idref="CLM-00001">claim 1</claim-ref> further comprising:<claim-text>forward-facing sensor circuitry on the vehicle body that is configured to capture a three-dimensional image of a surface in front of the vehicle body and that is configured to measure the headlight illumination from the headlight as the headlight illuminates the surface, wherein the control circuitry is configured to use the three-dimensional image and the measured headlight illumination on the surface in determining the expected direction of the headlight illumination.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The vehicle defined in <claim-ref idref="CLM-00002">claim 2</claim-ref> wherein the forward-facing sensor circuitry comprises a two-dimensional image sensor configured to measure the headlight illumination.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The vehicle defined in <claim-ref idref="CLM-00002">claim 2</claim-ref> wherein the forward-facing sensor circuitry comprises a lidar sensor that captures the three-dimensional image.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The vehicle defined in <claim-ref idref="CLM-00002">claim 2</claim-ref> wherein the forward-facing sensor circuitry comprises a three-dimensional sensor configured to capture the three-dimensional image and wherein the three-dimensional sensor comprises a three-dimensional sensor selected from the group consisting of: a radar sensor, a stereoscopic camera, and a structured light sensor.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The vehicle defined in <claim-ref idref="CLM-00001">claim 1</claim-ref> further comprising a sensor configured to measure the headlight illumination while the control circuitry adjusts the headlight to change the headlight illumination.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The vehicle defined in <claim-ref idref="CLM-00006">claim 6</claim-ref> wherein the headlight comprises multiple light sources and wherein the sensor is configured to measure the headlight illumination while the control circuitry adjusts the multiple light sources to produce different respective amounts of light.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The vehicle defined in <claim-ref idref="CLM-00006">claim 6</claim-ref> wherein the headlight is operable in a low-beam mode and high-beam mode and wherein the sensor is configured to measure the headlight illumination while the control circuitry changes the headlight between operation in the low-beam mode and the high-beam mode.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The vehicle defined in <claim-ref idref="CLM-00001">claim 1</claim-ref> further comprising a sensor configured to detect tilt of the vehicle body relative to a roadway, wherein the control circuitry is configured to adjust the electrically adjustable positioner based on the detected tilt.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The vehicle defined in <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the control circuitry is configured to adjust the electrically adjustable positioner based on information, wherein the information comprises information selected from the group consisting of: weather information, vehicle location information, and roadway information.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The vehicle defined in <claim-ref idref="CLM-00001">claim 1</claim-ref> further comprising:<claim-text>navigation system circuitry configured to determine vehicle location, wherein the control circuitry is configured to use the determined vehicle location to retrieve a three-dimensional surface shape from a database corresponding to a surface in front of the vehicle body; and</claim-text><claim-text>an image sensor configured to measure the headlight illumination from the headlight as the headlight illuminates the surface, wherein the control circuitry is configured to use the three-dimensional surface shape and the measured headlight illumination on the surface in determining the expected direction of the headlight illumination.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The vehicle defined in <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the control circuitry is further configured to calibrate the electrically adjustable positioner while the vehicle body is parked.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. A vehicle, comprising:<claim-text>a vehicle body;</claim-text><claim-text>a headlight that is configured to produce headlight illumination on a surface in front of the vehicle body;</claim-text><claim-text>sensor circuitry configured to obtain a surface measurement on the surface and configured to obtain a headlight illumination measurement of the headlight illumination on the surface;</claim-text><claim-text>an electrically adjustable positioner configured to move the headlight relative to the vehicle body; and</claim-text><claim-text>control circuitry configured to adjust the electrically adjustable positioner to align the headlight based on the surface measurement and the headlight illumination measurement.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The vehicle defined in <claim-ref idref="CLM-00013">claim 13</claim-ref> wherein the sensor circuitry comprises a three-dimensional sensor and wherein the surface measurement comprises a three-dimensional surface shape gathered by the three-dimensional sensor.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The vehicle defined in <claim-ref idref="CLM-00014">claim 14</claim-ref> wherein the three-dimensional sensor comprises a lidar sensor.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The vehicle defined in <claim-ref idref="CLM-00014">claim 14</claim-ref> wherein the three-dimensional sensor comprises a stereoscopic sensor having a pair of cameras.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The vehicle defined in <claim-ref idref="CLM-00014">claim 14</claim-ref> wherein the three-dimensional sensor comprises an optical sensor.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The vehicle defined in <claim-ref idref="CLM-00014">claim 14</claim-ref> wherein the three-dimensional sensor comprises a radar sensor.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The vehicle defined in <claim-ref idref="CLM-00013">claim 13</claim-ref> wherein the control circuitry is configured use the surface measurement to determine an expected position of the headlight illumination on the surface and is configured to compare the expected position to a measured position of the headlight illumination on the surface obtained from the headlight illumination measurement.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The vehicle defined in <claim-ref idref="CLM-00019">claim 19</claim-ref> wherein the control circuitry is configured to adjust the electrically adjustable positioner to align the headlight based on the comparison of the expected position to the measured position.</claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. A vehicle, comprising:<claim-text>a vehicle body;</claim-text><claim-text>a headlight;</claim-text><claim-text>a positioner configured to move the headlight relative to the body;</claim-text><claim-text>a three-dimensional sensor configured to measure a surface of an object in front of the vehicle body; and</claim-text><claim-text>an image sensor configured to measure a position on the surface where the headlight is pointed; and</claim-text><claim-text>control circuitry configured to:<claim-text>use the measured surface to determine a predicated position on the target where the headlight is expected to be aimed when the headlight is aligned relative to the vehicle body;</claim-text><claim-text>compare the measured position to the predicted position; and</claim-text><claim-text>use the positioner to move the headlight based on the comparison.</claim-text></claim-text></claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. The vehicle defined in <claim-ref idref="CLM-00021">claim 21</claim-ref> wherein the three-dimensional sensor is configured to measure the shape of the surface in three dimensions and the distance of the surface from to the vehicle body.</claim-text></claim></claims></us-patent-application>