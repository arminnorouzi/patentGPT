<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004723A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004723</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17903123</doc-number><date>20220906</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>30</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>284</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>289</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>151</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>54</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>62</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>44</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20200101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>30</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20200101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>284</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20200101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>289</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20200101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>151</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>542</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>6215</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>443</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">SYSTEMS AND METHODS FOR IDENTIFYING AN EVENT IN DATA</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16700746</doc-number><date>20191202</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11461555</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17903123</doc-number></document-id></child-doc></relation></continuation><us-provisional-application><document-id><country>US</country><doc-number>62773502</doc-number><date>20181130</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Thomson Reuters Enterprise Centre GmbH</orgname><address><city>Zug</city><country>CH</country></address></addressbook><residence><country>CH</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Ekmekci</last-name><first-name>Berk</first-name><address><city>Sterling</city><state>VA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Hagerman</last-name><first-name>Eleanor</first-name><address><city>Annandale</city><state>VA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Howald</last-name><first-name>Blake Stephen</first-name><address><city>Northfield</city><state>MN</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">The present disclosure includes systems, apparatuses, and methods for event identification. In some aspects, a method includes receiving data including text and performing natural language processing on the received data to generate processed data that indicates one or more sentences. The method also includes generating, based on a first keyword set, a second keyword set having more keywords than the first keyword set. The method further includes, for each of the first and second keyword sets: detecting one or more keywords and one or more entities included in the processed data, determining one or more matched pairs based on the detected keywords and entities, and extracting a sentence, such as a single sentence or multiple sentences, from a document based on the one or more sentences indicated by the processed data. The method may also include outputting at least one extracted sentence.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="92.03mm" wi="158.75mm" file="US20230004723A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="215.90mm" wi="135.38mm" orientation="landscape" file="US20230004723A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="237.57mm" wi="156.97mm" file="US20230004723A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="239.69mm" wi="145.37mm" orientation="landscape" file="US20230004723A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="237.49mm" wi="145.37mm" orientation="landscape" file="US20230004723A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="242.49mm" wi="154.77mm" file="US20230004723A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">The present application is a continuation of U.S. patent application Ser. No. 16/700,746 filed Dec. 2, 2019 and entitled &#x201c;SYSTEMS AND METHODS FOR IDENTIFYING AN EVENT IN DATA,&#x201d; which claims priority to U.S. Provisional Application No. 62/773,502 filed Nov. 30, 2018, and entitled, &#x201c;CEREAL: A Consolidated System for Robust Multi-Document Entity Risk Extraction and Taxonomy Augmentation,&#x201d; the disclosures of which are incorporated by reference herein in their entirety.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">The present subject matter is directed generally to event identification, and more particularly but without limitation, to identifying or predicting a risk event in textual data.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">Identifying or predicting risk events in textual data associated with individuals, companies, and other entities is a common natural language processing (NLP) task known as risk mining. Monitoring systems rely on risk mining to describe risk events that are passed on to an expert for analysis.</p><p id="p-0005" num="0004">Risk mining research is often focused on classification of risk from non-risk and the features that contribute to the distinction. For example, prior risk mining research has considered features such as textual association with stock price movement and financial documents, sentiment features in identifying banking risks, and textual features for risk in news. Additionally, heuristic classifications have been used in which seed patterns that describe risk relationships are used to augment a risk taxonomy used in an alert monitoring system for earnings reports. The risk taxonomy can be a source of information in risk mining whether the result of manual definition, machine learning classification, or both, crowdsourcing, and/or paraphrase detection. Although risk mining research has been explored, the quality of risk mining results are often less than acceptable and/or require manual review of the risk mining results to obtain filtered results. Additionally, large quantities of results can make the manual review process time consuming and delay or prolong timely review of the risk mining results.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0006" num="0005">The present application relates to systems and methods for event identification and/or prediction of events, such as risk event identification and/or prediction in textual data. To illustrate, the systems and methods may use a predefined keyword taxonomy, entities and data sources to generate and return text extractions based on bidirectional distances (e.g., token distances) between entities and keywords. For example, data including text from a data source, such as a streaming data source, news data, a database, or a combination thereof, may be received and natural language processing (NLP) is performed on the data. In some implementations, the NLP may generate processed data that indicates one or more sentences. Additionally, or alternatively, the NLP may include tokenization, lemmatization, and/or sentencization on the data. In a particular implementation, the NPL includes a natural language processing pipeline including (in sequence) a tokenizer, a part-of-speech tagger, a dependency parser, and a named entity recognizer. It is noted that a dependency-based sentencizer may be used as compared to a simpler stop-character based approach due to the unpredictable formatting of certain domains of text&#x2014;e.g. web-mined news and regulatory filings. After the NLP, a set of keywords are compared to the processed data to detect keywords included in the processed data. For each detected keyword, a corresponding entity is identified that is positioned closest to the corresponding keyword to determine a matched pair for the keyword. In some implementations, a distance between the keyword and the entity of the matched pair is compared to a threshold distance for purposes of discarding matched pairs. Based on a particular matched pair, the systems and methods may extract a sentence, such as a single sentence or multiple sentences, from a document based that includes the matched pair (e.g., the entity and the keyword). The systems and methods may also output at least one extracted sentence. The extracted sentence may be stored or provided to an electronic device for review and/or analysis.</p><p id="p-0007" num="0006">In some implementations, the systems and methods may also expand an initial seed taxonomy, such as a keyword set, using word vector encodings. For example, for at least one document of one or more documents corresponding to the data, a corresponding semantic vector may be generated&#x2014;e.g., based on a skipgram model that utilizes words and subwords from the document. For at least one keyword, the at least one keyword is compared to each of one or more semantic vectors to determine a corresponding similarity score. A semantic vector having a highest similarity score to the keyword is identified to determine a term of the identified semantic vector as a candidate term. In some implementations, the similarity score of the determined semantic vector having a highest similarity score is compared to a threshold to determine whether or not to discard the candidate term&#x2014;e.g., the term is discarded if the score is less than or equal to the threshold. The candidate term may be added to the keyword set to generate the expanded keyword set. The initial keyword set and/or the expanded keyword may be applied to the processed data to identify matched pairs and to extract one or more sentences as described above.</p><p id="p-0008" num="0007">Thus, the present disclosure provides the ability to extract events (e.g., risks) at scale, improve the quality of the extractions over time, grow the keyword taxonomy to increase coverage, and do so with minimum manual effort. The systems and methods described herein also include a scalable system for finding entity-event pairings which, as compared to other systems, better incorporate human insights in the initial system configuration to obviate the need for a risk classification engine. For example, the systems and methods described herein provide a hybrid human-automated system that provides scalable event-monitoring capabilities. Further, without relying on multiple complex NLP sub-systems to make connections between entities and event keywords, the system may derive maximum benefit from surface information. Because of the specificity of the initial taxonomy, extracts are assumed to express some degree of entity-risk relationships and are based on &#x201c;shallow&#x201d; surface parsing rather than deeper morpho-syntactic parsing. Additionally, the degree to which these two deviations hold is the subject of the evaluation&#x2014;e.g., comparing to a distance threshold between keyword and entity. To illustrate, the systems and methods described herein may utilize techniques, such as filtering and/or thresholding, to address high recall associated with the predefinition of the seed taxonomy. Additionally, or alternatively, the systems and methods described herein may address high system recall relative to maintaining flexibility for analyst users and dynamic definition of the risk problem space&#x2014;this may include summarization of results for better presentation, alternative source data at the direction of the analyst for given risk categories, and token distance thresholding.</p><p id="p-0009" num="0008">It is noted that while applied here to the risk mining space, in other implementations, the system could be used for any data, entities and taxonomies to support generalized event monitoring. To illustrate, the systems and methods may be equally applicable to other areas of identification and/or predication, such as document review, auditing, and the like, as illustrative, non-limiting examples.</p><p id="p-0010" num="0009">In one particular aspect, a method for identifying an event in data includes receiving data at a receiver from a data source, the data including one or more documents each including text. The method includes performing natural language processing on the received data to generate processed data. The processed data indicates one or more sentences. The method includes generating, based on the data and a first keyword set, a second keyword set having a greater number of keywords than the first keyword set. The method also includes, for each keyword set of the first keyword set and each keyword in the second keyword set: detecting one or more keywords and one or more entities included in the processed data based on the keyword set and an entity set, determining one or more matched pairs based on the detected one or more keywords and the detected one or more entities, extracting a sentence from a document based on the one or more sentences indicated by the processed data, and outputting the extracted sentence. The sentence corresponds to at least one matched pair of the one or more matched pairs and includes a single sentence or multiple sentences.</p><p id="p-0011" num="0010">In another aspect, a system may be provided. The system includes a data ingestor configured to receive data at a receiver from a data source and to perform natural language processing on the received data to generate processed data. The data includes one or more documents each including text, and the processed data indicates one or more sentences. The system includes a taxonomy expander configured to generate, based on the data and a first keyword set, a second keyword set having a greater number of keywords than the first keyword set. The system also includes a term detector configured to detect, for each keyword set of the first keyword set and each keyword in the second keyword set, one or more keywords and one or more entities included in the processed data based on the keyword set and an entity set. The system further includes an output generator configured to, for each keyword set of the first keyword set and the second keyword set: determine one or more matched pairs based on the detected one or more keywords and the detected one or more entities, extract a sentence from a document based on the one or more sentences indicated by the processed data, and output the extracted sentence. The sentence corresponds to at least one matched pair of the one or more matched pairs and includes a single sentence or multiple sentences.</p><p id="p-0012" num="0011">In yet another aspect, a computer-based tool may be provided. The computer-based tool may include non-transitory computer readable media having stored thereon computer code which, when executed by a processor, causes a computing device to perform operations that include receiving data at a receiver from a data source. The data includes one or more documents each including text. The operations include performing natural language processing on the received data to generate processed data. The processed data indicates one or more sentences. The operations also include generating, based on the data and a first keyword set, a second keyword set having a greater number of keywords than the first keyword set. The operations further include, for each keyword set of the first keyword set and each keyword in the second keyword set: detecting one or more keywords and one or more entities included in the processed data based on the keyword set and an entity set, determining one or more matched pairs based on the detected one or more keywords and the detected one or more entities, extracting a sentence from a document based on the one or more sentences indicated by the processed data, and outputting the extracted sentence. The sentence corresponds to at least one matched pair of the one or more matched pairs and includes a single sentence or multiple sentences.</p><p id="p-0013" num="0012">The foregoing broadly outlines the features and technical advantages of the present invention in order that the detailed description of the invention that follows may be better understood. Additional features and advantages of the invention will be described hereinafter which form the subject of the claims of the invention. It should be appreciated by those skilled in the art that the conception and specific embodiment disclosed may be readily utilized as a basis for modifying or designing other structures for carrying out the same purposes of the present invention. It should also be realized by those skilled in the art that such equivalent constructions do not depart from the spirit and scope of the invention as set forth in the appended claims. The novel features which are believed to be characteristic of the invention, both as to its organization and method of operation, together with further objects and advantages will be better understood from the following description when considered in connection with the accompanying figures. It is to be expressly understood, however, that each of the figures is provided for the purpose of illustration and description only and is not intended as a definition of the limits of the present invention.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0014" num="0013">For a more complete understanding of the present invention, reference is now made to the following descriptions taken in conjunction with the accompanying drawings, in which:</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows a system configured to perform operations in accordance with aspects of the present disclosure;</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows a flow diagram illustrating functionality of the system of <figref idref="DRAWINGS">FIG. <b>1</b></figref> implemented in accordance with aspects of the present disclosure</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a block diagram of a system for identifying an event in data in accordance with the present disclosure;</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates a graph of expert preference ratings; and</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flow chart illustrating an example of a method of identifying an event in data.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0020" num="0019">Various features and advantageous details are explained more fully with reference to the non-limiting embodiments that are illustrated in the accompanying drawings and detailed in the following description. Descriptions of well-known starting materials, processing techniques, components, and equipment are omitted so as not to unnecessarily obscure the invention in detail. It should be understood, however, that the detailed description and the specific examples, while indicating embodiments of the invention, are given by way of illustration only, and not by way of limitation. Various substitutions, modifications, additions, and/or rearrangements within the spirit and/or scope of the underlying inventive concept will become apparent to those skilled in the art from this disclosure.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram of an exemplary system <b>100</b> configured with capabilities and functionality for event identification. As shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, system <b>100</b> includes server <b>110</b>, at least one user terminal <b>160</b>, at least one data source <b>170</b>, and network <b>180</b>. These components, and their individual components, may cooperatively operate to provide functionality in accordance with the discussion herein. For example, in operation according to one or more implementations, data (e.g., textual data or documents) may be obtained from data sources <b>170</b> and may be provided as input to server <b>110</b>. The various components of server <b>110</b> may cooperatively operate to perform event identification and/or prediction. For example, the various components of server <b>110</b> may cooperative operate to perform natural language processing (NLP) is performed on the data In some implementations, the NLP may generate processed data that indicates one or more sentences. After the NLP, the various components of server <b>110</b> may compare a set of keywords to the processed data to detect keywords included in the processed data. For each detected keyword, a corresponding entity is identified that is positioned closest to the corresponding keyword to determine a matched pair for the keyword. In some implementations, a distance between the keyword and the entity of the matched pair is compared to a threshold distance for purposes of discarding matched pairs. Based on a particular matched pair, the various components of server <b>100</b> may extract a sentence, such as a single sentence or multiple sentences, from a document based that includes the matched pair (e.g., the entity and the keyword). The sentence may be stored provided to an electronic device for review and/or analysis. As such, various aspects of the present disclosure allow event identification (and generation of a text extraction) based on use of a predefined keyword taxonomy, entities and data sources and based on bidirectional distances (e.g., token distances) between entities and keywords detected within the data.</p><p id="p-0022" num="0021">It is noted that the functional blocks, and components thereof, of system <b>100</b> of embodiments of the present invention may be implemented using processors, electronics devices, hardware devices, electronics components, logical circuits, memories, software codes, firmware codes, etc., or any combination thereof. For example, one or more functional blocks, or some portion thereof, may be implemented as discrete gate or transistor logic, discrete hardware components, or combinations thereof configured to provide logic for performing the functions described herein. Additionally or alternatively, when implemented in software, one or more of the functional blocks, or some portion thereof, may comprise code segments operable upon a processor to provide logic for preforming the functions described herein.</p><p id="p-0023" num="0022">It is also noted that various components of system <b>100</b> are illustrated as single and separate components. However, it will be appreciated that each of the various illustrated components may be implemented as a single component (e.g., a single application, server module, etc.), may be functional components of a single component, or the functionality of these various components may be distributed over multiple devices/components. In such aspects, the functionality of each respective component may be aggregated from the functionality of multiple modules residing in a single, or in multiple devices.</p><p id="p-0024" num="0023">In some aspects, server <b>110</b>, user terminal <b>160</b>, and data sources <b>170</b> may be communicatively coupled via network <b>180</b>. Network <b>180</b> may include a wired network, a wireless communication network, a cellular network, a cable transmission system, a Local Area Network (LAN), a Wireless LAN (WLAN), a Metropolitan Area Network (MAN), a Wide Area Network (WAN), the Internet, the Public Switched Telephone Network (PSTN), etc., that may be configured to facilitate communications between user terminal <b>160</b> and server <b>110</b>.</p><p id="p-0025" num="0024">User terminal <b>160</b> may be implemented as a mobile device, a smartphone, a tablet computing device, a personal computing device, a laptop computing device, a desktop computing device, a computer system of a vehicle, a personal digital assistant (PDA), a smart watch, another type of wired and/or wireless computing device, or any part thereof. User terminal <b>160</b> may be configured to provide a graphical user interface (GUI) via which a user may be provided with information related to data and information received from server <b>100</b>. For example, User terminal <b>160</b> may receive results of event identification and/or prediction from server <b>100</b>. The results may include a match pair including a keyword, an entity, or both, one or more extracted sentences, a document identifier, or a combination thereof, as illustrative, non-limiting examples. A user may review the results and provide an analysis or feedback regarding the results. The analysis or feedback may be provided to server <b>100</b> from user terminal <b>160</b> as an input.</p><p id="p-0026" num="0025">Data sources <b>170</b> may comprise at least one source of textual data. For example, the data source(s) may include a streaming data source, news data, a database, a social media feed, a data room, another data source, the like, or a combination thereof. In a particular implementation, the data from data source <b>170</b> may include or correspond to one or more entities. The one or more entities may include an individual, a company, a government, an agency, an organization, the like, or a combination thereof, as illustrative, non-limiting examples.</p><p id="p-0027" num="0026">Server <b>110</b> may be configured to receive data from data source(s) <b>170</b>, to apply customized natural language processing algorithms and/or other processing to identify one or more events based on the received data. This functionality of server <b>110</b> may be provided by the cooperative operation of various components of server <b>110</b>, as will be described in more detail below. Although <figref idref="DRAWINGS">FIG. <b>1</b></figref> shows a single server <b>110</b>, it will be appreciated that server <b>110</b> and its individual functional blocks may be implemented as a single device or may be distributed over multiple devices having their own processing resources, whose aggregate functionality may be configured to perform operations in accordance with the present disclosure. In some implementations, server <b>110</b> may be implemented, wholly or in part, on an on-site system, or on a cloud-based system.</p><p id="p-0028" num="0027">As shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, server <b>110</b> includes processor <b>111</b>, memory <b>112</b>, database <b>113</b>, data ingestor <b>120</b>, output generator <b>122</b>, and, optionally, taxonomy expander <b>124</b>. It is noted that the various components of server <b>110</b> are illustrated as single and separate components in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. However, it will be appreciated that each of the various components of server <b>110</b> may be a single component (e.g., a single application, server module, etc.), may be functional components of a same component, or the functionality may be distributed over multiple devices/components. In such aspects, the functionality of each respective component may be aggregated from the functionality of multiple modules residing in a single, or in multiple devices.</p><p id="p-0029" num="0028">In some aspects, processor <b>111</b> may comprise a processor, a microprocessor, a controller, a microcontroller, a plurality of microprocessors, an application-specific integrated circuit (ASIC), an application-specific standard product (ASSP), or any combination thereof, and may be configured to execute instructions to perform operations in accordance with the disclosure herein. In some aspects, implementations of processor <b>111</b> may comprise code segments (e.g., software, firmware, and/or hardware logic) executable in hardware, such as a processor, to perform the tasks and functions described herein. In yet other aspects, processor <b>111</b> may be implemented as a combination of hardware and software. Processor <b>111</b> may be communicatively coupled to memory <b>112</b>.</p><p id="p-0030" num="0029">Memory <b>112</b> may comprise read only memory (ROM) devices, random access memory (RAM) devices, one or more hard disk drives (HDDs), flash memory devices, solid state drives (SSDs), other devices configured to store data in a persistent or non-persistent state, network memory, cloud memory, local memory, or a combination of different memory devices. Memory <b>112</b> may store instructions that, when executed by processor <b>111</b>, cause processor <b>111</b> to perform operations in accordance with the present disclosure. In aspects, memory <b>112</b> may also be configured to facilitate storage operations. For example, memory <b>112</b> may comprise database <b>113</b> for storing one or more keywords (e.g., one or more keyword sets), one or more entities (e.g., an entity set), one or more thresholds, one or more matched pairs, one or more semantic vectors, one or more candidate terms, one or more similarity scores, one or more extracted sentence, input (e.g., from user terminal <b>160</b>), other information, etc., which system <b>100</b> may use to provide the features discussed herein. Database <b>113</b> may be integrated into memory <b>112</b>, or may be provided as a separate module. In some aspects, database <b>113</b> may be a single database, or may be a distributed database implemented over a plurality of database modules. In some embodiments, database <b>113</b> may be provided as a module external to server <b>110</b>. Additionally, or alternatively, server <b>110</b> may include an interface configured to enable communication with data source <b>170</b>, user terminal <b>160</b> (e.g., an electronic device), or a combination thereof</p><p id="p-0031" num="0030">Data ingestor <b>120</b> may be configured to receive data at a receiver from a data source, the data comprising one or more documents each comprising text, and to perform natural language processing on the received data to generate processed data, the processed data indicating one or more sentences.</p><p id="p-0032" num="0031">Term detector <b>121</b> may be configured to detect, for each keyword set of the first keyword set and each keyword in the second keyword set, one or more keywords and one or more entities included in the processed data based on the keyword set and an entity set.</p><p id="p-0033" num="0032">Output generator <b>122</b> may be configured to, for each keyword set of the first keyword set and the second keyword set, determine one or more matched pairs based on the detected one or more keywords and the detected one or more entities, and extract a sentence from a document based on the one or more sentences indicated by the processed data, where the sentence corresponds to at least one matched pair of the one or more matched pairs and comprises a single sentence or multiple sentences. To determine the one or more matched pairs, output generator <b>122</b> is further configured to for each keyword of the detected one or more keywords, identify a corresponding entity of the detected one or more entities that is positioned closest to the corresponding keyword to determine a matched pair for the keyword, Additionally, output generator <b>122</b> may be further configured to store and/or output the extracted sentence.</p><p id="p-0034" num="0033">In some implementations, output generator <b>122</b> includes a filter <b>123</b>. Filter <b>123</b> is configured to for each of the one or more matched pairs based on the first keyword set and each of the one or more matched pairs based on the second keyword set, determine a distance between the keyword and the entity of the matched pair, perform a comparison between the determined distance and a threshold, and determine to retain the matched pair or discard the matched pair based on whether or not the comparison indicate the determined distance is greater than or equal to the threshold. For example, filter <b>123</b> may discard the matched pair based on the determined distance being greater than or equal to the threshold. Alternatively, filter may retain the matched pair for further processing and/or consideration based on the distance being less than the threshold.</p><p id="p-0035" num="0034">Taxonomy expander <b>124</b> may be configured to generate, based on the data and a first keyword set, a second keyword set having a greater number of keywords than the first keyword set. Additional functionality of taxonomy expander <b>124</b> is described further herein at least with reference to blocks <b>240</b>-<b>248</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>. It is noted that the functionality of taxonomy expander <b>124</b> to expand a keyword set to generate an expanded keyword set may be used prior to, during, or after event identification or prediction.</p><p id="p-0036" num="0035">The database <b>113</b> may be coupled to data ingestor <b>120</b>, term detector <b>121</b>, output generator <b>122</b>, taxonomy expander <b>124</b>, or a combination thereof. In some implementations, database <b>113</b> is configured to store the first keyword set, the second keyword set, the entity set, the processed data, one or more thresholds, one or more extracted sentences, a plurality of matched pairs, or a combination thereof.</p><p id="p-0037" num="0036">The functionality of server <b>110</b> will now be discussed with respect to the block flow diagram illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. <figref idref="DRAWINGS">FIG. <b>2</b></figref> shows a flow diagram illustrating functionality of system <b>100</b> for detecting an event in data. Blocks of method <b>200</b> illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref> may be performed by one or more components of system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. For example, blocks <b>210</b>, <b>212</b>, and <b>214</b> may be performed by data ingestor <b>120</b>, block <b>216</b> may be performed by term detector <b>121</b>, blocks <b>218</b>-<b>222</b> may be performed by output generator <b>122</b>, and blocks <b>240</b>-<b>248</b> may be performed by taxonomy expander <b>124</b>.</p><p id="p-0038" num="0037">At block <b>210</b>, data is received (e.g., at a receiver). For example, the data may include one or more documents and may be received from data sources <b>170</b>. In some implementations, data sources <b>170</b> may include a streaming data source, news data, a database, or a combination thereof. At block <b>212</b>, a keyword set and an entity set are received. In some implementations, the keyword set (e.g., seed values) and the entity set may be received based on user input or retrieved from a memory. At block <b>214</b>, the data is provided to a NLP pipeline to generate processed data that indicates one or more sentences. In some implementations, the NLP pipeline includes (in sequence) a tokenizer, a part-of-speech tagger, a dependency parser, and a named entity recognizer. The dependency parser may be preferable to other types of sentence detectors, such as a stop-character parser, due to the unpredictable formatting of certain domains of text, such as web-mined news and/or regulatory filings.</p><p id="p-0039" num="0038">At block <b>216</b>, keyword and entity identification is performed. For example, based on a taxonomy, keywords (from the keyword set) may be identified in a list of tokens. To illustrate, the set of keywords are compared to the processed data to detect keywords in the processed data. Similarly, entities (from the entity set) may be identified in a list of tokens. At block <b>218</b>, keyword and entity matching is performed. For example, for each detected keyword, a corresponding entity is identified that is positioned closest to the corresponding keyword to determine a matched pair for the keyword. The closest entity may be before or after the keyword, and may be in the same sentence or a different sentence.</p><p id="p-0040" num="0039">At block <b>220</b>, matched pair filtering is performed. For example, a distance (in tokens) between the keyword and the entity of a matched pair is determined, and if the distance is greater than or equal to a threshold, the matched pair is discarded (e.g., filtered out). At block <b>222</b>, a sentence output result is generated. For example, a sentence that contains a matched pair may be extracted and output. The sentence may be a single sentence (if the keyword and the entity are in the same sentence) or multiple sentences (if the keyword and the entity are in different sentences). The extracted sentence may be output to an electronic device for display to a user for review and/or analysis or the extracted sentence may be stored in a memory for later processing.</p><p id="p-0041" num="0040">Method <b>200</b> also enables expansion of an initial seed taxonomy. To illustrate, at block <b>240</b>, semantic vectors are generated. For example, for at least one document of the received data, a corresponding semantic vector may be generated. In some implementations, the semantic vector may be generated based on a skipgram model that utilizes words and subwords from the document. At block <b>242</b>, a similarity calculation is performed. For example, at least one keyword is compared to each of the generated semantic vectors to determine corresponding similarity scores.</p><p id="p-0042" num="0041">At block <b>244</b>, candidate term identification is performed. For example, a semantic vector having a highest similarity score to the keyword is identified to identify a term of the semantic vector as a candidate term. Optionally, at block <b>246</b>, candidate terms are filtered. For example, the similarity score of the candidate term is compared to a threshold to determine whether or not to discard the candidate term (e.g., the candidate term is discarded if the score is less than or equal to the threshold). At block <b>248</b>, the taxonomy is expanded. For example, one or more candidate terms are added to the taxonomy to generate the expanded taxonomy (e.g., an expanded keyword set). The expanded taxonomy may be used in performing keyword-entity matching, as described with reference to the operations of blocks <b>212</b>-<b>222</b>.</p><p id="p-0043" num="0042">In some implementations of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, system <b>100</b> provides a hybrid-automated system that leverages human insight at the outset with expert (a) defined entities; (b) defined data sources; and (c) defined keyword taxonomy. For example, by leveraging (a-c), system <b>100</b> (e.g., server <b>110</b>) may return extractions based on bidirectional entity keyword surface distances for expert evaluation. For example in example sentences (1) below, the risk term &#x201c;pipe bomb&#x201d; is a risk associated with the entity Time Warner.<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0000">    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0043">(1)(a) Later Wednesday, CNN received <u style="single">a pipe bomb</u> at its Time Warner Center headquarters at Manhattan sent to ex-CIA director John Brennan and a <u style="single">suspicious package</u> sent to Rep. Maxine Waters . . . .</li>        <li id="ul0002-0002" num="0044">(1)(b) On Monday, a pipe bomb was found in a mailbox at the Katonah home of billionaire business magnet and political activist George Soros.</li>    </ul>    </li></ul></p><p id="p-0044" num="0045">Referring to sentences (1)(a) and (1)(b), system <b>100</b> may return both the single sentence (1)(a) and multi-sentences (1)(a-b) without the need for a separate risk classification engine or more complex NLP. System <b>100</b> may also increase taxonomy coverage by encoding word vectors trained on the same sources of data (suspicious package in (1)(a)). Accordingly, server <b>110</b> may return high value information to analysts collaborating with server <b>100</b> It is noted that the operations and functionality provided by server <b>110</b> combines the efficiencies that tuned Machine Learning (ML) systems can offer with the rich depth of experience and insight that analysts bring to the process.</p><p id="p-0045" num="0046">Thus, system <b>100</b> (e.g., server <b>110</b>) and its corresponding operations and functions, provides the ability to extract events (e.g., risks) at scale, improve the quality of the extractions over time, grow the keyword taxonomy to increase coverage, and do so with minimum manual effort. System <b>100</b> also includes a scalable system for finding entity-event pairings which, as compared to other systems, better incorporate human insights in the initial system configuration to obviate the need for a risk classification engine. For example, system <b>100</b> provides a hybrid human-automated system that provides scalable event-monitoring capabilities. Further, without relying on multiple complex NLP sub-systems to make connections between entities and event keywords, system <b>100</b> (e.g., server <b>110</b>) may derive maximum benefit from surface information. Because of the specificity of the initial taxonomy, extracts are assumed to express some degree of entity-risk relationships and are based on &#x201c;shallow&#x201d; surface parsing rather than deeper morpho-syntactic parsing. Additionally, the degree to which these two deviations hold is the subject of the evaluation&#x2014;e.g., comparing to a distance threshold between keyword and entity. To illustrate, system <b>100</b> may utilize the techniques described herein, such as filtering and/or thresholding, to address high recall associated with the predefinition of the seed taxonomy. Additionally, or alternatively, system <b>100</b> may address high system recall relative to maintaining flexibility for analyst users and dynamic definition of the risk problem space&#x2014;this may include summarization of results for better presentation, alternative source data at the direction of the analyst for given risk categories, and token distance thresholding.</p><p id="p-0046" num="0047">A system of the present disclosure is a custom NLP processing pipeline capable of the ingesting and analyzing hundreds of thousands of text documents. The system includes at least four components:<ul id="ul0003" list-style="none">    <li id="ul0003-0001" num="0000">    <ul id="ul0004" list-style="none">        <li id="ul0004-0001" num="0048">1. Document Ingest and Processing: Raw text documents are read and tokenization, lemmatization, and sentencization are performed.</li>        <li id="ul0004-0002" num="0049">2. Keyword/Entity Detection: Instances of both keywords and entities are identified in the processed text, and each risk keyword occurrence is matched to the nearest entity token.</li>        <li id="ul0004-0003" num="0050">3. Match Filtering and Sentence Retrieval: Matches within the documents are filtered and categorized by pair distance and/or sentence co-occurrence, and the filtered sentences are retrieved for context.</li>        <li id="ul0004-0004" num="0051">4. Semantic Encoding and Taxonomy Expansion: A semantic vectorization algorithm is trained on domain-specific text and used to perform automated expansion of the keyword taxonomy.</li>    </ul>    </li></ul></p><p id="p-0047" num="0052">This design architecture allows for significant customization, high throughput, and modularity for uses in experimental evaluation and deployment in production use-cases. The system may support decentralized or streaming architectures, with each document being processed independently and learning systems (specifically at the semantic encoding/expansion steps) configured for continuous learning or batch model training.</p><p id="p-0048" num="0053">One or more known systems can be used for document ingest and low level NLP, such as spaCy, as a non-limiting example. For example, a default NLP pipeline may run tokenizer&#x2192;part-of-speech tagger&#x2192;dependency parser&#x2192;named entity recognizer.</p><p id="p-0049" num="0054">Sentence breaks found by the dependency parser may be used to annotate each of the found keyword-entity pairs as being either in the same or different sentences. A dependency-based sentencizer is preferred to a simpler stop-character based approach due to the unpredictable formatting of certain domains of text, e.g., web-mined news and regulatory filings.</p><p id="p-0050" num="0055">The system may allow for a text generator object to be provided, and may take advantage of multi-core processing to parallelize batching. In such an implementation, each processed document piped in by the system is converted to its lemmatized form with sentence breaks noted so that sentence and multi-sentence identification of keyword/entity distances can be captured.</p><p id="p-0051" num="0056">In the absence of intervening information or a more sophisticated approach to parsing, the mention of an entity and risk keyword in a phrase or sentence is the most coherent semantically and pragmatically (as well as morpho-syntactically). For example, example sentence (2) below describes the entity Verizon and its litigation risk associated with lawsuit settlement (keywords being <u style="single">settle</u> and <u style="single">lawsuit</u>).<ul id="ul0005" list-style="none">    <li id="ul0005-0001" num="0000">    <ul id="ul0006" list-style="none">        <li id="ul0006-0001" num="0057">(2) In 2011, Verizon agreed to pay $20 million to <u style="single">settle</u> a class-action <u style="single">lawsuit</u> by the federal Equal Employment Opportunity Commission alleging that the company violated the Americans with Disabilities Act by denying reasonable accommodations for hundreds of employees with disabilities.</li>    </ul>    </li></ul></p><p id="p-0052" num="0058">Returning the entire sentence yields additional information&#x2014;the lawsuit is class-action and the allegation in the complaint is that Verizon &#x201c;denied reasonable accommodations for hundreds of employees with disabilities.&#x201d; The detection process performed by the system begins by testing for matches of each keyword with each entity, for every possible keyword-entity pairing in the document. Algorithm 1 provides the simplified pseudocode for this process.</p><p id="p-0053" num="0000"><tables id="TABLE-US-00001" num="00001"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="center"/><thead><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>Algorithm 1 Entity-Keyword Pairing</entry></row><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="left"/><tbody valign="top"><row><entry>Require: taxonomy and entities lists</entry></row><row><entry>&#x2003;for keyword in taxonomy do</entry></row><row><entry>&#x2003;&#x2003;for entity in entities do</entry></row><row><entry>&#x2003;&#x2003;&#x2003;keywordLocs =&#x2212;findLocs(keyword)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;entityLocs = findLocs(entity)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;for kLoc in keywordLocs do</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;bestHit = findClosestPair(kLoc, entityLocs)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;results.append((key word, entity, bestHit))</entry></row><row><entry>&#x2003;&#x2003;&#x2003;end for</entry></row><row><entry>&#x2003;&#x2003;end for</entry></row><row><entry>&#x2003;end for</entry></row><row><entry>&#x2003;return findClosestPair is two token indicies</entry></row><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0054" num="0059">In some implementations, for every instance of every keyword, the nearest instance of every available entity is paired&#x2014;regardless of whether it precedes or proceeds the keyword. Furthermore, an entity may be found to have multiple risk terms associated with it, but each instance of a risk term will only apply itself to the closest entity. This helps prevent overreaching conclusions of risk while allowing the system to remain flexible. For example, example sentence (3) extends the extract of example sentence (2) to the prior contiguous sentence which contains settlement. This extension provides greater context for Verizon's lawsuit. Example sentence (3) is actually background for a larger proposition being made in the document that Verizon is in violation of settlement terms from a previous lawsuit.<ul id="ul0007" list-style="none">    <li id="ul0007-0001" num="0000">    <ul id="ul0008" list-style="none">        <li id="ul0008-0001" num="0060">(3) McDonald says this treatment violated the terms of a <u style="single">settlement</u> the company reached a few years earlier regarding its treatment of employees with disabilities. In 2011, Verizon agreed to pay $20 million to settle a class-action lawsuit by the federal Equal Employment Opportunity Commission . . . .</li>    </ul>    </li></ul></p><p id="p-0055" num="0061">The &#x201c;shallow&#x201d; parsing approach of the system promotes efficiency and is preferable to more complex NLP, e.g., chunking or co-reference resolution. Nonetheless, this flexibility comes at a computational cost: a total of (m&#xb7;a)&#xd7;(n&#xb7;b) comparisons must be made for each document, where m is the number of keyword terms across all taxonomic categories, a the average number of instances of each keyword per document, n the number of entities provided, and b the average number of entity instances per document. Changing any single one of these variables will result in computational load changing with O(n) complexity, but their cumulative effects can quickly add up. For parallelization purposes, each keyword is independent of each other keyword and each entity is independent of each other entity. This means that in an infinitely parallel (theoretical) computational scheme, the system runs on O(a&#xd7;b), which will vary as a function of the risk and text domains.</p><p id="p-0056" num="0062">After keyword-entity pairing, the system has completed a substantial part of document processing and risk identification. The next component seeks to (a) filter away results unlikely to hold analytic value; and (b) identify the hits as being either single sentence or multi-sentence using the sentence information. The first of these goals may be achieved with a simple hit distance cutoff where any keyword-entity pair with more than a particular count of intervening tokens is discarded. A particular setting of a hard cutoff (e.g., a particular amount) improves keyword-entity spans by not including cross-document matches for large documents. Once filtering is complete, the system uses document sentence breaks to determine the membership of each keyword and entity for each pairing and, ultimately, whether they belong to the same sentence.</p><p id="p-0057" num="0063">The system may automate term expansion by using similarity calculations of semantic vectors. These vectors are generated by training a skipgram model, which relies on words and subwords from the same data source as the initial extractions. This ensures that domain usage of language is well-represented, and any rich domain-specific text may be used to train semantic vectors.</p><p id="p-0058" num="0064">For each taxonomic risk term encountered, the model vocabulary for the minimized normalized dot product r&#xb7;w/&#x2225;r&#x2225;w&#x2225; (e.g., a basic similarity score) is searched, and the system returns the top-scoring vocabulary terms as candidates for taxonomic expansion.</p><heading id="h-0007" level="1">EXAMPLE</heading><p id="p-0059" num="0065"><figref idref="DRAWINGS">FIG. <b>3</b></figref> shows an example system <b>300</b> in accordance with the present disclosure. The system <b>300</b> includes a document ingest <b>302</b>, a semantic expansion <b>304</b>, a keyword taxonomy <b>306</b>, an expanded keyword taxonomy <b>308</b>, a tokenizer and sentencizer <b>310</b>, an entity detector <b>312</b>, an entity list <b>314</b>, a keyword location detector <b>316</b>, keyword sentences <b>318</b>, a keyword-based selector <b>320</b>, a keyword entity pairing <b>322</b>, a filter <b>324</b>, a same sentence match pool <b>326</b>, a cross-sentence match pool <b>328</b>, a comparator <b>330</b>, and method rankings <b>332</b>.</p><p id="p-0060" num="0066">Document ingest <b>302</b> includes a corpus of documents (e.g., one or more documents) that are ingested into system <b>300</b> (e.g., via a document ingestor). The document corpus is provided as a semantic model training corpus to semantic expansion <b>304</b>. Semantic expansion <b>304</b> is a domain-specific semantic expander configured to expand a taxonomy based on the semantic model training corpus. For example, semantic expansion <b>304</b> may receive keyword taxonomy <b>306</b> as an input of seed terms. Keyword taxonomy <b>306</b> may be generated by one or more users who enter keywords and related categories. Semantic expansion <b>304</b> may expand the seed terms (e.g., the keyword taxonomy <b>306</b>) based on the input documents to generate the expanded keyword taxonomy <b>308</b>. For example, semantic expansion <b>304</b> may generate one or more semantic vectors and identify semantic vectors (corresponding to new keywords) for inclusion in second keyword taxonomy <b>308</b> based on similarity scores of the semantic vectors. The expanded keyword taxonomy <b>308</b> may include more detailed keywords in various categories, as well as common misspellings of keywords, such that words intended as keywords may be identified by system <b>300</b>. The expanded keyword taxonomy <b>308</b> may be combined with the keyword taxonomy <b>306</b> to generate a taxonomy used by system <b>300</b>. Additionally, or alternatively, the expanded keyword taxonomy <b>308</b> and the keyword taxonomy <b>306</b> may be separately used to provide for an ability to rate each taxonomy individually.</p><p id="p-0061" num="0067">In addition to being provided to semantic expansion <b>304</b>, the ingested documents (e.g., from document ingest <b>302</b>) are provided to tokenizer and sentencizer <b>310</b>. Tokenizer and sentencizer <b>310</b> is configured to detect one or more tokens from the input documents, and to detect one or more sentences from the input documents. For example, tokenizer and sentencizer <b>310</b> may break an input document into words (or words, names, and/or phrases) that are represented by tokens. Additionally, tokenizer and sentencizer <b>310</b> may detect sentence breaks in the documents using a dependency parser.</p><p id="p-0062" num="0068">Tokens generated by tokenizer and sentencizer <b>310</b> may be provided to entity detector <b>312</b>. Entity detector <b>312</b> is configured to receive the tokens and entity list <b>314</b> and to determine locations of the entities within the tokens. For example, the entities may be a list of companies or corporations, and entity detector <b>312</b> may determine locations (within a list of tokens, or in a sentence) of one or more entities. As an example, entity detector <b>312</b> may detect a token that corresponds to Verizon and may identify the location of the token. Locations of the entities may also be referred to as entity hit locations. The entity hit locations may be provided from entity detector <b>312</b> to keyword-entity pairing <b>322</b>.</p><p id="p-0063" num="0069">Tokens generated by tokenizer and sentencizer <b>310</b> may also be provided to keyword location detector <b>316</b>. Keyword location detector <b>316</b> may also receive one or more taxonomies and may identify keyword hit locations and keyword hit text (e.g., which keywords were identified) from the list of tokens. For example, a keyword in the taxonomy may be &#x201c;lawsuit&#x201d;, and keyword location detector <b>316</b> may output a location (e.g., in the list of tokens, or in a sentence) and the text &#x201c;lawsuit&#x201d;. The keyword hit locations may be provided to keyword-entity pairing <b>322</b>, and the keyword hit text may be pooled together as keyword sentences <b>318</b>.</p><p id="p-0064" num="0070">The one or more taxonomies provided to keyword location detector <b>316</b> may also be provided to keyword-based selector <b>320</b>. Keyword-based selector <b>320</b> may be configured to select each keyword for determining whether there is a keyword-entity match in one of a plurality of input data sources, as further described herein.</p><p id="p-0065" num="0071">Keyword-entity pairing <b>322</b> is configured to receive entity hit location from entity detector <b>312</b>, sentence breaks from tokenizer and sentencizer <b>310</b>, and keyword hit locations from keyword location detector <b>316</b>. Keyword-entity pairing <b>322</b> is configured to determine, for each keyword, the nearest entity to the keyword (and whether it is in the same sentence or not). Keyword-entity pairing <b>322</b> may be configured to determine bi-directional pairings, e.g., the entity that is closest to the keyword, whether the entity is before or after the keyword. For example, if the keyword &#x201c;lawsuit&#x201d; is detected, keyword-entity pairing <b>322</b> may pair the keyword with a first entity that is four tokens before the keyword instead of a second entity that is <b>20</b> tokens after the keyword, even if the first entity is in a different sentence. In some implementations, keyword-entity pairing <b>322</b> is configured to operate in parallel such that multiple keywords may be paired with entities concurrently. After pairing keywords and entities, the pairings are provided to filter <b>324</b>.</p><p id="p-0066" num="0072">Filter <b>324</b> is configured to filter out keyword-entity pairs that are more than a threshold number of tokens apart. For example, filter <b>324</b> may compare the distance between the keyword and the entity of a keyword-entity pair to a threshold, and if the distance is greater than or equal to the threshold, the keyword-entity pair is discarded. In some implementations, the threshold may be 100 tokens. Additionally, filter <b>324</b> may assign a score to each filtered keyword-entity pair based on the distance. For example, a first keyword-entity pair with a lower distance may have a higher score than a second keyword-entity pair with a higher distance. The keyword-entity pairs may be mapped to the sentences that contain them, and the sentences may be assigned to pools based on the number of sentences. For example, if the keyword and the entity are both in the same sentence, the sentence is assigned to same sentence match pool <b>326</b>. Alternatively, if the entity is in a different sentence than the keyword, the sentences are assigned to cross-sentence match pool <b>328</b>.</p><p id="p-0067" num="0073">Keyword sentences <b>318</b>, same sentence match pool <b>318</b>, and cross-sentence match pool <b>328</b> are used as inputs, based on a keyword selected by keyword-based selector <b>320</b>, to output results for comparison. The comparison may be between sentence(s) from same sentence match pool <b>326</b> or cross sentence match pool <b>328</b> and a random baseline sentence selected from keyword sentences <b>318</b>. Comparator <b>330</b> may perform a pairwise comparison between the selected sentence(s) and the baseline sentence based on scores of the sentences, user input, or a combination thereof. For example, the user input may indicate a rating of each sentence by a user, such as a subject matter expert. The comparisons result in rankings, which are stored as method rankings <b>332</b>.</p><p id="p-0068" num="0074">To test the performance of system <b>300</b> (and/or system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>), an experiment comparing the performances of systems using single-sentence risk detection and systems only using multi-sentence risks was designed. This experiment measured the performance of purely multi-sentence hits against purely single-sentence hits. In addition, a baseline system was tested that detected only risk terms without searching for corresponding entities. Taken together, the three hypotheses tested are as follows:</p><p id="p-0069" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>H</i><sub>1</sub><i>: p</i><sub>multi</sub><i>&#x3e;p</i><sub>base</sub><i>, H</i><sub>&#xd8;</sub><i>: p</i><sub>multi</sub><i>=p</i><sub>base </sub><?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0070" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>H</i><sub>2</sub><i>: p</i><sub>single</sub><i>&#x3e;p</i><sub>base</sub><i>, H</i><sub>&#xd8;</sub><i>: p</i><sub>single</sub><i>=p</i><sub>base </sub><?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0071" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>H</i><sub>3</sub><i>: p</i><sub>multi</sub><i>&#x3e;p</i><sub>single</sub><i>, H</i><sub>&#xd8;</sub><i>:p</i><sub>multi</sub><i>=p</i><sub>single </sub><?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0072" num="0075">H<sub>1 </sub>and H<sub>2 </sub>test whether each method of detecting risk co-occurrence with an entity performs better than random chance at selecting a risk term in the document and assessing it to have been associated with the entity. H<sub>3 </sub>tests whether the distance-based measure corresponding to the system outperforms a sentential approach.</p><p id="p-0073" num="0076">A virtualized Ubuntu 14.04 machine with 8 vC-PUs&#x2014;running on 2.30 GHz Intel Xeon E5-2670 processors and 64 GB RAM was chosen to support the first experiment.</p><p id="p-0074" num="0077">The names of the top Fortune 100 companies from 2017 were fed as input into a series of news requests from Thomson Reuters' CLEAR System to System platform for the most recent <b>1000</b> articles mentioning each company. Ignoring low coverage and bodiless news articles, 99,424 individual documents were returned. Each article was then fed into the system and risk detections were found with a distance cutoff of <b>100</b> tokens. For each identified risk, whether single or multi-sentence, the system also selected a baseline sentence at random from the corresponding document for pairwise comparison.</p><p id="p-0075" num="0000"><tables id="TABLE-US-00002" num="00002"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="center"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE 1</entry></row></thead><tbody valign="top"><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>Sample risk terms from the seed and expanded sets.</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="1" colwidth="49pt" align="center"/><colspec colname="2" colwidth="84pt" align="center"/><colspec colname="3" colwidth="84pt" align="center"/><tbody valign="top"><row><entry>Risk Category</entry><entry>Keyword Seed Taxonomy</entry><entry>Enriched Keyword Taxonomy</entry></row><row><entry namest="1" nameend="3" align="center" rowsep="1"/></row><row><entry>Cybersecurity</entry><entry>n = 26</entry><entry>n = 123</entry></row><row><entry/><entry>cybercrime, hack, DDOS,</entry><entry>4front security, cyber</entry></row><row><entry/><entry>antivirus, data breach,</entry><entry>deterrence, cloakware,</entry></row><row><entry/><entry>ransomware, penetration, . . .</entry><entry>unauthenticated, . . .</entry></row><row><entry>Terrorism</entry><entry>n = 37</entry><entry>n = 147</entry></row><row><entry/><entry>terrorism, bio-terrorism,</entry><entry>anti-terrorism, bomb maker,</entry></row><row><entry/><entry>extremist, car bombing,</entry><entry>explosives, hezbollah, jihadi,</entry></row><row><entry/><entry>hijack, guerrilla, . . .</entry><entry>nationalist, . . .</entry></row><row><entry>Legal</entry><entry>n = 38</entry><entry>n = 162</entry></row><row><entry/><entry>litigation, indictment,</entry><entry>appropriation, concealment,</entry></row><row><entry/><entry>allegation, failure to comply,</entry><entry>counter suit, debtor,</entry></row><row><entry/><entry>sanctions violations, . . .</entry><entry>expropriation, issuer, . . .</entry></row><row><entry namest="1" nameend="3" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0076" num="0078">A dependency parse was the largest bottleneck with an expected total runtime for the near 100,000 documents at approximately 7 calendar-days of computation. In the interest of runtime, only the first 21,000 documents read in order of machine-generated news article ID were analyzed. Once all selected documents were processed, single and multi-sentence spans relating to the same risk category were paired, but potentially different entities and documents, for pairwise evaluations.</p><p id="p-0077" num="0079">As summarized in Table 1, starting with manually-created seed terms in each category of risk, encodings were learned from a concatenation of the news article text. Selecting the top ten most similar terms for each in-vocabulary seed term resulted in an expanded taxonomy with a 326.31% increase on average across the three categories. This term expansion not only introduced new vocabulary to the taxonomy, but also variants and common misspellings of keywords, which are important in catching risk terms &#x201c;in the wild&#x201d;. Some cleanup of the term expansion was required to filter out punctuation and tokenization variants.</p><p id="p-0078" num="0080">Analysts were asked to give their preference for &#x201c;System A&#x201d; or &#x201c;System B&#x201d; or &#x201c;Neither&#x201d; when presented with randomized pairs of output. Percentage preferences for the overarching system and each of six pairings was tested for significance with Pearson's &#x3c7;<sup>2 </sup>using raw counts.</p><p id="p-0079" num="0081">4514 judgments were collected from eight subject matter experts to compute system preferences associated with single, multi- and baseline sentence extractions. Roughly 28% of all evaluated extractions (1266/4514) received a preference judgment (32% from the seed set (698/2198) and 24% from the expansion set (568/2316)) where <b>72</b>% received a &#x201c;Neither&#x201d; rating.</p><p id="p-0080" num="0000"><tables id="TABLE-US-00003" num="00003"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="center"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE 2</entry></row></thead><tbody valign="top"><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>Pearson&#x2019;s x<sup>2 </sup>andp values (d.f. = 1)</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="offset" colwidth="21pt" align="left"/><colspec colname="1" colwidth="91pt" align="center"/><colspec colname="2" colwidth="105pt" align="center"/><tbody valign="top"><row><entry/><entry>System</entry><entry>x<sup>2</sup></entry></row><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row><row><entry/><entry>Overall</entry><entry>&#x2002;8.530</entry></row><row><entry/><entry>single v. multi (seed v. expand)</entry><entry>p = 0.003</entry></row><row><entry/><entry>CEREALsbs</entry><entry>28.088</entry></row><row><entry/><entry>single v. baseline (seed)</entry><entry>p = l.159e&#x2212;07</entry></row><row><entry/><entry>CEREALmbs</entry><entry>25.358</entry></row><row><entry/><entry>multi v. baseline (seed)</entry><entry>p = 4.762e&#x2212;07</entry></row><row><entry/><entry>CEREALsms</entry><entry>37.763</entry></row><row><entry/><entry>single v. multi (seed)</entry><entry>p = 7.99e&#x2212;10</entry></row><row><entry/><entry>CEREALsbe</entry><entry>&#x2002;6.858</entry></row><row><entry/><entry>single v. baseline (expand)</entry><entry>p = 0.008</entry></row><row><entry/><entry>CEREALmbe</entry><entry>25.705</entry></row><row><entry/><entry>multi v. baseline (expand)</entry><entry>p = 3.978e&#x2212;07</entry></row><row><entry/><entry>CEREALsme</entry><entry>&#x2002;6.316</entry></row><row><entry/><entry>single v. multi (expand)</entry><entry>p = 0.011</entry></row><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0081" num="0082">As summarized in Table 2 (where &#x201c;CEREAL&#x201d; corresponds to the system) and by graph <b>400</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>, all single and multi-sentence extractions across the seed and expansion sets outperform the baseline by statistically significant margins. For the seed set, the single sentence extractions outperform the multi-sentence extractions by a statistically significant margin as well (p&#x2264;0.01). However, for the expansion set, the multi-sentence extractions gain significant ground (26% to 38% increase in preference).</p><p id="p-0082" num="0083">1283 (28%) of evaluations were doubly annotated for calculation of Cohen's Kappa. Average K for the seed set was 0.284 and 0.144 for the expansion set (which suffers from low sample size). This is uniformly low across all categories, but not an unusual result given the task and the range of analyst expertise.</p><p id="p-0083" num="0084">The system's distance metric provides benefit well above the baseline, so H<sub>1 </sub>and H<sub>2 </sub>can be accepted. However, while the high recall is not surprising, i.e., there is no classification engine and all keywords in the taxonomy are assumed to capture risk in some context, 72% of unwanted results need to be effectively managed to not nullify any analytical benefits from the preferred results. Adjusting token distance thresholds is one potential way forward that requires further analysis&#x2014;although this may be subject to document or genre effects. The current distribution of distances under the <b>100</b> threshold are fairly uniform.</p><p id="p-0084" num="0085">H<sub>3</sub>, which tests the assumption that multi-sentence returns will have greater analytical utility, is not accepted. However, the data indicates that as the taxonomy expands, the preference for the multi-sentences increases by 46% over the single sentence seed set extractions. In the evaluation set, there were 18 keywords from the expanded taxonomy&#x2014;action, assertion, attack, authentication, claim, complaint, compliance, conduct, foreclosure, harassment, identity, military, professionalism, require, requirement, security, separation, and suit. The expanded keywords exhibit a greater range of specificity relative to a risk category than the seed terms. The more specific a risk term is, the less likely it is to appear in ambiguous contexts&#x2014;arguably why the expert analyst constructs the seed list as they do. Absent additional information, the more general the risk term, the higher likelihood of ambiguity. It seems here that additional study is potentially required to understand the nature of the risk of the expanded keywords because of their general nature. Based on the proliferation of more general terms in the expanded set, perhaps cull high recall, the thresholding should consider the degree of semantic granularity as well.</p><p id="p-0085" num="0086">Thus, the system described herein includes a scalable system for finding entity-event pairings which, as compared to other systems, better incorporates human insights in the initial system configuration to obviate the need for a risk classification engine. Further, without relying on multiple complex NLP sub-systems to make connections between entities and event keywords, the system may derive maximum benefit from surface information.</p><p id="p-0086" num="0087">While applied here to the risk mining space, in other implementations, the system could be used for any data, entities and taxonomies to support generalized event monitoring. Additionally, the system may address high system recall relative to maintaining flexibility for analyst users and dynamic definition of the risk problem space&#x2014;this may include summarization of results for better presentation, alternative source data at the direction of the analyst for given risk categories, and token distance thresholding.</p><heading id="h-0008" level="1">END EXAMPLE</heading><p id="p-0087" num="0088">Referring to <figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flow diagram of a method <b>500</b> of identifying an event in data. In some implementations, the method <b>400</b> may be performed by system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> (e.g.,), one or more components to execute operations of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, or system <b>300</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0088" num="0089">Method <b>500</b> includes receiving data at a receiver from a data source, the data comprising one or more documents each comprising text, at block <b>502</b>. For example, data ingestor <b>120</b> may receive documents (e.g., the data) from data sources <b>170</b>..</p><p id="p-0089" num="0090">Method <b>500</b> includes performing natural language processing on the received data to generate processed data, the processed data indicating one or more sentences, at block <b>504</b>. For example, data ingestor <b>120</b> may perform natural language processing on the received documents to generate processed data that indicates one or more sentences.</p><p id="p-0090" num="0091">Method <b>500</b> also includes generating, based on the data and a first keyword set, a second keyword set having a greater number of keywords than the first keyword set, at block <b>506</b>. For example, taxonomy expander <b>124</b> may generate a second keyword set having a greater number of keywords than a first keyword set (e.g., a seed set)..</p><p id="p-0091" num="0092">Method <b>500</b> further includes apply each keyword set of the first keyword set and the second keyword set to the processed data, at block <b>508</b>. Applying each keyword set may include one or more blocks, such as blocks <b>510</b>, <b>512</b>, <b>514</b>, and <b>516</b>, as described further herein.</p><p id="p-0092" num="0093">Method <b>500</b> further includes detecting one or more keywords and one or more entities included in the processed data based on the keyword set and an entity set, at block <b>510</b>. For example, term detector <b>121</b> may detect one or more keywords and one or more entities included in the processed data based on the keyword set and an entity set.</p><p id="p-0093" num="0094">Method <b>500</b> further includes determining one or more matched pairs based on the detected one or more keywords and the detected one or more entities, at block <b>512</b>. For example, output generator <b>122</b> may determine one or more matched pairs based on the detected one or more keywords and the detected one or more entities.</p><p id="p-0094" num="0095">Method <b>500</b> further includes extracting a sentence from a document based on the one or more sentences indicated by the processed data, where the sentence corresponds to at least one matched pair of the one or more matched pairs and comprises a single sentence or multiple sentences, at block <b>514</b>. For example, output generator <b>122</b> may extract a sentence from a document based on the one or more sentences indicated by the processed data, such that the sentence corresponds to at least one matched pair. The sentence may be a single sentence or multiple sentences (e.g., if a matched pair crosses two or more sentences).</p><p id="p-0095" num="0096">Method <b>500</b> further includes outputting the extracted sentence, at block <b>516</b>. For example, output generator <b>122</b> may output the extracted sentence, for example, for display to a user. Additionally, or alternatively, the extracted sentence may be stored at a memory.</p><p id="p-0096" num="0097">In some implementations, performing the natural language processing includes performing, on the data, tokenization, lemmatization, sentencization, or a combination thereof. For example, data ingestor <b>102</b> may perform tokenization, lemmatization, sentencization, or a combination thereof, on data from data sources <b>170</b>. Additionally, or alternatively, method <b>500</b> may further comprise, for each matched pair based on the first keyword set and each matched pair based on the second keyword set, determining whether the keyword and the entity of the matched pair are included in the same sentence. Additionally, extracting the sentence from the document includes extracting a single sentence based on a determination that the keyword and the entity are included in the same sentence and extracting multiple sentences based on a determination that the keyword and the entity are not included in the same sentence. For example, if the keyword and the entity of a matched pair are in the same sentence, output generator <b>122</b> may extract a single sentence. Alternatively, if the keyword and the entity of a matched pair are in different sentences, output generator <b>122</b> may extract multiple sentences.</p><p id="p-0097" num="0098">In some implementations, method <b>500</b> further includes initiating a pipe( ) operation on the data to perform the natural language processing. Performing the natural language processing further includes using a dependency based sentencizer to generate the processed data indicating one or more sentences, converting the data to a lemmatized format, using a tokenizer to generate one or more tokens, using a part-of-speech tagger to generate part-of-speech data, using a named entity recognizer to identify one or more entities, or a combination thereof. In some such implementations, the processed data is in a format that is compatible with Python. For example, data ingestor <b>120</b> may include a dependency based sentencizer, a lemmatizer, a tokenizer, a part-of-speech tagger, a named entity recognizer, or a combination thereof</p><p id="p-0098" num="0099">In some implementations, generating the second keyword set includes generating one or more semantic vectors. Generating the second keyword set also includes, for each keyword of the first keyword set: determining a semantic vector having a highest similarity score to the keyword and identifying one or more terms of the determined semantic vector as a candidate term. Generating the second keyword set also includes selecting at least one candidate term to be added to the first keyword set to generate the second keyword set. For example, taxonomy expander <b>124</b> may generate semantic vectors and identify terms of semantic vectors as candidate terms based on similarity scores. In some such implementations, generating the one or more semantic vectors includes, for each document of one or more documents corresponding to the data, generating a corresponding semantic vector based on a skipgram model that utilizes words and subwords from the document. For example, a skipgram generator, such as Fasttext, may be used to generate the semantic vectors. Generating the second keyword set further includes, for each keyword of the first keyword set, comparing a similarity score of the determined semantic vector having a highest similarity score to a threshold. The semantic vector is used to identify the candidate term based on a determination that the similarity score of the determined semantic vector is greater than or equal to the threshold.</p><p id="p-0099" num="0100">In some implementations, the extracted sentence is output to an electronic device associated with an analyst, and method <b>500</b> further includes receiving an input from the analyst responsive to the extracted sentence, storing an indication of the input, and sending a notification corresponding to the extracted sentence, the input, or both. The notification includes a link to a data source, a text extraction from a document, the matched pair corresponding to the extracted sentence, the input, an identifier of the analyst, or a combination thereof.</p><p id="p-0100" num="0101">In some implementations, method <b>500</b> further includes receiving a selection of a first event category of multiple event categories and retrieving the first keyword set based on the selection of the first event category. For example, different keyword sets may correspond to different event categories. To illustrate, one keyword set may correspond to &#x201c;terrorism&#x201d; and another keyword set may correspond to &#x201c;legal.&#x201d; In some such implementations, the multiple event categories include cybersecurity, terrorism, legal/non-compliance, or a combination thereof. In some such implementations, method <b>500</b> further includes receiving a selection of a second event category of the multiple event categories, retrieving a third keyword set based on the selection of the second event category, generating, based on the third keyword set, a fourth keyword set having a greater number of keywords than the third keyword set, and, for each keyword set of the third keyword set and each keyword in the fourth keyword set: detecting one or more keywords and one or more entities included in the processed data based on the keyword set and the entity set and determining one or more matched pairs based on the detected one or more keywords and the detected one or more entities. For example, different taxonomies corresponding to different entity lists may be generated and used to determine keyword-entity pairs.</p><p id="p-0101" num="0102">In some implementations, the sentence includes the multiple sentences, and the multiple sentences include a sentence that includes the at least one matched pair, a sentence that includes the keyword of the at least one matched pair, a sentence preceding the sentence that includes the keyword of the at least one matched pair, a sentence following the sentence with the keyword the at least one matched pair, a sentence that includes the entity of the at least one matched pair, a sentence preceding the sentence that includes the entity of the at least one matched pair, a sentence following the sentence with the entity of the at least one matched pair, or a combination thereof. Additionally, or alternatively, the data source includes a streaming data source, news data, a database, or a combination thereof, and the entity set indicates an individual, a company, a government, an organization, or a combination thereof</p><p id="p-0102" num="0103">Those of skill would further appreciate that the various illustrative logical blocks, modules, circuits, and algorithm steps described in connection with the disclosure herein may be implemented as electronic hardware, computer software, or combinations of both. To clearly illustrate this interchangeability of hardware and software, various illustrative components, blocks, modules, circuits, and steps have been described above generally in terms of their functionality. Whether such functionality is implemented as hardware or software depends upon the particular application and design constraints imposed on the overall system. Skilled artisans may implement the described functionality in varying ways for each particular application, but such implementation decisions should not be interpreted as causing a departure from the scope of the present disclosure. Skilled artisans will also readily recognize that the order or combination of components, methods, or interactions that are described herein are merely examples and that the components, methods, or interactions of the various aspects of the present disclosure may be combined or performed in ways other than those illustrated and described herein.</p><p id="p-0103" num="0104">Functional blocks and modules in <figref idref="DRAWINGS">FIGS. <b>1</b>-<b>5</b></figref> may comprise processors, electronics devices, hardware devices, electronics components, logical circuits, memories, software codes, firmware codes, etc., or any combination thereof. Consistent with the foregoing, various illustrative logical blocks, modules, and circuits described in connection with the disclosure herein may be implemented or performed with a general-purpose processor, a digital signal processor (DSP), an application specific integrated circuit (ASIC), a field programmable gate array (FPGA) or other programmable logic device, discrete gate or transistor logic, discrete hardware components, or any combination thereof designed to perform the functions described herein. A general-purpose processor may be a microprocessor, but in the alternative, the processor may be any conventional processor, controller, microcontroller, or state machine. A processor may also be implemented as a combination of computing devices, e.g., a combination of a DSP and a microprocessor, a plurality of microprocessors, one or more microprocessors in conjunction with a DSP core, or any other such configuration.</p><p id="p-0104" num="0105">The steps of a method or algorithm described in connection with the disclosure herein may be embodied directly in hardware, in a software module executed by a processor, or in a combination of the two. A software module may reside in RAM memory, flash memory, ROM memory, EPROM memory, EEPROM memory, registers, hard disk, a removable disk, a CD-ROM, or any other form of storage medium known in the art. An exemplary storage medium is coupled to the processor such that the processor can read information from, and write information to, the storage medium. In the alternative, the storage medium may be integral to the processor. The processor and the storage medium may reside in an ASIC. The ASIC may reside in a user terminal, base station, a sensor, or any other communication device. In the alternative, the processor and the storage medium may reside as discrete components in a user terminal.</p><p id="p-0105" num="0106">In one or more exemplary designs, the functions described may be implemented in hardware, software, firmware, or any combination thereof. If implemented in software, the functions may be stored on or transmitted over as one or more instructions or code on a computer-readable medium. Computer-readable media includes both computer storage media and communication media including any medium that facilitates transfer of a computer program from one place to another. Computer-readable storage media may be any available media that can be accessed by a general purpose or special purpose computer. By way of example, and not limitation, such computer-readable media can comprise RAM, ROM, EEPROM, CD-ROM or other optical disk storage, magnetic disk storage or other magnetic storage devices, or any other medium that can be used to carry or store desired program code means in the form of instructions or data structures and that can be accessed by a general-purpose or special-purpose computer, or a general-purpose or special-purpose processor. Also, a connection may be properly termed a computer-readable medium. For example, if the software is transmitted from a website, server, or other remote source using a coaxial cable, fiber optic cable, twisted pair, or digital subscriber line (DSL), then the coaxial cable, fiber optic cable, twisted pair, or DSL, are included in the definition of medium. Disk and disc, as used herein, includes compact disc (CD), laser disc, optical disc, digital versatile disc (DVD), floppy disk and blu-ray disc where disks usually reproduce data magnetically, while discs reproduce data optically with lasers. Combinations of the above should also be included within the scope of computer-readable media.</p><p id="p-0106" num="0107">Although the present invention and its advantages have been described in detail, it should be understood that various changes, substitutions and alterations can be made herein without departing from the spirit and scope of the invention as defined by the appended claims. Moreover, the scope of the present application is not intended to be limited to the particular embodiments of the process, machine, manufacture, composition of matter, means, methods, and steps described in the specification. As one of ordinary skill in the art will readily appreciate from the disclosure of the present invention, processes, machines, manufacture, compositions of matter, means, methods, or steps, presently existing or later to be developed that perform substantially the same function or achieve substantially the same result as the corresponding embodiments described herein may be utilized according to the present invention. Accordingly, the appended claims are intended to include within their scope such processes, machines, manufacture, compositions of matter, means, methods, or steps.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method for identifying an event in data, the method comprising:<claim-text>receiving data at a receiver from a data source, the data comprising one or more documents each comprising text;</claim-text><claim-text>performing natural language processing on the received data to generate processed data, the processed data indicating one or more sentences;</claim-text><claim-text>generating, based on the data and a first keyword set, a second keyword set having a greater number of keywords than the first keyword set; and</claim-text><claim-text>for each keyword set of the first keyword set and the second keyword set:<claim-text>detecting one or more keywords and one or more entities included in the processed data based on the keyword set and an entity set;</claim-text><claim-text>determining one or more matched pairs based on the detected one or more keywords and the detected one or more entities, each matched pair of the one or more matched pairs including a keyword from the first keyword set or the second keyword set and an entity from the entity set that are separated in the processed data by a corresponding distance that is less than a threshold;</claim-text><claim-text>extracting at least one sentence from a document based on the one or more sentences indicated by the processed data, where the at least one sentence corresponds to at least one matched pair of the one or more matched pairs and comprises a single sentence or multiple sentences; and</claim-text><claim-text>outputting the extracted at least one sentence.</claim-text></claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein determining the one or more matched pairs comprises, for each matched pair of keyword and entity:<claim-text>determining a distance between the keyword and the entity of the matched pair in the processed data;</claim-text><claim-text>comparing the distance to the threshold; and</claim-text><claim-text>discarding the matched pair based on the distance failing to satisfy the threshold.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein determining the one or more matched pairs further comprises, for each matched pair of keyword and entity:<claim-text>retaining the matched pair based on the distance satisfying the threshold.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein performing the natural language processing comprises performing, on the data, tokenization, lemmatization, sentencization, or a combination thereof.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising, for each matched pair based on the first keyword set and each matched pair based on the second keyword set:<claim-text>determining whether the keyword and the entity of the matched pair are included in the same sentence; and</claim-text><claim-text>wherein extracting the at least one sentence from the document comprises:<claim-text>extracting a single sentence based on a determination that the keyword and the entity are included in the same sentence; and</claim-text><claim-text>extracting multiple sentences based on a determination that the keyword and the entity are not included in the same sentence.</claim-text></claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>initiating a pipe( ) operation on the data to perform the natural language processing; and</claim-text><claim-text>wherein performing natural language processing further comprises:<claim-text>using a dependency based sentencizer to generate the processed data indicating the one or more sentences;</claim-text><claim-text>converting the data to a lemmatized format;</claim-text><claim-text>using a tokenizer to generate one or more tokens;</claim-text><claim-text>using a part-of-speech tagger to generate part-of-speech data;</claim-text><claim-text>using a named entity recognizer to identify one or more entities;</claim-text><claim-text>or a combination thereof; and</claim-text></claim-text><claim-text>wherein the processed data is in a format that is compatible with Python.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein generating the second keyword set comprises:<claim-text>adding keywords of the first keyword set to the second keyword set;</claim-text><claim-text>generating one or more semantic vectors; and</claim-text><claim-text>for each keyword of the first keyword set:<claim-text>determining a semantic vector having a highest similarity score to the keyword;</claim-text><claim-text>identifying one or more terms of the determined semantic vector as candidate terms based on a determination that the similarity score of the determined semantic vector is greater than or equal to a similarity threshold; and</claim-text><claim-text>selecting at least one candidate term to be added to the second keyword set.</claim-text></claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the extracted at least one sentence is output to an electronic device associated with an analyst, and further comprising:<claim-text>receiving an input from the analyst responsive to the extracted at least one sentence;</claim-text><claim-text>storing an indication of the input; and</claim-text><claim-text>sending a notification corresponding to the extracted at least one sentence, the input, or both,</claim-text><claim-text>wherein the notification includes a link to a data source, a text extraction from a document, the at least one matched pair corresponding to the extracted at least one sentence, the input, an identifier of the analyst, or a combination thereof</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. A system comprising:<claim-text>a data ingestor configured to:<claim-text>receive data at a receiver from a data source, the data comprising one or more documents each comprising text; and</claim-text><claim-text>perform natural language processing on the received data to generate processed data, the processed data indicating one or more sentences;</claim-text></claim-text><claim-text>a taxonomy expander configured to:<claim-text>generate, based on the data and a first keyword set, a second keyword set having a greater number of keywords than the first keyword set;</claim-text></claim-text><claim-text>a term detector configured to:<claim-text>detect, for each keyword set of the first keyword set and the second keyword set, one or more keywords and one or more entities included in the processed data based on the keyword set and an entity set; and</claim-text></claim-text><claim-text>an output generator configured to, for each keyword set of the first keyword set and the second keyword set:<claim-text>determine one or more matched pairs based on the detected one or more keywords and the detected one or more entities, each matched pair of the one or more matched pairs including a keyword from the first keyword set or the second keyword set and an entity from the entity set that are separated in the processed data by a corresponding distance that is less than a threshold;</claim-text><claim-text>extract at least one sentence from a document based on the one or more sentences indicated by the processed data, where the at least one sentence corresponds to at least one matched pair of the one or more matched pairs and comprises a single sentence or multiple sentences; and</claim-text><claim-text>output the extracted at least one sentence.</claim-text></claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, further comprising:<claim-text>a database coupled to the data ingestor, the taxonomy expander, the term detector, the output generator, or a combination thereof.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the database is configured to store the first keyword set, the second keyword set, the entity set, the processed data, one or more thresholds, one or more extracted sentences, a plurality of matched pairs, or a combination thereof.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, further comprising:<claim-text>an interface configured to enable communication with the data source, an electronic device, or a combination thereof.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, further comprising:<claim-text>a filter configured to:<claim-text>for each matched pair of keyword and entity:<claim-text>determine a distance between the keyword and the entity of the matched pair in the processed data;</claim-text><claim-text>perform a comparison between the determined distance and the threshold; and</claim-text><claim-text>determine to retain the matched pair or discard the matched pair based on whether or not the comparison indicates the determined distance is greater than or equal to the threshold.</claim-text></claim-text></claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein, to determine the one or more matched pairs, the output generator is further configured to:<claim-text>for each keyword of the detected one or more keywords, identify a corresponding entity of the detected one or more entities that is positioned closest to the corresponding keyword to determine a matched pair for the keyword.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. A computer-based tool including non-transitory computer readable media having stored thereon computer code which, when executed by a processor, causes a computing device to perform operations comprising:<claim-text>receiving data at a receiver from a data source, the data comprising one or more documents each comprising text;</claim-text><claim-text>performing natural language processing on the received data to generate processed data, the processed data indicating one or more sentences;</claim-text><claim-text>generating, based on the data and a first keyword set, a second keyword set having a greater number of keywords than the first keyword set;</claim-text><claim-text>for each keyword of the first keyword set and the second keyword set:<claim-text>detecting one or more keywords and one or more entities included in the processed data based on the keyword set and an entity set;</claim-text><claim-text>determining one or more matched pairs based on the detected one or more keywords and the detected one or more entities, each matched pair of the one or more matched pairs including a keyword from the first keyword set or the second keyword set and an entity from the entity set that are separated in the processed data by a corresponding distance that is less than a threshold;</claim-text><claim-text>extracting at least one sentence from a document based on the one or more sentences indicated by the processed data, where the at least one sentence corresponds to at least one matched pair of the one or more matched pairs and comprises a single sentence or multiple sentences; and</claim-text><claim-text>outputting the extracted at least one sentence.</claim-text></claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The computer-based tool of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the corresponding distance between keywords and entities of the one or more matched pairs is a distance in tokens of the processed data.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The computer-based tool of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the operations further comprise:<claim-text>receiving a selection of a first event category of multiple event categories; and</claim-text><claim-text>retrieving the first keyword set based on the selection of the first event category.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The computer-based tool of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein:<claim-text>the multiple event categories comprise cybersecurity, terrorism, legal/non-compliance, or a combination thereof;</claim-text><claim-text>the data source comprises a streaming data source, news data, a database, or a combination thereof; and</claim-text><claim-text>the entity set indicates an individual, a company, a government, an organization, or a combination thereof</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The computer-based tool of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the operations further comprise:<claim-text>receiving a selection of a second event category of the multiple event categories;</claim-text><claim-text>retrieving a third keyword set based on the selection of the second event category;</claim-text><claim-text>generating, based on the third keyword set, a fourth keyword set having a greater number of keywords than the third keyword set;</claim-text><claim-text>for each keyword set of the third keyword set and the fourth keyword set:<claim-text>detecting one or more keywords and one or more entities included in the processed data based on the keyword set and the entity set; and</claim-text><claim-text>determining one or more matched pairs based on the detected one or more keywords and the detected one or more entities.</claim-text></claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The computer-based tool of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein:<claim-text>the at least one sentence comprises the multiple sentences; and</claim-text><claim-text>the multiple sentences comprise a sentence that includes the at least one matched pair, a sentence that includes the keyword of the at least one matched pair, a sentence preceding the sentence that includes the keyword of the at least one matched pair, a sentence following the sentence with the keyword the at least one matched pair, a sentence that includes the entity of the at least one matched pair, a sentence preceding the sentence that includes the entity of the at least one matched pair, a sentence following the sentence with the entity of the at least one matched pair, or a combination thereof.</claim-text></claim-text></claim></claims></us-patent-application>