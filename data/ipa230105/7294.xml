<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230007295A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230007295</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17945261</doc-number><date>20220915</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>517</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>137</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>176</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>182</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20141101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>517</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20141101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>137</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20141101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>176</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20141101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>182</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">PROCESSING METHOD FOR MOTION COMPENSATION, ENCODER AND DECODER</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>17643522</doc-number><date>20211209</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11490113</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17945261</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>PCT/CN2019/092829</doc-number><date>20190625</date></document-id><parent-status>PENDING</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>17643522</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>GUANGDONG OPPO MOBILE TELECOMMUNICATIONS CORP., LTD.</orgname><address><city>Dongguan</city><country>CN</country></address></addressbook><residence><country>CN</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>HUO</last-name><first-name>Junyan</first-name><address><city>Dongguan</city><country>CN</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>MA</last-name><first-name>Yanzhuo</first-name><address><city>Dongguan</city><country>CN</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>WAN</last-name><first-name>Shuai</first-name><address><city>Dongguan</city><country>CN</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>YANG</last-name><first-name>Fuzheng</first-name><address><city>Dongguan</city><country>CN</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>GUO</last-name><first-name>Ze</first-name><address><city>Dongguan</city><country>CN</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Processing methods for motion compensation, an encoder, and a decoder are provided. The method includes the following. Determine a search position constellation with a point pointed by an initial motion vector as a center point, where the search position constellation includes N directions, search for at least one search point by using at least one type of step lengths in the directions, where the at least one search point at least includes at least one search point in diagonal directions. Search for at least one search position from the search position constellation based on a preset range coverage rule, where the preset range coverage rule is to arrange search points in the directions for search. Obtain a new motion vector according to the at least one search position, and perform motion compensation based on the new motion vector to obtain a prediction value of a coding unit (CU).</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="116.84mm" wi="138.43mm" file="US20230007295A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="127.25mm" wi="140.46mm" file="US20230007295A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="215.98mm" wi="122.51mm" file="US20230007295A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="94.32mm" wi="87.88mm" file="US20230007295A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="145.63mm" wi="133.01mm" file="US20230007295A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="193.12mm" wi="151.55mm" file="US20230007295A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="148.25mm" wi="122.43mm" file="US20230007295A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="161.71mm" wi="128.35mm" file="US20230007295A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION(S)</heading><p id="p-0002" num="0001">This application is a continuation of U.S. application Ser. No. 17/643,522, filed on Dec. 9, 2021, which is a continuation of International Application No. PCT/CN2019/092829, filed on Jun. 25, 2019, the entire disclosures of which are hereby incorporated by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">Implementations of the present disclosure relate to the technical field of video coding and decoding, and in particular, to processing methods for motion compensation, an encoder, and a decoder.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">Merge with motion vector difference (MMVD) technology, which is used in skip and merge modes in inter prediction, is a special expression of motion vectors. The MMVD technology is realized mainly through the following process: reuse motion vector merging candidates in versatile video coding (VVC), and among the candidates, select one candidate as an initial motion vector, and then expand the initial motion vector through the following methods. Some simple syntax elements, mainly including a starting point, a motion magnitude, and a motion direction, are used to obtain a final expansion expression of the motion vector, thereby forming a new motion vector. However, at present, the above solution is difficult to meet motion conditions of most objects, resulting in insufficient accuracy of motion information that can be expressed.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0005" num="0004">Implementations of the present disclosure provide processing methods for motion compensation, an encoder, and a decoder.</p><p id="p-0006" num="0005">In a first aspect, implementations of the present disclosure provide a processing method for motion compensation. The method is applied to a decoder and includes the following. Determine a search position constellation with a point pointed by an initial motion vector as a center point, where the search position constellation includes N directions, search for at least one search point by using at least one type of step lengths in the directions, N is an integer larger than or equal to 1, where the at least one search point at least includes at least one search point in diagonal directions, and the diagonal directions are respectively: upper left, lower left, upper right, and lower right relative to the center point. Search for at least one search position from the search position constellation based on a preset range coverage rule, where the preset range coverage rule is to arrange search points in the directions for search. Obtain a new motion vector according to the at least one search position, and perform motion compensation based on the new motion vector to obtain a prediction value of a coding unit (CU).</p><p id="p-0007" num="0006">In a second aspect, implementations of the present disclosure provide a processing method for motion compensation. The method is applied to an encoder and includes the following. Determine a search position constellation with a point pointed by an initial motion vector as a center point, where the search position constellation includes N directions, search for at least one search point by using at least one type of step lengths in the directions, N is an integer larger than or equal to 1, where the at least one search point at least includes at least one search point in diagonal directions, and the diagonal directions are respectively: upper left, lower left, upper right, and lower right relative to the center point. Search for at least one search position from the search position constellation based on a preset range coverage rule, where the preset range coverage rule is to arrange search points in the directions for search. Obtain a new motion vector according to the at least one search position, and perform motion compensation based on the new motion vector to obtain a prediction value of a CU.</p><p id="p-0008" num="0007">In a third aspect, implementations of the present disclosure provide a decoder. The decoder includes a processor and a memory storing a computer program which, when executed by the processor, causes the processor to perform the following. Determine a search position constellation with a point pointed by an initial motion vector as a center point, where the search position constellation includes N directions, search for at least one search point by using at least one type of step lengths in the directions, N is an integer larger than or equal to 1, where the at least one search point at least includes at least one search point in diagonal directions, and the diagonal directions are respectively: upper left, lower left, upper right, and lower right relative to the center point. Search for at least one search position from the search position constellation based on a preset range coverage rule, where the preset range coverage rule is to arrange search points in the directions for search. Obtain a new motion vector according to the at least one search position, and perform motion compensation based on the new motion vector to obtain a prediction value of a CU.</p><p id="p-0009" num="0008">In a fourth aspect, implementations of the present disclosure provide an encoder. The encoder includes a processor and a memory storing a computer program which, when executed by the processor, causes the processor to perform the following. Determine a search position constellation with a point pointed by an initial motion vector as a center point, where the search position constellation includes N directions, search for at least one search point by using at least one type of step lengths in the directions, N is an integer larger than or equal to 1, where the at least one search point at least includes at least one search point in diagonal directions, and the diagonal directions are respectively: upper left, lower left, upper right, and lower right relative to the center point. Search for at least one search position from the search position constellation based on a preset range coverage rule, where the preset range coverage rule is to arrange search points in the directions for search. Obtain a new motion vector according to the at least one search position, and perform motion compensation based on the new motion vector to obtain a prediction value of a CU.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic diagram illustrating using intra prediction to determine search positions.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a first schematic flow chart illustrating a processing method for motion compensation provided by an implementation of the present disclosure.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a schematic diagram illustrating determining search positions provided by an implementation of the present disclosure.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a schematic diagram illustrating search directions when being applied to merge with motion vector difference (MMVD) provided by the present disclosure.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a schematic diagram illustrating obtaining multiple search positions in MMVD provided by the present disclosure.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a schematic structural diagram illustrating a video coding system.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a schematic structural diagram illustrating a video decoding system.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a schematic structural diagram illustrating constitution of an encoder provided by an implementation of the present disclosure.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a second schematic flow chart illustrating a processing method for motion compensation provided by an implementation of the present disclosure.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a schematic structural diagram illustrating constitution of a decoder provided by an implementation of the present disclosure.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a schematic structural diagram illustrating hardware constitution provided by an implementation of the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0021" num="0020">Technical solutions in implementations of the present disclosure will be clearly and completely described below in conjunction with accompanying drawings in the implementations of the present disclosure. It can be understood that specific implementations described here are only used to explain the related disclosure, but not to limit the present disclosure. In addition, it is to be noted that, for ease of illustration, only parts related to the related disclosure are illustrated in the accompanying drawings.</p><p id="p-0022" num="0021">In the merge with motion vector difference (MMVD) technology, a motion vector candidate list of a current coding unit (CU) is obtained through a process of constructing a merging candidate list in versatile video coding (VVC). Then, perform syntax expansion on each candidate in the list as follows. As illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, take each candidate as an initial motion vector, and with a position pointed by the candidate in the reference picture as a starting point, search with 8 different step lengths in four directions: up, down, left, and right. For each initial motion vector, with each step length in each direction, a new motion vector can be formed, and perform motion compensation based on the motion vector to obtain a prediction value of the current CU. Then perform rate-distortion cost comparison on all current prediction values to select an optimal combination of syntax elements, and record three syntax elements including an index of a position of the initial motion vector in the merging candidate list, a motion direction index, and a search step length index.</p><p id="p-0023" num="0022">Considering the compromise between algorithm complexity and performance, at present, in the algorithm the first two candidates in the merging candidate list are taken as initial motion vectors. For indexes of initial motion vectors, indexes of 8 search step lengths, and indexes of 4 search directions, reference can be made to Tables 1, 2, and 3 illustrated as follows.</p><p id="p-0024" num="0000"><tables id="TABLE-US-00001" num="00001"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="4"><colspec colname="offset" colwidth="14pt" align="left"/><colspec colname="1" colwidth="91pt" align="left"/><colspec colname="2" colwidth="56pt" align="left"/><colspec colname="3" colwidth="56pt" align="left"/><thead><row><entry/><entry namest="offset" nameend="3" rowsep="1">TABLE 1</entry></row><row><entry/><entry namest="offset" nameend="3" align="center" rowsep="1"/></row><row><entry/><entry>Initial motion vector index</entry><entry>0</entry><entry>1</entry></row><row><entry/><entry namest="offset" nameend="3" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/><entry>Position in the merging</entry><entry>Candidate 1</entry><entry>Candidate 2</entry></row><row><entry/><entry>candidate list</entry></row><row><entry/><entry namest="offset" nameend="3" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0025" num="0000"><tables id="TABLE-US-00002" num="00002"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="9"><colspec colname="1" colwidth="63pt" align="left"/><colspec colname="2" colwidth="21pt" align="center"/><colspec colname="3" colwidth="21pt" align="center"/><colspec colname="4" colwidth="21pt" align="center"/><colspec colname="5" colwidth="21pt" align="center"/><colspec colname="6" colwidth="14pt" align="center"/><colspec colname="7" colwidth="21pt" align="center"/><colspec colname="8" colwidth="14pt" align="center"/><colspec colname="9" colwidth="21pt" align="center"/><thead><row><entry namest="1" nameend="9" rowsep="1">TABLE 2</entry></row><row><entry namest="1" nameend="9" align="center" rowsep="1"/></row><row><entry>Step length index</entry><entry>0</entry><entry>1</entry><entry>2</entry><entry>3</entry><entry>4</entry><entry>5</entry><entry>6</entry><entry>7</entry></row><row><entry namest="1" nameend="9" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry>Pixel distance</entry><entry>&#xbc;</entry><entry>&#xbd;</entry><entry>1</entry><entry>2</entry><entry>4</entry><entry>8</entry><entry>16</entry><entry>32</entry></row><row><entry namest="1" nameend="9" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0026" num="0000"><tables id="TABLE-US-00003" num="00003"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="6"><colspec colname="offset" colwidth="14pt" align="left"/><colspec colname="1" colwidth="70pt" align="left"/><colspec colname="2" colwidth="14pt" align="center"/><colspec colname="3" colwidth="56pt" align="center"/><colspec colname="4" colwidth="14pt" align="center"/><colspec colname="5" colwidth="49pt" align="center"/><thead><row><entry/><entry namest="offset" nameend="5" rowsep="1">TABLE 3</entry></row><row><entry/><entry namest="offset" nameend="5" align="center" rowsep="1"/></row><row><entry/><entry>Direction index</entry><entry>0</entry><entry>1</entry><entry>2</entry><entry>3</entry></row><row><entry/><entry namest="offset" nameend="5" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/><entry>X-axis</entry><entry>+</entry><entry>&#x2212;</entry><entry>4</entry><entry>4</entry></row><row><entry/><entry>Y-axis</entry><entry/><entry/><entry>+</entry><entry>&#x2212;</entry></row><row><entry/><entry namest="offset" nameend="5" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0027" num="0023">When selecting a new motion vector based on the above-mentioned MMVD technology, the use of the 8 search step lengths is not perfect. By counting the probability that the 8 step lengths are selected in the standard test sequence of VVC, it can be found that for each test sequence, the probability that the first four search step lengths (i.e., &#xbc;, &#xbd;, 1, and 2) are selected almost exceeds 90%, so remaining 4 step lengths are not fully utilized, thereby reducing coding efficiency. In addition, there are only 4 search directions: up, down, left, and right, so it is difficult to meet motion conditions of most objects, resulting in insufficient accuracy of motion information that can be expressed.</p><p id="p-0028" num="0024">Based on the above, the present disclosure provides a processing method for motion compensation. As illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the method includes the following.</p><p id="p-0029" num="0025">At block <b>11</b>, a search position constellation is constructed based on an initial motion vector, where the search position constellation includes N search directions, at least one search point is obtained by searching with at least one type of search step lengths in the search direction, Nis an integer larger than or equal to 1, and the N search directions at least include diagonal search directions.</p><p id="p-0030" num="0026">At block <b>12</b>, at least one search position is selected from the search position constellation based on a preset range coverage rule, where the preset range coverage rule is to scatter a preset number of search points in the N search directions for selection.</p><p id="p-0031" num="0027">At block <b>13</b>, at least one search position is set as a new motion vector, and perform motion compensation based on the new motion vector to obtain a prediction value of a CU.</p><p id="p-0032" num="0028">The method provided by the implementation can be applied to an encoder end in a video coding and decoding system.</p><p id="p-0033" num="0029">In general, according to the above solution, available search points are first allocated according to distances in multiple search directions, the search position constellation is constructed, and then a suitable number of search positions are selected from the constellation according to the preset range coverage rule.</p><p id="p-0034" num="0030">That is to say, in the implementation, original 4 search directions are expanded to 8 search directions on the premise that there are 32 preset search positions, then the number of search step lengths in each of the 8 search directions is reduced, and search step lengths are respectively allocated to different search directions to cover a relatively wide search range.</p><p id="p-0035" num="0031">Constructing the search position constellation based on the initial motion vector specifically further includes constructing the search position constellation with a point pointed by the initial motion vector as a center point.</p><p id="p-0036" num="0032">The 8 search directions are classified into two types, namely horizontal and vertical search directions and diagonal search directions.</p><p id="p-0037" num="0033">The horizontal and vertical search directions are respectively: up, down, left, and right. The diagonal search directions are respectively: upper left, lower left, upper right, and lower right.</p><p id="p-0038" num="0034">The search step length corresponds to a type of search directions.</p><p id="p-0039" num="0035">The method further includes the following. If a type of a search direction is the horizontal or vertical search direction, a first set of search step lengths is adopted. If the type of the search direction is the diagonal search direction, a second set of search step lengths is adopted.</p><p id="p-0040" num="0036">On the premise that there are 32 preset search positions, the first set of search step lengths contains at least one type of first search step lengths, and the second set of search step lengths contains at least one type of second search step lengths.</p><p id="p-0041" num="0037">It can be pointed out that in the implementation, the first set of search step lengths and the second set of search step lengths may be the same or different.</p><p id="p-0042" num="0038">If the first set of search step lengths and the second set of search step lengths are the same, it can be considered that same 4 search step lengths (a step length is a distance between a search position and the center point of the constellation) are used in the horizontal and vertical directions and the diagonal directions, respectively, to select search positions.</p><p id="p-0043" num="0039">Alternatively, the first set of search step lengths are different from the second set of search step lengths. In this case, the first set of search step lengths and the second set of search step lengths may intersect. For example, search step lengths in the horizontal and vertical directions and search step lengths in the diagonal directions are increased staggeredly. For example, distances in the horizontal and vertical directions are preferentially selected, and then each of the search step lengths in the diagonal direction is intersected between two adjacent search step lengths in the horizontal and vertical directions. In this way, search in the diagonal directions can cover areas that cannot be searched by search in the horizontal and vertical directions.</p><p id="p-0044" num="0040">That is to say, a second search step length of first type in the second set of search step lengths may be between a first search step length of first type and a first search step length of second type in the first set of search step lengths. Correspondingly, a first search step length of third type in the first set of search step lengths may be between a second search step length of second type and a second search step length of third type in the second set of search step lengths.</p><p id="p-0045" num="0041">A diagonal position is expressed as (lx, ly), where lx=ly, and the search step length is expressed as a single value lx in this specification.</p><p id="p-0046" num="0042">For example, the first search step length may be at least one of the following: &#xbc;-pixel, &#xbd;-pixel, 2-pixel, and 4-pixel, and the second search step length may be at least one of the following: &#x215b;-pixel, &#x215c;-pixel, &#xbe;-pixel, and 3/2-pixel.</p><p id="p-0047" num="0043">The solution provided by the implementation will be described below in conjunction with the accompanying drawings.</p><p id="p-0048" num="0044">Take the search step length increased by multiples as a radius and draw a circle with a position pointed by the initial motion vector as a center of the circle. Search step lengths in the diagonal directions are all set to be close to an edge of the circle and expand to the distance in turn, where the search step lengths in the diagonal direction can be determined based on the first set of search step lengths and the second set of search step lengths. For example, if the first set of search step lengths and the second set of search step lengths are the same, draw a circle with each of four types of first search step lengths or each of four types of second search step lengths as a radius and four circles are obtained. There are 4 search points in the diagonal direction, 4 search points in the horizontal direction, and 4 search points in the vertical direction. If the first set of search step lengths and the second set of search step lengths are different, multiple search points can be obtained in the diagonal search directions and the horizontal and vertical search directions based on 4 first search step lengths and 4 second search step lengths respectively.</p><p id="p-0049" num="0045">Then according to statistics, each representative search point is selected according to a coverage area, and approximated to a position that can be expressed by divided by a power of 2 based on the principle of convenient interpolation. As illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, in both the horizontal and vertical directions a &#xbc;-pixel position, a &#xbd;-pixel position, a 2-pixel position, and a 4-pixel position are selected, and in the diagonal direction a (&#x215b;, &#x215b;) pixel position, a (&#x215c;, &#x215c;) pixel position, a (&#xbe;, &#xbe;) pixel position, and a (3/2, 3/2) pixel position are selected. When selecting representative search points, considering different statistical characteristics of different resolution sequences, more search points can be allocated near a circle with a radius of &#xbd; and a circle with a radius of 2.</p><p id="p-0050" num="0046">The detailed illustration of implementing MMVD on this basis is as follows.</p><p id="p-0051" num="0047">First, select initial motion vectors from the candidate list.</p><p id="p-0052" num="0048">Specifically, the first two candidates in the merging candidate list in VVC are still reused as the initial motion vectors. For each initial motion vector, three syntax elements including an initial motion vector index, a search direction index, and a search step length index are still used for expression expansion.</p><p id="p-0053" num="0049">During the expression expansion, for search directions, as illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the original four directions of up, down, left, and right are modified to include the four directions of up, down, left, and right, and the four directions of upper left, upper right, lower right, and lower left, that is, based on the original directions, the diagonal directions are added.</p><p id="p-0054" num="0050">In <figref idref="DRAWINGS">FIG. <b>4</b></figref>, new directions are indicated via solid lines, and original directions are indicated via dashed lines. In this way, on the basis of the original horizontal and vertical directions, additional motion directions are newly added, which can increase the flexibility of motion vector expression.</p><p id="p-0055" num="0051">An index table of search directions in the implementation is illustrated as in Table 4.</p><p id="p-0056" num="0000"><tables id="TABLE-US-00004" num="00004"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="9"><colspec colname="1" colwidth="70pt" align="left"/><colspec colname="2" colwidth="21pt" align="center"/><colspec colname="3" colwidth="21pt" align="center"/><colspec colname="4" colwidth="14pt" align="center"/><colspec colname="5" colwidth="21pt" align="center"/><colspec colname="6" colwidth="14pt" align="center"/><colspec colname="7" colwidth="21pt" align="center"/><colspec colname="8" colwidth="14pt" align="center"/><colspec colname="9" colwidth="21pt" align="center"/><thead><row><entry namest="1" nameend="9" rowsep="1">TABLE 4</entry></row><row><entry namest="1" nameend="9" align="center" rowsep="1"/></row><row><entry>Direction sequence</entry><entry>0</entry><entry>1</entry><entry>2</entry><entry>3</entry><entry>4</entry><entry>5</entry><entry>6</entry><entry>7</entry></row><row><entry namest="1" nameend="9" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry>X-axis</entry><entry>+</entry><entry>&#x2212;</entry><entry/><entry/><entry>+</entry><entry>+</entry><entry>&#x2212;</entry><entry>&#x2212;</entry></row><row><entry>Y-axis</entry><entry/><entry/><entry>+</entry><entry>&#x2212;</entry><entry>+</entry><entry>&#x2212;</entry><entry>+</entry><entry>&#x2212;</entry></row><row><entry namest="1" nameend="9" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0057" num="0052">Then search step lengths are modified. First, reduce 8 search step lengths in the original algorithm to 4 search step lengths commonly used, as illustrated in Table 5.</p><p id="p-0058" num="0000"><tables id="TABLE-US-00005" num="00005"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="6"><colspec colname="offset" colwidth="14pt" align="left"/><colspec colname="1" colwidth="77pt" align="left"/><colspec colname="2" colwidth="14pt" align="center"/><colspec colname="3" colwidth="56pt" align="center"/><colspec colname="4" colwidth="7pt" align="center"/><colspec colname="5" colwidth="49pt" align="center"/><thead><row><entry/><entry namest="offset" nameend="5" rowsep="1">TABLE 5</entry></row><row><entry/><entry namest="offset" nameend="5" align="center" rowsep="1"/></row><row><entry/><entry>Step length index</entry><entry>0</entry><entry>1</entry><entry>2</entry><entry>3</entry></row><row><entry/><entry namest="offset" nameend="5" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/><entry>Pixel distance</entry><entry>&#xbc;</entry><entry>&#xbd;</entry><entry>2</entry><entry>4</entry></row><row><entry/><entry namest="offset" nameend="5" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0059" num="0053">On the basis of Table 5, in order to cover a relatively wide search range, in Table 6 in the diagonal directions search step lengths different from that in the up, down, left, and right directions are set.</p><p id="p-0060" num="0000"><tables id="TABLE-US-00006" num="00006"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="6"><colspec colname="offset" colwidth="21pt" align="left"/><colspec colname="1" colwidth="84pt" align="left"/><colspec colname="2" colwidth="14pt" align="center"/><colspec colname="3" colwidth="42pt" align="center"/><colspec colname="4" colwidth="14pt" align="center"/><colspec colname="5" colwidth="42pt" align="center"/><thead><row><entry/><entry namest="offset" nameend="5" rowsep="1">TABLE 6</entry></row><row><entry/><entry namest="offset" nameend="5" align="center" rowsep="1"/></row><row><entry/><entry>Step length index</entry><entry>0</entry><entry>1</entry><entry>2</entry><entry>3</entry></row><row><entry/><entry namest="offset" nameend="5" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/><entry>Pixel distance in the up,</entry><entry>&#xbc;</entry><entry>&#xbd;</entry><entry>2</entry><entry>4</entry></row><row><entry/><entry>down, left, and right</entry><entry/><entry/><entry/><entry/></row><row><entry/><entry>directions</entry><entry/><entry/><entry/><entry/></row><row><entry/><entry>Pixel distance in the</entry><entry>&#x215b;</entry><entry>&#x215c;</entry><entry>&#xbe;</entry><entry> 3/2</entry></row><row><entry/><entry>diagonal directions</entry></row><row><entry/><entry namest="offset" nameend="5" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0061" num="0054">Finally, for each initial motion vector, search directions and step lengths formed are illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0062" num="0055">Based on the foregoing solution, the implementation may further include the following. At least one search position is set as a new motion vector. Motion compensation is performed based on the new motion vector to obtain the prediction value of the CU. Then the rate-distortion cost comparison is performed on all current prediction values, the optimal combination of syntax elements is selected, and three syntax elements including an index of a position of the initial motion vector in the merging candidate list, a motion direction index and a search step length index are recorded.</p><p id="p-0063" num="0056">By adopting the above solution, a test sequence required by joint video experts team (JVET) is tested under random access conditions, and BD-rate average variations in Y, Cb, and Cr components are &#x2212;0.14%, &#x2212;0.12%, and 0.10%, respectively. This data represents that the technology improves coding performance.</p><p id="p-0064" num="0057"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a schematic structural diagram illustrating a video coding system. As illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the video coding system <b>100</b> includes a transform and quantization module <b>101</b>, an intra estimation module <b>102</b>, an intra prediction module <b>103</b>, a motion compensation module <b>104</b>, a motion estimation module <b>105</b>, an inverse transform and inverse quantization module <b>106</b>, a filter control analysis module <b>107</b>, a deblocking filtering and sample adaptive offset (SAO) filtering module <b>108</b>, a header information coding and context-based adaptive binary arithmetic coding (CABAC) coding module <b>109</b>, a decoded picture buffer <b>110</b>, and so on. <figref idref="DRAWINGS">FIG. <b>7</b></figref> is a schematic structural diagram illustrating a video decoding system. As illustrated in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the video decoding system <b>200</b> includes a header information decoding and CABAC decoding module <b>201</b>, an inverse transform and inverse quantization module <b>202</b>, an intra prediction module <b>203</b>, a motion compensation module <b>204</b>, a deblocking filtering and SAO filtering module <b>205</b>, a decoded picture buffer <b>206</b>, and so on. A video picture is processed by parts in the video coding system <b>100</b> such as the transform and quantization module <b>101</b>, the intra estimation module <b>102</b>, the intra prediction module <b>103</b>, the motion compensation module <b>104</b>, the motion estimation module <b>105</b>, the inverse transform and inverse quantization module <b>106</b>, the filter control analysis module <b>107</b>, the deblocking filtering and SAO filtering module <b>108</b>, the header information coding and CABAC coding module <b>109</b>, the decoded picture buffer <b>110</b>, and so on to output a bitstream of the video picture. The bitstream is input into the video decoding system <b>200</b> and processed by parts in the video decoding system <b>200</b> such as the header information decoding and CABAC decoding module <b>201</b>, the inverse transform and inverse quantization module <b>202</b>, the intra prediction module <b>203</b>, the motion compensation module <b>204</b> and so on, and finally the original video picture is restored.</p><p id="p-0065" num="0058">Further, the image coding method proposed in the present disclosure can affect an intra prediction part in a video coding hybrid framework, that is, is mainly applied to the motion compensation module <b>104</b> and the motion estimation module <b>105</b> in video coding and the motion compensation module <b>204</b> in video decoding, i.e., acts on both a coding end and a decoding end.</p><p id="p-0066" num="0059">Based on the foregoing solution, in the implementation, coding performance can be improved under a condition that the number of computations is substantially unchanged. In the related art, expression expansion on two initial motion vectors are needed, and a total of 2*4*8=64 comparisons are required, where 2 represents 2 initial motion vectors, 4 represents search directions, and 8 represents search step lengths. After modification, still perform expression expansion on 2 initial motion vectors, and a total of 2*8*4=64 comparisons are required, where 8 search directions and 4 search step lengths are included. It can be seen that this technology can improve the overall coding performance while maintaining the same amount of computations as the original technology.</p><p id="p-0067" num="0060">Correspondingly, based on the foregoing solution, the complexity of the decoding end can be kept unchanged. In the related art, at the decoding end, expression expansion on the initial motion vector is performed by decoding three syntax elements, so as to perform motion compensation and finally obtain a reconstructed block. In the solution, expression expansion on the initial motion vector is performed still by decoding three syntax elements, so the complexity remains unchanged.</p><p id="p-0068" num="0061">It can be seen that according to the solution provided by the implementation, search positions can be determined based on 8 search directions and 4 search step lengths corresponding to each direction. In this way, compared with the related art, the number of search step lengths in each direction is reduced, thereby avoiding a problem of reduced coding efficiency due to that some of the search step lengths are not selected, and by increasing the search directions, it can be ensured that motion conditions of most objects can be satisfied, so that motion information can be expressed more accurately.</p><p id="p-0069" num="0062">Based on the above, an implementation provides an encoder. As illustrated in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the encoder includes a constructing part <b>32</b> and a coding part <b>33</b>.</p><p id="p-0070" num="0063">The constructing part <b>32</b> is configured to construct a search position constellation based on an initial motion vector, where the search position constellation includes N search directions, at least one search point is obtained by searching with at least one type of search step lengths in the search direction, N is an integer larger than or equal to 1, and the N search directions at least include diagonal search directions.</p><p id="p-0071" num="0064">The coding part <b>33</b> is configured to select at least one search position from the search position constellation based on a preset range coverage rule, set at least one search position as a new motion vector, and perform motion compensation based on the new motion vector to obtain a prediction value of a CU, where the preset range coverage rule is to scatter a preset number of search points in the N search directions for selection.</p><p id="p-0072" num="0065">The method provided in the implementation can be applied to an encoder end in a video coding and decoding system.</p><p id="p-0073" num="0066">That is to say, in the implementation, original 4 search directions are expanded to 8 search directions on the premise that there are 32 preset search positions, then the number of search step lengths in each of the 8 search directions is reduced, and search step lengths are respectively allocated to different search directions to cover a relatively wide search range.</p><p id="p-0074" num="0067">Specifically, the constructing part is configured to construct the search position constellation with a point pointed by the initial motion vector as a center point.</p><p id="p-0075" num="0068">The N search directions in the search position constellation further include horizontal and vertical search directions. The horizontal and vertical search directions are respectively: up, down, left, and right. The diagonal search directions are respectively: upper left, lower left, upper right, and lower right.</p><p id="p-0076" num="0069">The search step length corresponds to a type of a search direction. The coding part is configured to adopt a first set of search step lengths if the type of the search direction is the horizontal or vertical search direction, and adopt a second set of search step lengths if the type of the search direction is the diagonal search direction.</p><p id="p-0077" num="0070">On the premise that there are 32 preset search positions, the first set of search step lengths includes at least one type of first search step lengths, and the second set of search step lengths includes at least one type of second search step lengths.</p><p id="p-0078" num="0071">It can be pointed out that in the implementation, the first set of search step lengths and the second set of search step lengths may be the same or different.</p><p id="p-0079" num="0072">If the first set of search step lengths and the second set of search step lengths are the same, it can be considered that same 4 search step lengths (a step length is a distance between a search position and the center point of the constellation) are used in the horizontal and vertical directions and the diagonal directions, respectively, to select search positions.</p><p id="p-0080" num="0073">Alternatively, the first set of search step lengths and the second set of search step lengths are different from each other. In this case, the first set of search step lengths and the second set of search step lengths may intersect. For example, search step lengths in the horizontal and vertical directions and search step lengths in the diagonal directions are increased staggeredly. For example, distances in the horizontal and vertical directions are preferentially selected, and then each of search step lengths in the diagonal direction is intersected between two adjacent search step lengths in the horizontal and vertical directions. In this way, the search in the diagonal directions can cover areas that cannot be searched by search in the horizontal and vertical directions.</p><p id="p-0081" num="0074">That is to say, a second search step length of first type in the second set of search step lengths may be between a first search step length of first type and a first search step length of second type in the first set of search step lengths. Correspondingly, a first search step length of third type in the first set of search step lengths may be between a second search step length of second type and a second search step length of third type in the second set of search step lengths.</p><p id="p-0082" num="0075">A diagonal position is expressed as (lx, ly), where lx=ly, and the search step length is expressed as a single value lx in this specification.</p><p id="p-0083" num="0076">For example, the first search step length may be at least one of the following: &#xbc;-pixel, &#xbd;-pixel, 2-pixel, 4-pixel, and the second search step length may be at least one of the following: &#x215b;-pixel, &#x215c;-pixel, &#xbe;-pixel, and 3/2-pixel.</p><p id="p-0084" num="0077">The solution provided by the implementation will be described below in conjunction with the accompanying drawings.</p><p id="p-0085" num="0078">Take the search step length increased by multiples as a radius and draw a circle with a position pointed by the initial motion vector as a center of the circle. Search step lengths in the diagonal directions are all set to be close to an edge of the circle and expand to the distance in turn, where the search step lengths in the diagonal direction can be determined based on the first set of search step lengths and the second set of search step lengths. For example, if the first set of search step lengths and the second set of search step lengths are the same, draw a circle with each of four types of first search step lengths or each of four types of second search step lengths as a radius and four circles are obtained. There are 4 search points in the diagonal direction, 4 search points in the horizontal direction, and 4 search points in the vertical direction. If the first set of search step lengths and the second set of search step lengths are different from each other, multiple search points can be obtained in the diagonal search directions and the horizontal and vertical search directions based on 4 first search step lengths and 4 second search step lengths respectively.</p><p id="p-0086" num="0079">Then according to statistics, each representative search point is selected according to a coverage range, and approximated to a position that can be expressed by divided by a power of 2 based on the principle of convenient interpolation. As illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, in both the horizontal and vertical directions a &#xbc;-pixel position, a &#xbd;-pixel position, a 2-pixel position, and a 4-pixel position are selected, and in the diagonal direction a (&#x215b;, &#x215b;) pixel position, a (&#x215c;, &#x215c;) pixel position, a (&#xbe;, &#xbe;) pixel position, and a (3/2, 3/2) pixel position are selected. When selecting representative search points, considering different statistical characteristics of different resolution sequences, more search points can be allocated near a circle with a radius of &#xbd; and a circle with a radius of 2.</p><p id="p-0087" num="0080">The detailed illustration of implementing MMVD on this basis is as follows.</p><p id="p-0088" num="0081">First, select initial motion vectors from the candidate list.</p><p id="p-0089" num="0082">Specifically, the first two candidates in the merging candidate list in VVC are still reused as initial motion vectors. For each initial motion vector, three syntax elements including an initial motion vector index, a search direction index, and a search step length index are still used for expression expansion.</p><p id="p-0090" num="0083">During the expression expansion, for search directions, as illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the original four directions of up, down, left, and right are modified to include the four directions of up, down, left, and right, and the four directions of upper left, upper right, lower right, and lower left, that is, based on the original directions, the diagonal directions are added.</p><p id="p-0091" num="0084">In <figref idref="DRAWINGS">FIG. <b>4</b></figref>, new directions are indicated via solid lines, and original directions are indicated via dashed lines. In this way, on the basis of the original horizontal and vertical directions, additional motion directions are newly added, which can increase the flexibility of motion vector expression.</p><p id="p-0092" num="0085">An index table of search directions in the implementation is illustrated as in Table 4.</p><p id="p-0093" num="0000"><tables id="TABLE-US-00007" num="00007"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="9"><colspec colname="1" colwidth="70pt" align="left"/><colspec colname="2" colwidth="21pt" align="center"/><colspec colname="3" colwidth="21pt" align="center"/><colspec colname="4" colwidth="14pt" align="center"/><colspec colname="5" colwidth="21pt" align="center"/><colspec colname="6" colwidth="14pt" align="center"/><colspec colname="7" colwidth="21pt" align="center"/><colspec colname="8" colwidth="14pt" align="center"/><colspec colname="9" colwidth="21pt" align="center"/><thead><row><entry namest="1" nameend="9" rowsep="1">TABLE 4</entry></row><row><entry namest="1" nameend="9" align="center" rowsep="1"/></row><row><entry>Direction sequence</entry><entry>0</entry><entry>1</entry><entry>2</entry><entry>3</entry><entry>4</entry><entry>5</entry><entry>6</entry><entry>7</entry></row><row><entry namest="1" nameend="9" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry>X-axis</entry><entry>+</entry><entry>&#x2212;</entry><entry/><entry/><entry>+</entry><entry>+</entry><entry>&#x2212;</entry><entry>&#x2212;</entry></row><row><entry>Y-axis</entry><entry/><entry/><entry>+</entry><entry>&#x2212;</entry><entry>+</entry><entry>&#x2212;</entry><entry>+</entry><entry>&#x2212;</entry></row><row><entry namest="1" nameend="9" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0094" num="0086">Then search step lengths are modified. First, reduce 8 search step lengths in the original algorithm to 4 search step lengths commonly used, as illustrated in Table 5.</p><p id="p-0095" num="0000"><tables id="TABLE-US-00008" num="00008"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="6"><colspec colname="offset" colwidth="14pt" align="left"/><colspec colname="1" colwidth="77pt" align="left"/><colspec colname="2" colwidth="14pt" align="center"/><colspec colname="3" colwidth="56pt" align="center"/><colspec colname="4" colwidth="7pt" align="center"/><colspec colname="5" colwidth="49pt" align="center"/><thead><row><entry/><entry namest="offset" nameend="5" rowsep="1">TABLE 5</entry></row><row><entry/><entry namest="offset" nameend="5" align="center" rowsep="1"/></row><row><entry/><entry>Step length index</entry><entry>0</entry><entry>1</entry><entry>2</entry><entry>3</entry></row><row><entry/><entry namest="offset" nameend="5" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/><entry>Pixel distance</entry><entry>&#xbc;</entry><entry>&#xbd;</entry><entry>2</entry><entry>4</entry></row><row><entry/><entry namest="offset" nameend="5" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0096" num="0087">On the basis of Table 5, in order to cover a relatively wide search range, in Table 6 in the diagonal directions search step lengths different from that in the up, down, left, and right directions are set.</p><p id="p-0097" num="0000"><tables id="TABLE-US-00009" num="00009"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="6"><colspec colname="offset" colwidth="21pt" align="left"/><colspec colname="1" colwidth="84pt" align="left"/><colspec colname="2" colwidth="14pt" align="center"/><colspec colname="3" colwidth="42pt" align="center"/><colspec colname="4" colwidth="14pt" align="center"/><colspec colname="5" colwidth="42pt" align="center"/><thead><row><entry/><entry namest="offset" nameend="5" rowsep="1">TABLE 6</entry></row><row><entry/><entry namest="offset" nameend="5" align="center" rowsep="1"/></row><row><entry/><entry>Step length index</entry><entry>0</entry><entry>1</entry><entry>2</entry><entry>3</entry></row><row><entry/><entry namest="offset" nameend="5" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/><entry>Pixel distance in the up,</entry><entry>&#xbc;</entry><entry>&#xbd;</entry><entry>2</entry><entry>4</entry></row><row><entry/><entry>down, left, and right</entry><entry/><entry/><entry/><entry/></row><row><entry/><entry>directions</entry><entry/><entry/><entry/><entry/></row><row><entry/><entry>Pixel distance in the</entry><entry>&#x215b;</entry><entry>&#x215c;</entry><entry>&#xbe;</entry><entry> 3/2</entry></row><row><entry/><entry>diagonal directions</entry></row><row><entry/><entry namest="offset" nameend="5" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0098" num="0088">Finally, for each initial motion vector, search directions and step lengths formed are illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0099" num="0089">Based on the foregoing solution, in the implementation the coding part sets least one search position as a new motion vector and perform motion compensation based on the new motion vector to obtain a prediction value of a CU. The coding part then performs rate-distortion cost comparison on all current prediction values, selects an optimal combination of syntax elements, and records three syntax elements including an index of a position of the initial motion vector in the merging candidate list, a motion direction index, and a search step length index.</p><p id="p-0100" num="0090">By adopting the above solution, a test sequence required by JVET is tested under random access conditions, and BD-rate average variations in Y, Cb, and Cr components are &#x2212;0.14%, &#x2212;0.12%, and 0.10%, respectively. This data represents that the technology improves coding performance.</p><p id="p-0101" num="0091">Based on the foregoing solution, in the implementation, coding performance can be improved under a condition that the number of computations is substantially unchanged. In the related art, expression expansion on two initial motion vectors is needed, and a total of 2*4*8=64 comparisons are required, where 2 represents 2 initial motion vectors, 4 represents search directions, and 8 represents search step lengths. After modification, expression expansion on 2 initial motion vectors are still performed, and a total of 2*8*4=64 comparisons are required, where 8 search directions and 4 search step lengths are included. It can be seen that this technology can improve the overall coding performance while maintaining the same amount of computations as the original technology.</p><p id="p-0102" num="0092">Correspondingly, based on the foregoing solution, the complexity of the decoding end can be kept unchanged. In the related art, at the decoding end, expression expansion on the initial motion vector is performed by decoding three syntax elements, so as to perform motion compensation and finally obtain a reconstructed block. In the solution, the expression expansion on the initial motion vector is still performed by decoding three syntax elements, so the complexity remains unchanged.</p><p id="p-0103" num="0093">It can be seen that according to the solution provided by the implementation, search positions can be determined based on 8 search directions and 4 search step lengths corresponding to each direction. In this way, compared with the related art, the number of search step lengths in each direction is reduced, thereby avoiding a problem of reduced coding efficiency due to that some of the search step lengths are not selected, and by increasing the search directions, it can be ensured that motion conditions of most objects can be satisfied, so that motion information can be expressed more accurately.</p><p id="p-0104" num="0094">An implementation provides a processing method for motion compensation. The processing method is applied to a decoder. As illustrated in <figref idref="DRAWINGS">FIG. <b>9</b></figref>, the processing method includes the following.</p><p id="p-0105" num="0095">At block <b>41</b>, expand at least one search position corresponding to an initial motion vector in a search position constellation based on syntax elements of the initial motion vector.</p><p id="p-0106" num="0096">At block <b>42</b>, perform motion compensation based on a new motion vector that is obtained by expanding the initial motion vector to obtain a reconstructed block.</p><p id="p-0107" num="0097">The syntax elements include an initial motion vector index, a search direction index, and a search step length direction, where the search direction indicated in the search direction index at least includes a diagonal search direction.</p><p id="p-0108" num="0098">Performing the motion compensation based on the new motion vector that is obtained by expanding the initial motion vector to obtain the reconstructed block can be understood as performing the motion compensation based on the new motion vector that is obtained by expanding the initial motion vector to obtain the reconstructed block of a current processing block. The current processing block is a current coding block.</p><p id="p-0109" num="0099">The method provided in the implementation can be applied to a decoder end in a video coding and decoding system.</p><p id="p-0110" num="0100">In general, according to the above solution, available search points are first allocated according to distances in multiple search directions, the search position constellation is constructed, and then a suitable number of search positions are selected from the constellation according to a range coverage rule.</p><p id="p-0111" num="0101">That is to say, in the implementation, original 4 search directions are expanded to 8 search directions on the premise that there are 32 preset search positions, then the number of search step lengths in each of the 8 search directions is reduced, and search step lengths are respectively allocated to different search directions to cover a relatively wide search range.</p><p id="p-0112" num="0102">Specifically, the search position constellation is constructed with a point pointed by the initial motion vector as a center point.</p><p id="p-0113" num="0103">Search directions in the search position constellation further include horizontal and vertical search directions. The horizontal and vertical search directions are respectively: up, down, left, and right. The diagonal search directions are respectively: upper left, lower left, upper right, and lower right.</p><p id="p-0114" num="0104">The search step length corresponds to a type of a search direction.</p><p id="p-0115" num="0105">The method further includes the following. Adopt a first set of search step lengths if the type of the search direction is the horizontal or vertical search direction, and adopt a second set of search step lengths if the type of the search direction is the diagonal search direction.</p><p id="p-0116" num="0106">On the premise that there are 32 preset search positions, the first set of search step lengths includes at least one type of first search step lengths, and the second set of search step lengths includes at least one type of second search step lengths.</p><p id="p-0117" num="0107">It can be pointed out that in the implementation, the first set of search step lengths and the second set of search step lengths may be the same or different. If the first set of search step lengths and the second set of search step lengths are the same, it can be considered that same 4 search step lengths (a step length is a distance between a search position and the center point of the constellation) are used in the horizontal and vertical directions and the diagonal directions, respectively, to select search positions.</p><p id="p-0118" num="0108">Alternatively, the first set of search step lengths and the second set of search step lengths are different from each other. In this case, the first set of search step lengths and the second set of search step lengths may intersect. For example, search step lengths in the horizontal and vertical directions and search step lengths in the diagonal directions are increased staggeredly. For example, distances in the horizontal and vertical directions are preferentially selected, and then each of search step lengths in the diagonal direction is intersected between two adjacent search step lengths in the horizontal and vertical directions. In this way, the search in the diagonal directions can cover areas that cannot be searched by search in the horizontal and vertical directions.</p><p id="p-0119" num="0109">That is to say, a second search step length of first type in the second set of search step lengths may be between a first search step length of first type and a first search step length of second type in the first set of search step lengths. Correspondingly, a first search step length of third type in the first set of search step lengths may be between a second search step length of second type and a second search step length of third type in the second set of search step lengths.</p><p id="p-0120" num="0110">A diagonal position is expressed as (lx, ly), where lx=ly, and the search step length is expressed as a single value lx in this specification.</p><p id="p-0121" num="0111">For example, the first search step length may be at least one of the following: &#xbc;-pixel, &#xbd;-pixel, 2-pixel, 4-pixel, and the second search step length may be at least one of the following: &#x215b;-pixel, &#x215c;-pixel, &#xbe;-pixel, and 3/2-pixel.</p><p id="p-0122" num="0112">The detailed illustration of implementing MMVD on this basis is as follows.</p><p id="p-0123" num="0113">First, select initial motion vectors from the candidate list.</p><p id="p-0124" num="0114">Specifically, first two candidates in a merging candidate list in VVC are still reused as the initial motion vectors. For each initial motion vector, three syntax elements including an initial motion vector index, a search direction index, and a search step length index are still used for expression expansion.</p><p id="p-0125" num="0115">During the expression expansion, for search directions, as illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the original four directions of up, down, left, and right are modified to include the four directions of up, down, left, and right, and the four directions of upper left, upper right, lower right, and lower left, that is, based on the original directions, the diagonal directions are added.</p><p id="p-0126" num="0116">In <figref idref="DRAWINGS">FIG. <b>4</b></figref>, new directions are indicated via solid lines, and original directions are indicated via dashed lines. In this way, on the basis of the original horizontal and vertical directions, additional motion directions are newly added, which can increase the flexibility of motion vector expression.</p><p id="p-0127" num="0117">An index table of search directions in the implementation is illustrated as in Table 4.</p><p id="p-0128" num="0000"><tables id="TABLE-US-00010" num="00010"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="9"><colspec colname="1" colwidth="70pt" align="left"/><colspec colname="2" colwidth="21pt" align="center"/><colspec colname="3" colwidth="21pt" align="center"/><colspec colname="4" colwidth="14pt" align="center"/><colspec colname="5" colwidth="21pt" align="center"/><colspec colname="6" colwidth="14pt" align="center"/><colspec colname="7" colwidth="21pt" align="center"/><colspec colname="8" colwidth="14pt" align="center"/><colspec colname="9" colwidth="21pt" align="center"/><thead><row><entry namest="1" nameend="9" rowsep="1">TABLE 4</entry></row><row><entry namest="1" nameend="9" align="center" rowsep="1"/></row><row><entry>Direction sequence</entry><entry>0</entry><entry>1</entry><entry>2</entry><entry>3</entry><entry>4</entry><entry>5</entry><entry>6</entry><entry>7</entry></row><row><entry namest="1" nameend="9" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry>X-axis</entry><entry>+</entry><entry>&#x2212;</entry><entry/><entry/><entry>+</entry><entry>+</entry><entry>&#x2212;</entry><entry>&#x2212;</entry></row><row><entry>Y-axis</entry><entry/><entry/><entry>+</entry><entry>&#x2212;</entry><entry>+</entry><entry>&#x2212;</entry><entry>+</entry><entry>&#x2212;</entry></row><row><entry namest="1" nameend="9" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0129" num="0118">Then search step lengths are modified. First, reduce 8 search step lengths in the original algorithm to 4 search step lengths commonly used, as illustrated in Table 5.</p><p id="p-0130" num="0000"><tables id="TABLE-US-00011" num="00011"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="6"><colspec colname="offset" colwidth="14pt" align="left"/><colspec colname="1" colwidth="77pt" align="left"/><colspec colname="2" colwidth="14pt" align="center"/><colspec colname="3" colwidth="56pt" align="center"/><colspec colname="4" colwidth="7pt" align="center"/><colspec colname="5" colwidth="49pt" align="center"/><thead><row><entry/><entry namest="offset" nameend="5" rowsep="1">TABLE 5</entry></row><row><entry/><entry namest="offset" nameend="5" align="center" rowsep="1"/></row><row><entry/><entry>Step length index</entry><entry>0</entry><entry>1</entry><entry>2</entry><entry>3</entry></row><row><entry/><entry namest="offset" nameend="5" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/><entry>Pixel distance</entry><entry>&#xbc;</entry><entry>&#xbd;</entry><entry>2</entry><entry>4</entry></row><row><entry/><entry namest="offset" nameend="5" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0131" num="0119">On the basis of Table 5, in order to cover a relatively wide search range, in Table 6 in the diagonal directions search step lengths different from that in the up, down, left, and right directions are set.</p><p id="p-0132" num="0000"><tables id="TABLE-US-00012" num="00012"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="6"><colspec colname="offset" colwidth="21pt" align="left"/><colspec colname="1" colwidth="84pt" align="left"/><colspec colname="2" colwidth="14pt" align="center"/><colspec colname="3" colwidth="42pt" align="center"/><colspec colname="4" colwidth="14pt" align="center"/><colspec colname="5" colwidth="42pt" align="center"/><thead><row><entry/><entry namest="offset" nameend="5" rowsep="1">TABLE 6</entry></row><row><entry/><entry namest="offset" nameend="5" align="center" rowsep="1"/></row><row><entry/><entry>Step length index</entry><entry>0</entry><entry>1</entry><entry>2</entry><entry>3</entry></row><row><entry/><entry namest="offset" nameend="5" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/><entry>Pixel distance in the up,</entry><entry>&#xbc;</entry><entry>&#xbd;</entry><entry>2</entry><entry>4</entry></row><row><entry/><entry>down, left, and right</entry><entry/><entry/><entry/><entry/></row><row><entry/><entry>directions</entry><entry/><entry/><entry/><entry/></row><row><entry/><entry>Pixel distance in the</entry><entry>&#x215b;</entry><entry>&#x215c;</entry><entry>&#xbe;</entry><entry> 3/2</entry></row><row><entry/><entry>diagonal directions</entry></row><row><entry/><entry namest="offset" nameend="5" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0133" num="0120">Finally, for each initial motion vector, search directions and step lengths formed are illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0134" num="0121">Based on the foregoing solution, the implementation may further include the following. Perform the motion compensation based on the new motion vector that is obtained by expanding the initial motion vector to obtain the reconstructed block.</p><p id="p-0135" num="0122"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a schematic structural diagram illustrating a video decoding system. As illustrated in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the video decoding system <b>200</b> includes the header information decoding and CABAC decoding module <b>201</b>, the inverse transform and inverse quantization module <b>202</b>, the intra prediction module <b>203</b>, the motion compensation module <b>204</b>, the deblocking filtering and SAO filtering module <b>205</b>, the decoded picture buffer <b>206</b>, and so on. A video picture is processed by parts in the video coding system <b>100</b> such as the transform and quantization module <b>101</b>, the intra estimation module <b>102</b>, the intra prediction module <b>103</b>, the motion compensation module <b>104</b>, the motion estimation module <b>105</b>, the inverse transform and inverse quantization module <b>106</b>, the filter control analysis module <b>107</b>, the deblocking filtering and SAO filtering module <b>108</b>, the header information coding and CABAC coding module <b>109</b>, the decoded picture buffer <b>110</b>, and so on to output a bitstream of the video picture. The bitstream is input into the video decoding system <b>200</b> and processed by parts in the video decoding system <b>200</b> such as the header information decoding and CABAC decoding module <b>201</b>, the inverse transform and inverse quantization module <b>202</b>, the intra prediction module <b>203</b>, the motion compensation module <b>204</b>, and so on, and finally the original video picture is restored.</p><p id="p-0136" num="0123">Further, the image coding method proposed in the present disclosure can affect an intra prediction part in a video coding hybrid framework, that is, is mainly applied to the motion compensation module <b>104</b> and the motion estimation module <b>105</b> in video coding and the motion compensation module <b>204</b> in video decoding, i.e., acts on both a coding end and a decoding end.</p><p id="p-0137" num="0124">Based on the foregoing solution, the complexity of the decoding end can be kept unchanged. In the related art, at the decoding end, expression expansion on the initial motion vector is performed by decoding three syntax elements, so as to perform motion compensation and finally obtain a reconstructed block. In the solution, expression expansion on the initial motion vector is performed still by decoding three syntax elements, so the complexity remains unchanged.</p><p id="p-0138" num="0125">It is to be noted that, for a coding block in an original picture frame, at the decoder side, a coding mode adopted by the coding block is first determined, then based on the determined coding mode, a candidate list corresponding to the coding mode is established. According to the candidate list corresponding to the coding mode, a decoding parameter corresponding to the coding block is obtained. Finally, prediction decoding is performed on the coding block based on the decoding parameter corresponding to the coding block.</p><p id="p-0139" num="0126">It is also to be noted that after the prediction coding on the coding block, the encoder will send a bitstream corresponding to the coding block to the decoder. In this way, at the decoder side, the decoding parameter can be determined according to a coding parameter and the candidate list in the bitstream.</p><p id="p-0140" num="0127">It can be seen that according to the solution provided by the implementation, search positions can be determined based on N search directions including the diagonal search directions and multiple types of search step lengths. In this way, compared with the related art, the number of search step lengths in each direction is reduced, thereby avoiding a problem of reduced coding efficiency due to that some of the search step lengths are not selected, and by increasing the search directions, it can be ensured that motion conditions of most objects can be satisfied, so that motion information can be expressed more accurately.</p><p id="p-0141" num="0128">An implementation provides a decoder. As illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, the decoder includes an expanding part <b>51</b> and a decoding part <b>52</b>.</p><p id="p-0142" num="0129">The expanding part <b>51</b> is configured to expand at least one search position corresponding to an initial motion vector in a search position constellation based on syntax elements of the initial motion vector, where the syntax elements include an initial motion vector index, a search direction index, and a search step length direction, and a search direction indicated in the search direction index at least includes a diagonal search direction.</p><p id="p-0143" num="0130">The decoding part <b>52</b> is configured to perform motion compensation based on a new motion vector that is obtained by expanding the initial motion vector to obtain a reconstructed block.</p><p id="p-0144" num="0131">The method provided in the implementation can be applied to a decoder end in a video coding and decoding system.</p><p id="p-0145" num="0132">In general, according to the above solution, available search points are first allocated according to distances in multiple search directions, the search position constellation is constructed, and then a suitable number of search positions are selected from the constellation according to a range coverage rule.</p><p id="p-0146" num="0133">That is to say, in the implementation, original 4 search directions are expanded to 8 search directions on the premise that there are 32 preset search positions, then the number of search step lengths in each of the 8 search directions is reduced, and search step lengths are respectively allocated to different search directions to cover a relatively wide search range.</p><p id="p-0147" num="0134">Specifically, the expanding part is configured to construct the search position constellation with a point pointed by the initial motion vector as a center point.</p><p id="p-0148" num="0135">Search directions in the search position constellation further include horizontal and vertical search directions. The horizontal and vertical search directions are respectively: up, down, left, and right. The diagonal search directions are respectively: upper left, lower left, upper right, and lower right.</p><p id="p-0149" num="0136">The search step length corresponds to a type of a search direction.</p><p id="p-0150" num="0137">The expanding part <b>51</b> is configured to adopt a first set of search step lengths if the type of the search direction is the horizontal or vertical search direction, and adopt a second set of search step lengths if the type of the search direction is the diagonal search direction.</p><p id="p-0151" num="0138">On the premise that there are 32 preset search positions, the first set of search step lengths includes at least one type of first search step lengths, and the second set of search step lengths includes at least one type of second search step lengths.</p><p id="p-0152" num="0139">It can be pointed out that in the implementation, the first set of search step lengths and the second set of search step lengths may be the same or different.</p><p id="p-0153" num="0140">If the first set of search step lengths and the second set of search step lengths are the same, it can be considered that same 4 search step lengths (a step length is a distance between a search position and the center point of the constellation) are used in the horizontal and vertical directions and the diagonal directions, respectively, to select search positions.</p><p id="p-0154" num="0141">Alternatively, the first set of search step lengths and the second set of search step lengths are different from each other. In this case, the first set of search step lengths and the second set of search step lengths may intersect. For example, search step lengths in the horizontal and vertical directions and search step lengths in the diagonal directions are increased staggeredly. For example, distances in the horizontal and vertical directions are preferentially selected, and then each of search step lengths in the diagonal direction is intersected between two adjacent search step lengths in the horizontal and vertical directions. In this way, the search in the diagonal directions can cover areas that cannot be searched by search in the horizontal and vertical directions.</p><p id="p-0155" num="0142">That is to say, a second search step length of first type in the second set of search step lengths may be between a first search step length of first type and a first search step length of second type in the first set of search step lengths. Correspondingly, a first search step length of third type in the first set of search step lengths may be between a second search step length of second type and a second search step length of third type in the second set of search step lengths.</p><p id="p-0156" num="0143">A diagonal position is expressed as (lx, ly), where lx=ly, and the search step length is expressed as a single value lx in this specification.</p><p id="p-0157" num="0144">For example, the first search step length may be at least one of the following: &#xbc;-pixel, &#xbd;-pixel, 2-pixel, 4-pixel, and the second search step length may be at least one of the following: &#x215b;-pixel, &#x215c;-pixel, &#xbe;-pixel, and 3/2-pixel.</p><p id="p-0158" num="0145"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a schematic structural diagram illustrating a video decoding system. As illustrated in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the video decoding system <b>200</b> includes the header information decoding and CABAC decoding module <b>201</b>, the inverse transform and inverse quantization module <b>202</b>, the intra prediction module <b>203</b>, the motion compensation module <b>204</b>, the deblocking filtering and SAO filtering module <b>205</b>, the decoded picture buffer <b>206</b>, and so on. A video picture is processed by parts in the video coding system <b>100</b> such as the transform and quantization module <b>101</b>, the intra estimation module <b>102</b>, the intra prediction module <b>103</b>, the motion compensation module <b>104</b>, the motion estimation module <b>105</b>, the inverse transform and inverse quantization module <b>106</b>, the filter control analysis module <b>107</b>, the deblocking filtering and SAO filtering module <b>108</b>, the header information coding and CABAC coding module <b>109</b>, the decoded picture buffer <b>110</b>, and so on to output a bitstream of the video picture. The bitstream is input into the video decoding system <b>200</b> and processed by parts of the video decoding system <b>200</b> such as the header information decoding and CABAC decoding module <b>201</b>, the inverse transform and inverse quantization module <b>202</b>, the intra prediction module <b>203</b>, the motion compensation module <b>204</b>, and so on, and finally the original video picture is restored.</p><p id="p-0159" num="0146">Further, the image coding method proposed in the present disclosure can affect an intra prediction part in a video coding hybrid framework, that is, is mainly applied to the motion compensation module <b>104</b> and the motion estimation module <b>105</b> in video coding and the motion compensation module <b>204</b> in video decoding, i.e., acts on both a coding end and a decoding end.</p><p id="p-0160" num="0147">Based on the foregoing solution, the complexity of the decoding end can be kept unchanged. In the related art, at the decoding end, expression expansion on the initial motion vector is performed by decoding three syntax elements, so as to perform motion compensation and finally obtain a reconstructed block. In the solution, expression expansion on the initial motion vector is performed still by decoding three syntax elements, so the complexity remains unchanged.</p><p id="p-0161" num="0148">It is to be noted that, for a coding block in an original picture frame, at the decoder side, a coding mode adopted by the coding block is first determined, then based on the determined coding mode, a candidate list corresponding to the coding mode is established. According to the candidate list corresponding to the coding mode, a decoding parameter corresponding to the coding block is obtained. Finally, prediction decoding is performed on the coding block based on the decoding parameter corresponding to the coding block.</p><p id="p-0162" num="0149">It is also to be noted that after the prediction coding on the coding block, the encoder will send a bitstream corresponding to the coding block to the decoder. In this way, at the decoder side, the decoding parameter can be determined according to a coding parameter and the candidate list in the bitstream.</p><p id="p-0163" num="0150">It can be seen that according to the solution provided by the implementation, search positions can be determined based on N search directions including the diagonal search directions and multiple types of search step lengths. In this way, compared with the related art, the number of search step lengths in each direction is reduced, thereby avoiding a problem of reduced coding efficiency due to that some of the search step lengths are not selected, and by increasing the search directions, it can be ensured that motion conditions of most objects can be satisfied, so that motion information can be expressed more accurately.</p><p id="p-0164" num="0151"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a schematic diagram illustrating hardware constitution provided by an implementation of the present disclosure. Hardware in the figure may be the aforementioned encoder or decoder. As illustrated in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, an encoder <b>300</b> provided in the implementation of the present disclosure may also include a processor <b>304</b>, a memory <b>305</b> storing instructions capable of being executed by the processor <b>304</b>, a communication interface <b>306</b>, and a bus <b>307</b> configured to connect the processor <b>304</b>, the memory <b>305</b>, and the communication interface <b>306</b>.</p><p id="p-0165" num="0152">Further, in the implementation of the present disclosure, the aforementioned processor <b>304</b> is configured to execute processing in the aforementioned encoder or decoder.</p><p id="p-0166" num="0153">In the implementation of the present disclosure, the above-mentioned processor <b>304</b> may be at least one of an application specific integrated circuit (ASIC), a digital signal processor (DSP), a digital signal processing device (DSPD), a programmable logic device (PLD), a field programmable gate array (FPGA), a central processing unit (CPU), a controller, a microcontroller, and a microprocessor. It is to be understood that for different apparatuses, the electronic devices used to implement the above-mentioned processor functions may also be other, which is not specifically limited in the implementation of the present disclosure. The memory <b>305</b> may be connected to the processor <b>304</b>, where the memory <b>305</b> is configured to store executable program codes. The program codes include computer operation instructions. The memory <b>305</b> may include a high-speed random access memory (RAM), and may also include a non-volatile memory, for example, at least two disk storage devices.</p><p id="p-0167" num="0154">In the implementation of the present disclosure, the bus <b>307</b> is configured to connect the communication interface <b>306</b>, the processor <b>304</b>, and the memory <b>305</b>, and used for mutual communication among these devices.</p><p id="p-0168" num="0155">In the implementation of the present disclosure, the memory <b>305</b> is configured to store instructions and data.</p><p id="p-0169" num="0156">In practical applications, the aforementioned memory <b>305</b> may be a volatile memory, such as a RAM, or a non-volatile memory such as a read-only memory (ROM), a flash memory, a hard disk drive (HDD) or a solid-state drive (SSD), or a combination of the above types of memory, and can provide the processor with instructions and data.</p><p id="p-0170" num="0157">In addition, the functional modules in the implementation may be integrated into one processing unit, or each unit may exist alone physically, or two or more units may be integrated into one unit. The above-mentioned integrated unit can be realized in the form of hardware or a software function module.</p><p id="p-0171" num="0158">If the integrated unit is implemented in the form of a software function module and is not sold or used as an independent product, it can be stored in a computer-readable storage medium. Based on this understanding, in the implementation, the essential technical solution, the part that contributes to the related art, or all or part of the technical solution can be embodied in the form of a software product. The computer software product is stored in a storage medium and includes various instructions to enable a computer device (which may be a personal computer, a server, or a network device, etc.) or a processor to execute all or part of the steps of the method in the implementation. The aforementioned storage medium includes various media that can store program codes such as a U disk, a mobile hard disk, a ROM, a RAM, a magnetic disk, or an optical disk.</p><p id="p-0172" num="0159">An implementation of the present disclosure provides a picture encoder configured to perform processing in the aforementioned encoder.</p><p id="p-0173" num="0160">An implementation of the present disclosure further provides a decoder configured to perform processing in the aforementioned decoder.</p><p id="p-0174" num="0161">An implementation of the present disclosure provides a computer-readable storage medium that stores a program. When the program is executed by a processor, the method as described in the above-mentioned implementation is implemented.</p><p id="p-0175" num="0162">Those skilled in the art can understand that the implementation of the present disclosure can provide a method, a system, or a computer program product. Therefore, the present disclosure may adopt the form of a hardware implementation, a software implementation, or an implementation combining software and hardware. Moreover, the present disclosure may adopt the form of a computer program product implemented on one or more computer-usable storage media (including but not limited to a disk memory, an optical memory, etc.) including computer-usable program codes.</p><p id="p-0176" num="0163">The present disclosure is described with reference to flow charts and/or block diagrams of the methods, equipment (systems), and computer program products according to the implementations of the present disclosure. It can be understood that each process and/or block in the schematic flow chart and/or block diagram or a combination of processes and/or blocks in the schematic flow chart or block diagram can be realized by computer program instructions. These computer program instructions can be provided to a processor of a general-purpose computer, a special-purpose computer, an embedded processor, or other programmable data processing equipment to generate a machine, so that instructions executed by the processor of the computer or other programmable data processing equipment generate an apparatus for implementing functions specified in one process or multiple processes in the schematic flow chart and/or one block or multiple blocks in the block diagram.</p><p id="p-0177" num="0164">These computer program instructions can also be stored in a computer-readable memory that can direct a computer or other programmable data processing equipment to work in a specific manner, so that instructions stored in the computer-readable memory produce a manufacture article including an instruction apparatus. The instruction apparatus realizes functions specified in one or more processes in the schematic flow diagram and/or one block or more blocks in the block diagram.</p><p id="p-0178" num="0165">These computer program instructions can also be loaded on a computer or other programmable data processing equipment, so that a series of operation steps are executed on the computer or other programmable equipment to produce computer-implemented processing, so that instructions executed on the computer or other programmable equipment are used for implementing functions specified in one or more processes in the schematic flow chart and/or one block or more blocks in the block diagram.</p><p id="p-0179" num="0166">The above are only preferred implementations of the present disclosure, and are not used to limit the protection scope of the present disclosure.</p><heading id="h-0007" level="1">INDUSTRIAL APPLICABILITY</heading><p id="p-0180" num="0167">The implementations of the present disclosure provide the processing method for motion compensation, the encoder, and the storage medium. The encoder constructs the search position constellation based on the initial motion vector, where the search position constellation includes the N search directions, and at least one search point is obtained by searching with at least one type of search step length in the search direction, Nis an integer greater than or equal to 1, and the N search directions at least include the diagonal search directions. The encoder selects at least one search position from the search position constellation based on a preset range coverage rule, where the preset range coverage rule is to scatter a preset number of search points in the N search directions for selection. The encoder sets at least one search position as the new motion vector, and performs motion compensation on the new motion vector to obtain the prediction value of the CU. In this way, compared with the related art, the number of search step lengths in each direction is reduced, thereby avoiding the problem of reduced coding efficiency due to that some of the search step lengths are not selected, and by increasing the search directions, it can be ensured that motion conditions of most objects can be satisfied, so that motion information can be expressed more accurately.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A processing method for motion compensation, applied to a decoder and comprising:<claim-text>determining an initial motion vector based on a merge candidate list;</claim-text><claim-text>determining a search position constellation with a point pointed to by the initial motion vector as a center point, wherein</claim-text><claim-text>the search position constellation comprises N directions,</claim-text><claim-text>N is an integer larger than or equal to two,</claim-text><claim-text>N directions include a first type of direction and a second type of direction,</claim-text><claim-text>the first type of direction includes a horizontal direction or a vertical direction,</claim-text><claim-text>the horizontal direction includes left or right relative to the center point, or the vertical direction includes up or down relative to the center point,</claim-text><claim-text>the second type of direction includes a diagonal direction,</claim-text><claim-text>the diagonal direction includes upper left, lower left, upper right, or lower right relative to the center point,</claim-text><claim-text>determining the search position constellation includes searching for one or more search points by using sets of step lengths in the N directions,</claim-text><claim-text>searching for the one or more search points includes adopting a first set of step lengths for the first type of direction and adopting a second set of step lengths for the second type of direction,</claim-text><claim-text>adjacent step lengths of the first set are separated from one another by a same first distance as other adjacent step lengths of the first set,</claim-text><claim-text>adjacent step lengths of the second set are separated from one another by a same second distance as other adjacent step lengths of the second set, and</claim-text><claim-text>the one or more search points comprise (a) at least one search point in the diagonal direction and (b) at least one search point in the horizontal direction or the vertical direction;</claim-text><claim-text>searching for at least one search position from the search position constellation based on a preset range coverage rule, wherein the preset range coverage rule is to arrange search points in the directions for search;</claim-text><claim-text>obtaining a new motion vector according to the at least one search position; and</claim-text><claim-text>performing motion compensation based on the new motion vector to obtain a prediction value of a coding unit (CU).</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the second same distance is different from the first same distance.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first and second sets of step lengths include at least one of the following step lengths: &#x215b;-pixel, &#xbc;-pixel, &#x215c; pixel, &#xbd;-pixel, &#xbe;-pixel, 3/2-pixel, 2-pixel, or 4-pixel.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. A method for motion compensation, applied to an encoder and comprising:<claim-text>determining an initial motion vector based on a merge candidate list;</claim-text><claim-text>determining a search position constellation with a point pointed to by the initial motion vector as a center point, wherein</claim-text><claim-text>the search position constellation comprises N directions,</claim-text><claim-text>N is an integer larger than or equal to two,</claim-text><claim-text>N directions include a first type of direction and a second type of direction,</claim-text><claim-text>the first type of direction includes a horizontal direction or a vertical direction,</claim-text><claim-text>the horizontal direction includes left or right relative to the center point, or the vertical direction includes up or down relative to the center point,</claim-text><claim-text>the second type of direction includes a diagonal direction,</claim-text><claim-text>the diagonal direction includes upper left, lower left, upper right, or lower right relative to the center point,</claim-text><claim-text>determining the search position constellation includes searching for one or more search points by using sets of step lengths in the N directions,</claim-text><claim-text>searching for the one or more search points includes adopting a first set of step lengths for the first type of direction and adopting a second set of step lengths for the second type of direction,</claim-text><claim-text>adjacent step lengths of the first set are separated from one another by a same first distance as other adjacent step lengths of the first set,</claim-text><claim-text>adjacent step lengths of the second set are separated from one another by a same second distance as other adjacent step lengths of the second set, and</claim-text><claim-text>the one or more search points comprise (a) at least one search point in the diagonal direction and (b) at least one search point in the horizontal direction or the vertical direction;</claim-text><claim-text>searching for at least one search position from the search position constellation based on a preset range coverage rule, wherein the preset range coverage rule is to arrange search points in the directions for search;</claim-text><claim-text>obtaining a new motion vector according to the at least one search position; and</claim-text><claim-text>performing motion compensation based on the new motion vector to obtain a prediction value of a coding unit (CU).</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the second same distance is different from the first same distance.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the first and second sets of step lengths include at least one of the following step lengths: &#x215b;-pixel, &#xbc;-pixel, &#x215c;-pixel, &#xbd;-pixel, &#xbe;-pixel, 3/2-pixel, 2-pixel, or 4-pixel.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. A decoder, comprising:<claim-text>a processor; and</claim-text><claim-text>a memory storing a computer program which, when executed by the processor, causes the processor to:</claim-text><claim-text>determine an initial motion vector based on a merge candidate list;</claim-text><claim-text>determine a search position constellation with a point pointed to by the initial motion vector as a center point, wherein</claim-text><claim-text>the search position constellation comprises N directions,</claim-text><claim-text>N is an integer larger than or equal to two,</claim-text><claim-text>N directions include a first type of direction and a second type of direction,</claim-text><claim-text>the first type of direction includes a horizontal direction or a vertical direction,</claim-text><claim-text>the horizontal direction includes left or right relative to the center point, or the vertical direction includes up or down relative to the center point,</claim-text><claim-text>the second type of direction includes a diagonal direction,</claim-text><claim-text>the diagonal direction includes upper left, lower left, upper right, or lower right relative to the center point,<claim-text>determining the search position constellation includes searching for one or more search points by using sets of step lengths in the N directions,</claim-text><claim-text>searching for the one or more search points includes adopting a first set of step lengths for the first type of direction and adopting a second set of step lengths for the second type of direction,</claim-text><claim-text>adjacent step lengths of the first set are separated from one another by a same first distance as other adjacent step lengths of the first set,</claim-text><claim-text>adjacent step lengths of the second set are separated from one another by a same second distance as other adjacent step lengths of the second set, and</claim-text><claim-text>the one or more search points comprise (a) at least one search point in the diagonal direction and (b) at least one search point in the horizontal direction or the vertical direction;</claim-text><claim-text>search for at least one search position from the search position constellation based on a preset range coverage rule, wherein the preset range coverage rule is to arrange search points in the directions for search;</claim-text><claim-text>obtain a new motion vector according to the at least one search position; and</claim-text><claim-text>perform motion compensation based on the new motion vector to obtain a prediction value of a coding unit (CU).</claim-text></claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The decoder of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the second same distance is different from the first same distance.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The decoder of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the first and second sets of step lengths include at least one of the following step lengths: &#x215b;-pixel, &#xbc;-pixel, &#x215c;-pixel, &#xbd;-pixel, &#xbe;-pixel, 3/2-pixel, 2-pixel, or 4-pixel.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. An encoder, comprising:<claim-text>a processor; and</claim-text><claim-text>a memory storing a computer program which, when executed by the processor, causes the processor to:<claim-text>determine an initial motion vector based on a merge candidate list;</claim-text><claim-text>determine a search position constellation with a point pointed to by the initial motion vector as a center point, wherein</claim-text><claim-text>the search position constellation comprises N directions,</claim-text><claim-text>N is an integer larger than or equal to two,</claim-text><claim-text>N directions include a first type of direction and a second type of direction,</claim-text><claim-text>the first type of direction includes a horizontal direction or a vertical direction,</claim-text><claim-text>the horizontal direction includes left or right relative to the center point, or the vertical direction includes up or down relative to the center point,</claim-text><claim-text>the second type of direction includes a diagonal direction,</claim-text><claim-text>the diagonal direction includes upper left, lower left, upper right, or lower right relative to the center point,</claim-text><claim-text>determining the search position constellation includes searching for one or more search points by using sets of step lengths in the N directions,</claim-text><claim-text>searching for the one or more search points includes adopting a first set of step lengths for the first type of direction and adopting a second set of step lengths for the second type of direction,</claim-text><claim-text>adjacent step lengths of the first set are separated from one another by a same first distance as other adjacent step lengths of the first set,</claim-text><claim-text>adjacent step lengths of the second set are separated from one another by a same second distance as other adjacent step lengths of the second set, and</claim-text><claim-text>the one or more search points comprise (a) at least one search point in the diagonal direction and (b) at least one search point in the horizontal direction or the vertical direction;</claim-text><claim-text>search for at least one search position from the search position constellation based on a preset range coverage rule, wherein the preset range coverage rule is to arrange search points in the directions for search;</claim-text><claim-text>obtain a new motion vector according to the at least one search position; and</claim-text><claim-text>perform motion compensation based on the new motion vector to obtain a prediction value of a coding unit (CU).</claim-text></claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The encoder of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the second same distance is different from the first same distance.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The encoder of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the first and second sets of step lengths include at least one of the following step lengths: &#x215b;-pixel, &#xbc;-pixel, &#x215c;-pixel, &#xbd;-pixel, &#xbe;-pixel, 3/2-pixel, 2-pixel, or 4-pixel.</claim-text></claim></claims></us-patent-application>