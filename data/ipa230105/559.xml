<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230000560A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230000560</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17899370</doc-number><date>20220830</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>34</main-group><subgroup>10</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>34</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20160201</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>34</main-group><subgroup>10</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20160201</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>34</main-group><subgroup>25</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>7435</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>743</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>748</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20160201</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>2034</main-group><subgroup>256</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20160201</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>2034</main-group><subgroup>108</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20160201</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>2034</main-group><subgroup>102</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20160201</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>2034</main-group><subgroup>107</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">SYSTEMS AND METHODS FOR PHYSICIAN DESIGNED SURGICAL PROCEDURES</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>17702591</doc-number><date>20220323</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11497559</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17899370</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>17497546</doc-number><date>20211008</date></document-id><parent-status>PENDING</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>17702591</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16048167</doc-number><date>20180727</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11166764</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17497546</doc-number></document-id></child-doc></relation></continuation><us-provisional-application><document-id><country>US</country><doc-number>62537869</doc-number><date>20170727</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Carlsmed, Inc.</orgname><address><city>Carlsbad</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Roh</last-name><first-name>Jeffrey</first-name><address><city>Seattle</city><state>WA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Esterberg</last-name><first-name>Justin</first-name><address><city>Mercer Island</city><state>WA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Systems and methods for providing assistance to a surgeon during an implant surgery are disclosed. A method includes defining areas of interest in diagnostic data of a patient and defining a screw bone type based on the surgeon's input. Post defining the areas of interest, salient points are determined for the areas of interest. Successively, an XZ angle, an XY angle, and a position entry point for a screw are determined based on the salient points of the areas of interest. Successively, a maximum screw diameter and a length of the screw are determined based on the salient points. Thereafter, the screw is identified and suggested to the surgeon for usage during the implant surgery.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="183.47mm" wi="138.35mm" file="US20230000560A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="200.32mm" wi="140.38mm" file="US20230000560A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="196.68mm" wi="163.24mm" file="US20230000560A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="120.14mm" wi="67.14mm" file="US20230000560A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="226.99mm" wi="131.23mm" file="US20230000560A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="221.57mm" wi="167.98mm" orientation="landscape" file="US20230000560A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="220.47mm" wi="158.41mm" orientation="landscape" file="US20230000560A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="242.74mm" wi="154.01mm" orientation="landscape" file="US20230000560A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="158.58mm" wi="111.08mm" file="US20230000560A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="120.99mm" wi="111.76mm" file="US20230000560A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="173.14mm" wi="111.76mm" file="US20230000560A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="173.14mm" wi="111.76mm" file="US20230000560A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="120.99mm" wi="111.76mm" file="US20230000560A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application is a continuation of U.S. patent application Ser. No. 17/702,591, filed on Mar. 23, 2022, titled &#x201c;SYSTEMS AND METHODS FOR PHYSICIAN DESIGNED SURGICAL PROCEDURES,&#x201d; which is a continuation of U.S. patent application Ser. No. 17/497,546, filed on Oct. 8, 2021, titled &#x201c;SYSTEMS AND METHODS FOR ASSISTING AND AUGMENTING SURGICAL PROCEDURES,&#x201d; which is a continuation of U.S. patent application Ser. No. 16/048,167, filed on Jul. 27, 2018 (now U.S. Pat. No. 11,166,764), titled &#x201c;SYSTEMS AND METHODS FOR ASSISTING AND AUGMENTING SURGICAL PROCEDURES,&#x201d; which claims priority to U.S. Provisional Patent Application No. 62/537,869, filed on Jul. 27, 2017, titled &#x201c;SYSTEMS AND METHODS OF PROVIDING ASSISTANCE DURING A SPINAL SURGERY,&#x201d; which are herein incorporated by reference in their entireties. All of these applications and patent are incorporated by reference in their entireties.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">The present disclosure is generally related to providing surgical assistance to a surgeon, and more particularly for providing surgical assistance for a surgical procedure.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">Assessing spinal deformity is of tremendous importance for a number of disorders affecting human spine. A pedicle is a dense stem-like structure that projects from the posterior of a vertebra. There are two pedicles per vertebra that connect to structures like a lamina and a vertebral arch. Conventionally available screws, used in spinal surgeries, are poly-axial pedicle screws made of titanium. Titanium is chosen as it is highly resistant to corrosion and fatigue, and is easily visible in MRI images.</p><p id="p-0005" num="0004">The pedicle screws were originally placed via a free-hand technique. Surgeons performing spinal surgeries merely rely on their experience and knowledge of known specific paths for performing the spinal surgeries. The free-hand techniques used by spinal surgeons rely on spinal anatomy of a patient. The spinal surgeon relies on pre-operative imaging and intra-operative anatomical landmarks for performing the spinal surgery. Assistive fluoroscopy and navigation are helpful in that they guide pedicle screw placement more or less in a real-time, but are limited by time and costs involved in fluoroscopy, and significant radiation exposure during fluoroscopy.</p><p id="p-0006" num="0005">The subject matter discussed in the background section should not be assumed to be prior art merely as a result of its mention in the background section. Similarly, a problem mentioned in the background section or associated with the subject matter of the background section should not be assumed to have been previously recognized in the prior art. The subject matter in the background section merely represents different approaches, which in and of themselves may also correspond to implementations of the claimed technology.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0007" num="0006">The accompanying drawings illustrate various embodiments of systems, methods, and embodiments of various other aspects of the disclosure. Any person with ordinary skills in the art will appreciate that the illustrated element boundaries (e.g. boxes, groups of boxes, or other shapes) in the figures represent one example of the boundaries. It may be that in some examples one element may be designed as multiple elements or that multiple elements may be designed as one element. In some examples, an element shown as an internal component of one element may be implemented as an external component in another, and vice versa. Furthermore, elements may not be drawn to scale. Non-limiting and non-exhaustive descriptions are described with reference to the following drawings. The components in the figures are not necessarily to scale, emphasis instead being placed upon illustrating principles.</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>1</b>A</figref> illustrates a system for providing assistance prior to or during an implant surgery, according to an embodiment.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>1</b>B</figref> illustrates a network connection diagram <b>100</b> of an implant surgery assistance system for providing assistance prior to or during an implant surgery, according to an embodiment.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates a block diagram showing components of an implant surgery assistance system, according to an embodiment.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>3</b>A</figref> shows salient points presented in a top view of a vertebra of the patient, according to an embodiment.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>3</b>B</figref> shows salient points present in a side view of the vertebra of the patient, according to an embodiment.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>4</b>A</figref> illustrates computations for determining an XZ angle for a spinal screw, according to an embodiment.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>4</b>B</figref> illustrates computations for determining an XY angle for a spinal screw, according to an embodiment.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates a spinal screw and dimensions of the spinal screw, according to an embodiment.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates a flowchart showing a method for providing assistance to the surgeon during the spinal surgery, according to an embodiment.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates a flowchart showing a method for generating implant configurations.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>8</b>A</figref> illustrates a flowchart showing a method for applying analysis procedures that can utilize machine learning models, according to an embodiment.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>8</b>B</figref> illustrates a flowchart showing a method for applying analysis procedures that can utilize virtual models, according to an embodiment.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>9</b></figref> illustrates a flowchart showing a method for training a machine learning model, according to an embodiment.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0021" num="0020">Embodiments of the present disclosure will be described more fully hereinafter with reference to the accompanying drawings in which like numerals represent like elements throughout the several figures, and in which example embodiments are shown. Embodiments of the claims may, however, be embodied in many different forms and should not be construed as limited to the embodiments set forth herein. The examples set forth herein are non-limiting examples and are merely examples among other possible examples.</p><p id="p-0022" num="0021">The words &#x201c;comprising,&#x201d; &#x201c;having,&#x201d; &#x201c;containing,&#x201d; and &#x201c;including,&#x201d; and other forms thereof, are intended to be equivalent in meaning and be open ended in that an item or items following any one of these words is not meant to be an exhaustive listing of such item or items, or meant to be limited to only the listed item or items.</p><p id="p-0023" num="0022">It must also be noted that as used herein and in the appended claims, the singular forms &#x201c;a,&#x201d; &#x201c;an,&#x201d; and &#x201c;the&#x201d; include plural references unless the context clearly dictates otherwise. Although any systems and methods similar or equivalent to those described herein can be used in the practice or testing of embodiments of the present disclosure, the preferred, systems and methods are now described.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>1</b>A</figref> illustrates a system <b>152</b> for providing assistance prior to or during an implant surgery, according to an embodiment. The system <b>152</b> can improve surgeries that involve implants by guiding selection and application of implants, delivery instruments, navigation tools, or the like. The system <b>152</b> can comprise hardware components that improve surgeries using, for example, a surgical assistance system <b>164</b>. In various implementations, the surgical assistance system <b>164</b> can obtain implant surgery information, converting the implant surgery information into a form compatible with an analysis procedure, applying the analysis procedure to obtain results, and using the results to provide a configuration for the implant surgery.</p><p id="p-0025" num="0024">An implant configuration can include characteristics of an implant such as various dimensions, angles, materials, application features (e.g., implant sizes, implant functionality, anchoring features, suture type, etc.), and/or aspects of applying the implant such as insertion point, delivery path, implant position/angle, rotation, amounts of force to apply (e.g., torque applied to a screw, rotational speed of a screw, rate of expansion of expandable implants, and so forth), etc. In some implementations, the implant surgery information can include images of a target area, such as MRI scans of a spine, patient information such as sex, weight, etc., or a surgeon's pre-operative plan. The surgical assistance system <b>164</b> can convert the implant surgery information, for example, by converting images into arrays of integers or histograms, entering patient information into feature vectors, or extracting values from the pre-operative plan.</p><p id="p-0026" num="0025">In some implementations, surgical assistance system <b>164</b> can analyze one or more images of a patient to identify one or more features of interest. The features of interest can include, without limitation, implantation sites, targeted features, non-targeted features, access paths, anatomical structures, or combinations thereof. The implantation sites can be determined based upon one or more of risk factors, patient information, surgical information, or combinations thereof. The risk factors can be determined by the surgical assistant system based upon the patient's medical history. For example, if the patient is susceptible to infections, the surgical assistant system <b>164</b> can recommend a minimally invasive procedure whereas the surgical assistant system may recommend open procedure access paths for patients less susceptible to infection. In some implementations, the physician can provide the risk factors before or during the procedure. Patient information can include, without limitation, patient sex, age, health rating, or the like. The surgical information can include available navigation systems, robotic surgery platforms, access tools, surgery kits, or the like.</p><p id="p-0027" num="0026">In some implementations, surgical assistance system <b>164</b> can apply analysis procedures by supplying the converted implant surgery information to a machine learning model trained to select implant configurations. For example, a neural network model can be trained to select pedicle screw configurations for a spinal surgery. The neural network can be trained with training items each comprising a set of images scans (e.g. camera, MRI, CT, x-ray, etc.) and patient information, an implant configuration used in the surgery, and/or a scored surgery outcome resulting from one or more of: surgeon feedback, patient recovery level, recovery time, results after a set number of years, etc. This neural network can receive the converted surgery information and provide output indicating the pedicle screw configuration.</p><p id="p-0028" num="0027">In other implementations, surgical assistance system <b>164</b> can apply the analysis procedure by A) localizing and classifying a surgery target, B) segmenting the target to determine boundaries, C) localizing optimal implant insertion points, D) identifying target structures (e.g. pedicles and isthmus), and/or computing implant configurations based on results of A-D.</p><p id="p-0029" num="0028">In yet further implementations, surgical assistance system <b>164</b> can apply the analysis procedure by building a virtual model of a surgery target area, localizing and classifying areas of interest within the virtual model, segmenting areas of interest, localizing insertion points, and computing implant configurations by simulating implant insertions in the virtual model. Each of the individual steps of these implementations can be accomplished using a machine learning model trained (as discussed below) to identify appropriate results for that step or by applying a corresponding algorithm. For example, an algorithm can measure an isthmus by determining an isthmus width in various images and tracking the minimal value across the images in different planes.</p><p id="p-0030" num="0029">In another example, surgical assistance system <b>164</b> can apply the analysis procedure by performing a finite element analysis on a generated three-dimensional model (e.g., a model of the patient's anatomy) to assess stresses, strains, deformation characteristics (e.g., load deformation characteristics), fracture characteristics (e.g., fracture toughness), fatigue life, etc. A virtual representation of the implant or other devices could be generated. The surgical assistance system <b>164</b> can generate a three-dimensional mesh to analyze the model. Machine learning techniques to create an optimized mesh based on a dataset of vertebrae or other bones and implants or other devices. After performing the analysis, the results could be used to refine the selection of screws or other implant components.</p><p id="p-0031" num="0030">The surgical assistance system <b>164</b> can incorporate results from the analysis procedure in suggestions for the implant surgery. For example, the results can be used to indicate suggested implants for a procedure, to annotate an image with suggested insertions points and angles, to generate a virtual reality or augmented reality representation including the suggested implant configurations, to provide warnings or other feedback to surgeons during a procedure, to automatically order the necessary implants, to generate surgical technique information (e.g., insertion forces/torques, imaging techniques, delivery instrument information, or the like), etc.</p><p id="p-0032" num="0031">The surgical assistance system <b>164</b> can improve the efficiency, precision, and/or efficacy of implant surgeries by providing more optimal implant configuration guidance. This can reduce operational risks and costs produced by surgical complications, reduce the resources required for preoperative planning efforts, and reduce the need for extensive implant variety to be prepared prior to an implant surgery. The surgical assistance system <b>164</b> provides increased precision and efficiency for patients and surgeons.</p><p id="p-0033" num="0032">In orthopedic surgeries, the surgical assistance system <b>164</b> can select or recommend implants (e.g., permanent implants, removable implants, etc.), surgical techniques, patient treatment plans, or the like. For example, the implants can be joint replacements, hip implants, removable bone screws, or the like. The surgical techniques can include access instruments selected based on one or more criteria, such as risk of adverse events, optical implant position, protected zones (e.g., zones with nerve tissue), or the like. In spinal surgeries, the surgical assistance system <b>164</b> can reduce incorrect selection of pedicle screw types, dimensions, and trajectories while making surgeons more efficient and precise, as compared to existing surgical procedures.</p><p id="p-0034" num="0033">The surgical assistance system <b>164</b> can also improve surgical robotics/navigation systems, providing improved intelligence for selecting implant application parameters. For example, the surgical assistance system <b>164</b> empowers surgical robots and navigation systems for spinal surgeries to increase procedure efficiency and reduce surgery duration by providing information on types and sizes, along with expected insertion angles. In addition, hospitals benefit from reduced surgery durations and reduced costs of purchasing, shipping, and storing alternative implant options. Medical imaging and viewing technologies can integrate with the surgical assistance system <b>164</b>, to provide more intelligent and intuitive results.</p><p id="p-0035" num="0034">The surgical assistance system <b>164</b> can be incorporated in system <b>152</b>, which can include one or more input devices <b>120</b> that provide input to the processor(s) <b>145</b> (e.g. CPU(s), GPU(s), HPU(s), etc.), notifying it of actions. The actions can be mediated by a hardware controller that interprets the signals received from the input device and communicates the information to the processors <b>145</b> using a communication protocol. Input devices <b>120</b> include, for example, a mouse, a keyboard, a touchscreen, an infrared sensor, a touchpad, a wearable input device, a camera- or image-based input device, a microphone, or other user input devices.</p><p id="p-0036" num="0035">Processors <b>145</b> can be a single processing unit or multiple processing units in a device or distributed across multiple devices. Processors <b>145</b> can be coupled to other hardware devices, for example, with the use of a bus, such as a PCI bus or SCSI bus. The processors <b>145</b> can communicate with a hardware controller for devices, such as for a display <b>130</b>. Display <b>130</b> can be used to display text and graphics. In some implementations, display <b>130</b> provides graphical and textual visual feedback to a user. In some implementations, display <b>130</b> includes the input device as part of the display, such as when the input device is a touchscreen or is equipped with an eye direction monitoring system. In some implementations, the display is separate from the input device. Examples of display devices are: an LCD display screen, an LED display screen, a projected, holographic, or augmented reality display (such as a heads-up display device or a head-mounted device), and so on. Other I/O devices <b>140</b> can also be coupled to the processor, such as a network card, video card, audio card, USB, firewire or other external device, camera, printer, speakers, CD-ROM drive, DVD drive, disk drive, or Blu-Ray device. Other I/O <b>140</b> can also include input ports for information from directly connected medical equipment such as MRI machines, X-Ray machines, etc. Other I/O <b>140</b> can further include input ports for receiving data from these types of machine from other sources, such as across a network or from previously captured data, e.g. stored in a database.</p><p id="p-0037" num="0036">In some implementations, the system <b>152</b> also includes a communication device capable of communicating wirelessly or wire-based with a network node. The communication device can communicate with another device or a server through a network using, for example, TCP/IP protocols. System <b>152</b> can utilize the communication device to distribute operations across multiple network devices.</p><p id="p-0038" num="0037">The processors <b>145</b> can have access to a memory <b>150</b> in a device or distributed across multiple devices. A memory includes one or more of various hardware devices for volatile and non-volatile storage, and can include both read-only and writable memory. For example, a memory can comprise random access memory (RAM), various caches, CPU registers, read-only memory (ROM), and writable non-volatile memory, such as flash memory, hard drives, floppy disks, CDs, DVDs, magnetic storage devices, tape drives, device buffers, and so forth. A memory is not a propagating signal divorced from underlying hardware; a memory is thus non-transitory. Memory <b>150</b> can include program memory <b>160</b> that stores programs and software, such as an operating system <b>162</b>, surgical assistance system <b>164</b>, and other application programs <b>166</b>. Memory <b>150</b> can also include data memory <b>170</b> that can include, e.g. implant surgery information, configuration data, settings, user options or preferences, etc., which can be provided to the program memory <b>160</b> or any element of the system <b>152</b>.</p><p id="p-0039" num="0038">Some implementations can be operational with numerous other computing system environments or configurations. Examples of computing systems, environments, and/or configurations that may be suitable for use with the technology include, but are not limited to, personal computers, server computers, handheld or laptop devices, cellular telephones, wearable electronics, tablet devices, multiprocessor systems, microprocessor-based systems, programmable consumer electronics, network PCs, minicomputers, mainframe computers, distributed computing environments that include any of the above systems or devices, or the like.</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>1</b>B</figref> illustrates a network connection diagram <b>100</b> of a system <b>102</b> for providing assistance to a surgeon during a spinal surgery, according to an embodiment. The system <b>102</b> may be connected to a communication network <b>104</b>. The communication network <b>104</b> may further be connected with a network in the form of a precision spine network <b>106</b> for allowing data transfer between the system <b>102</b> and the precision spine network <b>106</b>.</p><p id="p-0041" num="0040">The communication network <b>104</b> may be a wired and/or a wireless network. The communication network <b>104</b>, if wireless, may be implemented using communication techniques such as Visible Light Communication (VLC), Worldwide Interoperability for Microwave Access (WiMAX), Long term evolution (LTE), Wireless local area network (WLAN), Infrared (IR) communication, Public Switched Telephone Network (PSTN), Radio waves, and other communication techniques known in the art.</p><p id="p-0042" num="0041">In one embodiment, the precision spine network <b>106</b> may be implemented as a facility over &#x201c;the cloud&#x201d; and may include a group of modules. The group of modules may include a Precision Spine Network Base (PSNB) module <b>108</b>, an abnormalities module <b>110</b>, an XZ screw angle module <b>112</b>, an XY screw module <b>114</b>, and a screw size module <b>116</b>.</p><p id="p-0043" num="0042">The PSNB module <b>108</b> may be configured to store images of patients and types of spinal screws, required in spinal surgeries. In some implementations, a similar module can be used for other types of surgeries. While the PSNB is referred to below, in each instance other similar modules can be used for other types of surgeries. For example, a Precision Knee Network Based can be used to assist in anterior cruciate ligament (ACL) replacement surgeries. The images may be any of camera images, Magnetic Resonance Imaging (MRI) images, ultrasound images, Computerized Aided Tomography (CAT) scan images, Positron Emission Tomography (PET) images, and X-Ray images. In one case, the images may be analyzed to identify abnormalities and salient features in the images, for performing spinal surgeries on the patients. In some implementations, the PSNB module <b>108</b> can store additional implant surgery information, such as patient information, (e.g. sex, age, height, weight, type of pathology, occupation, activity level, tissue information, health rating, etc.), specifics of implant systems (e.g. types and dimensions), availability of available implants, aspects of a surgeon's preoperative plan (e.g. surgeon's initial implant configuration, detection and measurement of the patient's anatomy on images, etc.), etc. In some implementations, the PSNB module <b>108</b> can convert the implant surgery information into formats useable for implant suggestion models and algorithms. For example, the implant surgery information can be tagged with particular identifiers for formulas or can be converted into numerical representations suitable for supplying to a machine learning model.</p><p id="p-0044" num="0043">The abnormalities module <b>110</b> may measure distances between a number of salient features of one vertebra with salient features of another vertebra, for identifying disk pinches or bulges. Based on the identified disk pinches or bulges, herniated disks may be identified in the patients. It should be obvious to those skilled in the art, that given a wide variety of salient features and geometric rules, many spinal abnormalities could be identified. If the spinal abnormalities are identified, the PSNB module <b>108</b> may graphically identify areas having the spinal abnormalities and may send such information to a user device <b>118</b>.</p><p id="p-0045" num="0044">In one embodiment, information related to spinal surgeries may be displayed through a Graphical User Interface (GUI) of the user device <b>118</b>, as illustrated using <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>. A smart phone is shown as the user device <b>118</b> in <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>, as an example. Further, the user device <b>118</b> may be any other device including a GUI, for example, a laptop, desktop, tablet, phablet, or other such devices known in the art.</p><p id="p-0046" num="0045">The XZ screw angle module <b>112</b> may determine an XZ angle of a spinal screw or other implant to be used during the surgery. Further, the XY screw angle module <b>114</b> may determine an XY angle of the implant. The XZ screw angle module <b>112</b> and the XY screw angle module <b>114</b> may determine a position entry point for at least one spinal screw. The XZ screw angle module <b>112</b> and the XY screw angle module <b>114</b> may graphically represent the identified data and may send such information to the user device <b>118</b>.</p><p id="p-0047" num="0046">The screw size module <b>116</b> may be used to determine a screw diameter (e.g., a maximum screw diameter) and a length of the screw based on the salient features identified from the images of the patients.</p><p id="p-0048" num="0047">In some implementations, the XZ screw angle module <b>112</b>, the XY screw angle module <b>114</b>, and the screw size module <b>116</b> can identify implant configurations for other types of implants in addition to, or other than screws (e.g., pedicle screws, facet screws, etc.) such as cages, plates, rods, disks, fusions devices, spacers, rods, expandable devices, etc. In addition, these modules may suggest implant configurations in relation to references other than an X, Y, Z, coordinate system. For example, in a spinal surgery, the suggestions can be in reference to the sagittal plane, mid-sagittal plane, coronal plane, frontal plane, or transverse plane. As another example, in an ACL replacement surgery, the suggestions can be an angle for a tibial tunnel in reference to the frontal plane of the femur. In various implementations, the XZ screw angle module <b>112</b>, the XY screw angle module <b>114</b>, or screw size module <b>116</b> can identify implant configurations using machine learning modules, algorithms, or combinations thereof, as described below in relation to <figref idref="DRAWINGS">FIGS. <b>6</b>-<b>9</b></figref>.</p><p id="p-0049" num="0048">In one embodiment, referring to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, a block diagram showing different components of the system <b>102</b> is explained. The system <b>102</b> includes a processor <b>202</b>, interface(s) <b>204</b>, and a memory <b>206</b>. The processor <b>202</b> may execute an algorithm stored in the memory <b>206</b> for augmenting an implant surgery, e.g. by providing assistance to a surgeon during a spinal or other implant surgery, by providing controls to a robotic apparatus (e.g., robotic surgery systems, navigation system, etc.) for an implant surgery or by generating suggestions for implant configurations to be used in an implant surgery. The processor <b>202</b> may also be configured to decode and execute any instructions received from one or more other electronic devices or server(s). The processor <b>202</b> may include one or more general purpose processors (e.g., INTEL&#xae; or Advanced Micro Devices&#xae; (AMD) microprocessors) and/or one or more special purpose processors (e.g., digital signal processors or Xilinx&#xae; System On Chip (SOC) Field Programmable Gate Array (FPGA) processor). The processor <b>202</b> may be configured to execute one or more computer-readable program instructions, such as program instructions to carry out any of the functions described in this description.</p><p id="p-0050" num="0049">The interface(s) <b>204</b> may help a user to interact with the system <b>102</b>. The user may be any of an operator, a technician, a doctor, a doctor's assistant, or another automated system controlled by the system <b>102</b>. The interface(s) <b>204</b> of the system <b>102</b> may either accept an input from the user or provide an output to the user, or may perform both the actions. The interface(s) <b>204</b> may either be a Command Line Interface (CLI), Graphical User Interface (GUI), or a voice interface.</p><p id="p-0051" num="0050">The memory <b>206</b> may include, but is not limited to, fixed (hard) drives, magnetic tape, floppy diskettes, optical disks, Compact Disc Read-Only Memories (CD-ROMs), and magneto-optical disks, semiconductor memories, such as ROMs, Random Access Memories (RAMs), Programmable Read-Only Memories (PROMs), Erasable PROMs (EPROMs), Electrically Erasable PROMs (EEPROMs), flash memory, magnetic or optical cards, or other type of media/machine-readable medium suitable for storing electronic instructions.</p><p id="p-0052" num="0051">The memory <b>206</b> may include modules, implemented as programmed instructions executed by the processor <b>202</b>. In one case, the memory <b>206</b> may include a design module <b>208</b> for receiving information from the abnormalities module <b>110</b>. The design module <b>208</b> may poll the surgeon for an information request. The design module <b>208</b> may allow the surgeon to design the spinal screw and change the generated implant configurations, such as the entry point (e.g., entry point into the patient, entry points into a vertebra, entry points to the implantation site, etc.), and screw or other implant angles in any of various planes. If the surgeon changes the entry point or angles, the system can automatically update other features of the implant configuration to account for the changes, such as the implant dimensions (e.g. screw diameter, thread pitch, or length). The design module <b>208</b> may include patient data <b>210</b>. The patient data <b>210</b> may include images of patients and may allow the surgeon to identify the patients. A patient may refer to a person on whom and operations is to be performed. The patient data <b>210</b> may include images of patients, received from the user device <b>118</b>.</p><p id="p-0053" num="0052">In one embodiment, areas of interest may be defined in diagnostic data of a patient. In one case, the system <b>102</b> may determine the areas of interest based on pre-defined rules or using machine learning models, as described below in relation to <figref idref="DRAWINGS">FIGS. <b>6</b>-<b>9</b></figref>. In another case, the areas of interest may be defined based on a surgeon's input. In one case, the diagnostic data may include images of the patient. The images may be any of camera images, Magnetic Resonance Imaging (MRI) images, ultrasound images, Computerized Aided Tomography (CAT) scan images, Positron Emission Tomography (PET) images, and X-Ray images. In one case, the images of the patients may be stored in the patient surgeon database <b>210</b>.</p><p id="p-0054" num="0053">Post defining the areas of interest, a screw bone type may be defined based on various models and/or the surgeon's input. Successively, salient features of the areas of interest may be identified in the images of the patients, e.g. by applying the procedures described below. <figref idref="DRAWINGS">FIG. <b>3</b>A</figref> shows salient points present in a top view of a vertebra of the patient, according to an embodiment. The salient points are shown as bubbles i.e. &#x2018;e<sub>1</sub>,&#x2019; &#x2018;e<sub>2</sub>,&#x2019; and &#x2018;f<sub>2</sub>.&#x2019; Further, <figref idref="DRAWINGS">FIG. <b>3</b>B</figref> shows salient points present in a side view of the vertebra of the patient, according to an embodiment. The salient points are shown as bubbles i.e. &#x2018;k<sub>i</sub>,&#x2019; &#x2018;k<sub>u</sub>,&#x2019; &#x2018;h<sub>u</sub>,&#x2019; &#x2018;i<sub>m</sub>,&#x2019; and &#x2018;z&#x2019;.&#x2032;</p><p id="p-0055" num="0054">Successively, based on the salient points of the areas of interest, the system <b>102</b> may determine implant configurations (e.g. angles and entry point, implant orientation, implant movement, etc.) using the analysis procedures. <figref idref="DRAWINGS">FIG. <b>4</b>A</figref> illustrates computations for determining the XZ angle (&#x3d5;) <b>402</b> using the salient points, according to an embodiment. It should be noted that positions of X and Y co-ordinates of the regions of interest may be determined based on a location of at least one salient feature present in the image.</p><p id="p-0056" num="0055"><figref idref="DRAWINGS">FIG. <b>4</b>B</figref> illustrates computations for determining the XY angle (&#x3b8;) <b>406</b> using the salient points, according to an embodiment. It should be noted that positions of X and Y co-ordinates of the regions of interest may be determined based on a location of at least one salient feature present in the image. Further, <figref idref="DRAWINGS">FIG. <b>4</b>A</figref> illustrates a position entry point <b>404</b> for the spinal screw, and <figref idref="DRAWINGS">FIG. <b>4</b>B</figref> illustrates a position entry point <b>408</b> for the spinal screw. Upon determining, MRI data including the XY angle, the XZ angle, and the position entry point for the spinal screw, may be stored in the abnormalities module <b>110</b>.</p><p id="p-0057" num="0056">Post identification of the angels and the entry point for an implant, the system <b>102</b> may determine additional implant configuration features. For example, the system <b>102</b> can determine a maximum implant (e.g. spinal screw) diameter, a minimum implant diameter, and a length of the implant to be used during a spinal surgery. For example, upon determining the maximum spinal screw diameter and the length of the spinal screw, the procedure MRI data may be updated in the abnormalities module <b>110</b>.</p><p id="p-0058" num="0057">In the spinal surgery example, the spinal screw having the determined maximum screw diameter and the length may be identified. The spinal screw may be suggested, to the surgeon, for usage during the spinal surgery. In one case, a spinal screw HA and dimensions of the spinal screw HA may be illustrated for the surgeon's selection, as shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>. As illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, a schematic showing different parameters of the spinal screw HA, dimensions of the spinal screw HA, and a schematic of threads of the spinal screw HA are shown, according to an embodiment. Further, different such details related to spinal screws HB, spinal screws HD, and other known spinal screws may be presented to the surgeon for usage during the spinal surgery, thereby assisting the surgeon during the spinal surgery.</p><p id="p-0059" num="0058">As another example, for an ACL replacement, upon determining the entry point and angle for a tibial tunnel for attaching a replacement graft, the system <b>102</b> can identify a depth for the tibial tunnel such that it will end above the center of the knee joint without piercing surrounding tissue. In addition, dimensions for the ACL graft itself and/or for screws or other fastening components can be suggested.</p><p id="p-0060" num="0059">The flowchart <b>600</b> of <figref idref="DRAWINGS">FIG. <b>6</b></figref> shows the architecture, functionality, and operation for providing assistance to a surgeon during a spinal surgery, according to an embodiment. One skilled in the art will appreciate that, for this and other processes and methods disclosed herein, the functions performed in the processes and methods may be implemented in differing order. Furthermore, the outlined steps and operations are only provided as examples, and some of the steps and operations may be optional, combined into fewer steps and operations, or expanded into additional steps and operations without detracting from the essence of the disclosed embodiments. For example, two blocks shown in succession in <figref idref="DRAWINGS">FIG. <b>6</b></figref> may in fact be executed substantially concurrently or the blocks may sometimes be executed in the reverse order, depending upon the functionality involved. Any process descriptions or blocks in flowcharts should be understood as representing modules, segments, or portions of code which include one or more executable instructions for implementing specific logical functions or steps in the process, and alternate implementations are included within the scope of the example embodiments in which functions may be executed out of order from that shown or discussed, including substantially concurrently or in reverse order, depending on the functionality involved. In addition, the process descriptions or blocks in flow charts should be understood as representing decisions made by a hardware structure such as a state machine. The flowchart <b>600</b> starts at step <b>602</b> and concludes at step <b>610</b>.</p><p id="p-0061" num="0060">At step <b>602</b>, areas of interest may be defined in diagnostic data of a patient and a screw bone type may be defined, during a spinal surgery. The diagnostic data may include images of the patient. The images may be any of camera images, Magnetic Resonance Imaging (MRI) images, ultrasound images, Computerized Aided Tomography (CAT) scan images, Positron Emission Tomography (PET) images, and X-Ray images.</p><p id="p-0062" num="0061">At step <b>604</b>, salient features of areas of interest may be identified from the diagnostic data. In one case, the images may be analyzed to identify abnormalities and the salient features, for performing spinal surgeries on the patients.</p><p id="p-0063" num="0062">At step <b>606</b>, an XZ angle, an XY angle, and a position entry point for an implant (e.g. a spinal screw) are determined. In one case, the XZ angle, the XY angle, and the position entry point may be determined based on the salient features.</p><p id="p-0064" num="0063">At step <b>608</b>, a maximum screw diameter and a length of the screw to be used during the spinal surgery may be determined based on the XY angle, the XZ angle, and the position entry point of the screw. Upon determining the maximum screw diameter and the length of the screw, the procedure MRI data may be updated in an abnormalities module <b>110</b>.</p><p id="p-0065" num="0064">At step <b>610</b>, the screw implant to be used during a surgery may be identified and suggested to a surgeon. The screw implant may be identified based on the maximum screw diameter and the length of the screw.</p><p id="p-0066" num="0065"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates a flowchart showing a method <b>700</b> for generating implant configurations. At block <b>702</b>, method <b>700</b> can obtain implant surgery information such as images, patent history, circumstance, test results, biographic data, surgeon recommendations, implant specifics, etc. Implant surgery images can be of parts of a patient, such as camera images, Magnetic Resonance Imaging (MRI) images, ultrasound images, Computerized Aided Tomography (CAT) scan images, Positron Emission Tomography (PET) images, X-Ray images, 2D or 3D virtual models, CAD models, etc. Additional implant surgery information can include, e.g. sex, age, height, weight, type of pathology, occupation, activity level, implant types and dimensions, availability of available implants, or aspects of a surgeon's preoperative plan (e.g. surgeon's initial implant configuration, detection and measurement of the patient's anatomy on images, etc.)</p><p id="p-0067" num="0066">The implant surgery information can be obtained in various manners such as through direct user input (e.g. through a terminal or by interacting with a web service), through automatic interfacing with networked databases (e.g. connecting to patient records stored by a hospital, laboratory, medical data repositories, etc.), by scanning documents, or through connected scanning, imaging, or other equipment. The patient data can be gathered with appropriate consent and safeguards to remain HIPPA compliant.</p><p id="p-0068" num="0067">At block <b>704</b>, method <b>700</b> can convert the implant surgery information obtained at block <b>702</b> to be compatible with analysis procedures. The conversion can depend on the analysis procedure that will be used. As discussed below in relation to block <b>706</b>, analysis procedures can include directly applying a machine learning model, applying an algorithm with multiple stages where any of the stages can provide machine learning model predictions (see <figref idref="DRAWINGS">FIG. <b>8</b>A</figref>), or applying a virtual modeling system (see <figref idref="DRAWINGS">FIG. <b>8</b>B</figref>).</p><p id="p-0069" num="0068">In some implementations, the conversion of the implant surgery information can include formatting the implant surgery information for entry to a machine learning model. For example, information such as patient sex, height, weight, etc. can be entered in a feature vector, such as a sparse vector with values corresponding to available patient characteristics. In some implementations, the conversions can include transforming images from the implant surgery information into a format suitable for input to a machine learning model, e.g. an array of integers representing pixels of the image, histograms, etc. In some implementations, the conversion can include identifying surgery target features (detection and measurement of the patient's anatomy on images), characterizing surgery targets, or modeling (i.e. creating a virtual model of) the implant surgery target. For example, in a spinal surgery, this can include measuring vertebrae features on an image, converting 2D images of vertebrae into a 3D model, or identifying which vertebrae from a series of images are to be the target of the implant operation. As another example, in an ACL replacement surgery, this can include identifying and measuring features in an image such as location, size, and spacing of anatomy such as of the femur, patella, remaining portion of meniscus, other ligaments, etc., converting 2D images of the knee into a 3D model, or identifying other areas of damage (e.g. fractures, torn cartilage, other ligament tears, etc.).</p><p id="p-0070" num="0069">In various implementations, the conversion process can be automatic, human supervised, or performed by a human technician, e.g. using tools such as a digital ruler and a digital angle finder. Further in the spinal surgery example, the conversion can include identifying a target set of vertebrae, initially localizing and marking the target set of vertebrae, performing segmentation for each of the target set of vertebrae, and marking cortical boundaries. In some implementations, input for the spinal implant surgery can specify a target set of vertebrae, however the surgical assistance system <b>164</b> can automatically perform calculations for additional vertebrae that weren't specified in the inputs. This can give the surgeon an option to expand the set of vertebrae to be fused, either prior to the operation or even during the procedure. In the ACL replacement surgery example, the conversion can include identifying a graft type (e.g. patella tendon, hamstring, cadaver ACL, etc.), initially localizing or marking the target drilling sites, performing segmentation for the target features (e.g. end of the femur), and marking boundaries (e.g. bone edges, meniscus edges, synovial membrane, etc.).</p><p id="p-0071" num="0070">At block <b>706</b>, method <b>700</b> can apply analysis procedures, using the converted implant surgery information from block <b>704</b>, to identify implant configuration(s). In various implementations, the analysis procedures can include directly applying a machine learning model, applying a sequence of steps that can include one or more machine learning models, and/or generating a virtual model of the surgery target area and applying heuristics for implant configuration selection.</p><p id="p-0072" num="0071">To apply a machine learning model directly, method <b>700</b> can provide the converted implant information to a machine learning model trained to specify implant configurations. A machine learning model can be trained to take input such as representations of a series of images and a feature vector for the patient and other aspects of the surgery (e.g. implant availability, surgeon specialties or ability indicators, equipment available, etc.) and can produce results that implant configurations. For example, for a spinal surgery, the machine learning model can suggest pedicle screw configurations, e.g. characteristics such as screw diameter, length, threading and application parameters such as screw insertion point, angle, rotation speed, etc. As another example, for an ACL replacement surgery, the machine learning model can suggest graft type, attachment type (e.g. screw material, length, or configuration features), graft attachment locations, drill depths, etc.</p><p id="p-0073" num="0072">In some implementations, the converted implant information can be used in a multi-stage process for selecting aspects of an implant configuration. For example, for a spinal surgery, the multi-stage process can include method <b>800</b> or method <b>850</b>, discussed below. In various steps of this these processes, either an algorithm can be used to generate results for that step or a machine learning model, trained for that step, can be applied.</p><p id="p-0074" num="0073">In some implementations, the procedure for identifying implant configurations for a spinal surgery can include processing implant surgery information to locate targeted vertebrae and their pedicles in images, on available axes; identifying and tagging vertebrae characteristics; determining a preferred screw insertion point based on a mapping between tags and insertion point criteria (e.g. where the mapping can be a representation of a medical definition of a pedicle screw insertion point&#x2014;described below); performing measurements, on the images, of the pedicle isthmus width and height and length of the pedicle and vertebral body, starting at the preferred insertion point; measuring the angle between the line used to determine length and the sagittal plane, in the axial view; and measuring the angle between that length line and the axial plane.</p><p id="p-0075" num="0074">In some implementations, machine learning models can be trained to perform some of these tasks for identifying implant configurations. For example, machine learning models can be trained to identify particular vertebral pedicles in various images, which can then be atomically measured and aggregated across images, e.g. storing minimal, maximal, median, or average values, as appropriate given the target being measured. As another example, a machine learning model can receive the set of images and determine an order or can select a subset of the images, for automatic or manual processing. In some implementations, a machine learning model can be used to localize and classify the target within an image, such as by identifying a target vertebra or localizing the end of the femur or meniscus edges. In some implementations, a machine learning model can be used to segment target vertebrae, femur, tibia, or other anatomical features in the target area, to determine their boundaries. In some implementations, a machine learning model can be used to localize insertion points. In some implementations, a machine learning model can be used to segment images to determine boundaries of anatomical structures (e.g., boundaries of bones, organs, vessels, etc.), density of tissue, characteristics of tissue, or the like.</p><p id="p-0076" num="0075">In various implementations, the results from the above stages can be used in inference formulae to compute the implant configurations. For example, a maximal screw diameter can be determined using the smallest pedicle isthmus dimension found across all the images of the target vertebrae (which can be adjusted to include a safety buffer). As another example, a maximal screw length can be determined using the smallest measurement of pedicle and vertebral body length, across all the target vertebra in question (which can be adjusted to include a safety buffer).</p><p id="p-0077" num="0076">Machine learning models, as used herein, can be of various types, such as Convolutional Neural Networks (CNNs), other types of neural networks (e.g. fully connected), decision trees, forests of classification trees, Support Vector Machines, etc. Machine learning models can be trained to produce particular types of results, as discussed below in relation to <figref idref="DRAWINGS">FIG. <b>9</b></figref>. For example, a training procedure can include obtaining suitable training items with input associated with a result, applying each training item to the model, and updating model parameters based on comparison of model result to training item result.</p><p id="p-0078" num="0077">In some implementations, automated selection of implant configurations can be limited to only cases likely to produce good results, e.g. only for certain pathologies, types of patients, surgery targets (e.g. the part of the spine that needs to be fused), or where confidence scores associated with machine learning model outputs are above a threshold. For example, in the spinal surgery example, automation can be limited to common types of spinal fusions, such as L3/L4, L4/L5, or L5/S1, certain pathologies such as spondylolisthesis or trauma, or patients with certain characteristics, such as being in a certain age group. As another example, for an ACL replacement, automation can be limited to cases without other ligament tears.</p><p id="p-0079" num="0078">At block <b>708</b>, method <b>700</b> can provide results specifying one or more features of an implant configuration. For example, the results for a spinal surgery can include selection of pedicle screw type and dimensions for each vertebra and guidance on an insertion point and angle for each screw. As another example, results for an ACL replacement surgery can include selection of implant graft type, connection type, joint dimensions, and guidance on connection points such as drill locations and depths. In some implementations, the results can be specified in a natural language, e.g. using templates that can be filled in with recommendations. In some cases, the results from the analysis of block <b>706</b> can be mapped to particular reasons for the implant configuration recommendations, and these reasons can be supplied along with the recommendations.</p><p id="p-0080" num="0079">In some implementations, the results can be based on medical definitions of preferred implant configurations, where the preferred implant configurations can be mapped to a particular surgical target area based on the results from block <b>706</b>. For example, results for spinal surgery pedicle screws can include a preferred insertion point, e.g. defined, for lumbar vertebrae, at the intersection of the superior articular facet, transverse process, and pars interarticularis; and for thoracic spine or cervical spine, at the intersection of the superior articular facet plane and transverse process. As another example, a preferred screw angle can be, in axial view, the angle between the sagittal plane and the line defined by the insertion point and midpoint of the pedicle isthmus. In sagittal view the preferred screw angle can be parallel to the superior vertebral endplate. In addition, a maximal screw length can be defined as the distance between the insertion point and the far cortical boundary of the vertebra, at a particular screw angle. A maximal screw diameter can be the minimal width of the pedicle isthmus, on any axis. Each of these can be modified to include a certain safety buffer, which can vary depending on the size of the vertebra. The results from block <b>706</b> can identify features of an individual patient, which can be used in conjunction with the foregoing implant configuration definitions to specify patient specific implant configurations, e.g. in natural language, as image annotations, in a 3D model, as discussed below.</p><p id="p-0081" num="0080">The implant configuration results can be specified in various formats. For example, results can be in a natural language, coordinates one of various coordinate systems, as instructions for a robotic system, as annotations to one or more images, or as a virtual model of the surgery target area. The results can be used to augment the implant surgery in multiple ways. For example, results can be added to a preoperative plan. Results can trigger acquisition of implant materials, such as by having selected implants ordered automatically or having designs for patient-specific screws provided for 3D-printing. As another example, results can be used to provide recommendations during a surgical procedure, e.g. with text or visual annotations provided as overlies on a flat panel display, through auditory or haptic feedback alerts, or using an AR or VR system, e.g. to display an overlay of the implant on the patient anatomy or to display guidance on the suggested insertion point and angle. In some implementations, the results can be used to control robotic systems, e.g. causing a robotic arm to align itself according to the recommended insertion point and angle, which may be first confirmed by a surgeon.</p><p id="p-0082" num="0081">The method <b>700</b> can be used in a wide range of procedures, e.g. open procedures, minimally invasive procedures, orthopedic procedures, neurological procedures, reconstructive implants, maxillofacial procedures (e.g., maxillary implants), or other procedure. In some surgical procedures, the implant information at block <b>702</b> can include implant dimensions, material information (e.g., composition of implant), and images of the patient. At block <b>706</b>, the implant configuration can be implant dimensions (e.g., when in a delivery state or implanted state), implant functionality, or the like.</p><p id="p-0083" num="0082"><figref idref="DRAWINGS">FIG. <b>8</b>A</figref> illustrates a flowchart showing a method <b>800</b> for applying analysis procedures that can utilize machine learning models, according to an embodiment. In some implementations, method <b>800</b> is performed as a sub-process of block <b>706</b>. At block <b>802</b>, method <b>800</b> can receive implant surgery information. This can be some of the converted implant surgery information from block <b>704</b>. In some implementations, the implant surgery information can include one or more images of the surgery target area, e.g. MRI scans of a spine, X-rays of a wrist, ultrasound images of an abdomen, etc.</p><p id="p-0084" num="0083">At block <b>804</b>, method <b>800</b> can localize and classify a target in one or more of the images of the surgery target area. In various implementations, this can be accomplished by applying a machine learning model trained for the particular target area to identify surgical targets or by finding a centroid point of each displayed vertebra, performing vertebral classification using image recognition algorithms, and determining whether the classified vertebrae match a list of vertebrae identified for the surgery. In some implementations, if the image does not contain at least one target of interest, the image can be disregarded from further processing.</p><p id="p-0085" num="0084">At block <b>806</b>, method <b>800</b> can segment the identified target(s) from block <b>804</b> to determine their boundaries. At block <b>808</b>, method <b>800</b> can localize implant insertion points. In some implementations, blocks <b>806</b> and <b>808</b> can be performed using machine learning models or algorithms, e.g. that identify particular patterns, changes in color, shapes, etc.</p><p id="p-0086" num="0085">At block <b>810</b>, method <b>800</b> can localize and segment individual target features. For example, in a spinal surgery where targets are vertebrae, at block <b>810</b> method <b>800</b> can identify vertebrae pedicles and their isthmus, and measure these features. In some implementations, this can be accomplished using a machine learning model trained to detect each type of feature. In some implementations, detecting the pedicle and the isthmus of vertebra from annotated images can include measuring the isthmus width and tracking the minimal value across images and planes, defining the angle between the line that passes through at least two midpoints in the pedicle, and the reference plane, measuring the maximal length through that line, and tracking the minimal value across measurements. In some implementations, isthmus determination and measurement can be accomplished by starting at a point inside a pedicle, computing the distance to pedicle borders in multiple directions, taking the minimum length. In other implementations, the isthmus determination and measurement can be accomplished by scanning, for example using horizontal lines that intersect with pedicle borders in an axial view, and finding the minimum-length line.</p><p id="p-0087" num="0086">In some implementations, the steps performed at any of blocks <b>804</b>-<b>810</b> can be repeated for each of multiple target area images, aggregating results from the multiple images. For example, in a step for identifying and measuring vertebrae pedicles, an aggregated measurement for a particular pedicle can be the minimum measured width of the pedicle from all of the images showing that particular pedicle.</p><p id="p-0088" num="0087">At block <b>812</b>, method <b>800</b> can use results from any of blocks <b>804</b>-<b>810</b> to compute an implant configuration (e.g. characteristics and application parameters). For example, the minimum width of a pedicle found across the images showing that pedicle, with some buffer added, can be the selected width characteristic of a pedicle screw implant. As another example, a screw angle could be determined using an identified insertion point and a center of the pedicle isthmus, with respect to center axis, depending on the image plane. The angles in axial and sagittal planes can be either the median or average angles across the multiple images. As a further example, a maximal screw length can be determined as the length of the line defined by the insertion point, the insertion angle, and the point where the line hits the cortical wall of the vertebra, minus some safety buffer. This length can be computed from multiple images and the minimum across all the images can be used for of this screw.</p><p id="p-0089" num="0088"><figref idref="DRAWINGS">FIG. <b>8</b>B</figref> illustrates a flowchart showing a method <b>850</b> for applying analysis procedures that can utilize virtual models, according to an embodiment. In some implementations, method <b>850</b> is performed as a sub-process of block <b>706</b>. At block <b>852</b>, method <b>850</b> can receive implant surgery information. This can be some of the converted implant surgery information from block <b>704</b>. In some implementations, the implant surgery information can include one or more images of the surgery target area.</p><p id="p-0090" num="0089">At block <b>854</b>, method <b>850</b> can build one or more virtual models of the target surgery area based on the images and/or other measurement data in the implant surgery information. A virtual model, as used herein, is a computerized representation of physical objects, such as the target area of a surgery (e.g. portions of a patient's spine) and/or implants (e.g. screws, rods, etc.). In some implementations, virtual models can be operated according to known physical properties, e.g. reactions to forces can be predicted according to known causal relationships. In various implementations, the virtual models generated by method <b>850</b> can be two-dimensional models or three-dimensional models. For example, a two-dimensional model can be generated by identifying portions of an image as corresponding to parts of a patient's anatomy, such that a computing system can determine how implant characteristics would fit in relation to the determined anatomy parts. As another example, a three-dimensional model can be generated by identifying shapes and features in individual images, from a set of successive images, and mapping the identified shapes and features into a virtual three-dimensional space, using relationships between images. Finite element analysis techniques can be used to predict stresses, strains, pressures, facture, and other information and be used to design implants, surgical tools, surgical techniques, etc. For example, the implant configuration can be determined based on predetermined stresses (e.g., maximum allowable stresses in the tissue and/or implant, yield strength of anatomical structures and/or implant components, etc.), fracture mechanics, or other criteria defined by the physician or automatically determined based on, for example, tissue characteristics, implant design, or the like. In some embodiments, fatigue life can be predicted using stress or strain based techniques.</p><p id="p-0091" num="0090">A virtual model can also analyze mechanical interaction between a patient's vertebrae, loading of implants, and other devices (e.g., rods, ties, brackets, plates, etc.) coupled to those implants. The output of these analyses can be used to select pedicle screw configurations, insertion trajectories, and placement location to optimize screw pull-out strength, maximum allowable loading (e.g., axial loads, shear loads, moments, etc.) to manage stresses between adjacent vertebrae, or maximum allowable stress in regions of the bone at risk for fracture.</p><p id="p-0092" num="0091">In some embodiments, a user could identify areas of weakened bone or areas on images of the patient where there is risk of a fracture due to the presence of a screw or other implant. This information can be provided to the virtual model. The virtual model can be used to evaluate whether the configuration or location of the implant would create an unacceptable risk of fracture in the identified region. If so, the system could alert the user to that risk or modify the implant configuration or the procedure to reduce the risk to an acceptable level. In other embodiments, the system could identify these areas of high fracture risk automatically. In yet another embodiment, the system could provide data to the user such as the maximum torque to apply to a given pedicle screw during the surgical procedure such that tissue trauma, risk of fracture, or adverse advents is minimized.</p><p id="p-0093" num="0092">At block <b>856</b>, method <b>850</b> can localize and classify areas of interest within the virtual model(s) from block <b>854</b>. This can be accomplished using object recognition that matches shapes of known objects to shapes within the virtual models. For example, in a virtual model for a spinal surgery, the images can be MRI images of vertebrae. The virtual vertebrae can be labeled (e.g. c1-S5) and virtual model vertebrae corresponding to the vertebrae for which the spinal procedure is planned can be selected as the areas of interest. In some implementations, additional areas around the selected areas can be added to the areas of interest, allowing the surgeon to select alternative options before or during the procedure. For example, the one or two vertebrae adjacent, on one or both sides, to the planned vertebrae can be additionally selected.</p><p id="p-0094" num="0093">At block <b>858</b>, method <b>850</b> can segment the areas of interest, identified at block <b>856</b>, to determine various boundaries and other features, such as the pedicle boundaries and the pedicle isthmus. In some implementations, the segmentation or boundary determinations can be performed using a machine learning model. The machine learning model can be trained, for the type of implant surgery to be performed, to receive a portion of a virtual model and identify target portion segmentations or boundaries.</p><p id="p-0095" num="0094">At block <b>860</b>, method <b>850</b> can localize an insertion point for the implant in the target area. In some implementations, this can be accomplished by applying a machine learning model trained to identify insertion points. In some implementations, localizing insertion points can be accomplish using an algorithm, e.g. that identify particular patterns, changes in color, shapes, etc. identified as corresponding to preferred implant insertion points.</p><p id="p-0096" num="0095">At block <b>862</b>, method <b>850</b> can compute an implant configuration based on the virtual model(s) and/or determinations made in blocks <b>856</b>-<b>860</b>. In some implementations, the implant can be associated with requirements for their application and properties to maximize or minimize. In these cases, the implant configuration can be specified as the configuration that fits with the virtual model, achieving all the requirements, and optimizing the maximizable or minimizable properties. For example, when method <b>850</b> is performed to determine pedicle screw configurations for a spinal surgery, virtual pedicle screws can be placed in a virtual model generated at block <b>854</b>, according to the insertion points determined at block <b>860</b>. The virtual pedicle screws can further be placed to: not breach cortical vertebral boundaries (e.g. determined at block <b>858</b>), with a specified amount of buffer, while maximizing the screw diameter and length, taking into consideration required buffers and close to optimal insertion angle, defined by the pedicle isthmus center and insertion point, for each vertebra (e.g. determined at block <b>858</b>). In some implementations, this placement of the implant can be performed as a constraint optimization problem. For example, a virtual screw can be placed inside the segmented vertebral body in the virtual model. The placement can then be adjusted until an equilibrium is reached that optimizes the parameters while conforming to the implant constraints. For example, method <b>850</b> can maximizing screw diameter and length while aligning with an optimal angle and avoiding cortical breaches.</p><p id="p-0097" num="0096"><figref idref="DRAWINGS">FIG. <b>9</b></figref> illustrates a flowchart showing a method for training a machine learning model, according to an embodiment. Machine learning models, such as neural networks, can be trained to produce types of results. A neural network can be trained by obtaining, at block <b>902</b>, a quantity of &#x201c;training items,&#x201d; where each training item includes input similar to input the model will receive when in use and a corresponding scored result. At block <b>904</b>, the input from each training item can be supplied to the model to produce a result. At block <b>906</b>, the result can be compared to the scored result. At block <b>908</b>, model parameters can then be updated, based on how similar the model result is to the scored result and/or whether the score is positive or negative.</p><p id="p-0098" num="0097">For example, a model can be trained using sets of pre-operative MRI scans of vertebrae paired with pedicle screw placements used in the surgery and corresponding scores for the result of that surgery. The images can be converted to arrays of integers that, when provided to the machine learning model, produce values that specify screw placements. The screw placements can be compared to the actual screw placement used in the surgery that produced the training item. The model parameters can then be adjusted so the model output is more like the screw placement used in the surgery if the surgery was a success or less like the screw placement used in the surgery if the surgery was a failure. The amount of adjustment to the model parameters can be a function of how different the model prediction was from the actual screw configuration used and/or the level of success or failure of the surgery.</p><p id="p-0099" num="0098">As discussed above, machine learning models for the surgical assistance system can be trained to produce various results such as: to directly produce implant configurations upon receiving implant surgery information, to identify particular vertebral pedicles in various images, to determine an order or subset of images for processing, to localize and classify the target within an image, to segment target vertebrae, to determine boundaries or other features, to localize insertion points, etc.</p><p id="p-0100" num="0099">In various implementations, the training data for a machine learning model can include input data such as medical imaging data, other patient data, or surgeon data. For example, model input can include images of the patient, patient sex, age, height, weight, type of pathology, occupation, activity level, etc., specifics of implant systems (e.g. types and dimensions), availability of available implants, or aspects of a surgeon's preoperative plan (e.g. surgeon's initial implant configuration, detection and measurement of the patient's anatomy on images, etc.) In some implementations, model training data input can include surgeon specifics, such as statistics or preferences for implant configurations used by the surgeon performing the implant surgery or outcomes for implant usages. For example, surgeons may have better skill or experience with particular implant configurations, and the system can be trained to select implant configurations the particular surgeon is more likely to use successfully. The training data input can be paired with results to create training items. The results can be, for example, human annotated medical imaging data (as a comparison for identifications such as boundaries and insertion points identified by a model), human feedback to model outputs, surgeons' post-operative suggestion feedback (e.g. whether the surgeon accepted model provided recommendations completely, or made certain changes, or disregarded), surgeons post-operative operation outcome success score, post-operative images that can be analyzed to determine results, the existence of certain positive or negative patient results, such as cortical breaches or other complications that might have occurred in the procedure, overall level of recovery, or recovery time.</p><p id="p-0101" num="0100">In an illustrative embodiment, any of the operations, processes, etc. described herein can be implemented as computer-readable instructions stored on a computer-readable medium. The computer-readable instructions can be executed by a processor of a mobile unit, a network element, and/or any other computing device.</p><p id="p-0102" num="0101">There is little distinction left between hardware and software implementations of aspects of systems; the use of hardware or software is generally (but not always, in that in certain contexts the choice between hardware and software can become significant) a design choice representing cost vs. efficiency tradeoffs. There are various vehicles by which processes and/or systems and/or other technologies described herein can be effected (e.g., hardware, software, and/or firmware), and that the preferred vehicle will vary with the context in which the processes and/or systems and/or other technologies are deployed. For example, if an implementer determines that speed and accuracy are paramount, the implementer may opt for a mainly hardware and/or firmware vehicle; if flexibility is paramount, the implementer may opt for a mainly software implementation; or, yet again alternatively, the implementer may opt for some combination of hardware, software, and/or firmware.</p><p id="p-0103" num="0102">The foregoing detailed description has set forth various embodiments of the devices and/or processes via the use of block diagrams, flowcharts, and/or examples. Insofar as such block diagrams, flowcharts, and/or examples contain one or more functions and/or operations, it will be understood by those within the art that each function and/or operation within such block diagrams, flowcharts, or examples can be implemented, individually and/or collectively, by a wide range of hardware, software, firmware, or virtually any combination thereof. In one embodiment, several portions of the subject matter described herein may be implemented via Application Specific Integrated Circuits (ASICs), Field Programmable Gate Arrays (FPGAs), digital signal processors (DSPs), or other integrated formats. However, those skilled in the art will recognize that some aspects of the embodiments disclosed herein, in whole or in part, can be equivalently implemented in integrated circuits, as one or more computer programs running on one or more computers (e.g., as one or more programs running on one or more computer systems), as one or more programs running on one or more processors (e.g., as one or more programs running on one or more microprocessors), as firmware, or as virtually any combination thereof, and that designing the circuitry and/or writing the code for the software and or firmware would be well within the skill of one of skill in the art in light of this disclosure. In addition, those skilled in the art will appreciate that the mechanisms of the subject matter described herein are capable of being distributed as a program product in a variety of forms, and that an illustrative embodiment of the subject matter described herein applies regardless of the particular type of signal bearing medium used to actually carry out the distribution. Examples of a signal bearing medium include, but are not limited to, the following: a recordable type medium such as a floppy disk, a hard disk drive, a CD, a DVD, a digital tape, a computer memory, etc.; and a transmission type medium such as a digital and/or an analog communication medium (e.g., a fiber optic cable, a waveguide, a wired communications link, a wireless communication link, etc.).</p><p id="p-0104" num="0103">Those skilled in the art will recognize that it is common within the art to describe devices and/or processes in the fashion set forth herein, and thereafter use engineering practices to integrate such described devices and/or processes into data processing systems. That is, at least a portion of the devices and/or processes described herein can be integrated into a data processing system via a reasonable amount of experimentation. Those having skill in the art will recognize that a typical data processing system generally includes one or more of a system unit housing, a video display device, a memory such as volatile and non-volatile memory, processors such as microprocessors and digital signal processors, computational entities such as operating systems, drivers, graphical user interfaces, and applications programs, one or more interaction devices, such as a touch pad or screen, and/or control systems including feedback loops and control motors (e.g., feedback for sensing position and/or velocity; control motors for moving and/or adjusting components and/or quantities). A typical data processing system may be implemented utilizing any suitable commercially available components, such as those typically found in data computing/communication and/or network computing/communication systems.</p><p id="p-0105" num="0104">The herein described subject matter sometimes illustrates different components contained within, or connected with, different other components. It is to be understood that such depicted architectures are merely examples, and that in fact many other architectures can be implemented which achieve the same functionality. In a conceptual sense, any arrangement of components to achieve the same functionality is effectively &#x201c;associated&#x201d; such that the desired functionality is achieved. Hence, any two components herein combined to achieve a particular functionality can be seen as &#x201c;associated with&#x201d; each other such that the desired functionality is achieved, irrespective of architectures or intermedial components. Likewise, any two components so associated can also be viewed as being &#x201c;operably connected&#x201d;, or &#x201c;operably coupled&#x201d;, to each other to achieve the desired functionality, and any two components capable of being so associated can also be viewed as being &#x201c;operably couplable&#x201d;, to each other to achieve the desired functionality. Specific examples of operably couplable include but are not limited to physically mateable and/or physically interacting components and/or wirelessly interactable and/or wirelessly interacting components and/or logically interacting and/or logically interactable components. All applications and patents referenced herein are incorporated by reference in their entireties.</p><p id="p-0106" num="0105">From the foregoing, it will be appreciated that various embodiments of the present disclosure have been described herein for purposes of illustration, and that various modifications may be made without departing from the scope and spirit of the present disclosure. Accordingly, the various embodiments disclosed herein are not intended to be limiting.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A computer-implemented method for providing a patient-specific implant for a patient, the method comprising:<claim-text>transmitting, from at least one user device, one or more criteria associated with a virtual anatomical model of a portion of a patient, wherein the virtual anatomical model is based on one or more images of the patient, and the virtual anatomical model is viewable via the at least one user device for evaluating a surgical procedure using the patient-specific implant;</claim-text><claim-text>receiving a notification indicating that at least one of the one or more criteria is unacceptable;</claim-text><claim-text>transmitting, from the at least one user device, an alternative option from the user for the at least one unacceptable criteria;</claim-text><claim-text>displaying, via the at least one user device, a modified virtual anatomical model of the patient's spine based on the alternative option from the user; and</claim-text><claim-text>transmitting, from the at least one user device, user approval of the modified anatomical model of the patient's spine so that a remote computing system designs the patient-specific implant that fits the modified virtual anatomical model of the patient's spine.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising displaying at least one anatomical analytic for the patient's spine based on the modified virtual anatomical model of the patient's spine, wherein displaying the modified virtual anatomical model includes displaying at least one annotated image of the modified virtual anatomical model.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising receiving virtual implantation simulation data of the surgical procedure for implanting the patient-specific implant based on the modified virtual anatomical model.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein displaying the modified virtual anatomical model is performed by displaying one or more annotated images of the modified virtual anatomical model of the patient's spine for review by the user, wherein the one or more annotated images includes at least one measurement between anatomical features of the patient's spine.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising displaying annotated anatomical images associated with the patient's spine, wherein the one or more criteria include implant design criteria.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising receiving, from a spine network of the remote computing system, abnormality analytics quantifying one or more anatomical features of the patient.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one or more criteria includes at least one of a target measurement for a target outcome of the patient's spine and a characteristic of an anatomical feature of the patient's spine.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. A computing system comprising:<claim-text>one or more processors; and</claim-text><claim-text>one or more memories storing instructions that, when executed by the one or more processors, cause the computing system to perform a process comprising:<claim-text>transmitting, from at least one user device, one or more criteria associated with a virtual anatomical model of a portion of a patient, wherein the virtual anatomical model is based on one or more images of the patient, and the virtual anatomical model is viewable via the at least one user device for evaluating a surgical procedure using the patient-specific implant;</claim-text><claim-text>receiving a notification indicating that at least one of the one or more criteria is unacceptable;</claim-text><claim-text>transmitting, from the at least one user device, an alternative option from the user for the at least one unacceptable criteria;</claim-text><claim-text>displaying, via the at least one user device, a modified virtual anatomical model of the patient's spine based on the alternative option from the user; and</claim-text><claim-text>transmitting, from the at least one user device, user approval of the modified anatomical model of the patient's spine so that a remote computing system designs the patient-specific implant that fits the modified virtual anatomical model of the patient's spine.</claim-text></claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The computing system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the process further comprises displaying at least one image of the modified virtual anatomical model and at least one anatomical analytic for the patient's spine based on the modified virtual anatomical model of the patient's spine.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The computing system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the process further comprises receiving virtual implantation simulation data for the patient-specific implant using the modified virtual anatomical model.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The computing system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein displaying the modified virtual anatomical model is performed by displaying one or more annotated images of the modified virtual anatomical model of the patient's spine for review by the user, wherein the one or more annotated images includes at least one measurement between anatomical features of the patient's spine.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The computing system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the process further comprises displaying annotated anatomical images associated with one or more images of a surgical target area.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The computing system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the process further comprises receiving, from a spine network of the remote computing system, abnormality analytics quantifying anatomical features of the patient.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The computing system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the one or more criteria includes at least one of a target measurement for a target outcome of the patient's spine and a characteristic of an anatomical feature of the patient's spine.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. A non-transitory computer-readable storage medium storing instructions that, when executed by a computing system, cause the computing system to perform a process comprising:<claim-text>transmitting, from at least one user device, one or more criteria associated with a virtual anatomical model of a portion of a patient, wherein the virtual anatomical model is based on one or more images of the patient, and the virtual anatomical model is viewable via the at least one user device for evaluating a surgical procedure for a patient-specific implant;</claim-text><claim-text>receiving a notification indicating that at least one of the one or more criteria is unacceptable;</claim-text><claim-text>transmitting, from the at least one user device, an alternative option from the user for the at least one unacceptable criteria;</claim-text><claim-text>displaying, via the at least one user device, a modified virtual anatomical model of the patient's spine based on the alternative option from the user; and</claim-text><claim-text>transmitting, from the at least one user device, user approval of the modified anatomical model of the patient's spine so that a remote computing system designs the patient-specific implant that fits the modified virtual anatomical model of the patient's spine.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the process further comprises displaying at least one image of the modified virtual anatomical model and at least one anatomical analytic for the patient's spine based on the modified virtual anatomical model of the patient's spine.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the process further comprises displaying one or more values for the patient's spine based on the virtual model of the patient's spine, wherein the one or more values indicate a configuration for the patient's spine.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the process further comprises receiving virtual implantation simulation data for the patient-specific implant using the modified virtual anatomical model.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein displaying the modified virtual anatomical model is performed by displaying one or more annotated images of the modified virtual anatomical model of the patient's spine for review by the user, wherein the one or more annotated images includes at least one measurement between anatomical features of the patient's spine.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the process further comprises: displaying annotated images associated with the one or more images or virtual model of a surgical target area.</claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. A digital personalized surgical plan for designing a patient-specific implant for an anatomical outcome, wherein the digital personalized surgical plan is displayable via an electronic screen of at least one user device to provide viewing of a modified virtual anatomical model of at least a portion of a patient's body illustrating the anatomical outcome, wherein the at least one user device is configured to receive one or more criteria from a user and to receive approval of the digital personalized surgical plan, and wherein the digital personalized surgical plan is made by a process comprising:<claim-text>transmitting, from at least one user device, one or more criteria associated with a virtual anatomical model of a portion of a patient, wherein the virtual anatomical model is based on one or more images of the patient, and the virtual anatomical model is viewable via the at least one user device for evaluating a surgical procedure with the patient-specific implant;</claim-text><claim-text>receiving a notification indicating that at least one of the one or more criteria is unacceptable;</claim-text><claim-text>transmitting, from the at least one user device, an alternative option from the user for the at least one unacceptable criteria;</claim-text><claim-text>displaying, via the at least one user device, a modified virtual anatomical model of the patient's spine based on the alternative option from the user; and</claim-text><claim-text>transmitting, from the at least one user device, user approval of the modified anatomical model of the patient's spine so that a remote computing system designs the patient-specific implant that fits the modified virtual anatomical model of the patient's spine.</claim-text></claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. The digital surgical plan of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the digital personalized surgical plan further includes one or more values for the patient's spine based on the at least one of a virtual anatomical model or a modified virtual three-dimensional model, wherein the one or more values indicate a configuration for the patient's spine.</claim-text></claim></claims></us-patent-application>