<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005165A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005165</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17808520</doc-number><date>20220623</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>50</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>50</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>0454</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20084</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20081</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">CROSS-TASK DISTILLATION TO IMPROVE DEPTH ESTIMATION</invention-title><us-related-documents><us-provisional-application><document-id><country>US</country><doc-number>63214727</doc-number><date>20210624</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>QUALCOMM Incorporated</orgname><address><city>San Diego</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>CAI</last-name><first-name>Hong</first-name><address><city>San Diego</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>MATAI</last-name><first-name>Janarbek</first-name><address><city>San Diego</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>BORSE</last-name><first-name>Shubhankar Mangesh</first-name><address><city>San Diego</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>ZHANG</last-name><first-name>Yizhe</first-name><address><city>San Diego</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>ANSARI</last-name><first-name>Amin</first-name><address><city>Federal Way</city><state>WA</state><country>US</country></address></addressbook></inventor><inventor sequence="05" designation="us-only"><addressbook><last-name>PORIKLI</last-name><first-name>Fatih Murat</first-name><address><city>San Diego</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Certain aspects of the present disclosure provide techniques for cross-task distillation. A depth map is generated by processing an input image using a first machine learning model, and a segmentation map is generated by processing the depth map using a second machine learning model. A segmentation loss is computed based on the segmentation map and a ground-truth segmentation map, and the first machine learning model is refined based on the segmentation loss.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="103.21mm" wi="158.75mm" file="US20230005165A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="202.61mm" wi="149.44mm" orientation="landscape" file="US20230005165A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="179.75mm" wi="108.37mm" orientation="landscape" file="US20230005165A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="235.80mm" wi="159.26mm" orientation="landscape" file="US20230005165A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="202.86mm" wi="151.30mm" orientation="landscape" file="US20230005165A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="246.72mm" wi="111.68mm" file="US20230005165A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="211.50mm" wi="113.96mm" file="US20230005165A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="211.75mm" wi="113.88mm" file="US20230005165A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="206.59mm" wi="143.43mm" file="US20230005165A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="212.94mm" wi="130.81mm" file="US20230005165A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="244.43mm" wi="160.95mm" file="US20230005165A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application claims the benefit of and priority to U.S. Provisional Patent Application No. 63/214,727, filed on Jun. 24, 2021, the entire contents of which are incorporated herein by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">INTRODUCTION</heading><p id="p-0003" num="0002">Aspects of the present disclosure relate to machine learning techniques.</p><p id="p-0004" num="0003">In recent times, machine learning techniques, including deep learning, have increasingly been used to provide considerable accuracy in computer vision tasks. One such task is monocular depth estimation, where the depth of each element in an input image is inferred or predicted using a single image from a single vantage point (e.g., as compared with systems that rely on two or more images, with binocular disparity between them, to provide stereopsis and thereby determine depth). Monocular depth estimation can play an important role in three-dimensional visual scene understanding, and is of significant importance for a variety of application domains such as self-driving vehicles, augmented reality (AR) and virtual reality (VR) devices, drones or other autonomous devices, Internet of Things (IoT) devices, and robotics. However, accurate depth estimation is computationally complex and difficult to achieve.</p><p id="p-0005" num="0004">Accordingly, techniques are needed for machine learning with accurate and computationally-efficient depth estimation.</p><heading id="h-0003" level="1">BRIEF SUMMARY</heading><p id="p-0006" num="0005">Certain aspects provide a method, comprising: generating a depth map by processing an input image using a first machine learning model; generating a segmentation map by processing the depth map using a second machine learning model; computing a segmentation loss based on the segmentation map and a ground-truth segmentation map; and refining the first machine learning model based on the segmentation loss.</p><p id="p-0007" num="0006">Certain aspects provide a method, comprising: receiving an input image; generating an output depth map by processing the input image using a first machine learning model; generating a segmentation map by processing the depth map using a second machine learning model; computing a segmentation loss based on the segmentation map; and refining the first machine learning model based on the segmentation loss.</p><p id="p-0008" num="0007">Other aspects provide processing systems configured to perform the aforementioned methods as well as those described herein; non-transitory, computer-readable media comprising instructions that, when executed by one or more processors of a processing system, cause the processing system to perform the aforementioned methods as well as those described herein; a computer program product embodied on a computer readable storage medium comprising code for performing the aforementioned methods as well as those further described herein; and a processing system comprising means for performing the aforementioned methods as well as those further described herein.</p><p id="p-0009" num="0008">The following description and the related drawings set forth in detail certain illustrative features of one or more aspects.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0010" num="0009">The appended figures depict certain aspects of the one or more aspects and are therefore not to be considered limiting of the scope of this disclosure.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>1</b>A</figref> depicts an example workflow for training a depth model and a depth-to-segmentation model.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>1</b>B</figref> depicts an example workflow for inferencing using a trained depth model.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>2</b>A</figref> depicts an example workflow for training a depth model using photometric loss and segmentation loss.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>2</b>B</figref> depicts an example workflow for training a depth model using ground-truth depth maps and segmentation loss.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>3</b></figref> depicts an example flow diagram illustrating a method for training machine learning models for depth estimation.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>4</b></figref> depicts an example flow diagram illustrating a method for generating segmentation loss to refine depth models.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>5</b></figref> depicts an example flow diagram illustrating a method for training and inferencing using a depth model trained with the aid of a depth-to-segmentation model.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>6</b></figref> depicts an example flow diagram illustrating a method for training a machine learning model based on segmentation loss.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>7</b></figref> depicts an example flow diagram illustrating a method for inferencing using a machine learning model trained using segmentation loss.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>8</b></figref> depicts an example processing system configured to perform various aspects of the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0021" num="0020">Additional aspects of the present disclosure can be found in the attached appendix.</p><p id="p-0022" num="0021">To facilitate understanding, identical reference numerals have been used, where possible, to designate identical elements that are common to the drawings. It is contemplated that elements and features of one aspect may be beneficially incorporated in other aspects without further recitation.</p><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0023" num="0022">Aspects of the present disclosure provide techniques for cross-task distillation to improve monocular depth estimation in machine learning models, such as neural networks.</p><p id="p-0024" num="0023">Training accurate depth models in a supervised manner may require high-quality (e.g., dense and correctly aligned) ground-truth depth maps, which are difficult and costly to obtain. Some self-supervision techniques have emerged for training monocular depth estimation models. Additionally, semantic segmentation information has recently been used to improve prediction accuracy.</p><p id="p-0025" num="0024">In existing systems, pre-trained or co-trained semantic segmentation models can be used to assist the depth model during both training and inferencing. While such approaches can improve accuracy, they incur significant extra computational expense. For example, using pre-trained models incurs vastly increased expense during inference, as they require running a separate (and generally computationally-expensive) segmentation model. Similarly, co-training a segmentation model that shares layers with the depth model incurs vastly increased expense during the training process.</p><p id="p-0026" num="0025">In some aspects, cross-task knowledge distillation may be used to impart knowledge from semantic segmentation models (e.g., neural networks trained to provide semantic segmentation) to the depth estimation models (e.g., neural networks trained to generate depth maps based on input images). That is, during training, the semantic knowledge from pre-trained segmentation models can be transferred to the depth models, enhancing their capabilities and accuracy.</p><p id="p-0027" num="0026">Notably, using a segmentation network to aid training of a depth network differs from more conventional knowledge distillation tasks, where the teacher and student networks share the same visual task. In aspects of the present disclosure, the outputs of the depth network (e.g., depth maps) and the semantic segmentation network (e.g., segmentation maps) are not directly comparable. Therefore, in some aspects, an efficient depth-to-segmentation model is trained to bridge this task-gap.</p><p id="p-0028" num="0027">Generally, a depth map indicates, for each pixel in an input image, the depth (e.g., the distance from the camera) of the corresponding object covered by the pixel. For example, the depth map may be visualized as a heat map, where each pixel is shaded based on the inferred depth from the camera. Other implementations are also possible.</p><p id="p-0029" num="0028">Generally, a semantic segmentation map classifies each pixel of an input image into a class based on the object covered by the pixel. For example, in a self-driving task, a segmentation model may be used to label each pixel based on whether it depicts a telephone pole, stop sign, road, sidewalk, tree, pedestrian, vehicle, and the like. Though tasks related to self-driving (such as depth estimation of vehicles and signs) are used in some examples herein, aspects of the present disclosure are readily applicable to a wide variety of tasks.</p><p id="p-0030" num="0029">In order to enable such knowledge distillation across two different visual tasks, in aspects of the present disclosure, a depth-to-segmentation model (e.g., a neural network) is used to translate the predicted depth map (generated by a depth model) to a semantic segmentation map, which can then be compared against segmentation maps generated by a teacher network (e.g., a pre-trained segmentation model). The resulting loss can then be used to refine both the depth model and the depth-to-segmentation model. In this way, this depth-to-segmentation model enables backpropagation from the semantic segmentation model to the depth network during training.</p><p id="p-0031" num="0030">In some aspects, once the depth model is trained, the other models (e.g., the depth-to-segmentation model and the pre-trained segmentation model) can be discarded, allowing the depth model to efficiently generate depth maps based on input images. This provides significant improvement over existing approaches that rely on segmentation models during inferencing. This can result in fewer operations and processing requirements, as well as reduced power use, processing time, and memory use.</p><p id="p-0032" num="0031">Additionally, conventional approaches can often predict inconsistent depth values on a variety of objects, which visually appear as missing parts on the depth map (e.g., regions with infinite or indeterminate depth). For example, small, thin, and/or reflective objects (such as bikers, pedestrians, lamp posts, car windows and surfaces, and the like) may result in significant death map inaccuracy using conventional approaches. However, as aspects of the present disclosure enable the model to better understand the semantics of the scene, the model is able to generate more accurate and more semantically-structured depth maps.</p><p id="p-0033" num="0032">Further, by using pre-trained segmentation models to generate segmentation maps, aspects of the present disclosure reduce the computational expense of training, as compared to existing approaches using semantic networks that share one or more layers with the depth networks. That is, because such approaches require co-training of semantic networks and depth networks, the expense of training is significant. In contrast, aspects of the present disclosure use pre-trained segmentation models and only train a small, lightweight depth-to-segmentation model alongside the depth model.</p><p id="p-0034" num="0033">In some aspects, the semantic classes provided by a pre-trained semantic segmentation model can be consolidated to ensure they are directly transferable to the depth task. That is, some of the object classes, which are useful for segmentation tasks, may be intractable or irrelevant for depth tasks. For example, though a semantic segmentation model may be configured to treat &#x201c;road&#x201d; and &#x201c;sidewalk&#x201d; as separate object classes, it is not necessary to treat them differently on a depth map, as both are generally ground surfaces. Indeed, treating such classes as distinct in the depth task can reduce model accuracy, as they exhibit highly similar depth variation patterns in the field of view.</p><p id="p-0035" num="0034">As such, in some aspects, the object classes are regrouped based on their visual and geometric characteristics. For example, a user may specify groupings of the available semantic classes. This allows the depth network to distill the key depth-relevant semantic information without introducing unnecessary difficulties to the learning process.</p><p id="p-0036" num="0035">In some aspects, this regrouping or consolidation can allow the model to accurately predict depths for thin structures better, as compared to conventional approaches. For example, the model may be able to generate a more accurate and clear depth estimation for objects such as lamp posts and telephone poles, as such thin objects may be grouped as a class in the semantics-to-depth distillation, which encourages the depth network to pay more attention to recognize such structures.</p><p id="p-0037" num="0036">Accordingly, aspects described herein overcome conventional limitations with monocular depth estimation through efficient cross-task distillation from a segmentation task to the depth task.</p><p id="p-0038" num="0037">Example Workflow for Training a Depth Model Based on a Depth-to-Segmentation Model</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>1</b>A</figref> depicts an example workflow <b>100</b>A for training a depth model <b>110</b> and a depth-to-segmentation model <b>125</b>.</p><p id="p-0040" num="0039">In the workflow <b>100</b>A, an input image <b>105</b> is processed using a depth model <b>110</b>. The depth model <b>110</b> is generally a machine learning model configured to generate depth maps based on input images. As discussed above, the depth map may indicate, for each pixel in the input image <b>105</b>, the depth (e.g., the distance from the camera) of the corresponding object covered by the pixel. In some aspects, the depth model is a neural network.</p><p id="p-0041" num="0040">As illustrated, during the training process, the generated depth map can be used to compute a depth loss <b>115</b>. In some aspects, the generated depth map is used to generate a synthesized version of the input image <b>105</b>, which can be compared against the original input image <b>105</b> in order to generate the depth loss <b>115</b>, as discussed in more detail below with reference to <figref idref="DRAWINGS">FIG. <b>2</b>A</figref>. In some aspects, the generated depth map is compared against a ground-truth depth map in order to generate the depth loss <b>115</b>, as discussed in more detail below with reference to <figref idref="DRAWINGS">FIG. <b>2</b>B</figref>. Generally, the depth loss <b>115</b> can be used to iteratively refine the depth model <b>110</b> (e.g., using backpropagation).</p><p id="p-0042" num="0041">In the illustrated workflow <b>100</b>A, a cross-task distillation module <b>120</b> is used to transfer semantic knowledge from a pre-trained segmentation model <b>130</b> to the depth model <b>110</b>. That is, if the depth model <b>110</b> is denoted as f<sub>D </sub>and the pre-trained semantic segmentation model is denoted as f<sub>S</sub>, then the cross-task distillation module <b>120</b> enables transfer of the knowledge of the teacher model, f<sub>S</sub>, to the student model, f<sub>D</sub>. However, unlike conventional knowledge distillation where teacher and student networks are used for the same visual task, f<sub>D </sub>and f<sub>S </sub>are used for two different tasks and their outputs are not directly comparable. In other words, given an input, the system cannot directly measure the difference between the outputs of f<sub>D </sub>(a depth map) and f<sub>S </sub>(a segmentation map) in order to generate a loss needed to train f<sub>D</sub>.</p><p id="p-0043" num="0042">In the illustrated aspect, therefore, a depth-to-segmentation model <b>125</b> (which may be denoted h<sub>D2S</sub>) is used. In an aspect, the depth-to-segmentation model <b>125</b> is a neural network. In some aspects, the depth-to-segmentation model <b>125</b> is a small neural network (e.g., with just two conventional convolution layers and one pointwise convolution layer, or with a pointwise convolutional layer preceded by zero to four convolution layers), enabling it to be trained efficiently and with minimal computational expenditure.</p><p id="p-0044" num="0043">For example, in one aspect, the depth-to-segmentation model <b>125</b> consists of two 3&#xd7;3 convolutional layers (or any number of convolutional layers), each followed by a BatchNorm layer and a ReLu layer, as well as a pointwise convolutional layer at the end which outputs the segmentation map. In some aspects, using a deeper network for the depth-to-segmentation model <b>125</b> may result in improved accuracy of the depth model <b>110</b>, but these improvements are not as significant as those achieved using a smaller depth-to-segmentation model <b>125</b>. Further, a larger or deeper depth-to-segmentation model <b>125</b> may take an outsized role in the learning, thereby weakening the knowledge flow to the depth model <b>110</b>.</p><p id="p-0045" num="0044">In the workflow <b>100</b>A, the depth-to-segmentation model <b>125</b> receives the depth map generated by the depth model <b>110</b> and translates it to a semantic segmentation map. Stated differently, the depth-to-segmentation model <b>125</b> generates a segmentation map based on an input depth map.</p><p id="p-0046" num="0045">Additionally, as illustrated, the pre-trained segmentation model <b>130</b> is used to generate a segmentation map based on the original input image <b>105</b>. Although a pre-trained segmentation model <b>130</b> is depicted, in some aspects, the cross-task distillation module <b>120</b> can use a ground-truth segmentation map for the input image <b>105</b> (e.g., provided by a user), rather than processing the input image <b>105</b> using the pre-trained segmentation model <b>130</b> to generate one. As used herein, the segmentation map used to generate the segmentation loss <b>135</b> (which may be provided by a user or generated by the pre-trained segmentation model <b>130</b>) may be referred to as a &#x201c;ground-truth&#x201d; segmentation map to reflect that it is used as a ground-truth in computing the loss, even though it may in fact be a psuedo ground-truth map generated by the trained model. As used herein, the term &#x201c;ground-truth segmentation map&#x201d; can include both true or actual segmentation maps (e.g., provided by a user), as well as psuedo or generated ground-truth segmentation maps (e.g., generated by the pre-trained segmentation model <b>130</b>).</p><p id="p-0047" num="0046">Given the segmentation map generated by the depth-to-segmentation model <b>125</b> (based on the predicted depth map generated by the depth model <b>110</b>) and the segmentation map generated by the pre-trained segmentation model <b>130</b> (based on the input image <b>105</b>), the system is able to construct a segmentation loss <b>135</b>. This segmentation loss <b>135</b> can then be used to distill the semantic knowledge from f<sub>S </sub>to f<sub>D</sub>. In one aspect, this new segmentation loss <b>135</b> is defined using Equation 1 below, where <img id="CUSTOM-CHARACTER-00001" he="2.46mm" wi="2.12mm" file="US20230005165A1-20230105-P00001.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>D2S</sub>(&#xb7;) is the segmentation loss <b>135</b>. S<sub>t</sub><sup>D </sup>is the semantic segmentation map generated by the depth-to-segmentation model <b>125</b> based on the predicted depth map generated by the depth model <b>110</b> given input image <b>105</b>. That is, S<sub>t</sub><sup>D</sup>=h<sub>D2S</sub>(f<sub>D </sub>(I<sub>t</sub>)), where I<sub>t </sub>is the input image <b>105</b>. Additionally, S<sub>t </sub>is the semantic segmentation output generated by the pre-trained semantic segmentation model <b>130</b>, <img id="CUSTOM-CHARACTER-00002" he="2.46mm" wi="2.12mm" file="US20230005165A1-20230105-P00001.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>CE </sub>denotes cross-entropy loss, and H and W are the height and width of the input image <b>105</b>.</p><p id="p-0048" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <msub>       <mi>&#x2112;</mi>       <mrow>        <mi>D</mi>        <mo>&#x2062;</mo>        <mn>2</mn>        <mo>&#x2062;</mo>        <mi>S</mi>       </mrow>      </msub>      <mo>(</mo>      <mrow>       <msubsup>        <mi>S</mi>        <mi>t</mi>        <mi>D</mi>       </msubsup>       <mo>,</mo>       <msub>        <mi>S</mi>        <mi>t</mi>       </msub>      </mrow>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mrow>      <munderover>       <mo>&#x2211;</mo>       <mrow>        <mi>i</mi>        <mo>=</mo>        <mn>1</mn>       </mrow>       <mi>H</mi>      </munderover>      <mrow>       <munderover>        <mo>&#x2211;</mo>        <mrow>         <mi>j</mi>         <mo>=</mo>         <mn>1</mn>        </mrow>        <mi>W</mi>       </munderover>       <mfrac>        <mrow>         <msub>          <mi>&#x2112;</mi>          <mi>CE</mi>         </msub>         <mo>(</mo>         <mrow>          <mrow>           <msubsup>            <mi>S</mi>            <mi>t</mi>            <mi>D</mi>           </msubsup>           <mo>(</mo>           <mrow>            <mi>i</mi>            <mo>,</mo>            <mi>j</mi>           </mrow>           <mo>)</mo>          </mrow>          <mo>,</mo>          <mrow>           <msub>            <mi>S</mi>            <mi>t</mi>           </msub>           <mo>(</mo>           <mrow>            <mi>i</mi>            <mo>,</mo>            <mi>j</mi>           </mrow>           <mo>)</mo>          </mrow>         </mrow>         <mo>)</mo>        </mrow>        <mi>HW</mi>       </mfrac>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mrow>      <mi>Eq</mi>      <mo>.</mo>      <mtext>   </mtext>      <mn>1</mn>     </mrow>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0049" num="0047">In the illustrated workflow <b>100</b>A, the segmentation loss <b>135</b> can be used to allow the depth-to-segmentation model <b>125</b> to be jointly trained with the depth model <b>110</b>. This makes it possible for the pre-trained segmentation model <b>130</b> to provide semantic supervision to the depth model <b>110</b>, by backpropagating the segmentation loss <b>135</b> through the depth-to-segmentation model <b>125</b>. That is, the segmentation loss <b>135</b> can be backpropagated through the depth-to-segmentation model <b>125</b> (e.g., generating gradients for each layer), and the resulting tensor or gradients output from the depth-to-segmentation model <b>125</b> can be backpropagated through the depth model <b>110</b>.</p><p id="p-0050" num="0048">Although the illustrated workflow <b>100</b>A depicts a single input image <b>105</b> (suggesting stochastic gradient descent) for conceptual clarity, in aspects, the training workflow <b>100</b>A may be used to provide training in batches of input images <b>105</b>.</p><p id="p-0051" num="0049">In some aspects, as discussed above, the semantic classes used by the pre-trained segmentation model <b>130</b> may be consolidated or grouped to enable improved training of the depth model <b>110</b>. The semantic segmentation can often contain more fine-grained visual recognition information that is not present or realistic in depth maps. For example, road objects and sidewalk objects are typically treated as two different semantic classes, but depth maps generally do not contain such classification information as both road and sidewalk are on the ground plane and have similar depth variations. As a result, it is not necessary to differentiate them on the depth map. On the other hand, the depth map does contain the information for differentiating certain classes. For instance, a road participant (e.g., pedestrian, vehicle) can be easily separated from the background (e.g., road, building) given the different patterns of their depth values.</p><p id="p-0052" num="0050">In some aspects, therefore, the semantic classes may be grouped or consolidated such that the semantic information is preserved while the unnecessary complexity is removed from the distillation. In one such aspect, the classes are consolidated to a first group for objects in the foreground (e.g., vehicles, pedestrians, signs, and the like) and a second group for objects in the background (e.g., buildings, the ground itself, and the like). In at least one aspect, the objects in the foreground are delineated into two subgroups based, for example, on their shapes. For example, the system may use a first group (or subgroup) for thin structures (e.g., traffic lights and signs, poles, and the like) and a second group (or subgroup) for broader shapes (such as people, vehicles, and the like).</p><p id="p-0053" num="0051">Similarly, objects in the background may be split into a third group (or subgroup) and a fourth group (or subgroup), where the third group contains the background objects (e.g., buildings, vegetation, and the like) while the fourth group includes the ground (e.g., roads, sidewalks, and the like). This class consolidation, applied to the segmentation map generated by the pre-trained segmentation model <b>130</b>, can improve the resulting accuracy of the depth model <b>110</b>. In some aspects, this consolidation is performed based on a user-specified configuration (e.g., indicating which classes should be consolidated to a given group). In one aspect, consolidating the classes includes relabeling the segmentation map based on the groupings of classes. For example, if light poles and signs are consolidated to the same class, then they will be assigned the same value in the (new) segmentation map. The depth-to-segmentation model <b>125</b> is generally configured to output segmentation maps based on the consolidated classes.</p><p id="p-0054" num="0052">As discussed above, the distillation approach only adds a small amount of computation to training, as the depth-to-segmentation model <b>125</b> can be small. Moreover, the segmentation maps from the teacher network (pre-trained segmentation model <b>130</b>) need only be computed once for each training input image <b>105</b>, and can thereafter be re-used as needed. This improves over existing systems that co-train a segmentation model alongside the depth model (which may require proccessing images with the segmentation model many times during training).</p><p id="p-0055" num="0053">Example Workflow for Inferencing Using a Trained Depth Model</p><p id="p-0056" num="0054"><figref idref="DRAWINGS">FIG. <b>1</b>B</figref> depicts an example workflow <b>100</b>B for inferencing using a trained depth model <b>110</b>.</p><p id="p-0057" num="0055">In the illustrated aspect, the depth model <b>110</b> has been trained using a cross-task distillation module, such as cross-task distillation module <b>120</b> discussed above with reference to <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>. That is, the depth model <b>110</b> may be trained based at least in part on a segmentation loss generated with the aid of a depth-to-segmentation model (such as depth-to-segmentation model <b>125</b>, discussed above with reference to <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>). In this way, the depth model <b>110</b> learns segmentation knowledge that can enable significantly improved depth estimations.</p><p id="p-0058" num="0056">Once the training is finished (e.g., determined based on termination criteria such as sufficient accuracy or otherwise determining that the model is sufficiently trained), the depth model <b>110</b> can run in a standalone manner, without requiring any extra computation of semantic information during inference. That is, input images <b>140</b> can be processed by the depth model <b>110</b> to generate accurate depth maps <b>145</b>, without passing any data through the depth-to-segmentation model <b>125</b> or pre-trained segmentation model <b>130</b> of <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>. In some aspects, therefore, the cross-task distillation module <b>120</b> can be discarded after training. In some aspects, the depth-to-segmentation model <b>125</b> can be stored for use with future refinements or training.</p><p id="p-0059" num="0057">In some aspects, during inferencing, however, only the depth model <b>110</b> is used. Because the depth model <b>110</b> is trained in a more semantic-aware manner using cross-task distillation, it exhibits superior accuracy as compared to existing systems.</p><p id="p-0060" num="0058">Further, because the workflow <b>100</b>B does not use a separate segmentation network or depth-to-segmentation model during inferencing, the computational resources needed (e.g., power consumption, latency, memory footprint, number of operations, and the like) are significantly reduced as compared to existing systems.</p><p id="p-0061" num="0059">Example Workflow for Training a Depth Model Based on Photometric Loss</p><p id="p-0062" num="0060"><figref idref="DRAWINGS">FIG. <b>2</b>A</figref> depicts an example workflow <b>200</b>A for training a depth model using photometric loss and segmentation loss. The workflow <b>200</b>A generally provides more detail for the computation of the depth loss <b>115</b>, discussed above with reference to <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>. Specifically, the workflow <b>200</b>A uses a self-supervised approach to enable computation of a depth loss <b>115</b>A and training of the depth model <b>110</b> without the need for ground-truth depth maps.</p><p id="p-0063" num="0061">In the illustrated workflow <b>200</b>B, input images <b>105</b>A and <b>105</b>B are neighboring (e.g., adjacent) or close (e.g., within a defined number of frames or timestamps) frames from a video. Both are provided to a pose model <b>205</b>, which is configured to determine the relative camera motion between the input images <b>105</b>A and <b>105</b>B. Generally, the pose model <b>205</b> is a machine learning model (e.g., a neural network) that infers camera pose (e.g., locations and orientations in six-degrees of freedom) for input images.</p><p id="p-0064" num="0062">For example, consider two neighboring or close video frames, I<sub>t </sub>and I<sub>s </sub>(e.g., input images <b>105</b>A and <b>105</b>B). Suppose that pixel P<sub>t</sub>&#x2208;I<sub>t </sub>and pixel P<sub>s</sub>&#x2208;I<sub>s </sub>are two different views of the same point of an object. In such a case, p<sub>t </sub>and p<sub>s </sub>are related geometrically as indicated in Equation 2 below, where h(p)=[h, w, 1] denotes the homogeneous coordinates of a pixel p with h and w being its vertical and horizontal positions on the image, d(p) is the depth at p, K is the camera intrinsic matrix, and T<sub>t&#x2192;s </sub>is the six-degree-of-freedom relative camera motion/nose from t to s</p><p id="p-0065" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mrow>       <mi>d</mi>       <mo>&#x2061;</mo>       <mo>(</mo>       <msub>        <mi>p</mi>        <mi>s</mi>       </msub>       <mo>)</mo>      </mrow>      <mo>&#x2062;</mo>      <mrow>       <mi>h</mi>       <mo>&#x2061;</mo>       <mo>(</mo>       <msub>        <mi>p</mi>        <mi>s</mi>       </msub>       <mo>)</mo>      </mrow>     </mrow>     <mo>=</mo>     <mrow>      <mrow>       <mo>[</mo>       <mrow>        <mi>K</mi>        <mo>&#x2062;</mo>        <mrow>         <semantics definitionURL="">          <mo>&#x2758;</mo>          <annotation encoding="Mathematica">"\[LeftBracketingBar]"</annotation>         </semantics>         <mn>0</mn>        </mrow>       </mrow>       <mo>]</mo>      </mrow>      <mo>&#x2062;</mo>      <mrow>       <msub>        <mi>T</mi>        <mrow>         <mi>t</mi>         <mo>&#x2192;</mo>         <mi>s</mi>        </mrow>       </msub>       <mo>[</mo>       <mtable>        <mtr>         <mtd>          <mrow>           <msup>            <mi>K</mi>            <mrow>             <mo>-</mo>             <mn>1</mn>            </mrow>           </msup>           <mo>&#x2062;</mo>           <mrow>            <mi>d</mi>            <mo>&#x2061;</mo>            <mo>(</mo>            <msub>             <mi>p</mi>             <mi>t</mi>            </msub>            <mo>)</mo>           </mrow>           <mo>&#x2062;</mo>           <mrow>            <mi>h</mi>            <mo>&#x2061;</mo>            <mo>(</mo>            <msub>             <mi>p</mi>             <mi>t</mi>            </msub>            <mo>)</mo>           </mrow>           <mtext>   </mtext>          </mrow>         </mtd>        </mtr>        <mtr>         <mtd>          <mrow>           <mn>1</mn>           <mtext>                                </mtext>          </mrow>         </mtd>        </mtr>       </mtable>       <mtext> </mtext>       <mo>]</mo>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mrow>      <mi>Eq</mi>      <mo>.</mo>      <mtext>   </mtext>      <mn>2</mn>     </mrow>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0066" num="0063">The determined pose, generated by the pose model <b>205</b>, is provided to a view synthesizer <b>210</b>. Additionally, the input image <b>105</b>B can be provided to the depth model <b>110</b> to generate a predicted depth map for the image <b>105</b>B. As depicted in the workflow <b>100</b>B, this generated depth map is then provided to the view synthesizer <b>210</b>.</p><p id="p-0067" num="0064">Given the generated depth map of I<sub>t </sub>(image <b>105</b>B), which is output by the depth model <b>110</b> and may be denoted D<sub>t</sub>, along with the relative camera pose from I<sub>t </sub>(image <b>105</b>B) to I<sub>s </sub>(image <b>105</b>A), which is output by the pose model <b>205</b>, the view synthesizer <b>210</b> can synthesize I<sub>t </sub>from I<sub>s </sub>based on Equation 2, assuming that the points captured in I<sub>t </sub>are also present in I<sub>s</sub>. The synthesized version of the input image <b>105</b>B (I<sub>t</sub>) may be denoted as &#xce;<sub>t</sub>.</p><p id="p-0068" num="0065">As illustrated, by minimizing the difference between the synthesized image &#xce;<sub>t </sub>and the actual image <b>105</b>B (indicated by depth loss <b>115</b>A), the system can train the pose model <b>205</b> and depth model <b>110</b>. In some aspects, this depth loss <b>115</b>A is referred to as a photometric loss (denoted <img id="CUSTOM-CHARACTER-00003" he="2.46mm" wi="2.12mm" file="US20230005165A1-20230105-P00001.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>H</sub>), and may be defined using Equation 3 below, where where &#x2225;&#xb7;&#x2225;<sub>1 </sub>denotes the <img id="CUSTOM-CHARACTER-00004" he="2.46mm" wi="2.12mm" file="US20230005165A1-20230105-P00001.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>1 </sub>norm and SSIM is the Structural Similarity Index Measure. Note that <img id="CUSTOM-CHARACTER-00005" he="2.46mm" wi="2.12mm" file="US20230005165A1-20230105-P00001.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>PH </sub>is computed in a per-pixel manner.</p><p id="p-0069" num="0000"><maths id="MATH-US-00003" num="00003"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <msub>       <mi>&#x2112;</mi>       <mi>PH</mi>      </msub>      <mo>(</mo>      <mrow>       <msub>        <mi>I</mi>        <mi>t</mi>       </msub>       <mo>,</mo>       <msub>        <mover>         <mi>I</mi>         <mo>^</mo>        </mover>        <mi>t</mi>       </msub>      </mrow>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mrow>      <mrow>       <mi>&#x3b1;</mi>       <mo>&#x2062;</mo>       <msub>        <mrow>         <mo>&#xf605;</mo>         <mrow>          <msub>           <mi>I</mi>           <mi>t</mi>          </msub>          <mo>-</mo>          <msub>           <mover>            <mi>I</mi>            <mo>^</mo>           </mover>           <mi>t</mi>          </msub>         </mrow>         <mo>&#xf606;</mo>        </mrow>        <mn>1</mn>       </msub>      </mrow>      <mo>+</mo>      <mrow>       <mrow>        <mo>(</mo>        <mrow>         <mn>1</mn>         <mo>-</mo>         <mi>&#x3b1;</mi>        </mrow>        <mo>)</mo>       </mrow>       <mo>&#x2062;</mo>       <mfrac>        <mrow>         <mn>1</mn>         <mo>-</mo>         <mrow>          <mi>SSIM</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <mrow>           <msub>            <mi>I</mi>            <mi>t</mi>           </msub>           <mo>,</mo>           <msub>            <mover>             <mi>I</mi>             <mo>^</mo>            </mover>            <mi>t</mi>           </msub>          </mrow>          <mo>)</mo>         </mrow>        </mrow>        <mn>2</mn>       </mfrac>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mrow>      <mi>Eq</mi>      <mo>.</mo>      <mtext>   </mtext>      <mn>3</mn>     </mrow>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0070" num="0066">In some aspects, the system may further include a smoothness regularization or loss to prevent drastic variations in the predicted depth map. Additionally, in some aspects, not all the 3D points in I<sub>t </sub>can be found in I<sub>s </sub>(e.g., due to occlusion and objects (entirely or partially) moving out of the frame). Some objects can also be moving (e.g., cars), which is not considered in the geometric model of Equation 2. In one such aspect, in order to correctly measure the photometric loss and train the networks, the system can mask out the pixel points that violate the geometric model.</p><p id="p-0071" num="0067">In the illustrated workflow <b>200</b>A, the depth model <b>110</b> is also refined based on the segmentation loss <b>135</b> (propagated through the depth-to-segmentation model <b>125</b>), as discussed above. In one aspect, the total loss (<img id="CUSTOM-CHARACTER-00006" he="2.46mm" wi="2.12mm" file="US20230005165A1-20230105-P00001.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>Total</sub>) for the depth model <b>110</b> can therefore defined using Equation 4 below, where the self-supervised depth loss is computed over N<sub>s </sub>scales, <img id="CUSTOM-CHARACTER-00007" he="2.46mm" wi="2.12mm" file="US20230005165A1-20230105-P00001.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>PH,k </sub>is the photometric loss for the k<sup>th </sup>scale, &#x3bb;<sub>SM,k </sub>and <img id="CUSTOM-CHARACTER-00008" he="2.46mm" wi="2.12mm" file="US20230005165A1-20230105-P00001.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>SM,k </sub>are the weight and loss for the smoothness regularization for the k<sup>th </sup>scale, and &#x3bb;<sub>D2S </sub>is the weight of the cross-task distillation loss, L<sub>D2S</sub>.</p><p id="p-0072" num="0000"><maths id="MATH-US-00004" num="00004"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <msub>      <mi>&#x2112;</mi>      <mi>Total</mi>     </msub>     <mo>=</mo>     <mrow>      <mrow>       <munderover>        <mo>&#x2211;</mo>        <mrow>         <mi>k</mi>         <mo>=</mo>         <mn>1</mn>        </mrow>        <msub>         <mi>N</mi>         <mi>s</mi>        </msub>       </munderover>       <msub>        <mi>&#x2112;</mi>        <mrow>         <mi>PH</mi>         <mo>,</mo>         <mi>k</mi>        </mrow>       </msub>      </mrow>      <mo>+</mo>      <mrow>       <munderover>        <mo>&#x2211;</mo>        <mrow>         <mi>k</mi>         <mo>=</mo>         <mn>1</mn>        </mrow>        <msub>         <mi>N</mi>         <mi>s</mi>        </msub>       </munderover>       <mrow>        <msub>         <mi>&#x3bb;</mi>         <mrow>          <mi>SM</mi>          <mo>,</mo>          <mi>k</mi>         </mrow>        </msub>        <mo>&#x2062;</mo>        <msub>         <mi>&#x2112;</mi>         <mrow>          <mi>SM</mi>          <mo>,</mo>          <mi>k</mi>         </mrow>        </msub>       </mrow>      </mrow>      <mtext> </mtext>      <mo>+</mo>      <mrow>       <msub>        <mi>&#x3bb;</mi>        <mrow>         <mi>D</mi>         <mo>&#x2062;</mo>         <mn>2</mn>         <mo>&#x2062;</mo>         <mi>s</mi>        </mrow>       </msub>       <mo>&#x2062;</mo>       <msub>        <mi>&#x2112;</mi>        <mrow>         <mi>D</mi>         <mo>&#x2062;</mo>         <mn>2</mn>         <mo>&#x2062;</mo>         <mi>s</mi>        </mrow>       </msub>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mrow>      <mi>Eq</mi>      <mo>.</mo>      <mtext>   </mtext>      <mn>4</mn>     </mrow>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0073" num="0068">Example Workflow for Training a Depth Model Based on Ground-Truth Maps</p><p id="p-0074" num="0069"><figref idref="DRAWINGS">FIG. <b>2</b>B</figref> depicts an example workflow <b>200</b>B for training a depth model using ground-truth depth maps and segmentation loss. The workflow <b>200</b>B generally provides more detail for the computation of the depth loss <b>115</b>, discussed above with reference to <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>. Specifically, the workflow <b>200</b>B uses ground-truth depth maps <b>230</b> to compute the depth loss <b>115</b>B.</p><p id="p-0075" num="0070">In the illustrated workflow <b>200</b>B, a depth-to-segmentation model <b>125</b> can be used to compute a segmentation loss <b>135</b> which is used to refine the depth model <b>110</b>, as discussed above with reference to <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>.</p><p id="p-0076" num="0071">As further illustrated, for each input image <b>105</b>, a corresponding ground-truth depth map <b>230</b> is used to compute the depth loss <b>115</b>B. For example, the system may use cross-entropy to compute the depth loss <b>115</b>B loss based on the ground-truth depth map <b>230</b> and predicted depth map generated by the depth model <b>110</b>. This depth loss <b>115</b>B can then be used, along with the segmentation loss <b>135</b>, the refine the depth model <b>110</b>.</p><p id="p-0077" num="0072">Example Method for Training Machine Learning Models for Depth Estimation</p><p id="p-0078" num="0073"><figref idref="DRAWINGS">FIG. <b>3</b></figref> depicts an example flow diagram illustrating a method <b>300</b> for training machine learning models for depth estimation.</p><p id="p-0079" num="0074">The method <b>300</b> begins at block <b>305</b>, where a training input image (e.g., input image <b>105</b> depicted in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>) is received. In one aspect, the input image is generally a two-dimensional image depicting a three-dimensional scene with various objects are various depths. For example, the input image may be captured by a camera on a self-driving vehicle, and depict objects such as pedestrians, vehicles, signs, the road, and the like.</p><p id="p-0080" num="0075">At block <b>310</b>, the system generates a depth map based on the received image. For example, a depth neural network (such as depth model <b>110</b>, discussed above with reference to <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>) may be used to translate the image to a depth map. As discussed above, the depth map generally indicates, for each pixel, the depth of the underlying object in the scene.</p><p id="p-0081" num="0076">At block <b>315</b>, the system computes a depth loss (e.g., depth loss <b>115</b> depicted in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>). In one aspect, the system does so using a photometric loss computed based on the input image and one or more adjacent or nearby images from a video stream. One such aspect is discussed above with reference to <figref idref="DRAWINGS">FIG. <b>2</b>A</figref>. In another aspect, the depth loss may be computed using a ground-truth depth map for the received image, as discussed above with reference to <figref idref="DRAWINGS">FIG. <b>2</b>B</figref>.</p><p id="p-0082" num="0077">At block <b>320</b>, the system generates a segmentation map based on the depth map generated in block <b>310</b> using the depth model. For example, as discussed above, the system may use a depth-to-segmentation model <b>125</b> as part of a cross-task distillation module <b>120</b> (discussed with reference to <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>) to generate the segmentation map. In some aspects, the depth-to-segmentation model is a lightweight neural network (e.g., with a relatively few number of parameters, as compared to the depth model or pre-trained segmentation model), reducing computational expense.</p><p id="p-0083" num="0078">At block <b>325</b>, the system can compute a segmentation loss based on the segmentation map generated in block <b>320</b>. For example, in one aspect, the system can compute a cross entropy loss between the generated segmentation map and a ground-truth segmentation map for the received image. In some aspects, the system uses a pre-trained segmentation model to generate a segmentation map (which may be referred to in some aspects as the ground-truth map) based on the image, and computes the loss based on the generated segmentation maps. In other aspects, the system uses a provided ground-truth segmentation map (e.g., provided by a user) to compute the loss. In at least one aspect, the semantic classes of the ground-truth segmentation map can be consolidated (e.g., to match the classes output by the depth-to-segmentation model), as discussed above, prior to generating the segmentation loss.</p><p id="p-0084" num="0079">At block <b>330</b>, the system refines a first model (e.g., the depth model used to generate the depth map in block <b>310</b>) based on the depth loss and the segmentation loss. For example, the system may use backpropagation to refine the internal weights or other parameters of the depth model based on the depth loss. Further, the system may backpropagate the segmentation loss through the depth-to-segmentation model and subsequently through the depth model in order to refine the weights to gain semantic segmentation knowledge.</p><p id="p-0085" num="0080">At block <b>335</b>, the system similarly refines a second model (e.g., the depth-to-segmentation model used to generate the segmentation map in block <b>320</b>) using the segmentation loss, as discussed above.</p><p id="p-0086" num="0081">In this way, a depth model and lightweight depth-to-segmentation model are co-trained to allow segmentation knowledge to be passed to the depth model. This process can significantly improve the depth estimation accuracy of the depth model, as discussed above.</p><p id="p-0087" num="0082">Though the method <b>300</b> depicts refining the models for each individual training sample (e.g., using stochastic gradient descent), in some aspects, the system may instead use batch training.</p><p id="p-0088" num="0083">Note that <figref idref="DRAWINGS">FIG. <b>3</b></figref> is just one example of a method, and other methods including fewer, additional, or alternative steps are possible consistent with this disclosure.</p><p id="p-0089" num="0084">Example Method for Generating Segmentation Loss to Refine Depth Models</p><p id="p-0090" num="0085"><figref idref="DRAWINGS">FIG. <b>4</b></figref> depicts an example flow diagram illustrating a method <b>400</b> for generating segmentation loss to refine depth models. In some aspects, the method <b>400</b> provides additional detail for blocks <b>320</b> and <b>325</b> in <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0091" num="0086">The method <b>400</b> begins at block <b>405</b>, where a segmentation map is generated by processing the received input image using a pre-trained segmentation model (e.g., the pre-trained segmentation model of <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>). This segmentation map can be used as a ground-truth to compute a segmentation loss.</p><p id="p-0092" num="0087">At block <b>410</b>, the system can consolidate the classes of the generated segmentation map. That is, as discussed above, the system may group two or more of the relevant classes based on a defined consolidation configuration (e.g., specified by a user). In one aspect, a user may specify a set of groups, where each group includes one or more classes output by the segmentation model. For example, the system may consolidate all ground classes (e.g., road, sidewalk, grass, and the like) to a first group, while ordinary obstacles (e.g., vehicles and pedestrians) are consolidated to a second group and thin obstacles (e.g., light poles) are consolidated to a third.</p><p id="p-0093" num="0088">As discussed above, this class consolidation can improve prediction accuracy by training the models to focus on depth-relevant features of each semantic class. In some aspects, consolidating the classes in the segmentation map is performed to ensure that the classes in the segmentation map match the classes that the depth-to-segmentation model is configured to output.</p><p id="p-0094" num="0089">At block <b>415</b>, the system generates a segmentation map based on the depth map. For example, as discussed above with reference to block <b>320</b> in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the system may process the depth map (generated by the depth model) using a depth-to-segmentation model that is configured to translate depth maps into segmentation maps. As discussed above, in some aspects, the depth-to-segmentation model is configured to output classes corresponding to the consolidated groups of classes discussed above.</p><p id="p-0095" num="0090">At block <b>420</b>, the system computes a segmentation loss based on these two segmentation maps. For example, the system may compute a cross-entropy loss. As discussed above, the segmentation loss can then be used to refine the depth-to-segmentation model via backpropagation, as well as the depth model by backpropagation through the depth-to-segmentation model and then through the segmentation model.</p><p id="p-0096" num="0091">Note that <figref idref="DRAWINGS">FIG. <b>4</b></figref> is just one example of a method, and other methods including fewer, additional, or alternative steps are possible consistent with this disclosure.</p><p id="p-0097" num="0092">Example Method for Training and Inferencing Using Depth Models</p><p id="p-0098" num="0093"><figref idref="DRAWINGS">FIG. <b>5</b></figref> depicts an example flow diagram illustrating a method <b>500</b> for training and inferencing using a depth model trained with the aid of a depth-to-segmentation model.</p><p id="p-0099" num="0094">The method begins at block <b>505</b>, where the system trains or refines a depth model (e.g., depth model <b>110</b> in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>) and a depth-to-segmentation model (e.g., depth-to-segmentation model <b>125</b> in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>). This may be accomplished, for example, using the method <b>300</b> discussed above with reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, and/or the workflow <b>100</b>A discussed above with reference to <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>. For example, the system may backpropagate a depth loss through the depth model. The system may also backpropagate a segmentation loss through the depth model, via the depth-to-segmentation model.</p><p id="p-0100" num="0095">At block <b>510</b>, the system determines whether training is complete or sufficient for deployment. This termination criteria may include a variety of elements, such as a minimum prediction accuracy of the depth model, a maximum time or amount of resources spent training the models, a number of epochs, whether any additional training samples remain, and the like. If the system determines that training is not complete, then the method <b>500</b> returns to block <b>505</b>.</p><p id="p-0101" num="0096">If, at block <b>510</b>, the system determines that training is complete or sufficient, then the method <b>500</b> continues to block <b>515</b>, where the depth model is deployed for use in inferencing. In some aspects, as discussed above, the depth-to-segmentation model is discarded or otherwise not used during inferencing. That is, during inferencing, input is not passed through the depth-to-segmentation model (or the pre-trained segmentation model, if one is used during training).</p><p id="p-0102" num="0097">At block <b>520</b>, the system can then process input (e.g., images) using the depth model in order to generate depth maps. As only the depth model is used during inferencing, the computational resources needed remain lower than conventional systems that use segmentation models. Further, as the depth model is trained using cross-task distillation, it provides improved accuracy over solely depth-based models.</p><p id="p-0103" num="0098">Note that <figref idref="DRAWINGS">FIG. <b>5</b></figref> is just one example of a method, and other methods including fewer, additional, or alternative steps are possible consistent with this disclosure.</p><p id="p-0104" num="0099">Example Method for Training Machine Learning Models Based on Segmentation Loss</p><p id="p-0105" num="0100"><figref idref="DRAWINGS">FIG. <b>6</b></figref> depicts an example flow diagram illustrating a method <b>600</b> for training a machine learning model based on segmentation loss.</p><p id="p-0106" num="0101">At block <b>605</b>, a depth map is generated by processing an input image (e.g., input image <b>105</b> in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>) using a first machine learning model (e.g., depth model <b>110</b> in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>).</p><p id="p-0107" num="0102">In some aspects, the input image is received from a monocular source.</p><p id="p-0108" num="0103">At block <b>610</b>, a segmentation map is generated by processing the depth map using a second machine learning model (e.g., depth-to-segmentation model <b>125</b> in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>).</p><p id="p-0109" num="0104">At block <b>615</b>, a segmentation loss (e.g., segmentation loss <b>135</b> in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>) is computed based on the segmentation map and a ground-truth segmentation map.</p><p id="p-0110" num="0105">In some aspects, the ground-truth segmentation map is generated by processing the input image using a pre-trained segmentation machine learning model (e.g., pre-trained segmentation model <b>130</b> in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>).</p><p id="p-0111" num="0106">In some aspects, the ground-truth segmentation map comprises a set of classes, and computing the segmentation loss comprises consolidating the set of classes to a subset of classes, wherein the subset of classes contains fewer classes than the set of classes.</p><p id="p-0112" num="0107">At block <b>620</b>, the first machine learning model is refined based on the segmentation loss.</p><p id="p-0113" num="0108">In some aspects, the method <b>600</b> further includes refining the second machine learning model based on the segmentation loss.</p><p id="p-0114" num="0109">In some aspects, refining the second machine learning model based on the segmentation loss comprises generating a plurality of gradients by backpropagating the segmentation loss through the second machine learning model, and refining the first machine learning model based on the segmentation loss comprises backpropagating the plurality of gradients through the first machine learning model.</p><p id="p-0115" num="0110">In some aspects, the method <b>600</b> further includes computing a depth loss based at least in part on the depth map, and refining the first machine learning model based on the depth loss.</p><p id="p-0116" num="0111">In some aspects, the depth loss is computed based further on a ground-truth depth map.</p><p id="p-0117" num="0112">In some aspects, the depth loss is a photometric loss computed by generating a synthesized version of the input image based on the depth map and at least a second input image and computing the photometric loss based on the synthesized version of the input image and the input image.</p><p id="p-0118" num="0113">In some aspects, to generate output during inferencing, the first machine learning model is used to generate depth maps based on input images and the second machine learning model is not used during inferencing.</p><p id="p-0119" num="0114">Note that <figref idref="DRAWINGS">FIG. <b>6</b></figref> is just one example of a method, and other methods including fewer, additional, or alternative steps are possible consistent with this disclosure.</p><p id="p-0120" num="0115">Example Method for Inferencing using a Machine Learning Model trained using</p><p id="p-0121" num="0116">Segmentation Loss</p><p id="p-0122" num="0117"><figref idref="DRAWINGS">FIG. <b>7</b></figref> depicts an example flow diagram illustrating a method <b>700</b> for inferencing using a machine learning model trained using segmentation loss.</p><p id="p-0123" num="0118">At block <b>705</b>, an input image is received.</p><p id="p-0124" num="0119">At block <b>710</b>, an output depth map is generated by processing the input image using a first machine learning model (e.g., depth model <b>110</b> in <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>).</p><p id="p-0125" num="0120">At block <b>715</b>, a segmentation map is generated by processing the output depth map using a second machine learning model.</p><p id="p-0126" num="0121">At block <b>720</b>, a segmentation loss (e.g., segmentation loss <b>135</b> in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>) is computed based on the segmentation map.</p><p id="p-0127" num="0122">At block <b>725</b>, the first machine learning model is refined based on the segmentation loss.</p><p id="p-0128" num="0123">In some aspects, the first machine learning model is used in a monocular system, and the input image is received from a monocular source.</p><p id="p-0129" num="0124">Note that <figref idref="DRAWINGS">FIG. <b>7</b></figref> is just one example of a method, and other methods including fewer, additional, or alternative steps are possible consistent with this disclosure.</p><p id="p-0130" num="0125">Example Processing System for Depth Models Based on Segmentation Loss</p><p id="p-0131" num="0126">In some aspects, the workflows, techniques, and methods described with reference to <figref idref="DRAWINGS">FIGS. <b>1</b>A-<b>7</b></figref> may be implemented on one or more devices or systems. <figref idref="DRAWINGS">FIG. <b>8</b></figref> depicts an example processing system <b>800</b> configured to perform various aspects of the present disclosure, including, for example, the techniques and methods described with respect to <figref idref="DRAWINGS">FIGS. <b>1</b>A-<b>7</b></figref>.</p><p id="p-0132" num="0127">Processing system <b>800</b> includes a central processing unit (CPU) <b>802</b>, which in some examples may be a multi-core CPU. Instructions executed at the CPU <b>802</b> may be loaded, for example, from a program memory associated with the CPU <b>802</b> or may be loaded from a memory partition <b>824</b>.</p><p id="p-0133" num="0128">Processing system <b>800</b> also includes additional processing components tailored to specific functions, such as a graphics processing unit (GPU) <b>804</b>, a digital signal processor (DSP) <b>806</b>, a neural processing unit (NPU) <b>808</b>, a multimedia processing unit <b>810</b>, and a wireless connectivity component <b>812</b>.</p><p id="p-0134" num="0129">An NPU, such as <b>808</b>, is generally a specialized circuit configured for implementing all the necessary control and arithmetic logic for executing machine learning algorithms, such as algorithms for processing artificial neural networks (ANNs), deep neural networks (DNNs), random forests (RFs), and the like. An NPU may sometimes alternatively be referred to as a neural signal processor (NSP), tensor processing units (TPU), neural network processor (NNP), intelligence processing unit (IPU), vision processing unit (VPU), or graph processing unit.</p><p id="p-0135" num="0130">NPUs, such as <b>808</b>, are configured to accelerate the performance of common machine learning tasks, such as image classification, machine translation, object detection, and various other predictive models. In some examples, a plurality of NPUs may be instantiated on a single chip, such as a system on a chip (SoC), while in other examples they may be part of a dedicated neural-network accelerator.</p><p id="p-0136" num="0131">NPUs may be optimized for training or inference, or in some cases configured to balance performance between both. For NPUs that are capable of performing both training and inference, the two tasks may still generally be performed independently.</p><p id="p-0137" num="0132">NPUs designed to accelerate training are generally configured to accelerate the optimization of new models, which is a highly compute-intensive operation that involves inputting an existing dataset (often labeled or tagged), iterating over the dataset, and then adjusting model parameters, such as weights and biases, in order to improve model performance. Generally, optimizing based on a wrong prediction involves propagating back through the layers of the model and determining gradients to reduce the prediction error.</p><p id="p-0138" num="0133">NPUs designed to accelerate inference are generally configured to operate on complete models. Such NPUs may thus be configured to input a new piece of data and rapidly process it through an already trained model to generate a model output (e.g., an inference).</p><p id="p-0139" num="0134">In one implementation, NPU <b>808</b> is a part of one or more of CPU <b>802</b>, GPU <b>804</b>, and/or DSP <b>806</b>.</p><p id="p-0140" num="0135">In some examples, wireless connectivity component <b>812</b> may include subcomponents, for example, for third generation (3G) connectivity, fourth generation (4G) connectivity (e.g., 4G LTE), fifth generation connectivity (e.g., 5G or NR), Wi-Fi connectivity, Bluetooth connectivity, and other wireless data transmission standards. Wireless connectivity component <b>812</b> is further connected to one or more antennas <b>814</b>.</p><p id="p-0141" num="0136">Processing system <b>800</b> may also include one or more sensor processing units <b>816</b> associated with any manner of sensor, one or more image signal processors (ISPs) <b>818</b> associated with any manner of image sensor, and/or a navigation processor <b>820</b>, which may include satellite-based positioning system components (e.g., GPS or GLONASS) as well as inertial positioning system components.</p><p id="p-0142" num="0137">Processing system <b>800</b> may also include one or more input and/or output devices <b>822</b>, such as screens, touch-sensitive surfaces (including touch-sensitive displays), physical buttons, speakers, microphones, and the like.</p><p id="p-0143" num="0138">In some examples, one or more of the processors of processing system <b>800</b> may be based on an ARM or RISC-V instruction set.</p><p id="p-0144" num="0139">Processing system <b>800</b> also includes memory <b>824</b>, which is representative of one or more static and/or dynamic memories, such as a dynamic random access memory, a flash-based static memory, and the like. In this example, memory <b>824</b> includes computer-executable components, which may be executed by one or more of the aforementioned processors of processing system <b>800</b>.</p><p id="p-0145" num="0140">In particular, in this example, memory <b>824</b> includes a depth component <b>824</b>A, a depth-to-segmentation component <b>824</b>B, a segmentation component <b>824</b>C, a training component <b>824</b>D, and an inferencing component <b>824</b>E. The memory <b>824</b> also includes model parameters <b>824</b>F. The depicted components, and others not depicted, may be configured to perform various aspects of the techniques described herein. Though depicted as discrete components for conceptual clarity in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, depth component <b>824</b>A, depth-to-segmentation component <b>824</b>B, segmentation component <b>824</b>C, training component <b>824</b>D, and inferencing component <b>824</b>E may be collectively or individually implemented in various aspects.</p><p id="p-0146" num="0141">Processing system <b>800</b> further comprises depth circuit <b>826</b>, depth-to-segmentation circuit <b>828</b>, and segmentation circuit <b>830</b>. The depicted circuits, and others not depicted, may be configured to perform various aspects of the techniques described herein.</p><p id="p-0147" num="0142">For example, depth component <b>824</b>A and depth circuit <b>826</b> may be used to generate depth maps based on input images. Depth-to-segmentation component <b>824</b>B and depth-to-segmentation circuit <b>828</b> may be used to generate segmentation maps based on generated depth maps. Segmentation component <b>824</b>C and segmentation circuit <b>830</b> may be used to generate segmentation maps based on input images. Training component <b>824</b>D may be used to train the various models, while inferencing component <b>824</b>E can be used to generate inferences using the trained depth model. The model parameters <b>824</b>F can include trainable parameters (such as weights and, in some aspects, scale values for various losses).</p><p id="p-0148" num="0143">Though depicted as separate components and circuits for clarity in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, depth circuit <b>826</b>, depth-to-segmentation circuit <b>828</b>, and segmentation circuit <b>830</b> may collectively or individually be implemented in other processing devices of processing system <b>800</b>, such as within CPU <b>802</b>, GPU <b>804</b>, DSP <b>806</b>, NPU <b>808</b>, and the like.</p><p id="p-0149" num="0144">Generally, processing system <b>800</b> and/or components thereof may be configured to perform the methods described herein.</p><p id="p-0150" num="0145">In some aspects, the processing system <b>800</b> can perform incremental on-device learning. For example, inferencing component <b>824</b>E may generate a depth map (e.g., using the depth component <b>824</b>A) for a received input image during runtime. This depth map may then be used (alone, or as part of a batch of maps generated during inferencing) to refine the depth model (e.g., by generating segmentation map(s) based on the depth map(s), computing segmentation loss(es) based on the segmentation map(s), and refining the depth model based on the segmentation loss(es), as discussed above).</p><p id="p-0151" num="0146">Notably, in other aspects, aspects of processing system <b>800</b> may be omitted, such as where processing system <b>800</b> is a server computer or the like. For example, multimedia component <b>810</b>, wireless connectivity component <b>812</b>, sensors <b>816</b>, ISPs <b>818</b>, and/or navigation component <b>820</b> may be omitted in other aspects. Further, aspects of processing system <b>800</b> maybe distributed between multiple devices.</p><heading id="h-0006" level="1">Example Clauses</heading><p id="p-0152" num="0147">Clause 1: A method, comprising: generating a depth map by processing an input image using a first machine learning model; generating a segmentation map by processing the depth map using a second machine learning model; computing a segmentation loss based on the segmentation map and a ground-truth segmentation map; and refining the first machine learning model based on the segmentation loss.</p><p id="p-0153" num="0148">Clause 2: The method according to Clause 1, further comprising refining the second machine learning model based on the segmentation loss.</p><p id="p-0154" num="0149">Clause 3: The method according to any one of Clauses 1-2, wherein: refining the second machine learning model based on the segmentation loss comprises generating a plurality of gradients by backpropagating the segmentation loss through the second machine learning model, and refining the first machine learning model based on the segmentation loss comprises backpropagating the plurality of gradients through the first machine learning model.</p><p id="p-0155" num="0150">Clause 4: The method according to any one of Clauses 1-3, further comprising: computing a depth loss based at least in part on the depth map; and refining the first machine learning model based on the depth loss.</p><p id="p-0156" num="0151">Clause 5: The method according to any one of Clauses 1-4, wherein the depth loss is computed based further on a ground-truth depth map.</p><p id="p-0157" num="0152">Clause 6: The method according to any one of Clauses 1-5, wherein the depth loss is a photometric loss computed by: generating a synthesized version of the input image based on the depth map and at least a second input image; and computing the photometric loss based on the synthesized version of the input image and the input image.</p><p id="p-0158" num="0153">Clause 7: The method according to any one of Clauses 1-6, wherein the ground-truth segmentation map is generated by processing the input image using a pre-trained segmentation machine learning model.</p><p id="p-0159" num="0154">Clause 8: The method according to any one of Clauses 1-7, wherein: the ground-truth segmentation map comprises a set of classes, and computing the segmentation loss comprises consolidating the set of classes to a subset of classes, wherein the subset of classes contains fewer classes than the set of classes.</p><p id="p-0160" num="0155">Clause 9: The method according to any one of Clauses 1-8, wherein, to generate output during inferencing: the first machine learning model is used to generate depth maps based on input images, and the second machine learning model is not used during inferencing.</p><p id="p-0161" num="0156">Clause 10: The method according to any one of Clauses 1-9, wherein the input image is received from a monocular source.</p><p id="p-0162" num="0157">Clause 11: A method, comprising: receiving an input image; generating an output depth map by processing the input image using a first machine learning model, generating a segmentation map by processing the depth map using a second machine learning model, computing a segmentation loss based on the segmentation map, and refining the first machine learning model based on the segmentation loss.</p><p id="p-0163" num="0158">Clause 12: The method according to Clause 11, wherein the first machine learning model is used in a monocular system, and the input image is received from a monocular source.</p><p id="p-0164" num="0159">Clause 13: A system, comprising: a memory comprising computer-executable instructions; and one or more processors configured to execute the computer-executable instructions and cause the processing system to perform a method in accordance with any one of Clauses 1-12.</p><p id="p-0165" num="0160">Clause 14: A system, comprising means for performing a method in accordance with any one of Clauses 1-12.</p><p id="p-0166" num="0161">Clause 15: A non-transitory computer-readable medium comprising computer-executable instructions that, when executed by one or more processors of a processing system, cause the processing system to perform a method in accordance with any one of Clauses 1-12.</p><p id="p-0167" num="0162">Clause 16: A computer program product embodied on a computer-readable storage medium comprising code for performing a method in accordance with any one of Clauses 1-12.</p><heading id="h-0007" level="1">Additional Considerations</heading><p id="p-0168" num="0163">The preceding description is provided to enable any person skilled in the art to practice the various aspects described herein. The examples discussed herein are not limiting of the scope, applicability, or aspects set forth in the claims. Various modifications to these aspects will be readily apparent to those skilled in the art, and the generic principles defined herein may be applied to other aspects. For example, changes may be made in the function and arrangement of elements discussed without departing from the scope of the disclosure. Various examples may omit, substitute, or add various procedures or components as appropriate. For instance, the methods described may be performed in an order different from that described, and various steps may be added, omitted, or combined. Also, features described with respect to some examples may be combined in some other examples. For example, an apparatus may be implemented or a method may be practiced using any number of the aspects set forth herein. In addition, the scope of the disclosure is intended to cover such an apparatus or method that is practiced using other structure, functionality, or structure and functionality in addition to, or other than, the various aspects of the disclosure set forth herein. It should be understood that any aspect of the disclosure disclosed herein may be embodied by one or more elements of a claim.</p><p id="p-0169" num="0164">As used herein, the word &#x201c;exemplary&#x201d; means &#x201c;serving as an example, instance, or illustration.&#x201d; Any aspect described herein as &#x201c;exemplary&#x201d; is not necessarily to be construed as preferred or advantageous over other aspects.</p><p id="p-0170" num="0165">As used herein, a phrase referring to &#x201c;at least one of a list of items refers to any combination of those items, including single members. As an example,&#x201d; at least one of: a, b, or c&#x201d; is intended to cover a, b, c, a-b, a-c, b-c, and a-b-c, as well as any combination with multiples of the same element (e.g., a-a, a-a-a, a-a-b, a-a-c, a-b-b, a-c-c, b-b, b-b-b, b-b-c, c-c, and c-c-c or any other ordering of a, b, and c).</p><p id="p-0171" num="0166">As used herein, the term &#x201c;determining&#x201d; encompasses a wide variety of actions. For example, &#x201c;determining&#x201d; may include calculating, computing, processing, deriving, investigating, looking up (e.g., looking up in a table, a database or another data structure), ascertaining and the like. Also, &#x201c;determining&#x201d; may include receiving (e.g., receiving information), accessing (e.g., accessing data in a memory) and the like. Also, &#x201c;determining&#x201d; may include resolving, selecting, choosing, establishing and the like.</p><p id="p-0172" num="0167">As used herein, the term &#x201c;connected to&#x201d;, in the context of sharing electronic signals and data between the elements described herein, may generally mean in data communication between the respective elements that are connected to each other. In some cases, elements may be directly connected to each other, such as via one or more conductive traces, lines, or other conductive carriers capable of carrying signals and/or data between the respective elements that are directly connected to each other. In other cases, elements may be indirectly connected to each other, such as via one or more data busses or similar shared circuitry and/or integrated circuit elements for communicating signals and data between the respective elements that are indirectly connected to each other.</p><p id="p-0173" num="0168">The methods disclosed herein comprise one or more steps or actions for achieving the methods. The method steps and/or actions may be interchanged with one another without departing from the scope of the claims. In other words, unless a specific order of steps or actions is specified, the order and/or use of specific steps and/or actions may be modified without departing from the scope of the claims. Further, the various operations of methods described above may be performed by any suitable means capable of performing the corresponding functions. The means may include various hardware and/or software component(s) and/or module(s), including, but not limited to a circuit, an application specific integrated circuit (ASIC), or processor. Generally, where there are operations illustrated in figures, those operations may have corresponding counterpart means-plus-function components with similar numbering.</p><p id="p-0174" num="0169">The following claims are not intended to be limited to the aspects shown herein, but are to be accorded the full scope consistent with the language of the claims. Within a claim, reference to an element in the singular is not intended to mean &#x201c;one and only one&#x201d; unless specifically so stated, but rather &#x201c;one or more.&#x201d; Unless specifically stated otherwise, the term &#x201c;some&#x201d; refers to one or more. No claim element is to be construed under the provisions of 35 U.S.C. &#xa7; 112(f) unless the element is expressly recited using the phrase &#x201c;means for&#x201d; or, in the case of a method claim, the element is recited using the phrase &#x201c;step for.&#x201d; All structural and functional equivalents to the elements of the various aspects described throughout this disclosure that are known or later come to be known to those of ordinary skill in the art are expressly incorporated herein by reference and are intended to be encompassed by the claims. Moreover, nothing disclosed herein is intended to be dedicated to the public regardless of whether such disclosure is explicitly recited in the claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230005165A1-20230105-M00001.NB"><img id="EMI-M00001" he="8.47mm" wi="76.20mm" file="US20230005165A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002" nb-file="US20230005165A1-20230105-M00002.NB"><img id="EMI-M00002" he="6.35mm" wi="76.20mm" file="US20230005165A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00003" nb-file="US20230005165A1-20230105-M00003.NB"><img id="EMI-M00003" he="6.35mm" wi="76.20mm" file="US20230005165A1-20230105-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00004" nb-file="US20230005165A1-20230105-M00004.NB"><img id="EMI-M00004" he="8.13mm" wi="76.20mm" file="US20230005165A1-20230105-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A processor-implemented method, comprising:<claim-text>generating a depth map by processing an input image using a first machine learning model;</claim-text><claim-text>generating a segmentation map by processing the depth map using a second machine learning model;</claim-text><claim-text>computing a segmentation loss based on the segmentation map and a ground-truth segmentation map; and</claim-text><claim-text>refining the first machine learning model based on the segmentation loss.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The processor-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising refining the second machine learning model based on the segmentation loss.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The processor-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>refining the second machine learning model based on the segmentation loss comprises generating a plurality of gradients by backpropagating the segmentation loss through the second machine learning model; and</claim-text><claim-text>refining the first machine learning model based on the segmentation loss comprises backpropagating the plurality of gradients through the first machine learning model.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The processor-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>computing a depth loss based at least in part on the depth map; and</claim-text><claim-text>refining the first machine learning model based on the depth loss.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The processor-implemented method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the depth loss is computed based further on a ground-truth depth map.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The processor-implemented method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the depth loss is a photometric loss computed by:<claim-text>generating a synthesized version of the input image based on the depth map and at least a second input image; and</claim-text><claim-text>computing the photometric loss based on the synthesized version of the input image and the input image.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The processor-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the ground-truth segmentation map is generated by processing the input image using a pre-trained segmentation machine learning model.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The processor-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>the ground-truth segmentation map comprises a set of classes, and</claim-text><claim-text>computing the segmentation loss comprises consolidating the set of classes to a subset of classes, wherein the subset of classes contains fewer classes than the set of classes.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The processor-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein, to generate output during inferencing:<claim-text>the first machine learning model is used to generate depth maps based on input images, and</claim-text><claim-text>the second machine learning model is not used during inferencing.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The processor-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the input image is received from a monocular source.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. A processing system, comprising:<claim-text>a memory comprising computer-executable instructions; and</claim-text><claim-text>one or more processors configured to execute the computer-executable instructions and cause the processing system to perform an operation comprising:<claim-text>generating a depth map by processing an input image using a first machine learning model;</claim-text><claim-text>generating a segmentation map by processing the depth map using a second machine learning model;</claim-text><claim-text>computing a segmentation loss based on the segmentation map and a ground-truth segmentation map; and</claim-text><claim-text>refining the first machine learning model based on the segmentation loss.</claim-text></claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The processing system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, the operation further comprising refining the second machine learning model based on the segmentation loss.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The processing system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein:<claim-text>refining the second machine learning model based on the segmentation loss comprises generating a plurality of gradients by backpropagating the segmentation loss through the second machine learning model; and</claim-text><claim-text>refining the first machine learning model based on the segmentation loss comprises backpropagating the plurality of gradients through the first machine learning model.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The processing system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, the operation further comprising:<claim-text>computing a depth loss based at least in part on the depth map; and</claim-text><claim-text>refining the first machine learning model based on the depth loss.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The processing system of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the depth loss is computed based further on a ground-truth depth map.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The processing system of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the depth loss is a photometric loss computed by:<claim-text>generating a synthesized version of the input image based on the depth map and at least a second input image; and</claim-text><claim-text>computing the photometric loss based on the synthesized version of the input image and the input image.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The processing system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the ground-truth segmentation map is generated by processing the input image using a pre-trained segmentation machine learning model.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The processing system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein:<claim-text>the ground-truth segmentation map comprises a set of classes, and</claim-text><claim-text>computing the segmentation loss comprises consolidating the set of classes to a subset of classes, wherein the subset of classes contains fewer classes than the set of classes.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The processing system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein, to generate output during inferencing:<claim-text>the first machine learning model is used to generate depth maps based on input images, and</claim-text><claim-text>the second machine learning model is not used during inferencing.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The processing system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the input image is received from a monocular source.</claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. A non-transitory computer-readable medium comprising computer-executable instructions that, when executed by one or more processors of a processing system, cause the processing system to perform an operation comprising:<claim-text>generating a depth map by processing an input image using a first machine learning model;</claim-text><claim-text>generating a segmentation map by processing the depth map using a second machine learning model;</claim-text><claim-text>computing a segmentation loss based on the segmentation map and a ground-truth segmentation map; and</claim-text><claim-text>refining the first machine learning model based on the segmentation loss.</claim-text></claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein:<claim-text>refining the second machine learning model based on the segmentation loss comprises generating a plurality of gradients by backpropagating the segmentation loss through the second machine learning model; and</claim-text><claim-text>refining the first machine learning model based on the segmentation loss comprises backpropagating the plurality of gradients through the first machine learning model.</claim-text></claim-text></claim><claim id="CLM-00023" num="00023"><claim-text><b>23</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00021">claim 21</claim-ref>, the operation further comprising:<claim-text>computing a depth loss based at least in part on the depth map; and</claim-text><claim-text>refining the first machine learning model based on the depth loss.</claim-text></claim-text></claim><claim id="CLM-00024" num="00024"><claim-text><b>24</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00023">claim 23</claim-ref>, wherein the depth loss is a photometric loss computed by:<claim-text>generating a synthesized version of the input image based on the depth map and at least a second input image; and</claim-text><claim-text>computing the photometric loss based on the synthesized version of the input image and the input image.</claim-text></claim-text></claim><claim id="CLM-00025" num="00025"><claim-text><b>25</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein:<claim-text>the ground-truth segmentation map comprises a set of classes, and</claim-text><claim-text>computing the segmentation loss comprises consolidating the set of classes to a subset of classes, wherein the subset of classes contains fewer classes than the set of classes.</claim-text></claim-text></claim><claim id="CLM-00026" num="00026"><claim-text><b>26</b>. A method, comprising:<claim-text>receiving an input image;</claim-text><claim-text>generating an output depth map by processing the input image using a first machine learning model; and</claim-text><claim-text>refining the first machine learning model, comprising:<claim-text>generating a segmentation map by processing the output depth map using a second machine learning model,</claim-text><claim-text>computing a segmentation loss based on the segmentation map, and</claim-text><claim-text>refining the first machine learning model based on the segmentation loss.</claim-text></claim-text></claim-text></claim><claim id="CLM-00027" num="00027"><claim-text><b>27</b>. The method of <claim-ref idref="CLM-00026">claim 26</claim-ref>, wherein:<claim-text>the first machine learning model is used in a monocular system, and</claim-text><claim-text>the input image is received from a monocular source.</claim-text></claim-text></claim><claim id="CLM-00028" num="00028"><claim-text><b>28</b>. The method of <claim-ref idref="CLM-00026">claim 26</claim-ref>, further comprising:<claim-text>computing a depth loss based at least in part on the output depth map; and</claim-text><claim-text>refining the first machine learning model based further on the depth loss.</claim-text></claim-text></claim><claim id="CLM-00029" num="00029"><claim-text><b>29</b>. The method of <claim-ref idref="CLM-00028">claim 28</claim-ref>, wherein the depth loss is a photometric loss computed by:<claim-text>generating a synthesized version of the input image based on the output depth map and at least a second input image; and</claim-text><claim-text>computing the photometric loss based on the synthesized version of the input image and the input image.</claim-text></claim-text></claim><claim id="CLM-00030" num="00030"><claim-text><b>30</b>. The method of <claim-ref idref="CLM-00029">claim 29</claim-ref>, wherein:<claim-text>the segmentation loss is computed based further on a ground-truth segmentation map,</claim-text><claim-text>the ground-truth segmentation map comprises a set of classes, and</claim-text><claim-text>computing the segmentation loss comprises consolidating the set of classes to a subset of classes, wherein the subset of classes contains fewer classes than the set of classes.</claim-text></claim-text></claim></claims></us-patent-application>