<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005470A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005470</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17943745</doc-number><date>20220913</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20130101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>07</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20130101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>28</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>1</main-group><subgroup>16</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>07</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>28</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>1</main-group><subgroup>163</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">DETECTION OF SPEECH</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16876373</doc-number><date>20200518</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11488583</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17943745</doc-number></document-id></child-doc></relation></continuation><us-provisional-application><document-id><country>US</country><doc-number>62854388</doc-number><date>20190530</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Cirrus Logic International Semiconductor Ltd.</orgname><address><city>Edinburgh</city><country>GB</country></address></addressbook><residence><country>GB</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>LESSO</last-name><first-name>John Paul</first-name><address><city>Edinburgh</city><country>GB</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>Cirrus Logic International Semiconductor Ltd.</orgname><role>03</role><address><city>Edinburgh</city><country>GB</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A method of own voice detection is provided for a user of a device. A first signal is detected, representing air-conducted speech using a first microphone of the device. A second signal is detected, representing bone-conducted speech using a bone-conduction sensor of the device. The first signal is filtered to obtain a component of the first signal at a speech articulation rate, and the second signal is filtered to obtain a component of the second signal at the speech articulation rate. The component of the first signal at the speech articulation rate and the component of the second signal at the speech articulation rate are compared, and it is determined that the speech has not been generated by the user of the device, if a difference between the component of the first signal at the speech articulation rate and the component of the second signal at the speech articulation rate exceeds a threshold value.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="117.43mm" wi="108.12mm" file="US20230005470A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="139.78mm" wi="110.15mm" file="US20230005470A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="190.25mm" wi="161.80mm" orientation="landscape" file="US20230005470A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="231.22mm" wi="126.66mm" file="US20230005470A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="238.08mm" wi="146.05mm" file="US20230005470A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="166.29mm" wi="59.94mm" orientation="landscape" file="US20230005470A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="212.01mm" wi="102.53mm" file="US20230005470A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="127.51mm" wi="159.09mm" file="US20230005470A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="212.01mm" wi="126.75mm" file="US20230005470A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="132.50mm" wi="107.61mm" file="US20230005470A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="247.31mm" wi="146.56mm" orientation="landscape" file="US20230005470A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0002" num="0001">This relates to the detection of speech, and in particular to detecting when a speaker is a person who is using a device, for example wearing a wearable accessory such as an earphone.</p><heading id="h-0002" level="1">BACKGROUND OF THE INVENTION</heading><p id="p-0003" num="0002">Wearable accessories, such as earphones, smart glasses and smart watches, are common. There are situations in which it is desirable to know whether the person speaking, when speech is detected, was the person wearing a particular accessory. For example, when an accessory is being used in connection with a device such as a smartphone that has speech recognition functionality, it is useful to know whether detected speech was spoken by the person wearing the accessory. In that case, speech that was spoken by the person wearing the accessory can be supplied to the speech recognition functionality so that any spoken commands can be acted upon, while speech that was not spoken by the person wearing the accessory can be ignored in some circumstances.</p><heading id="h-0003" level="1">SUMMARY OF THE INVENTION</heading><p id="p-0004" num="0003">According to the embodiments described herein, there is provided a method and a system which reduce or avoid one or more of the disadvantages mentioned above.</p><p id="p-0005" num="0004">According to a first aspect of the invention, there is provided a method of own voice detection for a user of a device, the method comprising:<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0000">    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0005">detecting a first signal representing air-conducted speech using a first microphone of the device;</li>        <li id="ul0002-0002" num="0006">detecting a second signal representing bone-conducted speech using a bone-conduction sensor of the device;</li>        <li id="ul0002-0003" num="0007">filtering the first signal to obtain a component of the first signal at a speech articulation rate;</li>        <li id="ul0002-0004" num="0008">filtering the second signal to obtain a component of the second signal at the speech articulation rate;</li>        <li id="ul0002-0005" num="0009">comparing the component of the first signal at the speech articulation rate and the component of the second signal at the speech articulation rate; and</li>        <li id="ul0002-0006" num="0010">determining that the speech has not been generated by the user of the device, if a difference between the component of the first signal at the speech articulation rate and the component of the second signal at the speech articulation rate exceeds a threshold value.</li>    </ul>    </li></ul></p><p id="p-0006" num="0011">According to a second aspect of the invention, there is provided a system, for own voice detection for a user of a device, the system comprising:<ul id="ul0003" list-style="none">    <li id="ul0003-0001" num="0000">    <ul id="ul0004" list-style="none">        <li id="ul0004-0001" num="0012">an input for receiving a first signal representing air-conducted speech from a first microphone of the device and a second signal representing bone-conducted speech from a bone-conduction sensor of the device;</li>        <li id="ul0004-0002" num="0013">at least one filter for filtering the first signal to obtain a component of the first signal at a speech articulation rate and for filtering the second signal to obtain a component of the second signal at the speech articulation rate;</li>        <li id="ul0004-0003" num="0014">a comparator for comparing the component of the first signal at the speech articulation rate and the component of the second signal at the speech articulation rate; and</li>        <li id="ul0004-0004" num="0015">a processor for determining that the speech has not been generated by the user of the device, if a difference between the component of the first signal at the speech articulation rate and the component of the second signal at the speech articulation rate exceeds a threshold value.</li>    </ul>    </li></ul></p><p id="p-0007" num="0016">According to a third aspect of the invention, there is provided a method of detecting a spoof attack on a speaker recognition system, the method comprising:<ul id="ul0005" list-style="none">    <li id="ul0005-0001" num="0000">    <ul id="ul0006" list-style="none">        <li id="ul0006-0001" num="0017">detecting a first signal representing air-conducted speech using a first microphone of a device;</li>        <li id="ul0006-0002" num="0018">detecting a second signal representing bone-conducted speech using a bone-conduction sensor of the device;</li>        <li id="ul0006-0003" num="0019">filtering the first signal to obtain a component of the first signal at a speech articulation rate;</li>        <li id="ul0006-0004" num="0020">filtering the second signal to obtain a component of the second signal at the speech articulation rate;</li>        <li id="ul0006-0005" num="0021">comparing the component of the first signal at the speech articulation rate and the component of the second signal at the speech articulation rate;</li>        <li id="ul0006-0006" num="0022">determining that the speech has not been generated by a user of the device, if a difference between the component of the first signal at the speech articulation rate and the component of the second signal at the speech articulation rate exceeds a threshold value; and</li>        <li id="ul0006-0007" num="0023">performing speaker recognition on the first signal representing speech, if the difference between the component of the first signal at the articulation rate and the component of the second signal at the articulation rate does not exceed the threshold value.</li>    </ul>    </li></ul></p><p id="p-0008" num="0024">The method may further comprise:<ul id="ul0007" list-style="none">    <li id="ul0007-0001" num="0000">    <ul id="ul0008" list-style="none">        <li id="ul0008-0001" num="0025">determining whether the device is being worn or held, and</li>        <li id="ul0008-0002" num="0026">performing at least the step of comparing the component of the first signal at the speech articulation rate and the component of the second signal at the speech articulation rate only if it is determined that the device is being worn or held.</li>    </ul>    </li></ul></p><p id="p-0009" num="0027">The method may further comprise:<ul id="ul0009" list-style="none">    <li id="ul0009-0001" num="0000">    <ul id="ul0010" list-style="none">        <li id="ul0010-0001" num="0028">performing voice keyword detection on the first signal representing speech; and</li>        <li id="ul0010-0002" num="0029">performing speaker recognition on the first signal representing speech only if a predetermined voice keyword is detected in the first signal representing speech.</li>    </ul>    </li></ul></p><p id="p-0010" num="0030">The method may further comprise:<ul id="ul0011" list-style="none">    <li id="ul0011-0001" num="0000">    <ul id="ul0012" list-style="none">        <li id="ul0012-0001" num="0031">if the result of performing speaker recognition is a determination that the speech was generated by an enrolled user, performing speech recognition on the first signal representing speech.</li>    </ul>    </li></ul></p><p id="p-0011" num="0032">The device may be a wearable device, in which case the user of the device is a wearer of the device. The wearable device may be an earphone, smart glasses, or a smart watch.</p><p id="p-0012" num="0033">Alternatively, the device may be a handheld device. The handheld device may be a mobile phone.</p><p id="p-0013" num="0034">According to a fourth aspect of the invention, there is provided a speaker recognition system, comprising:<ul id="ul0013" list-style="none">    <li id="ul0013-0001" num="0000">    <ul id="ul0014" list-style="none">        <li id="ul0014-0001" num="0035">an input for receiving a first signal representing air-conducted speech from a first microphone of a device and a second signal representing bone-conducted speech from a bone-conduction sensor of the device;</li>        <li id="ul0014-0002" num="0036">at least one filter for filtering the first signal to obtain a component of the first signal at a speech articulation rate, and for filtering the second signal to obtain a component of the second signal at the speech articulation rate;</li>        <li id="ul0014-0003" num="0037">a comparator, for comparing the component of the first signal at the speech articulation rate and the component of the second signal at the speech articulation rate;</li>        <li id="ul0014-0004" num="0038">a processor for determining that the speech has not been generated by a user of the device, if a difference between the component of the first signal at the speech articulation rate and the component of the second signal at the speech articulation rate exceeds a threshold value; and</li>        <li id="ul0014-0005" num="0039">a speaker recognition block, for performing speaker recognition on the first signal representing speech, if the difference between the component of the first signal at the articulation rate and the component of the second signal at the articulation rate does not exceed the threshold value.</li>    </ul>    </li></ul></p><p id="p-0014" num="0040">The system may further comprise:<ul id="ul0015" list-style="none">    <li id="ul0015-0001" num="0000">    <ul id="ul0016" list-style="none">        <li id="ul0016-0001" num="0041">a detection circuit for determining whether the device is being worn or held,</li>        <li id="ul0016-0002" num="0042">wherein the system is configured for performing at least the step of comparing the component of the first signal at the speech articulation rate and the component of the second signal at the speech articulation rate only if it is determined that the device is being worn or held.</li>    </ul>    </li></ul></p><p id="p-0015" num="0043">The system may further comprise:<ul id="ul0017" list-style="none">    <li id="ul0017-0001" num="0000">    <ul id="ul0018" list-style="none">        <li id="ul0018-0001" num="0044">a voice keyword detection block, for receiving the first signal representing speech,</li>        <li id="ul0018-0002" num="0045">wherein the system is configured for performing speaker recognition on the first signal representing speech only if a predetermined voice keyword is detected in the first signal representing speech.</li>    </ul>    </li></ul></p><p id="p-0016" num="0046">The system may be configured for performing speech recognition on the first signal representing speech, if the result of performing speaker recognition is a determination that the speech was generated by an enrolled user.</p><p id="p-0017" num="0047">The device may be a wearable device, and the user of the device may be a wearer of the device. The wearable device may be an earphone, smart glasses, or a smart watch.</p><p id="p-0018" num="0048">The device may be a handheld device. The handheld device may be a mobile phone.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0019" num="0049">For a better understanding of the invention, and to show more clearly how it may be carried into effect, reference will now be made, by way of example only, to the accompanying drawings in which:</p><p id="p-0020" num="0050"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic view of an electronic device and an associated accessory;</p><p id="p-0021" num="0051"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a further schematic diagram of an electronic device and an accessory;</p><p id="p-0022" num="0052"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flow chart, illustrating a method;</p><p id="p-0023" num="0053"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates a wearable device;</p><p id="p-0024" num="0054"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a block diagram, illustrating a part of a system as described herein;</p><p id="p-0025" num="0055"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a block diagram, illustrating a part of the system of <figref idref="DRAWINGS">FIG. <b>5</b></figref>;</p><p id="p-0026" num="0056"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates a stage in the method of <figref idref="DRAWINGS">FIG. <b>3</b></figref>;</p><p id="p-0027" num="0057"><figref idref="DRAWINGS">FIG. <b>8</b></figref> further illustrates a stage in the method of <figref idref="DRAWINGS">FIG. <b>3</b></figref>;</p><p id="p-0028" num="0058"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a block diagram, illustrating a form of system as described herein;</p><p id="p-0029" num="0059"><figref idref="DRAWINGS">FIG. <b>10</b></figref> illustrates a stage in the use of the system of <figref idref="DRAWINGS">FIG. <b>9</b></figref>;</p><p id="p-0030" num="0060"><figref idref="DRAWINGS">FIG. <b>11</b></figref> illustrates a stage in the use of the system of <figref idref="DRAWINGS">FIG. <b>9</b></figref>;</p><p id="p-0031" num="0061"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a block diagram, illustrating an alternative form of system as described herein; and</p><p id="p-0032" num="0062"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a block diagram, illustrating an alternative form of system as described herein.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION OF EMBODIMENTS</heading><p id="p-0033" num="0063">The description below sets forth example embodiments according to this disclosure. Further example embodiments and implementations will be apparent to those having ordinary skill in the art. Further, those having ordinary skill in the art will recognize that various equivalent techniques may be applied in lieu of, or in conjunction with, the embodiments discussed below, and all such equivalents should be deemed as being encompassed by the present disclosure.</p><p id="p-0034" num="0064">For clarity, it will be noted here that this description refers to speaker recognition and to speech recognition, which are intended to have different meanings. Speaker recognition refers to a technique that provides information about the identity of a person speaking. For example, speaker recognition may determine the identity of a speaker, from amongst a group of previously registered individuals, or may provide information indicating whether a speaker is or is not a particular individual, for the purposes of identification or authentication. Speech recognition refers to a technique for determining the content and/or the meaning of what is spoken, rather than recognising the person speaking.</p><p id="p-0035" num="0065"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows a device in accordance with one aspect of the invention. The device may be any suitable type of device, such as a mobile computing device, for example a laptop or tablet computer, a games console, a remote control device, a home automation controller or a domestic appliance including a domestic temperature or lighting control system, a toy, a machine such as a robot, an audio player, a video player, or the like, but in this illustrative example the device is a mobile telephone, and specifically a smartphone <b>10</b>, having a microphone <b>12</b> for detecting sounds. The smartphone <b>10</b> may, by suitable software, be used as the control interface for controlling any other further device or system.</p><p id="p-0036" num="0066"><figref idref="DRAWINGS">FIG. <b>1</b></figref> also shows an accessory, which in this case is a wireless earphone <b>30</b>, which in this example takes the form of an in-ear earphone or an earbud. The earphone <b>30</b> may be one of a pair of earphones or a part of a headset, or may be used on its own. A wireless earphone <b>30</b> is shown, but an earphone with a wired connection to the device may equally be used. The accessory may be any suitable device that can be worn by a person and used in conjunction with the device. For example, the accessory may be a smart watch, or a pair of smart glasses.</p><p id="p-0037" num="0067"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a schematic diagram, illustrating the form of the smartphone <b>10</b> and the wireless earphone <b>30</b>.</p><p id="p-0038" num="0068">Specifically, <figref idref="DRAWINGS">FIG. <b>2</b></figref> shows various interconnected components of the smartphone <b>10</b> and the wireless earphone <b>30</b>. It will be appreciated that the smartphone <b>10</b> and the wireless earphone <b>30</b> will in practice contain many other components, but the following description is sufficient for an understanding of the present invention. In addition, it will be appreciated that similar components to those shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref> may be included in any suitable device and any suitable wearable accessory.</p><p id="p-0039" num="0069">Thus, <figref idref="DRAWINGS">FIG. <b>2</b></figref> shows that the smartphone <b>10</b> includes the microphone <b>12</b> mentioned above.</p><p id="p-0040" num="0070"><figref idref="DRAWINGS">FIG. <b>2</b></figref> also shows a memory <b>14</b>, which may in practice be provided as a single component or as multiple components. The memory <b>14</b> is provided for storing data and program instructions.</p><p id="p-0041" num="0071"><figref idref="DRAWINGS">FIG. <b>2</b></figref> also shows a processor <b>16</b>, which again may in practice be provided as a single component or as multiple components. For example, one component of the processor <b>16</b> may be an applications processor of the smartphone <b>10</b>.</p><p id="p-0042" num="0072">Thus, the memory <b>14</b> may act as a tangible computer-readable medium, storing code, for causing the processor <b>16</b> to perform methods as described below.</p><p id="p-0043" num="0073"><figref idref="DRAWINGS">FIG. <b>2</b></figref> also shows a transceiver <b>18</b>, which is provided for allowing the smartphone <b>10</b> to communicate with external networks. For example, the transceiver <b>18</b> may include circuitry for establishing an internet connection either over a WiFi local area network or over a cellular network.</p><p id="p-0044" num="0074">In addition, the transceiver <b>18</b> allows the smartphone <b>10</b> to communicate with the wireless earphone <b>30</b>, for example using Bluetooth or another short-range wireless communications protocol.</p><p id="p-0045" num="0075"><figref idref="DRAWINGS">FIG. <b>2</b></figref> also shows audio processing circuitry <b>20</b>, for performing operations on the audio signals detected by the microphone <b>12</b> as required. For example, the audio processing circuitry <b>20</b> may filter the audio signals or perform other signal processing operations.</p><p id="p-0046" num="0076"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows that the wireless earphone <b>30</b> includes a transceiver <b>32</b>, which allows the wireless earphone <b>30</b> to communicate with the smartphone <b>10</b>, for example using Bluetooth or another short-range wireless communications protocol.</p><p id="p-0047" num="0077"><figref idref="DRAWINGS">FIG. <b>2</b></figref> also shows that the wireless earphone <b>30</b> includes a first sensor <b>34</b> and a second sensor <b>36</b>, which will be described in more detail below. Signals that are generated by the first sensor <b>34</b> and the second sensor <b>36</b>, in response to external stimuli, are transmitted to the smartphone <b>10</b> by means of the transceiver <b>32</b>.</p><p id="p-0048" num="0078">Thus, in this illustrated embodiment, signals are generated by sensors on an accessory, and these signals are transmitted to a host device, where they are processed. In other embodiments, the signals are processed on the accessory itself.</p><p id="p-0049" num="0079">In this embodiment, the smartphone <b>10</b> is provided with speaker recognition functionality, and with control functionality. Thus, the smartphone <b>10</b> is able to perform various functions in response to spoken commands from an enrolled user. The speaker recognition functionality is able to distinguish between spoken commands from an enrolled user, and the same commands when spoken by a different person. Thus, certain embodiments of the invention relate to operation of a smartphone or another portable electronic device with some sort of voice operability, for example a tablet or laptop computer, a games console, a home control system, a home entertainment system, an in-vehicle entertainment system, a domestic appliance, or the like, in which the speaker recognition functionality is performed in the device that is intended to carry out the spoken command. Certain other embodiments relate to systems in which the speaker recognition functionality is performed on a smartphone or other device, which then transmits the commands to a separate device if the speaker recognition functionality is able to confirm that the speaker was an enrolled user.</p><p id="p-0050" num="0080">In some embodiments, while speaker recognition functionality is performed on the smartphone <b>10</b> or other device that is located close to the user, the spoken commands are transmitted using the transceiver <b>18</b> to a remote speech recognition system, which determines the meaning of the spoken commands. For example, the speech recognition system may be located on one or more remote server in a cloud computing environment. Signals based on the meaning of the spoken commands are then returned to the smartphone <b>10</b> or other local device.</p><p id="p-0051" num="0081">When a spoken command is received, it is often required to perform a process of speaker verification, in order to confirm that the speaker is an enrolled user of the system. It is known to perform speaker verification by first performing a process of enrolling a user to obtain a model of the user's speech. Then, when it is desired to determine whether a particular test input is the speech of that user, a first score is obtained by comparing the test input against the model of the user's speech. In addition, a process of score normalization may be performed. For example, the test input may also be compared against a plurality of models of speech obtained from a plurality of other speakers. These comparisons give a plurality of cohort scores, and statistics can be obtained describing the plurality of cohort scores. The first score can then be normalized using the statistics to obtain a normalized score, and the normalised score can be used for speaker verification.</p><p id="p-0052" num="0082">In embodiments described herein, it can be determined whether detected speech was spoken by the person wearing the earphone <b>30</b>. This is referred to as &#x201c;own voice detection&#x201d;. If it is determined that the detected speech was spoken by the person wearing the earphone <b>30</b>, the speech signal may be sent for speaker recognition and/or speech recognition. If it is determined that the detected speech was not spoken by the person wearing the earphone <b>30</b>, a decision may be taken that the speech signal should not be sent for speaker recognition and/or speech recognition.</p><p id="p-0053" num="0083"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flow chart illustrating an example of a method in accordance with the present disclosure, and specifically a method for own voice detection for a user wearing a wearable device, that is, a method for detecting whether the person wearing the wearable device is the person speaking.</p><p id="p-0054" num="0084">Essentially the same method can be used for detecting whether a person holding a handheld device such as a mobile phone is the person speaking.</p><p id="p-0055" num="0085"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates in more detail the form of the wearable device, in one embodiment. Specifically, <figref idref="DRAWINGS">FIG. <b>4</b></figref> shows the earphone <b>30</b> being worn in the ear canal <b>70</b> of a wearer.</p><p id="p-0056" num="0086">In this example, the first sensor <b>34</b> shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref> takes the form of an out-of-ear microphone <b>72</b>, that is, a microphone that detects acoustic signals in the air surrounding the wearer's ear. In this example, the second sensor <b>36</b> shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref> takes the form of a bone-conduction sensor <b>74</b>. This may be an in-ear microphone that is able to detect acoustic signals in the wearer's ear canal, that result from the wearer's speech and that are transmitted through the bones of the wearer's head, and that may also be able to detect vibrations of the wearer's ear canal itself. Alternatively, the bone-conduction sensor <b>74</b> may be an accelerometer, positioned such that it is in contact with the wearer's ear canal, and can detect contact vibrations that result from the wearer's speech and that are transmitted through the bones and/or soft tissue of the wearer's head.</p><p id="p-0057" num="0087">Similarly, if the wearable device is a pair of smart glasses, the first sensor may take the form of an externally directed microphone that picks up the wearer's speech (and other sounds) by means of conduction through the air, and the second sensor may take the form of an accelerometer that is positioned to be in contact with the wearer's head, and can detect contact vibrations that result from the wearer's speech and that are transmitted through the bones and/or soft tissue of the wearer's head.</p><p id="p-0058" num="0088">Similarly, if the wearable device is a smart watch, the first sensor may take the form of an externally directed microphone that picks up the wearer's speech (and other sounds) by means of conduction through the air, and the second sensor may take the form of an accelerometer that is positioned to be in contact with the wearer's wrist, and can detect contact vibrations that result from the wearer's speech and that are transmitted through the wearer's bones and/or soft tissue.</p><p id="p-0059" num="0089">If the method is used for detecting whether a person holding a handheld device such as a mobile phone is the person speaking, the first sensor may be the microphone <b>12</b> that picks up the wearer's speech (and other sounds) by means of conduction through the air, and the second sensor may take the form of an accelerometer that is positioned within the handheld device (and therefore not visible in <figref idref="DRAWINGS">FIG. <b>1</b></figref>), that can detect contact vibrations that result from the user's speech and that are transmitted through the wearer's bones and/or soft tissue. When the handheld device is being pressed against the user's head, the accelerometer can detect contact vibrations that result from the wearer's speech and that are transmitted through the bones and/or soft tissue of the user's head, while, when the handheld device is not being pressed against the user's head, the accelerometer can detect contact vibrations that result from the wearer's speech and that are transmitted through the bones and/or soft tissue of the user's arm and hand.</p><p id="p-0060" num="0090">In step <b>50</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the method then comprises detecting a first signal representing air-conducted speech using the first microphone <b>72</b> of the wearable device <b>30</b>. That is, when the wearer speaks, the sound leaves their mouth and travels through the air, and can be detected by the microphone <b>72</b>.</p><p id="p-0061" num="0091">In step <b>52</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the method also comprises detecting a second signal representing bone-conducted speech using the bone-conduction sensor <b>74</b> of the wearable device <b>30</b>. That is, when the wearer speaks, the vibrations are conducted through the bones of their head (and/or may be conducted to at least some extent through the surrounding soft tissue), and can be detected by the bone-conduction sensor <b>74</b>.</p><p id="p-0062" num="0092">In principle, the process of own voice detection can be achieved by comparing the signals generated by the microphone <b>72</b> and the bone-conduction sensor <b>74</b>.</p><p id="p-0063" num="0093">However, since a typical bone-conduction sensor <b>74</b> is based on an accelerometer, and accelerometers typically run at a low sample rate, for example in the region of 100 Hz-1 kHz, the signal generated by the bone-conduction sensor <b>74</b> has a restricted frequency range. In addition, a typical bone-conduction sensor is prone to picking up contact noise (for example resulting from the wearer's head turning, or from contact with other objects). Further, it has been found that in general voiced speech is transmitted more efficiently via bone conduction than unvoiced speech.</p><p id="p-0064" num="0094"><figref idref="DRAWINGS">FIG. <b>5</b></figref> therefore shows filter circuitry for filtering the signals generated by the microphone <b>72</b> and the bone-conduction sensor <b>74</b> to make them more useful for the purposes of own voice detection.</p><p id="p-0065" num="0095">Specifically, <figref idref="DRAWINGS">FIG. <b>5</b></figref> shows the signal from the microphone <b>72</b> (i.e. the first signal mentioned at step <b>50</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>) being received at a first input <b>90</b>, and the signal from the bone-conduction sensor <b>72</b> (i.e. the second signal mentioned at step <b>52</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>) being received at a second input <b>92</b>.</p><p id="p-0066" num="0096">As mentioned above, it may be expected that voiced speech is transmitted more efficiently via bone conduction than unvoiced speech. That being so, the second signal will be expected to contain significant signal content only during periods when the wearer's speech contains voiced speech.</p><p id="p-0067" num="0097">Therefore, the first signal, received at the first input <b>90</b>, is passed to a voiced speech detection block <b>94</b>. This generates a flag when it is determined that the first signal represents voiced speech.</p><p id="p-0068" num="0098">Voiced speech can be identified, for example, by: using a deep neural network (DNN), trained against a golden reference, for example using Praat software; performing an autocorrelation with unit delay on the speech signal (because voiced speech has a higher autocorrelation for non-zero lags); performing a linear predictive coding (LPC) analysis (because the initial reflection coefficient is a good indicator of voiced speech); looking at the zero-crossing rate of the speech signal (because unvoiced speech has a higher zero-crossing rate); looking at the short term energy of the signal (which tends to be higher for voiced speech); tracking the first formant frequency F<b>0</b> (because unvoiced speech does not contain the first format frequency); examining the error in a linear predictive coding (LPC) analysis (because the LPC prediction error is lower for voiced speech); using automatic speech recognition to identify the words being spoken and hence the division of the speech into voiced and unvoiced speech; or fusing any or all of the above.</p><p id="p-0069" num="0099">In step <b>54</b> of the method of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the first signal is filtered to obtain a component of the first signal at a speech articulation rate. Therefore, the first signal, received at the first input <b>90</b>, is passed to a first articulation rate filter <b>96</b>.</p><p id="p-0070" num="0100">In step <b>56</b> of the method of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the second signal is filtered to obtain a component of the second signal at a speech articulation rate. Therefore, the second signal, received at the second input <b>92</b>, is passed to a second articulation rate filter <b>98</b>.</p><p id="p-0071" num="0101"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a schematic diagram, illustrating in more detail the form of the first articulation rate filter <b>96</b> and the second articulation rate filter <b>98</b>.</p><p id="p-0072" num="0102">In each case, the respective input signal is passed to a low pass filter <b>110</b>, with a cut-off frequency that may for example be in the region of 1 kHz. the low-pass filtered signal is passed to an envelope detector <b>112</b> for detecting the envelope of the filtered signal. The resulting envelope signal may optionally be passed to a decimator <b>114</b>, and then to a band-pass filter <b>116</b>, which is tuned to allow signals at typical articulation rates to pass. For example, the band-pass filter <b>116</b> may have a pass band between 5-15 Hz or between 5-10 Hz.</p><p id="p-0073" num="0103">Thus, the articulation rate filters <b>96</b>, <b>98</b> detect the power modulating at frequencies that correspond to typical articulation rates for speech, where the articulation rate is the rate at which the speaker is speaking, which may for example be measured as the rate at which the speaker generates distinct speech sounds, or phones.</p><p id="p-0074" num="0104">In step <b>58</b> of the method of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the component of the first signal at the speech articulation rate and the component of the second signal at the speech articulation rate are compared. Thus, as shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the outputs of the first articulation rate filter <b>96</b> and the second articulation rate filter <b>98</b> are passed to a comparison and decision block <b>100</b>.</p><p id="p-0075" num="0105">In step <b>60</b> of the method of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, it is then determined that the speech has not been generated by the user wearing the wearable device, if a difference between the component of the first signal at the speech articulation rate and the component of the second signal at the speech articulation rate exceeds a threshold value. Conversely, if the difference between the component of the first signal at the speech articulation rate and the component of the second signal at the speech articulation rate does not exceed the threshold value, it may be determined that the speech has been generated by the user wearing the wearable device.</p><p id="p-0076" num="0106">More specifically, in embodiments comprising the voiced speech detection block <b>94</b>, the comparison may be performed only when the flag is generated, indicating that the first signal represents voiced speech. Either the entire filtered first signal may be passed to the comparison and decision block <b>100</b>, with the comparison being performed only when the flag is generated, indicating that the first signal represents voiced speech, or unvoiced speech may be rejected and only those segments of the filtered first signal representing voiced speech may be passed to the comparison and decision block <b>100</b>.</p><p id="p-0077" num="0107">The processing of the signals detected by the first and second sensors is therefore such that, when the wearer of the wearable device is the person speaking, it would be expected that the processed version of the first signal would be similar to the processed version of the second signal. By contrast, if the wearer of the wearable device is not the person speaking, the first sensor would still be able to detect the air-conducted speech, but the second sensor would not be able to detect any bone-conducted speech, and so it would be expected that the processed version of the first signal would be quite different from the processed version of the second signal.</p><p id="p-0078" num="0108"><figref idref="DRAWINGS">FIGS. <b>7</b> and <b>8</b></figref> are examples of this, showing the magnitudes of the processed versions of the first and second signal in these two cases. In each figure, the signals are divided into frames, having a duration of 20 ms (for example), and the magnitude during each frame period is plotted over time.</p><p id="p-0079" num="0109">Specifically, <figref idref="DRAWINGS">FIG. <b>7</b></figref> shows a situation in which the wearer of the wearable device is the person speaking, and so the processed version of the first signal <b>130</b> is similar to the processed version of the second signal <b>132</b>.</p><p id="p-0080" num="0110">By contrast, <figref idref="DRAWINGS">FIG. <b>8</b></figref> shows a situation in which the wearer of the wearable device is not the person speaking, and so, while the processed version of the first signal <b>140</b> contains components resulting from the air-conducted speech, this is quite different from the processed version of the second signal <b>142</b>, because this does not contain any component resulting from bone-conducted speech.</p><p id="p-0081" num="0111">Different methods exist for performing the comparison between the component of the first signal at the speech articulation rate and the component of the second signal at the speech articulation rate, in order to determine whether a difference exceeds a threshold value.</p><p id="p-0082" num="0112"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a schematic illustration, showing a first form of the comparison and decision block <b>100</b> from <figref idref="DRAWINGS">FIG. <b>5</b></figref>, in which the processed version of the first signal, i.e. a component of the first signal at the speech articulation rate, is generated by the first articulation rate filter <b>96</b> and passed to a first input <b>120</b> of the comparison and decision block <b>100</b>. The processed version of the second signal, i.e. a component of the second signal at the speech articulation rate, is generated by the second articulation rate filter <b>98</b> and passed to a second input <b>122</b> of the comparison and decision block <b>100</b>.</p><p id="p-0083" num="0113">The signal at the first input <b>120</b> is then passed to a block <b>124</b>, in which an empirical cumulative distribution function (ECDF) is formed. Similarly, the signal at the second input <b>122</b> is then passed to a block <b>126</b>, in which an empirical cumulative distribution function (ECDF) is formed.</p><p id="p-0084" num="0114">In each case, the ECDF is calculated on a frame-by-frame basis, using the magnitude of the signal during the frames. The ECDF then indicates, for each possible signal magnitude, the proportion of the frames in which the actual signal magnitude is below that level.</p><p id="p-0085" num="0115"><figref idref="DRAWINGS">FIG. <b>10</b></figref> shows the two ECDFs calculated in a situation similar to <figref idref="DRAWINGS">FIG. <b>7</b></figref>, in which the two signals are generally similar. It can thus also be seen that the two ECDFs, namely the ECDF <b>142</b> formed from the first signal and the ECDF <b>140</b> formed from the second signal, are generally similar.</p><p id="p-0086" num="0116">One measure of similarity between two ECDFs <b>142</b>, <b>140</b> is to measure the maximum vertical distance between them, in this case d<b>1</b>. Alternatively, several measures of the vertical distance between the two ECDFs can be made, and then summed or averaged, in order to arrive at a suitable measure.</p><p id="p-0087" num="0117"><figref idref="DRAWINGS">FIG. <b>11</b></figref> shows the two ECDFs calculated in a situation similar to <figref idref="DRAWINGS">FIG. <b>8</b></figref>, in which the two signals are substantially different. It can thus also be seen that the two ECDFs, namely the ECDF <b>152</b> formed from the first signal and the ECDF <b>150</b> formed from the second signal, are substantially different. Specifically, because the second signal does not contain any component resulting from bone-conducted speech, it is generally at a much lower level than the first signal, and so the form of the ECDF shows that the magnitude of the second signal is generally low.</p><p id="p-0088" num="0118">In this case, the maximum vertical distance between the ECDFs <b>152</b>, <b>150</b> is d<b>2</b>.</p><p id="p-0089" num="0119">More generally, the step of comparing the components of the first and second signals at the speech articulation rate may comprise forming respective first and second distribution functions from those components, and calculating a value of a statistical distance between the second distribution function and the first distribution function.</p><p id="p-0090" num="0120">For example, as mentioned above, the value of the statistical distance between the second distribution function and the first distribution function may be calculated as:</p><p id="p-0091" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>d</i><sub>KS</sub>=max{|<i>F</i>1&#x2212;<i>F</i>2|}<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0092" num="0121">where</p><p id="p-0093" num="0122">F1 is the first distribution function and</p><p id="p-0094" num="0123">F2 is the second distribution function, and hence</p><p id="p-0095" num="0124">|F1&#x2212;F2| is the vertical distance between the two functions at a given frequency.</p><p id="p-0096" num="0125">Alternatively, the value of the statistical distance between the second distribution function and the first distribution function may be calculated as:</p><p id="p-0097" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>d</i><sub>IN</sub><i>=&#x222b;|F</i>1&#x2212;<i>F</i>2|<i>df </i><?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0098" num="0126">where</p><p id="p-0099" num="0127">F1 is the first distribution function and</p><p id="p-0100" num="0128">F2 is the second distribution function, and hence</p><p id="p-0101" num="0129">|F1&#x2212;F2| is the vertical distance between the two functions at a given frequency.</p><p id="p-0102" num="0130">Alternatively, the value of the statistical distance between the second distribution function and the first distribution function may be calculated as:</p><p id="p-0103" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mrow>  <msub>   <mi>d</mi>   <mi>CVM</mi>  </msub>  <mo>=</mo>  <mroot>   <mrow>    <mo>&#x222b;</mo>    <mrow>     <msup>      <mrow>       <mo>(</mo>       <mrow>        <mrow>         <mi>F</mi>         <mo>&#x2062;</mo>         <mn>1</mn>        </mrow>        <mo>-</mo>        <mrow>         <mi>F</mi>         <mo>&#x2062;</mo>         <mn>2</mn>        </mrow>       </mrow>       <mo>)</mo>      </mrow>      <mi>p</mi>     </msup>     <mo>&#x2062;</mo>     <mi>df</mi>    </mrow>   </mrow>   <mi>p</mi>  </mroot> </mrow></math></maths></p><p id="p-0104" num="0131">or, more specifically, when p=2:</p><p id="p-0105" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>d</i><sub>CVM</sub>=&#x221a;{square root over (&#x222b;(<i>F</i>1&#x2212;<i>F</i>2)<sup>2</sup><i>df</i>)}<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0106" num="0132">where</p><p id="p-0107" num="0133">F1 is the first distribution function and</p><p id="p-0108" num="0134">F2 is the second distribution function, and hence</p><p id="p-0109" num="0135">|F1&#x2212;F2| is the vertical distance between the two functions at a given frequency.</p><p id="p-0110" num="0136">In other examples, the step of comparing the components may use a machine learning system that has been trained to distinguish between the components that are generated from a wearer's own speech and the speech of a non-wearer.</p><p id="p-0111" num="0137">Although examples are given here in which the distribution functions are cumulative distribution functions, other distribution functions, such as probability distribution functions may be used, with appropriate methods for comparing these functions. The methods for comparing may include using machine learning systems as described above.</p><p id="p-0112" num="0138">Thus, returning to <figref idref="DRAWINGS">FIG. <b>9</b></figref>, the ECDFs are passed to a block <b>128</b>, which calculates the statistical distance d between the ECDFs, and passes this to a block <b>130</b>, where this is compared with a threshold &#x3b8;.</p><p id="p-0113" num="0139">As discussed with reference to step <b>60</b> of the method of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, it is then determined that the speech has not been generated by the user wearing the wearable device, if the statistical distance between the ECDFs generated from the component of the first signal at the speech articulation rate and the component of the second signal at the speech articulation rate exceeds the threshold value &#x3b8;. Conversely, if the statistical distance between the ECDFs generated from the component of the first signal at the speech articulation rate and the component of the second signal at the speech articulation rate does not exceed the threshold value &#x3b8;, it may be determined that the speech has been generated by the user wearing the wearable device.</p><p id="p-0114" num="0140"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a schematic illustration, showing a second form of the comparison and decision block <b>100</b> from <figref idref="DRAWINGS">FIG. <b>5</b></figref>, in which the processed version of the first signal, i.e. a component of the first signal at the speech articulation rate, is generated by the first articulation rate filter <b>96</b> and passed to a first input <b>160</b> of the comparison and decision block <b>100</b>. The processed version of the second signal, i.e. a component of the second signal at the speech articulation rate, is generated by the second articulation rate filter <b>98</b> and passed to a second input <b>162</b> of the comparison and decision block <b>100</b>.</p><p id="p-0115" num="0141">The signal at the first input <b>160</b> is subtracted from the signal at the second input <b>162</b> in a subtractor <b>164</b>, and the difference &#x394; is passed to a comparison block <b>166</b>. In some embodiments, the value of the difference &#x394;, that is calculated in each frame, is compared with a threshold value. If the difference exceeds the threshold value in any frame, then it may be determined that the speech has not been generated by the user wearing the wearable device. In other embodiments, the values of the difference &#x394;, calculated in multiple frames, are analysed statistically. For example, the mean value of &#x394;, calculated over a block of, say, 20 frames, or the running mean, may be compared with the threshold value. As another example, the median value of &#x394;, calculated over a block of frames, may be compared with the threshold value. In any of these cases, if the parameter calculated from the separate difference values exceeds the threshold value, then it may be determined that the speech (or at least the relevant part of the speech from which the difference values were calculated) was not generated by the user wearing the wearable device.</p><p id="p-0116" num="0142"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a block schematic diagram of a system using the method of own speech detection described previously.</p><p id="p-0117" num="0143">Specifically, <figref idref="DRAWINGS">FIG. <b>13</b></figref> shows a first sensor <b>200</b> and a second sensor <b>202</b>, which may be provided on a wearable device. As described with reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the first sensor <b>200</b> may take the form of a microphone that detects acoustic signals transmitted through the air, while the second sensor <b>202</b> may take the form of an accelerometer for detecting signals transmitted by bone-conduction (including transmission through the wearer's soft tissues).</p><p id="p-0118" num="0144">The signals from the first sensor <b>200</b> and a second sensor <b>202</b> are passed to a wear detection block <b>204</b>, which compares the signals generated by the sensors, and determines whether the wearable device is being worn at that time. For example, when the wearable device is an earphone, the wear detection block <b>204</b> may take the form of an in-ear detection block. In the case of an earphone, for example as shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the signals detected by the sensors <b>200</b> and <b>202</b> are substantially the same if the earphone is out of the user's ear, but are significantly different if the earphone is being worn. Therefore, comparing the signals allows a determination as to whether the wearable device is being worn.</p><p id="p-0119" num="0145">Other systems are available for detecting whether a wearable device is being worn, and these may or may not use either of the sensors <b>200</b>, <b>202</b>. For example, optical sensors, conductivity sensors, or proximity sensors may be used.</p><p id="p-0120" num="0146">In addition, the wear detection block <b>204</b> may be configured to perform &#x201c;liveness detection&#x201d;, that is, to determine whether the wearable device is being worn by a live person at that time. For example, the signal generated by the second sensor <b>202</b> may be analysed to detect evidence of a wearer's pulse, to confirm that a wearable device such as an earphone or watch is being worn by a person, rather than being placed in or on an inanimate object.</p><p id="p-0121" num="0147">When the method is used for detecting whether a person holding a handheld device such as a mobile phone is the person speaking, the detection block <b>204</b> may be configured for determining whether the device is being held by the user (as opposed to, for example, being used while lying on a table or other surface). For example, the detection block may receive signals from the sensor <b>202</b>, or from one or more separate sensor, that can be used to determine whether the device is being held by the user and/or being pressed against the user's head. Again, for example, optical sensors, conductivity sensors, or proximity sensors may be used.</p><p id="p-0122" num="0148">If it is determined that the wearable device is being worn, or the handheld device is being held, a signal from the detection block <b>204</b> is used to close switches <b>206</b>, <b>208</b>, and the signals from the sensors <b>200</b>, <b>202</b> are passed to the inputs <b>90</b>, <b>92</b> of the circuitry shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0123" num="0149">As described with reference to <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the comparison and decision block <b>100</b> generates an output signal indicating whether the detected speech was spoken by the person wearing the wearable device.</p><p id="p-0124" num="0150">In this example system, the signal from the first sensor <b>200</b> is also passed to a voice keyword detection block <b>220</b>. The voice keyword detection block <b>220</b> detects a particular &#x201c;wake phrase&#x201d; that is used by a user of the device to wake it from a low power standby mode, and put it into a mode in which speech recognition is possible.</p><p id="p-0125" num="0151">When the voice keyword detection block <b>220</b> detects the particular &#x201c;wake phrase&#x201d;, the signal from the first sensor <b>200</b> is sent to a speaker recognition block <b>222</b>.</p><p id="p-0126" num="0152">If the output signal from the comparison and decision block <b>100</b> indicates that the detected speech was not spoken by the person wearing the wearable device, this is considered to be a spoof, and so it is not desirable to perform speech recognition on the detected speech.</p><p id="p-0127" num="0153">However, if the output signal from the comparison and decision block <b>100</b> indicates that the detected speech was spoken by the person wearing the wearable device, a speaker recognition process may be performed by the speaker recognition block <b>222</b> on the signal from the first sensor <b>200</b>. In general terms, the process of speaker recognition extracts features from the speech signal, and compares them with features of a model generated by enrolling a known user into the speaker recognition system. If the comparison finds that the features of the speech are sufficiently similar to the model, then it is determined that the speaker is the enrolled user, to a sufficiently high degree of probability.</p><p id="p-0128" num="0154">When the wearable device is an earphone, the process of speaker recognition may be omitted and replaced by an ear biometric process, in which the wearer of the earphone is identified. For example, this may be done by examining signals generated by an in-ear microphone provided on the earphone (which microphone may also act as the second sensor <b>202</b>), and comparing features of the signals with an acoustic model of an enrolled user's ear. If it can be determined with sufficient confidence for the relevant application both that the earphone is being worn by the enrolled user, and also that the speech is the speech of the person wearing the earphone, then this acts as a form of speaker recognition.</p><p id="p-0129" num="0155">If it is determined that the speaker is the enrolled user (either by way of an output from the speaker recognition block <b>222</b>, or by using confirmation that the wearable device is being worn by the enrolled user), a signal is sent to a speech recognition block <b>224</b>, which also receives the signal from the first sensor <b>200</b>.</p><p id="p-0130" num="0156">The speech recognition block <b>224</b> then performs a process of speech recognition on the received signal. If it detects that the speech contains a command, for example, it may send an output signal to a further application to act on that command.</p><p id="p-0131" num="0157">Thus, the availability of a bone-conducted signal can be used for the purposes of own speech detection. Further, the result of the own speech detection can be used to improve the reliability of speaker recognition and speech recognition systems.</p><p id="p-0132" num="0158">The skilled person will recognise that some aspects of the above-described apparatus and methods may be embodied as processor control code, for example on a non-volatile carrier medium such as a disk, CD- or DVD-ROM, programmed memory such as read only memory (Firmware), or on a data carrier such as an optical or electrical signal carrier. For many applications embodiments of the invention will be implemented on a DSP (Digital Signal Processor), ASIC (Application Specific Integrated Circuit) or FPGA (Field Programmable Gate Array). Thus the code may comprise conventional program code or microcode or, for example code for setting up or controlling an ASIC or FPGA. The code may also comprise code for dynamically configuring re-configurable apparatus such as re-programmable logic gate arrays. Similarly the code may comprise code for a hardware description language such as Verilog&#x2122; or VHDL (Very high speed integrated circuit Hardware Description Language). As the skilled person will appreciate, the code may be distributed between a plurality of coupled components in communication with one another. Where appropriate, the embodiments may also be implemented using code running on a field-(re)programmable analogue array or similar device in order to configure analogue hardware.</p><p id="p-0133" num="0159">Note that as used herein the term module shall be used to refer to a functional unit or block which may be implemented at least partly by dedicated hardware components such as custom defined circuitry and/or at least partly be implemented by one or more software processors or appropriate code running on a suitable general purpose processor or the like. A module may itself comprise other modules or functional units. A module may be provided by multiple components or sub-modules which need not be co-located and could be provided on different integrated circuits and/or running on different processors.</p><p id="p-0134" num="0160">Embodiments may be implemented in a host device, especially a portable and/or battery powered host device such as a mobile computing device for example a laptop or tablet computer, a games console, a remote control device, a home automation controller or a domestic appliance including a domestic temperature or lighting control system, a toy, a machine such as a robot, an audio player, a video player, or a mobile telephone for example a smartphone.</p><p id="p-0135" num="0161">It should be noted that the above-mentioned embodiments illustrate rather than limit the invention, and that those skilled in the art will be able to design many alternative embodiments without departing from the scope of the appended claims. The word &#x201c;comprising&#x201d; does not exclude the presence of elements or steps other than those listed in a claim, &#x201c;a&#x201d; or &#x201c;an&#x201d; does not exclude a plurality, and a single feature or other unit may fulfil the functions of several units recited in the claims. Any reference numerals or labels in the claims shall not be construed so as to limit their scope.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230005470A1-20230105-M00001.NB"><img id="EMI-M00001" he="6.69mm" wi="76.20mm" file="US20230005470A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method of own voice detection for a user of a device, the method comprising:<claim-text>detecting a first signal representing air-conducted speech using a first microphone of the device;</claim-text><claim-text>detecting a second signal representing bone-conducted speech using a bone-conduction sensor of the device;</claim-text><claim-text>filtering the first signal to obtain a component of the first signal at a speech articulation rate;</claim-text><claim-text>filtering the second signal to obtain a component of the second signal at the speech articulation rate;</claim-text><claim-text>comparing the component of the first signal at the speech articulation rate and the component of the second signal at the speech articulation rate; and</claim-text><claim-text>determining that the speech has not been generated by the user of the device, if a difference between the component of the first signal at the speech articulation rate and the component of the second signal at the speech articulation rate exceeds a threshold value.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. A method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, comprising:<claim-text>detecting signal components representing voiced speech in the first signal; and</claim-text><claim-text>comparing the component of the first signal at the speech articulation rate and the component of the second signal at the speech articulation rate only during periods when signal components representing voiced speech are present in the first signal.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. A method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, comprising:<claim-text>filtering the first signal in a first bandpass filter to obtain the component of the first signal at the speech articulation rate; and</claim-text><claim-text>filtering the second signal in a second bandpass filter to obtain the component of the second signal at the speech articulation rate;</claim-text><claim-text>wherein the first bandpass filter and the second bandpass filter have respective pass bands that include the frequency range from 5-15 Hz.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. A method according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, further comprising, before filtering the first signal in the first bandpass filter and filtering the second signal in the second bandpass filter, low pass filtering the first signal and the second signal, and detecting an envelope of each filtered signal.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. A method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>comparing the component of the first signal at the speech articulation rate and the component of the second signal at the speech articulation rate comprises:</claim-text><claim-text>forming a Cumulative Distribution Function of the component of the first signal at the speech articulation rate;</claim-text><claim-text>forming a Cumulative Distribution Function of the component of the second signal at the speech articulation rate; and</claim-text><claim-text>determining a difference between the Cumulative Distribution Function of the component of the first signal at the speech articulation rate and the Cumulative Distribution Function of the component of the second signal at the speech articulation rate,</claim-text><claim-text>and wherein determining that the speech has not been generated by the user of the device, if a difference between the component of the first signal at the speech articulation rate and the component of the second signal at the speech articulation rate exceeds a threshold value, comprises determining that the speech has not been generated by the user of the device, if the difference between the Cumulative Distribution Function of the component of the first signal at the speech articulation rate and the Cumulative Distribution Function of the component of the second signal at the speech articulation rate exceeds a threshold value.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. A method according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, comprising:<claim-text>obtaining the component of the first signal at the speech articulation rate and the component of the second signal at the speech articulation rate in each of a plurality of frames; and</claim-text><claim-text>forming said Cumulative Distribution Functions on a frame-by-frame basis, using the magnitude of the respective signal during a plurality of said frames.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. A method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>comparing the component of the first signal at the speech articulation rate and the component of the second signal at the speech articulation rate comprises:</claim-text><claim-text>subtracting the component of the second signal at the speech articulation rate from the component of the first signal at the speech articulation rate;</claim-text><claim-text>and wherein determining that the speech has not been generated by the user of the device, if a difference between the component of the first signal at the speech articulation rate and the component of the second signal at the speech articulation rate exceeds a threshold value, comprises determining that the speech has not been generated by the user of the device, if a result of subtracting the component of the second signal at the speech articulation rate from the component of the first signal at the speech articulation rate exceeds a threshold value.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. A method according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, comprising:<claim-text>obtaining the component of the first signal at the speech articulation rate and the component of the second signal at the speech articulation rate in each of a plurality of frames;</claim-text><claim-text>subtracting the component of the second signal at the speech articulation rate from the component of the first signal at the speech articulation rate on a frame-by-frame basis; and</claim-text><claim-text>forming the result of subtracting the component of the second signal at the speech articulation rate from the component of the first signal at the speech articulation rate from a plurality of values calculated on a frame-by-frame basis.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. A method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the device is a wearable device, and the user of the device is a wearer of the device.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. A method according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the wearable device is an earphone, smart glasses, or a smart watch.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. A method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the device is a handheld device.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. A method according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the handheld device is a mobile phone.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. A system, for own voice detection for a user of a device, the system comprising:<claim-text>an input for receiving a first signal representing air-conducted speech from a first microphone of the device and a second signal representing bone-conducted speech from a bone-conduction sensor of the device;</claim-text><claim-text>at least one filter for filtering the first signal to obtain a component of the first signal at a speech articulation rate and for filtering the second signal to obtain a component of the second signal at the speech articulation rate;</claim-text><claim-text>a comparator for comparing the component of the first signal at the speech articulation rate and the component of the second signal at the speech articulation rate; and</claim-text><claim-text>a processor for determining that the speech has not been generated by the user of the device, if a difference between the component of the first signal at the speech articulation rate and the component of the second signal at the speech articulation rate exceeds a threshold value.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. A system according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, comprising:<claim-text>wherein the at least one filter comprises at least one bandpass filter for filtering the first signal and filtering the second signal; and</claim-text><claim-text>wherein the at least one bandpass filter has a pass band that includes the frequency range from 5-15 Hz.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. A system according to <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the at least one filter comprises:<claim-text>at least one low pass filter, for low pass filtering the first signal and the second signal, before filtering the first signal and the second signal in the at least one bandpass filter, and</claim-text><claim-text>an envelope detector, for detecting an envelope of each filtered signal.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. A system according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the device is a wearable device, and the user of the device is a wearer of the device, and wherein the system is implemented in a device separate from said wearable device.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. A system according to <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the wearable device is an earphone, smart glasses, or a smart watch.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. A system according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the device is a handheld device, and the system is implemented in said device.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. A system according to <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the handheld device is a mobile phone.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. A speaker recognition system, comprising:<claim-text>an input for receiving a first signal representing air-conducted speech from a first microphone of a device and a second signal representing bone-conducted speech from a bone-conduction sensor of the device;</claim-text><claim-text>at least one filter for filtering the first signal to obtain a component of the first signal at a speech articulation rate, and for filtering the second signal to obtain a component of the second signal at the speech articulation rate;</claim-text><claim-text>a comparator, for comparing the component of the first signal at the speech articulation rate and the component of the second signal at the speech articulation rate;</claim-text><claim-text>a processor for determining that the speech has not been generated by a user of the device, if a difference between the component of the first signal at the speech articulation rate and the component of the second signal at the speech articulation rate exceeds a threshold value; and</claim-text><claim-text>a speaker recognition block, for performing speaker recognition on the first signal representing speech, if the difference between the component of the first signal at the articulation rate and the component of the second signal at the articulation rate does not exceed the threshold value.</claim-text></claim-text></claim></claims></us-patent-application>