<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004589A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004589</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17577561</doc-number><date>20220118</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>CN</country><doc-number>202110734020.1</doc-number><date>20210630</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20190101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>34</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20200101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>51</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20200101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>56</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20190101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>345</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20200101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>51</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20200101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>56</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">SUMMARY GENERATION MODEL TRAINING METHOD AND APPARATUS, DEVICE AND STORAGE MEDIUM</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>BEIJING BAIDU NETCOM SCIENCE TECHNOLOGY CO., LTD.</orgname><address><city>Beijing</city><country>CN</country></address></addressbook><residence><country>CN</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>WU</last-name><first-name>Wenhao</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>LI</last-name><first-name>Wei</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>XIAO</last-name><first-name>Xinyan</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>LIU</last-name><first-name>Jiachen</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>BEIJING BAIDU NETCOM SCIENCE TECHNOLOGY CO., LTD.</orgname><role>03</role><address><city>Beijing</city><country>CN</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">The present disclosure provides a summary generation model training method and apparatus, a device and a storage medium, and relates to the field of computer technologies, and in particular, to the field of artificial intelligence such as natural language processing and deep learning. The summary generation model training method includes: acquiring a document representation corresponding to a document sample; constructing, based on the document representation, a summary representation corresponding to the document representation, the summary representation including a positive summary representation and a negative summary representation; and constructing a total contrastive loss function based on the document representation, the positive summary representation and the negative summary representation, and training a summary generation model based on the total contrastive loss function. The present disclosure may improve accuracy of the summary generation model.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="100.67mm" wi="157.23mm" file="US20230004589A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="111.68mm" wi="159.26mm" file="US20230004589A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="185.50mm" wi="145.29mm" file="US20230004589A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="79.76mm" wi="150.28mm" file="US20230004589A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="201.42mm" wi="156.63mm" file="US20230004589A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="198.04mm" wi="116.42mm" file="US20230004589A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">The present application claims the priority of Chinese Patent Application No. 202110734020.1, filed on Jun. 30, 2021, with the title of &#x201c;SUMMARY GENERATION MODEL TRAINING METHOD AND APPARATUS, DEVICE AND STORAGE MEDIUM.&#x201d; The disclosure of the above application is incorporated herein by reference in its entirety.</p><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">The present disclosure relates to the field of computer technologies, specifically to the field of artificial intelligence such as natural language processing and deep learning, and in particular, to a summary generation model training method and apparatus, a device and a storage medium.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">Automatic summarization is intended to automatically generate a concise summary for one or more documents, and the generated summary is required to be coherent in meaning, smooth in language and faithful to content of an original text. The automatic summarization may be divided into extractive summarization and generative summarization. The generative summarization is intended to understand an inputted document and organize language to generate a target summary by imitating human summarization through a big data technology. Processing of the generative summarization may include processing an inputted document by using a summary generation model, to obtain a summary corresponding to the inputted document.</p><p id="p-0005" num="0004">In the related art, a maximum likelihood probability function is used as a loss function to train the summary generation model.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0006" num="0005">The present disclosure provides a summary generation model training method and apparatus, a device and a storage medium.</p><p id="p-0007" num="0006">According to one aspect of the present disclosure, a summary generation model training method is provided, including: acquiring a document representation corresponding to a document sample; constructing, based on the document representation, a summary representation corresponding to the document representation, the summary representation including a positive summary representation and a negative summary representation; and constructing a total contrastive loss function based on the document representation, the positive summary representation and the negative summary representation, and training a summary generation model based on the total contrastive loss function.</p><p id="p-0008" num="0007">According to another aspect of the present disclosure, there is provided an electronic device, including: at least one processor; and a memory communicatively connected with the at least one processor; wherein the memory stores instructions executable by the at least one processor, and the instructions are executed by the at least one processor to enable the at least one processor to perform a summary generation model training method, wherein the summary generation model training method including: acquiring a document representation corresponding to a document sample; constructing, based on the document representation, a summary representation corresponding to the document representation, the summary representation including a positive summary representation and a negative summary representation; and constructing a total contrastive loss function based on the document representation, the positive summary representation and the negative summary representation, and training a summary generation model based on the total contrastive loss function.</p><p id="p-0009" num="0008">According to another aspect of the present disclosure, there is provided a non-transitory computer readable storage medium with computer instructions stored thereon, wherein the computer instructions are used for causing a computer to perform a summary generation model training method, wherein the summary generation model training method includes: acquiring a document representation corresponding to a document sample; constructing, based on the document representation, a summary representation corresponding to the document representation, the summary representation comprising a positive summary representation and a negative summary representation; and constructing a total contrastive loss function based on the document representation, the positive summary representation and the negative summary representation, and training a summary generation model based on the total contrastive loss function.</p><p id="p-0010" num="0009">According to the technical solutions of the present disclosure, accuracy of the summary generation model may be improved.</p><p id="p-0011" num="0010">It should be understood that the content described in this part is neither intended to identify key or significant features of the embodiments of the present disclosure, nor intended to limit the scope of the present disclosure. Other features of the present disclosure will be made easier to understand through the following description.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0012" num="0011">The accompanying drawings are intended to provide a better understanding of the solutions and do not constitute a limitation on the present disclosure. In the drawings,</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic diagram of a first embodiment according to the present disclosure;</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a schematic diagram of a second embodiment according to the present disclosure;</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a schematic diagram of a third embodiment according to the present disclosure;</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a schematic diagram of a fourth embodiment according to the present disclosure;</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a schematic diagram of a fifth embodiment according to the present disclosure;</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a schematic diagram of a sixth embodiment according to the present disclosure; and</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a schematic diagram of an electronic device configured to perform any one of the summary generation model training methods according to embodiments of the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0020" num="0019">Exemplary embodiments of the present disclosure are illustrated below with reference to the accompanying drawings, which include various details of the present disclosure to facilitate understanding and should be considered only as exemplary. Therefore, those of ordinary skill in the art should be aware that various changes and modifications can be made to the embodiments described herein without departing from the scope and spirit of the present disclosure. Similarly, for clarity and simplicity, descriptions of well-known functions and structures are omitted in the following description.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic diagram of a first embodiment according to the present disclosure. This embodiment provides a summary generation model training method, including the following steps:</p><p id="p-0022" num="0021">In <b>101</b>, a document representation corresponding to a document sample is acquired.</p><p id="p-0023" num="0022">In <b>102</b>, a summary sample corresponding to the document sample is acquired based on the document sample, the summary sample including a positive summary sample and a negative summary sample.</p><p id="p-0024" num="0023">In <b>103</b>, a total contrastive loss function is constructed based on the document sample, the positive summary sample and the negative summary sample, and a summary generation model is trained based on the total contrastive loss function.</p><p id="p-0025" num="0024">In this embodiment, the method may be performed by a summary generation model training apparatus. The training apparatus may be located in a terminal, a server or the like.</p><p id="p-0026" num="0025">The document sample may be acquired from an existing data set. For example, through historical collection or construction, a large amount of existing data may be obtained, which may include existing documents. The existing documents are taken as document samples.</p><p id="p-0027" num="0026">Representation is information configured to describe data. For example, a representation for pixels may be RGB data or HSV data. The representation may generally describe data in a vector form.</p><p id="p-0028" num="0027">The summary generation model (hereinafter referred to as &#x201c;model&#x201d;) refers to a model processing a document to obtain a summary corresponding to the document. For example, as shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, in an application stage, a document for which a summary is to be generated is inputted to the summary generation model, and the summary generation model processes the document and outputs a summary corresponding to the document. The summary corresponding to the document refers to key information in the document. New words and phrases that are not in an original document may be generated based on the summary generated by the summary generation model.</p><p id="p-0029" num="0028">A positive summary representation refers to a representation of the positive summary sample corresponding to the document sample. A negative summary representation refers to a representation of the negative summary sample corresponding to the document sample.</p><p id="p-0030" num="0029">The positive summary sample refers to a summary sample semantically consistent with the document sample, while the negative summary sample refers to a summary sample semantically inconsistent with the document sample. For example, a document sample says that a movie is good. If a summary sample also says that the movie is good or semantically similar, such as awesome or Okay, the summary sample is a positive summary sample. If a summary sample says that the movie is bad, for example, awful, the summary sample is a negative summary sample.</p><p id="p-0031" num="0030">In the related art, during the training of the summary generation model, a maximum likelihood probability function is generally used as a loss function. The maximum likelihood probability function is a 2-tuple function based on a predictive representation and a real representation of a sample. However, since the maximum likelihood function only reflects a statistical relationship and cannot accurately reflect a semantic relationship, the accuracy of the summary generation model may be affected.</p><p id="p-0032" num="0031">In the embodiment of the present disclosure, during the training of the summary generation model, the loss function used is a contrastive loss function. The contrastive loss function is a loss function constructed based on a triple to better compare a relationship between positive and negative samples. In order to be distinguished from a subsequent contrastive loss function, the contrastive loss function finally adopted in model training may be referred to as a total contrastive loss function.</p><p id="p-0033" num="0032">Specifically, a triple on which the total contrastive loss function is based includes: a document representation, a positive summary representation and a negative summary representation. A training objective of the total contrastive loss function based on the triple is sample representations with strong semantic relevance being close in distance, and sample representations with weak semantic relevance being far in distance. In this way, during prediction (i.e., a summary of a document is generated using a model), even if there are some fluctuations in the generated summary caused by noise, the model may generate a summary with good semantic relevance because semantically unrelated texts are far away from each other.</p><p id="p-0034" num="0033">As shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, assuming that a document is a review document about a movie A and the review document indicates a good movie (A), ideally, a summary (Movie A is great) corresponding to the review document may be represented by a white dot. Even in the presence of noise, the generated summary (&#x201c;Movie A is awesome&#x201d; or &#x201c;The movie is Okay&#x201d;) may be represented by a slash filled dot. However, since the semantically unrelated texts are far away from each other in model training, semantically unrelated summaries may not be generated. For example, a summary (The movie is awful) represented by a black dot may not be generated. It may be understood that, to simplify the illustration, the dots in <figref idref="DRAWINGS">FIG. <b>3</b></figref> are represented in a mainfold space.</p><p id="p-0035" num="0034">A triple-based total contrastive loss function may be constructed based on the triple. Then, the summary generation model may be trained based on the total contrastive loss function. That is, model parameters may be adjusted based on the total contrastive loss function until the total contrastive loss function converges.</p><p id="p-0036" num="0035">In this embodiment, a total contrastive loss function is constructed based on the document representation, the positive summary representation corresponding to the document representation and the negative summary representation corresponding to the document representation, and the summary generation model is trained based on the total contrastive loss function, so that contractive learning is introduced in model training, which improves the accuracy of the summary generation model.</p><p id="p-0037" num="0036">As shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, in the summary generation model, input is one type of text (document) and output is another type of text (summary). Therefore, the summary generation model may be a seq2seq model. The seq2seq model generally includes an encoder and a decoder.</p><p id="p-0038" num="0037">As shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, for example, the summary generation model includes an encoder and a decoder. In the application stage, a document is inputted to the encoder, the encoder processes the document, an encoding representation is obtained, the encoding representation may be then inputted to the decoder and processed by the decoder, a decoding representation is obtained, and a text corresponding to the decoding representation may be acquired by table lookup based on the decoding representation as a summary corresponding to the document. The seq2seq model including an encoder and a decoder may be of a model structure in the related art, such as a transformer model. In a training stage, the inputted document may be referred to as a document sample. An encoding representation and/or a decoding representation corresponding to the document sample may be taken as the document representation.</p><p id="p-0039" num="0038">Obtaining a document representation based on an encoder and a decoder may be applied to seq2seq model scenarios.</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a schematic diagram of a fifth embodiment according to the present disclosure. This embodiment provides a summary generation model training method. In conjunction with the structure shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the method may include the following steps:</p><p id="p-0041" num="0040">In <b>501</b>, the document sample is processed by using the encoder in the summary generation model, to obtain an encoding representation.</p><p id="p-0042" num="0041">In <b>502</b>, the encoding representation is processed by using the decoder in the summary generation model, to obtain a decoding representation.</p><p id="p-0043" num="0042">The encoder and the decoder may be the encoder and the decoder in the seq2seq model in the related art. The seq2seq model is, for example, a transformer model.</p><p id="p-0044" num="0043">In <b>503</b>, a generation text corresponding to the decoding representation is acquired.</p><p id="p-0045" num="0044">The decoding representation is generally a multi-dimensional vector, and a table of correspondences between vectors and texts may be pre-configured. A text corresponding to the decoding representation may be obtained as the generation text by looking up the table.</p><p id="p-0046" num="0045">In <b>504</b>, a positive summary sample and a negative summary sample are constructed based on the generation text.</p><p id="p-0047" num="0046">The positive summary sample may be obtained by loopback translation.</p><p id="p-0048" num="0047">That is, loopback translation may be performed on the generation text to obtain a loopback translation result, and the loopback translation result is taken as the positive summary sample.</p><p id="p-0049" num="0048">For example, if the generation text is Chinese 1, Chinese 1 is correspondingly translated into English 0 by using a translator, and English 0 is correspondingly translated into Chinese 2 by using the translator, Chinese 2 is a loopback translation result of Chinese 1. That is, Chinese 2 may be taken as a positive summary sample of Chinese 1.</p><p id="p-0050" num="0049">Syntactically different but semantically consistent positive summary samples may be constructed by loopback translation.</p><p id="p-0051" num="0050">The negative summary sample may be obtained by using one or more of the following.</p><p id="p-0052" num="0051">(1) Entity replacement is performed on the generation text to obtain an entity replacement result, and the entity replacement result is taken as the negative summary sample.</p><p id="p-0053" num="0052">For example, if the generation text includes a geographical-name entity &#x201c;Beijing&#x201d;, geographical-name entity &#x201c;Beijing&#x201d; may be replaced with another geographical-name entity, such as &#x201c;Tianjin&#x201d;, so as to construct an error in an entity relationship, and the replaced text including &#x201c;Tianjin&#x201d; is taken as the negative summary sample.</p><p id="p-0054" num="0053">(2) Pronoun replacement is performed on the generation text to obtain a pronoun replacement result, and the pronoun replacement result is taken as the negative summary sample.</p><p id="p-0055" num="0054">For example, if the generation text includes a personal pronoun &#x201c;he&#x201d;, it may be replaced with &#x201c;she&#x201d;, so as to construct an error in the personal pronoun, and the replaced text including &#x201c;she&#x201d; is taken as the negative summary sample.</p><p id="p-0056" num="0055">(3) Emotion replacement is performed on the generation text to obtain an emotion replacement result, and the emotion replacement result is taken as the negative summary sample.</p><p id="p-0057" num="0056">For example, a positive statement is replaced with a negative statement. Specifically, &#x201c;yes&#x201d; in a text is replaced with &#x201c;no&#x201d;, so as to construct an emotional error. The replaced text including &#x201c;no&#x201d; is taken as the negative summary sample.</p><p id="p-0058" num="0057">(4) A similar text of the generation text is acquired, and the similar text is taken as the negative summary sample.</p><p id="p-0059" num="0058">The similar text may refer to a text strongly similar to the generation text. Specifically, similarities between the generation texts and existing candidate texts may be calculated, and a candidate text with the highest similarity (top-1) or N (N may be set) (top-N) candidate texts with higher similarities may be taken as the negative summary sample(s).</p><p id="p-0060" num="0059">(5) Virtual adversarial processing is performed on the generation text to obtain a virtual adversarial result, and the virtual adversarial result is taken as the negative summary sample.</p><p id="p-0061" num="0060">Virtual adversarial is a data enhancement technology. A key step of virtual adversarial is to add disturbance to input to enable output of a model to be different from that of undisturbed input. In this embodiment, disturbance may be added to the representation corresponding to the generation text by virtual adversarial, and the representation to which disturbance is added is taken as a negative summary representation.</p><p id="p-0062" num="0061">A strong negative summary sample with factual errors and not easily distinguishable on the surface may be constructed through the above negative summary sample construction technology, thereby effectively improving model performance</p><p id="p-0063" num="0062">A number of samples may be enriched by constructing the positive summary sample and the negative summary sample based on the generation text, thereby improving a model effect.</p><p id="p-0064" num="0063">In <b>505</b>, a positive summary representation corresponding to the positive summary sample and a negative summary representation corresponding to the negative summary sample are acquired.</p><p id="p-0065" num="0064">Taking the positive summary sample as an example, the positive summary sample may be converted to a corresponding vector form by using a word2vec model or other text-to-vector conversion models, to serve as the positive summary representation. The negative summary representation may also be acquired in a similar manner.</p><p id="p-0066" num="0065">In <b>506</b>, a first contrastive loss function is constructed based on the encoding representation, the positive summary representation and the negative summary representation; a second contrastive loss function is constructed based on the decoding representation, the positive summary representation and the negative summary representation; and the total contrastive loss function is constructed based on the first contrastive loss function and the second contrastive loss function.</p><p id="p-0067" num="0066">As shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the positive summary representation is represented by P, and the negative summary representation is represented by N. In this embodiment, two semantic contrasts are included, of which one may be referred to as an input-side semantic contrast and the other may be referred to as an output-side semantic contrast. A contrast triplet of the input-side semantic contrast includes an encoding representation, a positive summary representation and a negative summary representation. A contrast triplet of the output-side semantic contrast includes a decoding representation, a positive summary representation and a negative summary representation.</p><p id="p-0068" num="0067">A specific form of the contrastive loss function may be set as required, and one calculation formula may be as follows:</p><p id="p-0069" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mrow>  <mrow>   <msub>    <mi>l</mi>    <mn>1</mn>   </msub>   <mo>=</mo>   <mrow>    <mrow>     <mo>-</mo>     <mi>log</mi>    </mrow>    <mo>&#x2062;</mo>    <mfrac>     <mrow>      <mi>exp</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mrow>       <mi>s</mi>       <mo>&#x2062;</mo>       <mi>i</mi>       <mo>&#x2062;</mo>       <mrow>        <mrow>         <mi>m</mi>         <mo>&#x2061;</mo>         <mo>(</mo>         <mrow>          <msub>           <mi>z</mi>           <mn>0</mn>          </msub>          <mo>,</mo>          <msub>           <mi>z</mi>           <mn>1</mn>          </msub>         </mrow>         <mo>)</mo>        </mrow>        <mo>/</mo>        <mi>&#x3c4;</mi>       </mrow>      </mrow>      <mo>)</mo>     </mrow>     <mrow>      <mover>       <munder>        <mo>&#x2211;</mo>        <mrow>         <mi>k</mi>         <mo>=</mo>         <mn>1</mn>        </mrow>       </munder>       <mi>n</mi>      </mover>      <mrow>       <mi>exp</mi>       <mo>&#x2061;</mo>       <mo>(</mo>       <mrow>        <mi>s</mi>        <mo>&#x2062;</mo>        <mi>i</mi>        <mo>&#x2062;</mo>        <mrow>         <mrow>          <mi>m</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <mrow>           <msub>            <mi>z</mi>            <mn>0</mn>           </msub>           <mo>,</mo>           <msub>            <mi>z</mi>            <mi>k</mi>           </msub>          </mrow>          <mo>)</mo>         </mrow>         <mo>/</mo>         <mi>&#x3c4;</mi>        </mrow>       </mrow>       <mo>)</mo>      </mrow>     </mrow>    </mfrac>   </mrow>  </mrow>  <mo>&#x2062;</mo>  <mtext></mtext>  <mrow>   <msub>    <mi>l</mi>    <mn>2</mn>   </msub>   <mo>=</mo>   <mrow>    <mrow>     <mo>-</mo>     <mi>log</mi>    </mrow>    <mo>&#x2062;</mo>    <mfrac>     <mrow>      <mi>exp</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mrow>       <mrow>        <mi>sim</mi>        <mo>&#x2061;</mo>        <mo>(</mo>        <mrow>         <msubsup>          <mi>z</mi>          <mn>0</mn>          <mo>&#x2032;</mo>         </msubsup>         <mo>,</mo>         <msub>          <mi>z</mi>          <mn>1</mn>         </msub>        </mrow>        <mo>)</mo>       </mrow>       <mo>/</mo>       <mi>&#x3c4;</mi>      </mrow>      <mo>)</mo>     </mrow>     <mrow>      <mover>       <munder>        <mo>&#x2211;</mo>        <mrow>         <mi>k</mi>         <mo>=</mo>         <mn>1</mn>        </mrow>       </munder>       <mi>n</mi>      </mover>      <mrow>       <mi>exp</mi>       <mo>&#x2061;</mo>       <mo>(</mo>       <mrow>        <mi>s</mi>        <mo>&#x2062;</mo>        <mrow>         <mrow>          <mi>im</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <mrow>           <msubsup>            <mi>z</mi>            <mn>0</mn>            <mo>&#x2032;</mo>           </msubsup>           <mo>,</mo>           <mtext>&#x205f;</mtext>           <msub>            <mi>z</mi>            <mi>k</mi>           </msub>          </mrow>          <mo>)</mo>         </mrow>         <mo>/</mo>         <mi>&#x3c4;</mi>        </mrow>       </mrow>       <mo>)</mo>      </mrow>     </mrow>    </mfrac>   </mrow>  </mrow>  <mo>&#x2062;</mo>  <mspace linebreak="newline"/>  <mrow>   <mi>L</mi>   <mo>=</mo>   <mrow>    <msub>     <mi>l</mi>     <mn>1</mn>    </msub>    <mo>+</mo>    <msub>     <mi>l</mi>     <mn>2</mn>    </msub>   </mrow>  </mrow> </mrow></math></maths></p><p id="p-0070" num="0000">L denotes the total contrastive loss function, l<sub>1 </sub>denotes the first contrastive loss function, l<sub>2 </sub>denotes the second contrastive loss function, z<sub>0 </sub>denotes the encoding representation, z<sub>0 </sub>denotes the decoding representation, z<sub>1 </sub>denotes the positive summary representation, z<sub>2 </sub>denotes the negative summary representation, n denotes a total number of negative summary representations, and &#x3c4; denotes a preset hyper-parameter.</p><p id="p-0071" num="0068">Factual consistency between the decoding representation and the encoding representation may be learned through the input-side semantic contrast. That is, if one encoding representation and a plurality of decoding representations are given, the model may learn to give a greater similarity for correctly matched decoding representations and a smaller similarity for mismatched decoding representations. Similarities between output representations may be learned through the output-side semantic contrast. That is, similarities between positive summary representations having factual consistency with the decoding representation are greater, while similarities between positive summary representations and negative summary representations are smaller.</p><p id="p-0072" num="0069">In <b>507</b>, a summary generation model is trained based on the total contrastive loss function.</p><p id="p-0073" num="0070">For example, parameters of the summary generation model are adjusted using the total contrastive loss function until the total contrastive loss function converges or reaches a preset number of iterations.</p><p id="p-0074" num="0071">In this embodiment, through the two semantic contrasts, a phenomenon that the summary generation model generates summaries with factual errors may be alleviated, which is more faithful to the original text and guarantees the quality of generation compared with summaries generated by an ordinary seq2seq model. In addition, when the summary generation model according to this embodiment is adopted, there is no need to pre-process and post-process document samples during training and documents during prediction, which may improve the efficiency of training or prediction.</p><p id="p-0075" num="0072"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a schematic diagram of a sixth embodiment according to the present disclosure. This embodiment provides a summary generation model training apparatus. As shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the summary generation model training apparatus <b>600</b> includes: an acquisition module <b>601</b>, a construction module <b>602</b>, and a training module <b>603</b>.</p><p id="p-0076" num="0073">The acquisition module <b>601</b> is configured to acquire a document representation corresponding to a document sample. The construction module <b>602</b> is configured to construct, based on the document representation, a summary representation corresponding to the document representation, the summary representation including a positive summary representation and a negative summary representation. The training module <b>603</b> is configured to construct a total contrastive loss function based on the document representation, the positive summary representation and the negative summary representation, and train a summary generation model based on the total contrastive loss function.</p><p id="p-0077" num="0074">In some embodiments, the summary generation model includes: an encoder and a decoder, and the acquisition module <b>601</b> is specifically configured to: process the document sample by using the encoder, to obtain an encoding representation; process the encoding representation by using the decoder, to obtain a decoding representation; and take the encoding representation or the decoding representation as the document representation.</p><p id="p-0078" num="0075">In some embodiments, the document representation includes the encoding representation and the decoding representation, and the training module <b>603</b> is specifically configured to: construct a first contrastive loss function based on the encoding representation, the positive summary representation and the negative summary representation; construct a second contrastive loss function based on the decoding representation, the positive summary representation and the negative summary representation; and construct the total contrastive loss function based on the first contrastive loss function and the second contrastive loss function.</p><p id="p-0079" num="0076">In some embodiments, the document representation includes the decoding representation, and the construction module <b>602</b> is specifically configured to: acquire a generation text corresponding to the decoding representation; construct a positive summary sample and a negative summary sample based on the generation text; and acquire a positive summary representation corresponding to the positive summary sample and a negative summary representation corresponding to the negative summary sample.</p><p id="p-0080" num="0077">In some embodiments, the construction module <b>602</b> is further specifically configured to: perform loopback translation on the generation text to obtain a loopback translation result, and take the loopback translation result as the positive summary sample.</p><p id="p-0081" num="0078">In some embodiments, the construction module <b>602</b> is further specifically configured to perform at least one of the following: performing entity replacement on the generation text to obtain an entity replacement result, and taking the entity replacement result as the negative summary sample; performing pronoun replacement on the generation text to obtain a pronoun replacement result, and taking the pronoun replacement result as the negative summary sample; performing emotion replacement on the generation text to obtain an emotion replacement result, and taking the emotion replacement result as the negative summary sample; acquiring a similar text of the generation text, and taking the similar text as the negative summary sample; and performing virtual adversarial training on the generation text to obtain a virtual adversarial result, and taking the virtual adversarial result as the negative summary sample.</p><p id="p-0082" num="0079">In this embodiment, a total contrastive loss function is constructed based on the document representation, the positive summary representation corresponding to the document representation and the negative summary representation corresponding to the document representation, and the summary generation model is trained based on the total contrastive loss function, so that contractive learning is introduced in model training, which improves the accuracy of the summary generation model.</p><p id="p-0083" num="0080">It may be understood that the same or similar contents in different embodiments may be referred to each other in the embodiments of the present disclosure.</p><p id="p-0084" num="0081">It may be understood that &#x201c;first&#x201d;, &#x201c;second&#x201d; and the like in the embodiments of the present disclosure are intended only for differentiation, and do not indicate a degree of importance or sequence.</p><p id="p-0085" num="0082">According to embodiments of the present disclosure, the present disclosure further provides an electronic device, a readable storage medium and a computer program product.</p><p id="p-0086" num="0083"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a schematic block diagram of an exemplary electronic device <b>700</b> configured to perform embodiments of the present disclosure. The electronic device is intended to represent various forms of digital computers, such as laptops, desktops, workbenches, personal digital assistants, servers, blade servers, mainframe computers and other suitable computing devices. The electronic device may further represent various forms of mobile devices, such as personal digital assistants, cellular phones, smart phones, wearable devices and other similar computing devices. The components, their connections and relationships, and their functions shown herein are examples only, and are not intended to limit the implementation of the present disclosure as described and/or required herein.</p><p id="p-0087" num="0084">As shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the electronic device <b>700</b> includes a computing unit <b>701</b>, which may perform various suitable actions and processing according to a computer program stored in a read-only memory (ROM) <b>702</b> or a computer program loaded from a storage unit <b>708</b> into a random access memory (RAM) <b>703</b>. The RAM <b>703</b> may also store various programs and data required to operate the electronic device <b>700</b>. The computing unit <b>701</b>, the ROM <b>702</b> and the RAM <b>703</b> are connected to one another by a bus <b>704</b>. An input/output (I/O) interface <b>705</b> is also connected to the bus <b>704</b>.</p><p id="p-0088" num="0085">A plurality of components in the electronic device <b>700</b> are connected to the I/O interface <b>705</b>, including an input unit <b>706</b>, such as a keyboard and a mouse; an output unit <b>707</b>, such as various displays and speakers; a storage unit <b>708</b>, such as disks and discs; and a communication unit <b>709</b>, such as a network card, a modem and a wireless communication transceiver. The communication unit <b>709</b> allows the electronic device <b>700</b> to exchange information/data with other devices over computer networks such as the Internet and/or various telecommunications networks.</p><p id="p-0089" num="0086">The computing unit <b>701</b> may be a variety of general-purpose and/or special-purpose processing components with processing and computing capabilities. Some examples of the computing unit <b>701</b> include, but are not limited to, a central processing unit (CPU), a graphics processing unit (GPU), various artificial intelligence (AI) computing chips, various computing units that run machine learning model algorithms, a digital signal processor (DSP), and any appropriate processor, controller or microcontroller, etc. The computing unit <b>701</b> performs the methods and processing described above, such as the summary generation model training method. For example, in some embodiments, the summary generation model training method may be implemented as a computer software program that is tangibly embodied in a machine-readable medium, such as the storage unit <b>708</b>. In some embodiments, part or all of a computer program may be loaded and/or installed on the electronic device <b>700</b> via the ROM <b>702</b> and/or the communication unit <b>709</b>. One or more steps of the summary generation model training method described above may be performed when the computer program is loaded into the RAM <b>703</b> and executed by the computing unit <b>701</b>. Alternatively, in other embodiments, the computing unit <b>701</b> may be configured to perform the summary generation model training method described in the present disclosure by any other appropriate means (for example, by means of firmware).</p><p id="p-0090" num="0087">Various implementations of the systems and technologies disclosed herein can be realized in a digital electronic circuit system, an integrated circuit system, a field programmable gate array (FPGA), an application-specific integrated circuit (ASIC), an application-specific standard product (ASSP), a system on chip (SOC), a load programmable logic device (CPLD), computer hardware, firmware, software, and/or combinations thereof. Such implementations may include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which can be special or general purpose, configured to receive data and instructions from a storage system, at least one input apparatus, and at least one output apparatus, and to transmit data and instructions to the storage system, the at least one input apparatus, and the at least one output apparatus.</p><p id="p-0091" num="0088">Program codes configured to implement the methods in the present disclosure may be written in any combination of one or more programming languages. Such program codes may be supplied to a processor or controller of a general-purpose computer, a special-purpose computer, or another programmable data processing apparatus to enable the function/operation specified in the flowchart and/or block diagram to be implemented when the program codes are executed by the processor or controller. The program codes may be executed entirely on a machine, partially on a machine, partially on a machine and partially on a remote machine as a stand-alone package, or entirely on a remote machine or a server.</p><p id="p-0092" num="0089">In the context of the present disclosure, machine-readable media may be tangible media which may include or store programs for use by or in conjunction with an instruction execution system, apparatus or device. The machine-readable media may be machine-readable signal media or machine-readable storage media. The machine-readable media may include, but are not limited to, electronic, magnetic, optical, electromagnetic, infrared, or semiconductor systems, apparatuses or devices, or any suitable combinations thereof. More specific examples of machine-readable storage media may include electrical connections based on one or more wires, a portable computer disk, a hard disk, a random access memory (RAM), a read-only memory (ROM), an erasable programmable read only memory (EPROM or flash memory), an optical fiber, a compact disk read only memory (CD-ROM), an optical storage device, a magnetic storage device, or any suitable combination thereof.</p><p id="p-0093" num="0090">To provide interaction with a user, the systems and technologies described here can be implemented on a computer. The computer has: a display apparatus (e.g., a cathode-ray tube (CRT) or a liquid crystal display (LCD) monitor) for displaying information to the user; and a keyboard and a pointing apparatus (e.g., a mouse or trackball) through which the user may provide input for the computer. Other kinds of apparatuses may also be configured to provide interaction with the user. For example, a feedback provided for the user may be any form of sensory feedback (e.g., visual, auditory, or tactile feedback); and input from the user may be received in any form (including sound input, voice input, or tactile input).</p><p id="p-0094" num="0091">The systems and technologies described herein can be implemented in a computing system including background components (e.g., as a data server), or a computing system including middleware components (e.g., an application server), or a computing system including front-end components (e.g., a user computer with a graphical user interface or web browser through which the user can interact with the implementation mode of the systems and technologies described here), or a computing system including any combination of such background components, middleware components or front-end components. The components of the system can be connected to each other through any form or medium of digital data communication (e.g., a communication network). Examples of the communication network include: a local area network (LAN), a wide area network (WAN) and the Internet.</p><p id="p-0095" num="0092">The computer system may include a client and a server. The client and the server are generally far away from each other and generally interact via the communication network. A relationship between the client and the server is generated through computer programs that run on a corresponding computer and have a client-server relationship with each other. The server may be a cloud server, also known as a cloud computing server or cloud host, which is a host product in the cloud computing service system to solve the problems of difficult management and weak business scalability in the traditional physical host and a virtual private server (VPS). The server may also be a distributed system server, or a server combined with blockchain.</p><p id="p-0096" num="0093">It should be understood that the steps can be reordered, added, or deleted using the various forms of processes shown above. For example, the steps described in the present disclosure may be executed in parallel or sequentially or in different sequences, provided that desired results of the technical solutions disclosed in the present disclosure are achieved, which is not limited herein.</p><p id="p-0097" num="0094">The above specific implementations do not limit the extent of protection of the present disclosure. Those skilled in the art should understand that various modifications, combinations, sub-combinations, and replacements can be made according to design requirements and other factors. Any modifications, equivalent substitutions and improvements made within the spirit and principle of the present disclosure all should be included in the extent of protection of the present disclosure.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230004589A1-20230105-M00001.NB"><img id="EMI-M00001" he="29.29mm" wi="76.20mm" file="US20230004589A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A summary generation model training method, comprising:<claim-text>acquiring a document representation corresponding to a document sample;</claim-text><claim-text>constructing, based on the document representation, a summary representation corresponding to the document representation, the summary representation comprising a positive summary representation and a negative summary representation; and</claim-text><claim-text>constructing a total contrastive loss function based on the document representation, the positive summary representation and the negative summary representation, and training a summary generation model based on the total contrastive loss function.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the summary generation model comprises: an encoder and a decoder, and the step of acquiring a document representation corresponding to a document sample comprises:<claim-text>processing the document sample by using the encoder, to obtain an encoding representation;</claim-text><claim-text>processing the encoding representation by using the decoder, to obtain a decoding representation; and</claim-text><claim-text>taking the encoding representation or the decoding representation as the document representation.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the document representation comprises the encoding representation and the decoding representation, and the step of constructing a total contrastive loss function based on the document representation, the positive summary representation and the negative summary representation comprises:<claim-text>constructing a first contrastive loss function based on the encoding representation, the positive summary representation and the negative summary representation;</claim-text><claim-text>constructing a second contrastive loss function based on the decoding representation, the positive summary representation and the negative summary representation; and</claim-text><claim-text>constructing the total contrastive loss function based on the first contrastive loss function and the second contrastive loss function.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the document representation comprises the decoding representation, and the step of constructing, based on the document representation, a summary representation corresponding to the document representation comprises:<claim-text>acquiring a generation text corresponding to the decoding representation;</claim-text><claim-text>constructing a positive summary sample and a negative summary sample based on the generation text; and</claim-text><claim-text>acquiring a positive summary representation corresponding to the positive summary sample and a negative summary representation corresponding to the negative summary sample.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the step of constructing a positive summary sample based on the generation text comprises:<claim-text>performing loopback translation on the generation text to obtain a loopback translation result, and taking the loopback translation result as the positive summary sample.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the step of constructing a negative summary sample based on the generation text comprises at least one of the following:<claim-text>performing entity replacement on the generation text to obtain an entity replacement result, and taking the entity replacement result as the negative summary sample;</claim-text><claim-text>performing pronoun replacement on the generation text to obtain a pronoun replacement result, and taking the pronoun replacement result as the negative summary sample;</claim-text><claim-text>performing emotion replacement on the generation text to obtain an emotion replacement result, and taking the emotion replacement result as the negative summary sample;</claim-text><claim-text>acquiring a similar text of the generation text, and taking the similar text as the negative summary sample; and</claim-text><claim-text>performing virtual adversarial training on the generation text to obtain a virtual adversarial result, and taking the virtual adversarial result as the negative summary sample.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. An electronic device, comprising:<claim-text>at least one processor; and</claim-text><claim-text>a memory communicatively connected with the at least one processor;</claim-text><claim-text>wherein the memory stores instructions executable by the at least one processor, and the instructions are executed by the at least one processor to enable the at least one processor to perform a summary generation model training method, wherein the summary generation model training method comprises:</claim-text><claim-text>acquiring a document representation corresponding to a document sample;</claim-text><claim-text>constructing, based on the document representation, a summary representation corresponding to the document representation, the summary representation comprising a positive summary representation and a negative summary representation; and</claim-text><claim-text>constructing a total contrastive loss function based on the document representation, the positive summary representation and the negative summary representation, and training a summary generation model based on the total contrastive loss function.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The electronic device according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the summary generation model comprises: an encoder and a decoder, and the step of acquiring a document representation corresponding to a document sample comprises:<claim-text>processing the document sample by using the encoder, to obtain an encoding representation;</claim-text><claim-text>processing the encoding representation by using the decoder, to obtain a decoding representation; and</claim-text><claim-text>taking the encoding representation or the decoding representation as the document representation.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The electronic device according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the document representation comprises the encoding representation and the decoding representation, and the step of constructing a total contrastive loss function based on the document representation, the positive summary representation and the negative summary representation comprises:<claim-text>constructing a first contrastive loss function based on the encoding representation, the positive summary representation and the negative summary representation;</claim-text><claim-text>constructing a second contrastive loss function based on the decoding representation, the positive summary representation and the negative summary representation; and</claim-text><claim-text>constructing the total contrastive loss function based on the first contrastive loss function and the second contrastive loss function.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The electronic device according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the document representation comprises the decoding representation, and the step of constructing, based on the document representation, a summary representation corresponding to the document representation comprises:<claim-text>acquiring a generation text corresponding to the decoding representation;</claim-text><claim-text>constructing a positive summary sample and a negative summary sample based on the generation text; and</claim-text><claim-text>acquiring a positive summary representation corresponding to the positive summary sample and a negative summary representation corresponding to the negative summary sample.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The electronic device according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the step of constructing a positive summary sample based on the generation text comprises:<claim-text>performing loopback translation on the generation text to obtain a loopback translation result, and taking the loopback translation result as the positive summary sample.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The electronic device according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the step of constructing a negative summary sample based on the generation text comprises at least one of the following:<claim-text>performing entity replacement on the generation text to obtain an entity replacement result, and taking the entity replacement result as the negative summary sample;</claim-text><claim-text>performing pronoun replacement on the generation text to obtain a pronoun replacement result, and taking the pronoun replacement result as the negative summary sample;</claim-text><claim-text>performing emotion replacement on the generation text to obtain an emotion replacement result, and taking the emotion replacement result as the negative summary sample;</claim-text><claim-text>acquiring a similar text of the generation text, and taking the similar text as the negative summary sample; and</claim-text><claim-text>performing virtual adversarial training on the generation text to obtain a virtual adversarial result, and taking the virtual adversarial result as the negative summary sample.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. A non-transitory computer readable storage medium with computer instructions stored thereon, wherein the computer instructions are used for causing a computer to perform a summary generation model training method, wherein the summary generation model training method comprises:<claim-text>acquiring a document representation corresponding to a document sample;</claim-text><claim-text>constructing, based on the document representation, a summary representation corresponding to the document representation, the summary representation comprising a positive summary representation and a negative summary representation; and</claim-text><claim-text>constructing a total contrastive loss function based on the document representation, the positive summary representation and the negative summary representation, and training a summary generation model based on the total contrastive loss function.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The non-transitory computer readable storage medium according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the summary generation model comprises: an encoder and a decoder, and the step of acquiring a document representation corresponding to a document sample comprises:<claim-text>processing the document sample by using the encoder, to obtain an encoding representation;</claim-text><claim-text>processing the encoding representation by using the decoder, to obtain a decoding representation; and</claim-text><claim-text>taking the encoding representation or the decoding representation as the document representation.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The non-transitory computer readable storage medium according to <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the document representation comprises the encoding representation and the decoding representation, and the step of constructing a total contrastive loss function based on the document representation, the positive summary representation and the negative summary representation comprises:<claim-text>constructing a first contrastive loss function based on the encoding representation, the positive summary representation and the negative summary representation;</claim-text><claim-text>constructing a second contrastive loss function based on the decoding representation, the positive summary representation and the negative summary representation; and</claim-text><claim-text>constructing the total contrastive loss function based on the first contrastive loss function and the second contrastive loss function.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The non-transitory computer readable storage medium according to <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the document representation comprises the decoding representation, and the step of constructing, based on the document representation, a summary representation corresponding to the document representation comprises:<claim-text>acquiring a generation text corresponding to the decoding representation;</claim-text><claim-text>constructing a positive summary sample and a negative summary sample based on the generation text; and</claim-text><claim-text>acquiring a positive summary representation corresponding to the positive summary sample and a negative summary representation corresponding to the negative summary sample.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The non-transitory computer readable storage medium according to <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the step of constructing a positive summary sample based on the generation text comprises:<claim-text>performing loopback translation on the generation text to obtain a loopback translation result, and taking the loopback translation result as the positive summary sample.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The non-transitory computer readable storage medium according to <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the step of constructing a negative summary sample based on the generation text comprises at least one of the following:<claim-text>performing entity replacement on the generation text to obtain an entity replacement result, and taking the entity replacement result as the negative summary sample;</claim-text><claim-text>performing pronoun replacement on the generation text to obtain a pronoun replacement result, and taking the pronoun replacement result as the negative summary sample;</claim-text><claim-text>performing emotion replacement on the generation text to obtain an emotion replacement result, and taking the emotion replacement result as the negative summary sample;</claim-text><claim-text>acquiring a similar text of the generation text, and taking the similar text as the negative summary sample; and</claim-text><claim-text>performing virtual adversarial training on the generation text to obtain a virtual adversarial result, and taking the virtual adversarial result as the negative summary sample.</claim-text></claim-text></claim></claims></us-patent-application>