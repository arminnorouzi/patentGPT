<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005595A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005595</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17365283</doc-number><date>20210701</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>16</class><subclass>H</subclass><main-group>20</main-group><subgroup>70</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>11</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>16</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20180101</date></cpc-version-indicator><section>G</section><class>16</class><subclass>H</subclass><main-group>20</main-group><subgroup>70</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>6898</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>7264</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>11</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>165</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>2562</main-group><subgroup>0219</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>2560</main-group><subgroup>0493</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">ADMINISTERING EXPOSURE TREATMENTS OF A COGNITIVE BEHAVIORAL THERAPY USING A SMARTPHONE APP</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Koa Health B.V.</orgname><address><city>Barcelona</city><country>ES</country></address></addressbook><residence><country>ES</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Garriga Calleja</last-name><first-name>Roger</first-name><address><city>Barcelona</city><country>ES</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Estella Aguerri</last-name><first-name>I&#xf1;aki</first-name><address><city>Barcelona</city><country>ES</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A method for administering an exposure treatment of a cognitive behavioral therapy (CBT) uses a mobile app and a server application. A user state of a patient undergoing a first step of the CBT based on the patient's condition during the first step is detected by sensors of the patient's smartphone. A situational state of the patient's surroundings during the first step is detected by the smartphone sensors. The mobile app determines whether the patient has made progress performing the first step. A user prompt is generated based on the user state and situational state. A next step of the CBT is configured based on the user state and situational state. The characteristics of the user prompt are generated using machine learning based on past task completions by the patient and other users so as to increase the likelihood that the patient will complete the next step of the CBT.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="216.58mm" wi="158.75mm" file="US20230005595A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="226.91mm" wi="144.10mm" file="US20230005595A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="188.13mm" wi="168.83mm" file="US20230005595A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="237.07mm" wi="173.65mm" file="US20230005595A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="202.78mm" wi="167.64mm" file="US20230005595A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="242.15mm" wi="177.97mm" file="US20230005595A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="182.54mm" wi="177.12mm" file="US20230005595A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0002" num="0001">The present invention relates to cognitive behavioral therapies for overcoming obsessive compulsive disorders through the guided exposure to anxiety triggers administered using a smartphone app.</p><heading id="h-0002" level="1">BACKGROUND INFORMATION</heading><p id="p-0003" num="0002">Cognitive behavioral therapy (CBT) is a form of psychotherapy based on both cognitive and behavioral principles designed to modify a patient's irrational thinking and behavior. CBT is used to treat behavioral conditions that cannot be controlled through rational thought, but rather result from earlier environmental conditioning. Such treatable conditions include adult anxiety disorders such as obsessive compulsive disorder (OCD).</p><p id="p-0004" num="0003">The basis for CBT treatments is the controlled exposure of the patient to the situation or object that causes the anxiety. The therapist encourages the patient directly to confront the feared situation or object. By exposing the patient to the anxiety trigger, the prior environmental conditioning can be undone, and the patient's undesired subconscious adverse response to the situation or object can be unlearned. Conventionally, the exposure therapy of a CBT program is administered in face-to-face sessions between the therapist and the patient in which the patient performs an exposure assignment. If the exposure assignment is too difficult, the therapist must recognize the patient's inability to proceed with the treatment and suggest an easier assignment.</p><p id="p-0005" num="0004">The success of the CBT program depends on the ability of the therapist to assess the patient's engagement in the exposure therapy and to encourage the patient to complete each successively more difficult assignment. However, the cost of a human therapist to accompany the patient in every step of the exposure treatment is prohibitively expensive in many cases. CBT programs can be more cost effectively administered through an interactive computer interface instead of requiring a human therapist to be physically present. However, even having a human therapist being remotely present for the exposure treatment is expensive.</p><p id="p-0006" num="0005">A system is sought that can replace the human therapist in a CBT program, but yet that can monitor the patient's progress, provide encouragement to the patient, and suggest easier or more difficult exposure assignments at the appropriate times.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0007" num="0006">A method for administering an exposure treatment of a cognitive behavioral therapy (CBT) is performed using a mobile app and a server application. The CBT involves the controlled exposure of a patient to an object that causes the patient to experience anxiety. A user state is detected of the patient while the patient is currently undergoing a first step of the CBT. The user state is based on the patient's behavior and physiological condition during the first step of the CBT as detected by sensors of a smartphone used by the patient. A situational state is detected of the patient's surroundings while the patient is undergoing the first step of the CBT. The situational state is detected by sensors of the smartphone used by the patient. The mobile app controls the detection of the user state and the situational state. The mobile app determines whether the patient has performed the first step in a manner that achieves progress in the CBT. Whether the patient has achieved progress in the CBT is determined based on the stress level and the struggle level of the patient during the first step. The stress level of the patient during the first step is determined based on touch interaction data and motion data detected by sensors on the smartphone during the first step.</p><p id="p-0008" num="0007">A verbal user prompt is generated based on the detected user state and the detected situational state during the first step. The verbal user prompt is output by a loudspeaker of the smartphone. A next step of the CBT is configured based on the detected user state and the detected situational state during the first step. The next step of the CBT is also configured using machine learning based on the past task completions by the patient and other users so as to minimize how many steps are required for the patient to complete the CBT.</p><p id="p-0009" num="0008">The content and character of the verbal user prompt is generated using machine learning based on past task completions by the patient and other users in order to make it more likely that the patient will complete the next step of the CBT. The machine learning generates the content of the verbal user prompt so as to include a level of reassurance most likely to motivate the patient to proceed with the next step of the cognitive behavioral therapy. The character of the verbal user prompt is defined by parameters such as the tone of the voice, the pitch of the voice, and the cadence of the verbal user prompt. The machine learning that generates the verbal user prompt is performed by a deep neural network.</p><p id="p-0010" num="0009">In another embodiment, a system for administering an exercise task of an exposure treatment includes a smartphone, a central server and a database storage system. Data regarding past therapy steps by the patient and other users is stored in the database storage system.</p><p id="p-0011" num="0010">Instructions of a mobile application are stored in the device memory of the smartphone. Instructions of a server application are stored in the server memory of the central server. The instructions of the mobile application when executed cause the smartphone to detect a user state and a situational state of a patient currently undergoing a first step of a CBT, determine whether the patient has achieved progress in performing the first step, and output a user prompt based on the user state and situational state detected during the first step. The user state is based on the patient's behavior and physiological condition during the first step as detected by sensors of the smartphone. The situational state of the patient's surroundings while the patient is undergoing the first step is also detected by the sensors of the smartphone.</p><p id="p-0012" num="0011">The instructions of the server application when executed cause the central server to generate the user prompt using machine learning based on past therapy steps by the patient and other users of the mobile application so as to have a content and character adapted to influence the patient to complete the next step of the CBT. The instructions of the server application also configure the next step of the CBT based on the user state and situational state detected during the first step.</p><p id="p-0013" num="0012">The mobile application determines whether the patient has achieved progress in the CBT based on the stress level of the patient during the first step. The stress level of the patient during the first step is determined based on touch interaction data and motion data detected by the sensors of the smartphone during the first step. The touch interaction data is sensed by a touchscreen of the smartphone, and the motion data is sensed by an accelerometer of the smartphone.</p><p id="p-0014" num="0013">Further details and embodiments and methods and techniques are described in the detailed description below. This summary does not purport to define the invention. The invention is defined by the claims.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0015" num="0014">The accompanying drawings, where like numerals indicate like components, illustrate embodiments of the invention.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a simplified diagram of a system used to administer an exposure treatment of a cognitive behavioral therapy.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a flowchart illustrating the operation of server software in a novel method for administering an exposure treatment.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flowchart illustrating the operation of a mobile app in the novel method for administering an exposure treatment.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows the form of a user profile generated by the method for administering an exposure treatment.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>5</b></figref> shows the form of task experience data collected by a mobile app performing the method for administering an exposure treatment.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flowchart of steps of the novel method for administering the exemplary exposure treatment of touching a toilet flush handle.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates a video image taken by the user's smartphone while the user is performing the exemplary exposure treatment of <figref idref="DRAWINGS">FIG. <b>6</b></figref>.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0023" num="0022">Reference will now be made in detail to some embodiments of the invention, examples of which are illustrated in the accompanying drawings.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram of a system used to administer an exposure assignment of a cognitive behavioral therapy (CBT). A novel method for administering the exposure treatment is performed using the system. The system includes a smartphone <b>1</b> and a central server <b>2</b>. The method is implemented using a mobile application (&#x201c;app&#x201d;) <b>3</b> running on smartphone <b>1</b> and central server software <b>4</b> running on central server <b>2</b>. Mobile app <b>3</b> is stored in a device memory of smartphone <b>1</b>, and central server software <b>4</b> is stored in server memory of central server <b>2</b>. Central server software <b>4</b> includes a server application that works together with mobile app <b>3</b>. The user <b>5</b> of app <b>3</b> is a patient undergoing the exposure treatment. As the patient <b>5</b> is undergoing the exposure treatment, app <b>3</b> on smartphone <b>1</b> sends collected task experience data to central server <b>2</b>, where the data is stored in a database storage system <b>6</b>.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a simplified diagram illustrating the operation of server software <b>4</b>. <figref idref="DRAWINGS">FIG. <b>3</b></figref> is a simplified diagram illustrating the operation of mobile app <b>3</b>, which is the application program that together with the server application of server software <b>4</b> administers the cognitive behavioral therapy. App <b>3</b> and server software <b>4</b> execute simultaneously. Steps <b>100</b>-<b>113</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref> are steps of central server software <b>4</b>, whereas steps <b>200</b>-<b>222</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref> are steps of app <b>3</b>.</p><p id="p-0026" num="0025">The cognitive behavioral therapy cannot begin until patient <b>5</b> installs app <b>3</b> on his or her smartphone <b>1</b>. <figref idref="DRAWINGS">FIG. <b>3</b></figref> begins with step <b>200</b>, in which user <b>5</b> installs app <b>3</b> on smartphone <b>1</b>, and app <b>3</b> starts running in step <b>201</b>. If it is determined in step <b>202</b> that user <b>5</b> is using app <b>3</b> for the first time, then app <b>3</b> requests and prompts user <b>5</b> in step <b>203</b> to enter personal information and characteristics and a disorder or improvement to be made. Examples of personal information and characteristics include age, sex, weight, height, medical issues and conditions, the user's profession, amount of daily physical exercise, and eating habits. Examples of disorders include obsessive-compulsive disorder (OCD), phobias, panic disorders, social anxiety disorder, post-traumatic stress disorder, and generalized anxiety disorder. An example of an improvement to be made is a muscle or body part to be strengthened or conditioned. One embodiment described herein involves OCD and a phobia for touching dirty objects. The improvement sought is the ability to touch an object that is perceived to be dirty without washing ones hands immediately. This phobia can be overcome with an exposure treatment.</p><p id="p-0027" num="0026">In step <b>203</b>, app <b>3</b> queries user <b>5</b> for this personal information using a series of visual prompts provided to user <b>5</b> on the touchscreen of smartphone <b>1</b>. User <b>5</b> responds by making appropriate selections on the touchscreen of the smartphone and entering information text. In step <b>204</b>, app <b>3</b> sends the collected information to the central server <b>2</b> via wireless cellular telephone communication. As seen in the flowchart of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the server application of server software <b>4</b> (step <b>100</b>) and then receives (step <b>101</b>) the personal information and characteristics and information on the disorder and improvement to be made. In step <b>102</b>, central server software <b>4</b> then stores the collected information in database storage system <b>6</b> and assembles the information in the form of a user profile <b>7</b>.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows the form of an exemplary user profile <b>7</b>. There is one such user profile stored in database storage system <b>6</b> for each of a plurality of users of the system. A user is sometimes referred to herein as a &#x201c;patient&#x201d;. Database storage system <b>6</b> also stores a large number of descriptions of possible exercise tasks. Reference numeral <b>8</b> in <figref idref="DRAWINGS">FIG. <b>2</b></figref> identifies one such description of an exercise task. Examples of exercise tasks include exposure treatment tasks, touching a bathroom door handle exercise task, touching a toilet flush handle exercise task, touching a toilet seat exercise task, putting a hand in toilet bowl water exercise task, touching a toilet wall exercise task, touching the bottom of a shoe exercise task, touching raw poultry or hamburger meat exercise task, shaking hands with a stranger exercise task, pressing a button on a vending machine exercise task, handling money exercise task, leaving one's locked house without rechecking the door exercise task, turning off the stove and leaving the room without rechecking exercise task, refraining from phoning one's partner to check that they arrived at work exercise task, and physical exercise tasks such as lifting a barbell exercise task, stretching a stretchable cord exercise task, squeezing a squeezable ball exercise task, taking a walk outdoors exercise task, eating by a certain hour of the day exercise task, a breathing exercise task, a sit-ups exercise task, and a conversation with a friend exercise task. There is one task description for each of the possible tasks.</p><p id="p-0029" num="0028">The remaining steps of the novel method of <figref idref="DRAWINGS">FIGS. <b>2</b>-<b>3</b></figref> are explained with reference to an exemplary exposure treatment to overcome the phobia of touching a toilet flush handle. In step <b>103</b>, a machine learning process <b>103</b> in central server software <b>4</b> maintains and uses the descriptions of tasks as they are stored in database storage system <b>6</b>. A subset of the tasks is selected in step <b>104</b> based on user profile <b>7</b> of user <b>5</b>, task experience data <b>9</b> collected previously for the various tasks, and the descriptions of tasks as they are stored in database storage system <b>6</b>. In step <b>105</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the subset of the tasks is communicated to app <b>3</b> on smartphone <b>1</b>. In step <b>205</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, app <b>3</b> receives the subset of tasks from central server system <b>2</b> and prompts user <b>5</b> to enter Subjective Task Difficulty (STD) information pertaining to each of the tasks of the subset of tasks. This may involve prompting user <b>5</b> via the touchscreen of smartphone <b>1</b> to rank the exercise tasks in order according to the user's belief as to how much stress or effort (struggle) would be involved were user <b>5</b> to performs the tasks. In step <b>207</b>, app <b>3</b> receives this STD information for each task of the subset of tasks. In step <b>208</b>, app <b>3</b> communicates this collected STD information to central server system <b>2</b>. In step <b>106</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the STD information is received by central server system <b>2</b>. In step <b>107</b>, central server system <b>2</b> creates a hierarchy of the tasks in an order that user <b>5</b> can then attempt to perform. In step <b>108</b>, central server system <b>2</b> communicates this task hierarchy back to app <b>3</b>. In step <b>110</b>, central server system <b>2</b> sends a task model and parameters to app <b>3</b> for each task in the hierarchy.</p><p id="p-0030" num="0029">In step <b>209</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, app <b>3</b> receives the task models and parameters. Then app <b>3</b> assigns a first task for user <b>5</b> to perform in step <b>210</b>, and requests user <b>5</b> to start the first task in step <b>211</b>. App <b>3</b> does this, for example, by playing an audio message that is output from the loudspeaker of smartphone <b>1</b>. After the task model and the particular circumstances of the exercise are configured (step <b>212</b>), app <b>3</b> prompts user <b>5</b> to perform a first act of the exercise task in step <b>213</b>. In the case of touching the flush handle of a toilet exercise task, the instruction or prompt may be an audio or verbal instruction to place the smartphone in the user's right hand. The type of voice used (female voice, male voice), the tone of the voice, the pitch of the voice, and the cadence of the message are determined by the parameters that were configured in step <b>212</b>. In step <b>214</b>, app <b>3</b> then uses the sensors and detectors of smartphone <b>1</b> to monitor user <b>5</b> as the user attempts to perform the first act of the assigned task. Physical quantities and occurrences are detected and sensed by smartphone <b>1</b> to make a determination as to a &#x201c;user state&#x201d;. Physical quantities and occurrences are also detected and sensed to make a determination as to a &#x201c;situational state&#x201d;. For example, the detected data is sensed by an accelerometer, a camera, a microphone, or a touchscreen of smartphone <b>1</b>.</p><p id="p-0031" num="0030">Examples of physical quantities and occurrences detected in order to determine &#x201c;user state&#x201d; include: the hand of user <b>5</b> shaking as detected by the accelerometer of smartphone <b>1</b>, the detected amount of time user <b>5</b> spends performing a prompted act, an audio response uttered by user <b>5</b> as detected by the smartphone microphone and detected by voice recognition performed by smartphone <b>1</b>. User <b>5</b> may, for example, be detected to have stated, &#x201c;okay, I have done it,&#x201d; or &#x201c;I can't do this.&#x201d;</p><p id="p-0032" num="0031">Examples of physical quantities and occurrences are detected and sensed to make a determination as to the &#x201c;situational state&#x201d; include detected proximity to objects, scene mapping, detected intensity of light, GPS coordinates, relative detected movement from a previously determined position of the smartphone, detection of other individuals speaking or making noises, and detection of background noises and sounds.</p><p id="p-0033" num="0032">In decision step <b>215</b>, if app <b>3</b> determines that user <b>5</b> has made progress in performing the assigned act of an exercise task, then collected data is stored in step <b>216</b>. This information is temporarily stored on smartphone <b>1</b>. In step <b>217</b>, app <b>3</b> uses the task model to generate feedback to provide to user <b>5</b> and/or to determine how to proceed in prompting user <b>5</b> to continue performing the assigned act of the exercise task. If, for example, user <b>5</b> is struggling to perform the assigned act that user <b>5</b> has been prompted to perform, and user <b>5</b> has not completed the act, the feedback may determine another audio message to be output to user <b>5</b> via the smartphone speaker. Such a message may, for example, be &#x201c;you can do it.&#x201d; The feedback determines the content of the message as well as the voice, tone, pitch and cadence. In this way, the conversation between user <b>5</b> and smartphone app <b>3</b> can be modified and tailored for the particular user so that app <b>3</b> is more comforting, reassuring and soft. Alternatively, app <b>3</b> can be more firm, strong and confident in its communication with user <b>5</b>. App <b>3</b> performs steps <b>212</b>-<b>217</b> of the method repeatedly as user <b>5</b> carries out the various discrete assigned acts of the exercise task. In the example of the exercise task &#x201c;touching toilet flush handle&#x201d;, user <b>5</b> is prompted in step <b>213</b> by audio message to use the user's right hand to take a photograph of user <b>5</b> touching the toilet flush handle with the user's left hand. For example, app <b>3</b> causes a verbal user prompt in the form of an audio message &#x201c;snap the picture&#x2014;you can do it&#x201d; to be output from the loudspeaker of smartphone <b>1</b>. App <b>3</b> places smartphone <b>1</b> in the camera mode so that the pressing of the camera shutter icon by user <b>5</b> causes the image on the screen to be captured as digital image data.</p><p id="p-0034" num="0033">Returning to decision step <b>215</b>, if app <b>3</b> determines that user <b>5</b> has not made progress in carrying out the assigned act of the exercise task, then in step <b>218</b> app <b>3</b> stores the latest task completion information, user state information, and situational state information. This information is temporarily stored on smartphone <b>1</b>. In step <b>219</b>, if app <b>3</b> determines that the exercise task has not been completed, then in step <b>222</b>, app <b>3</b> sends the collected task experience data to the central server system <b>2</b>. One way that app <b>3</b> can determine that the task has not been completed is if the user is prompted in step <b>213</b> to take a photograph of the exercise completion act, and app <b>3</b> detects no such image having been taken. Then user <b>5</b> may be provided with an audio prompt to verbally indicate that user <b>5</b> did not complete the assigned act of the exercise task. Speech recognition functionality of smartphone <b>1</b> (either performed entirely locally on the smartphone, or performed partially on central server system <b>2</b>) is used for this speech recognition purpose. In step <b>219</b>, if app <b>3</b> determines that the exercise completion act has been completed, then app <b>3</b> determines in step <b>220</b> whether the detected user state has a particular desired characteristic. In the case of the assigned act being a physically intensive exercise, the detected user state might be the physical fatigue of user <b>5</b> and whether the exercise completion act took more than a predetermined amount of time to complete. In the case of the exercise task being &#x201c;touching the toilet flush handle&#x201d;, the detected user state might be a detected stress level of user <b>5</b>. In step <b>220</b>, app <b>3</b> determines whether the detected user stress level in carrying out the task was below a predetermined threshold of user stress. If the determined user stress is still above the maximum desired level, then app <b>3</b> prompts user <b>5</b> to perform the same exercise task again until the user's detected stress level drops below the desired threshold. Once app <b>3</b> determines in step <b>220</b> that the user state has the particular desired characteristics, in step <b>221</b> app <b>3</b> sets the current assigned act to be the next exercise task in the task hierarchy. For example, the user state would have the desired characteristics in step <b>220</b> if user <b>5</b> performed the &#x201c;touching the toilet flush handle&#x201d; task, and the user's detected stress level in doing so was below the maximum desired stress level. Then app <b>3</b> would proceed in step <b>221</b> to the next task of the task hierarchy. In step <b>222</b>, app <b>3</b> sends the collected task experience data <b>10</b> to central server system <b>2</b>.</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>5</b></figref> shows the form of exemplary task experience data <b>10</b> collected by app <b>3</b>. In this example, the collected task experience data <b>10</b> is for the &#x201c;touching the toilet flush handle&#x201d; exercise task. The task experience data <b>10</b> includes parameters for the user state, the situational state, the task completion state and experience data.</p><p id="p-0036" num="0035">In step <b>112</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, this task experience data <b>10</b> is received by central server system <b>2</b>. In step <b>112</b>, data <b>10</b> is stored in database storage system <b>6</b>. As more and more instances of task experience data <b>10</b> are collected over time, the machine learning process performed in step <b>113</b> updates the task models and parameters to optimize the various adjustable aspects of the interaction with user <b>5</b> in order to optimize the successful completion of the exercise task by the user. For example, using a deep neural network, the machine learning process generates the content of the verbal user prompts to include a level of reassurance most likely to motivate the user to proceed with the next step of the exercise task. Each set of task experience data includes an indication of the satisfactory/unsatisfactory nature of the exercise performance, along with a number of parameters that indicate what occurred and what was detected during the exercise.</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flowchart of another embodiment of the novel method <b>300</b> for administering an exposure treatment that is directed to the exemplary exercise task of touching a toilet flush handle. Method <b>300</b> is performed using the system of <figref idref="DRAWINGS">FIG. <b>1</b></figref> that includes smartphone <b>1</b> and central server <b>2</b>. This embodiment of method <b>300</b> begins at step <b>301</b>, in which app <b>3</b> prompts user <b>5</b> to perform a task that is part of the exposure treatment. In this example, app <b>3</b> generates an audio prompt requesting user <b>5</b> to touch the flush handle of a toilet with the left hand while making a video recording of the act using smartphone <b>1</b> in the right hand. <figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates the video image that user <b>5</b> is recording with smartphone <b>1</b> while touching the flush handle with the user's finger. In this example, the verbal prompt is, &#x201c;touch the handle&#x2014;you can do it!&#x201d; The tone, pitch, cadence and content of the verbal prompt is selected based on the personal information and phobia description entered by user <b>5</b> as matched by machine learning to the experience of other users with similar characteristics. In step <b>302</b>, app <b>3</b> monitors user <b>5</b> using the sensors and detectors of smartphone <b>1</b> as user <b>5</b> attempts to perform the first step of the assigned task. Sensor signals are received from an accelerometer, a touchscreen, a microphone and a video camera. The motions of user <b>5</b> are detected indirectly through the movements of smartphone <b>1</b> as measured by an accelerometer in the phone. In addition, the touch interactions of user <b>5</b> with smartphone <b>1</b> are detected by a contact intensity sensor of the touchscreen, such as a piezoelectric force sensor, a capacitive force sensor, an electric force sensor or an optical force sensor. Touch interaction data includes contact with the touchscreen (a fingerdown event), the pressure of the contact, the size of the contact, movement of the contact across the touchscreen, and the end of the contact (a finger-up event).</p><p id="p-0038" num="0037">In step <b>303</b>, the data from the sensor signals is used to calculate &#x201c;user state&#x201d; parameters of user <b>5</b> during the task, such as stress level, struggle level, heart rate, breathing rate, and whether user <b>5</b> is seeking reassurance or is following rituals. Heart rate and breathing rate can be determined using accelerometer motion data. The user's breathing movement and beating heart are sensed by the accelerometer. App <b>3</b> uses the touch interaction data and the motion data to determine the current stress level of user <b>5</b>. The user's struggle level with the current task is determined based on the motion data, the heart rate, the breathing rate and the audio input from the microphone, as well as the amount of time user <b>5</b> has spent performing the assigned task. For example, the audio input could be the voice recognition of the user's statement, &#x201c;I can't do this.&#x201d;</p><p id="p-0039" num="0038">In step <b>304</b>, the data from the sensor signals is used to calculate &#x201c;situational state&#x201d; parameters of user <b>5</b> during the task. For the exemplary exercise task of touching a toilet flush handle, these &#x201c;situational state&#x201d; parameters include: distance of user <b>5</b> to toilet, distance of user's hand to toilet flush handle, angle between user's face and toilet flush handle, number of objects between user <b>5</b> and toilet, number of people in room, light intensity in the room, and whether user <b>5</b> washed his or her hands. For example, the audio input is used to determine the number of people in the room by detecting other individuals speaking or making noises. The video camera input is used to determine the distance of the user's hand to the toilet flush handle.</p><p id="p-0040" num="0039">In step <b>305</b>, app <b>3</b> determines the task completion state parameters based on the other parameters and on the data from the sensor signals. Examples of task completion state parameters are the time the user's hand was touching the toilet flush handle, the time elapsed after touching the handle before user <b>5</b> washed his or her hands, and the percentage of task steps completed.</p><p id="p-0041" num="0040">In step <b>306</b>, app <b>3</b> generates a real-time verbal user prompt based on the user's current stress level and struggle level, on the intensity of the current step of the exposure treatment task, and on the user's progress achieved in the current step. The flowchart of <figref idref="DRAWINGS">FIG. <b>6</b></figref> shows the decision steps <b>307</b>-<b>316</b> that app <b>3</b> performs to generate the most appropriate verbal prompt based on the user state, the situational state during the current step of the task, and the task completion state. The most appropriate verbal prompt is the prompt that conveys a level of reassurance most likely to motivate the user to proceed with the next step of the exercise task.</p><p id="p-0042" num="0041">In decision step <b>307</b>, app <b>3</b> determines the progress user <b>5</b> has achieved in performing the current step of the exposure treatment task. In this example, app <b>3</b> determines whether the time user <b>5</b> spent touching the toilet flush handle exceeded a minimum time threshold, for example, one second in the first task step. App <b>3</b> determines the user's progress in performing the task based on the video camera input and the other sensor signals. If user <b>5</b> achieved the minimum time threshold, the decision flow proceeds to decision step <b>308</b>; otherwise the decision flow proceeds to decision step <b>309</b>.</p><p id="p-0043" num="0042">In decision step <b>308</b>, app <b>3</b> determines whether user <b>5</b> abstained from washing his or her hands after touching the toilet handle for a minimum time threshold, for example, thirty second in the first task step. If user <b>5</b> achieved the minimum time threshold, the decision flow proceeds to step <b>310</b>; otherwise the decision flow proceeds to decision step <b>311</b>. If user <b>5</b> refrained from washing his hands for the minimum time threshold associated with the first task step, then the first task step is completed in step <b>310</b>, and a prompt is generated in step <b>306</b>. After user <b>5</b> completes the first task step in step <b>310</b>, the user is given an audio prompt such as, &#x201c;Well done, you have completed the task. You are now ready to take on a more complicated version of the task.&#x201d;</p><p id="p-0044" num="0043">Depending on the user stress level and struggle level and the intensity level of the first task step, app <b>3</b> generates a user prompt that instructs user <b>5</b> to proceed to a subsequent task step that has a greater intensity level. The intensity of the subsequent task step is selected to achieve the fastest completion of the exposure therapy attainable by user <b>5</b> based on both input from machine learning on the knowledge base <b>317</b> and mobile app intelligence analyzing the user and situational states.</p><p id="p-0045" num="0044">When app <b>3</b> detects the user state parameters, the situational state parameters, and the task completion state parameters in steps <b>303</b>-<b>305</b>, those parameters are transmitted to database storage system <b>6</b> and stored in knowledge base <b>317</b>. In addition, knowledge base <b>317</b> includes parameters collected from cognitive behavioral therapies undertaken by other users. In step <b>318</b>, machine learning is performed on knowledge base <b>317</b> in order to identify the characteristics of the subsequent task step that best match the capabilities of user <b>5</b> to complete the exposure therapy in the shortest time. In one embodiment, the machine learning is performed using a deep neural network. For example, the machine learning in step <b>318</b> might determine that user <b>5</b> is able to skip an intensity level so as to complete the overall exposure therapy in a shorter time. In step <b>319</b>, machine learning and mobile app intelligence configure the characteristics of the subsequent task step. Examples of task configuration characteristics include task intensity, minimum time for the user's hand to touch the toilet flush handle, minimum time for the user not to wash hands after touching toilet handle, struggle level threshold to decrease intensity of next task step, struggle level threshold to stop task, maximum permitted stress level to consider task completed, and total number of task repetition steps.</p><p id="p-0046" num="0045">The user prompt generated in step <b>306</b> after the task step is completed in step <b>310</b> includes both positive feedback regarding the current task step as well as encouragement and reassurance regarding the next task step, which is described to user <b>5</b> based on the task configuration performed in step <b>319</b>.</p><p id="p-0047" num="0046">Returning to the decision step <b>308</b>, if user <b>5</b> is unable to refrain for the minimum time threshold from washing his or her hands, the decision flow proceeds to step <b>311</b>, where app <b>3</b> determines whether user <b>5</b> immediately washed his or her hands after touching the toilet handle. If at least some time elapsed after touching the toilet handle and before washing hands, the decision flow proceeds to step <b>312</b>; otherwise the decision flow proceeds to decision step <b>309</b>. If user <b>5</b> waited some time between touching the handle and washing hands, then in step <b>312</b> the configuration of the next task step is maintained with the current configuration. A prompt is generated in step <b>306</b> encouraging user <b>5</b> to attempt the assigned task again with the current configuration. The tone of the prompt voice, the pitch of the prompt voice, and the cadence of the prompt message are determined using machine learning on the knowledge base <b>317</b> and mobile app intelligence analyzing the user state parameters and situational state parameters.</p><p id="p-0048" num="0047">If in decision step <b>311</b>, no time elapsed after touching the toilet handle and washing hands, the decision flow proceeds to step <b>309</b>. The decision flow also proceeds to step <b>309</b> from step <b>307</b> if user <b>5</b> did not touch the toilet flush handle for at least the minimum time threshold. In decision step <b>309</b>, app <b>3</b> determines whether the struggle level of user <b>5</b> during the first task step did not exceed the maximum struggle level threshold for decreasing the task intensity of the next task step. If the struggle level of user <b>5</b> during the first task step is less than (within) the maximum allowed struggle level threshold for decreasing the task intensity, then the decision flow proceeds to step <b>312</b>, and the configuration of the next task step is maintained with the current configuration and intensity. If, however, the struggle level of user <b>5</b> during the first task step exceeds the maximum allowed struggle level threshold for decreasing the task intensity, then the decision flow proceeds to step <b>313</b>.</p><p id="p-0049" num="0048">In decision step <b>313</b>, app <b>3</b> determines whether the struggle level of user <b>5</b> during the first task step both exceeds the maximum struggle level threshold for decreasing the task intensity and is less than the maximum struggle level threshold for stopping the exercise task. If the struggle level of user <b>5</b> during the first task step is between the struggle level threshold for decreasing the task intensity and the struggle level threshold for stopping the exercise, then the decision flow proceeds to step <b>314</b>; otherwise the task is stopped in step <b>315</b>.</p><p id="p-0050" num="0049">In decision step <b>314</b>, app <b>3</b> determines whether the task intensity level of the current task step is set at the lowest task intensity level. If the task intensity level of the current task step is already set at the lowest task intensity level, then the task is stopped in step <b>315</b>. If, however, the task intensity level of the current task step is not set at the lowest task intensity level, the decision flow proceeds to step <b>316</b>, in which the configuration of the next task step is modified to run with a lower task intensity level.</p><p id="p-0051" num="0050">In addition, a prompt is generated in step <b>306</b> after both step <b>315</b> and step <b>316</b>. If user <b>5</b> has stopped trying to completed the task in step <b>315</b>, then the voice, tone, pitch, cadence and content of the audio prompt will be composed to motivate the user not to give up on the exposure therapy altogether. The audio prompt would be the most comforting, reassuring and soft. An example of the prompt content is, &#x201c;Let's pause for now and continue another time. You showed improvement and are closer to completing the task.&#x201d; If user <b>5</b> will be prompted to repeat the task at a lower intensity level in step <b>316</b>, an example of the prompt content would be, &#x201c;Continue, focus on the task and don't look away. Feel the discomfort, but keep going for five more seconds.&#x201d; The tone of the prompt after step <b>316</b> would be somewhat firmer and confident than the tone of the prompt after step <b>315</b>.</p><p id="p-0052" num="0051">Machine Learning: As described above, the central server <b>2</b> receives many different task experience data sets from numerous smartphones used by many different users including smartphone <b>1</b>, and performs machine learning using that data to optimize parameter values of a task model. In one example, the task model is a decision tree for a particular exercise. A parameter of that decision tree may, for example, have a threshold value. By machine learning, this threshold value is adjusted. After adjustment, the entire task model complete with all its parameter values (including the adjusted threshold value) is returned to the smartphone <b>1</b>. The next time the user <b>5</b> uses smartphone <b>1</b>, the new adjusted threshold value will be used in the decision tree to determine what next step in the decision tree to perform or proceed to. Each task experience data set includes task completion state information.</p><p id="p-0053" num="0052">In order to determine the values of parameters of the decision tree (A), the machine learning method estimates the likelihood of successful task completion given the contextual information about the user and determined the parameter values as a function of the likelihood. This contextual information includes the user profile, the subjective task difficulty (STD) introduced by the user and information from previous &#x201c;task experiences&#x201d; (TE) the user had (e.g. number of times the user tried the exercise before, percentage of completion, minimum stress level in the tasks completed, stress level at the end of the last task trial, etc.). The contextual information is arranged as a numerical vector (S). The machine learning method (M) estimates the likelihood given S and a policy (p) as a function of the estimated likelihood can be used to make the decision. In one example, a contextual bandit method is used to determine the parameter values: Each time (t) that the central server <b>2</b> needs to select the parameter values for a user (u), the contextual information is computed as a function of the previous task experiences S<sub>t,u</sub>=f(TE<sub>1,u</sub>, . . . , TE<sub>t-1,u</sub>). Given S<sub>t,u</sub>&#x2208;R<sup>n</sup>, the central server <b>2</b> determines the decision parameter values for the decision tree A<sub>t,u</sub>&#x2208;R<sup>m </sup>following a certain policy &#x3bc;(S<sub>t,u</sub>) (vector of probabilities of choosing each of the decision parameter values given S<sub>t,u</sub>). Once the user finishes the task, the new TE<sub>t,u </sub>is received by the central server <b>2</b>, which contains the result R<sub>t,u</sub>=1 if the task was successfully completed or R<sub>t,u</sub>=0 otherwise. The central server <b>2</b> uses the collected tuples {(S<sub>1,u</sub>, A<sub>1,u</sub>, R<sub>1,u</sub>), . . . , (S<sub>t-1,u</sub>, A<sub>t-1,u</sub>, R<sub>t-1,u</sub>)}<sub>u </sub>of all users u to determine the result R<sub>t-1,u </sub>as a function of the contextual information S<sub>t-1,u </sub>and the decision parameters A<sub>t-1</sub>, u:R<sub>t-1,u</sub>=g<sub>&#x3b8;</sub>(S<sub>t-1,u</sub>, A<sub>t-1,u</sub>). This function g<sub>&#x3b8; </sub>is a machine learning model, e.g. a logistic regression or a feedforward neural network parameterized by the parameter vector &#x3b8;. The machine learning model is trained in all the collected tuples using the vector S<sub>t-1,u </sub>A<sub>t-1,u </sub>as features and R<sub>t-1,u </sub>as target for all users u, and time t to estimate the optimal parameter vector &#x3b8;*. Then, when the parameter values of the decision tree need to be determined, the central server <b>2</b> uses the trained model (get) to estimate the result R<sub>t,u </sub>given S<sub>t,u </sub>for each possible value of the parameters of the decision tree. The central server <b>2</b> determines the updated model and parameter values according to a policy p. In one example, the policy p is an E-greedy policy (epsilon-greedy reinforcement learning policy) that selects the decision tree parameter values with the highest estimated R<sub>t,u </sub>argmax<sub>At,u </sub>g<sub>&#x3b8;</sub>*(S<sub>t,u</sub>, A<sub>t,u</sub>) with probability (1&#x2212;&#x3b5;) and any other decision tree parameter value randomly with probability &#x3b5;. Once determined, the updated model including its parameter values is sent to the smartphone <b>1</b>.</p><p id="p-0054" num="0053">Examples of methods usable to determine user state include: 1) An accelerometer-based method such as the stress detection method described in U.S. patent application Ser. No. 17/227,308, by Joao Guerreiro, entitled &#x201c;Determining A Stress Level Of A Smartphone User Based On The User's Touch Interactions And Motion Sensor Data&#x201d;, filed Apr. 10, 2021 (the entire subject matter of which is incorporated herein by reference). 2) A camera-based method such as is described in &#x201c;Instant Stress: Detection of Perceived Mental Stress Through Smartphone Photoplethysmography And Thermal Imaging&#x201d;, by Youngjun Cho et al., JMIR Mental Health, Vol. 6, No. 4 (2019), or as explained in &#x201c;VitaMon: Measuring Heart Rate Variability Using Smartphone Front Camera&#x201d;, by Sinh Huynh, et al., SenSys 2019: Proceedings of the 17th Conference On Embedded Networked Sensor Systems, New York, Nov. 10-13, 2019, pages 1-14. Examples of methods usable to determine situational state include: 1) &#x201c;Learning To Detect Human-Object Interactions&#x201d;, by Yu-Wei Chao, et al., IEEE Winter Conference on Applications Of Computer Vision (WACV), pages 381-389 (2018). 2) &#x201c;PPDM: Parallel Point Detection And Matching For Real-Time Human-Object Interaction Detection&#x201d;, Yue Liao, et al., Proceedings of the IEEE/CVF Conference On Computer Vision and Pattern Recognition (CVPR), pages 482-490 (2020).</p><p id="p-0055" num="0054">Although certain specific embodiments are described above for instructional purposes, the teachings of this patent document have general applicability and are not limited to the specific embodiments described above. For example, the same methodology for encouraging a user to continue with an exposure therapy task can be used to motivate a user to continue with physical exercise training, such as weight training with ever more repetitions and/or weight. Accordingly, various modifications, adaptations, and combinations of various features of the described embodiments can be practiced without departing from the scope of the invention as set forth in the claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method, comprising:<claim-text>detecting a user state of a patient who is currently undergoing a first step of a cognitive behavioral therapy, wherein the user state is based on the patient's behavior and physiological condition during the first step of the cognitive behavioral therapy as detected by sensors of a smartphone used by the patient;</claim-text><claim-text>detecting a situational state of the patient's surroundings while the patient is undergoing the first step of the cognitive behavioral therapy, wherein the situational state is detected by sensors of the smartphone used by the patient;</claim-text><claim-text>determining whether the patient has performed the first step in a manner that achieves progress in the cognitive behavioral therapy;</claim-text><claim-text>generating a verbal user prompt based on the detected user state and the detected situational state during the first step, wherein the verbal user prompt is output by a loudspeaker of the smartphone; and</claim-text><claim-text>configuring a next step of the cognitive behavioral therapy based on the detected user state and the detected situational state during the first step, wherein the verbal user prompt has a content and character generated using machine learning based on past task completions by the patient and other users to render more likely that the patient will complete the next step of the cognitive behavioral therapy.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the detecting the user state and the detecting the situational state is performed by a mobile app running on the smartphone.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the next step of the cognitive behavioral therapy is configured using machine learning based on the past task completions by the patient and other users so as to minimize how many steps are required for the patient to complete the cognitive behavioral therapy.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the verbal user prompt is characterized by parameters selected from the group consisting of: a tone of voice, a pitch of voice, and a cadence of the verbal user prompt.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the machine learning that generates the verbal user prompt is performed by a deep neural network.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the machine learning generates the content of the verbal user prompt to include a level of reassurance most likely to motivate the patient to proceed with the next step of the cognitive behavioral therapy.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the cognitive behavioral therapy involves a controlled exposure of the patient to an object that causes the patient to experience anxiety.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the determining whether the patient achieves progress in the cognitive behavioral therapy is based on a stress level of the patient during the first step and a struggle level of the patient during the first step.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the stress level of the patient during the first step is determined based on touch interaction data and motion data detected by sensors on the smartphone during the first step.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. A system comprising:<claim-text>a smartphone with a device memory, wherein instructions of a mobile application are stored in the device memory;</claim-text><claim-text>a central server with server memory, wherein instructions of a server application are stored in the server memory; and</claim-text><claim-text>a database storage system, wherein the instructions of the mobile application when executed cause the smartphone to (a) detect a user state of a patient who is currently undergoing a first step of a cognitive behavioral therapy (CBT), wherein the user state is based on the patient's behavior and physiological condition during the first step of the CBT as detected by sensors of the smartphone, (b) detect a situational state of the patient's surroundings while the patient is undergoing the first step of the CBT, wherein the situational state is detected by the sensors of the smartphone, (c) determine whether the patient has performed the first step in a manner that achieves progress in the CBT, and (d) output a user prompt based on the detected user state and the detected situational state during the first step, wherein the instructions of the server application when executed cause the central server to (e) generate the user prompt using machine learning based on past therapy steps by the patient and other users of the mobile application to have a content and character adapted to influence the patient to complete a next step of the CBT, and (f) configure the next step of the CBT based on the detected user state and the detected situational state during the first step, and wherein data regarding the past therapy steps by the patient and other users is stored in the database storage system.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the CBT involves a controlled exposure of the patient to an object that causes the patient to experience anxiety.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the mobile application determines whether the patient has achieved progress in the CBT based on a stress level of the patient during the first step.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the stress level of the patient during the first step is determined based on touch interaction data and motion data detected by the sensors of the smartphone during the first step.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The system of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the touch interaction data is sensed by a touchscreen of the smartphone, and wherein the motion data is sensed by an accelerometer of the smartphone.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the user prompt is an audio user prompt output by a loudspeaker of the smartphone, and wherein the content and character of the user prompt is adapted by varying parameters selected from the group consisting of: a tone of voice, a pitch of voice, and a cadence of the user prompt.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. A system for administering a cognitive behavioral therapy, comprising:<claim-text>a smartphone with a mobile device memory, wherein instructions of a mobile application are stored in the mobile device memory; and</claim-text><claim-text>a central server with server memory, wherein instructions of a server application are stored in the server memory, wherein the instructions of the mobile application when executed cause the smartphone to<claim-text>(a) detect a user state of a patient who is currently undergoing a first step of an exposure therapy, wherein the user state is based on the patient's condition during the first step of the exposure therapy as detected by sensors of the smartphone,</claim-text><claim-text>(b) detect a situational state of the patient's surroundings while the patient is undergoing the first step of the exposure therapy, wherein the situational state is detected by the sensors of the smartphone,</claim-text><claim-text>(c) determine whether the patient has achieved progress in performing the first step in the exposure therapy,</claim-text><claim-text>(d) output an audio user prompt based on the detected user state and the detected situational state during the first step, wherein the audio user prompt is output by a loudspeaker of the smartphone, wherein the instructions of the server application when executed cause the central server to</claim-text><claim-text>(e) generate the audio user prompt using machine learning based on past therapy steps by the patient and other users to have a content and character adapted to influence the patient to complete a next step of the exposure therapy, and</claim-text><claim-text>(f) configure the next step of the exposure therapy based on the detected user state and the detected situational state during the first step.</claim-text></claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The system of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the next step of the exposure therapy is configured using machine learning based on the past therapy steps by the patient and other users so as to minimize how many steps are required for the patient to complete the exposure therapy.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The system of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the audio user prompt is characterized by parameters selected from the group consisting of: a tone of voice, a pitch of voice, and a cadence of the audio user prompt.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The system of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the machine learning that generates the audio user prompt is performed by a deep neural network.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The system of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the mobile application determines whether the patient has achieved progress in performing the first step of the exposure therapy based on a stress level of the patient during the first step and a struggle level of the patient during the first step.</claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. The system of <claim-ref idref="CLM-00020">claim 20</claim-ref>, wherein the stress level of the patient during the first step is determined based on touch interaction data and motion data detected by the sensors of the smartphone during the first step.</claim-text></claim></claims></us-patent-application>