<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004813A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004813</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17943176</doc-number><date>20220912</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>08</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>082</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">JOINTLY PRUNING AND QUANTIZING DEEP NEURAL NETWORKS</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16396619</doc-number><date>20190426</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11475308</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17943176</doc-number></document-id></child-doc></relation></continuation><us-provisional-application><document-id><country>US</country><doc-number>62819484</doc-number><date>20190315</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Samsung Electronics Co., Ltd.</orgname><address><city>Suwon-si</city><country>KR</country></address></addressbook><residence><country>KR</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>GEORGIADIS</last-name><first-name>Georgios</first-name><address><city>Porter Ranch</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>DENG</last-name><first-name>Weiran</first-name><address><city>Woodland Hills</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A system and a method generate a neural network that includes at least one layer having weights and output feature maps that have been jointly pruned and quantized. The weights of the layer are pruned using an analytic threshold function. Each weight remaining after pruning is quantized based on a weighted average of a quantization and dequantization of the weight for all quantization levels to form quantized weights for the layer. Output feature maps of the layer are generated based on the quantized weights of the layer. Each output feature map of the layer is quantized based on a weighted average of a quantization and dequantization of the output feature map for all quantization levels. Parameters of the analytic threshold function, the weighted average of all quantization levels of the weights and the weighted average of each output feature map of the layer are updated using a cost function.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="113.45mm" wi="150.28mm" file="US20230004813A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="140.12mm" wi="152.32mm" file="US20230004813A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="204.81mm" wi="115.82mm" orientation="landscape" file="US20230004813A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="222.08mm" wi="175.34mm" orientation="landscape" file="US20230004813A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="211.92mm" wi="98.98mm" file="US20230004813A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="240.71mm" wi="161.12mm" orientation="landscape" file="US20230004813A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="218.27mm" wi="123.78mm" file="US20230004813A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="203.88mm" wi="161.46mm" orientation="landscape" file="US20230004813A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION</heading><p id="p-0002" num="0001">This application is a continuation of U.S. patent application Ser. No. 16/396,619, filed on Apr. 26, 2019, which claims the priority benefit under 35 U.S.C. &#xa7; 119(e) of U.S. Provisional Patent Application Ser. No. 62/819,484 filed on Mar. 15, 2019, the disclosures of which are incorporated herein by reference in their entirety.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">The subject matter disclosed herein relates to neural networks. More specifically, the subject matter disclosed herein relates to a system and a method that jointly prunes the weights and quantizes the weights and output feature maps of a layer of a neural network.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">The weights and output feature maps of the activation functions occupy a huge amount of memory during an inference phase of a deep neural network (DNN). Neural networks also use a huge amount of operations to compute the output inference. Pruning of the weights and quantization of the weights and output feature maps of the activation functions may reduce the memory requirements and amount of computation of neural networks.</p><p id="p-0005" num="0004">A conventional approach to pruning and quantizing a neural network is to first prune and then quantize the neural network as separate independent operations. See, for example, S. Han et. al., &#x201c;Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding,&#x201d; arxiv.org/1510.00149). The effect of the separate operations disclosed by Han et al. is that the quantization portion receives a pruned network, which makes optimizing of the quantization parameters harder. In effect, the pruning shrinks the allowable state-space of the network by only considering one available pruned model of the neural network.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0006" num="0005">An example embodiment provides a neural network that may include a plurality of layers in which at least one layer may include jointly pruned and quantized weights and output feature maps. The jointly pruned weights may be pruned using an analytic threshold function. Each weight remaining after being pruned may further be quantized based on a weighted average of a quantization and dequantization of the weight for all quantization levels. The output feature maps may be formed based on the pruned and quantized weights of the layer. Each output feature map may be quantized based on a weighted average of a quantization and dequantization of the output feature map for all quantization levels, and parameters of the analytic threshold function. The weighted average of all quantization levels of the weights and the weighted average of each output feature map of the layer may be updated based on a cost function. In one embodiment, the neural network is a full-precision trained neural network before the weights and the output feature maps of the at least one layer are jointly pruned and quantized. In another embodiment, the cost function includes a pruning loss term, a weight quantization loss term and a feature map quantization loss term.</p><p id="p-0007" num="0006">An example embodiment provides a method to prune weights and output feature maps of a layer of a neural network that may include pruning weights of a layer of a neural network using an analytic threshold function, the neural network being a trained neural network; quantizing each weight of the layer remaining after pruning based on a weighted average of a quantization and dequantization of the weight for all quantization levels to form quantized weights for the layer; determining output feature maps of the layer based on the quantized weights of the layer; quantizing each output feature map of the layer based on a weighted average of a quantization and dequantization of the output feature map for all quantization levels; and updating parameters of the analytic threshold function, the weighted average of all quantization levels of the weights and the weighted average of each output feature map of the layer using a cost function. In one embodiment, updating the parameters of the analytic threshold function, the weighted average of all quantization levels of the weights and the weighted average of each output feature map of the layer further comprises optimizing the cost function.</p><p id="p-0008" num="0007">An example embodiment provides a neural network analyzer that may include an interface and a processing device. The interface may receive a neural network that may include a plurality of layers. The processing device may generate a neural network that may include at least one layer having weights and output feature maps that have been jointly pruned and quantized. The processing device may prune the weights of the at least one layer of the neural network using an analytic threshold function, quantize each weight of the at least one layer remaining after pruning based on a weighted average of a quantization and dequantization of the weight for all quantization levels to form quantized weights for the at least one layer, determine output feature maps of the at least one layer based on the quantized weights of the at least one layer, quantize each output feature map of the at least one layer based on a weighted average of a quantization and dequantization of the output feature map for all quantization levels, and update parameters of the analytic threshold function, the weighted average of all quantization levels of the weights and the weighted average of each output feature map of the at least one layer using a cost function. In one embodiment, the interface further outputs the neural network that includes at least one layer having weights and output feature maps that have been jointly pruned and quantized.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWING</heading><p id="p-0009" num="0008">In the following section, the aspects of the subject matter disclosed herein will be described with reference to exemplary embodiments illustrated in the figure, in which:</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>1</b></figref> depicts a block diagram of a neural network analyzer that may perform a joint pruning and quantization operation on the layers of a deep neural network (DNN) according to the subject matter disclosed herein;</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>2</b></figref> depicts an example architecture of a DNN that may be input to the neural network analyzer of <figref idref="DRAWINGS">FIG. <b>1</b></figref> as a full-precision trained neural network;</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>3</b></figref> depicts an example layer in a DNN and application of an analytic threshold function to form an analytic weight function &#x192; (w) for optimally pruning the weights of the example layer according to the subject matter disclosed herein;</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>4</b>A</figref> depicts a graph of the example threshold function h(w) of Eq. (1) according to the subject matter disclosed herein;</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>4</b>B</figref> depicts a graph of the weight function &#x192; (w) of Eq. (2) according to the subject matter disclosed herein;</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIGS. <b>5</b>A-<b>5</b>E</figref> depict five example graphs of the threshold function h(w) of Eq. (1) with different values for the parameters &#x3b1; and &#x3b2; to provide a visual sense for how the parameters &#x3b1; and &#x3b2; may affect the threshold function h(w) and the weight function &#x192;(w) according to the subject matter disclosed herein;</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flow chart of an example embodiment of a method for performing a joint pruning and quantization operation on the layers of a neural network, such as a DNN, according to the subject matter disclosed herein; and</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>7</b></figref> depicts a block diagram of an exemplary architecture for a data processing system that may be used to implement neural network analyzer of <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0018" num="0017">In the following detailed description, numerous specific details are set forth in order to provide a thorough understanding of the disclosure. It will be understood, however, by those skilled in the art that the disclosed aspects may be practiced without these specific details. In other instances, well-known methods, procedures, components and circuits have not been described in detail not to obscure the subject matter disclosed herein.</p><p id="p-0019" num="0018">Reference throughout this specification to &#x201c;one embodiment&#x201d; or &#x201c;an embodiment&#x201d; means that a particular feature, structure, or characteristic described in connection with the embodiment may be included in at least one embodiment disclosed herein. Thus, the appearances of the phrases &#x201c;in one embodiment&#x201d; or &#x201c;in an embodiment&#x201d; or &#x201c;according to one embodiment&#x201d; (or other phrases having similar import) in various places throughout this specification may not be necessarily all referring to the same embodiment. Furthermore, the particular features, structures or characteristics may be combined in any suitable manner in one or more embodiments. In this regard, as used herein, the word &#x201c;exemplary&#x201d; means &#x201c;serving as an example, instance, or illustration.&#x201d; Any embodiment described herein as &#x201c;exemplary&#x201d; is not to be construed as necessarily preferred or advantageous over other embodiments. Additionally, the particular features, structures, or characteristics may be combined in any suitable manner in one or more embodiments. Also, depending on the context of discussion herein, a singular term may include the corresponding plural forms and a plural term may include the corresponding singular form. Similarly, a hyphenated term (e.g., &#x201c;two-dimensional,&#x201d; &#x201c;pre-determined,&#x201d; &#x201c;pixel-specific,&#x201d; etc.) may be occasionally interchangeably used with a corresponding non-hyphenated version (e.g., &#x201c;two dimensional,&#x201d; &#x201c;predetermined,&#x201d; &#x201c;pixel specific,&#x201d; etc.), and a capitalized entry (e.g., &#x201c;Counter Clock,&#x201d; &#x201c;Row Select,&#x201d; &#x201c;PIXOUT,&#x201d; etc.) may be interchangeably used with a corresponding non-capitalized version (e.g., &#x201c;counter clock,&#x201d; &#x201c;row select,&#x201d; &#x201c;pixout,&#x201d; etc.). Such occasional interchangeable uses shall not be considered inconsistent with each other.</p><p id="p-0020" num="0019">Also, depending on the context of discussion herein, a singular term may include the corresponding plural forms and a plural term may include the corresponding singular form. It is further noted that various figures (including component diagrams) shown and discussed herein are for illustrative purpose only, and are not drawn to scale. Similarly, various waveforms and timing diagrams are shown for illustrative purpose only. For example, the dimensions of some of the elements may be exaggerated relative to other elements for clarity. Further, if considered appropriate, reference numerals have been repeated among the figures to indicate corresponding and/or analogous elements.</p><p id="p-0021" num="0020">The terminology used herein is for the purpose of describing some example embodiments only and is not intended to be limiting of the claimed subject matter. As used herein, the singular forms &#x201c;a,&#x201d; &#x201c;an&#x201d; and &#x201c;the&#x201d; are intended to include the plural forms as well, unless the context clearly indicates otherwise. It will be further understood that the terms &#x201c;comprises&#x201d; and/or &#x201c;comprising,&#x201d; when used in this specification, specify the presence of stated features, integers, steps, operations, elements, and/or components, but do not preclude the presence or addition of one or more other features, integers, steps, operations, elements, components, and/or groups thereof. The terms &#x201c;first,&#x201d; &#x201c;second,&#x201d; etc., as used herein, are used as labels for nouns that they precede, and do not imply any type of ordering (e.g., spatial, temporal, logical, etc.) unless explicitly defined as such. Furthermore, the same reference numerals may be used across two or more figures to refer to parts, components, blocks, circuits, units, or modules having the same or similar functionality. Such usage is, however, for simplicity of illustration and ease of discussion only; it does not imply that the construction or architectural details of such components or units are the same across all embodiments or such commonly-referenced parts/modules are the only way to implement some of the example embodiments disclosed herein.</p><p id="p-0022" num="0021">It will be understood that when an element or layer is referred to as being on, &#x201c;connected to&#x201d; or &#x201c;coupled to&#x201d; another element or layer, it can be directly on, connected or coupled to the other element or layer or intervening elements or layers may be present. In contrast, when an element is referred to as being &#x201c;directly on,&#x201d; &#x201c;directly connected to&#x201d; or &#x201c;directly coupled to&#x201d; another element or layer, there are no intervening elements or layers present. Like numerals refer to like elements throughout. As used herein, the term &#x201c;and/or&#x201d; includes any and all combinations of one or more of the associated listed items.</p><p id="p-0023" num="0022">The terms &#x201c;first,&#x201d; &#x201c;second,&#x201d; etc., as used herein, are used as labels for nouns that they precede, and do not imply any type of ordering (e.g., spatial, temporal, logical, etc.) unless explicitly defined as such. Furthermore, the same reference numerals may be used across two or more figures to refer to parts, components, blocks, circuits, units, or modules having the same or similar functionality. Such usage is, however, for simplicity of illustration and ease of discussion only; it does not imply that the construction or architectural details of such components or units are the same across all embodiments or such commonly-referenced parts/modules are the only way to implement some of the example embodiments disclosed herein.</p><p id="p-0024" num="0023">Unless otherwise defined, all terms (including technical and scientific terms) used herein have the same meaning as commonly understood by one of ordinary skill in the art to which this subject matter belongs. It will be further understood that terms, such as those defined in commonly used dictionaries, should be interpreted as having a meaning that is consistent with their meaning in the context of the relevant art and will not be interpreted in an idealized or overly formal sense unless expressly so defined herein.</p><p id="p-0025" num="0024">As used herein, the term &#x201c;module&#x201d; refers to any combination of software, firmware and/or hardware configured to provide the functionality described herein in connection with a module. The software may be embodied as a software package, code and/or instruction set or instructions, and the term &#x201c;hardware,&#x201d; as used in any implementation described herein, may include, for example, singly or in any combination, hardwired circuitry, programmable circuitry, state machine circuitry, and/or firmware that stores instructions executed by programmable circuitry. The modules may, collectively or individually, be embodied as circuitry that forms part of a larger system, for example, but not limited to, an integrated circuit (IC), system on-chip (SoC) and so forth. The various components and/or functional blocks disclosed herein may be embodied as modules that may include software, firmware and/or hardware that provide functionality described herein in connection with the various components and/or functional blocks.</p><p id="p-0026" num="0025">The subject matter disclosed herein provide joint pruning and quantization that allows the optimization of a neural network to select the pruned model that is best for quantization and at the same time, select the quantized model that is best for pruning. Accordingly, the subject matter disclosed herein allows a much larger set of allowable solutions to be considered, thereby providing better pruning and quantization results.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>1</b></figref> depicts a block diagram of a neural network analyzer <b>100</b> that may perform a joint pruning and quantization operation on the layers of a deep neural network (DNN) according to the subject matter disclosed herein. In one example embodiment, the neural network analyzer <b>100</b> may be implemented as a data-processing system executing suitable software. As depicted, the neural network analyzer <b>100</b> may receive a full-precision trained neural network <b>101</b> as an input. The neural network analyzer <b>100</b> may operate on the layers of the neural network <b>101</b> by performing a joint pruning and quantization operation <b>104</b> on the weights and the output feature maps of each respective layer of the neural network <b>101</b>. The neural network analyzer <b>100</b> outputs a neural network <b>102</b> in which the layers of the neural network have been jointly pruned and quantized.</p><p id="p-0028" num="0027">The joint pruning and quantization operation <b>104</b> that is performed on each layer may include multiple forward and backward passes, or iterations, that fine tune the pruned and quantized weights and output feature maps for each respective layer of a DNN. During a forward pass through a layer of the DNN, the weights are pruned, then quantized. The output feature maps are then computed using the quantized weights. The output feature maps are then quantized. During a backward pass, the pruning parameters and the parameters for quantizing the pruned weights and the output feature maps are updated based on an optimization of a cost function.</p><p id="p-0029" num="0028">In one embodiment, the full-precision trained neural network <b>101</b> and the pruned and quantized neural network <b>102</b> may be DNNs. <figref idref="DRAWINGS">FIG. <b>2</b></figref> depicts an example architecture <b>200</b> of a DNN that may be input to the neural network analyzer <b>100</b> as a full-precision trained neural network <b>101</b>. The example architecture <b>200</b> of the VGG <b>16</b> specifically includes 13 convolutional (CONV) layers, three fully connected (FC) layers, and five pooling layers (PLs). An input to the VGG <b>16</b> architecture <b>200</b> is applied to convolutional layer CONV1_<b>1</b>, and an output is provided at Classification.</p><p id="p-0030" num="0029">To prune the weights of a layer of a DNN, each weight may be multiplied by an analytic threshold function h(w) that may be generally characterized as having the qualities of setting the values of weights that have magnitudes less than a threshold to zero without affecting the values of weights having magnitude greater than the threshold. <figref idref="DRAWINGS">FIG. <b>3</b></figref> depicts an example layer <b>300</b> in a DNN and application of an analytic threshold function to form an analytic weight function &#x192;(w) for optimally pruning the weights of the example layer <b>300</b> according to the subject matter disclosed herein. Regardless of the level of complexity of a neural network that may include the example layer <b>300</b>, an output feature map of the layer <b>300</b> may be formed as a sum of products of an input feature map with the connecting weights w to the output feature map, as indicated on the left side of <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0031" num="0030">The analytic threshold function h(w) may be applied to the weights w of layer <b>300</b> at <b>301</b> to form a weight function &#x192; (w) that replaces the weights w, thereby forming a layer <b>300</b>&#x2032;. In one embodiment, each weight w may be multiplied by the analytic function h(w). A graph of an example weight function &#x192;(w) is shown at the top center of <figref idref="DRAWINGS">FIG. <b>3</b></figref>. In one embodiment, the analytic threshold function h(w) may be</p><p id="p-0032" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mrow>       <mi>h</mi>       <mo>&#x2061;</mo>       <mo>(</mo>       <mi>w</mi>       <mo>)</mo>      </mrow>      <mo>=</mo>      <mfrac>       <mn>1</mn>       <mrow>        <mn>1</mn>        <mo>+</mo>        <mrow>         <msup>          <mi>e</mi>          <mrow>           <mrow>            <mo>-</mo>            <msup>             <mi>&#x3b2;</mi>             <mn>2</mn>            </msup>           </mrow>           <mo>&#x2062;</mo>           <msup>            <mi>w</mi>            <mn>2</mn>           </msup>          </mrow>         </msup>         <mo>/</mo>         <msup>          <mi>&#x3b1;</mi>          <mn>2</mn>         </msup>        </mrow>       </mrow>      </mfrac>     </mrow>     <mo>,</mo>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>1</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0033" num="0000">in which &#x3b1; is a parameter that controls a sharpness of the threshold function h(w), and &#x3b2; is a parameter that controls a distance (or range) between the first and second edges of the threshold function. The smaller the parameter &#x3b1;, the sharper the profile of h(w), and the smaller the parameter &#x3b2;, the wider the width of h(w). The parameters &#x3b1; and &#x3b2; are scaling values and have no units.</p><p id="p-0034" num="0031"><figref idref="DRAWINGS">FIG. <b>4</b>A</figref> depicts a graph of the example threshold function h(w) of Eq. (1). The parameter &#x3b1; controls a sharpness of the edges <b>401</b><i>a </i>and <b>401</b><i>b </i>of the threshold function, that is, the parameter &#x3b1; controls a rate of change of the edges <b>401</b><i>a </i>and <b>401</b><i>b </i>between h(w)=0 and h(w)=1. The parameter &#x3b2; in Eq. (1) controls a width <b>402</b> between the two edges <b>401</b><i>a </i>and <b>401</b><i>b </i>at h(w)=0.5.</p><p id="p-0035" num="0032">As depicted in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, the threshold function h(w) has a value of 0 for a first set <b>403</b> of continuous weight values that are centered around 0. Additionally, the threshold function h(w) has a value of 1 for a second set <b>404</b> of continuous weight values that are greater than the first set <b>403</b> of continuous weight values, and has a value of 1 for a third set <b>405</b> of continuous weight values that are less than the first set <b>403</b> of weight values. The first edge <b>401</b><i>a </i>of the threshold function h(w) is between the first set <b>403</b> of continuous weight values and the second set <b>404</b> of continuous weight values. The second edge <b>401</b><i>b </i>is between the first set <b>403</b> of continuous weight values and the third set <b>405</b> of continuous weight values.</p><p id="p-0036" num="0033">The values of the threshold function h(w) transition between 0 and 1 in the regions of the first and second edges <b>401</b><i>a </i>and <b>401</b><i>b</i>. A sharpness of each of the first and second edges <b>401</b><i>a </i>and <b>401</b><i>b </i>as the threshold function h(w) transitions between 0 and 1 is based on a value of the parameter &#x3b1; (i.e., the sharpness of the edges <b>401</b><i>a </i>and <b>401</b><i>b </i>of the threshold function h(w)) and a distance <b>402</b> between the first and second edges <b>401</b><i>a </i>and <b>401</b><i>b </i>at h(w)=0.5 is based on a value of the parameter &#x3b2;.</p><p id="p-0037" num="0034">In one embodiment, each of the weights w of a layer may be multiplied by the threshold function h(w) to form a weight function &#x192; (w) as,</p><p id="p-0038" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mi>f</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mi>w</mi>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mrow>      <mfrac>       <mi>w</mi>       <mrow>        <mn>1</mn>        <mo>+</mo>        <mrow>         <msup>          <mi>e</mi>          <mrow>           <mrow>            <mo>-</mo>            <msup>             <mi>&#x3b2;</mi>             <mn>2</mn>            </msup>           </mrow>           <mo>&#x2062;</mo>           <msup>            <mi>w</mi>            <mn>2</mn>           </msup>          </mrow>         </msup>         <mo>/</mo>         <msup>          <mi>&#x3b1;</mi>          <mn>2</mn>         </msup>        </mrow>       </mrow>      </mfrac>      <mo>.</mo>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>2</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0039" num="0035"><figref idref="DRAWINGS">FIG. <b>4</b>B</figref> depicts a graph of the weight function &#x192; (w) of Eq. (2). For the first set <b>403</b> of weight values, the weight function &#x192; (w) has a value of 0. The weight function &#x192; (w) has a value of w for the second set <b>404</b> of continuous weight values that are greater than the first set <b>403</b> of continuous weight values, and has a value of w for the third set <b>405</b> of continuous weight values that are less than the first set <b>403</b> of weight values. The weight function &#x192; (w) includes transition regions <b>406</b><i>a </i>and <b>406</b><i>b </i>that respectively correspond to the first edge <b>401</b><i>a </i>and the second edge <b>401</b><i>b</i>. As the value of the parameter &#x3b1; becomes smaller, the first and second edges <b>401</b><i>a </i>and <b>401</b><i>b </i>become sharper, and the transition regions <b>406</b><i>a </i>and <b>406</b><i>b </i>become smaller in the w direction.</p><p id="p-0040" num="0036"><figref idref="DRAWINGS">FIGS. <b>5</b>A-<b>5</b>E</figref> depict five example graphs of the threshold function h(w) of Eq. (1) with different values for the parameters &#x3b1; and &#x3b2; to provide a visual sense for how the parameters &#x3b1; and &#x3b2; may affect the threshold function h(w) and the weight function &#x192; (w). In <figref idref="DRAWINGS">FIG. <b>5</b>A-<b>5</b>C</figref>, the parameter &#x3b1; is varied from &#x3b1;=1.0&#xd7;10<sup>&#x2212;8 </sup>(<figref idref="DRAWINGS">FIG. <b>5</b>A</figref>) to &#x3b1;=1.0&#xd7;10<sup>&#x2212;2 </sup>(<figref idref="DRAWINGS">FIG. <b>5</b>C</figref>), while the parameter &#x3b2; is constant at &#x3b2;=4. As can be seen in <figref idref="DRAWINGS">FIGS. <b>5</b>A-<b>5</b>C</figref>, the smaller the value of a, the sharper the threshold function h(w) transitions between 0 and 1. In <figref idref="DRAWINGS">FIGS. <b>5</b>D and <b>5</b>E</figref>, the parameter &#x3b2; is varied from &#x3b2;=8 (<figref idref="DRAWINGS">FIG. <b>5</b>D</figref>) to &#x3b2;=16 (<figref idref="DRAWINGS">FIG. <b>5</b>E</figref>), while the parameter &#x3b1; is constant at &#x3b1;=1.0&#xd7;10<sup>&#x2212;8</sup>. From <figref idref="DRAWINGS">FIGS. <b>5</b>D and <b>5</b>E</figref>, it can be seen that the smaller the parameter &#x3b2;, the greater the width between the edges of the threshold function h(w) at h(w)=0.5. Thus, smaller values of the parameters &#x3b1; and &#x3b2; provide a more robust threshold and a wider width for the threshold function h(w), which in turn provides a relatively greater number of zero weights or, in other words, relatively fewer non-zero weights.</p><p id="p-0041" num="0037">The parameters &#x3b1; and &#x3b2; in the threshold function h(w) may be trainable and optimized. As the parameters &#x3b1; and &#x3b2; become optimized, the shape of the threshold function h(w) converges to have optimally sharp edges and an optimal spacing at h(w)=0.5. The trainability of the parameters &#x3b1; and &#x3b2; provides a significant advantage over other pruning techniques that rely on iterative pruning and re-training because using the threshold function h(w) results in the number of non-zero weights being automatically optimized during back-propagation instead of empirically selecting thresholds to eventually arrive at an acceptable number of non-zero weights in the different layers of a DNN. Optimally reducing the number of non-zero weights, in turn, optimally reduces the computational burden on a device running the DNN.</p><p id="p-0042" num="0038">Returning to the joint pruning and quantization operation <b>104</b> in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the pruned weights w<sub>l,p </sub>of a layer may be determined as,</p><p id="p-0043" num="0000"><maths id="MATH-US-00003" num="00003"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <msub>       <mi>w</mi>       <mrow>        <mi>l</mi>        <mo>,</mo>        <mi>p</mi>       </mrow>      </msub>      <mo>=</mo>      <mfrac>       <msub>        <mi>w</mi>        <mi>l</mi>       </msub>       <mrow>        <mn>1</mn>        <mo>+</mo>        <mrow>         <mrow>          <mi>exp</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <mrow>           <mrow>            <mo>-</mo>            <msubsup>             <mi>&#x3b2;</mi>             <mrow>              <mi>l</mi>              <mo>,</mo>              <mi>p</mi>             </mrow>             <mn>2</mn>            </msubsup>           </mrow>           <mo>&#x2062;</mo>           <msubsup>            <mi>w</mi>            <mi>l</mi>            <mn>2</mn>           </msubsup>          </mrow>          <mo>)</mo>         </mrow>         <mo>/</mo>         <msubsup>          <mi>&#x3b1;</mi>          <mrow>           <mi>l</mi>           <mo>,</mo>           <mi>p</mi>          </mrow>          <mn>2</mn>         </msubsup>        </mrow>       </mrow>      </mfrac>     </mrow>     <mo>,</mo>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>3</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0044" num="0000">in which w<sub>l </sub>are the weights of layer l, &#x3b2;<sub>l,p</sub><sup>2 </sup>is a trainable parameter that controls the range of values to be pruned, and &#x3b1;<sub>l,p</sub><sup>2 </sup>is a fixed constant that controls the sharpness of the pruned weights w<sub>l,p </sub>function. The parameters &#x3b2;<sub>l,p</sub><sup>2 </sup>and &#x3b1;<sub>l,p</sub><sup>2 </sup>respectively correspond to the parameters &#x3b2; and &#x3b1; of Eq. (1).</p><p id="p-0045" num="0039">Learning the parameters for quantization of the weighs and the parameters for the quantization of the output feature maps entails finding the number of bits that provides the best balance between accuracy and complexity. Rather than looking for one optimal quantization level, a probability distribution may be used that includes a preference to each available quantization level. The most preferred quantization level receives the highest probability.</p><p id="p-0046" num="0040">To learn the parameters for quantization, the range of allowable quantization levels of the weights may be set as q<sub>w</sub>=q<sub>w,min</sub>, . . . , q<sub>w,max</sub>. For example, q<sub>w </sub>may take the allowable values between 4 and 8 bits. The preferences of the quantization levels for the weights may be represented using a categorical distribution (i.e., a discrete probability distribution) as</p><p id="p-0047" num="0000"><maths id="MATH-US-00004" num="00004"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <msubsup>       <mi>p</mi>       <mrow>        <mi>l</mi>        <mo>,</mo>        <mi>w</mi>       </mrow>       <msub>        <mi>q</mi>        <mi>w</mi>       </msub>      </msubsup>      <mo>=</mo>      <mfrac>       <mrow>        <mi>exp</mi>        <mo>&#x2061;</mo>        <mo>(</mo>        <mfrac>         <msubsup>          <mi>&#x3b1;</mi>          <mrow>           <mi>l</mi>           <mo>,</mo>           <mi>w</mi>          </mrow>          <msub>           <mi>q</mi>           <mi>w</mi>          </msub>         </msubsup>         <msub>          <mi>t</mi>          <mrow>           <mi>l</mi>           <mo>,</mo>           <mi>w</mi>          </mrow>         </msub>        </mfrac>        <mo>)</mo>       </mrow>       <mrow>        <msubsup>         <mo>&#x2211;</mo>         <mrow>          <mi>k</mi>          <mo>=</mo>          <msub>           <mi>q</mi>           <mrow>            <mi>w</mi>            <mo>,</mo>            <mi>min</mi>           </mrow>          </msub>         </mrow>         <msub>          <mi>q</mi>          <mrow>           <mi>w</mi>           <mo>,</mo>           <mi>max</mi>          </mrow>         </msub>        </msubsup>        <mrow>         <mi>exp</mi>         <mo>&#x2061;</mo>         <mo>(</mo>         <mfrac>          <msubsup>           <mi>&#x3b1;</mi>           <mrow>            <mi>l</mi>            <mo>,</mo>            <mi>w</mi>           </mrow>           <mi>k</mi>          </msubsup>          <msub>           <mi>t</mi>           <mrow>            <mi>l</mi>            <mo>,</mo>            <mi>w</mi>           </mrow>          </msub>         </mfrac>         <mo>)</mo>        </mrow>       </mrow>      </mfrac>     </mrow>     <mo>,</mo>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>4</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0048" num="0000">in which p<sub>l,w</sub><sup>q</sup><sup><sub2>w </sub2></sup>is a number between [0,1] (with 1 representing the highest score a quantization level can achieve), w is a subscript indicating that the probability is associated with the weights, q<sub>w </sub>indicates a quantization level (e.g., 5 bits), l denotes the layer of the weights being quantized, &#x3b1;<sub>l,w</sub><sup>q</sup><sup><sub2>w </sub2></sup>denotes a trainable parameter corresponding to the un-normalized probability of quantization level q, and t<sub>l,w </sub>is a trainable scaling parameter of the categorical distribution.</p><p id="p-0049" num="0041">Quantization of the weights of a layer is not performed only at one selected quantization level, but instead as a weighted average of all quantization levels as</p><p id="p-0050" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>w </i><sub>l,q</sub>=&#x3a3;<sub>k=q</sub><sub><sub2>w,min </sub2></sub><sup>q</sup><sup><sub2>w,max dequant</sub2></sup><sub>k</sub>[quant<sub>k</sub>(<i>w </i><sub>l,p</sub>)]&#xd7;p<sub>l,w</sub><sup>k</sup>,&#x2003;&#x2003;(5)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0051" num="0000">in which w<sub>l,q </sub>is the quantized-dequantized weights for layer l, quant<sub>k </sub>is a quantization of the weights at k bits, and dequant<sub>k </sub>is a dequantization of the weights at k bits.</p><p id="p-0052" num="0042">The pre-quantized output feature maps for the layer are computed using the quantized weights as determined by Eq. (5). Similar to the weights, the range of allowable quantization levels of the output feature maps may be set as q<sub>&#x192;</sub>=q<sub>&#x192;, min</sub>, . . . q<sub>&#x192;,max</sub>, and the preference of a quantization level for the output feature maps may be represented using a categorical distribution as</p><p id="p-0053" num="0000"><maths id="MATH-US-00005" num="00005"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <msubsup>       <mi>p</mi>       <mrow>        <mi>l</mi>        <mo>,</mo>        <mi>f</mi>       </mrow>       <msub>        <mi>q</mi>        <mi>f</mi>       </msub>      </msubsup>      <mo>=</mo>      <mfrac>       <mrow>        <mi>exp</mi>        <mo>&#x2061;</mo>        <mo>(</mo>        <mfrac>         <msubsup>          <mi>&#x3b1;</mi>          <mrow>           <mi>l</mi>           <mo>,</mo>           <mi>f</mi>          </mrow>          <msub>           <mi>q</mi>           <mi>f</mi>          </msub>         </msubsup>         <msub>          <mi>t</mi>          <mrow>           <mi>l</mi>           <mo>,</mo>           <mi>f</mi>          </mrow>         </msub>        </mfrac>        <mo>)</mo>       </mrow>       <mrow>        <msubsup>         <mo>&#x2211;</mo>         <mrow>          <mi>k</mi>          <mo>=</mo>          <msub>           <mi>q</mi>           <mrow>            <mi>f</mi>            <mo>,</mo>            <mi>min</mi>           </mrow>          </msub>         </mrow>         <msub>          <mi>q</mi>          <mrow>           <mi>f</mi>           <mo>,</mo>           <mi>max</mi>          </mrow>         </msub>        </msubsup>        <mrow>         <mi>exp</mi>         <mo>&#x2061;</mo>         <mo>(</mo>         <mfrac>          <msubsup>           <mi>&#x3b1;</mi>           <mrow>            <mi>l</mi>            <mo>,</mo>            <mi>f</mi>           </mrow>           <mi>k</mi>          </msubsup>          <msub>           <mi>t</mi>           <mrow>            <mi>l</mi>            <mo>,</mo>            <mi>f</mi>           </mrow>          </msub>         </mfrac>         <mo>)</mo>        </mrow>       </mrow>      </mfrac>     </mrow>     <mo>,</mo>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>6</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0054" num="0000">in which p<sub>l,&#x192;</sub><sup>q&#x192;</sup>is a number between [0,1] (with 1 representing the highest score a quantization level can achieve), &#x192; is a subscript indicating that the probability is associated with the feature maps, q<sub>&#x192;</sub>indicates a quantization level, l denotes the layer of the feature maps being quantized, &#x3b1;<sub>l,&#x192;</sub><sup>q&#x192;</sup>denotes a trainable parameter corresponding to the un-normalized probability of quantization level q, and t<sub>l,&#x192;</sub>is a trainable scaling parameter of the categorical distribution.</p><p id="p-0055" num="0043">The output feature maps are also not quantized at just one selected quantization level, but instead as a weighted average of all quantization levels as</p><p id="p-0056" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>&#x192;<sub>l,q</sub>=&#x3a3;<sub>k=q</sub><sub><sub2>&#x192;,min</sub2></sub><sup>q</sup><sup><sub2>&#x192;,max </sub2></sup>dequant<sub>k</sub>[quant<sub>k</sub>(&#x192;<sub>l</sub>)]&#xd7;p<sub>l,&#x192;</sub><sup>k</sup>,&#x2003;&#x2003;(7)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0057" num="0000">in which &#x192;<sub>l,q </sub>is the quantized-dequantized output feature maps for layer l, quant<sub>k </sub>is a quantization of the output feature maps at k bits, and dequant<sub>k </sub>is a dequantization of the output feature maps at k bits.</p><p id="p-0058" num="0044">In addition to the trainable weights of a layer, the trainable parameters for each layer l include the pruning parameters &#x3b2;<sub>l,p</sub>, the un-normalized probabilities &#x3b1;<sub>l,w</sub><sup>q</sup><sup><sub2>w </sub2></sup>and &#x3b1;<sub>l,&#x192;</sub><sup>q&#x192;</sup>for each quantization level, and the scaling parameters of the distribution t<sub>l,w </sub>and t<sub>l,&#x192;</sub>.</p><p id="p-0059" num="0045">For a DNN that is pruned and quantized in a conventional manner (i.e., pruning and quantizing being separate independent operations), the total loss E (w) may be defined as</p><p id="p-0060" num="0000"><maths id="MATH-US-00006" num="00006"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mrow>       <mi>E</mi>       <mo>&#x2061;</mo>       <mo>(</mo>       <mi>w</mi>       <mo>)</mo>      </mrow>      <mo>=</mo>      <mrow>       <mrow>        <mfrac>         <mn>1</mn>         <mi>N</mi>        </mfrac>        <mo>&#x2062;</mo>        <mrow>         <msubsup>          <mo>&#x2211;</mo>          <mrow>           <mi>n</mi>           <mo>=</mo>           <mn>1</mn>          </mrow>          <mi>N</mi>         </msubsup>         <mrow>          <msub>           <mi>E</mi>           <mi>c</mi>          </msub>          <mo>(</mo>          <mi>w</mi>          <mo>)</mo>         </mrow>        </mrow>       </mrow>       <mo>+</mo>       <mrow>        <msub>         <mi>&#x3bb;</mi>         <mi>r</mi>        </msub>        <mo>&#x2062;</mo>        <mrow>         <mi>E</mi>         <mo>&#x2061;</mo>         <mo>(</mo>         <mi>w</mi>         <mo>)</mo>        </mrow>       </mrow>      </mrow>     </mrow>     <mo>,</mo>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>8</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0061" num="0000">in which E<sub>c </sub>is the cross-entropy loss and E<sub>r </sub>is the L2 regularization on the weights.</p><p id="p-0062" num="0046">For the joint pruning and quantizing operation <b>104</b> in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, three pruning losses may be included in a cost function. A pruning loss E<sub>p</sub>=&#x3a3;<sub>l=1</sub><sup>L </sup>&#x3b2;<sub>l,p</sub><sup>2</sup>, in which L is the maximum number of layers, may be added that helps &#x3b2; to be small, and in effect increases pruning. The pruning loss E<sub>p</sub>, however, may have a negative effect on the cross-entropy loss E<sub>c </sub>of Eq. (8), so the two terms should be balanced.</p><p id="p-0063" num="0047">A weight quantization loss E<sub>w,q</sub>=&#x3a3;<sub>l=1</sub><sup>L </sup>&#x3a3;<sub>k=q</sub><sub><sub2>w,min</sub2></sub><sup>q</sup><sup><sub2>w,max </sub2></sup>k&#xd7;p<sub>l,w</sub><sup>k </sup>may be added that helps the optimization to place all the probability mass on the smallest quantization level. Doing so may also negatively affect the cross-entropy loss E<sub>c</sub>.</p><p id="p-0064" num="0048">A feature map quantization loss E<sub>&#x192;,q</sub>=&#x3a3;<sub>l=1</sub><sup>L </sup>&#x3a3;<sub>k=q</sub><sub><sub2>&#x192;,min</sub2></sub><sup>q</sup><sup><sub2>&#x192;,max </sub2></sup>k&#xd7;p<sub>l,&#x192;</sub><sup>k </sup>may be added that operates in the same manner as the weight quantization loss E<sub>w,q</sub>.</p><p id="p-0065" num="0049">The cost function that may be optimized during a backward pass of the joint pruning and quantizing operation may be</p><p id="p-0066" num="0000"><maths id="MATH-US-00007" num="00007"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mrow>       <mi>E</mi>       <mo>&#x2061;</mo>       <mo>(</mo>       <mi>w</mi>       <mo>)</mo>      </mrow>      <mo>=</mo>      <mrow>       <mrow>        <mfrac>         <mn>1</mn>         <mi>N</mi>        </mfrac>        <mo>&#x2062;</mo>        <mrow>         <msubsup>          <mo>&#x2211;</mo>          <mrow>           <mi>n</mi>           <mo>=</mo>           <mn>1</mn>          </mrow>          <mi>N</mi>         </msubsup>         <mrow>          <msub>           <mi>E</mi>           <mi>c</mi>          </msub>          <mo>(</mo>          <mi>w</mi>          <mo>)</mo>         </mrow>        </mrow>       </mrow>       <mo>+</mo>       <mrow>        <msub>         <mi>&#x3bb;</mi>         <mi>r</mi>        </msub>        <mo>&#x2062;</mo>        <mrow>         <msub>          <mi>E</mi>          <mi>r</mi>         </msub>         <mo>(</mo>         <mi>w</mi>         <mo>)</mo>        </mrow>       </mrow>       <mo>+</mo>       <mrow>        <msub>         <mi>&#x3bb;</mi>         <mi>p</mi>        </msub>        <mo>&#x2062;</mo>        <mrow>         <msub>          <mi>E</mi>          <mi>p</mi>         </msub>         <mo>(</mo>         <mi>w</mi>         <mo>)</mo>        </mrow>       </mrow>       <mo>+</mo>       <mrow>        <msub>         <mi>&#x3bb;</mi>         <mrow>          <mi>w</mi>          <mo>,</mo>          <mi>q</mi>         </mrow>        </msub>        <mo>&#x2062;</mo>        <mrow>         <msub>          <mi>E</mi>          <mrow>           <mi>w</mi>           <mo>,</mo>           <mi>q</mi>          </mrow>         </msub>         <mo>(</mo>         <mi>w</mi>         <mo>)</mo>        </mrow>       </mrow>       <mo>+</mo>       <mrow>        <msub>         <mi>&#x3bb;</mi>         <mrow>          <mi>f</mi>          <mo>,</mo>          <mi>q</mi>         </mrow>        </msub>        <mo>&#x2062;</mo>        <mrow>         <msub>          <mi>E</mi>          <mrow>           <mi>f</mi>           <mo>,</mo>           <mi>q</mi>          </mrow>         </msub>         <mo>(</mo>         <mi>w</mi>         <mo>)</mo>        </mrow>       </mrow>      </mrow>     </mrow>     <mo>,</mo>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>9</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0067" num="0050">in which N is the size of a mini batch that may be manually set beforehand and may be a number, such as, but not limited to 32, 64, 128 or 256, and the constants &#x3bb;<sub>r</sub>, &#x3bb;<sub>p</sub>, &#x3bb;<sub>w,q </sub>and &#x3bb;<sub>&#x192;,q </sub>may be determined by a grid-based parameter optimization or, alternatively, by a random search.</p><p id="p-0068" num="0051"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flow chart of an example embodiment of a method <b>600</b> for performing a joint pruning and quantization operation on the layers of a neural network, such as a DNN, according to the subject matter disclosed herein. For one layer, the process starts at <b>601</b>. At <b>602</b>, the weights of the layer are pruned using an analytic threshold function, such as, but not limited to, the analytic threshold function of Eq. (1). At <b>603</b>, each weight of the layer that remains after pruning is quantized based on a weighted average of a quantization and dequantization of the weight for all quantization levels to form quantized weights for the layer. At <b>604</b>, output feature maps for the layer are determined, or calculated, using the quantized weights formed in <b>603</b>. At <b>605</b>, each output feature map of the layer is quantized based on a quantization and dequantization of the output feature map for all quantization levels. At <b>606</b>, parameters of the analytic function, the weighted average of all quantization levels of the weights and the weighted average of all quantization levels of the output feature maps are updated using a cost function. At <b>607</b>, the method <b>600</b> for the layer ends. The method <b>600</b> may be iteratively performed for a given layer of a neural network. Additionally, the method <b>600</b> may be performed on one or more of the layers of a neural network.</p><p id="p-0069" num="0052"><figref idref="DRAWINGS">FIG. <b>7</b></figref> depicts a block diagram of an exemplary architecture <b>700</b> for a data processing system. In one embodiment, the architecture <b>700</b> may be used to implement neural network analyzer <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0070" num="0053">The architecture <b>700</b> includes at least one processor, e.g., a central processing unit (CPU) <b>701</b> coupled to memory elements <b>702</b> through a system bus <b>703</b> or other suitable circuitry. The architecture <b>700</b> may store program code within the memory elements <b>702</b>. The processor <b>701</b> may execute the program code accessed from the memory elements <b>702</b> via system bus <b>703</b>. As such, processor <b>701</b> may serve as a special processor. The memory elements <b>702</b> may include one or more physical memory devices such as, but not limited to, a local memory <b>704</b> and one or more bulk storage devices <b>705</b>. The local memory <b>704</b> may be a random access memory (RAM) or other non-persistent memory device(s) generally used during actual execution of the program code. The bulk storage device <b>705</b> may be implemented as a hard disk drive (HDD), solid-state drive (SSD), and/or other persistent data storage device. The architecture <b>700</b> may also include one or more cache memories (not shown) that provide temporary storage of at least some program code in order to reduce the number of times program code must be retrieved from bulk storage device <b>705</b> during execution.</p><p id="p-0071" num="0054">The architecture <b>700</b> may also include input/output (I/O) devices, such as a keyboard <b>706</b>, a display device <b>707</b>, and/or a pointing device <b>708</b> that may optionally be coupled to the architecture <b>700</b>. In some embodiments, one or more of the I/O devices may be combined as in a touchscreen that is used as display device <b>707</b>. Such a display device <b>707</b> may also include a keyboard <b>706</b> and pointing device <b>708</b>. One or more network adapters <b>709</b> may also be coupled to the architecture <b>700</b> to enable the architecture <b>700</b> to become coupled to other systems, computer systems, remote printers, and/or remote storage devices through intervening private or public networks. Modems, cable modems, Ethernet cards, and wireless transceivers are non-limiting examples of different types of network adapters that may be used with the architecture <b>700</b>. Depending upon the particular device implemented with the architecture <b>700</b>, the specific type of network adapter, or network adapters may vary as the case may be. The I/O devices may be coupled to the architecture <b>700</b> either directly or through intervening I/O controllers.</p><p id="p-0072" num="0055">As depicted in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the memory elements <b>702</b> may store an operating system <b>710</b> and one or more applications <b>711</b>. In one aspect, the operating system <b>710</b> and the application(s) <b>711</b>, being implemented in the form of executable program code, may be executed by the architecture <b>700</b>. As such, the operating system <b>710</b> and the application(s) <b>711</b> may be considered an integrated part of the architecture <b>700</b>. The operating system <b>710</b>, the application(s) <b>711</b>, and any data items used, generated, and/or operated upon by the architecture <b>700</b> may be functional data structures that impart functionality when employed as part of a system implemented using the architecture <b>700</b>.</p><p id="p-0073" num="0056">In one arrangement, an application <b>711</b> may include one or more modules that when executed by a system using the architecture <b>700</b> or an architecture similar to the architecture <b>700</b>, may perform the various operations described herein with reference to <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>3</b>-<b>6</b></figref>.</p><p id="p-0074" num="0057">In another arrangement, the architecture <b>700</b> may be coupled to a platform <b>712</b> through a communication link <b>713</b>. In one example, the architecture <b>700</b> may be coupled to the platform <b>712</b> through a network adapter <b>709</b>. In another example, the architecture <b>700</b> may include one or more other I/O devices, such as Universal Serial Bus (USB) interface, or other communication port, that may be used to couple the architecture <b>700</b> to the platform <b>712</b>. The platform <b>712</b> may be a circuit board and have a neural network accelerator <b>714</b> coupled thereto. In one arrangement, the neural network accelerator <b>714</b> may be implemented as an integrated circuit (IC) or a plurality of ICs. For example, neural network accelerator <b>714</b> may be implemented as one or more programmable ICs, such as field programmable gate arrays, one or more application-specific ICs (ASICs), or the like.</p><p id="p-0075" num="0058">Architecture <b>700</b> may be configured to perform the operations described herein on a neural network or a DNN to generate a pruned and quantized network, such as the pruned and quantized network <b>102</b> in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. That is, the architecture <b>700</b> may receive the trained neural network <b>101</b> through an I/O device, operate on one or more layers of the trained neural network, and output through an I/O device a trained neural network having one or more layers that have weights and output feature maps that have been jointly pruned and quantized according to the subject matter disclosed herein. The architecture <b>700</b> further may provide a definition of the pruned and quantized network <b>102</b> to neural network accelerator <b>714</b> for execution therein.</p><p id="p-0076" num="0059"><figref idref="DRAWINGS">FIG. <b>75</b></figref> is provided for purposes of illustration only and, as such, is not intended as a limitation of the inventive arrangements described herein. In some cases, the particular system implemented using the architecture <b>700</b> may include fewer components or more components than shown. Further, the particular operating system and/or application(s) included as part of the architecture <b>700</b> may vary.</p><p id="p-0077" num="0060">As will be recognized by those skilled in the art, the innovative concepts described herein can be modified and varied over a wide range of applications. Accordingly, the scope of claimed subject matter should not be limited to any of the specific exemplary teachings discussed above, but is instead defined by the following claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230004813A1-20230105-M00001.NB"><img id="EMI-M00001" he="7.03mm" wi="76.20mm" file="US20230004813A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002" nb-file="US20230004813A1-20230105-M00002.NB"><img id="EMI-M00002" he="6.35mm" wi="76.20mm" file="US20230004813A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00003" nb-file="US20230004813A1-20230105-M00003.NB"><img id="EMI-M00003" he="6.35mm" wi="76.20mm" file="US20230004813A1-20230105-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00004" nb-file="US20230004813A1-20230105-M00004.NB"><img id="EMI-M00004" he="14.48mm" wi="76.20mm" file="US20230004813A1-20230105-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00005" nb-file="US20230004813A1-20230105-M00005.NB"><img id="EMI-M00005" he="15.49mm" wi="76.20mm" file="US20230004813A1-20230105-M00005.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00006" nb-file="US20230004813A1-20230105-M00006.NB"><img id="EMI-M00006" he="5.67mm" wi="76.20mm" file="US20230004813A1-20230105-M00006.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00007" nb-file="US20230004813A1-20230105-M00007.NB"><img id="EMI-M00007" he="5.67mm" wi="76.20mm" file="US20230004813A1-20230105-M00007.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A neural network, comprising a plurality of layers, at least one layer comprising jointly pruned and quantized weights and output feature maps, the jointly pruned weights being pruned using an analytic threshold function, each weight remaining after being pruned further being quantized based on a weighted average of a quantization and dequantization of the weight for all quantization levels.</claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The neural network of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the output feature maps are formed based on pruned and quantized weights of the layer, each output feature map being quantized based on a weighted average of a quantization and dequantization of the output feature map for all quantization levels and parameters of the analytic threshold function, and<claim-text>wherein the weighted average of all quantization levels of the weights and the weighted average of each output feature map of the layer are updated based on a cost function.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The neural network of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the neural network is a full-precision trained neural network before weights and output feature maps of the at least one layer are jointly pruned and quantized.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The neural network of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the cost function includes a pruning loss term, a weight quantization loss term and a feature map quantization loss term.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The neural network of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the parameters of the analytic threshold function, the weighted average of all quantization levels of the weights and the weighted average of each output feature map of the layer are updated based on an optimization of the cost function.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The neural network of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the parameters of the analytic threshold function, the weighted average of all quantization levels of the weights and the weighted average of each output feature map of the layer are iteratively updated based on an optimization of the cost function.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The neural network of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the parameters of the analytic threshold function include a first parameter that controls a sharpness of the analytic threshold function, and second parameter that controls a distance between a first edge and a second edge of the analytic threshold function.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. A method to prune weights and output feature maps of a layer of a neural network, the method comprising:<claim-text>pruning weights of a layer of a neural network using an analytic threshold function, the neural network being a trained neural network;</claim-text><claim-text>quantizing each weight of the layer remaining after pruning based on a weighted average of a quantization and dequantization of the weight for all quantization levels to form quantized weights for the layer; and</claim-text><claim-text>determining output feature maps of the layer based on quantized weights of the layer.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, further comprising:<claim-text>quantizing each output feature map of the layer based on a weighted average of a quantization and dequantization of the output feature map for all quantization levels; and</claim-text><claim-text>updating parameters of the analytic threshold function, the weighted average of all quantization levels of the weights and the weighted average of each output feature map of the layer using a cost function.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein updating the parameters of the analytic threshold function, the weighted average of all quantization levels of the weights and the weighted average of each output feature map of the layer further comprises optimizing the cost function.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the cost function includes a pruning loss term, a weight quantization loss term and a feature map quantization loss term.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, further comprising iteratively pruning the weights, quantizing each weight of the layer, determining the output feature maps of the layer, quantizing each output feature map of the layer, and updating the parameters of the analytic threshold function, the weighted average of all quantization levels of the weights and the weighted average of each output feature map of the layer to optimize the cost function.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the layer of the neural network is a first layer,<claim-text>the method further comprising:</claim-text><claim-text>pruning weights of a second layer of the neural network using the analytic threshold function, the second layer being subsequent to the first layer in the neural network;</claim-text><claim-text>quantizing each weight of the second layer remaining after pruning based on a weighted average of a quantization and dequantization of the weight for all quantization levels to form quantized weights for the second layer;</claim-text><claim-text>determining the output feature maps of the second layer based on the quantized weights of the second layer;</claim-text><claim-text>quantizing each output feature map of the second layer based on a weighted average of a quantization and a dequantization of the output feature map for all quantization levels; and</claim-text><claim-text>updating parameters of the analytic threshold function, the weighted average of all quantization levels of the weights and the weighted average of each output feature map of the second layer by optimizing the cost function.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the parameters of the analytic threshold function include a first parameter that controls a sharpness of the analytic threshold function, and second parameter that controls a distance between a first edge and a second edge of the analytic threshold function.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. A neural network analyzer, comprising:<claim-text>an interface configured to receive a neural network that comprises a plurality of layers; and</claim-text><claim-text>a processing device configured to generate a neural network comprising at least one layer having weights and output feature maps that have been jointly pruned and quantized, to prune the weights of the at least one layer of the neural network using an analytic threshold function, to quantize each weight of the at least one layer remaining after pruning based on a weighted average of a quantization and dequantization of the weight for all quantization levels to form quantized weights for the at least one layer.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The neural network analyzer of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the processing device is further configured to determine output feature maps of the at least one layer based on the quantized weights of the at least one layer, to quantize each output feature map of the at least one layer based on a weighted average of a quantization and dequantization of the output feature map for all quantization levels, and to update parameters of the analytic threshold function, the weighted average of all quantization levels of the weights and the weighted average of each output feature map of the at least one layer using a cost function.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The neural network analyzer of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the interface is further configured to output the neural network comprising at least one layer having weights and output feature maps that have been jointly pruned and quantized.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The neural network analyzer of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the neural network is a full-precision trained neural network before the weights and the output feature maps of the at least one layer are jointly pruned and quantized.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The neural network analyzer of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the cost function includes a pruning loss term, a weight quantization loss term and a feature map quantization loss term.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The neural network analyzer of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the parameters of the analytic threshold function, the weighted average of all quantization levels of the weights and the weighted average of each output feature map of the at least one layer are iteratively updated based on an optimization of the cost function, and<claim-text>wherein the parameters of the analytic threshold function include a first parameter that controls a sharpness of the analytic threshold function, and second parameter that controls a distance between a first edge and a second edge of the analytic threshold function.</claim-text></claim-text></claim></claims></us-patent-application>