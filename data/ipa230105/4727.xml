<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004728A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004728</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17364988</doc-number><date>20210701</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>44</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>20</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>5</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>47</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20200101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>44</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20190101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>20</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>5</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20200101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>47</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">MODEL MAPPING AND ENRICHMENT SYSTEM</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>SAP SE</orgname><address><city>Walldorf</city><country>DE</country></address></addressbook><residence><country>DE</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>MOUR</last-name><first-name>Vishal</first-name><address><city>Guwahati</city><country>IN</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>DEY</last-name><first-name>Sreya</first-name><address><city>Banagalore</city><country>IN</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>JAIN</last-name><first-name>Shipra</first-name><address><city>Bangalore</city><country>IN</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>LODHE</last-name><first-name>Rahul</first-name><address><city>Bangalore</city><country>IN</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Disclosed herein are various embodiments for training and enriching a natural language processing system. An embodiment operates by determining that a first prediction from a first machine model has been generated based on a dataset comprising a plurality of attributes. A technical map identifying a first subset of attributes of the plurality of attributes used to generate the first prediction by the first machine model is generated. Natural language translations corresponding to at least a portion of the first subset of attributes used to generate the first prediction by the first machine model are identified. A natural language map of the first subset of attributes is generated based on the natural language translations. The natural language map is provided with the first prediction.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="104.99mm" wi="158.75mm" file="US20230004728A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="227.50mm" wi="164.17mm" orientation="landscape" file="US20230004728A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="217.42mm" wi="164.08mm" orientation="landscape" file="US20230004728A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="224.87mm" wi="133.77mm" orientation="landscape" file="US20230004728A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="203.37mm" wi="161.80mm" orientation="landscape" file="US20230004728A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="211.67mm" wi="161.80mm" orientation="landscape" file="US20230004728A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="205.15mm" wi="153.92mm" orientation="landscape" file="US20230004728A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="205.15mm" wi="153.92mm" orientation="landscape" file="US20230004728A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="229.45mm" wi="148.25mm" file="US20230004728A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="184.40mm" wi="166.37mm" orientation="landscape" file="US20230004728A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">BACKGROUND</heading><p id="p-0002" num="0001">Artificial intelligence (AI) and machine learning (ML) models are increasing in use, complexity, and popularity. As an example, a processor may analyze a set of inputs against a model and generate a prediction. The prediction may indicate a recommended action for a user. Because the action may have serious financial, business, or other implications or consequences, for the user to act on the prediction, the user must first trust the model. For a user to trust a model, the user often wants or needs to understand how the model arrived at its prediction. However, a technical breakdown of how the model works with various specific technical variables will often be unhelpful to a non-technical user who is unfamiliar with the technical aspects and specific terminology of the model but who is tasked with acting on the prediction.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0002" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0003" num="0002">The accompanying drawings are incorporated herein and form a part of the specification.</p><p id="p-0004" num="0003"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram illustrating functionality for a model mapping and enrichment system, according to some example embodiments.</p><p id="p-0005" num="0004"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is another block diagram illustrating example operations of a model mapping and enrichment system, according to some additional example embodiments.</p><p id="p-0006" num="0005"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a block diagram illustrating an example dataset, according to some example embodiments.</p><p id="p-0007" num="0006"><figref idref="DRAWINGS">FIGS. <b>4</b>A and <b>4</b>B</figref> are block diagrams illustrating an examples of feature stores, according to some example embodiments.</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIGS. <b>5</b>A and <b>5</b>B</figref> are block diagrams illustrating example functionality related to a model mapping and enrichment system, according to some example embodiments.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flowchart illustrating example operations for functionality related to a model mapping and enrichment system, according to some embodiments.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is an example computer system useful for implementing various embodiments.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0011" num="0010">In the drawings, like reference numbers generally indicate identical or similar elements. Additionally, generally, the left-most digit(s) of a reference number identifies the drawing in which the reference number first appears.</p><heading id="h-0003" level="1">DETAILED DESCRIPTION</heading><p id="p-0012" num="0011">Artificial intelligence (AI) and machine learning (ML) models are increasing in use, complexity, and popularity. As an example, a processor may analyze a set of inputs against a model and generate a prediction. The prediction may indicate a recommended action for a user. Because the action may have serious financial, business, or other implications or consequences, for the user to act on the prediction, the user must first trust the model. For a user to trust a model, the user often wants or needs to understand how the model arrived at its prediction. However, a technical breakdown of how the model works with various specific technical variables will often be unhelpful to a non-technical user who is unfamiliar with the technical aspects and specific terminology of the model but who is tasked with acting on the prediction.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram <b>100</b> illustrating functionality for a model mapping and enrichment system, according to some example embodiments. In some embodiments, a model <b>106</b>A may generate a prediction <b>108</b>A that may be provided to a user device <b>118</b> (as prediction <b>109</b>) for a user <b>104</b>. The user <b>104</b> may be tasked with acting on the prediction <b>109</b>. However, for the user <b>104</b> to trust the prediction <b>109</b>, the user <b>104</b> will need to understand how the model <b>106</b>A generated the prediction <b>109</b>.</p><p id="p-0014" num="0013">Providing this functionality to help a user understand the prediction <b>109</b> becomes increasingly difficult if the user <b>104</b> actually represents multiple non-technical users, and there are numerous different models <b>106</b>A, <b>106</b>B each providing a variety of predictions <b>108</b>A, <b>108</b>B simultaneously to the set of users <b>104</b>.</p><p id="p-0015" num="0014">This is because models <b>106</b>A, <b>106</b>B often operate like black boxes and are very time consuming or even impossible to fully understand. Understanding a model often requires that a person has both technical experience and an already existing understanding of what terminology was used to represent which attributes or variables when the models <b>106</b>A, <b>106</b>B were developed. Without this background and experience, it would be impossible for user <b>104</b> to understand how a model <b>106</b>A, <b>106</b>B reached a prediction <b>109</b>.</p><p id="p-0016" num="0015">The model mapping system (MMS) <b>102</b> may automatically generate natural language maps <b>116</b>A, <b>116</b>B that explain how the models <b>106</b>A, <b>106</b>B arrived at the predictions <b>108</b>A, <b>108</b>B in language that is configured and customized to each user's specific technical or non-technical knowledge and existing skillset. The NLM <b>111</b> provided to the users <b>104</b> may facilitate understanding, trust, and adoption of the predictions <b>109</b> provided by the models <b>106</b>A, <b>106</b>B which will save organizations time, money, and resources, and increase profitability.</p><p id="p-0017" num="0016">The MMS <b>102</b> may help bridge the gap in understanding for a non-technical user <b>104</b> (or a number of different users <b>104</b> (not shown)) when using or deciding whether or not to act on a model's prediction <b>109</b> by providing an easy to understand NLM <b>111</b> providing a non-technical description of how a particular model <b>106</b>A, <b>106</b>B arrived at the stated prediction <b>109</b>.</p><p id="p-0018" num="0017">NLM <b>111</b>, as generated by MMS <b>102</b>, will help the user <b>104</b> decide whether or not to trust the model <b>106</b>A, <b>106</b>B and/or act on prediction <b>109</b>. NLM <b>111</b> may further help the user <b>104</b> provide valuable feedback on how the model <b>106</b>A, <b>106</b>B may be improved. For example, the model <b>106</b>A, <b>106</b>B may generate a prediction <b>109</b>, if based on NLM <b>111</b>, the user <b>104</b> identifies a factor that was not considered by the model <b>106</b>A, <b>106</b>B, this feedback may be used to retrain or update the model <b>106</b>A, <b>106</b>B, so as to provide better predictions <b>109</b> in the future.</p><p id="p-0019" num="0018">Further, providing an NLM <b>111</b> with a prediction <b>109</b> may increase the adoption rate of various models <b>106</b>A, <b>106</b>B by various users <b>104</b> which may help organizations increase revenue, productivity, and customer satisfaction while often also decreasing decrease costs. If a user <b>104</b> understands or can see how the model <b>106</b>A, <b>106</b>B arrived at prediction <b>109</b>, the user <b>104</b> will be more likely to trust and act on the prediction <b>109</b>.</p><p id="p-0020" num="0019">In some embodiments, models <b>106</b>A, <b>106</b>B (hereinafter collectively referred to as model <b>106</b> or models <b>106</b>) may be artificial intelligence (AI) or machine learning (ML) models. In some embodiments, a model <b>106</b> may be trained on a set of training data to make decisions or predictions <b>108</b>A, <b>108</b>B without explicitly being programmed to do so.</p><p id="p-0021" num="0020">Prediction <b>109</b> represents either prediction <b>108</b>A or prediction <b>108</b>B as provided to user device <b>118</b> (e.g., mobile device, mobile phone, laptop, Internet of Things (IoT) device, desktop, tablet computer, or other computing device). As used herein, prediction <b>109</b> may be used to refer to prediction <b>108</b>A, prediction <b>108</b>B, or any or all predictions <b>108</b>A, <b>108</b>B generated by different models <b>106</b>A, <b>106</b>B.</p><p id="p-0022" num="0021">Based on prediction <b>109</b>, a user <b>104</b> may choose to perform a course of action. The user <b>104</b> may act on the prediction <b>109</b>, or if the user <b>104</b> believes the prediction <b>109</b> can be improved, the user <b>104</b> may also or alternatively provide feedback MMS <b>102</b> which may be used to improve the model <b>106</b> and its predictive capabilities.</p><p id="p-0023" num="0022">In some embodiments, if a new model <b>106</b> is trained or a user <b>104</b> has not worked with a particular model <b>106</b> before, and the model <b>106</b> provides a prediction <b>109</b> to user device <b>118</b>, the user <b>104</b> may not yet trust the model <b>106</b> and may be hesitant to act on the prediction <b>109</b>. If the user <b>104</b> does not trust the model <b>106</b>, the user <b>104</b> may either ignore, act contrary to the prediction <b>109</b>, or may waste time and resources redoing their own calculations only to arrive at the same conclusion as prediction <b>109</b>. No matter the course of action ultimately decided by the user <b>104</b>, an organization employing the user <b>104</b> wastes valuable time, money, productivity, and/or other resources if the user <b>104</b> does not understand and trust the models <b>106</b> being used to provide a prediction <b>109</b>.</p><p id="p-0024" num="0023">For the user <b>104</b> to trust and act on prediction <b>109</b>, the user <b>104</b> often wants to understand how a model <b>106</b> arrived at prediction <b>109</b>. While there may be technical tools available which can map the inner technical workings of a model <b>106</b> to generate a technical map <b>110</b> (referring to technical map <b>110</b>A and/or technical map <b>110</b>B). This technical map <b>110</b> will not be helpful to the non-technical user <b>104</b> if the user <b>104</b> is not already familiar with both technical terminology used to create or train a model <b>106</b> and/or the technical aspects of building and training a model <b>106</b>. It would be impossible to provide every user <b>104</b> with the type of training and knowledge that would be required to understand a technical map <b>110</b>.</p><p id="p-0025" num="0024">For example, unless user <b>104</b> has memorized and is familiar with all the various terminology of how fields and/or data of a dataset <b>112</b> are named and their relationships, the technical map <b>110</b> will be of little or no use or help to user <b>104</b> in understanding how a model <b>106</b> arrived at prediction <b>109</b>, thus will not resolve the issue. It is to be appreciated that the user <b>104</b> described herein is not a technical user who is capable of understanding a technical map <b>110</b>.</p><p id="p-0026" num="0025">To address or bridge the gap in understanding between the technical map <b>110</b>, which may be automatically generated by a technical mapping tool, and the user's existing non-technical knowledge, MMS <b>102</b> may general a natural language map (NLM) <b>111</b> (NLM <b>111</b> may refer to NLM <b>116</b>A, NLM <b>116</b>B, or both NLM <b>116</b>A and <b>116</b>B). As will be discussed in greater detail below, NLM <b>111</b> may convert technical terminology used in technical map <b>110</b> into language and terminology that is understandable by the user <b>104</b>. NLM <b>111</b> provides an understanding of how model <b>106</b> arrived at prediction <b>109</b> using non-technical terminology. This understanding, though NLM <b>111</b>, may also better enable user <b>104</b> to provide relevant and valuable feedback to improve model <b>106</b>.</p><p id="p-0027" num="0026">In some embodiments, model <b>106</b> may be trained on and/or refer to a dataset <b>112</b> in generating prediction <b>109</b>. Dataset <b>112</b> may include one or more tables, columns, or rows of a database or a spreadsheet. Dataset <b>112</b> may include training data and/or newly added or real-time data that was used or processed by a model <b>106</b> to generate prediction <b>109</b>. Dataset <b>112</b> may be stored or managed across a variety of geographic locations, computing devices, and by a variety of different administrators or data scientists.</p><p id="p-0028" num="0027">A technical tool may use the terminology of dataset <b>112</b> to generate a technical map <b>110</b>. An example technical tool that may be used to generate a technical map is LIME (local interpretable model-agnostic explanations), which may identify which portions, features, factors, or variables of the model <b>106</b> and/or dataset <b>112</b> that contributed or most contributed to the prediction <b>109</b>.</p><p id="p-0029" num="0028">MMS <b>102</b> may generate a NLM <b>111</b> based on the generated technical map <b>110</b>. In some embodiments, MMS <b>102</b> may refer to a feature store <b>114</b> in generating NLM <b>111</b>. Feature store <b>114</b> may be table, database, spreadsheet, or other file or group of files that includes translations of the technical terminology, shorthand, and/or codes that exist in dataset <b>112</b> that may otherwise be inexplicable or difficult to understand for a user <b>104</b> who was not involved in building or designing the dataset <b>112</b> and/or training a model <b>106</b>.</p><p id="p-0030" num="0029">In some embodiments, feature store <b>114</b> may be put together by various developers or administrators or other data scientists who are familiar with at least portions of the technical aspects of how data is organized, stored, and/or how models <b>106</b> operate. These experts may provide translations or definitions that may be beneficial to various non-technical users <b>104</b> who may be trying to understand how a model <b>106</b> arrives at prediction <b>109</b>.</p><p id="p-0031" num="0030">While this process of initially assembling a feature store <b>114</b> for a dataset <b>112</b> may be both a time and resource intensive process, that may require manual input by the data scientists, MMS <b>102</b> allows for the reusability of feature store <b>114</b> across various different models <b>106</b> that were trained on and/or use or otherwise refer to the same dataset <b>112</b> in generating predictions <b>108</b>A, <b>108</b>B. The reusability allows MMS to general NLMs <b>111</b> for a wide variety users <b>104</b> who are received predictions <b>109</b> from any number of models <b>106</b> referring to, at least in part, the same dataset <b>112</b> for which a feature store <b>114</b> exists. Furthermore, a single enhancement to an entry of feature store <b>114</b> may automatically be applied to all NLMs <b>116</b>A, <b>116</b>B across the different models <b>106</b>A, <b>106</b>B.</p><p id="p-0032" num="0031">Feature store <b>114</b> illustrates an example of a simple feature store <b>114</b> in which different values from dataset <b>112</b> each correspond to a single word. For example, the various column names C1, C2, and C3 translate to Name, City, and State, respectively. It is understood that a user <b>104</b> unfamiliar with the inner workings of a model <b>106</b>, will not be able to understand C1, C2, C3 as used in a technical map <b>110</b>, but could understand the terms Name, City, and State as used in a corresponding NLM <b>111</b>. In other embodiments, feature store <b>114</b> may include row and/or data value translations (not shown).</p><p id="p-0033" num="0032">In some embodiments, a single entry in feature store <b>114</b> may include multiple definitions, explanations, or translations. The varying translations, or even varying combinations of translations may vary based the model <b>106</b>A, <b>106</b>B and/or role/experience/technical understanding of a user <b>104</b> for which NLM <b>111</b> is being generated.</p><p id="p-0034" num="0033">For example, feature store <b>114</b> may include both entry 1 &#x201c;Name&#x201d; and entry 2 &#x201c;Customer Name&#x201d;. Depending on the role of a user <b>104</b> to which prediction <b>109</b> has been provided, the corresponding NLM <b>116</b>A for model <b>106</b>A may include either &#x201c;Name&#x201d; or &#x201c;Customer Name&#x201d;.</p><p id="p-0035" num="0034">Or, for example, feature store <b>114</b> may include both entry 1 &#x201c;Name&#x201d; and entry 2 &#x201c;Customer&#x201d;. Then, for example, if NLM <b>116</b>A is being generated for model <b>106</b>A, entry 1 may be used in NLM <b>116</b>A, resulting in &#x201c;Name&#x201d;. However, if NLM <b>116</b>B is being generated for model <b>106</b>B, entry 2+entry 1 may be used in NLM <b>116</b>B, resulting in &#x201c;Customer Name&#x201d;.</p><p id="p-0036" num="0035">In some embodiments, the same entries in feature store <b>114</b> can be used across different data sets <b>112</b>. For example, both a first column of a first table and a fourth column of a third table of dataset <b>112</b> may refer to or include a pointer to the same &#x201c;Name&#x201d; entry in feature store <b>114</b>. This cross-referencing may save time and resources in both generating, storing, and using feature store <b>114</b> for generating NLMs <b>116</b>A, <b>116</b>B. One of the advantages of MMS <b>102</b> is that feature store <b>114</b> may be operable across various different models <b>106</b> so that a new feature store <b>114</b> does not have to be created for each new model <b>106</b>A, <b>106</b>B that is trained or used.</p><p id="p-0037" num="0036">In some embodiments, user <b>104</b> may view prediction <b>109</b> on user device <b>118</b> and may submit a request <b>120</b> for NLM <b>111</b>. Then, for example, upon receiving the request <b>120</b>, MMS <b>102</b> may identify the role/experience of user <b>104</b>, generate or retrieve a technical map <b>110</b>, and generate and output a corresponding NLM <b>111</b> based on the entries in feature store <b>114</b>.</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is another block diagram <b>200</b> illustrating example operations of a model mapping and enrichment system, according to some additional example embodiments. <figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates how the same feature store <b>214</b> for a dataset <b>212</b> may be used across any different number of models <b>206</b>A, <b>206</b>B, <b>206</b>C to generate corresponding NLMs <b>216</b>A, <b>216</b>B, <b>216</b>C from technical maps <b>210</b>A, <b>210</b>B, <b>210</b>C for a variety of users with different technical understandings or roles (not shown).</p><p id="p-0039" num="0038">Column mapping <b>202</b> may refer to performing a mapping of column names to entries in feature store <b>214</b> (as illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>). Feature mapping may refer to performing a mapping of row or column entries to corresponding entries in feature store <b>214</b>. For example, the city value &#x201c;Alexandria&#x201d; may be referred to as &#x201c;A1&#x201d; in dataset <b>212</b>. Then, in generating an NLMs <b>216</b>A, <b>216</b>B and <b>216</b>C, &#x201c;A1&#x201d; in the corresponding technical maps <b>210</b>A, <b>210</b>B, and <b>210</b>C may be replaced with &#x201c;Alexandria&#x201d; as identified from feature store <b>214</b>. This is an extension of reusability of feature store <b>214</b> as described above.</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a block diagram illustrating an example dataset <b>312</b>, according to some example embodiments. As can be seen in the example dataset <b>312</b>, unless a user was involved in deciding the shorthand or codes of the column names or field/data values, or has otherwise memorized them (which may impossible because there may be billions of values or codes), providing a non-technical user a description that a model relied on OIPL or value A93 in generating a prediction <b>109</b> will not be beneficial to a user in understanding how a model relying on dataset <b>312</b> arrived at its prediction.</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIGS. <b>4</b>A and <b>4</b>B</figref> are block diagrams illustrating an examples of feature stores <b>414</b>A, <b>414</b>B, according to some example embodiments. <figref idref="DRAWINGS">FIG. <b>4</b>A</figref> illustrates an example column mapping feature store <b>414</b>A. As illustrated, the column mapping feature store <b>414</b>A includes descriptions or translations of the various column names of the dataset <b>312</b> (of <figref idref="DRAWINGS">FIG. <b>3</b></figref>). For example, the column name CRAM includes the translation or description &#x201c;credit amount.&#x201d; Then, for example, if CRAM was used in technical map <b>110</b>, &#x201c;credit amount&#x201d; would be used in corresponding natural language map <b>116</b>.</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>4</b>B</figref> illustrates an example feature or data mapping feature store <b>414</b>A. As illustrated, the data mapping feature store <b>414</b>A includes descriptions or translations of the various data value codes or shorthand of the dataset <b>312</b> (of <figref idref="DRAWINGS">FIG. <b>3</b></figref>).</p><p id="p-0043" num="0042">In some embodiments, data mapping feature store <b>414</b>A may include both the column id/name/code/shorthand and its corresponding data value entry (e.g., &#x201c;CHRI&#x201d;, &#x201c;A32&#x201d;). So for the column CHRI, if the code A32 is in a record or row, the value A32 is replaced with &#x201c;existing credits paid back duly till now&#x201d;. This dual level of specificity allows the same data code to be reused across different columns while corresponding to different values. For example, a different column may include the same data code A32 which may correspond to the value &#x201c;New York City&#x201d;.</p><p id="p-0044" num="0043">In some embodiments, each data code may have its own entry in data mapping feature store <b>414</b>B and may be column agnostic. For example, data mapping feature store <b>414</b>B may include the A value codes without the corresponding column names or column codes. Or, in some embodiments, the data codes may correspond to particular tables in data mapping feature store <b>414</b>B. So, for example, data mapping feature store <b>414</b>B may include a table name (in addition to or instead of column name) and a corresponding data code or data value, and the same data code may have different entries or values in different tables.</p><p id="p-0045" num="0044"><figref idref="DRAWINGS">FIGS. <b>5</b>A and <b>5</b>B</figref> are block diagrams illustrating example functionality related to a model mapping and enrichment system, according to some example embodiments. In <figref idref="DRAWINGS">FIG. <b>5</b>A</figref>, MMS <b>102</b> may generate or retrieve a technical map <b>510</b>A for a model <b>106</b> relying on dataset <b>312</b> (of <figref idref="DRAWINGS">FIG. <b>3</b></figref>). As may be seen, the technical map includes various impossible to comprehend codes corresponding to various values of dataset <b>312</b> and their relative scores or weights in producing a particular prediction (Good Credit).</p><p id="p-0046" num="0045">A non-technical user would not find the information of technical map <b>510</b>A beneficial when trying to understand or improve a model <b>106</b>. MMS <b>102</b> may then generate natural language map <b>516</b>A using feature stores <b>414</b>A and <b>414</b>B. The natural language map <b>516</b>A would then reveal to the non-technical user how the corresponding model <b>106</b> generated the Good Credit prediction. <figref idref="DRAWINGS">FIG. <b>5</b>B</figref> is similar to <figref idref="DRAWINGS">FIG. <b>5</b>A</figref>, except that it illustrates a technical map <b>510</b>B and a corresponding natural language map <b>516</b>B for a Bad Credit prediction.</p><p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flowchart <b>600</b> illustrating example operations for functionality related to a model mapping and enrichment system, according to some embodiments. Method <b>600</b> can be performed by processing logic that can comprise hardware (e.g., circuitry, dedicated logic, programmable logic, microcode, etc.), software (e.g., instructions executing on a processing device), or a combination thereof. It is to be appreciated that not all steps may be needed to perform the disclosure provided herein. Further, some of the steps may be performed simultaneously, or in a different order than shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, as will be understood by a person of ordinary skill in the art. Method <b>600</b> shall be described with reference to the figures.</p><p id="p-0048" num="0047">In <b>610</b>, it is determined that a first prediction from a first machine model has been generated based on a dataset comprising a plurality of attributes. For example, user <b>104</b> may receive a display of prediction <b>109</b> on user device <b>118</b>. Prediction <b>109</b> may correspond to prediction <b>108</b>A generated by model <b>106</b>A based on dataset <b>112</b>.</p><p id="p-0049" num="0048">In <b>620</b>, a technical map is generated, the technical map identifying a first subset of attributes of the plurality of attributes used to generate the first prediction by the first machine model. For example, MMS <b>102</b> may generate technical map <b>110</b>A for model <b>106</b>A that indicates which columns and/or rows or data values were used to generate prediction <b>108</b>A. In an embodiment, technical map <b>110</b>A may be generated responsive to a user request <b>120</b> for an understanding or breakdown of how prediction <b>109</b> was generated.</p><p id="p-0050" num="0049">In <b>630</b>, natural language translations are identified from a feature store, the natural language translations corresponding to at least a portion of the first subset of attributes used to generate the first prediction by the first machine model. For example, <figref idref="DRAWINGS">FIGS. <b>4</b>A and <b>4</b>B</figref> illustrate example embodiments of feature stores <b>414</b>A and <b>414</b>B (corresponding to feature store <b>114</b>) for a dataset <b>312</b>. Feature stores <b>414</b>A, <b>414</b>B may include new terminology or explanations of the data values or codes from dataset <b>312</b> (<figref idref="DRAWINGS">FIG. <b>3</b></figref>).</p><p id="p-0051" num="0050">In <b>640</b>, a natural language map of the first subset of attributes is generated based on the natural language translations. For example, at least a portion of the values of dataset <b>312</b>, as used in a corresponding technical map (e.g., <b>510</b>A, <b>510</b>B) may include corresponding entries in a feature store <b>414</b>A, <b>414</b>B. MMS <b>102</b> may generate the corresponding natural language maps <b>516</b>A, <b>516</b>B based on combining technical maps <b>510</b>A, <b>510</b>B with the corresponding entries from feature stores <b>414</b>A, <b>414</b>B&#x2014;which may include multiple entries depending on a role of a user <b>104</b>. In an embodiment, MMS <b>102</b> may replace the codes of technical maps <b>510</b>A, <b>510</b>B with the corresponding the terminology from feature stores <b>414</b>A, <b>414</b>B.</p><p id="p-0052" num="0051">In <b>650</b>, the natural language map may be provided with the first prediction. For example, MMS <b>102</b> may provide NLM <b>111</b> for display on user device <b>118</b> with prediction <b>109</b>. In some embodiments, the same feature store <b>114</b> may be used to generate natural language maps <b>116</b> across different data models <b>106</b> that refer back to the same dataset <b>112</b> for which feature store <b>114</b> was generated.</p><p id="p-0053" num="0052">Various embodiments may be implemented, for example, using one or more well-known computer systems, such as computer system <b>700</b> shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>. One or more computer systems <b>700</b> may be used, for example, to implement any of the embodiments discussed herein, as well as combinations and sub-combinations thereof.</p><p id="p-0054" num="0053">Computer system <b>700</b> may include one or more processors (also called central processing units, or CPUs), such as a processor <b>704</b>. Processor <b>704</b> may be connected to a communication infrastructure or bus <b>706</b>.</p><p id="p-0055" num="0054">Computer system <b>700</b> may also include customer input/output device(s) <b>703</b>, such as monitors, keyboards, pointing devices, etc., which may communicate with communication infrastructure <b>706</b> through customer input/output interface(s) <b>702</b>.</p><p id="p-0056" num="0055">One or more of processors <b>704</b> may be a graphics processing unit (GPU). In an embodiment, a GPU may be a processor that is a specialized electronic circuit designed to process mathematically intensive applications. The GPU may have a parallel structure that is efficient for parallel processing of large blocks of data, such as mathematically intensive data common to computer graphics applications, images, videos, etc.</p><p id="p-0057" num="0056">Computer system <b>700</b> may also include a main or primary memory <b>708</b>, such as random-access memory (RAM). Main memory <b>708</b> may include one or more levels of cache. Main memory <b>708</b> may have stored therein control logic (i.e., computer software) and/or data.</p><p id="p-0058" num="0057">Computer system <b>700</b> may also include one or more secondary storage devices or memory <b>710</b>. Secondary memory <b>710</b> may include, for example, a hard disk drive <b>712</b> and/or a removable storage device or drive <b>714</b>. Removable storage drive <b>714</b> may be a floppy disk drive, a magnetic tape drive, a compact disk drive, an optical storage device, tape backup device, and/or any other storage device/drive.</p><p id="p-0059" num="0058">Removable storage drive <b>714</b> may interact with a removable storage unit <b>718</b>. Removable storage unit <b>718</b> may include a computer usable or readable storage device having stored thereon computer software (control logic) and/or data. Removable storage unit <b>718</b> may be a floppy disk, magnetic tape, compact disk, DVD, optical storage disk, and/any other computer data storage device. Removable storage drive <b>714</b> may read from and/or write to removable storage unit <b>718</b>.</p><p id="p-0060" num="0059">Secondary memory <b>710</b> may include other means, devices, components, instrumentalities or other approaches for allowing computer programs and/or other instructions and/or data to be accessed by computer system <b>700</b>. Such means, devices, components, instrumentalities or other approaches may include, for example, a removable storage unit <b>722</b> and an interface <b>720</b>. Examples of the removable storage unit <b>722</b> and the interface <b>720</b> may include a program cartridge and cartridge interface (such as that found in video game devices), a removable memory chip (such as an EPROM or PROM) and associated socket, a memory stick and USB port, a memory card and associated memory card slot, and/or any other removable storage unit and associated interface.</p><p id="p-0061" num="0060">Computer system <b>700</b> may further include a communication or network interface <b>724</b>. Communication interface <b>724</b> may enable computer system <b>700</b> to communicate and interact with any combination of external devices, external networks, external entities, etc. (individually and collectively referenced by reference number <b>728</b>). For example, communication interface <b>724</b> may allow computer system <b>700</b> to communicate with external or remote devices <b>728</b> over communications path <b>726</b>, which may be wired and/or wireless (or a combination thereof), and which may include any combination of LANs, WANs, the Internet, etc. Control logic and/or data may be transmitted to and from computer system <b>700</b> via communication path <b>726</b>.</p><p id="p-0062" num="0061">Computer system <b>700</b> may also be any of a personal digital assistant (PDA), desktop workstation, laptop or notebook computer, netbook, tablet, smart phone, smart watch or other wearable, appliance, part of the Internet-of-Things, and/or embedded system, to name a few non-limiting examples, or any combination thereof.</p><p id="p-0063" num="0062">Computer system <b>700</b> may be a client or server, accessing or hosting any applications and/or data through any delivery paradigm, including but not limited to remote or distributed cloud computing solutions; local or on-premises software (&#x201c;on-premise&#x201d; and/or cloud-based solutions); &#x201c;as a service&#x201d; models (e.g., content as a service (CaaS), digital content as a service (DCaaS), software as a service (SaaS), managed software as a service (MSaaS), platform as a service (PaaS), desktop as a service (DaaS), framework as a service (FaaS), backend as a service (BaaS), mobile backend as a service (MBaaS), infrastructure as a service (IaaS), etc.); and/or a hybrid model including any combination of the foregoing examples or other services or delivery paradigms.</p><p id="p-0064" num="0063">Any applicable data structures, file formats, and schemas in computer system <b>700</b> may be derived from standards including but not limited to JavaScript Object Notation (JSON), Extensible Markup Language (XML), Yet Another Markup Language (YAML), Extensible Hypertext Markup Language (XHTML), Wireless Markup Language (WML), MessagePack, XML User Interface Language (XUL), or any other functionally similar representations alone or in combination. Alternatively, proprietary data structures, formats or schemas may be used, either exclusively or in combination with known or open standards.</p><p id="p-0065" num="0064">In some embodiments, a tangible, non-transitory apparatus or article of manufacture comprising a tangible, non-transitory computer useable or readable medium having control logic (software) stored thereon may also be referred to herein as a computer program product or program storage device. This includes, but is not limited to, computer system <b>700</b>, main memory <b>708</b>, secondary memory <b>710</b>, and removable storage units <b>718</b> and <b>722</b>, as well as tangible articles of manufacture embodying any combination of the foregoing. Such control logic, when executed by one or more data processing devices (such as computer system <b>700</b>), may cause such data processing devices to operate as described herein.</p><p id="p-0066" num="0065">Based on the teachings contained in this disclosure, it will be apparent to persons skilled in the relevant art(s) how to make and use embodiments of this disclosure using data processing devices, computer systems and/or computer architectures other than that shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>. In particular, embodiments can operate with software, hardware, and/or operating system implementations other than those described herein.</p><p id="p-0067" num="0066">It is to be appreciated that the Detailed Description section, and not any other section, is intended to be used to interpret the claims. Other sections can set forth one or more but not all exemplary embodiments as contemplated by the inventor(s), and thus, are not intended to limit this disclosure or the appended claims in any way.</p><p id="p-0068" num="0067">While this disclosure describes exemplary embodiments for exemplary fields and applications, it should be understood that the disclosure is not limited thereto. Other embodiments and modifications thereto are possible, and are within the scope and spirit of this disclosure. For example, and without limiting the generality of this paragraph, embodiments are not limited to the software, hardware, firmware, and/or entities illustrated in the figures and/or described herein. Further, embodiments (whether or not explicitly described herein) have significant utility to fields and applications beyond the examples described herein.</p><p id="p-0069" num="0068">Embodiments have been described herein with the aid of functional building blocks illustrating the implementation of specified functions and relationships thereof. The boundaries of these functional building blocks have been arbitrarily defined herein for the convenience of the description. Alternate boundaries can be defined as long as the specified functions and relationships (or equivalents thereof) are appropriately performed. Also, alternative embodiments can perform functional blocks, steps, operations, methods, etc. using orderings different than those described herein.</p><p id="p-0070" num="0069">References herein to &#x201c;one embodiment,&#x201d; &#x201c;an embodiment,&#x201d; &#x201c;an example embodiment,&#x201d; or similar phrases, indicate that the embodiment described can include a particular feature, structure, or characteristic, but every embodiment can not necessarily include the particular feature, structure, or characteristic. Moreover, such phrases are not necessarily referring to the same embodiment. Further, when a particular feature, structure, or characteristic is described in connection with an embodiment, it would be within the knowledge of persons skilled in the relevant art(s) to incorporate such feature, structure, or characteristic into other embodiments whether or not explicitly mentioned or described herein. Additionally, some embodiments can be described using the expression &#x201c;coupled&#x201d; and &#x201c;connected&#x201d; along with their derivatives. These terms are not necessarily intended as synonyms for each other. For example, some embodiments can be described using the terms &#x201c;connected&#x201d; and/or &#x201c;coupled&#x201d; to indicate that two or more elements are in direct physical or electrical contact with each other. The term &#x201c;coupled,&#x201d; however, can also mean that two or more elements are not in direct contact with each other, but yet still co-operate or interact with each other.</p><p id="p-0071" num="0070">The breadth and scope of this disclosure should not be limited by any of the above-described exemplary embodiments, but should be defined only in accordance with the following claims and their equivalents.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method comprising:<claim-text>determining that a first prediction from a first machine model has been generated based on a dataset comprising a plurality of attributes;</claim-text><claim-text>automatically generating, by at least one processor, a technical map identifying a first subset of attributes of the plurality of attributes used to generate the first prediction by the first machine model;</claim-text><claim-text>identifying, from a feature store, natural language translations corresponding to at least a portion of the first subset of attributes used to generate the first prediction by the first machine model, wherein the natural language translations include one of new terminology or explanations corresponding to the portion of the first subset of attributes;</claim-text><claim-text>automatically generating, by the at least one processor, a natural language map of the first subset of attributes based on the natural language translations, wherein the portion of the first subset of attributes have been replaced, on the technical map, with the corresponding new terminology or explanations from the feature store; and</claim-text><claim-text>providing the natural language map with the first prediction.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>determining that a second prediction from a second machine model has been generated based on the dataset, wherein the second prediction is different from the first prediction, and wherein the second machine model is different from the first machine model; and</claim-text><claim-text>automatically generating a second technical map identifying a second subset of attributes of the plurality of attributes used to generate the second prediction by the second machine model, wherein at least one attribute of the second subset of attributes is an identical attribute to at least one attribute of the first subset of attributes, and wherein the feature store includes a natural language translation of the identical attribute.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the first machine model and the second machine model are both machine learning models.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the dataset comprises one or more column codes corresponding to column names corresponding to columns of the dataset, and wherein the feature store includes natural language translations for each of the one or more column codes.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the dataset comprises one or more data codes corresponding to data stored across a plurality of records of the dataset and in the one or more columns, and wherein the feature store includes natural language translations for each of the one or more data codes.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the technical map includes a stemmed version of a first attribute from the first subset of attributes, wherein the stemmed version includes a shorter version of the first attribute relative to a longer version of the first attribute as it is stored in the dataset.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the generating the natural language map comprises reverse mapping the stemmed version of the first attribute to the longer version of the first attribute as it is stored in the dataset, wherein the longer version is included in the natural language map.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the stemmed version includes a single word, and wherein the longer version includes a plurality of words.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>receiving a request from a user to generate the natural language map, wherein the generating the technical map and the generating the natural language map are both performed responsive to the request.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. A system, comprising:<claim-text>a memory; and</claim-text><claim-text>at least one processor coupled to the memory and configured to perform instructions that cause the at least one processor to perform operations comprising:</claim-text><claim-text>determining that a first prediction from a first machine model has been generated based on a dataset comprising a plurality of attributes;</claim-text><claim-text>automatically generating a technical map identifying a first subset of attributes of the plurality of attributes used to generate the first prediction by the first machine model;</claim-text><claim-text>identifying, from a feature store, natural language translations corresponding to at least a portion of the first subset of attributes used to generate the first prediction by the first machine model, wherein the natural language translations include one of new terminology or explanations corresponding to the portion of the first subset of attributes;</claim-text><claim-text>automatically generating a natural language map of the first subset of attributes based on the natural language translations, wherein the portion of the first subset of attributes have been replaced, on the technical map, with the corresponding new terminology or explanations from the feature store; and</claim-text><claim-text>providing the natural language map with the first prediction.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, the operations further comprising:<claim-text>determining that a second prediction from a second machine model has been generated based on the dataset, wherein the second prediction is different from the first prediction, and wherein the second machine model is different from the first machine model; and</claim-text><claim-text>automatically generating a second technical map identifying a second subset of attributes of the plurality of attributes used to generate the second prediction by the second machine model, wherein at least one attribute of the second subset of attributes is an identical attribute to at least one attribute of the first subset of attributes, wherein the feature store includes a natural language translation of the identical attribute, and wherein the first machine model and the second machine model are both machine learning models.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the dataset comprises one or more column codes corresponding to column names corresponding to columns of the dataset, and wherein the feature store includes natural language translations for each of the one or more column codes.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the dataset comprises one or more data codes corresponding to data stored across a plurality of records of the dataset and in the one or more columns, and wherein the feature store includes natural language translations for each of the one or more data codes.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the technical map includes a stemmed version of a first attribute from the first subset of the plurality of attributes, wherein the stemmed version includes a shorter version of the first attribute relative to a longer version of the first attribute as it is stored in the dataset.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The system of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the generating the natural language map comprises reverse mapping the stemmed version of the first attribute to the longer version of the first attribute as it is stored in the dataset, wherein the longer version is included in the natural language map.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The system of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the stemmed version includes a single word, and wherein the longer version includes a plurality of words.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, the operations further comprising:<claim-text>receiving a request from a user to generate the natural language map, wherein the generating the technical map and the generating the natural language map are both performed responsive to the request.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. A non-transitory computer-readable medium having instructions stored thereon that, when executed by at least one computing device, cause the at least one computing device to perform operations comprising:<claim-text>determining that a first prediction from a first machine model has been generated based on a dataset comprising a plurality of attributes;</claim-text><claim-text>automatically generating a technical map identifying a first subset of attributes of the plurality of attributes used to generate the first prediction by the first machine model;</claim-text><claim-text>identifying, from a feature store, natural language translations corresponding to at least a portion of the first subset of attributes used to generate the first prediction by the first machine model, wherein the natural language translations include one of new terminology or explanations corresponding to the portion of the first subset of attributes;</claim-text><claim-text>automatically generating a natural language map of the first subset of attributes based on the natural language translations, wherein the portion of the first subset of attributes have been replaced, on the technical map, with the corresponding new terminology or explanations from the feature store; and</claim-text><claim-text>providing the natural language map with the first prediction.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the operations further comprise:<claim-text>determining that a second prediction from a second machine model has been generated based on the dataset, wherein the second prediction is different from the first prediction, and wherein the second machine model is different from the first machine model; and</claim-text><claim-text>automatically generating a second technical map identifying a second subset of attributes of the plurality of attributes used to generate the second prediction by the second machine model, wherein at least one attribute of the second subset of attributes is an identical attribute to at least one attribute of the first subset of attributes, wherein the feature store includes a natural language translation of the identical attribute, and wherein the first machine model and the second machine model are both machine learning models.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the dataset comprises one or more column codes corresponding to column names corresponding to columns of the dataset, and wherein the feature store includes natural language translations for each of the one or more column codes.</claim-text></claim></claims></us-patent-application>