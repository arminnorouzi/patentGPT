<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004604A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004604</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17854348</doc-number><date>20220630</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>93</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20190101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>93</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc></classifications-cpc><invention-title id="d2e43">AI-AUGMENTED AUDITING PLATFORM INCLUDING TECHNIQUES FOR AUTOMATED DOCUMENT PROCESSING</invention-title><us-related-documents><us-provisional-application><document-id><country>US</country><doc-number>63217119</doc-number><date>20210630</date></document-id></us-provisional-application><us-provisional-application><document-id><country>US</country><doc-number>63217123</doc-number><date>20210630</date></document-id></us-provisional-application><us-provisional-application><document-id><country>US</country><doc-number>63217127</doc-number><date>20210630</date></document-id></us-provisional-application><us-provisional-application><document-id><country>US</country><doc-number>63217131</doc-number><date>20210630</date></document-id></us-provisional-application><us-provisional-application><document-id><country>US</country><doc-number>63217134</doc-number><date>20210630</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>PricewaterhouseCoopers LLP</orgname><address><city>New York</city><state>NY</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>LI</last-name><first-name>Chung-Sheng</first-name><address><city>Scarsdale</city><state>NY</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>CHENG</last-name><first-name>Winnie</first-name><address><city>West New York</city><state>NJ</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>FLAVELL</last-name><first-name>Mark John</first-name><address><city>Madison</city><state>NJ</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>HALLMARK</last-name><first-name>Lori Marie</first-name><address><city>Xenia</city><state>OH</state><country>US</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>LIZOTTE</last-name><first-name>Nancy Alayne</first-name><address><city>Saline</city><state>MI</state><country>US</country></address></addressbook></inventor><inventor sequence="05" designation="us-only"><addressbook><last-name>RAO</last-name><first-name>Anand Srinivasa</first-name><address><city>Lexington</city><state>MA</state><country>US</country></address></addressbook></inventor><inventor sequence="06" designation="us-only"><addressbook><last-name>LEONG</last-name><first-name>Kevin Ma</first-name><address><city>Randolph</city><state>NJ</state><country>US</country></address></addressbook></inventor><inventor sequence="07" designation="us-only"><addressbook><last-name>ZHU</last-name><first-name>Di</first-name><address><city>Jersey City</city><state>NJ</state><country>US</country></address></addressbook></inventor><inventor sequence="08" designation="us-only"><addressbook><last-name>DELILLE</last-name><first-name>Timothy</first-name><address><city>New York</city><state>NY</state><country>US</country></address></addressbook></inventor><inventor sequence="09" designation="us-only"><addressbook><last-name>RAMIREZ</last-name><first-name>Maria Jesus Perez</first-name><address><city>New York</city><state>NY</state><country>US</country></address></addressbook></inventor><inventor sequence="10" designation="us-only"><addressbook><last-name>WAN</last-name><first-name>Yuan</first-name><address><city>Irvine</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="11" designation="us-only"><addressbook><last-name>SINGH</last-name><first-name>Ratna Raj</first-name><address><city>Mumbai</city><country>IN</country></address></addressbook></inventor><inventor sequence="12" designation="us-only"><addressbook><last-name>BANSAL</last-name><first-name>Vishakha</first-name><address><city>Meerut</city><country>IN</country></address></addressbook></inventor><inventor sequence="13" designation="us-only"><addressbook><last-name>HODA</last-name><first-name>Shaz</first-name><address><city>New York</city><state>NY</state><country>US</country></address></addressbook></inventor><inventor sequence="14" designation="us-only"><addressbook><last-name>SINGH</last-name><first-name>Amitoj</first-name><address><city>New Delhi</city><country>IN</country></address></addressbook></inventor><inventor sequence="15" designation="us-only"><addressbook><last-name>ZANJ</last-name><first-name>Siddhesh Shivaji</first-name><address><city>Mumbai</city><country>IN</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>PricewaterhouseCoopers LLP</orgname><role>02</role><address><city>New York</city><state>NY</state><country>US</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Systems and methods for automated document processing for use in AI-augmented auditing platforms are provided. A system for determining the composition of document bundles extracts substantive content information and metadata information from a document bundle and generates, based on the extracted information regarding a composition of the document bundle. A system for validating signatures in documents extracts data representing a spatial location for respective signatures and generates a confidence level for respective signatures, and determines, based on location and confidence level, whether signature criteria are met. A system for extracting information from documents applies a set of data conversion processing steps to a plurality received documents to generate structured data, and then applies a set of knowledge-based modeling processing steps to the structured data to generating output data extracted from the plurality of electronic documents.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="70.70mm" wi="158.75mm" file="US20230004604A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="233.60mm" wi="126.07mm" orientation="landscape" file="US20230004604A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="220.13mm" wi="140.63mm" orientation="landscape" file="US20230004604A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="260.35mm" wi="167.47mm" orientation="landscape" file="US20230004604A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="208.28mm" wi="104.39mm" orientation="landscape" file="US20230004604A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="144.44mm" wi="134.62mm" file="US20230004604A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="221.91mm" wi="166.45mm" orientation="landscape" file="US20230004604A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="201.76mm" wi="151.21mm" orientation="landscape" file="US20230004604A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="254.42mm" wi="164.25mm" orientation="landscape" file="US20230004604A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="224.03mm" wi="170.10mm" file="US20230004604A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="228.43mm" wi="169.08mm" file="US20230004604A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="230.29mm" wi="168.49mm" file="US20230004604A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="228.18mm" wi="169.42mm" file="US20230004604A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="227.92mm" wi="169.16mm" file="US20230004604A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="228.18mm" wi="168.74mm" file="US20230004604A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="228.43mm" wi="169.67mm" file="US20230004604A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="228.26mm" wi="169.33mm" file="US20230004604A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="228.26mm" wi="167.72mm" file="US20230004604A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="228.01mm" wi="165.27mm" file="US20230004604A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00019" num="00019"><img id="EMI-D00019" he="252.56mm" wi="122.51mm" orientation="landscape" file="US20230004604A1-20230105-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00020" num="00020"><img id="EMI-D00020" he="160.44mm" wi="155.62mm" file="US20230004604A1-20230105-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00021" num="00021"><img id="EMI-D00021" he="220.13mm" wi="123.02mm" orientation="landscape" file="US20230004604A1-20230105-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00022" num="00022"><img id="EMI-D00022" he="179.24mm" wi="150.88mm" file="US20230004604A1-20230105-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00023" num="00023"><img id="EMI-D00023" he="224.03mm" wi="166.96mm" file="US20230004604A1-20230105-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00024" num="00024"><img id="EMI-D00024" he="197.61mm" wi="104.73mm" orientation="landscape" file="US20230004604A1-20230105-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00025" num="00025"><img id="EMI-D00025" he="155.62mm" wi="124.54mm" file="US20230004604A1-20230105-D00025.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION(S)</heading><p id="p-0002" num="0001">This application claims the benefit of U.S. Provisional Application No. 63/217,119 filed Jun. 30, 2021; U.S. Provisional Application No. 63/217,123 filed Jun. 30, 2021; U.S. Provisional Application No. 63/217,127 filed Jun. 30, 2021; U.S. Provisional Application No. 63/217,131 filed Jun. 30, 2021; and U.S. Provisional Application No. 63/217,134, filed Jun. 30, 2021, the entire contents of each of which are incorporated herein by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">FIELD</heading><p id="p-0003" num="0002">This relates generally to document processing, and more specifically to AI-augmented auditing platform including techniques for automated document processing.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">AI-augmented auditing platforms benefit from automated document processing techniques including automated document classification and clustering, automated signature detection and validation, and automated information extraction from PDF documents and other document formats.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0005" num="0004">Known techniques for document classification do not adequately leverage context data to guide document classification, especially in the context of audit processes. As described herein, context data that is available in audit processes may be effectively and efficiently leveraged in order to improve the accuracy and efficiency of document classification and clustering for use in AI-augmented auditing platforms.</p><p id="p-0006" num="0005">In some embodiments, a system for automated document processing may be configured to perform automated document classification (e.g., classifying documents according to different document types) and/or document bundling. As described herein, the system may apply a set of AI methods to leverage context data in combination with a multi-page document classification ML models to accurately determine the composition of document bundles, such as document bundles received by an AI-augmented auditing platform as part of an audit review process.</p><p id="p-0007" num="0006">Document processing, for example for the purposes of assurance, often requires verifying that a signature (or initials) appear in specific area or in association with a specific topic within a document. There may be more than one section, more than one topic, and/or more than one signature present in a single document or document bundle. Known techniques for signature detection require manual review and verification, which is inefficient and inaccurate and does not allow for processing documents at scale.</p><p id="p-0008" num="0007">In some embodiments, a system for automated document processing may be configured to perform automated signature detection, including by applying AI models that learn where signatures are likely to occur on a given document type. During document ingestion and processing, the system may then validate that documents being processed do in fact have signatures at the expected/required locations within the documents. The systems and methods provided herein may be used to automatically process documents to determine whether said documents provide evidence, with required and sufficient signatures, to meet vouching criteria for shipments of goods, receipt of goods, agreement to contracts, or the like.</p><p id="p-0009" num="0008">Documents stored in PDF format, image format, and other formats can contain a lot of information, and extracting said information can be an important part of AI-driven assurance processes and other tasks performed by AI-augmented auditing platforms. For example, an AI-driven assurance process may rely on automated extraction of data stored in PDFs, such that invoices and/or other pieces of piece of information (e.g., evidentiary information) may be fully considered, correctly understood, and applied as part of the audit process. Efficient processing of documents may enable an audit process to exhaustively consider all available evidentiary (e.g., documentary) data, rather than simply considering a small sample thereof.</p><p id="p-0010" num="0009">In some embodiments, document processing and information-extraction systems described herein leverage a unique combination of (a) natural language processing using semantic and morphological analysis with (b) weak labelling based on fuzzy matching and deep learning based on text and computer vision. The combined model, configured to extract information from PDFs, may be provided an ensemble of NLP, text, and computer vision.</p><p id="p-0011" num="0010">In some embodiments a first system is provided, the first system being for determining the composition of document bundles, the first system comprising one or more processors configured to cause the first system to: receive first input data comprising a document bundle; extract, from the document bundle, first information comprising substantive content of one or more documents of the document bundle; extract, from the document bundle, second information comprising metadata associated with one or more documents of the document bundle; generate, based on the first information and the second information, output data representing a composition of the document bundle.</p><p id="p-0012" num="0011">In some embodiments of the first system, the output data representing a composition of the document bundle represents one or more delineations between page boundaries in the document bundle.</p><p id="p-0013" num="0012">In some embodiments of the first system, generating the output data is further based on information obtained from an ERP system of an entity associated with the document bundle.</p><p id="p-0014" num="0013">In some embodiments of the first system, the metadata comprises one or more of: a file name, a file extension, a file creator, a file date, and information regarding an automation process flow for acquiring the data.</p><p id="p-0015" num="0014">In some embodiments of the first system, extracting the first information comprises applying embedded object type detection.</p><p id="p-0016" num="0015">In some embodiments of the first system, generating the output data comprises applying a page similarity assessment model to a plurality of pages of the document bundle.</p><p id="p-0017" num="0016">In some embodiments, a first non-transitory computer-readable storage medium is provided, the first non-transitory computer-readable storage medium storing instructions for determining the composition of document bundles, the instructions configured to be executed by one or more processors of a system to cause the system to: receive first input data comprising a document bundle; extract, from the document bundle, first information comprising substantive content of one or more documents of the document bundle; extract, from the document bundle, second information comprising metadata associated with one or more documents of the document bundle; generate, based on the first information and the second information, output data representing a composition of the document bundle.</p><p id="p-0018" num="0017">In some embodiments, a first method is provided, the first method being for determining the composition of document bundles, wherein the first method is performed by a system comprising one or more processors, the first method comprising: receiving first input data comprising a document bundle; extracting, from the document bundle, first information comprising substantive content of one or more documents of the document bundle; extracting, from the document bundle, second information comprising metadata associated with one or more documents of the document bundle; generating, based on the first information and the second information, output data representing a composition of the document bundle.</p><p id="p-0019" num="0018">In some embodiments, a second system is provided, the second system being for validating signatures in documents, the second system comprising one or more processors configured to cause the second system to: receive an electronic document comprising one or more signatures; apply one or more signature-extraction model to the electronic document to generate, for each of the one or more signatures in the electronic document, data representing a spatial location for the respective signature and a confidence level for the respective signature; determine, based on the data representing the spatial location and the confidence level, whether the electronic document satisfies a set of signature criteria.</p><p id="p-0020" num="0019">In some embodiments of the second system, the one or more signature-extraction models comprise a first signature-extraction model configured to recognize signatures regardless of spatial location.</p><p id="p-0021" num="0020">In some embodiments of the second system, the one or more signature-extraction models comprise a second signature-extraction model configured to recognize signatures based on in-document spatial location.</p><p id="p-0022" num="0021">In some embodiments of the second system, applying the second signature-extraction model comprises: determining a predicted spatial location within the electronic document based on one or more of a structure, format, and type of the electronic document; and extracting a signature from the predicted spatial location.</p><p id="p-0023" num="0022">In some embodiments of the second system, determining whether the electronic document satisfies the set of signature criteria comprises determining whether a signature appears in the electronic document at a required spatial location.</p><p id="p-0024" num="0023">In some embodiments of the second system, determining whether the electronic document satisfies the set of signature criteria comprises determining the confidence level exceeds a predefined threshold.</p><p id="p-0025" num="0024">In some embodiments of the second system, determining whether the electronic document satisfies the set of signature criteria comprises determining whether a signature appears in the electronic document within a required spatial proximity to context data extracted from the electronic document.</p><p id="p-0026" num="0025">In some embodiments of the second system, determining whether the electronic document satisfies the set of signature criteria comprises generating an association score indicting a level of association between a signature extracted from the electronic document and context data extracted from the electronic document.</p><p id="p-0027" num="0026">In some embodiments of the second system, the system is configured to determine the set of signature criteria based at least in part on context data extracted from the electronic document, wherein the context data indicates one or more of: document type, document structure, and document format.</p><p id="p-0028" num="0027">In some embodiments, a second non-transitory computer-readable storage medium is provided, the second non-transitory computer-readable storage medium storing instructions for validating signatures in documents, the instructions configured to be executed by a one or more processors of a system to cause the system to: receive an electronic document comprising one or more signatures; apply one or more signature-extraction model to the electronic document to generate, for each of the one or more signatures in the electronic document, data representing a spatial location for the respective signature and a confidence level for the respective signature; determine, based on the data representing the spatial location and the confidence level, whether the electronic document satisfies a set of signature criteria.</p><p id="p-0029" num="0028">In some embodiments, a second method is provided, t second method being for validating signatures in documents, wherein the second method is performed by a system comprising one or more processors, the second method comprising: receiving an electronic document comprising one or more signatures; applying one or more signature-extraction model to the electronic document to generate, for each of the one or more signatures in the electronic document, data representing a spatial location for the respective signature and a confidence level for the respective signature; determining, based on the data representing the spatial location and the confidence level, whether the electronic document satisfies a set of signature criteria.</p><p id="p-0030" num="0029">In some embodiments, a third system is provided, the third system being for extracting information from documents, the third system comprising one or more processors configured to cause the third system to: receive a data set comprising a plurality of electronic documents; apply a set of data conversion processing steps to the plurality of electronic documents to generate a processed data set comprising structured data generated based on the plurality of electronic documents, wherein applying set of data conversion processing steps comprises applying one or more deep-learning-based optical character recognition (OCR) models; and apply a set of knowledge-based modeling processing steps to the structured data, wherein applying the set of knowledge-based modeling processing steps comprises: receiving user input indicating a plurality of data labels for the structured data; and applying a knowledge-based deep learning model based on the structured data and the plurality of data labels; and generating output data extracted from the plurality of electronic documents.</p><p id="p-0031" num="0030">In some embodiments of the third system, applying the set of data conversion processing steps comprises, before applying the one or more deep-learning-based OCR models, applying an automated orientation correction processing step.</p><p id="p-0032" num="0031">In some embodiments of the third system, applying the set of data conversion processing steps comprises, before applying the one or more deep-learning-based OCR models, applying a denoising function.</p><p id="p-0033" num="0032">In some embodiments of the third system, applying the one or more deep-learning-based OCR models comprises: applying a text-detection model; and applying a text-recognition model.</p><p id="p-0034" num="0033">In some embodiments of the third system, applying the set of data conversion processing steps comprises, after applying the one or more deep-learning-based OCR models, applying an image-level feature engineering processing step to generate the structured data.</p><p id="p-0035" num="0034">In some embodiments of the third system, applying the set of data conversion processing steps comprises applying a post-processing method that uses morphology to parse structural relationships amongst words.</p><p id="p-0036" num="0035">In some embodiments of the third system, applying the set of knowledge-based modeling processing steps comprises, before receiving the user input indicating the plurality of data labels, applying one or more feature engineering processing steps to the structured data to generate</p><p id="p-0037" num="0036">In some embodiments of the third system, applying the one or more feature engineering processing steps comprises predicting word groups based on morphology.</p><p id="p-0038" num="0037">In some embodiments of the third system, applying the set of knowledge-based modeling processing steps comprises receiving user input specifying user-defined feature engineering.</p><p id="p-0039" num="0038">In some embodiments of the third system, applying the set of knowledge-based modeling processing steps comprises applying fuzzy matching, wherein the system is configured to consider a partial match sufficient for labeling purposes, to automatically label documents on a word-by-word basis.</p><p id="p-0040" num="0039">In some embodiments of the third system, applying the set of knowledge-based modeling processing steps comprises automatically correcting one or more text-recognition errors during a training process.</p><p id="p-0041" num="0040">In some embodiments of the third system, the knowledge-based deep learning model comprises a loss function that is configured to accelerate convergence of the knowledge-based deep learning model.</p><p id="p-0042" num="0041">In some embodiments of the third system, the knowledge-based deep learning model comprises one or more layers using natural language processing (NLP) embedding such that the model learns both content information and related location information.</p><p id="p-0043" num="0042">In some embodiments of the third system, the knowledge-based deep learning model is trained using an adaptive feeding method.</p><p id="p-0044" num="0043">In some embodiments of the third system, the knowledge-based deep learning model comprises an input layer that applies merged embedding and feature engineering.</p><p id="p-0045" num="0044">In some embodiments of the third system, the knowledge-based deep learning model comprises an input layer that is configured for variant batch sizes.</p><p id="p-0046" num="0045">In some embodiments of the third system, the knowledge-based deep learning model comprises an input layer that applies a sliding window.</p><p id="p-0047" num="0046">In some embodiments of the third system, the knowledge-based deep learning model comprises one or more fully-dense layers disposed between an input layer and a prediction layer.</p><p id="p-0048" num="0047">In some embodiments of the third system, the knowledge-based deep learning model comprises a prediction layer that generates one or more metrics for presentation to a user.</p><p id="p-0049" num="0048">In some embodiments, a third non-transitory computer-readable storage medium is provided, the third non-transitory computer-readable storage medium storing instructions for extracting information from documents, the instructions configured to be executed by one or more processors of a system to cause the system to: receive a data set comprising a plurality of electronic documents; apply a set of data conversion processing steps to the plurality of electronic documents to generate a processed data set comprising structured data generated based on the plurality of electronic documents, wherein applying set of data conversion processing steps comprises applying one or more deep-learning-based optical character recognition (OCR) models; and apply a set of knowledge-based modeling processing steps to the structured data, wherein applying the set of knowledge-based modeling processing steps comprises: receiving user input indicating a plurality of data labels for the structured data; and applying a knowledge-based deep learning model based on the structured data and the plurality of data labels; and generating output data extracted from the plurality of electronic documents.</p><p id="p-0050" num="0049">In some embodiments, a third method is provided, the third method for extracting information from documents, wherein the third method is executed by a system comprising one or more processors, the third method comprising: receiving a data set comprising a plurality of electronic documents; applying a set of data conversion processing steps to the plurality of electronic documents to generate a processed data set comprising structured data generated based on the plurality of electronic documents, wherein applying set of data conversion processing steps comprises applying one or more deep-learning-based optical character recognition (OCR) models; and applying a set of knowledge-based modeling processing steps to the structured data, wherein applying the set of knowledge-based modeling processing steps comprises: receiving user input indicating a plurality of data labels for the structured data; and applying a knowledge-based deep learning model based on the structured data and the plurality of data labels; and generating output data extracted from the plurality of electronic documents.</p><p id="p-0051" num="0050">In some embodiments, a fourth system is provided, the fourth system being for determining the composition of document bundles, the fourth system comprising one or more processors configured to cause the first system to: receive data comprising a document bundle; extract, from the document bundle, first information comprising substantive content of one or more documents of the document bundle; extract, from the document bundle, second information comprising metadata associated with one or more documents of the document bundle; generate, based on the first information and the second information, output data representing a composition of the document bundle.</p><p id="p-0052" num="0051">In some embodiments, a fourth non-transitory computer-readable storage medium is provided, the fourth non-transitory computer-readable storage medium storing instructions for determining the composition of document bundles, the instructions configured to be executed by one or more processors of a system to cause the system to: receive data comprising a document bundle; extract, from the document bundle, first information comprising substantive content of one or more documents of the document bundle; extract, from the document bundle, second information comprising metadata associated with one or more documents of the document bundle; generate, based on the first information and the second information, output data representing a composition of the document bundle.</p><p id="p-0053" num="0052">In some embodiments, a fourth method is provided, the fourth method being for determining the composition of document bundles, wherein the fourth method is performed by a system comprising one or more processors, the fourth method comprising: receiving data comprising a document bundle; extracting, from the document bundle, first information comprising substantive content of one or more documents of the document bundle; extracting, from the document bundle, second information comprising metadata associated with one or more documents of the document bundle; generating, based on the first information and the second information, output data representing a composition of the document bundle.</p><p id="p-0054" num="0053">In some embodiments, a fifth system is provided, the fifth system being for validating signatures in documents, the fifth system comprising one or more processors configured to cause the fifth system to: receive an electronic document comprising one or more signatures; apply one or more signature-extraction models to the electronic document to generate, for each of the one or more signatures in the electronic document, data representing a spatial location for the respective signature and a confidence level for the respective signature; determine, based on the data representing the spatial location and the confidence level, whether the electronic document satisfies a set of signature criteria.</p><p id="p-0055" num="0054">In some embodiments, a fifth non-transitory computer-readable storage medium is provided, the fifth non-transitory computer-readable storage medium storing instructions for validating signatures in documents, the instructions configured to be executed by a one or more processors of a system to cause the system to: receive an electronic document comprising one or more signatures; apply one or more signature-extraction models to the electronic document to generate, for each of the one or more signatures in the electronic document, data representing a spatial location for the respective signature and a confidence level for the respective signature; determine, based on the data representing the spatial location and the confidence level, whether the electronic document satisfies a set of signature criteria.</p><p id="p-0056" num="0055">In some embodiments, a fifth method is provided, the fifth method being for validating signatures in documents, wherein the fifth method is performed by a system comprising one or more processors, the fifth method comprising: receiving an electronic document comprising one or more signatures; applying one or more signature-extraction models to the electronic document to generate, for each of the one or more signatures in the electronic document, data representing a spatial location for the respective signature and a confidence level for the respective signature; determining, based on the data representing the spatial location and the confidence level, whether the electronic document satisfies a set of signature criteria.</p><p id="p-0057" num="0056">In some embodiments, a sixth method is provided, the sixth method being for extracting information from documents, the system comprising one or more processors configured to cause the system to: receive a data set comprising a plurality of electronic documents; apply a set of data conversion processing steps to the plurality of electronic documents to generate a processed data set comprising structured data generated based on the plurality of electronic documents, wherein applying set of data conversion processing steps comprises applying one or more deep-learning-based optical character recognition (OCR) models; and apply a set of knowledge-based modeling processing steps to the structured data, wherein applying the set of knowledge-based modeling processing steps comprises: receiving user input indicating a plurality of data labels for the structured data; and applying a knowledge-based deep learning model trained based on the structured data and the a plurality of data labels indicated by one or more user inputs; and generating output data extracted from the plurality of electronic documents by the deep learning model.</p><p id="p-0058" num="0057">In some embodiments, a sixth non-transitory computer-readable storage medium is provided, the sixth non-transitory computer-readable storage medium storing instructions for extracting information from documents, the instructions configured to be executed by one or more processors of a system to cause the system to: receive a data set comprising a plurality of electronic documents; apply a set of data conversion processing steps to the plurality of electronic documents to generate a processed data set comprising structured data generated based on the plurality of electronic documents, wherein applying set of data conversion processing steps comprises applying one or more deep-learning-based optical character recognition (OCR) models; and apply a set of knowledge-based modeling processing steps to the structured data, wherein applying the set of knowledge-based modeling processing steps comprises: applying a knowledge-based deep learning model trained based on the structured data and a plurality of data labels indicated by one or more user inputs; and generating output data extracted from the plurality of electronic documents by the deep learning model.</p><p id="p-0059" num="0058">In some embodiments, a sixth method is provided the sixth method being for extracting information from documents, wherein the sixth method is executed by a system comprising one or more processors, the sixth method comprising: receiving a data set comprising a plurality of electronic documents; applying a set of data conversion processing steps to the plurality of electronic documents to generate a processed data set comprising structured data generated based on the plurality of electronic documents, wherein applying set of data conversion processing steps comprises applying one or more deep-learning-based optical character recognition (OCR) models; and applying a set of knowledge-based modeling processing steps to the structured data, wherein applying the set of knowledge-based modeling processing steps comprises: applying a knowledge-based deep learning model trained based on the structured data and a plurality of data labels indicated by one or more user inputs; and generating output data extracted from the plurality of electronic documents by the deep learning model.</p><p id="p-0060" num="0059">In some embodiments, any one or more of the features, characteristics, or aspects of any one or more of the above systems, methods, or non-transitory computer-readable storage media may be combined, in whole or in part, with one another and/or with any one or more of the features, characteristics, or aspects (in whole or in part) of any other embodiment or disclosure herein.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE FIGURES</heading><p id="p-0061" num="0060">Various embodiments are described with reference to the accompanying figures, in which:</p><p id="p-0062" num="0061"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows an exemplary architecture for text deep-learning model, in accordance with some embodiments.</p><p id="p-0063" num="0062"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows an exemplary architecture for a visual deep learning model, in accordance with some embodiments.</p><p id="p-0064" num="0063"><figref idref="DRAWINGS">FIG. <b>3</b></figref> shows a schematic diagram of a two-part pipeline for knowledge-based information extraction from richly formatted digital documentation, in accordance with some embodiments.</p><p id="p-0065" num="0064"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows samples of ICDAR13 images, in accordance with some embodiments.</p><p id="p-0066" num="0065"><figref idref="DRAWINGS">FIG. <b>5</b></figref> shows samples of ICDAR2015 images, in accordance with some embodiments.</p><p id="p-0067" num="0066"><figref idref="DRAWINGS">FIG. <b>6</b></figref> shows a comparison of text models, in accordance with some embodiments.</p><p id="p-0068" num="0067"><figref idref="DRAWINGS">FIG. <b>7</b></figref> shows a comparison between DeepOCR and OCR Engine, in accordance with some embodiments.</p><p id="p-0069" num="0068"><figref idref="DRAWINGS">FIG. <b>8</b></figref> shows a schematic diagram of a two-part pipeline for knowledge-based information extraction from richly formatted digital documentation, in accordance with some embodiments.</p><p id="p-0070" num="0069"><figref idref="DRAWINGS">FIGS. <b>9</b>-<b>18</b></figref> show images of a PDF document as processed by techniques disclosed herein, in accordance with some embodiments.</p><p id="p-0071" num="0070"><figref idref="DRAWINGS">FIG. <b>19</b></figref> shows output generated by techniques disclosed herein, in accordance with some embodiments.</p><p id="p-0072" num="0071"><figref idref="DRAWINGS">FIG. <b>20</b></figref> shows labeling of a CSV file, in accordance with some embodiments.</p><p id="p-0073" num="0072"><figref idref="DRAWINGS">FIG. <b>21</b></figref> shows an example image that may be used as a basis for feature engineering, in accordance with some embodiments.</p><p id="p-0074" num="0073"><figref idref="DRAWINGS">FIG. <b>22</b></figref> shows an architecture for a named-entity recognition model, in accordance with some embodiments.</p><p id="p-0075" num="0074"><figref idref="DRAWINGS">FIG. <b>23</b></figref> shows output data from a named-entity recognition model, in accordance with some embodiments.</p><p id="p-0076" num="0075"><figref idref="DRAWINGS">FIG. <b>24</b></figref> shows results of processing a PDF using an NER model, in accordance with some embodiments.</p><p id="p-0077" num="0076"><figref idref="DRAWINGS">FIG. <b>25</b></figref> shows the application of the NER model to a full sentence, in accordance with some embodiments.</p><p id="p-0078" num="0077"><figref idref="DRAWINGS">FIG. <b>26</b></figref> depicts a computer, in accordance with some embodiments.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0079" num="0078">Disclosed herein are systems and methods for providing AI-augmented auditing platforms, including techniques for automated document processing. As described below, automated document processing that may be performed by an AI-augmented auditing platform may include one or more of: automated classification (and clustering) of documents, automated signature detection within documents, and weak-leaning AI/ML processing techniques for extracting information from documents.</p><p id="p-0080" num="0079">As described herein, a system for providing AI-augmented auditing platforms may be configured to receive one or more documents as input data and to perform automated processing of the input documents. The documents may be received as structured or unstructured electronic data, received from one or more data sources, and the system may subject the received documents to one or more document processing techniques in order recognize information content within the documents, extract information content from the documents, and generate, store, and leverage data resulting from the document processing techniques. As explained herein, the document processing techniques may, in some embodiments, include application of one or more machine learning models.</p><heading id="h-0007" level="2">Document Classification and Clustering</heading><p id="p-0081" num="0080">Known techniques for document classification do not adequately leverage context data to guide document classification, especially in the context of audit processes. As described herein, context data that is available in audit processes may be effectively and efficiently leveraged in order to improve the accuracy and efficiency of document classification and clustering for use in AI-augmented auditing platforms.</p><p id="p-0082" num="0081">In some embodiments, a system for automated document processing may be configured to perform automated document classification (e.g., classifying documents according to different document types) and/or document bundling. As described herein, the system may apply a set of AI methods to leverage context data in combination with a multi-page document classification ML models to accurately determine the composition of document bundles, such as document bundles received by an AI-augmented auditing platform as part of an audit review process.</p><p id="p-0083" num="0082">The system may be configured to receive data representing one or more documents and to apply one or more AI methods to the received data in order to recognize and extract information from said documents and in order to classify and/or cluster said documents. The AI methods may be configured to perform analyses on the basis of substantive document content (e.g., characters, text, and/or images in the documents), on the basis of metadata stored as a part of or in association with said document, and/or on the basis of context data associated with said documents.</p><p id="p-0084" num="0083">In some embodiments, metadata stored as a part of or in association with said document may include data such as document format data, document section data, page number data, font data, document layout data, document creator data, document creation time data, document title data, and/or any other suitable metadata that may pertain to all or part of a document bundle. In some embodiments, metadata may include one or more of: information obtained from file names of one or more documents, information obtained from file extensions of one or more documents, information obtained from file metadata (e.g., creator, date, etc.) of one or more documents</p><p id="p-0085" num="0084">In some embodiments, external context data may include one or more of: information regarding one or more automation processes used in acquiring the document data (and/or context data) from one or more systems (e.g., from enterprise resource planning (ERP) systems or databases); information regarding one or more requests to which the documents were responsive; information regarding one or more parties from whom the documents were requested and/or to whom the documents pertain; and information regarding a manner (e.g., a communication medium) by which the documents were provided</p><p id="p-0086" num="0085">In some embodiments, contextual data may include information regarding one or more processes, protocols, and/or standards to which the documents pertain. For example, contextual data may indicate information about a series of steps in a predefined process (e.g., a business process) or a series of documents types in a predefined set of document types. In determining demarcations between document boundaries, one or more data processing models applied by the system may be configured to identify document types (e.g., to identify demarcations between documents in a bundle) in a predefined set of document types and/or to identify documents pertaining to steps in a predefined process. In some embodiments, a data processing operation may be configured to identify document types (e.g., to identify demarcations between documents in a bundle) in accordance with a predefined order of steps and/or a predefined order of document types as indicated by contextual data. (Any data processing operation referenced herein may include application of one or more models trained by machine-learning.)</p><p id="p-0087" num="0086">Context data may be received by the system from any one or more suitable data sources, may be indicated by one or more user inputs detected by the system, and/or may be deduced by one or more data processing models of the system. Leveraging context data may provide a bridge for the system to introduce prior knowledge and understand the documents within the environment in which the document data (e.g., unstructured data) is provided.</p><p id="p-0088" num="0087">The system may be configured to apply one or more data processing algorithms, models, and/or machine learning models (including, e.g., a sequence of machine learning techniques) to identify document types for document bundles, for single documents, and/or for single pages of documents. In some embodiments, the system (e.g., the one or more machine learning models) may be configured to delineate document-type boundaries within a document bundle in order to identify demarcations between separate documents within the document bundle Identification of document-type boundaries within a document bundle may be based on one or more of the following: determination of a document type for a page within the document bundle, determination of similarity (e.g., a similarity score) between two or more pages within a document bundle, and/or detection and assessment of one or more embedded objects within a document (including determination of similarity (e.g., a similarity score) between two or more embedded objects within a document). The system may be configured to detect transitions within a document bundle&#x2014;e.g., detected on the basis of a change within the document bundle in one or more of document content, document type, document metadata, document format, and/or embedded object characteristics&#x2014;and to classify different portions of the document (and identify document boundaries within the document bundle) on the basis of said transitions.</p><p id="p-0089" num="0088">The system may be configured for the purposes of information integrity in the auditing process.</p><p id="p-0090" num="0089">In some embodiments, the system may receive data comprising a document bundle and may extract, from the received data, document content information and/or metadata information. In some embodiments, the system may extract context information from the received document data. In some embodiments, the system may receive context information from one or more additional data sources (e.g., separate from the data sources from which the document data was received), and may correlate the received context information with the document bundle data. In some embodiments, extracting the document content information includes applying embedded object type detection.</p><p id="p-0091" num="0090">The system may then use the document content information, metadata extracted from said documents, and/or the context information to generate output data representing a composition of the document bundle, wherein the output information may indicate one or more document types for the document bundle, a plurality of document types within the document bundle, and/or information regarding demarcations between (e.g., page breaks between) different documents within the document bundle. In some embodiments, generating the output data comprises applying a page similarity assessment model to a plurality of pages of the document bundle.</p><p id="p-0092" num="0091">In some embodiments, generating the output data comprises applying one or more data processing operations to model a state of a document bundle being processed. In some embodiments, the document bundle may be modeled using a finite state model. In some embodiments, a model of the document bundle may be used to leverage a calculated likelihood that a subsequent page in a document bundle is part of the same document (e.g., the same classification, the same type) as the current page of the document. For example, a model may be used to make determinations by leveraging contextual data about the manner in which documents are normally arranged (for example about the manner in which pages from different documents are not normally randomly interleaved with one another, but are usually arranged into contiguous portions of a document bundle).</p><p id="p-0093" num="0092">In some embodiments, generating the output data comprises applying one or more data processing operations to analyze the presence or absence of one or more embedded objects within a document. For example, the system may apply one or more rules and/or models regarding whether certain document types are associated with certain embedded object types. For example, embedded signature objects may be associated with certain document types and therefore may be recognized by the system and used to identify said associated certain document types.</p><p id="p-0094" num="0093">In some embodiments, the system may apply a page-similarity model as part of a document-understanding pipeline. In some embodiments, a page-similarity model may be the first step applied in a document-understanding pipeline. In some embodiments, a page similarity model (e.g., Random Forest) may determine if two pages belong to the same document. This may be useful because multiple documents may be bundled into a single PDF file before being provided to the system. The page-similarity model may include one or more of the following: a random forest classification of image features (e.g., low-level image features) such as Oriented FAST and rotated BRIEF (ORB), Structural Similarity (SSIM) index, and histograms of images using different distance metrics such as correlation, chi-squared, intersection, Hellinger, etc.</p><p id="p-0095" num="0094">In some embodiments, the system may apply a text and low-level features model (TFIDF+VGG16+SVM). The text and low-level features model may include two parts: a page-similarity module and a page-classification module. In some embodiments, the page-similarity module of the text and low-level features model may share any one or more characteristics in common with the page-similarity model described above. In some embodiments, the page-classification module may be configured to classifying the one or more pages (e.g., the first page) of a bundle of documents determined using a Support Vector Machine (SVM) classifier and features as the image text through TFIDF and visual features of the VGG16 Model.</p><p id="p-0096" num="0095">In some embodiments, the system may apply a text deep-learning model (e.g., embeddings +1D-CNN). In some embodiments, the text deep-leaning model may use text extracted from an image to classify documents using embeddings. More specifically, the words may be tokenized and embedded using Word2Vec, and they may then be passed through a shallow CNN for classification. The architecture, according to some embodiments, is shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0097" num="0096">In some embodiments, the system may apply a visual deep learning model (e.g., VGG19 Transfer Learning). <figref idref="DRAWINGS">FIG. <b>2</b></figref> shows an exemplary architecture for a visual deep learning model, in accordance with some embodiments. The visual deep learning model may be configured to identify visual features using the VGG19 Deep Convolutional Neural Network architecture shown below. The model may load weights trained using, e.g., imagenet, and may train the last two layers of the model.</p><p id="p-0098" num="0097">In some embodiments, the system may apply a Siamese model (e.g., Embeddings &#x26; 1D-CNN+VGG19 Transfer Learning). The Siamese model may combine text and visual features for a Siamese deep-learning classification. The features coming in from the two above models may be concatenated and passed through a dense layer for classification.</p><p id="p-0099" num="0098">In some embodiments, the system may apply a document clustering model. The document clustering model may select a diverse sample data set from a large data set for model training purposes.</p><p id="p-0100" num="0099">Table 1 below shows performance of various models, in one example. The test data used to generate the results data in Table 1 included data from the same clients whose data was used to train the models. The pilot data included data from clients that the model was not trained on. Therefore, the pilot data result may be a better indicator of the model's performance with unseen data.</p><p id="p-0101" num="0000"><tables id="TABLE-US-00001" num="00001"><table frame="none" colsep="0" rowsep="0" pgwide="1"><tgroup align="left" colsep="0" rowsep="0" cols="7"><colspec colname="1" colwidth="28pt" align="left"/><colspec colname="2" colwidth="35pt" align="left"/><colspec colname="3" colwidth="42pt" align="center"/><colspec colname="4" colwidth="49pt" align="center"/><colspec colname="5" colwidth="49pt" align="center"/><colspec colname="6" colwidth="28pt" align="center"/><colspec colname="7" colwidth="28pt" align="center"/><thead><row><entry namest="1" nameend="7" rowsep="1">TABLE 1</entry></row><row><entry namest="1" nameend="7" align="center" rowsep="1"/></row><row><entry/><entry/><entry/><entry>Word2Vec</entry><entry>Keras</entry><entry>Deep</entry><entry/></row><row><entry/><entry>Precision</entry><entry>Retrained</entry><entry>Embeddings +</entry><entry>Embeddings +</entry><entry>CNN</entry><entry>Siamese</entry></row><row><entry>&#x2014;</entry><entry>Score</entry><entry>SVM Model</entry><entry>CNN</entry><entry>CNN</entry><entry>Model</entry><entry>Model</entry></row><row><entry namest="1" nameend="7" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="7"><colspec colname="1" colwidth="28pt" align="left"/><colspec colname="2" colwidth="35pt" align="left"/><colspec colname="3" colwidth="42pt" align="char" char="."/><colspec colname="4" colwidth="49pt" align="char" char="."/><colspec colname="5" colwidth="49pt" align="char" char="."/><colspec colname="6" colwidth="28pt" align="char" char="."/><colspec colname="7" colwidth="28pt" align="char" char="."/><tbody valign="top"><row><entry>Test</entry><entry>BOL</entry><entry>0.92</entry><entry>0.9</entry><entry>0.91</entry><entry>0.4</entry><entry>0.96</entry></row><row><entry/><entry>Invoice</entry><entry>0.93</entry><entry>0.98</entry><entry>1.00</entry><entry>0.31</entry><entry>0.97</entry></row><row><entry/><entry>Others</entry><entry>0.94</entry><entry>0.88</entry><entry>0.93</entry><entry>0.06</entry><entry>0.98</entry></row><row><entry/><entry>PO</entry><entry>0.84</entry><entry>0.87</entry><entry>0.98</entry><entry>0.04</entry><entry>0.76</entry></row><row><entry>Pilot</entry><entry>BOL</entry><entry>0.28</entry><entry>0.35</entry><entry>0.52</entry><entry>0.29</entry><entry>0.11</entry></row><row><entry/><entry>Invoice</entry><entry>0.25</entry><entry>0.9</entry><entry>0.92</entry><entry>0.00</entry><entry>0.00</entry></row><row><entry/><entry>Others</entry><entry>0.5</entry><entry>0.67</entry><entry>0.00</entry><entry>0.33</entry><entry>0.33</entry></row><row><entry/><entry>PO</entry><entry>0.09</entry><entry>0.12</entry><entry>0.13</entry><entry>0.09</entry><entry>0.08</entry></row><row><entry namest="1" nameend="7" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0102" num="0100">In some embodiments, the system may automatically leverage output data generated as described herein in or more functionalities provided by the AI-augmented auditing platform. For example, the system may automatically generate and store individual document files for each separate document recognized within the document bundle. In some embodiments, the system may individually leverage separate documents recognized within the document bundle as separate pieces of evidence in one or more auditing assessments, including an AI-augmented auditing process that uses the document data in order to perform one or more vouching processes, adjudication processes, recommendation generation processes, information integrity processes, and/or data integrity processes.</p><heading id="h-0008" level="2">Signature Detection</heading><p id="p-0103" num="0101">Document processing, for example for the purposes of assurance, often requires verifying that a signature (or initials) appear in specific area or in association with a specific topic within a document. There may be more than one section, more than one topic, and/or more than one signature present in a single document or document bundle. Known techniques for signature detection require manual review and verification, which is inefficient and inaccurate and does not allow for processing documents at scale.</p><p id="p-0104" num="0102">In some embodiments, a system for automated document processing may be configured to perform automated signature detection, including by applying AI models that learn where signatures are likely to occur on a given document type. During document ingestion and processing, the system may then validate that documents being processed do in fact have signatures at the expected/required locations within the documents. The systems and methods provided herein may be used to automatically process documents to determine whether said documents provide evidence, with required and sufficient signatures, to meet vouching criteria for shipments of goods, receipt of goods, agreement to contracts, or the like.</p><p id="p-0105" num="0103">As explained herein, the system may receive one or more input documents to be processed for signature detection and/or automated vouching analysis. The system may apply one or more AI models to detect information regarding the document type, document structure, and/or document format of the received document. In some embodiments, determination of document type may be based at least in part on recognition of one or more signatures within the document. For example, the presence of a single signature, corresponding pairs of signatures, no signatures, certain kinds of signatures, and/or signatures in certain pages and/or certain sections may be associated by one or more rules or models with certain document types, and the system may leverage said rules/models in order to recognize said document types.</p><p id="p-0106" num="0104">In some embodiments, once the system has generated information for the document to be analyzed representing the document type, document structure, and/or document format, the system may then determine, for the document be analyzed, one or more signature requirement criteria. The signature requirement criteria may be determined based on the document type, document structure, and/or document format. In some embodiments, the system may determine signature requirement criteria for various document types, document structures, and/or document formats using one or more machine learning models trained on signed documents of various types, structures, and/or formats. In some embodiments, the system may determine signature requirement criteria based on one or more predefined signature criteria rules.</p><p id="p-0107" num="0105">In some embodiments, the signature criteria that are determined may include one or more of: a location for a signature, a document section to which a signature corresponds, document content to which a signature corresponds, a type of signature (e.g., hand-written, e-signature, initials, etc.), an order of signatures, and/or a date of a signature.</p><p id="p-0108" num="0106">One the system has determined the signature criteria for the document, the system may then assess the document to determine whether those one or more signature criteria are satisfied. The system may for example, apply one or more signature detection models to extract signature information from the document, wherein the extracted information may indicate signature presence, signature identity, signature location, association of a signature with a document section and/or with document content, and/or signature type. (In some embodiments signature detection models may be applied before and/or after document-type detection is performed and before and/or after signature criteria for the document are required. For example, in instances in which signature detection is used to determine document type, the signature detection models may have been applied before determination of the signature criteria for the document.)</p><p id="p-0109" num="0107">In some embodiments, the one or more signature detection models may include one or more context-less signature detection models that have been trained on signatures and non-signatures regardless of location within a document. In some embodiments, the one or more signature detection models may include one or more context-dependent signature detection models that account for context in determining whether and where a signature is detected.</p><p id="p-0110" num="0108">In some embodiments, the system may be configured such that, for each signature detected within a document, the system generates (a) a spatial location within the document at which the signature was detected and (b) a confidence level for the detected signature. In some embodiments, the generated confidence level may indicate a degree of confidence that a signature was detected and/or may indicate a degree of confidence regarding the location at which the signature was detected. In some embodiments, the system may be configured such that, for each signature detected within a document, the system generates (c) signature characteristic data indicating one or more characteristics of the signature (e.g., signature quality, signature type, signature identity, signature date, signature order, etc.) and optionally indicating respective confidence values associated with one or more of said characteristics.</p><p id="p-0111" num="0109">The system may compare the extracted signature information to the determined signature criteria and may generate one or more outputs indicating whether one or more signature criteria for the document are satisfied. The system may, in some embodiments, indicate that signature criteria are satisfied, that signature criteria are not satisfied, or that a determination as to whether signature criteria are satisfied cannot be made. In some embodiments, outputs indicating whether signature criteria are satisfied may include one or more confidence scores indicating a degree of confidence in one or more of the conclusions.</p><p id="p-0112" num="0110">In some embodiments, evaluating whether a signature meets signature criteria for a document may be based, at least in part, on associating signature-context data (wherein the context data may be associated with and/or extracted from the document) with one or more signatures within the document. For example, the system may associate signature-context data&#x2014;such as information regarding document sections, identities of one or more parties relevant to the document, spatial location within a document, etc. &#x2014;with one or more detected signatures. Detected signatures may, in some embodiments, be associated with signature-context data from the document on the basis of spatial proximity of the signature location and of a location from which the context data was extracted. In some embodiments, association between a signature and signature-context data may be quantified by an association score (e.g., indicating a level of confidence in the association). In some embodiments, the system may then evaluate the document's compliance with one or more signature criteria on the basis of the determined association and/or the determined association score.</p><p id="p-0113" num="0111">In some embodiments, selection of one or more signatures for use in evaluating compliance with signature criteria may be based on one or both of: (a) a confidence score for identification of the signature and/or signature information itself, and (b) an association score for association of an identified signature with document context (e.g., based on spatial proximity in the document). In some embodiments, evaluation of compliance with signature criteria may be based on one or both of a confidence score and an association score. In some embodiments, an overall relevance ranking may be based on both a confidence score and an association score.</p><p id="p-0114" num="0112">Associations between signatures and signature-context data made by the system may be one-to-one, one-to-many, many-to-one, or many-to-many. In some embodiments, the system may rank associations between a signature and various signature-context data (or between a signature-context data and various signatures) and may assign as association score to each association. In some embodiments, the system may select the highest-ranked association and may evaluate compliance with signature criteria on the basis of the signature-context association indicated by the highest-ranked association (and/or on the basis of the association score of the highest-ranked association).</p><p id="p-0115" num="0113">In some embodiments, signatures may be ranked by signature confidence score for detection/recognition of a signature, association score, and/or an overall (e.g., combined) confidence score based on both of the preceding scores (and optional other factors). In some embodiments, selection of a signature for evaluation and/or evaluation itself of a signature for signature-criteria compliance may be based on any one or more of: signature confidence, association score, and/or overall (e.g., combined) confidence score.</p><heading id="h-0009" level="1">Signature-Detection Example</heading><p id="p-0116" num="0114">A customized pipeline was developed with a YOLO model that leverages transfer learning. The pipeline is configured to receive PDF documents, to detect pages within the PDF documents that contains a signature, and to generate output data indicating a page number and a confidence score for each signature detected.</p><p id="p-0117" num="0115">A connected component analysis approach was developed as follows:<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0000">    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0116">Step 1&#x2014;Detect designated boxes using contour detection with parameters (lower half of page-30%, min height &#x26; width of contour)</li>        <li id="ul0002-0002" num="0117">Step 2&#x2014;Identify box type by Tesseract OCR (keywords &#x2018;SHIPPER&#x2019; and &#x2018;CARRIER&#x2019;)</li>        <li id="ul0002-0003" num="0118">Step 3&#x2014;Perform CCL Analysis on each box to extract larger connected components (like signatures and handwritten text)</li>        <li id="ul0002-0004" num="0119">Step 4&#x2014;Generate output by overlaying outputs of only bounding boxes on whitespace</li>        <li id="ul0002-0005" num="0120">Step 5&#x2014;Get Abby Ground Truth for boxes by parsing xml files to get bounding box details</li>        <li id="ul0002-0006" num="0121">Step 6&#x2014;Check accuracy by performing IoU of ground truth bounding boxes on input image and output image</li>    </ul>    </li></ul></p><heading id="h-0010" level="2">Weak Learning for AI-Augmented Assurance</heading><p id="p-0118" num="0122">Documents stored in PDF format, image format, and other formats can contain a lot of information, and extracting said information can be an important part of AI-driven assurance processes and other tasks performed by AI-augmented auditing platforms. For example, an AI-driven assurance process may rely on automated extraction of data stored in PDFs, such that invoices and/or other pieces of piece of information (e.g., evidentiary information) may be fully considered, correctly understood, and applied as part of the audit process. Efficient processing of documents may enable an audit process to exhaustively consider all available evidentiary (e.g., documentary) data, rather than simply considering a small sample thereof.</p><p id="p-0119" num="0123">Existing solutions include, for richly formatted PDF's: creation of &#x2018;knowledge base construction.&#x2019; However, this solution relies on underlying structure of PDFs, and cannot work on scanned PDF documents where the underlying structure is not known. Existing solutions include, for scanned PDF documents, optical character recognition (OCR) and NLP. However, these solutions rely on templatization of PDFs and surrounding words, and they cannot work with too many varied formats and/or visual relations. According to known techniques, automatic extraction of information from electronic formats such as PDF and image formats is inefficient, inaccurate, and time consuming. The known alternative&#x2014;review by humans&#x2014;is also costly and inefficient. Known automated information-extraction solutions use pre-trained models to extract text from data, but they require annotated PDFs to train computer vision models that can extract such information. Creating these annotations to train models is in itself an expensive activity.</p><p id="p-0120" num="0124">Known solutions are Fonduer and OCR assisted methods. Fonduer's pipeline strongly relies on parsing PDFs to HTMLs. A perfect conversion could retain as much information as possible, which makes Fonduer advanced. However, the application of Fonduer is limited because few software can completely support this process. As for OCR assisted methods, OCR engines such as Abbyy deal with well scanned documentation. Abbyy can extract information from documents, but users still need to apply extra efforts to extract entities that are actually needed. NLP and other AI methods, which use semantic information among all extracted words to improve extraction on target entities, are commonly used to work toward that goal. As these solutions do not consider structural information, they is not robust enough for noisy documents with complex underlying structures.</p><p id="p-0121" num="0125">The systems and methods described herein may address one or more of the above-identified shortcomings of existing solutions.</p><p id="p-0122" num="0126">Disclosed herein are systems and methods for automated information extraction that may address one or more of the above-identified shortcomings. In some embodiments, document processing and information-extraction systems described herein leverage a unique combination of (a) natural language processing using semantic and morphological analysis with (b) weak labelling based on fuzzy matching and deep learning based on text and computer vision. The combined model, configured to extract information from PDFs, may be provided an ensemble of NLP, text, and computer vision. The systems and methods described herein may provide accurate and efficient information extraction from PDF documents and from evidence data provided in other formats, may overcome one or more of the above-identified shortcomings of known solutions, and may overcome the problem of cold start for documents (where annotated data does not exist and creation of annotations is expensive). Information that may be accurately and efficiently extracted by the techniques disclosed herein include, for example, invoice amount, number, agency name, committee, etc.</p><p id="p-0123" num="0127">Regarding the task of creation of annotations, ground truth data from which annotations can be created may, in some embodiments, exist in one or more data sources, such as in an ERP database or system. However, the ground truth data may exist in a format (e.g., a normalized format) that does not perfectly (e.g., word for word) match content in documents to be processed by the system word. This may further complicate the task of creating annotations. The systems and methods disclosed herein overcome this challenge by applying weak labeling (fuzzy matching), in which an entity in a ground-truth data source (e.g., an ERP system) only needs to partially match an entity in a processed document (e.g., in a PDF) for the system to generate labels based on that partial match, such that the model can learn from those labels.</p><p id="p-0124" num="0128">Described below are some embodiments of systems and methods for knowledge-based information extraction from richly formatted digital documentation. While the below description is made mostly with reference to PDF documents, the techniques described herein may also be applied to webpages, business reports, product specifications, scientific literature, and any other suitable document type. As described below, systems may process input documents/data as an image, so any input data that is (or can be) formatted as an image may be suitable.</p><p id="p-0125" num="0129">Systems and methods described herein may provide a pipeline for knowledge-based information extraction from richly formatted digital documentation, wherein the pipeline includes two portions: first, the document conversion portion and, second, a knowledge modeling portion. <figref idref="DRAWINGS">FIG. <b>3</b></figref> depicts a schematic diagram of a training process for a two-part pipeline <b>300</b> for knowledge-based information extraction from richly formatted digital documentation, in accordance with some embodiments. The model may include an ensemble of an NLP model on handcrafted features and a computer vision model that improves in accuracy over time through self-learning and validation mechanisms. Described herein are characteristics of such pipelines for knowledge-based information extraction from richly formatted digital documentation, in accordance with some embodiments.</p><p id="p-0126" num="0130">As shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, pipeline <b>300</b> may include a data-conversion portion <b>310</b> and a knowledge-based modeling portion <b>330</b>.</p><p id="p-0127" num="0131">In the first portion <b>310</b> of the two-part pipeline <b>300</b>, the system may convert PDFs to database. For this process, one or more deep learning models (e.g., DeepOCR) may be applied; said models may include a text detection model and a text recognition model. Said models may be used to extract words (e.g., every word) instead of using OCR engine. This may enable stronger abilities and more robust performance on extracting information from both clean and noisy documents. Due to the capacity constraint of OCR, it is not guaranteed that all the information in the documents can be detected. Thus, systems and methods described herein may combine computer vision with DeepOCR called &#x2018;Canvas&#x2019;, which could automatically supplement the missed information by DeepOCR without human interactions. After the conversion, a specific post processing method may be applied, wherein the post-processing method introduces morphology to better parse the structural relationship among words. For instance, dilation and erosion with customized kernels may be used to tell whether nearby words are from the same word group or paragraph.</p><p id="p-0128" num="0132">In some embodiments, in the first portion <b>310</b> of the two-part pipeline <b>300</b>, steps <b>312</b>-<b>322</b> as described below may be applied.</p><p id="p-0129" num="0133">At block <b>312</b>, in some embodiments, the system may receive input data comprising one or more documents to be processed. In some embodiments, the input data may comprise PDF data, image data, and/or document data in any other suitable format. The input data may be received from any suitable data source such as one or more databases, data stores, network sources, or the like. The input data may be received according to a predefined schedule, as part of an inbound network transmission, as part of a scraping operation, in response to a user request, and/or as part of a manual data upload. The received data may be stored locally and/or remotely following receipt.</p><p id="p-0130" num="0134">At block <b>314</b>, in some embodiments, the system may apply one or more automated orientation correction data processing operations to the received data in order to correct/normalize the orientation of pages in the documents.</p><p id="p-0131" num="0135">At block <b>316</b>, in some embodiments, the system may apply one or more denoising data processing operations to the orientation-corrected data. The one or more denoising operations may in some embodiments comprise data normalization operations. The one or more denoising operations may in some embodiments be selected based on user input, system settings, identity of one or more parties associated with the documents being processed, industry of one or more parties associated with the documents being processed, and/or document type (e.g., as automatically determined by the system) of one or more of the documents being processed.</p><p id="p-0132" num="0136">At block <b>318</b>, in some embodiments, the system may apply one or more deep-learning based text detection and recognition operations. In some embodiments, said operations may include a flexible OCR operation. In some embodiments, the text detected and recognized at block <b>318</b> may comprise all character data that can be recognized within processed data. In some embodiments, the recognized character data may be stored in association with metadata indicating a spatial location of each recognized character within the document in which it was recognized.</p><p id="p-0133" num="0137">At block <b>320</b>, in some embodiments, one or more image-level feature engineering processes may be applied to the data generated at block <b>318</b> in order to select features to be used to generate feature data. During the training process, block <b>320</b> may be applied in order to determine which features to use to train the model. During subsequent application of the model, after training has been completed, block <b>320</b> may simply entail extracting the features that have been previously identified by the feature engineering process during training, and using those extracted features to generate feature data to be processed and analyzed by the trained model. Feature data generated at block <b>320</b> may comprise text data such as character data, word data, sentence data, paragraph data, section data. Feature data generated at block <b>320</b> may comprise location data (e.g., indicating a spatial location within a page) associated with any text data. Feature data generated at block <b>320</b> may comprise document structure data indicating a section (e.g., a page, a section, a chapter, etc.) within a document that is associated with any text data. Feature data generated at block <b>320</b> may comprise text characteristic data, for example indicating a font, a style, a size, and/or an orientation associated with any text data.</p><p id="p-0134" num="0138">At block <b>322</b>, in some embodiments, the system may store the data generated at block <b>320</b> (e.g., word-level tokens with location information and other features) in any suitable format, for example in CSV format. The data may be stored in any suitable computer storage system locally and/or remotely.</p><p id="p-0135" num="0139">In the second portion <b>330</b> of two-part pipeline <b>300</b>, the following steps may be applied. Semantic, document, structural, and/or morphological information may be utilized, separately and/or together, as inputs. The method may include weak supervised learning in which the label for the documents does not need to be purely correct. This method may be robust in handling incorrect label information. A user may only needs to provide their domain knowledge, and the system may automatically label documents word-by-word using fuzzy matching. Based on this weak labeling method, the system can correct some errors from the text recognition during the training process. With the efficient design of the model, the systems described herein enable strong abilities to extract information from unseen documents in the same domain.</p><p id="p-0136" num="0140">In some embodiments, in the second portion <b>330</b> of the two-part pipeline <b>300</b>, steps <b>332</b>-<b>338</b> as described below may be applied.</p><p id="p-0137" num="0141">At block <b>332</b>, in some embodiments, the system may access stored data generated by the first portion <b>310</b> of pipeline <b>300</b>. In some embodiments, the accessed data may be the same data (or a portion thereof, and/or data based thereon) that was stored at block <b>322</b>.</p><p id="p-0138" num="0142">At block <b>334</b>, in some embodiments, the system may apply one or more feature engineering processes to the data generated at block <b>332</b> in order select features to be used to generate feature data. The feature engineering process may select features such as character, word (e.g., with more than one character), length of word, surrounding environment (e.g., next to a border (which could come from a table)), etc. During the training process, block <b>334</b> may be applied in order to determine which features to use to train the model. During subsequent application of the model, after training has been completed, block <b>334</b> may simply entail extracting the features that have been previously identified by the feature engineering process during training, and using those extracted features to generate feature data to be processed and analyzed by the trained model.</p><p id="p-0139" num="0143">At block <b>336</b>, in some embodiments, the system may apply labels and perform user-defined feature engineering in order select features to be used to generate feature data. During the training process, block <b>336</b> may be applied in order to determine which labels to apply to train the model and which features to use to train the model. During subsequent application of the model, after training has been completed, block <b>336</b> may simply entail extracting the features that have been previously identified by the feature engineering process during training, and using those extracted features to generate feature data to be processed and analyzed by the trained model.</p><p id="p-0140" num="0144">In applying labels, the system may utilize domain knowledge, for example relying on one or more domain knowledge sources such as dictionaries or third-party data source. Domain knowledge may include known patterns that associate certain content types (e.g., page numbers) with certain spatial locations (e.g., the top of the page or the bottom of the page). During training, the system may label all tokens (e.g., characters, words, etc.), even if a confidence level in the accuracy of all labels is less than 100%. In performing labeling during training, the system may seek to achieve high recall (e.g., covering target entities as much as possible) and high precision (e.g., by mislabeling tokens as little as possible).</p><p id="p-0141" num="0145">In performing user-defined feature engineering during training, the system may apply one or more feature engineering processes that leverage user input, in order to select features to use to generate feature data based on the user's domain knowledge. Leveraging user domain knowledge in order to select features to use to generate feature data for training may improve model quality and may improve model performance during implementation. The system may receive one or more user inputs indicating one or more of: section heading, customer name, customer address, date, billing address, shipping address, etc.</p><p id="p-0142" num="0146">At block <b>338</b>, in some embodiments, the system may generate, configure, and/or apply a knowledge-based deep-learning model using the feature data generated at block <b>320</b>, <b>334</b>, and/or <b>336</b>. During training, the system may generate and configure the model based on the features selected for training. During application, the system may apply the trained model in order to generate output data indicating information extracted from analyzed input data (e.g., input documents), classifications for input data, and/or confidence levels associated with model outputs. The knowledge-based deep learning model may be a deep learning model that was trained using feature data generated based on the features selected at blocks <b>320</b>, <b>334</b>, and/or <b>336</b> during training. The deep-learning model(s) may generate output data that indicate one or more pieces of recognized content of the input documents, optionally along with associated confidence scores. The deep-learning model(s) may generate output data that classified the input documents into one or more classifications, optionally along with associated confidence scores. The output data may, for example indicate original token (e.g., location, words), basic features, and/or user defined features.</p><p id="p-0143" num="0147">By applying deep-learning-based text detection and text recognition instead of (or in addition to) OCR engines, systems and methods disclosed herein may be more flexible in being able to be applied in different scenarios, and they may offer more control and customizability for the output of text recognition and detection.</p><p id="p-0144" num="0148">In some embodiments, labels generated from the trained model may be used for further training of the model.</p><p id="p-0145" num="0149">In some embodiments, the systems and methods disclosed herein may apply one or more of classical syntactic, semantic, and/or morphological analysis of documents to extract templates and weak labels.</p><p id="p-0146" num="0150">In some embodiments, the systems and methods disclosed herein may include a customized loss function that may accelerate the model's convergence.</p><p id="p-0147" num="0151">In some embodiments, the systems and methods disclosed herein may include one or more customized layers that leverage NLP embedding to allow the model to learn both content information and related location information.</p><p id="p-0148" num="0152">In some embodiments, the systems and methods disclosed herein may leverage morphology as a part of feature engineering to improve performance on predicting word groups.</p><p id="p-0149" num="0153">In some embodiments, the systems and methods disclosed herein may include one or more adaptive feeding methods for model training (e.g., feed model with 10 PDFs with distinct format for one step).</p><p id="p-0150" num="0154">Regarding Deep Learning based OCR (DeepOCR), three approaches may be used: text detection, text recognition, and end-to-end combination of the two.</p><p id="p-0151" num="0155">If targeting on finding information in images, text detection may be used to tell which parts in an image are likely to be text, and then a recognition model may be used model to tell content information in those parts of the image. Using two deep learning models may make a pipeline slow but more durable to customize on intermediate output. Alternately, an end-to-end solution may be used to directly recognize what is the text and where it is. This is only one deep learning model, and the inference speed thus be faster than a pipeline using two deep learning models.</p><p id="p-0152" num="0156">In some embodiments, steps applied by the pipeline may be as follows. As a first step, as part of OCR feature engineering, OCR supplementation and line-labeling may be applied. This may include performing initial text detection, performing missing value supplementation, and detecting lines.</p><p id="p-0153" num="0157">As a second step, as part of OCR feature engineering, word group segmentation, cluster segmentation, and structural segmentation may be applied.</p><p id="p-0154" num="0158">As a third step, as part of OCR feature engineering, OCR feature engineering (structural) may be performed. Word-level features may include word coordinates, word heights (font size), word size, count upper/lower characteristics, and/or line label. Word-group-level features may include word-group coordinates, count of words, count of strings/digits, total white space, and/or word cluster label. Word-cluster-level features may include word-cluster coordinates, count of words, count of word groups, total white space, and/or count of lines. Word-structure-level features may include word structure coordinates, count of words/word groups, count of word clusters, total white space, and/or count of lines. An output, such as a CSV output, may be generated with related coordinates and other structural information. Using morphology as a part of feature engineering may improve performance on predicting word groups.</p><p id="p-0155" num="0159">As a fourth step, as part of entity extraction, weak labeling for knowledge model training may be applied.</p><p id="p-0156" num="0160">In some embodiments, the model architecture may utilize semantic information, structure information, and/or morphology information. The model may include a customized network including an input layer, a body part, and a prediction part. The input layer may include (a) merged embedding and feature engineering and/or (b) variant batch size and sliding windows. The body part may include (a) fully dense layers and/or (b) customized penalization. The prediction may include customized metrics to monitor. Customized layers with NLP embedding may allow the model to learn both content information and related location information. The model may apply a sliding window from left to right. The model may leverage structure-enabled training. In terms of deep learning based computer vision, DeepOCR models target scenarios that are more inconsistent and noisy rather than normal OCR engines targeting specific cases such as well-scanned or printed documentation. Three main datasets were sourced for either training and testing, which are: ICDAR13, ICDAR15, and ICDAR17. Scenarios in these images are mostly scene text. Some samples of ICDAR13 images are shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0157" num="0161">Some samples of ICDAR2015 images are shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0158" num="0162">Comparing the two solutions described above, the combined solution (text detection+text recognition) processed slowly, but was agile for customization depending on the separated architecture. The second end-to-end solution was faster, but the performance is relatively low. Details are shown below in Table 2, showing a comparison of top scores.</p><p id="p-0159" num="0000"><tables id="TABLE-US-00002" num="00002"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="4"><colspec colname="offset" colwidth="77pt" align="left"/><colspec colname="1" colwidth="14pt" align="center"/><colspec colname="2" colwidth="49pt" align="center"/><colspec colname="3" colwidth="77pt" align="center"/><thead><row><entry/><entry namest="offset" nameend="3" rowsep="1">TABLE 2</entry></row><row><entry/><entry namest="offset" nameend="3" align="center" rowsep="1"/></row><row><entry/><entry/><entry>ICDAR2013</entry><entry>ICDAR2015</entry></row><row><entry/><entry namest="offset" nameend="3" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="6"><colspec colname="offset" colwidth="14pt" align="left"/><colspec colname="1" colwidth="63pt" align="left"/><colspec colname="2" colwidth="42pt" align="right"/><colspec colname="3" colwidth="21pt" align="left"/><colspec colname="4" colwidth="42pt" align="right"/><colspec colname="5" colwidth="35pt" align="left"/><tbody valign="top"><row><entry/><entry>End-to-End</entry><entry>0.8477</entry><entry>(F1)</entry><entry>0.6533</entry><entry>(F1)</entry></row><row><entry/><entry>Text Detection</entry><entry>0.952</entry><entry>(F1)</entry><entry>0.869</entry><entry>(F1)</entry></row><row><entry/><entry>Text Recognition</entry><entry>0.95</entry><entry>(Acc)</entry><entry>0.933</entry><entry>(Acc)</entry></row><row><entry/><entry namest="offset" nameend="5" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0160" num="0163">Among models in the first solution, the model from Clova was selected as a base model. As shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, comparing text detection models, the performance of the model from Clova was competitive, and the predictions were more flexible.</p><p id="p-0161" num="0164">Using mostly scanned images, OCR engines (ABBYY) output the word group. Examples are shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, showing a comparison between DeepOCR and OCR Engine.</p><p id="p-0162" num="0165"><figref idref="DRAWINGS">FIG. <b>8</b></figref> depicts a schematic diagram of a two-part pipeline for knowledge-based information extraction from richly formatted digital documentation, in accordance with some embodiments. In some embodiments, the pipeline shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref> may share any one or more characteristics in common with pipeline <b>300</b> shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref> above and/or with any other embodiments described herein. Described herein are characteristics of such pipelines for knowledge-based information extraction from richly formatted digital documentation, in accordance with some embodiments.</p><p id="p-0163" num="0166"><figref idref="DRAWINGS">FIG. <b>9</b></figref> shows a first page of a PDF document, which may serve as an input into the pipeline of <figref idref="DRAWINGS">FIG. <b>8</b></figref>.</p><p id="p-0164" num="0167"><figref idref="DRAWINGS">FIG. <b>10</b></figref> shows the result of the input PDF page being subject to a denoising operation and then binarized as an image.</p><p id="p-0165" num="0168">After denoising and binarizing as an image, a text detection model may be applied. <figref idref="DRAWINGS">FIG. <b>11</b></figref> shows bounding boxes, which may bound a word level, applied by a text detection model. In applying the text detection model, the function may be: detection_net. In applying the, the following customizations may be available:<ul id="ul0003" list-style="none">    <li id="ul0003-0001" num="0000">    <ul id="ul0004" list-style="none">        <li id="ul0004-0001" num="0169">trained_model: pretrained model for text detection</li>        <li id="ul0004-0002" num="0170">text_threshold: confidence threshold for detecting the text</li>        <li id="ul0004-0003" num="0171">low text: text low-bound score</li>        <li id="ul0004-0004" num="0172">link threshold: link confidence threshold</li>        <li id="ul0004-0005" num="0173">cuda: use cuda for inference (default:True)</li>        <li id="ul0004-0006" num="0174">canvas_size: max image size for inference</li>        <li id="ul0004-0007" num="0175">mag_ratio: image magnification ratio</li>        <li id="ul0004-0008" num="0176">poly: enable polygon type result</li>        <li id="ul0004-0009" num="0177">show_time: show processing time</li>        <li id="ul0004-0010" num="0178">test_folder: folder path to input images</li>        <li id="ul0004-0011" num="0179">refine: use link refiner for sentense-level dataset</li>        <li id="ul0004-0012" num="0180">refiner_model: pretrained refiner model</li>    </ul>    </li></ul></p><p id="p-0166" num="0181">After the text detection model is applied, missing information may be supplemented, for example using &#x2018;Canvas.&#x2019; <figref idref="DRAWINGS">FIG. <b>12</b></figref> shows how detected text may be covered by white boxes.</p><p id="p-0167" num="0182"><figref idref="DRAWINGS">FIG. <b>13</b></figref> shows how lines, dashes, and other noise may then be removed to keep only the missing information (shown in <figref idref="DRAWINGS">FIG. <b>13</b></figref> as white patches).</p><p id="p-0168" num="0183"><figref idref="DRAWINGS">FIG. <b>14</b></figref> shows how the identified white patches indicating missing information may be supplemented into the text detection results, with additional bounding-boxes (as compared to those from <figref idref="DRAWINGS">FIG. <b>11</b></figref>) showing the supplemented information.</p><p id="p-0169" num="0184">The system may then analyze different orientations and sizes of detected &#x201c;blobs,&#x201d; based on morphology (e.g., word group, paragraph, parts, etc.). As shown by the additional bounding boxes in <figref idref="DRAWINGS">FIG. <b>15</b></figref> (as compared to those from <figref idref="DRAWINGS">FIGS. <b>11</b> and <b>14</b></figref>), horizontal &#x201c;blobs&#x201d; may be identified as word groups.</p><p id="p-0170" num="0185">As shown by the additional bounding boxes in <figref idref="DRAWINGS">FIG. <b>16</b></figref> (as compared to those from <figref idref="DRAWINGS">FIGS. <b>11</b> and <b>14</b></figref>), larger &#x201c;blobs&#x201d; (e.g., as compared to others on the same page, others in the same document, others in the same document bundle, and/or as compared based on prior training of the system) may be identified as paragraphs and/or as sections with obvious distance.</p><p id="p-0171" num="0186">As shown by the additional bounding boxes in <figref idref="DRAWINGS">FIG. <b>17</b></figref> (as compared to those from <figref idref="DRAWINGS">FIGS. <b>11</b> and <b>14</b></figref>), the largest &#x201c;blobs&#x201d; (e.g., as compared to others on the same page, others in the same document, others in the same document bundle, and/or as compared based on prior training of the system) may be identified as indicative of structural segmentation of the document. In <figref idref="DRAWINGS">FIG. <b>17</b></figref>, in some embodiments, the additional bounding boxes correspond to the blobs that may be indicative of structural segmentation of the document. Identification of paragraphs as shown in <figref idref="DRAWINGS">FIG. <b>16</b></figref> and identification of structural segmentation as shown in <figref idref="DRAWINGS">FIG. <b>17</b></figref> may be carried out using different &#x201c;blob&#x201d; size thresholds. In some embodiments, information regarding structural segmentation of documents may be used to feed the network.</p><p id="p-0172" num="0187">The system may then, in some embodiments, sort words ordered from left to right, and may determine line labels, as shown for example in <figref idref="DRAWINGS">FIG. <b>18</b></figref>. With respect to how structural information may influence performance, &#x201c;blobs&#x201d; with different scales may not only be used for feature engineering but may also be used for inference correction. Thus, the model may use, for example, word-group-level &#x201c;blobs&#x201d; and line information to localize which entity a predicted word is located in.</p><p id="p-0173" num="0188">The system may then apply one or more text recognition algorithms on each bounding box and may thereby generate output data, for example by generating a CSV file with line and word group information for each token, as shown for example in <figref idref="DRAWINGS">FIG. <b>19</b></figref>.</p><p id="p-0174" num="0189">Data included in the output, as shown in <figref idref="DRAWINGS">FIG. <b>19</b></figref>, may include one or more of the following:<ul id="ul0005" list-style="none">    <li id="ul0005-0001" num="0000">    <ul id="ul0006" list-style="none">        <li id="ul0006-0001" num="0190">Slug: name of the document</li>        <li id="ul0006-0002" num="0191">Page: the page that the token belongs to</li>        <li id="ul0006-0003" num="0192">X0, y0: coordinates of the top left corner of the bounding box</li>        <li id="ul0006-0004" num="0193">X1, y1: coordinates of the bottom right corner of the bounding box</li>        <li id="ul0006-0005" num="0194">Rel_x0, rel_y0, rel_x1, rel_y1: relative coordinates; coordinates adjusted by the size of the document</li>        <li id="ul0006-0006" num="0195">Token: word identified by the text detection algorithm</li>        <li id="ul0006-0007" num="0196">Line_label: which line the token locates in the document</li>        <li id="ul0006-0008" num="0197">Word_group_label: generated by horizontal blobs (used for identify word group)</li>    </ul>    </li></ul></p><p id="p-0175" num="0198">The system may then utilize domain knowledge information, for example as received via one or more user inputs from a user of the system, to label the dataset. This may be referred to as a &#x201c;weak label function.&#x201d; The system may create annotations that may be validated by one or more users and may be used to train computer vision models that can extract information, thus allowing bootstrapping with little or no labeled data.</p><p id="p-0176" num="0199">For example, a user may want to extract data regarding committee names from documents, and the user's domain knowledge may include that &#x2018;JONES FOR SENATE&#x2019; is a committee name. After the user inputs this information into the solution, the system may scan all the training documents and label the words identified by the DeepOCR. For example, DeepOCR output for the document may be &#x201c;For instance, the &#x2018;JONES FOR SENATE&#x2019; is a committee name. Then the CSV file may be labeled as shown in <figref idref="DRAWINGS">FIG. <b>20</b></figref>.</p><p id="p-0177" num="0200">In this example, the solution correctly labeled the committee name (in the lines including the words &#x201c;JONES&#x201d;, &#x201c;FOR&#x201d;, and &#x201c;SENATE&#x201d;) and also incorrectly labeled the first instance of the word &#x201c;For&#x201d; as a part of the committee name. This demonstrates the weak label function that may generate some errors in the label column. In weak labeling, true labels may be interspersed with incorrect labels as noise; this noise may not damage the model performance significantly if the recall is high enough. Thus, it may be used in the systems described herein, with appropriate configurations as described herein, to label data.</p><p id="p-0178" num="0201">The system may be configured to perform feature engineering, for example as follows for the example image shown in <figref idref="DRAWINGS">FIG. <b>21</b></figref>.<ul id="ul0007" list-style="none">    <li id="ul0007-0001" num="0000">    <ul id="ul0008" list-style="none">        <li id="ul0008-0001" num="0202">Length_words: How many characters in this token; 16 characters</li>        <li id="ul0008-0002" num="0203">Word_size: the area of the bounding box; (x1-x0)*(y1-y0)</li>        <li id="ul0008-0003" num="0204">Relative_location: the order of the word; start from top left to bottom right</li>        <li id="ul0008-0004" num="0205">Num_upper_chars: number of upper characters; 14 upper characters</li>        <li id="ul0008-0005" num="0206">Title_word: tile word or not</li>        <li id="ul0008-0006" num="0207">Relative_word_size: word size over the maximum word size in the same page</li>        <li id="ul0008-0007" num="0208">Max_word_size: maximum word size in the page</li>        <li id="ul0008-0008" num="0209">Max_word_size_page: maximum word size in the page over the maximum word size in the document</li>        <li id="ul0008-0009" num="0210">Num_words_in_page: number of words in the page</li>        <li id="ul0008-0010" num="0211">X0_min: the minimum value of x0 in the page</li>        <li id="ul0008-0011" num="0212">X1_max: the maximum value of x1 in the page</li>        <li id="ul0008-0012" num="0213">Y0_min: the minimum value of the y0 in the page</li>        <li id="ul0008-0013" num="0214">Y1_max: the maximum value of y1 in the page</li>        <li id="ul0008-0014" num="0215">Line_label_max: the number of lines in the page</li>    </ul>    </li></ul></p><p id="p-0179" num="0216">The system may apply one or more knowledge models. The system may apply an activation function, for example a self-regularized non-monotonic neural activation function. The derivative of the activation function used by the system may be smoother than Relu, which may improve the rate of convergence.</p><p id="p-0180" num="0217">In order to make sure the distribution of the label is stable, variational batch size may be used to train the model, which can ensure the model is trained with the same amount of documents in each batch. This may reduce the risk of gradient explosion.</p><p id="p-0181" num="0218">In some embodiments, an embedding layer of the network may be constructed with the fasttext word embedding. This method may improve the rate of convergence and the accuracy rate of the model.</p><p id="p-0182" num="0219">In some embodiments, the systems and methods described herein may offer superior performance as compared to named-entity recognition (NER) models.</p><p id="p-0183" num="0220">Named-entity recognition is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentage, etc. <figref idref="DRAWINGS">FIG. <b>22</b></figref> shows an architecture for a named-entity recognition model, in accordance with some embodiments.</p><p id="p-0184" num="0221">Tools for NER, including Spacy, StanfordNLP, and Bert, were trained with a large number of documents. However, the major part for the documents that were used for said training are paragraphs, not word groups. This means the NER pretrained model may not be suitable for processing of all document types, such as richly formatted documents.</p><p id="p-0185" num="0222"><figref idref="DRAWINGS">FIG. <b>23</b></figref> shows output data from a named-entity recognition model, in accordance with some embodiments.</p><p id="p-0186" num="0223">A NER model was applied to the same testing data as described hereinabove, and different bounding boxes were used to annotate the named entities. Results are shown in <figref idref="DRAWINGS">FIG. <b>24</b></figref>, where different bounding box types (e.g., which may be displayed by a display system in different colors and may correspond to different stored metadata associated with the bounding box) may correspond to the following meanings:<ul id="ul0009" list-style="none">    <li id="ul0009-0001" num="0000">    <ul id="ul0010" list-style="none">        <li id="ul0010-0001" num="0224">&#x2018;CARDINAL&#x2019;</li>        <li id="ul0010-0002" num="0225">&#x2018;ORG&#x2019;</li>        <li id="ul0010-0003" num="0226">&#x2018;DATE&#x2019;</li>        <li id="ul0010-0004" num="0227">&#x2018;LANGUAGE&#x2019;</li>        <li id="ul0010-0005" num="0228">&#x2018;GPE&#x2019;</li>        <li id="ul0010-0006" num="0229">&#x2018;PRODUCT&#x2019;</li>        <li id="ul0010-0007" num="0230">&#x2018;PERSON&#x2019;</li>        <li id="ul0010-0008" num="0231">&#x2018;Target Entities&#x2019; (Ground Truth)</li>    </ul>    </li></ul></p><p id="p-0187" num="0232">It was observed that the NER model did not detect the ground truth, &#x2018;Smart Media Group&#x2019; and &#x2018;SPENC-Spence for Governor&#x2019;, which are an agency name and a committee, respectively.</p><p id="p-0188" num="0233">But using the NER model on a full sentence like &#x2018;SMART MEDIA GROUP advertises in KSHB-TV.&#x2019;, the NER correctly recognizes the &#x2018;SMART MEDIA GROUP&#x2019; as an Organization, as shown by the application of the NER model to the full sentence in <figref idref="DRAWINGS">FIG. <b>25</b></figref>.</p><p id="p-0189" num="0234">Thus, for documents with paragraph structure, an NER model may be a good solution. However, for documents that are richly formatted and in which paragraphs are not a major part of the documents, NER models may have only limited applicability, and the other systems and methods described herein may offer improvements and advantages.</p><heading id="h-0011" level="2">Computer</heading><p id="p-0190" num="0235"><figref idref="DRAWINGS">FIG. <b>26</b></figref> illustrates an example of a computer, according to some embodiments. Computer <b>2600</b> can be a component of a system for providing an AI-augmented auditing platform including techniques for providing AI-explainability for processing data through multiple layers. In some embodiments, computer <b>2600</b> may execute any one or more of the methods described herein.</p><p id="p-0191" num="0236">Computer <b>2600</b> can be a host computer connected to a network. Computer <b>2600</b> can be a client computer or a server. As shown in <figref idref="DRAWINGS">FIG. <b>26</b></figref>, computer <b>2600</b> can be any suitable type of microprocessor-based device, such as a personal computer, workstation, server, or handheld computing device, such as a phone or tablet. The computer can include, for example, one or more of processor <b>2610</b>, input device <b>2620</b>, output device <b>2630</b>, storage <b>2640</b>, and communication device <b>2660</b>. Input device <b>2620</b> and output device <b>2630</b> can correspond to those described above and can either be connectable or integrated with the computer.</p><p id="p-0192" num="0237">Input device <b>2620</b> can be any suitable device that provides input, such as a touch screen or monitor, keyboard, mouse, or voice-recognition device. Output device <b>2630</b> can be any suitable device that provides an output, such as a touch screen, monitor, printer, disk drive, or speaker.</p><p id="p-0193" num="0238">Storage <b>2640</b> can be any suitable device that provides storage, such as an electrical, magnetic, or optical memory, including a random access memory (RAM), cache, hard drive, CD-ROM drive, tape drive, or removable storage disk. Communication device <b>2660</b> can include any suitable device capable of transmitting and receiving signals over a network, such as a network interface chip or card. The components of the computer can be connected in any suitable manner, such as via a physical bus or wirelessly. Storage <b>2640</b> can be a non-transitory computer-readable storage medium comprising one or more programs, which, when executed by one or more processors, such as processor <b>2610</b>, cause the one or more processors to execute methods described herein.</p><p id="p-0194" num="0239">Software <b>2650</b>, which can be stored in storage <b>2640</b> and executed by processor <b>2610</b>, can include, for example, the programming that embodies the functionality of the present disclosure (e.g., as embodied in the systems, computers, servers, and/or devices as described above). In some embodiments, software <b>2650</b> can include a combination of servers such as application servers and database servers.</p><p id="p-0195" num="0240">Software <b>2650</b> can also be stored and/or transported within any computer-readable storage medium for use by or in connection with an instruction execution system, apparatus, or device, such as those described above, that can fetch and execute instructions associated with the software from the instruction execution system, apparatus, or device. In the context of this disclosure, a computer-readable storage medium can be any medium, such as storage <b>2640</b>, that can contain or store programming for use by or in connection with an instruction execution system, apparatus, or device.</p><p id="p-0196" num="0241">Software <b>2650</b> can also be propagated within any transport medium for use by or in connection with an instruction execution system, apparatus, or device, such as those described above, that can fetch and execute instructions associated with the software from the instruction execution system, apparatus, or device. In the context of this disclosure, a transport medium can be any medium that can communicate, propagate, or transport programming for use by or in connection with an instruction execution system, apparatus, or device. The transport-readable medium can include but is not limited to, an electronic, magnetic, optical, electromagnetic, or infrared wired or wireless propagation medium.</p><p id="p-0197" num="0242">Computer <b>2600</b> may be connected to a network, which can be any suitable type of interconnected communication system. The network can implement any suitable communications protocol and can be secured by any suitable security protocol. The network can comprise network links of any suitable arrangement that can implement the transmission and reception of network signals, such as wireless network connections, T1 or T3 lines, cable networks, DSL, or telephone lines.</p><p id="p-0198" num="0243">Computer <b>2600</b> can implement any operating system suitable for operating on the network. Software <b>2650</b> can be written in any suitable programming language, such as C, C++, Java, or Python. In various embodiments, application software embodying the functionality of the present disclosure can be deployed in different configurations, such as in a client/server arrangement or through a Web browser as a Web-based application or Web service, for example.</p><p id="p-0199" num="0244">Following is a list of embodiments:<ul id="ul0011" list-style="none">    <li id="ul0011-0001" num="0000">    <ul id="ul0012" list-style="none">        <li id="ul0012-0001" num="0245">Embodiment 1. A system for determining the composition of document bundles, the system comprising one or more processors configured to cause the system to:        <ul id="ul0013" list-style="none">            <li id="ul0013-0001" num="0246">receive data comprising a document bundle;</li>            <li id="ul0013-0002" num="0247">extract, from the document bundle, first information comprising substantive content of one or more documents of the document bundle;</li>            <li id="ul0013-0003" num="0248">extract, from the document bundle, second information comprising metadata associated with one or more documents of the document bundle; and</li>            <li id="ul0013-0004" num="0249">generate, based on the first information and the second information, output data representing a composition of the document bundle.</li>        </ul>        </li>        <li id="ul0012-0002" num="0250">Embodiment 2. The system of embodiment 1, wherein the output data representing a composition of the document bundle represents one or more delineations between page boundaries in the document bundle.</li>        <li id="ul0012-0003" num="0251">Embodiment 3. The system of embodiment 1-2, wherein generating the output data is further based on context information received from a data source separate from the document bundle.</li>        <li id="ul0012-0004" num="0252">Embodiment 4. The system of embodiment 3, wherein the context information comprises ERP data received from an ERP system of an entity associated with the document bundle.</li>        <li id="ul0012-0005" num="0253">Embodiment 5. The system of embodiment 3-4, wherein the context information comprises data specifying a predefined set of events associated with a process associated with the document bundle.</li>        <li id="ul0012-0006" num="0254">Embodiment 6. The system of embodiment 3-5, wherein the context information comprises data characterizing a request, wherein the data comprising the document bundle was received by the system in response to the request.</li>        <li id="ul0012-0007" num="0255">Embodiment 7. The system of embodiment 3-6, wherein the context information comprises data characterizing an automation process flow for acquiring the data.</li>        <li id="ul0012-0008" num="0256">Embodiment 8. The system of embodiment 1-7, wherein the metadata comprises one or more of: a file name, a file extension, a file creator, and a file date.</li>        <li id="ul0012-0009" num="0257">Embodiment 9. The system of embodiment 1-8, wherein extracting the first information comprises applying embedded object type detection.</li>        <li id="ul0012-0010" num="0258">Embodiment 10. The system of embodiment 1-9, wherein generating the output data comprises applying a page similarity assessment model to a plurality of pages of the document bundle.</li>        <li id="ul0012-0011" num="0259">Embodiment 11. The system of embodiment 1-10, wherein generating the output data comprises applying a finite state modeling data processing operation to the document bundle.</li>        <li id="ul0012-0012" num="0260">Embodiment 12. A non-transitory computer-readable storage medium storing instructions for determining the composition of document bundles, the instructions configured to be executed by one or more processors of a system to cause the system to:        <ul id="ul0014" list-style="none">            <li id="ul0014-0001" num="0261">receive data comprising a document bundle;</li>            <li id="ul0014-0002" num="0262">extract, from the document bundle, first information comprising substantive content of one or more documents of the document bundle;</li>            <li id="ul0014-0003" num="0263">extract, from the document bundle, second information comprising metadata associated with one or more documents of the document bundle; and</li>            <li id="ul0014-0004" num="0264">generate, based on the first information and the second information, output data representing a composition of the document bundle.</li>        </ul>        </li>        <li id="ul0012-0013" num="0265">Embodiment 13. A method for determining the composition of document bundles, wherein the method is performed by a system comprising one or more processors, the method comprising:        <ul id="ul0015" list-style="none">            <li id="ul0015-0001" num="0266">receiving data comprising a document bundle;</li>            <li id="ul0015-0002" num="0267">extracting, from the document bundle, first information comprising substantive content of one or more documents of the document bundle;</li>            <li id="ul0015-0003" num="0268">extracting, from the document bundle, second information comprising metadata associated with one or more documents of the document bundle; and</li>            <li id="ul0015-0004" num="0269">generating, based on the first information and the second information, output data representing a composition of the document bundle.</li>        </ul>        </li>        <li id="ul0012-0014" num="0270">Embodiment 14. A system for validating signatures in documents, the system comprising one or more processors configured to cause the system to:        <ul id="ul0016" list-style="none">            <li id="ul0016-0001" num="0271">receive an electronic document comprising one or more signatures;</li>            <li id="ul0016-0002" num="0272">apply one or more signature-extraction models to the electronic document to generate, for each of the one or more signatures in the electronic document, data representing a spatial location for the respective signature and a confidence level for the respective signature; and</li>            <li id="ul0016-0003" num="0273">determine, based on the data representing the spatial location and the confidence level, whether the electronic document satisfies a set of signature criteria.</li>        </ul>        </li>        <li id="ul0012-0015" num="0274">Embodiment 15. The system of embodiment 14, wherein the one or more signature-extraction models comprise a first signature-extraction model configured to recognize signatures regardless of spatial location.</li>        <li id="ul0012-0016" num="0275">Embodiment 16. The system of embodiment 14-15, wherein the one or more signature-extraction models comprise a second signature-extraction model configured to recognize signatures based on in-document spatial location.</li>        <li id="ul0012-0017" num="0276">Embodiment 17. The system of embodiment 16, wherein applying the second signature-extraction model comprises:        <ul id="ul0017" list-style="none">            <li id="ul0017-0001" num="0277">determining a predicted spatial location within the electronic document based on one or more of a structure, format, and type of the electronic document; and</li>            <li id="ul0017-0002" num="0278">extracting a signature from the predicted spatial location.</li>        </ul>        </li>        <li id="ul0012-0018" num="0279">Embodiment 18. The system of embodiment 14-17, wherein determining whether the electronic document satisfies the set of signature criteria comprises determining whether a signature appears in the electronic document at a required spatial location.</li>        <li id="ul0012-0019" num="0280">Embodiment 19. The system of embodiment 14-18, wherein determining whether the electronic document satisfies the set of signature criteria comprises determining whether the confidence level exceeds a predefined threshold.</li>        <li id="ul0012-0020" num="0281">Embodiment 20. The system of embodiment 14-19, wherein determining whether the electronic document satisfies the set of signature criteria comprises determining whether a signature appears in the electronic document within a required spatial proximity to a component extracted from the document.</li>        <li id="ul0012-0021" num="0282">Embodiment 21. The system of embodiment 14-20, wherein determining whether the electronic document satisfies the set of signature criteria comprises generating an association score indicting a level of association between a signature extracted from the electronic document and signature-context data generated based the electronic document.</li>        <li id="ul0012-0022" num="0283">Embodiment 22. The system of embodiment 14-21, wherein the system is configured to determine the set of signature criteria based at least in part on context data, wherein the context data indicates one or more of: document type, document structure, and document format.</li>        <li id="ul0012-0023" num="0284">Embodiment 23. The system of embodiment 14-22, wherein the system is configured to determine the set of signature criteria based at least in part on the one or more signatures detected in the document.</li>        <li id="ul0012-0024" num="0285">Embodiment 24. A non-transitory computer-readable storage medium storing instructions for validating signatures in documents, the instructions configured to be executed by a one or more processors of a system to cause the system to:        <ul id="ul0018" list-style="none">            <li id="ul0018-0001" num="0286">receive an electronic document comprising one or more signatures;</li>            <li id="ul0018-0002" num="0287">apply one or more signature-extraction models to the electronic document to generate, for each of the one or more signatures in the electronic document, data representing a spatial location for the respective signature and a confidence level for the respective signature; and</li>            <li id="ul0018-0003" num="0288">determine, based on the data representing the spatial location and the confidence level, whether the electronic document satisfies a set of signature criteria.</li>        </ul>        </li>        <li id="ul0012-0025" num="0289">Embodiment 25. A method for validating signatures in documents, wherein the method is performed by a system comprising one or more processors, the method comprising:        <ul id="ul0019" list-style="none">            <li id="ul0019-0001" num="0290">receiving an electronic document comprising one or more signatures;</li>            <li id="ul0019-0002" num="0291">applying one or more signature-extraction models to the electronic document to generate, for each of the one or more signatures in the electronic document, data representing a spatial location for the respective signature and a confidence level for the respective signature; and</li>            <li id="ul0019-0003" num="0292">determining, based on the data representing the spatial location and the confidence level, whether the electronic document satisfies a set of signature criteria.</li>        </ul>        </li>        <li id="ul0012-0026" num="0293">Embodiment 26. A system for extracting information from documents, the system comprising one or more processors configured to cause the system to:        <ul id="ul0020" list-style="none">            <li id="ul0020-0001" num="0294">receive a data set comprising a plurality of electronic documents;</li>            <li id="ul0020-0002" num="0295">apply a set of data conversion processing steps to the plurality of electronic documents to generate a processed data set comprising structured data generated based on the plurality of electronic documents, wherein applying set of data conversion processing steps comprises applying one or more deep-learning-based optical character recognition (OCR) models; and</li>            <li id="ul0020-0003" num="0296">apply a set of knowledge-based modeling processing steps to the structured data, wherein applying the set of knowledge-based modeling processing steps comprises:            <ul id="ul0021" list-style="none">                <li id="ul0021-0001" num="0297">applying a knowledge-based deep learning model trained based on the structured data and a plurality of data labels indicated by one or more user inputs; and</li>                <li id="ul0021-0002" num="0298">generating output data extracted from the plurality of electronic documents by the deep learning model.</li>            </ul>            </li>        </ul>        </li>        <li id="ul0012-0027" num="0299">Embodiment 27. The system of embodiment 26, wherein applying the set of data conversion processing steps comprises, before applying the one or more deep-learning-based OCR models, applying an automated orientation correction processing step.</li>        <li id="ul0012-0028" num="0300">Embodiment 28. The system of embodiment 26-27, wherein applying the set of data conversion processing steps comprises, before applying the one or more deep-learning-based OCR models, applying a denoising function.</li>        <li id="ul0012-0029" num="0301">Embodiment 29. The system of embodiment 26-28, wherein applying the one or more deep-learning-based OCR models comprises:        <ul id="ul0022" list-style="none">            <li id="ul0022-0001" num="0302">applying a text-detection model; and</li>            <li id="ul0022-0002" num="0303">applying a text-recognition model.</li>        </ul>        </li>        <li id="ul0012-0030" num="0304">Embodiment 30. The system of embodiment 26-29, wherein applying the set of data conversion processing steps comprises, after applying the one or more deep-learning-based OCR models, generating the structured data based on an image-level feature engineering step.</li>        <li id="ul0012-0031" num="0305">Embodiment 31. The system of embodiment 26-30, wherein applying the set of data conversion processing steps comprises applying a post-processing method that uses morphology to parse structural relationships amongst words.</li>        <li id="ul0012-0032" num="0306">Embodiment 32. The system of embodiment 26-31, wherein applying the set of knowledge-based modeling processing steps comprises, before receiving the user input indicating the plurality of data labels, generating the structured data based on one or more feature engineering processing steps.</li>        <li id="ul0012-0033" num="0307">Embodiment 33. The system of embodiment 32, wherein the one or more feature engineering processing steps comprise predicting word groups based on morphology.</li>        <li id="ul0012-0034" num="0308">Embodiment 34. The system of embodiment 26-33, wherein applying the set of knowledge-based modeling processing steps comprises applying a model trained based on user used for user-defined feature engineering.</li>        <li id="ul0012-0035" num="0309">Embodiment 35. The system of embodiment 26-34, wherein applying the set of knowledge-based modeling processing steps comprises applying fuzzy matching, wherein the system is configured to consider a partial match sufficient for labeling purposes, to automatically label documents on a word-by-word basis.</li>        <li id="ul0012-0036" num="0310">Embodiment 36. The system of embodiment 26-35, wherein applying the set of knowledge-based modeling processing steps comprises automatically correcting one or more text-recognition errors during a training process.</li>        <li id="ul0012-0037" num="0311">Embodiment 37. The system of embodiment 26-36, wherein the knowledge-based deep learning model comprises a loss function that is configured to accelerate convergence of the knowledge-based deep learning model.</li>        <li id="ul0012-0038" num="0312">Embodiment 38. The system of embodiment 26-37, wherein the knowledge-based deep learning model comprises one or more layers using natural language processing (NLP) embedding such that the model learns both content information and related location information</li>        <li id="ul0012-0039" num="0313">Embodiment 39. The system of embodiment 26-38, wherein the knowledge-based deep learning model is trained using an adaptive feeding method.</li>        <li id="ul0012-0040" num="0314">Embodiment 40. The system of embodiment 26-39, wherein the knowledge-based deep learning model comprises an input layer that applies merged embedding.</li>        <li id="ul0012-0041" num="0315">Embodiment 41. The system of embodiment 26-40, wherein the knowledge-based deep learning model comprises an input layer that is configured for variant batch sizes.</li>        <li id="ul0012-0042" num="0316">Embodiment 42. The system of embodiment 26-41, wherein the knowledge-based deep learning model comprises an input layer that applies a sliding window.</li>        <li id="ul0012-0043" num="0317">Embodiment 43. The system of embodiment 26-42, wherein the knowledge-based deep learning model comprises one or more fully-dense layers disposed between an input layer and a prediction layer.</li>        <li id="ul0012-0044" num="0318">Embodiment 44. The system of embodiment 26-43, wherein the knowledge-based deep learning model comprises a prediction layer that generates one or more metrics for presentation to a user.</li>        <li id="ul0012-0045" num="0319">Embodiment 45. A non-transitory computer-readable storage medium storing instructions for extracting information from documents, the instructions configured to be executed by one or more processors of a system to cause the system to:        <ul id="ul0023" list-style="none">            <li id="ul0023-0001" num="0320">receive a data set comprising a plurality of electronic documents;</li>            <li id="ul0023-0002" num="0321">apply a set of data conversion processing steps to the plurality of electronic documents to generate a processed data set comprising structured data generated based on the plurality of electronic documents, wherein applying set of data conversion processing steps comprises applying one or more deep-learning-based optical character recognition (OCR) models; and</li>            <li id="ul0023-0003" num="0322">apply a set of knowledge-based modeling processing steps to the structured data, wherein applying the set of knowledge-based modeling processing steps comprises:            <ul id="ul0024" list-style="none">                <li id="ul0024-0001" num="0323">applying a knowledge-based deep learning model trained based on the structured data and a plurality of data labels indicated by one or more user inputs; and</li>                <li id="ul0024-0002" num="0324">generating output data extracted from the plurality of electronic documents by the deep learning model.</li>            </ul>            </li>        </ul>        </li>        <li id="ul0012-0046" num="0325">Embodiment 46. A method for extracting information from documents, wherein the method is executed by a system comprising one or more processors, the method comprising:        <ul id="ul0025" list-style="none">            <li id="ul0025-0001" num="0326">receiving a data set comprising a plurality of electronic documents;</li>            <li id="ul0025-0002" num="0327">applying a set of data conversion processing steps to the plurality of electronic documents to generate a processed data set comprising structured data generated based on the plurality of electronic documents, wherein applying set of data conversion processing steps comprises applying one or more deep-learning-based optical character recognition (OCR) models; and</li>            <li id="ul0025-0003" num="0328">applying a set of knowledge-based modeling processing steps to the structured data, wherein applying the set of knowledge-based modeling processing steps comprises:            <ul id="ul0026" list-style="none">                <li id="ul0026-0001" num="0329">applying a knowledge-based deep learning model trained based on the structured data and a plurality of data labels indicated by one or more user inputs; and</li>                <li id="ul0026-0002" num="0330">generating output data extracted from the plurality of electronic documents by the deep learning model.</li>            </ul>            </li>        </ul>        </li>    </ul>    </li></ul></p><p id="p-0200" num="0331">This application incorporates by reference the entire contents of the U.S. patent application titled &#x201c;AI-AUGMENTED AUDITING PLATFORM INCLUDING TECHNIQUES FOR AUTOMATED ASSESSMENT OF VOUCHING EVIDENCE&#x201d;, filed Jun. 30, 2022, Attorney Docket no. 13574-20068.00.</p><p id="p-0201" num="0332">This application incorporates by reference the entire contents of the U.S. patent application titled &#x201c;AI-AUGMENTED AUDITING PLATFORM INCLUDING TECHNIQUES FOR AUTOMATED ADJUDICATION OF COMMERCIAL SUBSTANCE, RELATED PARTIES, AND COLLECTABILITY&#x201d;, filed Jun. 30, 2022, Attorney Docket no. 13574-20069.00.</p><p id="p-0202" num="0333">This application incorporates by reference the entire contents of the U.S. patent application titled &#x201c;AI-AUGMENTED AUDITING PLATFORM INCLUDING TECHNIQUES FOR, APPLYING A COMPOSABLE ASSURANCE? INTEGRITY FRAMEWORK&#x201d;, filed Jun. 30, 2022, Attorney Docket no. 13574-20070.00.</p><p id="p-0203" num="0334">This application incorporates by reference the entire contents of the U.S. patent application titled &#x201c;AI-AUGMENTED AUDITING PLATFORM INCLUDING TECHNIQUES FOR PROVIDING AI-EXPLAINABILITY FOR PROCESSING DATA THROUGH MULTIPLE LAYERS&#x201d;, filed Jun. 30, 2022, Attorney Docket no. 13574-20072.00.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A system for determining the composition of document bundles, the system comprising one or more processors configured to cause the system to:<claim-text>receive data comprising a document bundle;</claim-text><claim-text>extract, from the document bundle, first information comprising substantive content of one or more documents of the document bundle;</claim-text><claim-text>extract, from the document bundle, second information comprising metadata associated with one or more documents of the document bundle; and</claim-text><claim-text>generate, based on the first information and the second information, output data representing a composition of the document bundle.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the output data representing a composition of the document bundle represents one or more delineations between page boundaries in the document bundle.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein generating the output data is further based on context information received from a data source separate from the document bundle.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The system of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the context information comprises ERP data received from an ERP system of an entity associated with the document bundle.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The system of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the context information comprises data specifying a predefined set of events associated with a process associated with the document bundle.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The system of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the context information comprises data characterizing a request, wherein the data comprising the document bundle was received by the system in response to the request.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The system of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the context information comprises data characterizing an automation process flow for acquiring the data.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the metadata comprises one or more of: a file name, a file extension, a file creator, and a file date.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein extracting the first information comprises applying embedded object type detection.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein generating the output data comprises applying a page similarity assessment model to a plurality of pages of the document bundle.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein generating the output data comprises applying a finite state modeling data processing operation to the document bundle.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. A non-transitory computer-readable storage medium storing instructions for determining the composition of document bundles, the instructions configured to be executed by one or more processors of a system to cause the system to:<claim-text>receive data comprising a document bundle;</claim-text><claim-text>extract, from the document bundle, first information comprising substantive content of one or more documents of the document bundle;</claim-text><claim-text>extract, from the document bundle, second information comprising metadata associated with one or more documents of the document bundle; and</claim-text><claim-text>generate, based on the first information and the second information, output data representing a composition of the document bundle.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. A method for determining the composition of document bundles, wherein the method is performed by a system comprising one or more processors, the method comprising:<claim-text>receiving data comprising a document bundle;</claim-text><claim-text>extracting, from the document bundle, first information comprising substantive content of one or more documents of the document bundle;</claim-text><claim-text>extracting, from the document bundle, second information comprising metadata associated with one or more documents of the document bundle; and</claim-text><claim-text>generating, based on the first information and the second information, output data representing a composition of the document bundle.</claim-text></claim-text></claim></claims></us-patent-application>