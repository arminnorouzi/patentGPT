<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230007230A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230007230</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17757083</doc-number><date>20201102</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>JP</country><doc-number>2019-228788</doc-number><date>20191219</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>13</main-group><subgroup>366</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>143</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>74</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>40</main-group><subgroup>16</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>40</main-group><subgroup>18</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20180501</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>13</main-group><subgroup>366</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>143</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>761</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>40</main-group><subgroup>171</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>40</main-group><subgroup>193</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>10048</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>2201</main-group><subgroup>07</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">IMAGE PROCESSING DEVICE, IMAGE PROCESSING METHOD, AND IMAGE PROCESSING PROGRAM</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>SONY GROUP CORPORATION</orgname><address><city>TOKYO</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>ICHINOSE</last-name><first-name>TSUTOMU</first-name><address><city>TOKYO</city><country>JP</country></address></addressbook></inventor></inventors></us-parties><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/JP2020/041021</doc-number><date>20201102</date></document-id><us-371c12-date><date>20220609</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">An image processing device includes: a light emitting unit that emits a near-infrared ray to a target; a light emission controlling unit that controls a light emission amount of the light emitting unit on a basis of a distance value between the target and the light emitting unit; and a detecting unit that detects a feature point on the basis of a captured image of the target irradiated with the near-infrared ray.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="64.26mm" wi="158.75mm" file="US20230007230A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="177.21mm" wi="133.01mm" file="US20230007230A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="192.53mm" wi="57.91mm" orientation="landscape" file="US20230007230A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="221.40mm" wi="123.78mm" orientation="landscape" file="US20230007230A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="108.29mm" wi="151.30mm" file="US20230007230A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="205.06mm" wi="100.50mm" orientation="landscape" file="US20230007230A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="218.95mm" wi="50.21mm" orientation="landscape" file="US20230007230A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="238.00mm" wi="139.19mm" file="US20230007230A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="219.63mm" wi="107.02mm" orientation="landscape" file="US20230007230A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">FIELD</heading><p id="p-0002" num="0001">The present invention relates to an image processing device, an image processing method, and an image processing program.</p><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">Various types of technology for displaying a stereoscopic image on a display have been proposed. Among them, proposals have also been made regarding naked-eye stereoscopic display that does not use any tools such as glasses. For example, as displays related to naked-eye stereoscopic display, light field displays represented by a lenticular method are known.</p><p id="p-0004" num="0003">In a case where a stereoscopic image is displayed on a light field display, viewpoint positions on the right and left of a user are each detected, optimal light beams are collected at the viewpoint positions, and an image for the right eye and an image for the left eye are generated. For example, as technology for detecting a viewpoint position, Patent Literature 1 describes detecting a feature point of an image and tracking the feature point as a line-of-sight position.</p><heading id="h-0003" level="1">CITATION LIST</heading><heading id="h-0004" level="1">Patent Literature</heading><p id="p-0005" num="0000"><ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0004">Patent Literature 1: JP 2004-195141 A</li></ul></p><heading id="h-0005" level="1">Non Patent Literature</heading><p id="p-0006" num="0000"><ul id="ul0002" list-style="none">    <li id="ul0002-0001" num="0005">Non Patent Literature 1: Jean-Yves Bouguet &#x201c;Pyramidal Implementation of the Lucas Kanade Feature Tracker Description of the algorithm&#x201d; Intel Corporation Microprocessor Research Labs (2000) OpenCV Documents</li></ul></p><heading id="h-0006" level="1">SUMMARY</heading><heading id="h-0007" level="1">Technical Problem</heading><p id="p-0007" num="0006">However, in the above-described conventional technology, there is room for improvement in processing of tracking a viewpoint position of a user.</p><p id="p-0008" num="0007">Therefore, the present disclosure provides an image processing device, an image processing method, and an image processing program capable of appropriately tracking a viewpoint position of a user.</p><heading id="h-0008" level="1">Solution to Problem</heading><p id="p-0009" num="0008">To solve the problems described above, an image processing device according to an embodiment of the present disclosure includes: a light emitting unit that emits a near-infrared ray to a target; a light emission controlling unit that controls a light emission amount of the light emitting unit on a basis of a distance value between the target and the light emitting unit; and a detecting unit that detects a feature point on a basis of a captured image of the target irradiated with the near-infrared ray.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0009" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram illustrating an exemplary external appearance of an information processing device according to the present embodiment.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram illustrating a system configuration example of the information processing device according to the present embodiment.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a block diagram illustrating a configuration example of a tracking device according to the present embodiment.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram illustrating an example of current specifying information.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram for explaining a captured image according to the present embodiment.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a diagram for explaining other processing of a selection unit according to the embodiment.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart illustrating a processing procedure of a tracking device of the present embodiment.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a hardware configuration diagram illustrating an example of a computer that implements the functions of the tracking device.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0010" level="1">DESCRIPTION OF EMBODIMENTS</heading><p id="p-0018" num="0017">Hereinafter, embodiments of the present disclosure will be described in detail on the basis of the drawings. Note that in each of the following embodiments, the same parts are denoted by the same symbols, and redundant description will be omitted.</p><p id="p-0019" num="0018">In addition, the present disclosure will be described in the following order of items.</p><p id="p-0020" num="0019">1. Embodiments</p><p id="p-0021" num="0020">1.1 Exemplary External Appearance of Information Processing Device</p><p id="p-0022" num="0021">1.2 System Configuration Example of Information Processing Device</p><p id="p-0023" num="0022">1.3 Configuration Example of Tracking Device</p><p id="p-0024" num="0023">1.4 Processing Procedure of Tracking Device</p><p id="p-0025" num="0024">1.5 Effects of Tracking Device</p><p id="p-0026" num="0025">2. Hardware Configuration</p><p id="p-0027" num="0026">3. Conclusion</p><heading id="h-0011" level="1">1. Embodiments</heading><p id="p-0028" num="0027">[1.1 Exemplary External Appearance of Information Processing Device]</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram illustrating an exemplary external appearance of an information processing device <b>1</b> according to the present embodiment. For example, the information processing device <b>1</b> has a size similar to that of a laptop personal computer; however, it is possible to further downsized or upsized.</p><p id="p-0030" num="0029">The information processing device <b>1</b> includes a base <b>2</b> and a display <b>3</b> erected upward from the base <b>2</b>. The information processing device <b>1</b> includes a camera <b>4</b> above the display <b>3</b> and is configured so as to be able to capture an image of a user located in front of the display <b>3</b> by the camera <b>4</b>. Furthermore, in the information processing device <b>1</b>, a near-infrared ray emitting device <b>5</b> that emits a near-infrared ray to the user is disposed.</p><p id="p-0031" num="0030">For example, the information processing device <b>1</b> can display a stereoscopic image by the lenticular method on the display <b>3</b>. Schematically, the viewpoint positions of the user with naked eyes, who are not using glasses for stereoscopic display or the like, are detected using an image captured by the camera <b>4</b>. Images for the right eye and the left eye (parallax images) are generated by light beams condensed at the respective left and right viewpoint positions, and the generated images are displayed on the display <b>3</b> on which a lenticular lens is mounted. As a result, the user can view the stereoscopic image without using glasses, a head up display (HUD), or the like.</p><p id="p-0032" num="0031">[1.2 System Configuration Example of Information Processing Device]</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram illustrating a system configuration example of the information processing device <b>1</b> according to the present embodiment. The information processing device <b>1</b> schematically includes a tracking device <b>100</b> and a parallax image processing unit <b>20</b>. The tracking device <b>100</b> outputs information indicating the viewpoint positions of the user, for example, two-dimensional coordinates of the viewpoint positions, to the parallax image processing unit <b>20</b> as the subsequent stage. Note that details of the configuration, operation examples, and the like of the tracking device <b>100</b> will be described later. The tracking device <b>100</b> is an example of the &#x201c;image processing device&#x201d;.</p><p id="p-0034" num="0033">The parallax image processing unit <b>20</b> includes a spatial viewpoint coordinate generating unit <b>21</b>, a parallax image generating unit <b>22</b>, and a parallax image display unit <b>23</b>. The spatial viewpoint coordinate generating unit <b>21</b> converts the two-dimensional coordinates indicating the viewpoint positions output from the tracking device <b>100</b> into viewpoint coordinates at spatial positions by applying a known method and generates viewpoint coordinates in a space. The parallax image generating unit <b>22</b> generates a stereoscopic image by generating light beams (images) corresponding to the viewpoint coordinates in the space. The parallax image display unit <b>23</b> is a device that presents a stereoscopic image by continuously displaying parallax images generated by the parallax image generating unit <b>22</b> and corresponds to the display <b>3</b> described above.</p><p id="p-0035" num="0034">[1.3 Configuration Example of Tracking Device]</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a block diagram illustrating a configuration example of the tracking device according to the present embodiment. As illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the tracking device <b>100</b> includes a ranging unit <b>110</b>, a light emission controlling unit <b>120</b>, a wavelength controlling unit <b>130</b>, a near-infrared ray emitting unit <b>140</b>, a wavelength selection unit <b>140</b><i>a</i>, an image sensor <b>150</b>, a center of gravity detecting unit <b>150</b><i>a</i>, a face detecting unit <b>160</b>, and a selection unit <b>170</b>. Although not illustrated, it is based on the premise that the user is located in front of the near-infrared ray emitting unit <b>140</b>.</p><p id="p-0037" num="0036">In the example illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, an example in which the ranging unit <b>110</b>, the light emission controlling unit <b>120</b>, the wavelength controlling unit <b>130</b>, the near-infrared ray emitting unit <b>140</b>, the wavelength selection unit <b>140</b><i>a</i>, the image sensor <b>150</b>, the center of gravity detecting unit <b>150</b><i>a</i>, the face detecting unit <b>160</b>, and the selection unit <b>170</b> are included in the tracking device <b>100</b> is illustrated; however, it is not limited thereto. For example, the ranging unit <b>110</b>, the light emission controlling unit <b>120</b>, and the near-infrared ray emitting unit <b>140</b> may be included in another unit.</p><p id="p-0038" num="0037">The ranging unit <b>110</b> is a processing unit that calculates a distance value between the near-infrared ray emitting unit <b>140</b> and a target (user). The ranging unit <b>110</b> outputs the distance value to the light emission controlling unit <b>120</b>.</p><p id="p-0039" num="0038">For example, the ranging unit <b>110</b> calculates the distance value on the basis of the face detection result of the face detecting unit <b>160</b>. The face detection result includes coordinates of the user's left eye and coordinates of the user's right eye. The distance between the coordinates of the left eye and the coordinates of the right eye is referred to as an &#x201c;interocular distance&#x201d;. The ranging unit <b>110</b> calculates the distance value using a conversion table that defines the relationship between the interocular distance and the distance value. It is based on the premise that the relationship between the interocular distance and the distance value is set by calibration in advance.</p><p id="p-0040" num="0039">The light emission controlling unit <b>120</b> is a processing unit that controls the light emission amount of the near-infrared ray emitting unit <b>140</b> on the basis of the distance value. The light emission controlling unit <b>120</b> specifies the magnitude of the current depending on the distance value and outputs a current (or voltage) of the specified magnitude to the near-infrared ray emitting unit <b>140</b>.</p><p id="p-0041" num="0040">The light emission controlling unit <b>120</b> specifies the magnitude of the current depending on the distance value using current specifying information that defines the relationship between the distance value and the magnitude of the current. <figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram illustrating an example of current specifying information. Here, the current specifying information is illustrated by a two-axis graph. The vertical axis corresponds to the magnitude of the current, and the horizontal axis corresponds to the distance value. Amax denotes an upper limit value of the current. Amax satisfies the photobiological safety standards JIS C 7550 and IEC62471. That is, the user is safe when the near-infrared ray emitting unit <b>140</b> emits a near-infrared ray by a current having a magnitude less than Amax. For example, it can be said that the light emission controlling unit <b>120</b> specifies the magnitude of the current depending on the distance value by following the current specifying information in <figref idref="DRAWINGS">FIG. <b>4</b></figref> and, in a case where the magnitude of the current is larger than the threshold value (Amax), corrects the specified magnitude of the current to a magnitude less than the threshold value.</p><p id="p-0042" num="0041">As illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, according to the current definition information, as the distance value increases (the distance between the user and the near-infrared ray emitting unit <b>140</b> increases), the magnitude of the current increases. When the distance value exceeds d, the magnitude of the current is Ad. The current value Ad is a value less than the current value Amax and is set in advance.</p><p id="p-0043" num="0042">The wavelength controlling unit <b>130</b> is a processing unit that controls the wavelength of the near-infrared ray emitted by the near-infrared ray emitting unit <b>140</b>. For example, the wavelength controlling unit <b>130</b> selects a wavelength on the basis of a face detection result of the face detecting unit <b>160</b>, a detection result of the center of gravity of the center of gravity detecting unit <b>150</b><i>a</i>, and a captured image output from the image sensor <b>150</b>. First, the wavelength controlling unit <b>130</b> generates a binarized image by binarizing a captured image. As a result, pixels in dark portions of the captured image become &#x201c;1&#x201d;, and pixels in bright portions become &#x201c;0&#x201d;.</p><p id="p-0044" num="0043">The wavelength controlling unit <b>130</b> sets a partial region on the binary image on the basis of the face detection result of the face detecting unit <b>160</b> and the detection result of the center of gravity of the center of gravity detecting unit <b>150</b><i>a</i>. It is based on the premise that the partial region includes at least both eyes of the user. The wavelength controlling unit <b>130</b> calculates a ratio of pixels having a value of &#x201c;1&#x201d; (pixels in dark portions) to all the pixels included in the partial region. In a case where the ratio of pixels having a value of &#x201c;1&#x201d; is greater than or equal to a predetermined ratio, the wavelength controlling unit <b>130</b> selects such a wavelength that allows the ratio of pixels having a value of &#x201c;1&#x201d; to be less than the predetermined ratio.</p><p id="p-0045" num="0044">For example, there is a plurality of wavelength selection candidates. It is based on the premise that the wavelength of a selection candidate is included in the range of wavelengths of near infrared rays (0.75 to 1.4 &#x3bc;m). The wavelength controlling unit <b>130</b> selects one of the selection candidate wavelengths and outputs the selection result to the wavelength selection unit <b>140</b><i>a </i>of the near-infrared ray emitting unit <b>140</b>.</p><p id="p-0046" num="0045">Meanwhile, the wavelength controlling unit <b>130</b> may select a wavelength on the basis of a table in which the features of the user and selection candidates are associated with each other. The features of the user corresponds to the race of the user, the color of the eyes, and the like. It is based on the premise that information regarding the features of the user is specified in advance by an input device (not illustrated) or the like.</p><p id="p-0047" num="0046">The near-infrared ray emitting unit <b>140</b> is a device that emits a near-infrared ray to the user. The near-infrared ray emitting unit <b>140</b> corresponds to the near-infrared ray emitting device <b>5</b> described in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. The wavelength of the near-infrared ray emitted by the near-infrared ray emitting unit <b>140</b> is switched by the wavelength selection unit <b>140</b><i>a. </i></p><p id="p-0048" num="0047">The emission intensity of the near-infrared ray emitted by the near-infrared ray emitting unit <b>140</b> depends on the magnitude of the current (voltage) output from the light emission controlling unit <b>120</b>. That is, the emission intensity of the near-infrared ray emitted by the near-infrared ray emitting unit <b>140</b> increases as the current increases.</p><p id="p-0049" num="0048">The wavelength selection unit <b>140</b><i>a </i>is a processing unit that switches the wavelength of the near-infrared ray emitted by the near-infrared ray emitting unit <b>140</b> on the basis of the selection result of the wavelength output from the wavelength controlling unit <b>130</b>.</p><p id="p-0050" num="0049">The image sensor <b>150</b> is, for example, a complementary metal oxide semiconductor (CMOS) sensor. As the image sensor <b>150</b>, other sensors such as a charge coupled device (CCD) may be applied. The image sensor <b>150</b> captures an image of the user located in front of the display <b>3</b>, more specifically, the region around the face of the user and acquires the captured image. The captured image acquired by the image sensor <b>150</b> is analog-to-digital (A/D) converted and then output.</p><p id="p-0051" num="0050">Although not illustrated, an A/D converter or the like may be mounted on the image sensor <b>150</b> or may be provided between the image sensor <b>150</b> and the face detecting unit <b>160</b>. Note that the image sensor <b>150</b> according to the embodiment is configured so as to be capable of high-frame-rate imaging. As an example, the image sensor <b>150</b> can capture images at greater than or equal to 1000 frames per second (fps). In the embodiment, description is given on the premise that the image sensor <b>150</b> can capture images at 1000 fps.</p><p id="p-0052" num="0051">Here, the captured image includes a captured image in the IR band and a captured image of visible light. In the following description, a captured image in the IR band is referred to as an &#x201c;IR captured image&#x201d;. A captured image of visible light is referred to as a &#x201c;visible light captured image&#x201d;.</p><p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram for explaining a captured image according to the present embodiment. In the example illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the visible light captured image is a visible light captured image <b>30</b><i>a</i>. The IR captured image is an IR captured image <b>30</b><i>b. </i></p><p id="p-0054" num="0053">The face detecting unit <b>160</b> performs face detection on the basis of the captured image (visible light captured image) acquired by the image sensor <b>150</b> and detects the user's face frame, coordinates of the left eye, and coordinates of the right eye. The face detecting unit <b>160</b> outputs the face detection result to the ranging unit <b>110</b>, the center of gravity detecting unit <b>150</b><i>a</i>, and the selection unit <b>170</b>. As a method for detecting a face (face frame, coordinates of the left eye, and coordinates of the right eye) by the face detecting unit <b>160</b>, a known method, such as a method performed using features of an image, can be applied.</p><p id="p-0055" num="0054">If the distance between the user and the image sensor <b>150</b> is too long, the face detecting unit <b>160</b> may not be able to detect the left eye or the right eye from the captured image. In a case where the detection of the left eye or the right eye has failed, the face detecting unit <b>160</b> outputs information of the face frame to the ranging unit <b>110</b>, the center of gravity detecting unit <b>150</b><i>a</i>, and the selection unit <b>170</b> as a face detection result.</p><p id="p-0056" num="0055">Note that the image sensor <b>150</b> includes the center of gravity detecting unit <b>150</b><i>a</i>. The center of gravity detecting unit <b>150</b><i>a </i>is a processing unit that detects a gravity center point of the pupil or the iris in the face frame on the basis of the face detection result output from the face detecting unit <b>160</b>. For example, the center of gravity detecting unit <b>150</b><i>a </i>specifies the position of the pupil or the iris on the captured image (IR captured image) using the face detection result as a clue and calculates the position of the center of gravity of the pupil or the iris of the left eye and the right eye. The center of gravity detecting unit <b>150</b><i>a </i>outputs the detection result of the center of gravity to the wavelength controlling unit <b>130</b> and the selection unit <b>170</b>. Note that the center of gravity detecting unit <b>150</b><i>a </i>may be located outside the image sensor <b>150</b>.</p><p id="p-0057" num="0056">The selection unit <b>170</b> is a processing unit that selects one of the face detection result and the detection result of the center of gravity on the basis of the distance value output from the ranging unit <b>110</b>. For example, in a case where the distance value is greater than or equal to the threshold value, the selection unit <b>170</b> selects the face detection result. The selection unit <b>170</b> outputs the coordinates of the left eye and the coordinates of the right eye included in the face detection result to the parallax image processing unit <b>20</b> as viewpoint position information.</p><p id="p-0058" num="0057">On the other hand, in a case where the distance value is less than the threshold value, the selection unit <b>170</b> selects the detection result of the center of gravity. The selection unit <b>170</b> outputs information regarding the gravity center points (the gravity center points of the pupils or the irises) to the parallax image processing unit <b>20</b> as viewpoint position information.</p><p id="p-0059" num="0058">Meanwhile, the selection unit <b>170</b> may set a first threshold value and a second threshold value and select either the face detection result or the detection result of the center of gravity. <figref idref="DRAWINGS">FIG. <b>6</b></figref> is a diagram for explaining other processing of the selection unit <b>170</b> according to the embodiment. The axis illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref> corresponds to the distance value. It is based on the premise that the first threshold value and the second threshold value are set on the axis corresponding to the distance value and that the first threshold value is smaller than the second threshold value.</p><p id="p-0060" num="0059">In a case where the distance value acquired from the ranging unit <b>110</b> is less than the first threshold value, the selection unit <b>170</b> selects the detection result of the center of gravity. In a case where the distance value acquired from the ranging unit <b>110</b> is greater than or equal to the second threshold value, the selection unit <b>170</b> selects the face detection result.</p><p id="p-0061" num="0060">In a case where the distance value acquired from the ranging unit <b>110</b> is greater than or equal to the first threshold value and less than the second threshold value, the selection unit <b>170</b> refers to the face detection result and determines whether or not the coordinates of the left eye and the coordinates of the right eye are detected by the face detecting unit <b>160</b>. In a case where the coordinates of the left eye and the coordinates of the right eye are detected by the face detecting unit <b>160</b>, the selection unit <b>170</b> selects the face detection result. In a case where any one of the coordinates of the left eye and the coordinates of the right eye is not detected, the selection unit <b>170</b> selects the detection result of the center of gravity.</p><p id="p-0062" num="0061">[1.4 Processing Procedure of Tracking Device]</p><p id="p-0063" num="0062">Next, a processing procedure of the tracking device <b>100</b> of the present embodiment will be described. <figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart illustrating a processing procedure of the tracking device of the present embodiment. In the processing procedure described in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, an example of a processing procedure for controlling the light of the near-infrared ray emitting unit <b>140</b> in conjunction with the distance value will be described. The ranging unit <b>110</b> of the tracking device <b>100</b> calculates a distance value on the basis of the interocular distance (step S<b>101</b>).</p><p id="p-0064" num="0063">In a case where the distance value is not within a switching distance range (step S<b>102</b>, No), the light emission controlling unit <b>120</b> of the tracking device <b>100</b> ends the processing. On the other hand, the light emission controlling unit <b>120</b> calculates the magnitude of the current (current value) depending on the distance value.</p><p id="p-0065" num="0064">If the current value is not less than or equal to the current value Amax (step S<b>104</b>, No), the light emission controlling unit <b>120</b> outputs a current and a voltage corresponding to the current value Ad to the near-infrared ray emitting unit <b>140</b> (step S<b>105</b>) and proceeds to step S<b>107</b>. On the other hand, if the current value is less than or equal to the current value Amax (step S<b>104</b>, Yes), the light emission controlling unit <b>120</b> outputs a current and a voltage corresponding to the distance value to the near-infrared ray emitting unit <b>140</b> (step S<b>106</b>) and proceeds to step S<b>107</b>.</p><p id="p-0066" num="0065">The wavelength selection unit <b>140</b><i>a </i>of the tracking device <b>100</b> determines whether or not to switch the wavelength on the basis of the selection result of the wavelength controlling unit <b>130</b> (step S<b>107</b>). If the wavelength is switched (step S<b>107</b>, Yes), the wavelength selection unit <b>140</b><i>a </i>changes the wavelength (step S<b>108</b>) and proceeds to step S<b>109</b>. On the other hand, if the wavelength is not switched (step S<b>107</b>, No), the wavelength selection unit <b>140</b><i>a </i>proceeds to step S<b>109</b>.</p><p id="p-0067" num="0066">The near-infrared ray emitting unit <b>140</b> emits a near-infrared ray by the current (voltage) input to the light emission controlling unit <b>120</b> and the wavelength selected by the wavelength selection unit <b>140</b><i>a </i>(step S<b>109</b>). The center of gravity detecting unit <b>150</b><i>a </i>of the tracking device <b>100</b> outputs the detection result of the center of gravity to the selection unit <b>170</b> (step S<b>110</b>).</p><p id="p-0068" num="0067">[1.5 Effects of Tracking Device]</p><p id="p-0069" num="0068">Next, effects of the tracking device <b>100</b> according to the present embodiment will be described. The tracking device <b>100</b> controls the light emission amount of the near-infrared ray emitting unit <b>140</b> on the basis of the distance value between the user and the near-infrared ray emitting unit <b>140</b> and detects feature points of the user on the basis of the captured image by the image sensor <b>150</b>. This makes it possible to appropriately track the viewpoint position of the user.</p><p id="p-0070" num="0069">The tracking device <b>100</b> increases the emission intensity of the near-infrared ray emitting unit <b>140</b> as the distance value increases. This makes it possible to accurately detect the viewpoint position of the user even when the distance from the user is long.</p><p id="p-0071" num="0070">When adjusting the magnitude of the current input to the near-infrared ray emitting unit <b>140</b> on the basis of the distance value, the tracking device <b>100</b> prevents the magnitude of the current from exceeding the upper limit value. As a result, it is possible to suppress the emission intensity of the near-infrared ray emitting unit <b>140</b> from becoming too high and to satisfy the requirements of the photobiological safety standards JIS C 7550 and IEC62471. In addition, it is possible to control so that the user's face is not overexposed by emission of the near-infrared ray.</p><p id="p-0072" num="0071">The tracking device <b>100</b> selects the detection result of the center of gravity as the viewpoint position in a case where the distance value is less than the threshold value and selects the face detection result as the viewpoint position in a case where the distance value is greater than or equal to the threshold value. As a result, in a case where the user is nearby, it is possible to use center of gravity information having higher responsiveness. Furthermore, in a case where the user is at a distance, quick and stable viewpoint tracking can be executed by using the face detection result.</p><heading id="h-0012" level="1">2. Hardware Configuration</heading><p id="p-0073" num="0072">The tracking devices according to the above-described embodiments are implemented by, for example, a computer <b>200</b> having a configuration as illustrated in <figref idref="DRAWINGS">FIG. <b>8</b></figref>. Hereinafter, the tracking device <b>100</b> according to the embodiment will be described as an example. <figref idref="DRAWINGS">FIG. <b>8</b></figref> is a hardware configuration diagram illustrating an example of a computer <b>200</b> that implements the functions of the tracking device. The computer <b>200</b> includes a CPU <b>201</b> that executes various types of arithmetic processing, an input device <b>202</b> that receives an input of data from a user, and a display <b>203</b>. The computer <b>200</b> further has an interface <b>204</b> for connection with various devices.</p><p id="p-0074" num="0073">The interface <b>204</b> is connected to the near-infrared ray emitting device, the camera (image sensor), the parallax image processing unit, and the like.</p><p id="p-0075" num="0074">A hard disk device <b>206</b> includes an image processing program <b>206</b><i>a</i>. The CPU <b>201</b> reads the image processing program <b>206</b><i>a </i>and loads the image processing program <b>206</b><i>a </i>in a RAM <b>205</b>. The image processing program <b>206</b><i>a </i>functions as an image processing process <b>205</b><i>a. </i></p><p id="p-0076" num="0075">The processing of the image processing process <b>205</b><i>a </i>corresponds to the processing of the ranging unit <b>110</b>, the light emission controlling unit <b>120</b>, the wavelength controlling unit <b>130</b>, the wavelength selection unit <b>140</b><i>a</i>, the center of gravity detecting unit <b>150</b><i>a</i>, the face detecting unit <b>160</b>, and the selection unit <b>170</b> described in <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0077" num="0076">Note that the image processing program <b>206</b><i>a </i>may not necessarily be stored in the hard disk device <b>206</b> from the beginning. For example, each program is stored in a &#x201c;portable physical medium&#x201d; such as a flexible disk (FD), a CD-ROM, a DVD disc, a magneto-optical disk, or an IC card inserted into the computer <b>200</b>. And the computer <b>200</b> may read and execute the image processing program <b>206</b><i>a. </i></p><heading id="h-0013" level="1">3. Conclusion</heading><p id="p-0078" num="0077">An image processing device includes a light emitting unit, a light emission controlling unit, and a detecting unit. The light emitting unit emits a near-infrared ray to a target. The light emission controlling unit controls the light emission amount of the light emitting unit on the basis of a distance value between the target and the light emitting unit. The detecting unit detects feature points on the basis of a captured image of the target irradiated with the near-infrared ray. The detecting unit detects feature points regarding the face of the target on the basis of the captured image. The left eye and the right eye of the target are detected as feature points on the basis of the captured image. This makes it possible to appropriately track the viewpoint position of the user.</p><p id="p-0079" num="0078">The image processing device further includes a center of gravity detecting unit that detects the center of gravity of the pupil or the iris of the target. As a result, even in a case where the feature amounts of the target cannot be detected from the imaging device, it is possible to detect the center of gravity of the pupil or the iris of the target and to track the viewpoint position of the user.</p><p id="p-0080" num="0079">The image processing device includes a ranging unit, and the ranging unit further includes a ranging unit that calculates a distance value on the basis of a distance between feature points included in the captured image. As a result, the distance value can be calculated from the captured image even without using a distance sensor or the like.</p><p id="p-0081" num="0080">The light emission controlling unit of the image processing device specifies the magnitude of the current depending on the distance value and outputs a current of the specified magnitude to the light emitting unit, thereby controlling the light emission amount of the light emitting unit. The light emission controlling unit of the image processing device specifies the magnitude of the current depending on the distance value, corrects the specified magnitude of the current to a magnitude less than the threshold value in a case where the magnitude of the current is larger than a threshold value, and outputs the current of the corrected magnitude to the light emitting unit. As a result, a near-infrared ray having safe intensity can be emitted to the user.</p><p id="p-0082" num="0081">The image processing device includes a selection unit, and the selection unit selects one of a detection result of the detecting unit and a detection result of the center of gravity detecting unit. The selection unit selects one of the detection result of the detecting unit and the detection result of the center of gravity detecting unit on the basis of the detection result of the detecting unit and the detection result of the center of gravity detecting unit. This makes it possible to stably track the viewpoint position of the user at high speed.</p><p id="p-0083" num="0082">The selection unit outputs the detection result that has been selected to a device that generates a stereoscopic image. As a result, a stereoscopic image suitable for the user can be displayed.</p><p id="p-0084" num="0083">The image processing device further includes a wavelength controlling unit, and the wavelength controlling unit controls the wavelength of the near-infrared ray emitted by the light emitting unit on the basis of the detection result of the detecting unit. As a result, it is possible to stably execute processing of detecting the center of gravity of the pupil or the iris of the target.</p><p id="p-0085" num="0084">Note that the effects described herein are merely examples and are not limiting, and other effects may also be achieved.</p><p id="p-0086" num="0085">Note that the present technology can also have the following configurations.</p><p id="p-0087" num="0000">(1)</p><p id="p-0088" num="0086">An image processing device including:</p><p id="p-0089" num="0087">a light emitting unit that emits a near-infrared ray to a target;</p><p id="p-0090" num="0088">a light emission controlling unit that controls a light emission amount of the light emitting unit on a basis of a distance value between the target and the light emitting unit; and</p><p id="p-0091" num="0089">a detecting unit that detects a feature point on a basis of a captured image of the target irradiated with the near-infrared ray.</p><p id="p-0092" num="0000">(2)</p><p id="p-0093" num="0090">The image processing device according to (1), wherein the detecting unit detects a feature point regarding a face of the target on a basis of the captured image.</p><p id="p-0094" num="0000">(3)</p><p id="p-0095" num="0091">The image processing device according to (2), wherein the detecting unit detects a left eye and a right eye of the target as feature points on a basis of the captured image.</p><p id="p-0096" num="0000">(4)</p><p id="p-0097" num="0092">The image processing device according to (3), further including a center of gravity detecting unit that detects a center of gravity of a pupil or an iris of the target.</p><p id="p-0098" num="0000">(5)</p><p id="p-0099" num="0093">The image processing device according to (4), further including a ranging unit that calculates the distance value on a basis of a distance between feature points included in the captured image.</p><p id="p-0100" num="0000">(6)</p><p id="p-0101" num="0094">The image processing device according to (1), wherein the light emission controlling unit controls the light emission amount of the light emitting unit by specifying a magnitude of a current depending on the distance value and outputting a current of the specified magnitude to the light emitting unit.</p><p id="p-0102" num="0000">(7)</p><p id="p-0103" num="0095">The image processing device according to (6), wherein the light emission controlling unit specifies a magnitude of a current depending on the distance value, corrects the specified magnitude of the current to a magnitude less than a threshold value in a case where the magnitude of the current is larger than the threshold value, and outputs a current of the corrected magnitude to the light emitting unit.</p><p id="p-0104" num="0000">(8)</p><p id="p-0105" num="0096">The image processing device according to (5), further including a selection unit that selects one of a detection result of the detecting unit and a detection result of the center of gravity detecting unit on a basis of the distance value.</p><p id="p-0106" num="0000">(9)</p><p id="p-0107" num="0097">The image processing device according to (8), wherein the selection unit selects one of the detection result of the detecting unit and the detection result of the center of gravity detecting unit on a basis of the detection result of the detecting unit and the detection result of the center of gravity detecting unit.</p><p id="p-0108" num="0000">(10)</p><p id="p-0109" num="0098">The image processing device according to any one of (1) to (8), wherein the selection unit outputs the detection result that has been selected to a device that generates a stereoscopic image.</p><p id="p-0110" num="0000">(11)</p><p id="p-0111" num="0099">The image processing device according to any one of (1) to (10), further including a wavelength controlling unit that controls a wavelength of the near-infrared ray emitted by the light emitting unit on a basis of the detection result of the detecting unit.</p><p id="p-0112" num="0000">(12)</p><p id="p-0113" num="0100">An image processing method including:</p><p id="p-0114" num="0101">by a computer,</p><p id="p-0115" num="0102">controlling a light emission amount of a light emitting device that emits a near-infrared ray to a target on a basis of a distance value between the target and the light emitting device; and</p><p id="p-0116" num="0103">detecting a feature point on a basis of a captured image of the target irradiated with the near-infrared ray.</p><p id="p-0117" num="0000">(13)</p><p id="p-0118" num="0104">An image processing program for causing a computer to function as:</p><p id="p-0119" num="0105">a light emission controlling unit that controls a light emission amount of a light emitting device that emits a near-infrared ray to a target on a basis of a distance value between the target and the light emitting device; and</p><p id="p-0120" num="0106">a detecting unit that detects a feature point on a basis of a captured image of the target irradiated with the near-infrared ray.</p><heading id="h-0014" level="1">REFERENCE SIGNS LIST</heading><p id="p-0121" num="0000"><ul id="ul0003" list-style="none">    <li id="ul0003-0001" num="0000">    <ul id="ul0004" list-style="none">        <li id="ul0004-0001" num="0107"><b>100</b> TRACKING DEVICE</li>        <li id="ul0004-0002" num="0108"><b>110</b> RANGING UNIT</li>        <li id="ul0004-0003" num="0109"><b>120</b> LIGHT EMISSION CONTROLLING UNIT</li>        <li id="ul0004-0004" num="0110"><b>130</b> WAVELENGTH CONTROLLING UNIT</li>        <li id="ul0004-0005" num="0111"><b>140</b> NEAR-INFRARED RAY EMITTING UNIT</li>        <li id="ul0004-0006" num="0112"><b>140</b><i>a </i>WAVELENGTH SELECTION UNIT</li>        <li id="ul0004-0007" num="0113"><b>150</b> IMAGE SENSOR</li>        <li id="ul0004-0008" num="0114"><b>150</b><i>a </i>CENTER OF GRAVITY DETECTING UNIT</li>        <li id="ul0004-0009" num="0115"><b>160</b> FACE DETECTING UNIT</li>        <li id="ul0004-0010" num="0116"><b>170</b> SELECTION UNIT</li>    </ul>    </li></ul></p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An image processing device including:<claim-text>a light emitting unit that emits a near-infrared ray to a target;</claim-text><claim-text>a light emission controlling unit that controls a light emission amount of the light emitting unit on a basis of a distance value between the target and the light emitting unit; and</claim-text><claim-text>a detecting unit that detects a feature point on a basis of a captured image of the target irradiated with the near-infrared ray.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The image processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the detecting unit detects a feature point regarding a face of the target on a basis of the captured image.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The image processing device according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the detecting unit detects a left eye and a right eye of the target as feature points on a basis of the captured image.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The image processing device according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, further including a center of gravity detecting unit that detects a center of gravity of a pupil or an iris of the target.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The image processing device according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, further including a ranging unit that calculates the distance value on a basis of a distance between feature points included in the captured image.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The image processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the light emission controlling unit controls the light emission amount of the light emitting unit by specifying a magnitude of a current depending on the distance value and outputting a current of the specified magnitude to the light emitting unit.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The image processing device according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the light emission controlling unit specifies a magnitude of a current depending on the distance value, corrects the specified magnitude of the current to a magnitude less than a threshold value in a case where the magnitude of the current is larger than the threshold value, and outputs a current of the corrected magnitude to the light emitting unit.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The image processing device according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, further including a selection unit that selects one of a detection result of the detecting unit and a detection result of the center of gravity detecting unit on a basis of the distance value.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The image processing device according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the selection unit selects one of the detection result of the detecting unit and the detection result of the center of gravity detecting unit on a basis of the detection result of the detecting unit and the detection result of the center of gravity detecting unit.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The image processing device according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the selection unit outputs the detection result that has been selected to a device that generates a stereoscopic image.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The image processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further including a wavelength controlling unit that controls a wavelength of the near-infrared ray emitted by the light emitting unit on a basis of the detection result of the detecting unit.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. An image processing method including:<claim-text>by a computer,</claim-text><claim-text>controlling a light emission amount of a light emitting device that emits a near-infrared ray to a target on a basis of a distance value between the target and the light emitting device; and</claim-text><claim-text>detecting a feature point on a basis of a captured image of the target irradiated with the near-infrared ray.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. An image processing program for causing a computer to function as:<claim-text>a light emission controlling unit that controls a light emission amount of a light emitting device that emits a near-infrared ray to a target on a basis of a distance value between the target and the light emitting device; and</claim-text><claim-text>a detecting unit that detects a feature point on a basis of a captured image of the target irradiated with the near-infrared ray.</claim-text></claim-text></claim></claims></us-patent-application>