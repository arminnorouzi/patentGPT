<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004440A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004440</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17841552</doc-number><date>20220615</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>50</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>48</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>5055</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>4887</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">ALLOCATING OF COMPUTING RESOURCES FOR APPLICATIONS</invention-title><us-related-documents><us-provisional-application><document-id><country>US</country><doc-number>63212046</doc-number><date>20210617</date></document-id></us-provisional-application><us-provisional-application><document-id><country>US</country><doc-number>63212048</doc-number><date>20210617</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="obligated-assignee"><addressbook><orgname>Sync Computing Corp.</orgname><address><city>Cambridge</city><state>MA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Hanel</last-name><first-name>Carson</first-name><address><city>Bryan</city><state>TX</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Gorsky</last-name><first-name>Sean</first-name><address><city>Somerville</city><state>MA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Lin</last-name><first-name>Erica</first-name><address><city>Cambridge</city><state>MA</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Bramhavar</last-name><first-name>Suraj</first-name><address><city>Arlington</city><state>MA</state><country>US</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>Chou</last-name><first-name>Jeffrey</first-name><address><city>Boston</city><state>MA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A method for performing scheduling includes extracting information from at least one log file for an application. The method also includes determining an allocation of cloud resources for the application based on the information from the log file(s).</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="99.40mm" wi="156.63mm" file="US20230004440A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="226.40mm" wi="158.67mm" file="US20230004440A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="229.19mm" wi="99.06mm" file="US20230004440A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="155.96mm" wi="114.98mm" file="US20230004440A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="238.17mm" wi="126.66mm" orientation="landscape" file="US20230004440A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="239.01mm" wi="154.43mm" orientation="landscape" file="US20230004440A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="233.93mm" wi="167.30mm" file="US20230004440A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="237.49mm" wi="167.89mm" file="US20230004440A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="230.21mm" wi="118.79mm" orientation="landscape" file="US20230004440A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="231.82mm" wi="170.01mm" file="US20230004440A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="224.45mm" wi="172.89mm" file="US20230004440A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="220.47mm" wi="177.21mm" orientation="landscape" file="US20230004440A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="220.22mm" wi="177.38mm" orientation="landscape" file="US20230004440A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="239.01mm" wi="160.02mm" file="US20230004440A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="184.91mm" wi="162.56mm" file="US20230004440A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS REFERENCE TO OTHER APPLICATIONS</heading><p id="p-0002" num="0001">This application claims priority to U.S. Provisional Patent Application No. 63/212,046 entitled DYNAMIC ADJUSTMENT OF RESOURCES UTILIZING LOG FILES filed Jun. 17, 2021 and to U.S. Provisional Patent Application No. 63/212,048 entitled AUTOPROVISIONING CLOUD RESOURCES FOR APPLICATIONS filed Jun. 17, 2021, both of which are incorporated herein by reference for all purposes.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND OF THE INVENTION</heading><p id="p-0003" num="0002">One of the challenges of cloud computing is tackling the hundreds of different hardware configurations and settings a user can select when running their application. The consequences of a poor selection can lead to long run times and significant cloud computing costs. Both longer run times and larger costs are significant issues for users of a cloud infrastructure. A user could test run their application on all possible different instances of the cloud infrastructure using all possible combinations of settings and select the configuration which provides the lowest cost and runtime. This manual operation would be impractical as running the tests would cost more than running the actual application with sub-optimal settings and may require a significant amount of time to complete the tests. Accordingly, an improved mechanism for selecting a cloud infrastructure and, therefore provisioning resources for an application executed on the cloud infrastructure are desired.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0004" num="0003">Various embodiments of the invention are disclosed in the following detailed description and the accompanying drawings.</p><p id="p-0005" num="0004"><figref idref="DRAWINGS">FIG. <b>1</b></figref> depicts an embodiment of a computing system architecture utilizing a special purpose optimization processor for automatically provisioning resources.</p><p id="p-0006" num="0005"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a flow-chart depicting an embodiment of a method for automatically provisioning resources.</p><p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flow-chart depicting an embodiment of a method for automatically provisioning resources.</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flow-chart depicting an embodiment of a method for providing the predicted run time.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates an embodiment of how the measured stage information may be used to predict runs on different hardware node sizes (e.g. M and N).</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>6</b></figref> depicts an embodiment of a coarse-grained runtime prediction for a single stage.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>7</b></figref> depicts an embodiment of a graph of predicted run time versus number of nodes.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a flow-chart depicting an embodiment of a method for incorporating costs in allocating cloud resources.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>9</b></figref> depicts embodiments of the predicted cost and runtime of an application on seven different hardware infrastructures.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is an embodiment of a graph indicating how failure probability may be incorporated.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>11</b></figref> depicts an example block simulation for coarse grained prediction.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>12</b></figref> depicts an embodiment of the prediction of task duration distribution on new hardware.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>13</b></figref> depicts an embodiment of placement of tasks onto the cores having the earliest availability.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>14</b></figref> depicts an embodiment of the prediction of total time versus number of cores.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>15</b></figref> depicts a tree diagram of an embodiment of the stage dependency.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIGS. <b>16</b>A-<b>16</b>B</figref> depicts embodiments of the measured task placement with original stage ordering and after reordering stages according to descending priority score.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIGS. <b>17</b>A-<b>17</b>B</figref> depict embodiments of memory usage measured and predicted while running the same application with different garbage collection parameters.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>18</b></figref> is a flow-chart depicting an embodiment of a method for updating resources for an application.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0004" level="1">DETAILED DESCRIPTION</heading><p id="p-0023" num="0022">The invention can be implemented in numerous ways, including as a process; an apparatus; a system; a composition of matter; a computer program product embodied on a computer readable storage medium; and/or a processor, such as a processor configured to execute instructions stored on and/or provided by a memory coupled to the processor. In this specification, these implementations, or any other form that the invention may take, may be referred to as techniques. In general, the order of the steps of disclosed processes may be altered within the scope of the invention. Unless stated otherwise, a component such as a processor or a memory described as being configured to perform a task may be implemented as a general component that is temporarily configured to perform the task at a given time or a specific component that is manufactured to perform the task. As used herein, the term &#x2018;processor&#x2019; refers to one or more devices, circuits, and/or processing cores configured to process data, such as computer program instructions.</p><p id="p-0024" num="0023">A detailed description of one or more embodiments of the invention is provided below along with accompanying figures that illustrate the principles of the invention. The invention is described in connection with such embodiments, but the invention is not limited to any embodiment. The scope of the invention is limited only by the claims and the invention encompasses numerous alternatives, modifications and equivalents. Numerous specific details are set forth in the following description in order to provide a thorough understanding of the invention. These details are provided for the purpose of example and the invention may be practiced according to the claims without some or all of these specific details. For the purpose of clarity, technical material that is known in the technical fields related to the invention has not been described in detail so that the invention is not unnecessarily obscured.</p><p id="p-0025" num="0024">When running an application using cloud resources, a user is allowed to select from a number of different hardware configurations and settings. Based on these settings, cloud resources are allocated to processing the application. The consequences of a poor selection can lead to long run times and significant cloud computing costs. A user could manually test run their application on all possible instances of the cloud infrastructure using all possible combinations of settings. The user may then select the configuration which provides the lowest cost and/or run time. However, this technique for allocating resources is highly inefficient. As a result, most users simply choose the characteristics of the cloud infrastructure they believe may be appropriate and accept the consequences in run time and/or cost. Thus, processing in the cloud infrastructure may inefficiently utilize cloud resources, require larger times to complete a workload, consume more power than necessary, and result in the user incurring significant unnecessary financial costs.</p><p id="p-0026" num="0025">Further, scheduling for compute jobs (i.e. applications, each of which includes multiple tasks to be performed) to available processing, network, memory and disk resources (e.g. in the cloud) is an NP-hard optimization problem. Solving for the scheduling in an optimal/close-to-optimal fashion may take an extremely long time. This may lead to latency degradation, wasted resources, and high cost. Consequently, simple scheduling mechanisms are typically used. For example, some cloud computing systems split an application into resilient distributed dataset (RDD) objects and build a directed acyclic graph (DAG) from the RDD objects. A DAG scheduler splits the DAG into stages of tasks, which are submitted as each stage becomes ready. A task scheduler launches the tasks in a cluster (e.g. a set of cloud computing cores, or nodes) having parameters selected by the user. The nodes execute the tasks for the stages. Although utilizing the resources of the cloud and selections of the user, this scheduling may be inefficient. For example, various aspects of each application, interactions between applications, interactions between stages, and interactions between tasks may not be accounted for. Thus, although automated scheduling of tasks for a compute job is performed, it may result in a sub-optimal use of computing resources. This may lead to poor performance, longer times to complete a compute job, and higher power consumption. Consequently, techniques for improving the allocation of resources in computing systems such as cloud computing systems are desired.</p><p id="p-0027" num="0026">A method for allocating resources and performing scheduling for an application is described. The method includes extracting information from at least one log file for the application. In some embodiments, the log file(s) correspond to a single run of the application. The information extracted may include task data, cloud settings, hardware information, cloud economic information and/or cloud reliability information. The method also includes determining an allocation of cloud resources for the application based on the information from the log file(s). For example, the allocation of the cloud resources may include determination of a number of cores in a cluster for the cloud resources allocated to the application. The allocation of cloud resource may also include determining the scheduling of tasks and stages in some embodiments. Similarly, a system for provisioning cloud resources is described. The system includes processor(s) and memory. The memory is coupled to the processor and configured to provide the processor with instructions. The processor(s) are configured to extract information from log file(s) for the application and determine an allocation of cloud resources for the application based on the information from the log file(s). A computer program product embodied in a non-transitory computer readable medium is also described. The computer program product includes computer instructions for extracting information from log file(s) for the application and determining an allocation of cloud resources for the application based on the information from the log file(s).</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>1</b></figref> depicts an embodiment of computing system architecture <b>100</b> for performing scheduling for one or more application(s) <b>106</b>. For clarity, not all components are shown. In some embodiments, different and/or additional components may be present. In some embodiments, some components might be omitted. System <b>100</b> includes scheduler <b>110</b>, interface <b>103</b>, and cloud resources <b>104</b>. Also shown are applications <b>106</b> desired to be run using cloud resources <b>104</b> and log file(s) <b>102</b>. In some embodiments, information from the application is sent to the interface, including application meta data. For example, application meta data can include information about the input data size, schema, file type, skew, or user submitted information. Application metadata can also include information related to the code, ecosystem, or platform submitted by the user. Scheduler <b>110</b> includes processor(s) and/or control logic <b>112</b> and scheduling optimization coprocessor(s) (SOC) <b>114</b>. In some embodiments, scheduler <b>110</b> may also include memory (not shown). Processor <b>112</b> may simply be control logic, an FPGA, a CPU and/or a GPU used in controlling SOC <b>114</b>. In some embodiments, processor(s) <b>102</b> might be omitted. Similarly, although a single SOC <b>114</b> is shown, in some embodiments, scheduler <b>110</b> may include multiple SOCs <b>114</b>.</p><p id="p-0029" num="0028">This application is related to co-pending U.S. patent application Ser. No. 16/847,140 entitled OPTIMIZATION PROCESSING UNIT HAVING SUBUNITS THAT ARE PROGRAMMABLY AND PARTIALLY CONNECTED filed Apr. 13, 2020, which is incorporated herein by reference for all purposes. This application is related to co-pending U.S. patent application Ser. No. 17/387,294 entitled OPTIMIZATION PROCESSING UNIT UTILIZING DIGITAL OSCILLATORS filed Jul. 28, 2021, which is incorporated herein by reference for all purposes. This application is related to co-pending U.S. patent application Ser. No. 17/402,432 entitled REAL TIME SCHEDULING USING EXPECTED APPLICATION RESOURCE USAGE filed Aug. 13, 2021, which is incorporated herein by reference for all purposes. In some embodiments, scheduler <b>110</b> and/or SOC <b>114</b> may be provided utilizing the techniques in the above-identified co-pending patent applications.</p><p id="p-0030" num="0029">Cloud resources <b>104</b> may include one or more servers (or other computing systems) each of which includes multiple cores, memory resources, disk resources, networking resources, schedulers, and/or other computing components used in implementing tasks for executing application(s) <b>106</b>. In some embodiments, for example, cloud resources <b>104</b> may include a single server (or other computing system) having multiple cores and associated memory and disk resources. Interface <b>103</b> receives the application(s) <b>106</b> to be executed and log file(s) <b>102</b>. Application(s) <b>106</b> include one or more applications each of which includes multiple tasks to be performed by cloud resources <b>104</b>.</p><p id="p-0031" num="0030">Log file(s) <b>102</b> may be generated when the application(s) <b>106</b> are run (e.g. test run) on cloud resources <b>104</b>. In some embodiments, the application(s) <b>106</b> may be run through scheduler <b>110</b> without scheduler <b>110</b> attempting to allocate resources or with scheduler <b>110</b> utilizing default or user-selected settings. In some embodiments, the application(s) <b>106</b> may be provided to cloud resources <b>104</b> in another manner. Thus, cloud resources <b>104</b> may use internal scheduler(s) (not explicitly shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>) to complete the tasks for the application(s) <b>106</b> during the test run. In doing so, log file(s) <b>102</b> are generated.</p><p id="p-0032" num="0031">Scheduler <b>110</b> receives information about application(s) <b>106</b> and information about cloud resources <b>104</b>. In some embodiments, scheduler <b>110</b> receives at least some of this information from log file(s) <b>102</b>. more specifically, scheduler <b>110</b> extracts information from the log file(s) <b>102</b>. In some embodiments, one log file <b>102</b> is generated for each time a particular application is run on cloud resources <b>104</b>. Thus, scheduler <b>110</b> may extract the information from log file(s) <b>102</b> corresponding to a single run of each application <b>106</b>. This information from log file(s) <b>102</b> is used in provisioning resources for the application(s) <b>106</b>. For example, scheduler <b>110</b> may implement a parser for receiving and extracting information from a log file and a predictor for determining the resulting run time for an application based on the information extracted from the log file(s) <b>102</b>. Thus, based on the information in the log file(s) <b>102</b>, scheduler <b>110</b> provisions cloud resources <b>104</b> of the application.</p><p id="p-0033" num="0032">Scheduler <b>110</b> may optimize completion of the tasks for application(s) <b>106</b> by cloud resources <b>104</b> without requiring multiple runs of application(s) <b>106</b>. Thus, scheduling of tasks for application(s) <b>106</b> may be significantly more efficient. Scheduler <b>110</b> may also optimize the processing for application(s) <b>106</b>. For example, the scheduling of tasks, stages (units of execution for multiple tasks in, for example, APACHE SPARK&#x2122;), and/or different compute jobs (e.g. different application(s) <b>106</b>) may be improved. Thus, the time taken to complete workloads for application(s) <b>106</b> may be reduced, the resources utilized (e.g. the number of cores used) may be better matched to the workloads and/or the cost of completing application(s) <b>106</b> may be reduced. Thus, performance of system <b>100</b> and use of cloud resources <b>104</b> may be enhanced.</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a flow-chart depicting an embodiment of a method for automatically provisioning resources. Method <b>200</b> may be used in conjunction with system <b>100</b>. However, in other embodiments, method <b>200</b> may be utilized with other systems. Although certain processes are shown in a particular order for method <b>200</b>, other processes and/or other orders may be utilized in other embodiments. Method <b>200</b> is also described in the context of allocating resources for a single application. In some embodiments, resources for multiple applications may be allocated. In such embodiments, interactions between applications that are to be processed at overlapping times may be considered by method <b>200</b>.</p><p id="p-0035" num="0034">Method <b>200</b> starts after one or more log files for an application have already been generated. Thus, method <b>200</b> starts after the application has been run at least once. During processing for an application, a log file is typically generated by the cloud resources used. In some embodiments, the log file(s) for a single run of the application may be used in method <b>200</b>. In general, one log file is generated for each time an application is processed. Thus, a single log file may be used in method <b>200</b>. However, nothing prevents the use of multiple log files. As discussed above, cloud resources for the run of the application resulting in the log file may have been allocated using built-in schedulers, user selections related to cloud resources (e.g. the number of cores used), and/or other techniques. Thus, processing for the application may have been completed using settings for the cloud resources that are sub-optimal. Consequently, the log file used need not (and generally does not) include optimal resource allocation.</p><p id="p-0036" num="0035">Information is extracted from the log file(s) for an application, at <b>202</b>. In some embodiments, the information extracted may include task data and cloud settings. Task data relates to what the individual tasks for the application are and how each task is scheduled. For example, the time take to complete a specific task, how much data was provided for the task, and how much data was generated by the task may be identified. The cloud settings relate to characteristics of the cloud service for which cloud resources are desired to be allocated. Some of these settings may be selected by the user. For example, cloud settings may include the number of cores used, data partitions, the memory for each core, and/or other settings (e.g. SPARK&#x2122; settings). Hardware information, cloud economic information and/or cloud reliability information may also be obtained at <b>202</b>. Hardware information may be extracted from the log file and/or obtained other sources such as the user and/or public sites detailing the hardware configurations available for a particular cloud service. Hardware information may include the type and number of processing units, the type and size of memory, the network bandwidth and the disk bandwidth. Cloud economic information and/or cloud reliability information may be extracted from the log file and/or acquired from other sources (e.g. the user and/or public sites). Cloud economic information may include fixed prices (on-demand) or variable prices (spot instances), which vary daily and across geographical regions. Reliability information may include the general probability of failure for Spot instances.</p><p id="p-0037" num="0036">The allocation of cloud resources for the application is determined based on the information from the log file(s), at <b>204</b>. For example, the allocation of the cloud resources may include a number of cores in a cluster allocated to the application. In some embodiments, determining the allocation of the cloud resources includes determining a plurality of hardware infrastructures (e.g. a particular number of cores to be used) and determining a predicted run time for each of the hardware infrastructures based on the information extracted from the log file(s). Thus, the relationship between the predicted run time (the time taken to complete the tasks for the application) and features of the hardware infrastructure (e.g. the number of cores) may be identified. In some embodiments, a predicted cost for each of the hardware infrastructures is also determined at <b>204</b>. In such embodiments, the relationship between the predicted cost versus the predicted run time for each of the hardware infrastructures can be ascertained. Based on the predicted run time and/or predicted cost, the cloud resources may be provisioned. In some embodiments, the provisioning is automatically performed. For example, the number of cores corresponding to the lowest run time (or the lowest run time within a particular range of costs) may be automatically selected. In some embodiments, a user desiring to have the application processed selects the hardware infrastructure based on information provided by method <b>200</b>. For example, the relationship between predicted run time and predicted costs may be displayed to a user. In such embodiments, the user selects the hardware infrastructure based on the relationship. The user may select a hardware infrastructure configuration having a slightly longer run time for the application, but which is significantly lower in cost.</p><p id="p-0038" num="0037">For example, scheduler <b>110</b> may extract information from log file(s) <b>102</b> for application(s) <b>106</b>, at <b>202</b>. In some embodiments, scheduler <b>110</b> employs a parser to analyze log file(s) <b>102</b> and obtain task data, cloud setting, hardware configuration information, cloud cost information, cloud reliability information and/other relevant information. In some embodiments, scheduler may obtain some of this information (e.g. cloud cost and/or reliability information) from other sources. Based on the information extracted, scheduler <b>110</b> allocates the resources, at <b>204</b>. In some embodiments, scheduler <b>110</b> determines the predicted run time for various hardware infrastructures as part of <b>204</b>. Scheduler <b>110</b> may also determine the predicted cost for the various hardware infrastructures. Scheduler <b>110</b> may thus determine the predicted run time versus predicted cost for application <b>106</b>. Based on this information, the number of cores is allocated to the application at <b>204</b>. This allocation may be made by scheduler <b>110</b> and/or via user selections made in response to additional information (e.g. predicted run time versus predicted costs for various hardware configurations) provided to the user by scheduler <b>110</b>.</p><p id="p-0039" num="0038">Thus, resources may be allocated for the application. Whether this is performed automatically or by the user taking into account information provided by method <b>200</b>, the allocation of resources may be improved. The number of cores used and/or the scheduling of tasks for the application may be optimized. As a result, execution of the application may be more efficient. For example, run time and/or costs may be reduced. Power consumption may also be reduced (e.g. due to the reduction in run time). Further, the process of allocating resources may be made significantly more efficient. For example, the application need not be repeatedly run in order to determine an improved hardware infrastructure for the application. In some embodiments, method <b>200</b> may be completed in real time or close to real time. Thus, the time utilized in allocating resources may remain small. Thus, not only may the run time for the application be reduced, but the time taken to allocate resources for the application shortened. Consequently, performance and efficiency may be improved.</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flow-chart depicting an embodiment of method <b>300</b> for automatically provisioning resources. Method <b>300</b> may be used in conjunction with system <b>100</b>. However, in other embodiments, method <b>300</b> may be utilized with other systems. Although certain processes are shown in a particular order, other processes and/or other orders may be utilized in other embodiments. Method <b>300</b> is also described in the context of allocating resources for a single application. In some embodiments, resources for multiple applications may be allocated. In such embodiments, interactions between applications that are to be processed at overlapping times may be considered by method <b>300</b>. Method <b>300</b> starts after one or more log files for an application have already been generated. The log file(s) for the application may be generated in an analogous manner to that described with respect to <figref idref="DRAWINGS">FIGS. <b>1</b>-<b>2</b></figref>.</p><p id="p-0041" num="0040">One or more log file(s) for the application to be processed are received, at <b>302</b>. The log file(s) are analogous to those described in the context of method <b>200</b>. In some embodiments, the user is prompted to upload the log file(s). In some embodiments, the user may upload the application and the log files are automatically generated using predetermined and/or other default settings. Information is extracted from the log file(s), at <b>304</b>. In some embodiments, <b>304</b> is analogous to <b>202</b>.</p><p id="p-0042" num="0041">Additional information used in allocating resources and not present in the log file(s) is obtained, at <b>306</b>. In some embodiments, <b>306</b> includes receiving user preferences. For example, the desired configurations of the virtual server instances may be requested and received from a user via an interface. Similarly, cost and/or reliability information may be obtained from a site for the cloud services. In some embodiments, <b>306</b> may be omitted if enough information is obtained from the log file(s). Thus, sufficient information to allocate resources may be obtained via <b>302</b>, <b>304</b>, and <b>306</b>.</p><p id="p-0043" num="0042">A hardware infrastructure for the application is determined, at <b>308</b>. For example, the settings selected by the user or other information at <b>306</b> may be used. In some embodiments, <b>308</b> includes determining the number and type of cores to be used in a current iteration of modeling the workload for the application.</p><p id="p-0044" num="0043">For the hardware infrastructure identified at <b>308</b>, the predicted run time for the application is determined, at <b>310</b>. The predicted run time (or predicted makespan) for the application is the total time required to process the workload for the application using the hardware infrastructure selected at <b>308</b>. In some embodiments, the predicted runtime is determined by determining a total dead time (the amount of time a core spent not working tasks provided that it worked on at least one task) and the total task time (the time required to process all of the tasks for the application). In some embodiments, the total task time is scaled based upon the size of the data set, also at <b>310</b>. In some embodiments, <b>310</b> also includes distributing the tasks for the application over multiple cores of the hardware infrastructure. Stated differently, the parallelism is accounted for. In some embodiments, the amount of parallelism in the log file(s) is used at <b>310</b>. Thus, the distribution of tasks performed in parallel may be accounted for in the calculation of the predicted run time.</p><p id="p-0045" num="0044">The predicted costs for the hardware infrastructure may be determined, at <b>312</b>. In some embodiments, <b>312</b> is based on factors such as the run time, the cost per unit run time, and the number of cores utilized. In some embodiments, <b>308</b>, <b>310</b>, and <b>312</b> are repeated for other hardware infrastructures, at <b>314</b>. Thus, the relationships between the hardware infrastructures, the predicted run times, and the predicted costs are determined. The hardware infrastructure, cloud settings, and other aspects of the cloud computing are determined and used to select the hardware infrastructure configuration, at <b>316</b>. In some embodiments, <b>316</b> includes automatic allocation of the cloud resources, as discussed with respect to <b>204</b> of method <b>200</b>. In some embodiments, <b>316</b> include providing information to the user to allow the user to better select the cloud settings, as discussed with respect to <b>204</b> of method <b>200</b>. In some embodiments, <b>316</b> also include scheduling of tasks and stages.</p><p id="p-0046" num="0045">For example, scheduler <b>110</b> may receive and extract information from log file(s) <b>102</b> for application(s) <b>106</b>, at <b>302</b> and <b>304</b>. In some embodiments, scheduler <b>110</b> employs a parser to analyze log file(s) <b>102</b> and obtain task data, cloud setting, hardware configuration information, cloud cost information, cloud reliability information and/other relevant information. Scheduler <b>110</b> may obtain some of this information (e.g. user settings, cloud cost and/or reliability information) from other sources, at <b>306</b>. Based on the information extracted, scheduler <b>110</b> determines the predicted run times and, in some embodiments, the cost, at <b>310</b> and <b>312</b>. At <b>316</b>, scheduler <b>110</b> may allocate (i.e. assign or allow the user to select) cloud resources to application <b>106</b>.</p><p id="p-0047" num="0046">Thus, method <b>300</b> efficiently allocates cloud resources for the application. Whether this is performed automatically or by the user taking into account information provided by method <b>300</b>, the allocation of resources may be improved. The number of cores used and/or the scheduling of tasks for the application may be optimized. As a result, execution of the application may be more efficient. For example, run time and/or costs may be reduced. Power consumption may also be reduced (e.g. due to the reduction in run time). Further, the process of allocating resources may be made significantly more efficient. For example, the application need not be repeatedly run in order to determine an improved hardware infrastructure for the application. Thus, not only may the run time for the application be reduced, but the time taken to allocate resources for the application shortened. Consequently, performance and efficiency may be improved.</p><p id="p-0048" num="0047">In some embodiments, the predicted run time for the application is determined based upon the tasks for the application and stages into which the tasks may be divided. A stage may include a number of tasks which are related or which have been grouped together for execution. <figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flow-chart depicting an embodiment of a method for providing the predicted run time based on stages. Although certain processes are shown in a particular order, other processes and/or other orders may be utilized in other embodiments. Method <b>400</b> is also described in the context of determining the predicted run time for a single application. In some embodiments, resources for multiple applications may be allocated. In such embodiments, interactions between applications that are to be processed at overlapping times may be considered by method <b>400</b>. Method <b>400</b> may be used in conjunction with system <b>100</b> and methods <b>200</b> and/or <b>300</b>. For example, method <b>400</b> may be used for performing <b>310</b> for each hardware infrastructure. In other embodiments, method <b>400</b> may be utilized with other systems and/or methods.</p><p id="p-0049" num="0048">The total dead time for each stage is predicted, at <b>402</b>. The total dead time includes the time each core used in executing tasks for the stage is not working on a task. The total dead time excludes the time a core is not working on executing tasks if the core is unused for the stage. The determination of the total dead time (i.e. the predicted total dead time for each stage) in <b>402</b> is accomplished using information extracted from the log file(s).</p><p id="p-0050" num="0049">The total task time for each stage is also predicted, at <b>402</b>. The total task time includes the total time required to complete the tasks from the start of the first task to the end of the last task for the stage. The determination of the total task time (i.e. the predicted total task time for a stage) at <b>404</b> is accomplished using information extracted from the log file(s). In some embodiments, the task time is based on a particular time taken for tasks indicated in the log file(s). In some embodiments, the task time may be predicted based upon other factors, such as data scaling.</p><p id="p-0051" num="0050">Parallelism and scaling are accounted for at <b>406</b>. In some embodiments, <b>406</b> is performed for the total dead time and the total task time as part of <b>402</b> and <b>404</b>, respectively. Parallelism relates to the distribution of tasks based on the expected active cores. Stated differently, the tasks are not simply processed serially by a particular core. For example, if the number of tasks is less than the number of cores then there will be cores left unused by that stage. On the other hand, if there are more tasks than cores, then those tasks are spread across those cores. Scaling relates to the amount of data to be processed. For example, if the log file(s) correspond to a different amount of data than is expected to be processed by the application, the times may be scaled up or down. Thus, part of <b>406</b> may include profiling users' workload(s) and/or allowing a user to indicate the typical workload size.</p><p id="p-0052" num="0051">The predicted makespan, or predicted total run time, for the application is determined, at <b>408</b>. Based on the total task time and total dead time predicted, which may be scaled and parallelized, the predicted total stage run time for each stage is determined. The predicted total stage run time is used to provide the predicted total run time for the application.</p><p id="p-0053" num="0052">Using method <b>400</b>, the predicted run time may be determined for the application. This predicted run time may be used to determine the cloud resource allocation in method <b>200</b> and/or <b>300</b>. Consequently, performance and efficiency may be improved.</p><p id="p-0054" num="0053">In some embodiments, methods <b>200</b>, <b>300</b>, and/or <b>400</b> and system <b>100</b> may be used in connection with provisioning cloud resources for cloud services utilizing APACHE SPARK&#x2122;. APACHE SPARK&#x2122; (also termed SPARK&#x2122; herein) is a popular data analytics platform used extensively today by many companies to process big data. Scheduler <b>110</b> and methods <b>200</b>, <b>300</b>, and/or <b>400</b> may be used to predict both the time duration and resources used to compute a Spark application. Thus, methods <b>200</b>, <b>300</b>, and/or <b>400</b> may be further explained in the context of allocating resources, including scheduling of tasks and stages, for a cloud service utilizing APACHE SPARK&#x2122;. <figref idref="DRAWINGS">FIGS. <b>5</b>-<b>18</b></figref> further describe embodiments of system <b>100</b> and methods <b>200</b>, <b>300</b>, and/or <b>400</b> in the context of APACHE SPARK&#x2122;. However, the predictor works effectively for other platforms.</p><p id="p-0055" num="0054">Using methods <b>200</b>, <b>300</b>, and/or <b>400</b> and system <b>100</b>, users' workloads may be profiled and models utilized to predict an optimized cloud infrastructure to use. The prediction is generated substantially instantly (e.g. in real time), generally with no training required, due to its model-based process. The scheduler can also be tunable, based on the business needs of the customer. For example, a user may choose a longer runtime but for much lower costs, or they could choose the fastest runtime but at much larger costs&#x2014;the final decision depends on the priorities of the individual users. The scheduler thus provides a solution to a growing and critical problem on the cloud.</p><p id="p-0056" num="0055">In order to accurately predict the resources needed, the methods and systems described herein parse information from one or more SPARK&#x2122; log files, each of which is generated from a previous run. The log file contains information below which is extracted by scheduler <b>100</b> and/or methods <b>200</b>, <b>300</b>, and/or <b>400</b>:</p><p id="p-0057" num="0056">Task data&#x2014;e.g. how long does it take to complete one of the tasks, how much data goes in, how much data is generated</p><p id="p-0058" num="0057">User/Cloud service settings&#x2014;e.g. how many cores, different SPARK&#x2122; settings, data partitions, memory on each core</p><p id="p-0059" num="0058">Hardware information is collected on the public or cloud computing website which details the various hardware configurations such as type and number of CPUs, type and size of memory, disk bandwidth, and network bandwidth.</p><p id="p-0060" num="0059">Cloud economic information may also be pulled from the cloud service's public website to extract fixed prices (on-demand) or variable prices (spot instances), which vary daily and across geographical regions.</p><p id="p-0061" num="0060">Reliability information may also be also pulled from the public cloud service website, which lists the general probability of failure for Spot instances.</p><p id="p-0062" num="0061">The methods <b>200</b>, <b>300</b>, and <b>400</b> and/or scheduler <b>110</b> take in a parsed event log with the application data described above and a set of driver and worker infrastructure hardware types to perform the prediction on. Thus, the log file for the application may be uploaded for use by scheduler <b>100</b> and/or methods <b>200</b>, <b>300</b>, and/or <b>400</b>. The output is a cost-to-runtime prediction, which can be visualized as a curve, where each point on the curve represents a different hardware infrastructure.</p><p id="p-0063" num="0062">SPARK&#x2122; applications are broken down into stages, where each stage can perform a set of transformations on partitioned data, and dependent stages do not begin until the previous stage has completed. In the context of SPARK&#x2122;, scheduler <b>100</b> and/or methods <b>200</b>, <b>300</b>, and/or <b>400</b> may perform a stage-by-stage prediction, taking data specific to each stage, and calculating the predicted runtime of that particular stage on a specific hardware infrastructure. The total predicted application runtime is the sum of non-overlapping predicted runtime of individual stages and a predicted application overhead. The predicted application overhead includes time loading data from input data sources, such as files from AWS&#x2122; s3 buckets (or other analogous source), onto the cluster, and driver time in between stages.</p><p id="p-0064" num="0063">For each stage, the compute time, overhead time, IO time, and memory time is calculated. Memory time is blocking time the application spends on memory management, including time spilling data from RAM to disk or evicting blocks from cache. The number of tasks is also calculated based on the desired input data size and relevant SPARK&#x2122; parameters. Using this information, the predicted runtime for each stage is calculated for a given number and type of nodes using a mathematical model as described below. <figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates embodiment <b>500</b> of how the measured stage information may be used to predict runs on different hardware node sizes (e.g. M and N).</p><p id="p-0065" num="0064">Prediction of an application's makespan (i.e. run time) when projected onto a new set of hardware is complicated. At the task level, an ideal prediction would use knowledge of the number of tasks and the duration of each task (or at least a distribution of task times). Prediction of task time utilizes knowledge of how each element that contributes to task time (overhead, compute, memory, etc.) change on a new set of hardware, and these relationships can be complicated. At the stage level, the possibilities of stage concurrency and stage dependency are accounted for. Data skew also presents a unique challenge, as the makespan of a stage with large skew can be dominated by just a small subset of the tasks. Accounting for all these elements and more may ultimately use task-level simulation of applications on different hardware. Scheduler <b>100</b> and/or method(s) <b>200</b>, <b>300</b>, and/or <b>400</b> may be extended to such simulations.</p><p id="p-0066" num="0065">A coarse but surprisingly effective level of prediction can be achieved by restricting the scope of hardware changes and making some assumptions about how stage makespan scales with the hardware. At the core of this metric is the total task time, or the amount of core-time in each stage spent actively working (e.g. reading data, computing, etc.). This value is calculated by summing the duration of each task within a stage. The following restrictions and assumptions may be made in some embodiments:</p><p id="p-0067" num="0066">Hardware class is fixed (e.g. the AWS&#x2122; m5._class);</p><p id="p-0068" num="0067">Total task time is invariant to hardware changes (subclass or number of workers);</p><p id="p-0069" num="0068">Total task time scales linearly with input data size;</p><p id="p-0070" num="0069">No major data skew;</p><p id="p-0071" num="0070">Stages are initiated sequentially, though they may run concurrently. The stage order is preserved from the input log.</p><p id="p-0072" num="0071">Prediction of the application's run time begins by calculating the total task time and total dead time, where dead time is the amount of time a core spent not working tasks provided that it worked on at least one task. This time is distinct from unused cores which accounts for cores that did not receive any tasks. Dead time primarily exists only at the end of stages, when a core has no new tasks to work on while another core is still finishing a task. This value is larger for tasks more skew, where a small number of subset tasks may take a long time to finish on just a few cores.</p><p id="p-0073" num="0072">If the stage reads in data from either disk or a shuffle, then the total task time is scaled linearly with the projected change in input data size. In other words, if the user is going to operate on a data set that is twice as large as the original, then total task time is also twice the original total task time. Across different hardware types, the total task time is considered invariant.</p><p id="p-0074" num="0073">Finally, the scaled total task time and total dead time get redistributed across the number of expected active cores in the new cluster, here called the parallelism. In some embodiments, parallelism may be set equal to the minimum of either the total number of worker cores in the cluster or the predicted number of tasks for the stage. If the number of tasks is less than the number of cores, then there will be cores left unused by that stage. On the other hand, if there are more tasks than cores, then under the current assumptions those tasks will be spread evenly across those cores.</p><p id="p-0075" num="0074"><figref idref="DRAWINGS">FIG. <b>6</b></figref> depicts an embodiment <b>600</b> of a coarse-grained runtime prediction for a single stage. In particular, input stage data related to the task time and dead time may be obtained from the log file(s). This data may be aggregated and scaled. The scaled data may be redistributed to a number of cores corresponding to the selected hardware configuration. This provides a predicted run time for the stage, as well as indicates the unused cores. This process can be written as:</p><p id="p-0076" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mrow>  <mrow>   <mi>Total</mi>   <mo>&#x2062;</mo>   <mtext>   </mtext>   <mi>Dead</mi>   <mo>&#x2062;</mo>   <mtext>   </mtext>   <mi>Time</mi>  </mrow>  <mo>=</mo>  <mrow>   <mo>(</mo>   <mrow>    <mrow>     <mi>Stage</mi>     <mo>&#x2062;</mo>     <mtext>   </mtext>     <mi>Time</mi>    </mrow>    <mtext> </mtext>    <mo>-</mo>    <mrow>     <mi>Total</mi>     <mo>&#x2062;</mo>     <mtext>   </mtext>     <mi>Task</mi>     <mo>&#x2062;</mo>     <mtext>   </mtext>     <mrow>      <mi>Time</mi>      <mtext> </mtext>      <mo>/</mo>      <mi>parallelism</mi>     </mrow>    </mrow>   </mrow>   <mo>)</mo>  </mrow> </mrow></math></maths><maths id="MATH-US-00001-2" num="00001.2"><math overflow="scroll"> <mrow>  <mrow>   <mi>Pred</mi>   <mo>&#x2062;</mo>   <mtext>   </mtext>   <mi>Stage</mi>   <mo>&#x2062;</mo>   <mtext>   </mtext>   <mi>Time</mi>  </mrow>  <mo>=</mo>  <mfrac>   <mrow>    <mrow>     <mi>data</mi>     <mo>&#x2062;</mo>     <mtext>  </mtext>     <mi>Scale</mi>     <mo>*</mo>     <mi>Total</mi>     <mo>&#x2062;</mo>     <mtext>   </mtext>     <mi>Task</mi>     <mo>&#x2062;</mo>     <mtext>   </mtext>     <mi>Time</mi>    </mrow>    <mtext> </mtext>    <mo>+</mo>    <mrow>     <mi>Total</mi>     <mo>&#x2062;</mo>     <mtext>   </mtext>     <mi>Dead</mi>     <mo>&#x2062;</mo>     <mtext>   </mtext>     <mi>Time</mi>    </mrow>   </mrow>   <mrow>    <mi>pred</mi>    <mo>&#x2062;</mo>    <mtext>   </mtext>    <mi>Parallelism</mi>   </mrow>  </mfrac> </mrow></math></maths></p><p id="p-0077" num="0075">Here StageTime is the duration between the start of the first task and the end of the last task, dataScale accounts for changes in input data size, parallelism is the amount of parallelism in the input log, and predParallelism is the predicted amount of parallelism on the new set of hardware.</p><p id="p-0078" num="0076">For runtime prediction of an entire application, the predicted stage runtimes are combined into a makespan. The simplest model will stitch the predicted runtimes together end-to-end, in which case the predicted application runtime is the sum of all predicted stage runtimes. In reality, there can exist concurrency in stages, this is the case if one stage does not fill all available cores at a given time and the subsequent stage has no unfulfilled dependencies. A method for accounting for these effects is described in the Task Simulation Based Predictor section.</p><p id="p-0079" num="0077">The prediction model using end-to-end stage stitching was tested using a TPC-DS data set. <figref idref="DRAWINGS">FIG. <b>7</b></figref> depicts an embodiment of graph <b>700</b> of predicted run time versus number of nodes The input log was from a run using 1 TB of data on a cluster with 4 m5.4&#xd7;large workers and is indicated by the black star. This log was then projected onto different instance types, worker counts, and a different data size using scheduler <b>110</b> and/or method(s) <b>200</b>, <b>300</b>, and/or <b>400</b>. In most cases the prediction is very close to the measured values, only consistently falling short when projecting onto a low number of workers.</p><p id="p-0080" num="0078">In practice, one or more logs will be ingested by scheduler <b>110</b> and/or method(s) <b>200</b>, <b>300</b>, and/or <b>400</b> and then projected onto a set of potential instance types and numbers of nodes. The relevant set of instance types is determined by application characteristics (memory and compute requirements), user input (companies may wish to run on specific node types), available budget for the job, and availability (certain nodes are only available in certain regions). Then, for each hardware set and corresponding runtime prediction, the predicted cost is calculated using the mathematical model described herein.</p><p id="p-0081" num="0079">Once cost and runtime predictions are complete, they are combined into cost-to-runtime curves upon which static cloud configuration optimization can be performed. Depending on the user preferences and particular application requirements, the optimization can be tuned to weight runtime, cost, or reliability appropriately and return a tailored optimal configuration for the user.</p><p id="p-0082" num="0080">For example, <figref idref="DRAWINGS">FIG. <b>8</b></figref> is a flow-chart depicting an embodiment of method <b>800</b> for incorporating costs in allocating resources. Method <b>800</b> is also described in the context of determining predicted costs for a single application. In some embodiments, costs for multiple applications may be determined. In such embodiments, interactions between applications that are to be processed at overlapping times may be considered by method <b>800</b>. Method <b>800</b> may be used in conjunction with system <b>100</b> and methods <b>200</b> and/or <b>300</b>. For example, method <b>800</b> may be used for performing <b>312</b> for the hardware infrastructures selected. In other embodiments, method <b>800</b> may be utilized with other systems and/or methods.</p><p id="p-0083" num="0081">The predicted cost for each hardware infrastructure is determined, at <b>802</b>. In some embodiments, <b>802</b> includes applying the cloud economic information to the predicted run time determined for each hardware infrastructure. For example, the costs per unit time and/or core may be applied to the run time and cores for the application and corresponding hardware infrastructure. The relationship between the predicted costs and the predicted run time may be identified and presented to the user, at <b>804</b>. The cloud resources to be used for the application are determined based on the predicted costs and run time. Thus <b>806</b> is analogous to <b>316</b>. In some embodiments, the user may select the desired cost and run time and the resources automatically allocated accordingly.</p><p id="p-0084" num="0082">For example, <figref idref="DRAWINGS">FIG. <b>9</b></figref> depicts an embodiment of the predicted cost-to-runtime curves of a single SPARK&#x2122; machine learning benchmark on just 7 different AWS&#x2122; instances (i.e. seven different hardware infrastructures). The different instance types are indicated by lines, and each point therein represents a different number of worker nodes. Scheduler <b>100</b> and/or method(s) <b>200</b>, <b>300</b>, <b>400</b>, and/or <b>800</b> may quickly generate these curves to give both customers and deeper cost optimization models the critical information needed to make the proper decisions.</p><p id="p-0085" num="0083">Additional vectors can be added to the prediction(s) performed via methods <b>200</b>, <b>300</b>, <b>400</b>, and/or <b>800</b> and/or scheduler <b>100</b>, such as how reliability scales with the number of nodes, as indicated in <figref idref="DRAWINGS">FIG. <b>10</b></figref>. For example, on AWS&#x2122; spot instances, the probability of node failure is listed publicly. This information can be inserted into a 3rd axis to show &#x201c;probability of node failure&#x201d; as a function of cost and runtime.</p><p id="p-0086" num="0084">To account for stage overlap in runtime prediction, a simulation of task placement may be performed. In this simulation, the total task time for a stage is placed in blocks onto available cores, starting with the maximum of the either earliest available core time or the time when all dependencies are fulfilled. If two or more consecutive stages do not fill all cores and do not have any unfilled dependencies, then they will be placed in blocks at the same time onto different cores. The total task time for a stage may be broken down into several blocks depending on how core availability changes during that stage's placement.</p><p id="p-0087" num="0085"><figref idref="DRAWINGS">FIG. <b>11</b></figref> depicts an example block simulation <b>1100</b> for coarse grained prediction. Block simulation <b>1100</b> may be achieved from a TPC-DS query <b>40</b>. The stages <b>1102</b>, <b>1104</b>, and <b>1106</b> (furthest left and bottom, second from left, third from left), are all able to start concurrently as they are the first three stages and there are no interdependencies between them. Stages <b>1106</b> and <b>1108</b> place their total task time in separate blocks according to the core availability. Specifically, stage <b>1108</b> first has a small block placed (&#x2dc;1 to 1.5 s) that overlaps with stage <b>1106</b>, after which a second larger block is placed (&#x2dc;1.5 to 11 s) which represents the remainder of the total task time. The application is completed using stages <b>1110</b> and <b>1112</b></p><p id="p-0088" num="0086">The prediction indicated in the embodiment shown in <figref idref="DRAWINGS">FIG. <b>11</b></figref> neglects effects of individual task variance (e.g. stochastic variances or data skew), though it is computationally efficient. Another way to predict application run time would be to simulate the placement of individual tasks of varying length. This task-level simulation method is discussed further below.</p><p id="p-0089" num="0087">Full simulation of a SPARK&#x2122; application is another method for predicting application run time on different hardware configurations that is usable by methods <b>200</b>, <b>300</b>, <b>400</b>, and/or <b>800</b> and/or scheduler <b>100</b>. A distribution of task times is predicted for each stage. A set of tasks is drawn from that distribution. The placement of all tasks from all stages onto executors is simulated.</p><p id="p-0090" num="0088">Predicting task distributions begins by calculating the discrete task distribution of each stage of the input log file(s). The task distribution of an individual stage is represented by I(t<sub>i</sub>), which describes the probability I of drawing a task of duration t<sub>i </sub>when drawing randomly from the set of tasks {t<sub>i</sub>}. This is accomplished by creating a histogram of the task durations for each stage. It is then predicted how this distribution will change on a new set of hardware. A simple model for this prediction is one which preserves the shape of the input distribution but allows it to scale and shift according to the number of tasks and total duration of tasks on a new set of hardware. The predicted distribution is then given by:</p><p id="p-0091" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>P</i>(<i>t</i><sub>i</sub>)=<i>a*I</i>(<i>at</i><sub>i</sub><i>+b</i>)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0092" num="0089">where the coefficients a and b depend on the changes to the hardware. This process is depicted in <figref idref="DRAWINGS">FIG. <b>12</b></figref>, which shows an embodiment <b>1200</b> how the shape of the task distribution is preserved in the transformation to new hardware. The shape of the input distribution is informed by the set of input tasks (bars under I(t<sub>i</sub>)) while the set of predicted tasks (bars under P(t<sub>i</sub>)) are drawn from the predicted distribution P(t<sub>i</sub>).</p><p id="p-0093" num="0090">A set of tasks is drawn from the predicted distribution. If it is predicted that there will be N tasks for a stage, then N tasks are (e.g. randomly) drawn from the predicted distribution P(t<sub>i</sub>). This random drawing process makes this a stochastic method, so it may be desirable to repeat the simulation multiple times to understand the statistical expectation for application runtime. In some embodiments, another mechanism for drawing tasks from the predicted distribution.</p><p id="p-0094" num="0091">Once a set of tasks is generated for each stage, then the placement of these tasks onto executors is simulated. For a predicted set of hardware the set of cores {C<sub>i</sub>} is considered. Beginning with the set of tasks from the first stage, tasks are placed one at a time onto the core with the earliest unoccupied core. in some embodiments, tasks from the subsequent stage do not begin placement until all tasks from the previous stage have been placed, and the order of stages of the input log is respected in the prediction. In the case that the subsequent stage has a dependency on a prior stage, then the earliest unoccupied time for all cores is set to the latest completion time of the tasks from the parent stage.</p><p id="p-0095" num="0092">For example, <figref idref="DRAWINGS">FIG. <b>13</b></figref> depicts an embodiment <b>1300</b> in which a task t<sub>i </sub>from the set of tasks {t<sub>i</sub>} in the earliest stage (stage 1) is placed onto the core C<sub>i </sub>with the earliest availability. In the embodiment shown, once all tasks from Stage 1 have been placed, tasks from stage 2 begin placement.</p><p id="p-0096" num="0093">Once all tasks from all stages have been placed, the application runtime is predicted as the time from the application start (possibly prior to the placement of the first task) to the end of the last task. A significant benefit of this technique is that complexities such as task duration outliers, natural task duration variance, and task skew are organically accounted for by utilizing the input task distribution to predict the new task distribution. Furthermore, it is relatively simple to add or reduce the amount of variance to create more sophisticated models in the future.</p><p id="p-0097" num="0094">A sample result using a 1 TB TPC-DS data set is depicted in <figref idref="DRAWINGS">FIG. <b>14</b></figref>, which plots an embodiment <b>1400</b> the total run time of all queries versus number of workers for four different hardware sets. The log file used for the predictions was from an m5.4&#xd7;large run with four worker nodes (indicated by the black arrow) was input and predicted onto the other hardware sets. Predicted values are indicated with solid lines, while measured values are given by the markers. Both the measured and predicted run times are shown, as well as the percent error for each query.</p><p id="p-0098" num="0095">In some SPARK&#x2122; applications there is the potential to reduce application runtime by reordering stage placement. To do so, stages may be prioritized and placed accordingly as part of method(s) <b>200</b>, <b>300</b>, <b>400</b>, and/or <b>800</b>. In particular, occasionally stages may be placed early during an application when the results from that stage are not needed until the final stage. In practice, it may be better to delay the placement of such a stage to work on higher-priority stages sooner. One technique for prioritizing stage placement would be to give each stage equal to the number of total steps, including branches, to the final sink of the application. Heuristically, this type of scoring gives higher priority to stages which have more future dependencies.</p><p id="p-0099" num="0096"><figref idref="DRAWINGS">FIG. <b>15</b></figref> depicts a tree diagram of an embodiment of the stage dependency. The dependencies depicted in <figref idref="DRAWINGS">FIG. <b>15</b></figref> may be form a query <b>30</b> of the TPC-DS benchmark. The stage-id is indicated with each box. The original stage placement was sequential (<b>801</b>, <b>802</b>, <b>803</b>, <b>804</b>, <b>805</b>, <b>809</b>, <b>811</b>, <b>817</b>, <b>825</b>, <b>835</b>). The priority score given to each stage is indicated in above the corresponding box and is equal to the sum of the children's scores, plus the <b>1</b> for each connection to the children.</p><p id="p-0100" num="0097">In the original ordering, stages <b>803</b> and <b>804</b> come early in the application. However, both stages are only dependencies of the final stage <b>835</b>. Consequently, stages <b>803</b> and <b>804</b> are given low scores. Simulating this application with tasks placed according to descending stage score shows significant predicted improvement in run time (7.6 s to 6.5 s). <figref idref="DRAWINGS">FIGS. <b>16</b>A and <b>16</b>B</figref> depicts embodiments of the measured task placement with original stage ordering <b>1600</b>A and after reordering stages <b>1600</b>B according to descending priority score. As can be seen by a comparison of <figref idref="DRAWINGS">FIGS. <b>16</b>A and <b>16</b>H</figref>, task placement <b>1600</b>B indicates that tasks may be more tightly packed and result in an improvement in run time.</p><p id="p-0101" num="0098">A component of Java-based distributed computing frameworks is memory management and garbage collection. In these frameworks, memory does not need to be explicitly managed by the developer. Instead, it is automatically handled by the garbage collector. However, for the garbage collector to work more efficiently, there are a number of parameters to tune including which garbage collector to use (parallel, CMS, G1), when to initiate garbage collection (InitiatingOccupancyFraction), how to distribute the heap between different generations (NewRatio), among others. This results in the common practice of tuning garbage collection parameters for each individual application to avoid out of memory errors and use clusters effectively. Tuning garbage collection parameters can be expensive both in developer time and infrastructure cost due the high number of potential combinations of different parameters.</p><p id="p-0102" num="0099">One way to avoid costly tuning experiments is to create a model of memory usage dependent on garbage collection parameters. Then, with collected data from a single run of an application, memory usage when using different garbage collection parameters can be predicted. By performing a simulation of memory usage, the user can avoid actually running the program multiple times.</p><p id="p-0103" num="0100">Memory usage can be modeled by breaking down memory usage into two factors: allocation of memory for objects, and garbage collection of unused objects to free memory. At a high level, in order to avoid out of memory errors, the rate of memory allocation and the rate of garbage collection should be approximately equal. However, garbage collection is generally a responsive process. For example, garbage collection is typically triggered when certain conditions are met. Even if the average rate of garbage collection is the same as the average rate of memory allocation, out of memory errors can still occur. Therefore, modeling the memory usage across time, with the garbage collection as a responsive process, may improve performance. The change in memory usage at time t is the difference between memory allocated at time t and the memory freed from garbage collection at time t. Whether garbage collection is happening depends on the memory allocated and the garbage collection parameters. This can be expressed by the following equation:</p><p id="p-0104" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mrow>  <mrow>   <mfrac>    <mrow>     <mi>dM</mi>     <mo>&#x2062;</mo>     <mtext>  </mtext>     <mi>usage</mi>    </mrow>    <mi>dt</mi>   </mfrac>   <mo>=</mo>   <mrow>    <mrow>     <msub>      <mi>M</mi>      <mi>allocated</mi>     </msub>     <mo>(</mo>     <mrow>      <mi>t</mi>      <mo>,</mo>      <mtext> </mtext>      <mrow>       <mi>data</mi>       <mo>&#x2062;</mo>       <mtext>   </mtext>       <mi>size</mi>      </mrow>     </mrow>     <mo>)</mo>    </mrow>    <mo>-</mo>    <mrow>     <msub>      <mi>M</mi>      <mi>gc</mi>     </msub>     <mo>(</mo>     <mrow>      <mi>t</mi>      <mo>,</mo>      <mrow>       <mi>gc</mi>       <mo>&#x2062;</mo>       <mtext>   </mtext>       <mi>parameters</mi>      </mrow>     </mrow>     <mo>)</mo>    </mrow>   </mrow>  </mrow>  <mo>,</mo> </mrow></math></maths></p><p id="p-0105" num="0000">where M<sub>usage</sub>(t)=the memory used at time t,</p><p id="p-0106" num="0101">M<sub>allocated</sub>(t, data size)=the memory allocated at time t for a given data size</p><p id="p-0107" num="0102">M<sub>gc</sub>(t, M<sub>usage</sub>, gc parameters)=memory freed from garbage collection at time t,<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0000">    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0103">which is a function of M<sub>usage </sub>and garbage collection parameters</li>    </ul>    </li></ul></p><p id="p-0108" num="0104"><figref idref="DRAWINGS">FIGS. <b>17</b>A-<b>17</b>B</figref> depict embodiments of memory usage measured and predicted while running the same with different garbage collection parameters. In <figref idref="DRAWINGS">FIG. <b>17</b>A</figref> (Failed Run), the garbage collection cannot catch up with memory allocation, the memory usage is unstable and continually growing, and the application fails. The measured memory usage in this case stops because the application failed. In <figref idref="DRAWINGS">FIG. <b>17</b>B</figref> (Successful Run) with different garbage collection parameters, the memory usage is stable and under the memory limit. In both cases, the prediction captures the stability or instability of the memory usage as a function of the application and garbage collection parameters.</p><p id="p-0109" num="0105">This model can be used to both accurately size the driver and workers of a distributed computing application, and also to pick the optimal garbage collection parameters to use.</p><p id="p-0110" num="0106">In order to determine the predicted costs for an application using method(s) <b>200</b>, <b>300</b>, <b>400</b>, and/or <b>800</b>, costs may be modeled. The typical distributed cloud computing paradigm includes the temporary rental of computational resources, including &#x201c;nodes&#x201d; (virtual machines each associated with a fixed amount of virtual CPUs and RAM) each with some amount of fixed external memory storage (e.g. EBS storage on AWS&#x2122;). The nodes are interconnected in a cluster, on which a user executes some application designed to utilize the distributed resources. The typical workflow using a cluster is as follows: spin-up cluster&#x2192;load/install necessary applications&#x2192;run applications&#x2192;spin-down cluster. The cost of running such a workflow is the cost rate of the resources [$/hr] multiplied by the active time of the cluster [hr] consisting of the time between spin-up and spin-down. The cost can most simply be represented by:</p><p id="p-0111" num="0000"><maths id="MATH-US-00003" num="00003"><math overflow="scroll"> <mrow>  <mi>C</mi>  <mo>=</mo>  <mrow>   <mi>t</mi>   <mo>&#x2062;</mo>   <mrow>    <munderover>     <mo>&#x2211;</mo>     <mrow>      <mi>i</mi>      <mo>=</mo>      <mn>1</mn>     </mrow>     <mi>N</mi>    </munderover>    <mtext> </mtext>    <mrow>     <mo>&#x230a;</mo>     <mrow>      <msubsup>       <mi>P</mi>       <mi>i</mi>       <mrow>        <mo>(</mo>        <mi>node</mi>        <mo>)</mo>       </mrow>      </msubsup>      <mo>+</mo>      <mrow>       <msub>        <mi>M</mi>        <mi>i</mi>       </msub>       <mo>&#x2062;</mo>       <msubsup>        <mi>P</mi>        <mi>i</mi>        <mrow>         <mo>(</mo>         <mi>mem</mi>         <mo>)</mo>        </mrow>       </msubsup>      </mrow>     </mrow>     <mo>&#x230b;</mo>    </mrow>   </mrow>  </mrow> </mrow></math></maths><ul id="ul0003" list-style="none">    <li id="ul0003-0001" num="0000">    <ul id="ul0004" list-style="none">        <li id="ul0004-0001" num="0107">C&#x2261;Cost of cluster [$]</li>        <li id="ul0004-0002" num="0108">N&#x2261;Number of nodes in cluster</li>        <li id="ul0004-0003" num="0109">t&#x2261;Cluster Runtime [hr]</li>        <li id="ul0004-0004" num="0110">P<sub>i</sub><sup>(node)</sup>&#x2261;Rate of i<sup>th </sup>node [$/hr]</li>        <li id="ul0004-0005" num="0111">P<sub>i</sub><sup>(mem)</sup>&#x2261;Rate of added memory to the i<sup>th </sup>node [$/GB/hr]</li>        <li id="ul0004-0006" num="0112">M<sub>i</sub>&#x2261;Volume of memory added to i<sup>t h </sup>node [GB]</li>    </ul>    </li></ul></p><p id="p-0112" num="0113">This model is valid for both heterogeneous clusters (multiple node types) and homogeneous clusters (mixed node types), and also allows for heterogeneity in the added memory.</p><p id="p-0113" num="0114">When running SPARK&#x2122; applications using the YARN resource manager in client mode, the cluster has a single driver node which manages the application and distributes tasks, plus a number of worker nodes which do the computation. In this situation, it is common to utilize heterogeneous clusters, in which the workers are all the same node type while the driver is of a different node type with fewer resources. This split is performed because the driver is purchased as an on-demand instance so that it will not be removed during the application runtime. The workers, on the other hand, may be reserved as spot instances, which are much cheaper but may be removed at any time according to market demands. SPARK&#x2122; applications can recover from lost workers, but not from a lost driver. This split is possible because the driver requires much fewer resources than workers. With this cluster infrastructure, the cost may be written as:</p><p id="p-0114" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>C=t</i>(<i>N</i>)&#xb7;<i>P</i><sup>(dr)</sup><i>+t</i>(<i>N</i>)&#xb7;<i>N</i>&#xb7;(<i>P</i><sup>(wrk)</sup><i>+M&#xb7;P</i><sup>(mem)</sup>)<?in-line-formulae description="In-line Formulae" end="tail"?><ul id="ul0005" list-style="none">    <li id="ul0005-0001" num="0000">    <ul id="ul0006" list-style="none">        <li id="ul0006-0001" num="0115">C&#x2261;Cost of cluster [$]</li>        <li id="ul0006-0002" num="0116">N&#x2261;Number of nodes in cluster</li>        <li id="ul0006-0003" num="0117">t(N)&#x2261;Cluster Runtime [hr]</li>        <li id="ul0006-0004" num="0118">P<sup>(wrk)</sup>&#x2261;Rate of a worker node [$/hr]</li>        <li id="ul0006-0005" num="0119">P<sup>(dr)</sup>&#x2261;Rate of the driver node [$/GB/hr]</li>        <li id="ul0006-0006" num="0120">P<sup>(mem)</sup>&#x2261;Rate of added memory [$/GB/hr]</li>        <li id="ul0006-0007" num="0121">M&#x2261;Volume of memory added per worker[GB]</li>    </ul>    </li></ul></p><p id="p-0115" num="0122">Here it is assumed that the added storage is homogeneous and equal across all worker nodes, while the driver is given no additional storage. Note that the functional dependence of runtime on the number workers has been made explicit with t(N). Thus, costs may also be calculated for method(s) <b>200</b>, <b>300</b>, <b>400</b>, and/or <b>800</b>. Consequently, improved allocation of resources may still be achieved in such environments.</p><p id="p-0116" num="0123"><figref idref="DRAWINGS">FIG. <b>18</b></figref> is a flow-chart depicting an embodiment of method <b>1800</b> for autoprovisioning (i.e. automatically allocating or reallocating) resources for an application. Method <b>1800</b> may be used with APACHE AIRFLOW&#x2122; and is described in the context of SPARK&#x2122;. However, method <b>1800</b> may be used in other environments (e.g. AWS&#x2122; Glue) and with other platforms. Method <b>1800</b> is also described in the context of allocating resources for a single application. In some embodiments, costs for multiple applications may be determined. In such embodiments, interactions between applications that are to be processed at overlapping times may be considered by method <b>1800</b>. Method <b>1800</b> may be used in conjunction with system <b>100</b> and/or methods <b>200</b>, <b>300</b>, <b>400</b> and/or <b>800</b>. In other embodiments, method <b>1800</b> may be utilized with other systems and/or methods.</p><p id="p-0117" num="0124">The job is considered to start at <b>1802</b>. It is determined whether there has been a change for which allocation of resources for an application is to be updated, at <b>1804</b>. In some embodiments, therefore, resources have already been allocated to the application. The cloud resources (e.g. the cores in the cluster) may have been allocated using scheduler <b>100</b> and/or method(s) <b>200</b>, <b>300</b>, <b>400</b>, and/or <b>800</b> as described herein. In general, <b>1804</b> may be considered to identify whether a change to the cloud resources already allocated to the application, the application itself, or other features related to the application are such that a reallocation of resources may be desired. For example, the change may be a change to the application itself (e.g. a change in the SPARK&#x2122; code), a change in the data input to the application, a change within the cluster/cloud resources (e.g. a change to the cores in the cluster), a change in the cost and pricing of the cluster/cloud resources, and/or a change in the priorities of the cluster/cloud resources. Other changes resulting in a reallocation of resources are possible.</p><p id="p-0118" num="0125">If it is determined that a change for which resources are to be reallocated has not occurred, then the application is run using the previous resources allocated, at <b>1806</b>. Thus, the previous cluster settings may be used. In response to <b>1804</b> identifying the change for which resources should be reallocated, the predictor described herein is run, at <b>1808</b>. Thus, new cluster settings (e.g. the number of cores) may be obtained. A new cluster is created based on these settings, at <b>1810</b>. The application is run using the new cluster, at <b>1812</b>.</p><p id="p-0119" num="0126">Through method <b>1800</b>, the predictor may be used to autoprovision resources as part of running an application. Thus, clusters may be automatically configured for optimal cost and/or performance as the code, data, and/or priorities change. Performance of the application and use of the cluster may, therefore, be improved.</p><p id="p-0120" num="0127">Thus, using the methods and scheduler described herein, allocation of cloud resources may be improved both in the time taken to allocate resources and the optimization of resources allocated. In addition, scheduling may be improved at multiple levels. Inter-job dependencies may be monitored, workload precedence (e.g. in a multi-tenant cluster) may be better accounted for, and inter-job optimizations used. For example, all jobs may have resources allocated (including scheduling of tasks and stages) using the methods and system described herein. The reduction in run time may allow for additional jobs to utilize the cluster. Intra-job dependencies, skew, caching and optimization of the priorities of stages may be improved. Stage-level scheduling may be enhanced using the predicted run times for stages and the resource requirement knowledge obtained utilizing the log file(s). Task level scheduling may also be enhanced. For example, pairing of tasks in a stage with nodes executing the tasks may be optimized.</p><p id="p-0121" num="0128">Although the foregoing embodiments have been described in some detail for purposes of clarity of understanding, the invention is not limited to the details provided. There are many alternative ways of implementing the invention. The disclosed embodiments are illustrative and not restrictive.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001 MATH-US-00001-2" nb-file="US20230004440A1-20230105-M00001.NB"><img id="EMI-M00001" he="14.14mm" wi="76.20mm" file="US20230004440A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002" nb-file="US20230004440A1-20230105-M00002.NB"><img id="EMI-M00002" he="5.67mm" wi="76.20mm" file="US20230004440A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00003" nb-file="US20230004440A1-20230105-M00003.NB"><img id="EMI-M00003" he="8.13mm" wi="76.20mm" file="US20230004440A1-20230105-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method, comprising:<claim-text>extracting information from at least one log file for an application; and</claim-text><claim-text>determining an allocation of cloud resources for the application based on the information from the at least one log file.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the determining the allocation of the cloud resources further includes:<claim-text>determining a number of cores in a cluster for the cloud resources allocated to the application.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the at least one log file corresponds to a single run of the application.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the extracting information from the at least one log file further includes:<claim-text>obtaining at least one of task data, cloud settings, hardware information, cloud economic information or cloud reliability information.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the determining the allocation of the cloud resources further includes:<claim-text>determining a plurality of hardware infrastructures;</claim-text><claim-text>determining a predicted run time for each of the hardware infrastructures based on the information in the at least one log file.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the determining the allocation of the cloud resources further includes:<claim-text>determining a predicted cost for each of the hardware infrastructures; and</claim-text><claim-text>determining the predicted cost versus the predicted run time for each of the hardware infrastructures.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the predicted runtime determining further includes:<claim-text>determining a total dead time for a plurality of tasks corresponding to the application; and</claim-text><claim-text>determining a total task time for the plurality of tasks;</claim-text><claim-text>wherein the determining the total dead time and the total task time each include distributing the tasks over a plurality of cores in each of the plurality of hardware infrastructures.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the determining the allocation of the cloud resources further includes:<claim-text>creating a time-based model of memory usage including garbage collection parameters.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>determining whether a change has occurred in at least one of an application, input data is for the application, or cloud resources, the cloud resources including a cluster of cores assigned to the application; and</claim-text><claim-text>in response to determining that the change has occurred, performing the extracting and determining.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. A system, comprising:<claim-text>a processor configured to:<claim-text>extract information from at least one log file for an application; and</claim-text><claim-text>determine an allocation of cloud resources for the application based on the information from the at least one log file; and</claim-text></claim-text><claim-text>a memory coupled to the processor and configured to provide the processor with instructions.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the processor being configured to determine the allocation of the cloud resources the processor is further configured to:<claim-text>determine a number of cores in a cluster for the cloud resources allocated to the application.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein to determine the allocation of the cloud resources, the processor is further configured to:<claim-text>determine a plurality of hardware infrastructures;</claim-text><claim-text>determine a predicted run time for each of the hardware infrastructures based on the information in the at least one log file;</claim-text><claim-text>determine a predicted cost for each of the hardware infrastructures; and</claim-text><claim-text>determine the predicted cost versus the predicted run time for the hardware infrastructures.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein to determine the predicted runtime, the processor is further configured to:<claim-text>determine a total dead time for a plurality of tasks corresponding to the application; and</claim-text><claim-text>determining a total task time for the plurality of tasks; wherein the determining the total dead time and the total task time each include distributing the tasks over a plurality of cores in each of the plurality of hardware infrastructures.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. A computer program product embodied in a non-transitory computer readable medium and comprising computer instructions for:<claim-text>extracting information from at least one log file for an application; and</claim-text><claim-text>determining an allocation of cloud resources for the application based on the information from the at least one log file.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The computer program product of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the computer instructions for determining the allocation of the cloud resources further includes computer instructions for:<claim-text>determining a number of cores in a cluster for the cloud resources allocated to the application.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The computer program product of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the at least one log file corresponds to a single run of the application.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The computer program product of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the computer instructions for determining the allocation of the cloud resources further include computer instructions for:<claim-text>determining a plurality of hardware infrastructures;</claim-text><claim-text>determining a predicted run time for each of the hardware infrastructures based on the information in the at least one log file;</claim-text><claim-text>determining a predicted cost for each of the hardware infrastructures; and</claim-text><claim-text>determining the predicted cost versus the predicted run time for the set of hardware infrastructures.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The computer program product of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the predicted runtime determining computer instructions further include computer instructions for:<claim-text>determining a total dead time for a plurality of tasks corresponding to the application;</claim-text><claim-text>determining a total task time for the plurality of tasks; wherein the determining the total dead time and the total task time each include distributing the tasks over a plurality of cores in each of the plurality of hardware infrastructures.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The computer program product of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the computer instructions for determining the allocation of the cloud resources further includes computer instructions for:<claim-text>creating a time-based model of memory usage including garbage collection parameters.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The computer program product of <claim-ref idref="CLM-00014">claim 14</claim-ref>, further comprising computer instructions for:<claim-text>determining whether a change has occurred in at least one of an application, input data for the application, or cloud resources, the cloud resources including a cluster of cores assigned to the application; and</claim-text><claim-text>in response to determining that the change has occurred, performing the extracting and determining.</claim-text></claim-text></claim></claims></us-patent-application>