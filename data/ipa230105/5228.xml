<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005229A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005229</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17364704</doc-number><date>20210630</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>19</main-group><subgroup>20</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>579</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>90</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>B</section><class>33</class><subclass>Y</subclass><main-group>50</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>B</section><class>29</class><subclass>C</subclass><main-group>64</main-group><subgroup>386</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>19</main-group><subgroup>20</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>00201</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>00315</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>00342</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>579</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>90</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20141201</date></cpc-version-indicator><section>B</section><class>33</class><subclass>Y</subclass><main-group>50</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20170801</date></cpc-version-indicator><section>B</section><class>29</class><subclass>C</subclass><main-group>64</main-group><subgroup>386</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>441</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>10024</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2219</main-group><subgroup>012</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2219</main-group><subgroup>2021</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2219</main-group><subgroup>2016</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>0077</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">GENERATION OF DIGITAL 3D MODELS OF BODY SURFACES WITH AUTOMATIC FEATURE IDENTIFICATION</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="obligated-assignee"><addressbook><orgname>L'Oreal</orgname><address><city>Paris</city><country>FR</country></address></addressbook><residence><country>FR</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>KOSECOFF</last-name><first-name>David B.</first-name><address><city>San Francisco</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>SUWANTO</last-name><first-name>Aldina</first-name><address><city>Clichy</city><country>FR</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>CHARRAUD</last-name><first-name>Gr&#xe9;goire</first-name><address><city>Levallois-Perret</city><country>FR</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>L'Oreal</orgname><role>03</role><address><city>Paris</city><country>FR</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A computer system obtains at least one 3D scan of a body surface; automatically identifies, based on the at least one 3D scan, one or more features (e.g., nose, lips, eyes, eyebrows, cheekbones, or specific portions thereof, or other features) of the body surface; and generates a digital 3D model of the body surface. The digital 3D model includes the identified features of the human body surface. In an embodiment, the step of generating of the digital 3D model is based on the at least one 3D scan and the identified features of the body surface. In an embodiment, the digital 3D model comprises a 3D mesh file. The digital 3D model can be used in various ways. For example, output of a manufacturing process (e.g., a 3D printed item, a cosmetics product, a personal care product) can be based on the digital 3D model.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="72.31mm" wi="148.51mm" file="US20230005229A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="232.24mm" wi="150.54mm" file="US20230005229A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="143.09mm" wi="151.89mm" file="US20230005229A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="238.51mm" wi="162.31mm" file="US20230005229A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="190.08mm" wi="163.75mm" file="US20230005229A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="110.49mm" wi="161.80mm" file="US20230005229A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">SUMMARY</heading><p id="p-0002" num="0001">In one aspect, the present disclosure is directed to, among other things, a system, including a body feature unit including computational circuitry configured to generate a human body feature digital model responsive to one or more 2-dimensional and 3-dimensional digital representations of the human body feature; an augmented feature unit including computational circuitry configured to modify a sub-feature of the human body feature digital model responsive to one or more feature augmentation inputs and to generate a sub-feature modified digital model of the human body feature; and a reverse-feature unit including computational circuitry configured to generate a reverse-feature digital model of at least a portion of the sub-feature modified model of the human body feature.</p><p id="p-0003" num="0002">In one aspect, the present disclosure is directed to, among other things, a computer system that performs a method comprising obtaining one or more source images including at least one 3-dimensional (3D) scan of a human body surface; automatically identifying, based on the at least one 3D scan, one or more features (e.g., nose, lips, eyes, eyebrows, cheekbones, chin, ears, etc., or specific portions thereof, or other features) of the human body surface; generating a digital 3D model of the human body surface, wherein the digital 3D model includes the identified features of the human body surface; and transmitting the digital 3D model to a manufacturing computer system for use in a manufacturing process such as a 3D printing process. Output of the manufacturing process (e.g., a 3D-printed item, a cosmetics product, a personal care product) is based at least in part on the digital 3D model. In an embodiment, the output of the manufacturing process comprises a 3D-printed physical mold of at least a portion of the human body surface corresponding to the identified feature(s). Further details of the manufacture of such molds can be found, for example, in International Patent Publication Nos. WO2019077131 A1 and WO2019077129 A1, which are incorporated herein by reference.</p><p id="p-0004" num="0003">In an embodiment, the step of generating of the digital 3D model is based on the at least one 3D scan and the identified features of the body surface. In an embodiment, the digital 3D model comprises a 3D mesh file.</p><p id="p-0005" num="0004">In an embodiment, the at least one 3D scan includes multiple depth scans from different angles. Obtaining the 3D scan(s) includes, in an embodiment, checking for visual obstruction of the human body surface during scanning, or checking for and/or adapting the scanning process for changes in facial expression, or other movements or changes in lighting conditions.</p><p id="p-0006" num="0005">In an embodiment, the one or more source images include 2-dimensional (2D) color captures and 3D infrared captures, and generating the digital 3D model of the human body surface includes mapping the 2D color captures to the 3D infrared captures.</p><p id="p-0007" num="0006">In an embodiment, the automatic identification of the identified features includes comparing the 3D scan or one or more other source images against a database of reference images of the identified features. In an embodiment, the automatic identification of the identified features includes determining proximity to or orientation relative to at least one of the identified features to one or more other previously identified features.</p><p id="p-0008" num="0007">In one aspect, the present disclosure is directed to, among other things, a computer system that performs a method comprising obtaining at least one 3D scan of a body surface; automatically identifying, based on the at least one 3D scan of the body surface, a feature of the body surface, including comparing the at least one 3D scan against a database of reference images of the identified feature; and generating a digital 3D model of the body surface, wherein the digital 3D model includes the identified feature.</p><p id="p-0009" num="0008">In an embodiment, the digital 3D model is used in a computer-guided personal care routine, such as a mapping of skin features, administration of a skin treatment, application of a cosmetic product, diagnosis of a skin condition, or tracking changes in skin features over time.</p><p id="p-0010" num="0009">Computer-readable media having stored thereon computer-executable instructions configured to cause a computer system to perform techniques described herein are also disclosed. Corresponding computing devices and systems are also disclosed.</p><p id="p-0011" num="0010">This summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This summary is not intended to identify key features of the claimed subject matter, nor is it intended to be used as an aid in determining the scope of the claimed subject matter.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0002" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram of a software system for generation of digital 3D models based on 3D scans and automatic feature identification, according to various aspects of the present disclosure;</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>2</b>A</figref> is a schematic illustration of an arrangement in which a client computing device obtains a 3D scan of a subject and communicates with a modeling system and, optionally, a manufacturing system, according to various aspects of the present disclosure;</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>2</b>B</figref> is a schematic illustration of a modeling system, a manufacturing system, and a feature display unit configured for use in the arrangement of <figref idref="DRAWINGS">FIG. <b>2</b>A</figref>, according to various aspects of the present disclosure;</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a block diagram that illustrates an embodiment of a client computing device according to various aspects of the present disclosure;</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a block diagram that illustrates a computer system including a remote computer system, a client computing device, and a manufacturing or 3D printing computer system, in which various aspects of the present disclosure may be implemented;</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flowchart that illustrates an embodiment of a method of generating and using a digital 3D model, according to various aspects of the present disclosure; and</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a block diagram that illustrates aspects of an illustrative computing device appropriate for use as a computing device of the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0019" num="0018">The foregoing aspects and many of the attendant advantages of the present disclosure will become more readily appreciated as the same become better understood by reference to the following detailed description, when taken in conjunction with the accompanying drawings.</p><heading id="h-0003" level="1">DETAILED DESCRIPTION</heading><p id="p-0020" num="0019">Described embodiments include systems and techniques that are useful for obtaining 3-dimensional (3D) scans of features of a live subject, such as scans of the user's own face or a portion thereof, and identifying features (e.g., nose, lips, eyes, eyebrows, cheekbones, chin, ears, etc., or specific portions thereof) in those scans. Such scans and feature identification information can be used in many contexts, including making items such as custom cosmetics, personal care products, or applicators, recommending cosmetics (e.g., colors, tones, or application techniques) to enhance or diminish identified facial features, or diagnosing or tracking skin conditions (e.g. measuring depth of wrinkles, tracking size or geometry of moles or blemishes, depth of pores, etc.).</p><p id="p-0021" num="0020">Described embodiments include a capture phase and a model generation phase. In an illustrative capture phase, a software system obtains one or more source images of a feature or area of interest. The source images include a high-quality 3D scan, which is obtained through, for example, one or more depth scans of a body surface that includes the feature or area of interest. 3D scans are captured in different ways in various embodiments. In an embodiment, scanning software maps 2-dimensional (2D) color captures to 3D monochrome captures of a user in situ. In other embodiments, 3D depth data is captured directly in the IR spectrum (e.g., via IR lasers or IR cameras).</p><p id="p-0022" num="0021">In this regard, sensors suitable for use in described embodiments include 2D cameras, 3D cameras, and depth sensors. Depth sensors are used in an embodiment to obtain 3D information about surfaces and include a range of possible hardware suitable for this purpose, including RGB or infrared stereoscopic cameras, laser or infrared LiDAR sensors, and dot projectors.</p><p id="p-0023" num="0022">In a model generation phase, a software system uses output of the capture phase to produce a digital 3D model of the feature or area of interest that preserves dimensional scale of the feature or area.</p><p id="p-0024" num="0023">In an embodiment, the software system performs feature identification as part of the capture phase and/or model generation phase. In an embodiment, feature identification includes discerning the outline of the targeted feature by comparing the 3D scan against a reference set of relevant images of the same feature in a database using computer vision techniques. In such an embodiment, the reference images represent full views of the corresponding feature or area, either as full 3D scans or as image sets that capture the corresponding feature or area from multiple angles.</p><p id="p-0025" num="0024">In an embodiment, the software system transmits a digital 3D model to a manufacturing computer system for use in manufacturing a product. In an embodiment, the process of generating the digital 3D model is further guided by feature identification information.</p><p id="p-0026" num="0025">In an embodiment, a manufacturing computer system, such as a 3D printing system, uses the digital 3D model to create a custom mold or 3-dimensional stamp of a body surface using a 3D printer or other computer-controlled manufacturing device. As an example, a manufacturing computer system uses a 3D mesh file generated from a 3D scan of a person's face and feature identification information to manufacture a product. In an illustrative scenario, the software system identifies and locates a person's lips in a 3D scan, generates a 3D mesh file based on the date from the 3D scan as well as the feature identification information, and transmits the 3D mesh file to a 3D printing system, which prints a 3D mold or stamp of the user's lips. The mold or stamp helps the person apply lipstick or other cosmetics in a precise manner. As another example, 3D molds of fingernails, eyelids, or other features are created in a similar manner for use in application of cosmetics to corresponding body surfaces.</p><p id="p-0027" num="0026">In an embodiment, feature identification information is used to improve the accuracy of 3D scans while the scans are being performed, which helps to reduce scanning time and/or reduce the likelihood of erroneous data points in the scan. In an embodiment, automated filtering and/or manual touch-up of 3D scans is performed.</p><p id="p-0028" num="0027">In an embodiment, the accuracy of models generated from such scans is improved with feature identification, such as by making adjustments during point-cloud to 3D mesh conversions to remove artifacts that are not consistent with an identified feature. Such techniques reduce the time needed to generate accurate models in an embodiment, such as by reducing the need for manual adjustments after the initial model is generated. In an illustrative scenario, such techniques reduce the time between acquiring the 3D scan and manufacturing a corresponding product, such as by sending a print-ready file to a 3D printing facility to print a 3D element associated with the scanned feature or area. As a result of such techniques, the process of producing a manufactured product is made more accurate and/or efficient.</p><p id="p-0029" num="0028">In an embodiment, machine learning techniques are used to further improve the quality of efficiency of scans or conversions to 3D mesh files. In an embodiment, the number of vector points necessary to accurately reproduce a 3D model are reduced by identifying critical vector points through machine learning algorithms.</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram of an illustrative software system for generating digital 3D models based on one or more 3D scans, in accordance with described embodiments. In the example shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, a software system <b>100</b> includes scanning software <b>110</b>, object/feature identification software <b>120</b>, and digital 3D model generation software <b>130</b>. The components of the software system <b>100</b> may be implemented in a single computing device or by multiple computing devices working together. Illustrative computing devices, computer systems, and sensors suitable for implementing the software system <b>100</b> in various embodiments are described in detail below.</p><p id="p-0031" num="0030">The scanning software <b>110</b> obtains at least one 3D scan of a subject. In an embodiment, the 3D scan includes point-cloud data, which is obtained by the scanning software <b>110</b>. In an embodiment, the scanning software <b>110</b> also performs further processing on the 3D scan, such as filtering out erroneous or noisy point-cloud data, before providing the 3D scan to other components of the software system <b>100</b>.</p><p id="p-0032" num="0031">In the example shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the scanning software <b>110</b> provides the 3D scan to the object/feature identification software <b>120</b>. In an embodiment, the object/feature identification software <b>120</b> uses face recognition or object recognition techniques, such as those in the ARKit platform available from Apple Inc., to identify features in the 3D scan. In an illustrative scenario, the software identifies a subject's nose, lips, eyes, ears, or other facial features using object recognition techniques.</p><p id="p-0033" num="0032">Described embodiments are capable of implementation in many possible ways to identify features using computer vision techniques, including matching detected edges or contours, color/pixel values, depth information, or the like in different combinations, and at particular threshold levels of confidence, any of which may be adjusted based on lighting conditions, user preferences, system design constraints, or other factors.</p><p id="p-0034" num="0033">In various embodiments, feature identification information is used to improve the accuracy, speed, or efficiency of a completed scan in the capture phase or a subsequently generated digital 3D model in the model generation phase. In an embodiment, feature identification is performed remotely in a cloud computing arrangement, to take advantage of dedicated computing resources for machine learning or artificial intelligence techniques that are impractical to perform in a client computing device.</p><p id="p-0035" num="0034">In an embodiment, the software system <b>100</b> optionally performs feature identification during the capture phase, which allows the scanning process to be adapted or a completed scan to be adjusted for more accurate modeling of the identified feature. In an illustrative scenario, the scanning software <b>110</b> obtains and sends a first scan to the object/feature identification software <b>120</b>, which identifies prominent features such a nose, lips, or the like, and feeds this information back (as indicated by the dashed arrow) to the scanning software <b>110</b> to guide a subsequent scan, with strategies adapted for scanning particular identified areas. As an example, a scanning process that identifies a nose in a first, low-resolution scan adapts the strategy for a second, higher-resolution scan accordingly. Illustrative information on which adapted scanning strategies can be based include knowledge that a nose has approximate vertical symmetry and that point cloud data in the second scan that is inconsistent with this fact may be adjusted or discarded, or on more specific machine-learning or computer-vision inferences about the geometry of the identified feature or features being scanned.</p><p id="p-0036" num="0035">The scanning software <b>110</b> also provides the 3D scan to digital 3D model generation software <b>130</b>, which generates a digital 3D model. In the example shown in in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the model generation software comprises 3D mesh conversion software, which converts the 3D scan into a 3D mesh file. In an illustrative scenario, the 3D mesh conversion software converts point-cloud data from the 3D scan into a printable 3D mesh file. In an embodiment, the 3D mesh conversion software smooths the meshed contour surface as part of this process.</p><p id="p-0037" num="0036">In an embodiment, the 3D mesh file is an STL file that does not include color or texture information. In other embodiments, where color information is captured during scanning, the software performs color analysis on the scan, e.g., for color-based feature identification or to allow the system to include color information in a resulting 3D model. In still further embodiments, where texture information is captured during scanning, a 3D mesh file with color and/or texture information is generated, such as a VRML file or an OBJ file. Alternatively, the model generation software generates a digital 3D model other than a 3D mesh, such as a solid model.</p><p id="p-0038" num="0037">In an embodiment, the software system <b>100</b> is configured to send digital 3D models to a manufacturing system for use in a manufacturing process, such as in 3D printing of stamps or molds as described above, or in production of cosmetics in particular amounts, tints, textures, or the like, based on information in the digital 3D model. As but one example, a digital 3D model of a user's fact may be sent to a custom lipstick manufacturing facility, with tints, textures, or the like being customized based on the model. Alternatively, other uses of digital 3D models are possible that do not involve manufacturing or 3D printing. In an embodiment, a digital 3D model is used to map skin features (e.g., wrinkles, blemishes, visible pores, areas of hyperpigmentation, etc.). Mapping of skin features is useful, for example, to identify changes in skin conditions (e.g., changes in moles, skin pigmentation, skin texture, etc.), which can be helpful in diagnosis of dermatological conditions or for tracking progress of a skin care regimen to improve skin tone, reduce blemishes or acne lesions, minimize the appearance of wrinkles, or for other purposes.</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>2</b>A</figref> is a schematic illustration of an arrangement in which a client computing device obtains a 3D scan of a subject and communicates with a modeling system and, optionally, a manufacturing system, according to various aspects of the present disclosure. A sensor unit <b>250</b> of a client computing device <b>204</b> captures image data and/or depth data. In this regard, the sensor unit <b>250</b> includes, in an embodiment, one or more cameras (e.g., visible light cameras, IR cameras, etc.), one or more depth sensors, or a combination thereof. In the example shown in <figref idref="DRAWINGS">FIG. <b>2</b>A</figref>, the client computing device <b>204</b> is a mobile computing device such as a smartphone or tablet computer. Alternatively, other client computing devices can be used, such as a laptop or desktop computer, or a dedicated 3D scanning device.</p><p id="p-0040" num="0039">As shown in <figref idref="DRAWINGS">FIG. <b>2</b>A</figref>, the sensor unit <b>250</b> is integrated in the client computing device <b>204</b> and is positioned and configured to capture digital representations in the form of, e.g., video or still images, of a region of interest <b>212</b> of the subject's face <b>210</b> within a field of view <b>252</b>. Alternatively, the sensor unit <b>250</b> is external to the client computing device <b>204</b>. For example, one or more external 3D scanners or digital cameras in communication with a computing device may be used, or image information is captured in some other way.</p><p id="p-0041" num="0040">Referring to <figref idref="DRAWINGS">FIG. <b>2</b>A</figref>, in an embodiment, the client computing device communicates with a modeling system <b>220</b> configured to generate digital models responsive to one or more 2-dimensional and 3-dimensional digital representations of the human body feature. In an embodiment, the modeling system <b>220</b> is implemented in remote computing device separate from the client computing device <b>204</b>. Alternatively, the modeling system <b>220</b> is implemented in whole or in part by the client computing device <b>204</b>.</p><p id="p-0042" num="0041">Referring to <figref idref="DRAWINGS">FIG. <b>2</b>B</figref>, in an embodiment, the modeling system <b>220</b> includes a body feature unit <b>222</b> including computational circuitry configured to generate a human body feature digital model responsive to one or more 2-dimensional and 3-dimensional digital representations of the human body feature and an augmented feature unit <b>224</b> including computational circuitry configured to modify a sub-feature of the human body feature digital model responsive to one or more feature augmentation inputs and to generate a sub-feature modified digital model of the human body feature. In an embodiment, the augmented feature unit <b>224</b> includes computational circuitry configured to modify a texture, a geometric shape, a surface area, a volume, a pixel dimension, a voxel dimension of the human body feature digital model responsive to one or more feature augmentation inputs and to generate a sub-feature modified model of the human body feature. In an embodiment, the augmented feature unit <b>224</b> includes computational circuitry configured to modify the sub-feature of the human body feature digital model responsive to one or more augmentation inputs indicative of a change to a human body feature contour, a change to a human body feature dimension, a change to a human body feature shape, a change to a human body feature surface area, a change to a human body feature surface texture and to generate a sub-feature modified model of the human body feature.</p><p id="p-0043" num="0042">In the example shown in <figref idref="DRAWINGS">FIG. <b>2</b>A</figref>, the human body feature includes the mouth of the subject, and the sub-feature of the human body feature includes a lip. In an embodiment, the augmented feature unit <b>224</b> includes computational circuitry configured to modify the sub-feature of the human body feature digital model responsive to one or more augmentation inputs indicative of a change to a lip contour, a change to a lip dimension, a change to a lip shape, a change to a lip surface area, and/or a change to a lip surface texture, and to generate a sub-feature modified model of the human body feature. In an embodiment, the augmented feature unit <b>224</b> includes computational circuitry configured to modify the sub-feature of the human body feature digital model responsive to one or more augmentation inputs indicative of a change to a surface texture, a change to a surface area, a change to a volume, or a change to a geometric dimension. In the example shown in <figref idref="DRAWINGS">FIG. <b>2</b>A</figref>, the modified sub-feature is a modified lip, with one or more changes in terms of shape, contour, surface area, texture, or the like, relative to the original lip.</p><p id="p-0044" num="0043">Referring again to <figref idref="DRAWINGS">FIG. <b>2</b>B</figref>, in an embodiment, the modeling system <b>220</b> includes a reverse-feature unit <b>226</b> including computational circuitry configured to generate a reverse-feature digital model of at least a portion of the sub-feature modified model of the human body feature. In the example shown in <figref idref="DRAWINGS">FIG. <b>2</b>A</figref>, the reverse-feature digital model includes a concave surface that is a reverse feature of the convex surface of the modified lip. In the example shown in <figref idref="DRAWINGS">FIG. <b>2</b>A</figref>, the reverse-feature digital model is used to manufacture a custom applicator <b>242</b> on a stamp <b>240</b> that can be used to apply material, such as lipstick, to a lip, and thereby achieve a desired look or effect that is consistent with the modified sub-feature. The custom applicator <b>242</b> has a concave surface that is the reverse of the convex surface of the modified lip.</p><p id="p-0045" num="0044">Referring again to <figref idref="DRAWINGS">FIG. <b>2</b>B</figref>, in an embodiment, the modeling system <b>220</b> communicates with a feature display unit <b>238</b> including computational circuitry configured to generate one or more instances of the human body feature digital model, the sub-feature modified digital model of the human body feature, and the reverse-feature digital model for display. In an embodiment, the modeling system <b>220</b> communicates with manufacturing system <b>230</b>, which includes a feature-manufacturing unit <b>232</b> including computational circuitry configured to generate an optimized-for-manufacturing digital model of at least one of the human body feature digital model, the sub-feature modified digital model of the human body feature, or the reverse-feature digital model. In the example shown in <figref idref="DRAWINGS">FIG. <b>2</b>A</figref>, the manufacturing system <b>230</b> produces as output the stamp <b>240</b>, which includes the applicator <b>242</b> that has been manufactured according to the reverse-feature digital model.</p><p id="p-0046" num="0045">In an embodiment, the feature-manufacturing unit <b>238</b> includes computational circuitry configured to implement a discovery protocol that allows the modeling system <b>220</b> and an additive manufacturing apparatus to discover each other, to negotiate one or more pre-shared keys, and to exchange one or more parameters associated with the optimized-for-manufacturing digital model of at least one of the human body feature digital model, the sub-feature modified digital model of the human body feature, or the reverse-feature digital model.</p><p id="p-0047" num="0046">In various embodiments, referring again to <figref idref="DRAWINGS">FIG. <b>2</b>A</figref>, the sensor unit <b>250</b> includes one or more visible light cameras, infrared cameras, or depth sensors, such as LiDAR sensors or TrueDepth cameras available from Apple Inc. In an embodiment, the sensor unit <b>250</b> includes multiple cameras, such as for stereoscopic video capture and/or depth sensing. In an embodiment, the sensor unit <b>250</b> includes one or more sensors other than cameras (e.g., a LiDAR sensor or infrared dot projector for depth sensing, a proximity sensor, etc.). In an embodiment, an infrared dot projector projects infrared dots onto a surface, and reflections from the surface are measured by an infrared camera to determine the distance each dot is from the projector system. When working in conjunction with a 3D camera, depth measurements can be mapped onto a captured 3D image. This approach is used in an embodiment to generate a 3D model of a body surface.</p><p id="p-0048" num="0047">In an embodiment, the sensor unit <b>250</b> remains in a stationary position, while a user turns her head to allow the sensor unit <b>250</b> to capture images or video of the body surface <b>210</b> from different angles. In other embodiments, the sensor unit <b>250</b> is moved manually or automatically to allow the sensor unit <b>250</b> to capture images or video of the body surface <b>210</b> from different angles while the user's head remains stationary. Scanning software tracks the feature or area of interest as the camera is moved around during the capture process. In an embodiment, the software detects and compensates for changes in the feature or area of interest, such as the user's facial expression or other changes in position or orientation, during the scanning process.</p><p id="p-0049" num="0048">In an embodiment where the sensor unit <b>250</b> is moved manually, the user is provided with feedback to guide the user through the scanning process, such by audible or visible cues to guide the user to move her head or the camera in a particular pattern to assist in obtaining accurate scans. In an illustrative scenario, the client computing device <b>204</b> provides feedback to a user (e.g., via synthesized voice cues or visual indicators) to adjust the viewing angle of the sensor unit <b>250</b> or to follow a particular scanning pattern.</p><p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a block diagram that illustrates an embodiment of a client computing device <b>204</b> according to various aspects of the present disclosure. <figref idref="DRAWINGS">FIG. <b>3</b></figref> depicts a non-limiting example of client computing device features and configurations; many other features and configurations are possible within the scope of the present disclosure.</p><p id="p-0051" num="0050">In the example shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the client computing device <b>204</b> includes a sensor unit <b>250</b> and a client application <b>360</b>. The client application <b>360</b> includes a user interface <b>376</b>. In an embodiment, the user interface <b>376</b> includes interactive functionality such as graphical guides to assist a user in positioning the sensor unit <b>250</b> correctly, tutorial videos or animations, or other elements. Visual elements of the user interface <b>376</b> are presented on a display <b>340</b>, such as a touchscreen display. In an embodiment, the user interface <b>376</b> provides guidance (e.g., visual guides such as arrows or targets, progress indicators, audio/haptic feedback, synthesized speech, etc.) to guide a user to take images under particular lighting conditions, angles, etc., in order to ensure that sufficient data is collected to, e.g., obtain an accurate 3D scan.</p><p id="p-0052" num="0051">In an embodiment, the client application <b>360</b> also includes an image capture/3D scanning engine <b>370</b> configured to capture and process digital images (e.g., color images, infrared images, depth images, etc.) obtained from camera unit <b>150</b>. In an embodiment, such images are used to obtain a clean and precise 3D contour mapping of the target body surface (e.g., a face). In an embodiment, the digital images or scans are processed by the client computing device <b>204</b> and/or transmitted to a remote computer system for processing in a digital 3D model generation engine or an object/feature identification engine.</p><p id="p-0053" num="0052">In an embodiment, digital 3D models described herein are generated based on sensor data obtained by the client computing device <b>204</b>. The digital 3D models are generated by the client computing device <b>204</b> or by some other computing device, such as a remote cloud computing system, or a combination thereof. In an embodiment, digital 3D models include 3D topology and texture information, which can be used for reproducing an accurate representation of a body surface, such as facial structure and skin features.</p><p id="p-0054" num="0053">A communication module <b>378</b> of the client application <b>360</b> is used to prepare information for transmission to, or to receive and interpret information from other devices or systems, such as a remote computer system. Such information may include captured digital images, scans, or video, feature identification information, personal care device settings, user preferences, user identifiers, device identifiers, or the like.</p><p id="p-0055" num="0054">Other features of client computing devices are not shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref> for ease of illustration. Alternatively, the client computing device <b>204</b> includes different components or circuitry, or the components and circuitry described with reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref> are implemented in some other way. Further description of illustrative computing devices is provided below with reference to <figref idref="DRAWINGS">FIG. <b>6</b></figref>.</p><p id="p-0056" num="0055"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a block diagram that illustrates a system <b>400</b> in which various aspects of the present disclosure may be implemented. A client computing device <b>204</b> connects to and communicates with a remote computer system <b>410</b>. In an embodiment, the client computing device <b>204</b> captures data representative of body surfaces, such as point cloud data, and transmits this data to the remote computer system <b>410</b> for further processing or storage. The client computing device <b>204</b> may be used by a consumer, personal care professional, or some other entity to interact with other components of the system <b>400</b>.</p><p id="p-0057" num="0056">Illustrative components and functionality of the remote computer system <b>410</b> will now be described. The remote computer system <b>410</b> includes one or more server computers that implement one or more of the illustrated components, e.g., in a cloud computing arrangement. As illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the components implemented by the remote computer system <b>410</b> include a feature identification engine <b>412</b>, a 3D model generation engine <b>414</b>, a product data store <b>420</b>, and a user data store <b>422</b>.</p><p id="p-0058" num="0057">In an embodiment, the feature identification engine <b>412</b> processes data obtained during the capture phase (e.g., point cloud data, color image data, infrared image data, and/or depth data) and generates feature identification information that identifies features of a body surface to be modeled. In an embodiment, the feature identification engine <b>412</b> employs machine learning or artificial intelligence techniques (e.g., template matching, feature extraction and matching, classification, artificial neural networks, deep learning architectures, genetic algorithms, or the like) to identify features. For example, the feature identification engine <b>412</b> may analyze data obtained during the capture phase such as point cloud data, color data, depth data, or the like to identify facial structures, pigmentation, skin texture, etc., of the user's skin.</p><p id="p-0059" num="0058">In an embodiment, the 3D model generation engine <b>414</b> uses feature identification information generated by the feature identification engine <b>412</b> and the data obtained during the capture phase to generate a 3D model of a body surface or other object. In an embodiment, data obtained during the capture phase and feature identification information is associated with a user and stored in the user data store <b>422</b>. In an embodiment, user consent is obtained prior to storing any information that is private to a user or can be used to identify a user.</p><p id="p-0060" num="0059">As explained above, digital 3D models are used in manufacturing or 3D printing systems in an embodiment. Accordingly, the system <b>400</b> may include manufacturing/3D printing computer systems <b>440</b>, which receive digital 3D models from the remote computer system <b>410</b> for use in a manufacturing process, such as 3D printing of stamps or molds, production of custom cosmetics, or other processes.</p><p id="p-0061" num="0060">The devices shown in <figref idref="DRAWINGS">FIGS. <b>1</b>, <b>2</b>A, <b>2</b>B, <b>3</b>, and <b>4</b></figref>, or other devices used in described embodiments may communicate with each other via a network (not shown), which may include any suitable communication technology including but not limited to wired technologies such as DSL, Ethernet, fiber optic, USB, and Firewire; wireless technologies such as WiFi, WiMAX, 3G, 4G, LTE, 5G, and Bluetooth; and the Internet. In general, communication between computing devices or components in <figref idref="DRAWINGS">FIGS. <b>1</b>, <b>2</b>A, <b>2</b>B, <b>3</b>, and <b>4</b></figref>, or other components or computing devices used in accordance with described embodiments, occur directly or through intermediate components or devices.</p><p id="p-0062" num="0061">Many alternatives to the arrangements disclosed and described with reference to <figref idref="DRAWINGS">FIGS. <b>1</b>, <b>2</b>A, <b>2</b>B, <b>3</b>, and <b>4</b></figref> are possible. For example, functionality described as being implemented in multiple components may instead be consolidated into a single component, or functionality described as being implemented in a single component may be implemented in multiple illustrated components, or in other components that are not shown in <figref idref="DRAWINGS">FIGS. <b>1</b>, <b>2</b>A, <b>2</b>B, <b>3</b>, and <b>4</b></figref>. As another example, functionality described as being performed by a particular device may instead be performed by one or more other devices within a system. As an example, the feature identification engine <b>412</b> or the 3D model generation engine <b>414</b> may be implemented in client computing device <b>204</b> or in some other device or combination of devices.</p><p id="p-0063" num="0062">In addition to the technical benefits of described embodiments that are described elsewhere herein, numerous other technical benefits are achieved in an embodiment. For example, the system <b>400</b> allows some aspects of the process to be conducted independently by client computing devices, while moving other processing burdens to the remote computer system <b>410</b> (which may be a relatively high-powered and reliable computing system), thus improving performance and preserving battery life for functionality provided by client computing devices.</p><p id="p-0064" num="0063">In general, the word &#x201c;engine,&#x201d; as used herein, refers to logic embodied in hardware or software instructions written in a programming language, such as C, C++, COBOL, JAVA&#x2122;, PHP, Perl, HTML, CSS, JavaScript, VBScript, ASPX, Microsoft.NET&#x2122;, and/or the like. An engine may be compiled into executable programs or written in interpreted programming languages. Software engines may be callable from other engines or from themselves. Generally, the engines described herein refer to logical modules that can be merged with other engines or divided into sub-engines. The engines can be stored in any type of computer-readable medium or computer storage device and be stored on and executed by one or more general purpose computers, thus creating a special purpose computer configured to provide the engine or the functionality thereof.</p><p id="p-0065" num="0064">As understood by one of ordinary skill in the art, a &#x201c;data store&#x201d; as described herein may be any suitable device configured to store data for access by a computing device. One example of a data store is a highly reliable, high-speed relational database management system (DBMS) executing on one or more computing devices and accessible over a high-speed network. Another example of a data store is a key-value store. However, any other suitable storage technique and/or device capable of quickly and reliably providing the stored data in response to queries may be used, and the computing device may be accessible locally instead of over a network, or may be provided as a cloud-based service. A data store may also include data stored in an organized manner on a computer-readable storage medium, as described further below. One of ordinary skill in the art will recognize that separate data stores described herein may be combined into a single data store, and/or a single data store described herein may be separated into multiple data stores, without departing from the scope of the present disclosure.</p><p id="p-0066" num="0065"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flowchart that illustrates an embodiment of a method of generating and using a digital 3D model of a human body surface, according to various aspects of the present disclosure. The method <b>500</b> is performed by a computer system including one or more computing devices, such as remote computer system <b>410</b> or some other computing device or combination of devices.</p><p id="p-0067" num="0066">At block <b>502</b>, a computer system obtains one or more source images, including a 3D scan, of a human body surface, such as a face or other region of interest. In an embodiment, a remote computer system obtains a 3D scan from a client computing device or derives a 3D scan from other source images obtained from a client computing device or some other source. In an embodiment, the computer system obtains multiple depth scans from different angles. In an embodiment, the capture process for the 3D scan includes checking and compensating for changes in facial expression or other movements during scanning. In an embodiment, the capture process for the 3D scan includes checking for obstructions that interfere with scanning of the of the surface. If an obstruction is detected, the scan may be discarded or optionally followed by remedial measures, such as providing feedback to a user to remove the obstruction and restart the scanning process.</p><p id="p-0068" num="0067">At block <b>504</b>, the computer system automatically identifies one or more features based on the 3D scan. In an embodiment, a remote computer system automatically identifies one or more features in a 3D scan obtained from a client computing device. In an embodiment, the automatic identification of identified features includes determining proximity to and/or directional orientation relative to one or more other previously identified features. In an illustrative scenario, automatic identification of an eyebrow is based at least in part on its proximity to and location above a previously identified eyelid.</p><p id="p-0069" num="0068">At block <b>506</b>, the computer system generates a digital 3D model of the human body surface based on the 3D scan, in which the automatically identified features are included. In an embodiment, the digital 3D model comprises a 3D mesh file. In an embodiment, the generation of the digital 3D model is further based on feature identification information, as described in other examples herein.</p><p id="p-0070" num="0069">At block <b>508</b>, the computer system optionally transmits the digital 3D model to a manufacturing or 3D printing computer system, as described above. Alternatively, the digital 3D model is used for some other purpose, as described in other examples herein.</p><p id="p-0071" num="0070">Many alternatives to the process depicted in <figref idref="DRAWINGS">FIG. <b>5</b></figref> are possible, in accordance with embodiments described herein. For example, in an embodiment the process of obtaining a 3D scan is adapted based on identified features, as described above. As another example, techniques and devices described herein are capable of being used to generate models of objects or surfaces other than human body surfaces.</p><p id="p-0072" num="0071"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a block diagram that illustrates aspects of an exemplary computing device <b>600</b> appropriate for use as a computing device of the present disclosure. While multiple different types of computing devices were discussed above, the exemplary computing device <b>600</b> describes various elements that are common to many different types of computing devices. While <figref idref="DRAWINGS">FIG. <b>6</b></figref> is described with reference to a computing device that is implemented as a device on a network, the description below is applicable to servers, personal computers, mobile phones, smart phones, tablet computers, embedded computing devices, and other devices that may be used to implement portions of embodiments of the present disclosure. Moreover, those of ordinary skill in the art and others will recognize that the computing device <b>600</b> may be any one of any number of currently available or yet to be developed devices.</p><p id="p-0073" num="0072">In its most basic configuration, the computing device <b>600</b> includes at least one processor <b>602</b> and a system memory <b>604</b> connected by a communication bus <b>606</b>. Depending on the exact configuration and type of device, the system memory <b>604</b> may be volatile or nonvolatile memory, such as read only memory (&#x201c;ROM&#x201d;), random access memory (&#x201c;RAM&#x201d;), EEPROM, flash memory, or similar memory technology. Those of ordinary skill in the art and others will recognize that system memory <b>604</b> typically stores data and/or program modules that are immediately accessible to and/or currently being operated on by the processor <b>602</b>. In this regard, the processor <b>602</b> may serve as a computational center of the computing device <b>600</b> by supporting the execution of instructions.</p><p id="p-0074" num="0073">As further illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the computing device <b>600</b> may include a network interface <b>610</b> comprising one or more components for communicating with other devices over a network. Embodiments of the present disclosure may access basic services that utilize the network interface <b>610</b> to perform communications using common network protocols. The network interface <b>610</b> may also include a wireless network interface configured to communicate via one or more wireless communication protocols, such as WiFi, 2G, 3G, LTE, WiMAX, Bluetooth, Bluetooth low energy, and/or the like. As will be appreciated by one of ordinary skill in the art, the network interface <b>610</b> illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref> may represent one or more wireless interfaces or physical communication interfaces.</p><p id="p-0075" num="0074">In the exemplary embodiment depicted in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the computing device <b>600</b> also includes a storage medium <b>608</b>. However, services may be accessed using a computing device that does not include means for persisting data to a local storage medium. Therefore, the storage medium <b>608</b> depicted in <figref idref="DRAWINGS">FIG. <b>6</b></figref> is represented with a dashed line to indicate that the storage medium <b>608</b> is optional. In any event, the storage medium <b>608</b> may be volatile or nonvolatile, removable or nonremovable, implemented using any technology capable of storing information such as, but not limited to, a hard drive, solid state drive, CD ROM, DVD, or other disk storage, magnetic cassettes, magnetic tape, magnetic disk storage, and/or the like.</p><p id="p-0076" num="0075">As used herein, the term &#x201c;computer-readable medium&#x201d; includes volatile and non-volatile and removable and non-removable media implemented in any method or technology capable of storing information, such as computer readable instructions, data structures, program modules, or other data. In this regard, the system memory <b>604</b> and storage medium <b>608</b> depicted in <figref idref="DRAWINGS">FIG. <b>6</b></figref> are merely examples of computer-readable media.</p><p id="p-0077" num="0076">Suitable implementations of computing devices that include a processor <b>602</b>, system memory <b>604</b>, communication bus <b>606</b>, storage medium <b>608</b>, and network interface <b>610</b> are known and commercially available. For ease of illustration and because it is not important for an understanding of the claimed subject matter, <figref idref="DRAWINGS">FIG. <b>6</b></figref> does not show some of the typical components of many computing devices. In this regard, the computing device <b>600</b> may include input devices, such as a keyboard, keypad, mouse, microphone, touch input device, touch screen, and/or the like. Such input devices may be coupled to the computing device <b>600</b> by wired or wireless connections including RF, infrared, serial, parallel, Bluetooth, Bluetooth low energy, USB, or other suitable connections protocols using wireless or physical connections. Similarly, the computing device <b>600</b> may also include output devices such as a display, speakers, printer, etc. Since these devices are well known in the art, they are not illustrated or described further herein.</p><p id="p-0078" num="0077">In an embodiment, computational circuitry includes, among other things, one or more computing devices such as a processor (e.g., a microprocessor, a quantum processor, qubit processor, etc.), a central processing unit (CPU), a digital signal processor (DSP), an application-specific integrated circuit (ASIC), a field programmable gate array (FPGA), and the like, or any combinations thereof, and can include discrete digital or analog circuit elements or electronics, or combinations thereof. In an embodiment, computational circuitry includes one or more ASICs having a plurality of predefined logic components. In an embodiment, computational circuitry includes one or more FPGAs, each having a plurality of programmable logic components.</p><p id="p-0079" num="0078">In an embodiment, computational circuitry includes one or more electric circuits, printed circuits, flexible circuits, electrical conductors, electrodes, cavity resonators, conducting traces, ceramic patterned electrodes, electro-mechanical components, transducers, and the like.</p><p id="p-0080" num="0079">In an embodiment, computational circuitry includes one or more components operably coupled (e.g., communicatively, electromagnetically, magnetically, ultrasonically, optically, inductively, electrically, capacitively coupled, wirelessly coupled, and the like) to each other. In an embodiment, computational circuitry includes one or more remotely located components. In an embodiment, remotely located components are operably coupled, for example, via wireless communication. In an embodiment, remotely located components are operably coupled, for example, via one or more communication modules, receivers, transmitters, transceivers, and the like.</p><p id="p-0081" num="0080">In an embodiment, computational circuitry includes memory that, for example, stores instructions or information. Non-limiting examples of memory include volatile memory (e.g., Random Access Memory (RAM), Dynamic Random Access Memory (DRAM), and the like), non-volatile memory (e.g., Read-Only Memory (ROM), Electrically Erasable Programmable Read-Only Memory (EEPROM), Compact Disc Read-Only Memory (CD-ROM), and the like), persistent memory, and the like. Further non-limiting examples of memory include Erasable Programmable Read-Only Memory (EPROM), flash memory, and the like. In an embodiment, memory is coupled to, for example, one or more computing devices by one or more instructions, information, or power buses.</p><p id="p-0082" num="0081">In an embodiment, computational circuitry includes one or more computer-readable media drives, interface sockets, Universal Serial Bus (USB) ports, memory card slots, and the like, and one or more input/output components such as, for example, a graphical user interface, a display, a keyboard, a keypad, a trackball, a joystick, a touch-screen, a mouse, a switch, a dial, and the like, and any other peripheral device. In an embodiment, computational circuitry includes one or more user input/output components that are operably coupled to at least one computing device configured to control (electrical, electromechanical, software-implemented, firmware-implemented, or other control, or combinations thereof) at least one parameter.</p><p id="p-0083" num="0082">In an embodiment, computational circuitry includes a computer-readable media drive or memory slot that is configured to accept signal-bearing medium (e.g., computer-readable memory media, computer-readable recording media, and the like). In an embodiment, a program for causing a system to execute any of the disclosed methods can be stored on, for example, a computer-readable recording medium, a signal-bearing medium, and the like. Non-limiting examples of signal-bearing media include a recordable type medium such as a magnetic tape, floppy disk, a hard disk drive, a Compact Disc (CD), a Digital Video Disk (DVD), Blu-Ray Disc, a digital tape, a computer memory, and the like, as well as transmission type medium such as a digital or an analog communication medium (e.g., a fiber optic cable, a waveguide, a wired communications link, a wireless communication link (e.g., receiver, transmitter, transceiver, transmission logic, reception logic, etc.). Further non-limiting examples of signal-bearing media include, but are not limited to, DVD-ROM, DVD-RAM, DVD+RW, DVD-RW, DVD-R, DVD+R, CD-ROM, Super Audio CD, CD-R, CD+R, CD+RW, CD-RW, Video Compact Discs, Super Video Discs, flash memory, magnetic tape, magneto-optic disk, MINIDISC, non-volatile memory card, EEPROM, optical disk, optical storage, RAM, ROM, system memory, web server, and the like.</p><p id="p-0084" num="0083">In an embodiment, computational circuitry includes acoustic transducers, electroacoustic transducers, electrochemical transducers, electromagnetic transducers, electromechanical transducers, electrostatic transducers, photoelectric transducers, radio-acoustic transducers, thermoelectric transducers, ultrasonic transducers, and the like.</p><p id="p-0085" num="0084">In an embodiment, computational circuitry includes electrical circuitry operably coupled with a transducer (e.g., an actuator, a motor, a piezoelectric crystal, a Micro Electro Mechanical System (MEMS), etc.). In an embodiment, computational circuitry includes electrical circuitry having at least one discrete electrical circuit, electrical circuitry having at least one integrated circuit, or electrical circuitry having at least one application specific integrated circuit. In an embodiment, computational circuitry includes electrical circuitry forming a general purpose computing device configured by a computer program (e.g., a general purpose computer configured by a computer program which at least partially carries out processes and/or devices described herein, or a microprocessor configured by a computer program which at least partially carries out processes and/or devices described herein), electrical circuitry forming a memory device (e.g., forms of memory (e.g., random access, flash, read only, etc.)), electrical circuitry forming a communications device (e.g., a modem, communications switch, optical-electrical equipment, etc.), and/or any non-electrical analog thereto, such as optical or other analogs.</p><p id="p-0086" num="0085">While illustrative embodiments have been illustrated and described, it will be appreciated that various changes can be made therein without departing from the spirit and scope of the invention.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>The embodiments of the invention in which an exclusive property or privilege is claimed are defined as follows:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method comprising, by a computer system comprising one or more computing devices:<claim-text>obtaining one or more source images including at least one 3-dimensional scan of a human body surface;</claim-text><claim-text>automatically identifying, based on the at least one 3-dimensional scan, features of the human body surface;</claim-text><claim-text>generating a digital model of the human body surface, wherein the digital 3-dimensional model includes the identified features of the human body surface;</claim-text><claim-text>generate a reverse-feature digital 3D model of the digital 3D model of the human body surface; and</claim-text><claim-text>transmitting at least one of the digital 3-dimensional model or the reverse-feature digital 3D model to a manufacturing computer system for use in a manufacturing process, wherein output of the manufacturing process is based at least in part on the digital 3D model.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the at least one 3-dimensional scan includes multiple depth scans from different angles.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the identified features include a facial feature.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein obtaining the at least one 3-dimensional scan includes checking for changes in facial expression during scanning.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein obtaining the at least one 3-dimensional scan includes checking for movements of the human body surface during scanning.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein obtaining the at least one 3-dimensional scan includes checking for visual obstruction of the human body surface.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one or more source images include 2-dimensional color captures and 3-dimensional infrared captures, and wherein generating the digital 3-dimensional model of the human body surface includes mapping the 2-dimensional color captures to the 3-dimensional infrared captures.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the automatic identification of the identified features includes comparing the one or more source images against a database of reference images of the identified features.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the automatic identification of the identified features includes determining proximity to or orientation relative to at least one of the identified features to one or more other previously identified features.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the manufacturing process comprises a 3-dimensional printing process.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the output of the manufacturing process comprises a 3D-printed physical mold of at least a portion of the human body surface corresponding to the identified features.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the output of the manufacturing process includes a cosmetic product or personal care product having a physical characteristic adapted for at least one of the identified features.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the generating of the digital 3-dimensional model of the human body surface is based on the at least one 3-dimensional scan and the identified features of the human body surface.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. A method comprising, by a computer system comprising one or more computing devices:<claim-text>obtaining at least one 3-dimensional scan of a body surface;</claim-text><claim-text>automatically identifying, based on the at least one 3-dimensional scan of the body surface, a feature of the body surface, including comparing the at least one 3-dimensional scan against a database of reference images of the identified feature; and</claim-text><claim-text>generating a digital 3-dimensional model of the body surface, wherein the digital 3D model includes the identified feature.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the identified feature includes a facial feature.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref> further comprising using the digital 3-dimensional model in a computer-guided personal care routine.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the computer-guided personal care routine includes administration of a skin treatment, application of a cosmetic product, or diagnosis of a skin condition.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the generating of the digital 3-dimensional model of the body surface is based on the at least one 3D scan and the identified feature of the body surface.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. A non-transitory computer-readable medium having stored thereon computer-executable instructions configured to cause a computer system to perform steps comprising:<claim-text>obtaining at least one 3-dimensional scan of a body surface;</claim-text><claim-text>automatically identifying, based on the at least one 3-dimensional scan of the body surface, a feature of the body surface, including comparing the at least one 3-dimensional scan against a database of reference images of the identified feature; and</claim-text><claim-text>generating a digital 3-dimensional model of the body surface based on the at least one 3D scan and the identified feature of the body surface.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. A computer system comprising one or more computing devices including at least one processor and memory, the memory having stored therein computer-executable instructions configured to cause the computing device to perform steps comprising:<claim-text>obtaining at least one 3-dimensional scan of a body surface;</claim-text><claim-text>automatically identifying, based on the at least one 3-dimensional scan of the body surface, a feature of the body surface, including comparing the at least one 3-dimensional scan against a database of reference images of the identified feature; and</claim-text><claim-text>generating a digital 3-dimensional model of the body surface based on the at least one 3-dimensional scan and the identified feature of the body surface.</claim-text></claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. A system, comprising:<claim-text>a body feature unit including computational circuitry configured to generate a human body feature digital model responsive to one or more 2-dimensional and 3-dimensional digital representations of the human body feature;</claim-text><claim-text>an augmented feature unit including computational circuitry configured to modify a sub-feature of the human body feature digital model responsive to one or more feature augmentation inputs and to generate a sub-feature modified digital model of the human body feature; and</claim-text><claim-text>a reverse-feature unit including computational circuitry configured to generate a reverse-feature digital model of at least a portion of the sub-feature modified model of the human body feature.</claim-text></claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. The system of <claim-ref idref="CLM-00021">claim 21</claim-ref>, further comprising:<claim-text>a feature display unit including computational circuitry configured to generate one or more instances of the human body feature digital model, the sub-feature modified digital model of the human body feature, and the reverse-feature digital model for display.</claim-text></claim-text></claim><claim id="CLM-00023" num="00023"><claim-text><b>23</b>. The system of <claim-ref idref="CLM-00021">claim 21</claim-ref>, further comprising:<claim-text>a feature-manufacturing unit including computational circuitry configured to generate an optimized for manufacturing digital model of at least one of the human body feature digital model, the sub-feature modified digital model of the human body feature, or the reverse-feature digital model.</claim-text></claim-text></claim><claim id="CLM-00024" num="00024"><claim-text><b>24</b>. The system of <claim-ref idref="CLM-00023">claim 23</claim-ref>, wherein the feature-manufacturing unit includes computational circuitry configured to implement a discovery protocol that allows the system and an additive manufacturing apparatus to discover each other, to negotiate one or more pre-shared keys, and to exchange one or more parameter associated with the optimized for manufacturing digital model of at least one of the human body feature digital model, the sub-feature modified digital model of the human body feature, or the reverse-feature digital model.</claim-text></claim><claim id="CLM-00025" num="00025"><claim-text><b>25</b>. The system of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the augmented feature unit includes computational circuitry configured to modify a texture, a geometric shape, a surface area, a volume, a pixel dimension, a voxel dimension of the human body feature digital model responsive to one or more feature augmentation inputs and to generate a sub-feature modified model of the human body feature.</claim-text></claim><claim id="CLM-00026" num="00026"><claim-text><b>26</b>. The system of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the augmented feature unit includes computational circuitry configured to modify the sub-feature of the human body feature digital model responsive to one or more augmentation inputs indicative of a change to a human body feature contour, a change to a human body feature dimension, a change to a human body feature shape, a change to a human body feature surface area, a change to a human body feature surface texture and to generate a sub-feature modified model of the human body feature.</claim-text></claim><claim id="CLM-00027" num="00027"><claim-text><b>27</b>. The system of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the augmented feature unit includes computational circuitry configured to modify the sub-feature of the human body feature digital model responsive to one or more augmentation inputs indicative of a change to a lip contour, a change to a lip dimension, a change to a lip shape, a change to a lip surface area, a change to a lip surface texture and to generate a sub-feature modified model of the human body feature.</claim-text></claim><claim id="CLM-00028" num="00028"><claim-text><b>28</b>. The system of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the augmented feature unit includes computational circuitry configured to modify the sub-feature of the human body feature digital model responsive to one or more augmentation inputs indicative of a change to a surface texture, a change to a surface area, a change to a volume, or a change to a geometric dimension.</claim-text></claim></claims></us-patent-application>