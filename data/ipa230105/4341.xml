<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004342A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004342</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17842176</doc-number><date>20220616</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>16</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>22</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>R</subclass><main-group>3</main-group><subgroup>12</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>R</subclass><main-group>3</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>165</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>22</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>R</subclass><main-group>3</main-group><subgroup>12</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>R</subclass><main-group>3</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>2015</main-group><subgroup>223</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>R</subclass><main-group>2499</main-group><subgroup>13</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">SYSTEM AND METHOD FOR CONTROLLING OUTPUT SOUND IN A LISTENING ENVIRONMENT</invention-title><us-related-documents><us-provisional-application><document-id><country>US</country><doc-number>63216979</doc-number><date>20210630</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Harman International Industries, Incorporated</orgname><address><city>Stamford</city><state>CT</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Trestain</last-name><first-name>Christopher Michael</first-name><address><city>Livonia</city><state>MI</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Willis</last-name><first-name>Maxwell B.</first-name><address><city>Detroit</city><state>MI</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Winton</last-name><first-name>Riley</first-name><address><city>Opelika</city><state>AL</state><country>US</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>Harman International Industries, Incorporated</orgname><role>02</role><address><city>Stamford</city><state>CT</state><country>US</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A computing device for adjusting audio output in a listening environment having a user interface to select a source, from a plurality of sources, to be played at a sound zone in a plurality of sound zones; A processor receives user input source selections to determine combined sources for the plurality of sound zones and adjust audio parameters for the sound zones having combined sources based on combined sources and individual inputs for audio settings from each sound zone. The processor applies adjusted audio parameters to a sound zone algorithm for the sound zones having combined sources, and outputs audio based on the adjusted sound zone algorithm. A designated occupant may select the sound zones to which the adjusted sound zone algorithm is to be applied.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="219.79mm" wi="137.24mm" file="US20230004342A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="225.47mm" wi="140.55mm" file="US20230004342A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="230.97mm" wi="164.08mm" orientation="landscape" file="US20230004342A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="177.63mm" wi="165.95mm" file="US20230004342A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="230.89mm" wi="150.88mm" file="US20230004342A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS REFERENCE</heading><p id="p-0002" num="0001">This application claims the benefit of U.S. Provisional Application Ser. No. 63/216,979, filed Jun. 30, 2021, the disclosure of which is incorporated in its entirety by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">The present disclosure relates to controlling sound output by speakers in a vehicle and more particularly, to adjusting sound output in vehicle sound zones.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">An individual sound zone in a listening environment is intended to create individual listening zones for each occupant in the listening environment, enabling each occupant to personalize their own audio experience with limited disruption or interference from the other occupants. In each sound zone, acoustic signals corresponding to a respective audio source are taken into consideration along with the contribution of audio source signals associated with a different sound zone, to minimize the effect different sound zones have on each other.</p><p id="p-0005" num="0004">When multiple occupants are listening to the same source, the acoustic energy from the combined sound zones is effectively doubled and may leak, or bleed, into adjacent sound zones causing a disturbance to listeners in the other sound zones. For example, if the listening environment is in a vehicle and there are four sound zones, two front seats and two rear seats. Four occupants may start out listening to acoustic signals from four separate sources. When occupants in the two rear seats decide to listen to the same source, the acoustic energy from the combined sound zones will cause a disturbance to the occupants in the two front seats.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0006" num="0005">A system and method for determining combined sources selected from a plurality of sources to be played at one or more sound zones in a listening environment. User input source selections are received at a processor to determine combined sources for the plurality of sound zones. Audio parameters for the sound zones having combined sources based on combined sources and individual inputs for audio settings from each sound zone are used to adjust audio parameters for each sound zone. The processor applies adjusted audio parameters to the sound zone algorithm for the sound zones having combined sources, and outputs audio based on the adjusted sound zone algorithm. A designated occupant may select the sound zones to which the adjusted sound zone algorithm is to be applied.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">DESCRIPTION OF DRAWINGS</heading><p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram of an example listening environment;</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram of a computing system;</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a block diagram of a system of the inventive subject matter; and</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flow diagram of a method of the inventive subject matter.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0011" num="0010">Elements and steps in the figures are illustrated for simplicity and clarity and have not necessarily been rendered according to any sequence. For example, steps that may be performed concurrently or in different order are illustrated in the figures to help to improve understanding of embodiments of the present disclosure.</p><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0012" num="0011">While various aspects of the present disclosure are described with reference to an audio system that may be dynamically adjusted according to a user input for a listening environment in a vehicle setting. However, the present disclosure is not limited to such embodiments, and additional modifications, applications, and embodiments may be implemented without departing from the present disclosure. In the figures, like reference numbers will be used to illustrate the same components. Those skilled in the art will recognize that the various components set forth herein may be altered without varying from the scope of the present disclosure.</p><p id="p-0013" num="0012">Many sounds, like navigation prompts and vehicle alerts are only relevant to the driver. Each occupant in the vehicle may want to control their own volume. Phone calls may be directed to one occupant, while everyone else continues to listen to media from their own source. For example, each occupant in the vehicle may select a sound source, reducing noise from surrounding occupants. This occurs without the use of headphones which keeps occupants in communication with each other, while still enjoying media from their own selected source. However, when multiple occupants are listening to the same source, acoustic energy from the combined sound zones is effectively doubled and may leak, or bleed, into adjacent sound zones causing a disturbance to occupants in the other sound zones. There is a need to prevent bleed from combined sound zones.</p><p id="p-0014" num="0013">Further, if one or more seats in the listening environment is unoccupied, any leakage present in unoccupied seats is of no concern. Therefore, unweighting priority of unoccupied seats prioritizes sound zone control for the occupied seats, resulting in improved separation, or isolation, of each occupied sound zone.</p><p id="p-0015" num="0014">The inventive subject matter normalizes any combination of media sources in terms of perceptual loudness. For example, when rear occupants are sharing a media source, the source level of the combined media source may be turned down, which includes reducing the gain, or level, of the rear passenger and driver side seat source. Additionally, other audio processing may occur, for example, equalization, dynamics/compression, or other types of digital signal processing, as needed to provide a consistent listening experience for each occupant in the listening environment.</p><p id="p-0016" num="0015">Further, using occupant detection, an audio processing system has knowledge of which seats or zones are physically occupied. When one or more seats are unoccupied, core sound algorithms can optimize a filter set to promote an improved listening experience for occupants, and disregarding sound zones that are unoccupied.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is an example listening environment <b>100</b> having an audio, or sound, processing system <b>102</b> that is configured to carry out instructions generated in response to user inputs from a plurality of sound zones. In the example shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the listening environment <b>100</b> is a vehicle <b>104</b> that includes a sound zone <b>106</b> coinciding with a driver occupant seat, a sound zone <b>108</b> coinciding with a front passenger occupant seat, a sound zone <b>110</b> coinciding with a driver-side rear occupant seat, and a sound zone <b>112</b>, coinciding with a passenger-side rear occupant seat.</p><p id="p-0018" num="0017">In the example shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the vehicle <b>102</b> has, for example, a front center speaker <b>114</b>, a driver-side front speaker <b>116</b>, a passenger-side front speaker <b>118</b>, a driver-side rear speaker <b>120</b> and a passenger-side rear speaker <b>122</b>. The number, location, type of speakers in <figref idref="DRAWINGS">FIG. <b>1</b></figref> is for example purposes only and other speaker sets may be used. For example, a subwoofer or other drivers. The vehicle may also include one or more sensors <b>124</b>, such as microphones, cameras, weight sensors, millimeter wave sensors to name just a few. The sensors <b>124</b> are strategically located throughout the vehicle interior and may be used, among other purposes, to detect occupants and their locations within the listening environment. While a particular example configuration is shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, other configurations may be used without departing from the scope of the inventive subject matter. For example, the listening environment may be a larger vehicle with multiple rows of rear seats, it may be another type of vehicle such as a boat, or it may be a room such as a living room or theater. The audio processing system <b>102</b> supports the use of multiple audio sources (not shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>), such as radios, CDs, DVDs, mobile devices, etc. The audio processing system <b>102</b> receives audio signals from the sources, processes the audio signals, and outputs the audio signals to one or more speakers in the listening environment.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram of a computing system <b>200</b> in the listening environment that may perform one or more of the methods described later herein. In the present example, the computing system is an in-vehicle computing system <b>200</b> that is part of a vehicle infotainment system configured to provide media content to occupants in each sound zone. The source of media content may be selected by each occupant in each sound zone. The infotainment system may include, or be coupled to, various vehicle systems and subsystems. The in-vehicle computing system <b>200</b> may include one or more processors, including an operating system processor <b>202</b>, an interface processor <b>204</b> that communicates with a vehicle control system <b>206</b>, including the audio processing system <b>102</b>.</p><p id="p-0020" num="0019">The in-vehicle computing system <b>200</b> may include volatile memory <b>208</b> and a non-volatile storage medium <b>210</b> to store data, such as instructions executable by processors <b>202</b>, <b>204</b>, <b>206</b>, and <b>102</b>. The in-vehicle computing system <b>200</b> may include a microphone <b>212</b> to receive voice commands from the occupants. The microphone <b>212</b> may also measure ambient noise for detecting occupants and determine their location in the vehicle. The sensor data collected by the microphone is used to determine whether audio from speakers of the vehicle is tuned in accordance with an acoustic environment for detecting occupants, their locations in the vehicle, and their selected source of media.</p><p id="p-0021" num="0020">The in-vehicle computing system <b>200</b> may include a speech processing unit <b>214</b> to process voice commands detected by the microphone <b>212</b> and determine the occupants' locations in the vehicle. The in-vehicle computing system <b>200</b> may also include one or more sensors <b>124</b>, such as cameras or weight sensors, to receive inputs from occupants and to detect occupants and determined their locations in the vehicle.</p><p id="p-0022" num="0021">The in-vehicle computing system <b>200</b> may also include a user interface <b>216</b>, that may include a display, or touch screen <b>218</b>, for receiving and communicating user settings. Additionally, or alternatively, external devices <b>220</b>, wired or wireless, may communicate with the in-vehicle computing system <b>200</b> as a means for occupants to input selections to the in-vehicle computing system <b>200</b>. External devices <b>220</b> may include, but are not limited to, a mobile device <b>222</b>, a blue-tooth device <b>224</b>, an external storage device <b>226</b> such as a USB drive, a wearable device, a tablet, etc. The external device <b>220</b> may function as a user interface and may also be used to detect occupants and their locations in the vehicle. Additionally, the external device <b>220</b> may be a source of media, to be discussed later herein.</p><p id="p-0023" num="0022">The in-vehicle computing system <b>200</b> is in communication with the vehicle control system <b>206</b> which includes, but is not limited to the audio system <b>102</b>, a climate control system, <b>228</b> and various vehicle controls such as steering <b>230</b>, braking <b>232</b> and lighting <b>234</b> to name a few. Control signals from the various vehicle control systems may also control audio output at one or more speakers of the vehicle's audio system to adjust audio output characteristics, such as volume, equalization, audio image (e.g., the configuration of the audio signals to produce audio output that appears to a user to originate from one or more defined locations), audio distribution among a plurality of speakers, and dynamics, including range compression and volume limiting. A source selection matrix is designed, such that, each media source is routed to the corresponding sound zone. A sound zone algorithm for each sound zone ensures the resulting acoustic signal corresponds to the audio source signal associated with the same sound zone, and the contribution of audio source signals associated with a different sound zone is minimized. The directivity of a loudspeaker associated with a specific sound zone is such that the resulting acoustic signal level generated by the speaker is higher than that in the other sound zones. This is typically accomplished by decoupling the individual sound zones using crosstalk cancellation methods.</p><p id="p-0024" num="0023">A vehicle occupant may use the user interface <b>216</b> to provide inputs to the vehicle audio system <b>102</b> thereby selecting a media source and adjusting sound preferences with respect to the sound zone of their location within the listening environment. Preferences include, but are not limited to, selecting/configuring a virtual sound space and selecting sound qualities for sound output in the sound zone. The audio processing system <b>102</b> carries out the instructions generated in response to inputs at the user interface to adjust audio parameters for sound output at the speakers. Sound output by the speakers is adjusted to meet the settings associated with the sound zone. This could also be done using seat-specific speakers with no crosstalk cancellation.</p><p id="p-0025" num="0024">When multiple people are listening to the same source, the audio may bleed into other sound zones. <figref idref="DRAWINGS">FIG. <b>3</b></figref> is a block diagram of a system <b>300</b> for minimizing bleed from sound zones having the same source into surrounding sound zones. A plurality of sources <b>302</b>(<b>1</b>) to <b>302</b>(<i>n</i>) are available for selection by any one of the occupants in any one of the sound zones in the listening environment. The sources are sources of media that are output by the audio system. The sources <b>302</b>(<b>1</b>)-<b>302</b>(<i>n</i>) may include, but are not limited to, a vehicle infotainment system, a radio, a CD player, a DVD player, a mobile device (i.e., a smart phone, a tablet, a laptop, portable media player, etc.), a telephone, a navigation device, a streaming device, a microphone, a USB drive, or other device wirelessly connected to the vehicle infotainment system to be played back by the audio system. Sources <b>302</b>(<b>1</b>)-<b>302</b>(<i>n</i>) communicate with the in-vehicle computing system by way of the interface processor and/or the external device interface.</p><p id="p-0026" num="0025">A source level normalization block <b>304</b> includes instructions and control signals to bring audio signals from the sources <b>302</b>(<b>1</b>) to <b>302</b>(<i>n</i>) to an equal perceptual loudness level. Occupant media selections <b>306</b> of selected sources <b>302</b>(<b>1</b>)-<b>302</b>(<i>n</i>) are provided to a source selection matrix <b>308</b>. Based on the occupant's selection <b>308</b> made at the user interface, the source selection matrix <b>308</b> routes each source to the corresponding sound zone.</p><p id="p-0027" num="0026">The occupant may input one or more settings <b>310</b>, such as a volume setting, dynamics, equalization, gain, and tone controls including bass, midrange and treble, for example. The occupants communicate preferences and selections for the settings by way of the user interface which may include the display of an in-vehicle infotainment system or a display of a mobile device, or other mobile device that is in communication with the in-vehicle computing system, by way of the interface processor and/or the external device interface.</p><p id="p-0028" num="0027">A user/source mapping compensation block <b>312</b> uses source selection information, user media selection information, and user volume selection information to determine whether multiple occupants have selected, and are listening to, the same source. Additionally, in one or more embodiments, occupants may be detected <b>314</b> to identify relevant sound zones for the purpose of mapping compensation and sound zone algorithm selection and this information may also be used by the user/source mapping compensation block <b>312</b> to be discussed later herein.</p><p id="p-0029" num="0028">The user/source mapping compensation block <b>312</b> adjusts the audio signal output for sound zones having the same media source selection to avoid bleeding into other sound zones. For example, a gain adjustment may be made to the audio signal being output at combined sound zones, combined meaning the occupants have selected the same media source to be played at their respective sound zone. Additionally, or alternatively, processing may be applied to adjust equalization, dynamics, compression, or other types of audio parameters, to prevent audio being played at combined sound zones from bleeding into other sound zones in the listening environment.</p><p id="p-0030" num="0029">For example, referring to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, occupants in the rear driver-side seat and the rear passenger-side seat may select the same source. The combined sound zones <b>110</b>, <b>112</b> in the rear part of the listening environment need to be minimized so the audio being played does not affect the audio being played from sources selected by occupants in the other two sound zones <b>106</b>, <b>108</b> at the front of the listening environment. Referring again to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the user and source mapping compensation block <b>312</b> adjusts audio parameters so that a level for combined zones is set to prevent bleeding into the other sound zones.</p><p id="p-0031" num="0030">A sound zone algorithm output block <b>316</b> uses the settings from the source mapping compensation block <b>312</b> to select and apply an algorithm for each sound zone that is output <b>318</b> at the speakers in the audio system. To select and apply the proper algorithm for each sound zone, the sound zone algorithm output block <b>316</b> also receives information from a filter selection algorithm matrix <b>320</b>. The filter selection algorithm matrix uses information from occupant detection <b>314</b> to identify sound zones that are occupied by listeners. The filter selection algorithm matrix <b>320</b> identifies and selects a filter set to be applied to each sound zone by the sound zone algorithm output block <b>316</b>.</p><p id="p-0032" num="0031">Occupant detection block <b>314</b> allows the filter selection algorithm block <b>320</b> to optimize algorithm selection when certain sound zones are unoccupied. Occupant detection block <b>314</b> uses information from vehicle sensors and/or user input devices to determine which sound zones are occupied. For example, sound zones that are occupied may be prioritized, while sound zones that are unoccupied may be disregarded, when filter selection algorithms are being determined at the filter selection algorithm matrix <b>320</b>. Additionally, occupant detection block <b>314</b> may also provide information about occupant locations to be used by the user/source mapping compensation block <b>312</b> for mapping occupant selections of a source to the appropriate sound zone.</p><p id="p-0033" num="0032">A sound zone algorithm output block <b>316</b> applies the selections from the filter selection algorithm matrix <b>320</b> and applies signals, filters, inverse filters, and transfer functions, that are adjusted by the source mapping compensation block <b>312</b> to the audio signal that is to be output <b>318</b> at loudspeakers <b>322</b>(<b>1</b>)-<b>322</b>(<i>n</i>) to compensate for combined sound zones, i.e., sound zones that have the same source, thereby preventing audio from the combined sound zones from bleeding into the other sound zones.</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flow diagram of a method for adjusting audio output in at least two sound zones having a same media source selected to prevent the audio from bleeding into other sound zones in a listening environment. The method is carried out by executing instructions with one or more processors in combination with hardware devices, such as storage devices, memory, interfaces, sensors, switches, actuators, etc. The storage devices have a non-transitory computer-readable storage medium, configured to execute the instructions stored on the non-transitory computer-readable storage medium. At <b>402</b>, receiving a user input indicative of selecting a source for a sound zone. The user input selecting the source is made by way of the user interface, which may be, for example, an in-vehicle display, a mobile device, or an external device.</p><p id="p-0035" num="0034">At <b>404</b>, the selected source is routed to the sound zone. Because there are a plurality of sources and a plurality of sound zones, at <b>406</b>, the selected sources are mapped to the sound zones to determine if there are any combined sources. Combined sources are defined to be sources that have been selected by occupants at one or more sound zones. The occupants in respective sound zones have selected the same source for playback, thereby combining the audio output being played back at sound zones for the combined sources.</p><p id="p-0036" num="0035">At <b>408</b>, audio parameters are adjusted, based on the mapped sound zones and combined sources, to reduce or eliminate bleed for other sound zones. Other sound zones are sound zones that do not have the same source for media playback selected. Adjustments to the audio parameters, for example, may include a basic gain adjustment to the audio output at the sound zones having combined sources, thereby reducing the audio being output at the combined sources from affecting audio output at the surrounding sound zones. Other adjustments to the audio parameters may be made, including but not limited to equalization, dynamics, compression, or other types of signal processing.</p><p id="p-0037" num="0036">In one or more embodiments, at <b>410</b> an occupant is detected in one or more sound zones. During <b>408</b>, if a sound zone does not have an occupant detected, the step of adjusting audio settings based on mapping may take this into consideration. Only sound zones that have an occupant detected will be considered and the step of adjusting audio settings will prioritize audio adjustments only for sound zones that are occupied.</p><p id="p-0038" num="0037">Information about occupants detected <b>410</b> in each sound zone is also received at <b>412</b>. At <b>412</b>, based on the adjusted audio settings and occupied sound zones, a filter algorithm is selected from a filter selection algorithm matrix. Knowledge of which sound zones are occupied allows for an optimized filter set to be selected. The selected filter set is applied <b>414</b> to the standard core algorithm for sound zones. This algorithm is typically known to be a MIMO control signal block. The standard sound zone algorithm is modified by the selected filter set to modify the algorithm as it applies to each sound zone. At <b>418</b> the adjusted algorithm is applied to occupied sound zones, and more particularly to occupied sound zones having combined sources.</p><p id="p-0039" num="0038">In one or more embodiments, one occupant may be designated with the capability to manually prioritize the sound zones outside of source selection by occupants. This designated occupant may be considered a VIP and is allowed to make selections, by way of the user interface, that augments the audio experience in a sound zone of their selection, even if their selections may be detrimental to an experience for listeners in other sound zones of the listening environment. The VIP may be assigned at any point and may be changed at any point. Assigning a VIP is optional. Therefore, in the event a VIP status is assigned to any one of the listener positions, VIP status is checked and the VIP audio settings are adjusted accordingly before the adjusted algorithm is applied in step <b>418</b>.</p><p id="p-0040" num="0039">In the foregoing specification, the present disclosure has been described with reference to specific exemplary embodiments. The specification and figures are illustrative, rather than restrictive, and modifications are intended to be included within the scope of the present disclosure. Accordingly, the scope of the present disclosure should be determined by the claims and their legal equivalents rather than merely by the examples described.</p><p id="p-0041" num="0040">For example, the steps recited in any method or process claims may be executed in any order, may be executed repeatedly, and are not limited to the specific order presented in the claims. Additionally, the components and/or elements recited in any apparatus claims may be assembled or otherwise operationally configured in a variety of permutations and are accordingly not limited to the specific configuration recited in the claims. Any method or process described may be carried out by executing instructions with one or more devices, such as a processor or controller, memory (including non-transitory), sensors, network interfaces, antennas, switches, actuators to name just a few examples.</p><p id="p-0042" num="0041">Benefits, other advantages, and solutions to problems have been described above for embodiments; however, any benefit, advantage, solution to problem or any element that may cause any particular benefit, advantage, or solution to occur or to become more pronounced are not to be construed as critical, required, or essential features or components of any or all the claims.</p><p id="p-0043" num="0042">The terms &#x201c;comprise&#x201d;, &#x201c;comprises&#x201d;, &#x201c;comprising&#x201d;, &#x201c;having&#x201d;, &#x201c;including&#x201d;, &#x201c;includes&#x201d; or any variation thereof, are intended to reference a non-exclusive inclusion, such that a process, method, article, composition, or apparatus that comprises a list of elements does not include only those elements recited but may also include other elements not expressly listed or inherent to such process, method, article, composition or apparatus. Other combinations and/or modifications of the above-described structures, arrangements, applications, proportions, elements, materials, or components used in the practice of the present disclosure, in addition to those not specifically recited, may be varied, or otherwise particularly adapted to specific environments, manufacturing specifications, design parameters or other operating requirements without departing from the general principles of the same.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A computing device for adjusting audio output in a listening environment, the computing device comprising:<claim-text>a user interface to select a source, from a plurality of sources, to be played at a sound zone in a plurality of sound zones;</claim-text><claim-text>a processor; and</claim-text><claim-text>a storage device storing instructions executable by the processor to:<claim-text>receive user input source selections;</claim-text><claim-text>map the source selections to respective sound zones;</claim-text><claim-text>determine combined sources;</claim-text><claim-text>adjust audio parameters for the sound zones having combined sources;</claim-text><claim-text>apply adjusted audio parameters to a sound zone algorithm for the sound zones having combined sources; and</claim-text><claim-text>output audio based on the adjusted sound zone algorithm.</claim-text></claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The computing device as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising means for detecting an occupant in each sound zone of the plurality of sound zones and wherein the storage device further comprises instructions executable by the processor to:<claim-text>select a filter set, from a filter selection matrix, for occupied sound zones; and</claim-text><claim-text>apply the selected filter set to the sound zone algorithm.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The computing device as claimed in <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the means for detecting an occupant in each sound zone is selected from the group consisting of: a microphone receiving voice commands from one or more occupants, a millimeter wave sensor, a weight sensor, a sensor for measuring ambient noise in the listening environment, a camera, and the user interface.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The computing device as claimed in <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the user interface is an external device.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The computing device as claimed in <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein a designated occupant selects a priority for the occupied sound zones and the selected filter is applied to the sound zone algorithm based on the priority selected by the designated occupant.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. A method for adjusting audio output in a listening environment, the method comprising:<claim-text>receiving, at a user interface, a source selected from a plurality of sources, to be played at one or more sound zones in the listening environment;</claim-text><claim-text>in response to the source selections, mapping each source to each sound zone;</claim-text><claim-text>determining combined sources;</claim-text><claim-text>adjusting audio parameters in sound zones having combined sources; and</claim-text><claim-text>transmitting the adjusted audio parameters to an audio system for playback at the sound zones having combined sources.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method as claimed in <claim-ref idref="CLM-00006">claim 6</claim-ref>, further comprising the steps of:<claim-text>detecting an occupant in each sound zone; and</claim-text><claim-text>wherein the step of determining combined sources further comprises combining sources for sound zones that the same source selected and an occupant detected therein.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method as claimed in <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the step of detecting an occupant in each sound zone further comprises receiving sensor data that is indicative of an occupant in the sound zone.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method as claimed in <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the step of receiving sensor data further comprises receiving sensor data from at least one of a microphone, a millimeter wave sensor, a weight sensor, a camera, and the user interface.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method as claimed in <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the sensor data received from the microphone is processed by a speech recognition device to receive voice commands to detect an occupant in the sound zone.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method as claimed in <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein the user interface is an external device.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method as claimed in <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the step of transmitting the adjusted audio parameters further comprises the step of a designated occupant selecting the sound zones to which the adjusted audio parameters will be transmitted.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. An in-vehicle computing system of a vehicle, comprising:<claim-text>an audio processor;</claim-text><claim-text>a user interface in communication with the audio processor, the user interface communicates a source selected from a plurality of sources for which audio is to be played at a sound zone in a plurality of sound zones in the vehicle;</claim-text><claim-text>a storage device storing instructions executable by the audio processor to:<claim-text>receive user input source selections;</claim-text><claim-text>map the source selections to respective sound zones;</claim-text><claim-text>determine combined sources;</claim-text><claim-text>adjust audio parameters for the sound zones having combined sources;</claim-text><claim-text>apply adjusted audio parameters to a sound zone algorithm for the sound zones having combined sources; and</claim-text><claim-text>output audio based on the adjusted sound zone algorithm.</claim-text></claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The in-vehicle computing system as claimed in <claim-ref idref="CLM-00013">claim 13</claim-ref>, further comprising:<claim-text>an occupant detection system for detecting an occupant in each sound zone of the plurality of sound zones; and</claim-text><claim-text>wherein the storage device further comprises instructions executable by the processor to:<claim-text>select a filter set, from a filter selection matrix, for occupied sound zones; and</claim-text><claim-text>apply the selected filter set to the sound zone algorithm.</claim-text></claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The in-vehicle computing device as claimed in <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the occupant detection system further comprises a microphone receiving voice commands from one or more occupants.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The in-vehicle computing device as claimed in <claim-ref idref="CLM-00015">claim 15</claim-ref>, further comprising a speech recognition device to process the voice commands from the one or more occupants and determine a respective sound zone for the one or more occupants.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The in-vehicle computing device as claimed in <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the occupant detection system further comprises a camera.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The in-vehicle computing device as claimed in <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the occupant detection system further comprises the user interface.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The in-vehicle computing device as claimed in <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the user interface is an external device.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The in-vehicle computing device as claimed in <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein a designated occupant selects a priority for occupied sound zones and the selected filter is applied to the sound zone algorithm based on the selected priority.</claim-text></claim></claims></us-patent-application>