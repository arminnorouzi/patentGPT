<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230000427A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230000427</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17797664</doc-number><date>20210205</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>JP</country><doc-number>2020-026166</doc-number><date>20200219</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>1</main-group><subgroup>24</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>25</main-group><subgroup>66</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>4803</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>1</main-group><subgroup>24</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>25</main-group><subgroup>66</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">ORAL FUNCTION VISUALIZATION SYSTEM, ORAL FUNCTION VISUALIZATION METHOD, AND RECORDING MEDIUM MEDIUM</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Panasonic Intellectual Property Management Co., Ltd.</orgname><address><city>Osaka</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>MATSUMURA</last-name><first-name>Yoshihiro</first-name><address><city>Osaka</city><country>JP</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>NAKAJIMA</last-name><first-name>Junko</first-name><address><city>Osaka</city><country>JP</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>ISHIMARU</last-name><first-name>Masashi</first-name><address><city>Osaka</city><country>JP</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>KIYOSAKI</last-name><first-name>Jakusei</first-name><address><city>Hyogo</city><country>JP</country></address></addressbook></inventor></inventors></us-parties><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/JP2021/004380</doc-number><date>20210205</date></document-id><us-371c12-date><date>20220804</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">An oral function visualization system includes: an outputter that outputs information for prompting a user to utter a predetermined voice; an obtainer that obtains an uttered voice of the user uttered in accordance with the output; an analyzer that analyzes the uttered voice obtained by the obtainer; and an estimator that estimates a state of oral organs of the user from a result of analysis of the uttered voice by the analyzer. The outputter outputs, based on the state of the oral organs of the user estimated by the estimator, information for the user to achieve a state of the oral organs suitable for utterance of the predetermined voice.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="117.01mm" wi="97.37mm" file="US20230000427A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="214.04mm" wi="109.81mm" orientation="landscape" file="US20230000427A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="163.66mm" wi="132.50mm" orientation="landscape" file="US20230000427A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="152.91mm" wi="127.59mm" file="US20230000427A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="175.94mm" wi="99.40mm" file="US20230000427A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="168.15mm" wi="118.03mm" file="US20230000427A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="205.06mm" wi="110.49mm" orientation="landscape" file="US20230000427A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="201.08mm" wi="118.70mm" orientation="landscape" file="US20230000427A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="198.97mm" wi="107.10mm" orientation="landscape" file="US20230000427A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="190.33mm" wi="100.75mm" orientation="landscape" file="US20230000427A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="183.30mm" wi="100.84mm" file="US20230000427A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="175.34mm" wi="132.16mm" orientation="landscape" file="US20230000427A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="197.10mm" wi="132.16mm" orientation="landscape" file="US20230000427A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE OF RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application is the U.S. National Phase under 35 U.S.C. &#xa7; 371 of International Patent Application No. PCT/JP2021/004380, filed on Feb. 5, 2021, which in turn claims the benefit of Japanese Application No. 2020-026166, filed on Feb. 19, 2020, the entire disclosures of which Applications are incorporated by reference herein.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">The present disclosure relates to an oral function visualization system and an oral function visualization method.</p><heading id="h-0003" level="1">BACKGROUND ART</heading><p id="p-0004" num="0003">Patakala exercises etc. have been used for the elderly and others having declining oral functions to train them to open and close their mouth or to practice pronunciation that strengthens their swallowing function. Patent Literature (PTL) 1 discloses an augmented reality system etc. that utilize reflections, which displays a view of a user's face and the like on a display.</p><heading id="h-0004" level="1">CITATION LIST</heading><heading id="h-0005" level="1">Patent Literature</heading><p id="p-0005" num="0004">[PTL 1] Japanese Unexamined Patent Application Publication (Translation of PCT Application) No. 2019-511067</p><heading id="h-0006" level="1">SUMMARY OF INVENTION</heading><heading id="h-0007" level="1">Solution to Problem</heading><p id="p-0006" num="0005">An oral function visualization system according to an aspect of the present disclosure is an oral function visualization system including: an outputter that outputs information for prompting a user to utter a predetermined voice; an obtainer that obtains an uttered voice of the user uttered in accordance with the output; an analyzer that analyzes the uttered voice obtained by the obtainer; and an estimator that estimates a state of oral organs of the user from a result of analysis of the uttered voice by the analyzer, wherein the outputter further outputs, based on the state of the oral organs of the user estimated by the estimator, information for the user to achieve a state of the oral organs suitable for utterance of the predetermined voice.</p><p id="p-0007" num="0006">An oral function visualization method according to an aspect of the present disclosure is an oral function visualization method including: outputting information for prompting a user to utter a predetermined voice; obtaining an uttered voice of the user uttered in accordance with the outputting; analyzing the uttered voice obtained in the obtaining; estimating a state of oral organs of the user from a result of analysis of the uttered voice in the analyzing; and outputting, based on the state of the oral organs of the user estimated in the estimating, information for the user to achieve a state of the oral organs suitable for utterance of the predetermined voice.</p><p id="p-0008" num="0007">A recording medium according to an aspect of the present disclosure is a non-transitory computer-readable recording medium having recorded thereon a program for causing a computer to execute an oral function visualization method including: outputting information for prompting a user to utter a predetermined voice; obtaining an uttered voice of the user uttered in accordance with the outputting; analyzing the uttered voice obtained in the obtaining; estimating a state of oral organs of the user from a result of analysis of the uttered voice in the analyzing; and outputting, based on the state of the oral organs of the user estimated in the estimating, information for the user to achieve a state of the oral organs suitable for utterance of the predetermined voice.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0008" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram illustrating feature amounts obtained from a voice uttered by a human.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a diagram illustrating a configuration of the inside of the oral cavity of a human.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a block diagram illustrating a configuration of an oral function visualization system according to an embodiment of the present disclosure.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flowchart of operations of the oral function visualization system according to the embodiment of the present disclosure.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram illustrating the relationship between formant vowels and the state of the inside of the oral cavity.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a diagram illustrating an example of words used for training on oral functions and an example of waveforms of pronunciations of the words.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a diagram illustrating an example of the states of the inside of the oral cavity when particular consonants are pronounced.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a diagram illustrating an example of waveforms of pronunciations of words used for training on oral functions.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a diagram illustrating an example of the states of the inside of the oral cavity when particular consonants are pronounced.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a diagram illustrating an example of output of the oral function visualization system according to the embodiment of the present disclosure.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a diagram illustrating an example of data that associates voice feature amounts and image data items used by the oral function visualization system according to the embodiment of the present disclosure.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a table showing the contents of video display corresponding to the eating and swallowing process according to the embodiment of the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0009" level="1">DESCRIPTION OF EMBODIMENT</heading><p id="p-0021" num="0020">Hereinafter, an embodiment is described with reference to the drawings. Note that the following embodiment describes a general or specific example. The numerical values, shapes, materials, constituent elements, the arrangement and connection of the constituent elements, etc., illustrated in the following embodiment are mere examples, and are therefore not intended to limit the present disclosure. Also, among the constituent elements in the following embodiment, constituent elements not recited in any one of the independent claims will be described as optional constituent elements.</p><p id="p-0022" num="0021">Note that the drawings are represented schematically and are not necessarily precise illustrations. Furthermore, in the figures, the same reference signs are given to essentially the same constituent elements, and redundant descriptions may be omitted or simplified.</p><heading id="h-0010" level="1">Embodiment 1</heading><heading id="h-0011" level="2">[Voice Feature Amounts and Configuration of Inside of Oral Cavity]</heading><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram illustrating feature amounts obtained from a voice. Feature amounts obtained from the voice are a first formant frequency, a second formant frequency, a sound pressure difference, and a speaking speed. Plural peaks are identified in data that is obtained by converting voice data into frequency. The frequency of the peak having the lowest frequency among the plural peaks is first formant frequency F<b>1</b>. The frequency of the peak having the next lowest frequency after first formant frequency F<b>1</b> is second formant frequency F<b>2</b>. The frequency of the peak having the next lowest frequency after second formant frequency F<b>2</b> is third formant frequency F<b>3</b>. Each frequency is obtained by extracting a vowel part of an uttered voice by a known method and calculating the spectrum of the vowel part through data conversion of voice data of the extracted vowel part into the amplitude with respect to the frequency.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates an example of the first formant frequency and the second formant frequency. <figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates an example of the case where a phrase &#x201c;e wo ka ku ko to ni ki me ta&#x201d; (I decided to draw a picture) is pronounced.</p><p id="p-0025" num="0024">The first formant frequency is the peak frequency having the smallest amplitude counted from the low-frequency side of human voice, and is known to easily reflect voice features influenced by tongue movement (especially up-and-down movement). In addition, the first formant frequency is also known to easily reflect voice features influenced by the opening of the jaw.</p><p id="p-0026" num="0025">The second formant frequency is the peak frequency having the second amplitude counted from the low-frequency side of human voice, and is known to easily reflect the influence of the position of the tongue (especially the front-back position) among the resonances produced by the vocal cord sound source in the vocal tract, the oral cavity such as lips and tongue, and nasal cavity, etc. In addition, for example, since it is not possible to speak correctly without teeth, the occlusal state of the teeth (a total number of teeth) in the oral preparatory phase is considered to influence the second formant frequency. Also, for example, since it is not possible to speak correctly with low saliva, the saliva secretion function in the oral preparatory phase is considered to influence the second formant frequency. The motor function of the tongue, the saliva secretion function, or the occlusal state of the teeth (a total number of teeth) may be calculated from either a feature amount obtained from the first formant frequency or a feature amount obtained from the second formant frequency.</p><p id="p-0027" num="0026">The second formant frequency mainly represents back-and-forth movement of the tongue during pronunciation. <figref idref="DRAWINGS">FIG. <b>1</b></figref> also illustrates an example of a sound pressure difference and a speaking speed. In pronunciation, the sound pressure difference mainly represents the strength of tongue movement. The speaking speed represents tongue dexterity.</p><p id="p-0028" num="0027">In the graph illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, changes in sound pressure corresponding to &#x201c;e&#x201d;, &#x201c;wo&#x201d;, &#x201c;ka&#x201d;, &#x201c;ku&#x201d;, &#x201c;ko&#x201d;, &#x201c;to&#x201d;, &#x201c;ni&#x201d;, &#x201c;ki&#x201d;, &#x201c;me&#x201d;, and &#x201c;ta&#x201d; are identified. The oral function visualization system described below obtains the data illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref> as voice data representing the user's pronunciation. Using a known method, for example, the oral function visualization system calculates the sound pressures of &#x201c;t&#x201d; and &#x201c;o&#x201d; in &#x201c;to&#x201d; and the sound pressures of &#x201c;t&#x201d; and &#x201c;a&#x201d; in &#x201c;ta&#x201d; included in the voice data illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. Oral function visualization system <b>10</b> also calculates the sound pressures of &#x201c;t&#x201d; and &#x201c;o&#x201d; in &#x201c;to&#x201d; as feature amounts.</p><p id="p-0029" num="0028">In this way, the oral function visualization system extracts various feature amounts from the voice pronounced by the user.</p><p id="p-0030" num="0029">Next, the following describes a configuration of the inside of the oral cavity used when oral function visualization system <b>10</b> reproduces the inside of the oral cavity of user <b>2</b>. <figref idref="DRAWINGS">FIG. <b>2</b></figref> is a diagram illustrating a configuration of the inside of the oral cavity of a human. As illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the oral cavity includes, in order of proximity to the outside, the upper lip, lower lip, teeth, gums, and tip of the tongue. The tip of the tongue is followed by the blade of the tongue, front of the tongue, back of the tongue, and root of the tongue toward the back of the oral cavity. Also, the gums are followed by the hard palate gum, gum hard palate, hard palate, soft palate, and uvula. Oral function visualization system <b>10</b> models each of the parts enumerated here, analyzes user <b>2</b>'s pronunciation, and reproduces the position of each part. The reproduction need not be performed for all the parts inside the oral cavity, and may be performed for some of the parts inside the oral cavity.</p><heading id="h-0012" level="2">[Configuration of Oral Function Visualization System]</heading><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a block diagram illustrating a configuration of oral function visualization system <b>10</b> according to the embodiment of the present disclosure. Oral function visualization system <b>10</b> includes outputter <b>11</b>, obtainer <b>12</b>, controller <b>13</b>, storage <b>14</b>, analyzer <b>15</b>, and estimator <b>16</b>.</p><p id="p-0032" num="0031">Outputter <b>11</b> outputs, to a screen, data such as the voice feature amounts of user <b>2</b>'s pronunciation analyzed by analyzer <b>15</b> or a reproduction image etc. of the inside of the oral cavity of user <b>2</b> estimated by estimator <b>16</b>. Outputter <b>11</b> is implemented by a terminal, such as a display, capable of displaying an image on a screen, and a processor, microcomputer, or a dedicated circuit. The terminal may be a tablet terminal or a smartphone. The terminal may be a printer or the like in the case where oral function visualization system <b>10</b> outputs, for example, a state of the oral cavity of the user on paper. In addition to the image display function, outputter <b>11</b> may also have a function to output a voice realized by a loudspeaker or the like. Further, outputter <b>11</b> generates an image showing a state of the oral cavity of user <b>2</b> estimated by estimator <b>16</b>. Outputter <b>11</b> outputs information for user <b>2</b> to achieve a state of the oral organs suitable for utterance of a predetermined voice. The information may be, for example, words or the like that instruct how to move the tongue or lips, etc. Alternatively, outputter <b>11</b> may generate information that prompts the user to do training to achieve a state of the oral organs suitable for utterance of a predetermined voice.</p><p id="p-0033" num="0032">Obtainer <b>12</b> obtains voice data that is obtained by a microphone or the like collecting, in a non-contact manner, a voice uttered by user <b>2</b>. The voice is a voice of user <b>2</b> who has uttered a predetermined syllable or a predetermined sentence or word. Obtainer <b>12</b> may further obtain personal information about user <b>2</b>. For example, the personal information is information entered into a mobile terminal or the like, and is, for example, age, weight, height, gender, body mass index (BMI), dental information (e.g., a total number of teeth, whether or not a denture is used, the position of occlusal support, etc.), serum albumin level, or food intake rate. Obtainer <b>12</b> transmits the obtained data such as the voice to analyzer <b>15</b> included in controller <b>13</b>. Obtainer <b>12</b> is, for example, a communication interface for wired or wireless communication.</p><p id="p-0034" num="0033">Controller <b>13</b> includes outputter <b>11</b>, analyzer <b>15</b>, and estimator <b>16</b>. Controller <b>13</b> is implemented specifically by a processor, a microcomputer, or a dedicated circuit.</p><p id="p-0035" num="0034">Storage <b>14</b> stores information indicating a state of the oral organs suitable for user <b>2</b> to utter a predetermined voice. Storage <b>14</b> may also store, for example, the voice data of the voice uttered by user <b>2</b> obtained by obtainer <b>12</b>, data such as the voice feature amounts of user <b>2</b>'s pronunciation analyzed by analyzer <b>15</b>, the personal information about user <b>2</b>, and programs executed by outputter <b>11</b>, obtainer <b>12</b>, controller <b>13</b>, analyzer <b>15</b>, and estimator <b>16</b>. Storage <b>14</b> is implemented by, for example, read-only memory (ROM), random-access memory (RAM), semiconductor memory, or hard disk drive (HDD).</p><p id="p-0036" num="0035">Analyzer <b>15</b> analyzes the uttered voice of user <b>2</b> obtained by obtainer <b>12</b>. Analyzer <b>15</b> may analyze, from the uttered voice of user <b>2</b>, the voice feature amounts such as the first formant frequency, the second formant frequency, the sound pressure difference, and the speaking speed as described with reference to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, for example. Analyzer <b>15</b> is implemented specifically by a processor, a microcomputer, or a dedicated circuit.</p><p id="p-0037" num="0036">Estimator <b>16</b> estimates a state of the oral organs of user <b>2</b> from the result of the analysis of the uttered voice by analyzer <b>15</b>. Estimator <b>16</b> estimates, from the uttered voice of user <b>2</b>, a state of the oral cavity of user <b>2</b> based on, for example, the parts inside the oral cavity described with reference to <figref idref="DRAWINGS">FIG. <b>2</b></figref>. Specifically, estimator <b>16</b> may estimate, for example, the positions of the blade of the tongue, front of the tongue, and back of the tongue in the oral cavity or the positional relationship between the upper lip and the lower lip. Estimator <b>16</b> is implemented specifically by a processor, a microcomputer, or a dedicated circuit.</p><heading id="h-0013" level="2">[Processing Procedure of Oral Function Visualization System]</heading><p id="p-0038" num="0037">Next, the processing performed by oral function visualization system <b>10</b> will be described.</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flowchart of operations of the oral function visualization system according to the embodiment of the present disclosure.</p><p id="p-0040" num="0039">First, outputter <b>11</b> outputs, to the screen, information for prompting user <b>2</b> to utter a predetermined voice (step S<b>100</b>). For example, outputter <b>11</b> may output an image showing an example sentence or a word for user <b>2</b> to pronounce, or may output a voice indicating an example sentence or a word for user <b>2</b> to pronounce. For example, outputter <b>11</b> may output, to the screen, a character string indicating an example sentence &#x201c;e wo ka ku ko to ni ki me to (I decided to draw a picture)&#x201d;, a character string indicating syllables such as &#x201c;kala&#x201d; and &#x201c;sala&#x201d;, or a character string indicating a word such as &#x201c;ippai&#x201d;, &#x201c;ikkai&#x201d;, or &#x201c;ittai&#x201d;.</p><p id="p-0041" num="0040">Note that storage <b>14</b> may store in advance information indicating a state of the oral organs. For example, storage <b>14</b> stores oral cavity state data items which are images each showing a state of the oral organs that is associated with a predetermined voice feature amount. Image data showing a state of the oral organs that is associated with a predetermined voice feature amount will be described later.</p><p id="p-0042" num="0041">Next, obtainer <b>12</b> obtains an uttered voice of user <b>2</b> (step S<b>101</b>). Obtainer <b>12</b> obtains an uttered voice of user <b>2</b> through a microphone, for example.</p><p id="p-0043" num="0042">Then, analyzer <b>15</b> analyzes the uttered voice of user <b>2</b> obtained by obtainer <b>12</b> (step S<b>102</b>). Analyzer <b>15</b> analyzes the uttered voice of user <b>2</b> and extracts voice feature amounts. For example, analyzer <b>15</b> analyzes the uttered voice of user <b>2</b> to extract the first formant frequency, the second formant frequency, the sound pressure difference, and the like as voice feature amounts.</p><p id="p-0044" num="0043">Next, estimator <b>16</b> estimates a state of oral organs of user <b>2</b> from the voice feature amounts of the uttered voice of user <b>2</b> analyzed by analyzer <b>15</b> (step S<b>103</b>). Estimator <b>16</b> estimates, for example, the open or closed state of the mouth of user <b>2</b> or the positions of the blade of the tongue, front of the tongue, back of the tongue, and root of the tongue of user <b>2</b>.</p><p id="p-0045" num="0044">Outputter <b>11</b> then outputs, to a screen or loudspeaker, etc., information for user <b>2</b> to achieve a state of the oral organs suitable for utterance of a predetermined voice (Step S<b>104</b>). Here, a predetermined voice is a voice indicating a word or the like that outputter <b>11</b> presented in step S<b>100</b> for user <b>2</b> to pronounce. Outputter <b>11</b> may output, as the information for user <b>2</b> to achieve a state of the oral organs suitable for utterance of a predetermined voice, a diagram illustrating the state of the oral organs of user <b>2</b> estimated by estimator <b>16</b> and a diagram illustrating a state of the oral organs suitable for utterance of a predetermined voice. Further, outputter <b>11</b> may output, as the information for user <b>2</b> to achieve a state of the oral organs suitable for utterance of a predetermined voice, a word indicating the state of the oral organs of user <b>2</b> estimated by estimator <b>16</b> and a word indicating a state of the oral organs suitable for utterance of a predetermined voice, in the form of, for example, a voice or a character string. At that time, outputter <b>11</b> may output a result of comparison between the state of the oral organs of user <b>2</b> estimated by estimator <b>16</b> and the state of the oral organs suitable for utterance of a predetermined voice. Also, as the information for user <b>2</b> to achieve a state of the oral organs suitable for utterance of a predetermined voice, outputter <b>11</b> may output advice for achieving a state of the oral organs suitable for utterance of a predetermined voice, in the form of, for example, a voice, a character string, or a diagram.</p><p id="p-0046" num="0000">[Relationship between Formant Vowels and Oral Cavity]</p><p id="p-0047" num="0045"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram illustrating the relationship between formant vowels and the state of the inside of the oral cavity. As illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the first formant frequency (denoted by F<sub>1</sub>) is relevant to the open or closed state of the mouth, and the second formant frequency (denoted by F<sub>2</sub>) is relevant to the position of the tongue in the oral cavity. Specifically, when the mouth is closed and the tongue is in the front position, the pronunciation is &#x201c;i&#x201d;. When the mouth is half-closed and the tongue is in the front position, the pronunciation is &#x201c;e&#x201d;. Next, when the mouth is open and the tongue is in the middle position, the pronunciation is &#x201c;a&#x201d;. When the mouth is closed and the tongue is in the back position, the pronunciation is &#x201c;u&#x201d;. When the mouth is half-closed and the tongue is in the back position, the pronunciation is &#x201c;o&#x201d;.</p><p id="p-0048" num="0046">When the tongue is in the front position, the pronunciations of &#x201c;i&#x201d;, &#x201c;e&#x201d;, and &#x201c;a&#x201d; have high first formant frequencies in the stated order. When the tongue is in the back position, the pronunciation of &#x201c;o&#x201d; has a first formant frequency higher than that of the pronunciation of &#x201c;u&#x201d;.</p><p id="p-0049" num="0047">When the mouth is closed, the pronunciation of &#x201c;i&#x201d; has a second formant frequency higher than that of the pronunciation of &#x201c;u&#x201d;. When the mouth is half-closed, the pronunciation of &#x201c;e&#x201d; has a second formant frequency higher than that of the pronunciation of &#x201c;o&#x201d;.</p><heading id="h-0014" level="2">[Example of Pronunciation Analysis and Visualization]</heading><p id="p-0050" num="0048"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a diagram illustrating an example of words used for training on oral functions and an example of waveforms of pronunciations of the words. As illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, oral function visualization system <b>10</b> causes user <b>2</b> to pronounce words &#x201c;ippai&#x201d;, &#x201c;ittai&#x201d;, and &#x201c;ikkai&#x201d;, for example. As illustrated in portions (a), (b), and (c) of <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the areas denoted by &#x201c;Diff&#x201d; in the graphs indicate that the lips are closed or the tongue is obstructed. Also, as illustrated in portions (a), (b), and (c) of <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the areas indicated by &#x201c;Time&#x201d; in the graphs indicate the closure period of the lips or the obstruction period of the tongue. In each graph, when the waveform is above the reference line, the intensity of the voice is determined to be a certain level or higher.</p><p id="p-0051" num="0049"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a diagram illustrating an example of the states of the inside of the oral cavity when particular consonants are pronounced. First, the case where oral function visualization system <b>10</b> causes user <b>2</b> to pronounce the word &#x201c;ippai&#x201d; will be described. The word &#x201c;ippai&#x201d; contains the consonant &#x201c;p&#x201d;. Thus, as illustrated in portion (a) of <figref idref="DRAWINGS">FIG. <b>7</b></figref>, oral function visualization system <b>10</b> analyzes whether user <b>2</b> is being able to pronounce the word as if user <b>2</b> were bursting air after closing the lips completely. When oral function visualization system <b>10</b> causes user <b>2</b> to pronounce the word &#x201c;ippai&#x201d;, oral function visualization system <b>10</b> may visualize the state of the oral organs of user <b>2</b> by emphasizing the movement of the lips in particular.</p><p id="p-0052" num="0050">Next, the case where oral function visualization system <b>10</b> causes user <b>2</b> to pronounce the word &#x201c;ittai&#x201d; will be described. The word &#x201c;ittai&#x201d; contains the consonant &#x201c;t&#x201d;. Thus, as illustrated in portion (b) of <figref idref="DRAWINGS">FIG. <b>7</b></figref>, oral function visualization system <b>10</b> analyzes whether user <b>2</b> is being able to pronounce the word by having the tongue completely touch the gums of the upper part of the oral cavity and then releasing the tongue. When oral function visualization system <b>10</b> causes user <b>2</b> to pronounce the word &#x201c;ittai&#x201d;, oral function visualization system <b>10</b> may visualize the state of the oral organs of user <b>2</b> by emphasizing the movement of the tip of the tongue in particular.</p><p id="p-0053" num="0051">Next, the case where oral function visualization system <b>10</b> causes user <b>2</b> to pronounce the word &#x201c;ikkai&#x201d; will be described. The word &#x201c;ikkai&#x201d; contains the consonant &#x201c;k&#x201d;. Thus, as illustrated in portion (c) of <figref idref="DRAWINGS">FIG. <b>7</b></figref>, oral function visualization system <b>10</b> analyzes whether user <b>2</b> is being able to pronounce the word by having the back of the tongue completely touch the hard palate or the soft palate at the upper part of the oral cavity to close the airway and then opening the airway. When oral function visualization system <b>10</b> causes user <b>2</b> to pronounce the word &#x201c;ikkai&#x201d;, oral function visualization system <b>10</b> may visualize the state of the oral organs of user <b>2</b> by emphasizing the movement of the back of the tongue in particular.</p><p id="p-0054" num="0052"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a diagram illustrating an example of waveforms of pronunciations of words used for training on oral functions. The following describes the case where oral function visualization system <b>10</b> causes user <b>2</b> to pronounce the words &#x201c;kala&#x201d; and &#x201c;sala&#x201d;. As illustrated in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, a waveform in the form of two-humped mountains is observed when the word &#x201c;kala&#x201d; or &#x201c;sala&#x201d; is pronounced. When the humps of the observed waveform are smooth, it is interpreted that &#x201c;ka&#x201d; or &#x201c;sa&#x201d; is not pronounced well. If a waveform higher than the reference line is observed, it is interpreted that the voice volume is a certain level or higher.</p><p id="p-0055" num="0053">For example, when user <b>2</b> pronounces the word &#x201c;kala&#x201d;, the states of the oral organs as illustrated in portions (a) and (b) of <figref idref="DRAWINGS">FIG. <b>9</b></figref> are shown. Since the pronunciation of &#x201c;ka&#x201d; needs the back of the tongue in contact with the hard palate or the soft palate at the upper part of the oral cavity, a diagram in which the back of the tongue is in contact with the hard palate or the soft palate at the upper part of the oral cavity is shown if user <b>2</b> is being able to pronounce &#x201c;ka&#x201d; perfectly. On the other hand, if user <b>2</b> is not being able to pronounce &#x201c;ka&#x201d; perfectly, a diagram is shown in which the back of the tongue is not in contact with the hard palate or the soft palate at the upper part of the oral cavity and there is a gap between the tongue and the hard palate or the soft palate.</p><p id="p-0056" num="0054">For example, when user <b>2</b> pronounces the word &#x201c;sala&#x201d;, the states of the oral organs as illustrated in portions (c) and (d) of <figref idref="DRAWINGS">FIG. <b>9</b></figref> are shown. Since the pronunciation of &#x201c;sa&#x201d; needs the tip of the tongue in contact with the gum at the upper part of the oral cavity, a diagram in which the tip of the tongue is in contact with the gum at the upper part of the oral cavity is shown if user <b>2</b> is being able to pronounce &#x201c;sa&#x201d; properly. On the other hand, if user <b>2</b> is not being able to pronounce &#x201c;sa&#x201d; perfectly, a diagram is shown in which the tip of the tongue is not in contact with the gum at the upper part of the oral cavity and there is a gap between the tongue and the gum. Also, the pronunciation of &#x201c;sa&#x201d; needs the lips to be open, and thus a diagram in which there is a gap between the upper and lower lips is shown.</p><heading id="h-0015" level="2">[Example of Output of Oral Function Visualization System]</heading><p id="p-0057" num="0055"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a diagram illustrating an example of output of the oral function visualization system according to the embodiment of the present disclosure. The state of the oral organs of user <b>2</b> estimated from the voice uttered by user <b>2</b> and a state of the oral organs ideal for pronouncing the word that oral function visualization system <b>10</b> has caused user <b>2</b> to pronounce are illustrated. These two states may be illustrated in one diagram or two separate diagrams. The diagrams output by oral function visualization system <b>10</b> may include graphics such as an arrow indicating a direction or the like for improving the positions of the oral organs of user <b>2</b>. In addition, oral function visualization system <b>10</b> may display, for example, a sentence indicating a direction or the like for improving the positions of the oral organs of user <b>2</b>, or may display, for example, wording that prompts user <b>2</b> to further practice the pronunciation, on the screen.</p><p id="p-0058" num="0056">For example, after causing user <b>2</b> to pronounce a specific word, oral function visualization system <b>10</b> estimates a state of the oral organs of user <b>2</b> corresponding to a syllable uttered by user <b>2</b> and displays the estimated state using a diagram or words. Oral function visualization system <b>10</b> estimates the open or closed state of the mouth of user <b>2</b> or the positions of the blade of the tongue, front of the tongue, back of the tongue, and root of the tongue of user <b>2</b>, and displays the estimated state or positions on the screen using a diagram or words. The estimation may be performed for each syllable uttered by user <b>2</b>. Oral function visualization system <b>10</b> may also display a state of the oral organs ideal for pronouncing the word uttered by user <b>2</b>, on the screen using a diagram or words. Specifically, oral function visualization system <b>10</b> displays, on the screen, a diagram illustrating a state in which the blade of the tongue is oriented toward the gums but is not in contact with the gums, as the state of the oral organs of user <b>2</b> corresponding to the syllable uttered by user <b>2</b>. Oral function visualization system <b>10</b> then displays a diagram illustrating a state in which the blade of the tongue is closer to the gums, as the state of the oral organs ideal for pronouncing the word uttered by user <b>2</b>. An upward arrow prompting the user to move the entire tongue upward may be illustrated at the same time. In addition, a sentence &#x201c;Raise your tongue a little more.&#x201d; indicating a direction or the like for improving the positions of the oral organs of user <b>2</b> may be displayed on the screen. Also, a sentence &#x201c;Let's practice one more time.&#x201d; may be displayed on the screen as a sentence or the like that prompts user <b>2</b> to further practice the pronunciation. Note that such words or the like displayed on the screen may be read aloud.</p><p id="p-0059" num="0057">As described above, oral function visualization system <b>10</b> outputs a diagram or words that enable user <b>2</b> to achieve the state of the oral organs ideal for pronunciation.</p><heading id="h-0016" level="2">[Example of Image Data Used by Oral Function Visualization System]</heading><p id="p-0060" num="0058">Next, the following describes data used for oral function visualization system <b>10</b> to estimate and visualize a state of the oral organs of user <b>2</b>. <figref idref="DRAWINGS">FIG. <b>11</b></figref> is a diagram illustrating an example of data that associates voice feature amounts and image data items used by the oral function visualization system according to the embodiment of the present disclosure.</p><p id="p-0061" num="0059">Oral function visualization system <b>10</b> stores, in storage <b>14</b>, oral cavity state data items which are images each showing a state of the oral organs that is associated with a predetermined voice feature amount. For example, image A is stored in storage <b>14</b> as a data item corresponding to the voice &#x201c;a&#x201d; having a first formant frequency of 768 Hz, a second formant frequency of 1306 Hz, and a third formant frequency of 2552 Hz. Here, stored in storage <b>14</b> are image data items which correspond to, among sounds classified into &#x201c;a&#x201d;, sounds resulting from various combinations of the first formant frequency, the second formant frequency, and the third formant frequency. The same is true for other types of vowels and consonants. Although the first formant frequency, the second formant frequency, and the third formant frequency are employed here as voice feature amounts, other types of voice feature amounts may be used.</p><p id="p-0062" num="0060">Oral function visualization system <b>10</b> then outputs, from outputter <b>11</b>, oral cavity state data items each corresponding to a voice feature amount obtained as a result of analysis of the uttered voice by analyzer <b>15</b>. Oral function visualization system <b>10</b> selects and outputs an oral cavity state data item (e.g., an image data item) corresponding to voice feature amounts closest to the voice feature amounts of the uttered voice. Oral function visualization system <b>10</b> may continuously display plural oral cavity state data items to output them as video.</p><heading id="h-0017" level="2">[Application in Evaluation of Eating and Swallowing]</heading><p id="p-0063" num="0061">Oral function visualization system <b>10</b> can be applied to, for example, evaluation of eating and swallowing and improvement of the eating and swallowing functions.</p><p id="p-0064" num="0062">First, the eating and swallowing process will be described. The eating and swallowing functions are functions of the human body necessary to accomplish a series of processes from the recognition of food to the intake of the food into the mouth and eventually the stomach. The eating and swallowing functions have five phases: the pre-oral phase, the oral preparatory phase, the oral transit phase, the pharyngeal phase, and the esophageal phase.</p><p id="p-0065" num="0063">In the pre-oral phase (also called the cognitive phase) of eating and swallowing, the shape, hardness, temperature, etc., of food are determined. The eating and swallowing function in the pre-oral phase is, for example, the visual recognition function of the eyes. In the pre-oral phase, the nature and state of the food are recognized, and preparations necessary for eating, such as the eating style, saliva secretion, and posture, are made.</p><p id="p-0066" num="0064">In the oral preparatory phase (also called the mastication phase) of eating and swallowing, food taken into the oral cavity is chewed and ground (i.e., masticated) by teeth, and then the masticated food is mixed with saliva by the tongue to form a bolus. The eating and swallowing functions in the oral preparatory phase include, for example, the motor function of facial muscles (e.g., lip muscles and cheek muscles) to take food into the oral cavity without spilling it, the cognitive function of the tongue to recognize the taste and hardness of food, the motor function of the tongue to press food against the teeth and to mix small pieces of food with saliva to form a bolus, the occlusion of the teeth to chew and grind food, the motor function of the cheeks to prevent food from getting between the teeth and cheeks, the motor function (mastication function) of the masticatory muscles, that is, a general term for the muscles for mastication (e.g., the masseter muscle and temporal muscle), and the saliva secretion function to secrete saliva for mixing with small pieces of food. The mastication function is influenced by, for example, the occlusal state of the teeth, the motor function of the masticatory muscles, and the function of the tongue. With these eating and swallowing functions in the oral preparatory phase, the bolus is given properties (size, lump, viscosity) that enable easy swallowing, so that the bolus can smoothly move from the oral cavity to the stomach through the pharynx.</p><p id="p-0067" num="0065">In the oral transit phase of eating and swallowing, the tongue (the tip of the tongue) is lifted and moves the bolus from the inside of the oral cavity to the pharynx. The eating and swallowing functions in the oral transit phase include, for example, the motor function of the tongue to move the bolus to the pharynx and the ascending function of the soft palate which closes the space between the pharynx and the nasal cavity.</p><p id="p-0068" num="0066">In the pharyngeal phase of eating and swallowing, swallowing reflex occurs when the bolus reaches the pharynx, and the bolus is sent to the esophagus within a short period of time (about one second). Specifically, the soft palate rises to close the space between the nasal cavity and pharynx, the base of the tongue (specifically, the hyoid bone supporting the base of the tongue) and the larynx rise to allow the bolus to pass through the pharynx, at which time the epiglottis flips downward to close the entrance of the trachea and the bolus is sent to the esophagus in a manner that aspiration does not occur. The eating and swallowing functions in the pharyngeal phase include, for example, the motor function of the pharynx to close the space between the nasal cavity and the pharynx (specifically, the motor function to raise the soft palate), the motor function of the tongue (specifically, the base of the tongue) to send the bolus to the pharynx, and the motor function of the larynx to send the bolus from the pharynx to the esophagus and to close the glottis to block the trachea and hang the epiglottis down over it to cover the entrance of the trachea when the bolus flows into the pharynx.</p><p id="p-0069" num="0067">In the esophageal phase of eating and swallowing, peristaltic movement of the esophageal wall is induced and the bolus is sent from the esophagus to the stomach. The eating and swallowing function in the esophageal phase is, for example, the peristaltic function of the esophagus to move the bolus to the stomach.</p><p id="p-0070" num="0068">For example, as a person ages, he/she transits from a healthy state to a state requiring nursing care after going through the pre-frail stage and the frail stage. A decline in the eating and swallowing functions (also called oral frailty) is said to start appearing during the pre-frail stage. A decline in the eating and swallowing functions could hasten the progression to the state requiring nursing care which follows the frail stage. Therefore, by noticing how the eating and swallowing functions are declining in the pre-frail stage and taking preventive and remedial measures in advance, it is possible to reduce the risk of falling into the state requiring nursing care which follows the frail stage and to maintain a healthy and independent life for a longer period of time.</p><p id="p-0071" num="0069">Next, the following describes an example in which oral function visualization system <b>10</b> displays on a screen an image of an estimated state of the oral organs of user <b>2</b> in accordance with the eating and swallowing process. <figref idref="DRAWINGS">FIG. <b>12</b></figref> is a table showing the contents of video display corresponding to the eating and swallowing process according to the embodiment of the present disclosure.</p><p id="p-0072" num="0070">Oral function visualization system <b>10</b> focuses on &#x201c;i&#x201d;, &#x201c;e&#x201d;, and &#x201c;a&#x201d; as sounds corresponding to the opening and closing of the mouth which is a process in the mastication phase. To evaluate the process of the opening and closing of the mouth, oral function visualization system <b>10</b> causes user <b>2</b> to pronounce a word containing the sounds &#x201c;i&#x201d;, &#x201c;e&#x201d;, and &#x201c;a&#x201d;, and analyzes user <b>2</b>'s pronunciation. Oral function visualization system <b>10</b> then displays on the screen a state of the oral organs of user <b>2</b> which is estimated from user <b>2</b>'s pronunciation. At this time, oral function visualization system <b>10</b> may also display a state of the oral organs ideal for pronouncing the word containing the sounds &#x201c;i&#x201d;, &#x201c;e&#x201d;, and &#x201c;a&#x201d;.</p><p id="p-0073" num="0071">Also, for example, oral function visualization system <b>10</b> focuses on &#x201c;ka la&#x201d; as a sound corresponding to mastication which is a process in the mastication phase. To evaluate the process of the opening and closing of the mouth, oral function visualization system <b>10</b> causes user <b>2</b> to pronounce a word containing the sound &#x201c;ka la&#x201d;, and analyzes user <b>2</b>'s pronunciation. Oral function visualization system <b>10</b> then displays on the screen a state of the oral organs of user <b>2</b> which is estimated from user <b>2</b>'s pronunciation. At this time, oral function visualization system <b>10</b> may also display a state of the oral organs ideal for pronouncing the word containing the sound &#x201c;ka la&#x201d;.</p><p id="p-0074" num="0072">Also, for example, oral function visualization system <b>10</b> focuses on &#x201c;p&#x201d; as a sound corresponding to lip closure which is a process in the mastication phase. To evaluate the process of the opening and closing of the mouth, oral function visualization system <b>10</b> causes user <b>2</b> to pronounce a word containing the sound &#x201c;p&#x201d;, and analyzes user <b>2</b>'s pronunciation. Oral function visualization system <b>10</b> then displays on the screen a state of the oral organs of user <b>2</b> which is estimated from user <b>2</b>'s pronunciation. At this time, oral function visualization system <b>10</b> may also display a state of the oral organs ideal for pronouncing the word containing the sound &#x201c;p&#x201d;.</p><p id="p-0075" num="0073">Also, for example, oral function visualization system <b>10</b> focuses on &#x201c;t&#x201d; and &#x201c;e&#x201d; as sounds corresponding to forward tongue movement which is a process in the oral transit phase. To evaluate the process of forward tongue movement, oral function visualization system <b>10</b> causes user <b>2</b> to pronounce a word containing the sounds &#x201c;t&#x201d; and &#x201c;e&#x201d;, and analyzes user <b>2</b>'s pronunciation. Oral function visualization system <b>10</b> then displays on the screen a state of the oral organs of user <b>2</b> which is estimated from user <b>2</b>'s pronunciation. At this time, oral function visualization system <b>10</b> may also display a state of the oral organs ideal for pronouncing the word containing the sounds &#x201c;t&#x201d; and &#x201c;e&#x201d;.</p><p id="p-0076" num="0074">Also, for example, oral function visualization system <b>10</b> focuses on &#x201c;k&#x201d; and &#x201c;o&#x201d; as sounds corresponding to back tongue movement which is a process in the oral transit phase. To evaluate the process of back tongue movement, oral function visualization system <b>10</b> causes user <b>2</b> to pronounce a word containing the sounds &#x201c;k&#x201d; and &#x201c;o&#x201d;, and analyzes user <b>2</b>'s pronunciation. Oral function visualization system <b>10</b> then displays on the screen a state of the oral organs of user <b>2</b> which is estimated from user <b>2</b>'s pronunciation. At this time, oral function visualization system <b>10</b> may also display a state of the oral organs ideal for pronouncing the word containing the sounds &#x201c;k&#x201d; and &#x201c;o&#x201d;.</p><p id="p-0077" num="0075">Also, for example, oral function visualization system <b>10</b> focuses on &#x201c;ko&#x201d; as a sound corresponding to tongue palate closure which is a process in the pharyngeal phase. To evaluate the process of tongue palate closure, oral function visualization system <b>10</b> causes user <b>2</b> to pronounce a word containing the sound &#x201c;ko&#x201d;, and analyzes user <b>2</b>'s pronunciation. Oral function visualization system <b>10</b> then displays on the screen a state of the oral organs of user <b>2</b> which is estimated from user <b>2</b>'s pronunciation. At this time, oral function visualization system <b>10</b> may also display a state of the oral organs ideal for pronouncing the word containing the sound &#x201c;ko&#x201d;.</p><p id="p-0078" num="0076">In this way, oral function visualization system <b>10</b> can evaluate the eating and swallowing functions of user <b>2</b> by analyzing user <b>2</b>'s pronunciation. Oral function visualization system <b>10</b> can also prompt user <b>2</b> to do training for improving the eating and swallowing functions, by analyzing user <b>2</b>'s pronunciation and displaying an estimated state of the oral organs of user <b>2</b> on the screen. In such a manner, oral function visualization system <b>10</b> can be applied to, for example, the evaluation of eating and swallowing and improvement of the eating and swallowing functions.</p><heading id="h-0018" level="2">[Advantageous Effects etc.]</heading><p id="p-0079" num="0077">Oral function visualization system <b>10</b> includes: outputter <b>11</b> that outputs information for prompting user <b>2</b> to utter a predetermined voice; obtainer <b>12</b> that obtains an uttered voice of user <b>2</b> uttered in accordance with the output; analyzer <b>15</b> that analyzes the uttered voice obtained by obtainer <b>12</b>; and estimator <b>16</b> that estimates a state of oral organs of user <b>2</b> from a result of analysis of the uttered voice by analyzer <b>15</b>. Outputter <b>11</b> outputs, based on the state of the oral organs of user <b>2</b> estimated by estimator <b>16</b>, information for user <b>2</b> to achieve a state of the oral organs suitable for utterance of the predetermined voice.</p><p id="p-0080" num="0078">Accordingly, oral function visualization system <b>10</b> can visualize the inside of the oral cavity of user <b>2</b> that is based on user <b>2</b>'s pronunciation, by displaying a state of the oral organs on the screen, for example. Since user <b>2</b> can recognize the state of the oral organs outputted, user <b>2</b> can try to utter a voice correctly. As a result, user <b>2</b> will be able to utter a voice correctly.</p><p id="p-0081" num="0079">Oral function visualization system <b>10</b> includes storage <b>14</b> that stores information indicating a state of the oral organs suitable for utterance of the predetermined voice. Storage <b>14</b> further stores the an uttered voice of user <b>2</b> uttered in the past. Outputter <b>11</b> reproduces a state of the oral organs of user <b>2</b> estimated by estimator <b>16</b> from the uttered voice of user <b>2</b> uttered in the past and stored in storage <b>14</b>, reproduces a state of the oral organs of user <b>2</b> estimated by estimator <b>16</b> from an uttered voice uttered at present and obtained by obtainer <b>12</b>, and displays on a screen each of the states of the oral organs reproduced.</p><p id="p-0082" num="0080">Accordingly, by estimating and reproducing states of the oral organs of user <b>2</b>, oral function visualization system <b>10</b> can prompt user <b>2</b> to utter the predetermined voice correctly.</p><p id="p-0083" num="0081">Oral function visualization system <b>10</b> further includes storage <b>14</b> that stores an image showing the state of the oral organs suitable for utterance of the predetermined voice. Estimator <b>16</b> generates an image showing the state of the oral organs of user <b>2</b> estimated. Outputter <b>11</b> displays on the screen the image stored in storage <b>14</b> generated by estimator <b>16</b>.</p><p id="p-0084" num="0082">Accordingly, by displaying on the screen a state of the oral organs ideal for utterance of the predetermined voice and the estimated state of the oral organs of user <b>2</b>, oral function visualization system <b>10</b> can prompt user <b>2</b> to utter the predetermined voice correctly.</p><p id="p-0085" num="0083">Outputter <b>11</b> shows, as a state of the oral organs, a position of a tongue in an oral cavity and an open or closed state of the oral cavity, using a cross-sectional view of an inside of the oral cavity in a lateral view of a person's face.</p><p id="p-0086" num="0084">Outputter <b>11</b> further outputs information that prompts user <b>2</b> to do training to achieve the state of the oral organs suitable for utterance of the predetermined voice.</p><p id="p-0087" num="0085">Accordingly, oral function visualization system <b>10</b> can prompt user <b>2</b> to do training on uttering the predetermined voice.</p><p id="p-0088" num="0086">Storage <b>14</b> further stores oral cavity state data items which are images each showing a state of the oral organs that is associated with a predetermined voice feature amount. Outputter <b>11</b> outputs the oral cavity state data items each corresponding to a voice feature amount obtained as a result of analysis of the uttered voice by analyzer <b>15</b>.</p><p id="p-0089" num="0087">Accordingly, using plural images stored in advance, oral function visualization system <b>10</b> can reproduce the state of the oral organs of user <b>2</b> estimated by estimator <b>16</b>.</p><p id="p-0090" num="0088">In oral function visualization system <b>10</b>, each of the images is video.</p><p id="p-0091" num="0089">Accordingly, with use of video, oral function visualization system <b>10</b> can prompt user <b>2</b> to utter the predetermined voice correctly.</p><p id="p-0092" num="0090">Outputter <b>11</b> outputs on paper any or all of at least one of: information for user <b>2</b> to achieve the state of the oral organs suitable for utterance of the predetermined voice; information indicating the state of the oral organs suitable for utterance of the predetermined voice; or information that prompts user <b>2</b> to do training to achieve the state of the oral organs suitable for utterance of the predetermined voice.</p><p id="p-0093" num="0091">Accordingly, with use of a report provided on paper, oral function visualization system <b>10</b> can prompt user <b>2</b> to utter the predetermined voice correctly.</p><p id="p-0094" num="0092">An oral function visualization method includes: outputting information for prompting user <b>2</b> to utter a predetermined voice; obtaining an uttered voice of user <b>2</b> uttered in accordance with the outputting; analyzing the uttered voice obtained in the obtaining; and estimating a state of oral organs of user <b>2</b> from a result of analysis of the uttered voice in the analyzing; and outputting, based on the state of the oral organs of user <b>2</b> estimated in the estimating, information for user <b>2</b> to achieve a state of the oral organs suitable for utterance of the predetermined voice.</p><p id="p-0095" num="0093">Accordingly, the oral function visualization method can yield the same advantageous effects as those yielded by oral function visualization system <b>10</b> described above.</p><p id="p-0096" num="0094">A non-transitory computer-readable recording medium has recorded thereon a program that causes a computer to execute an oral function visualization method including: outputting information for prompting user <b>2</b> to utter a predetermined voice; obtaining an uttered voice of user <b>2</b> uttered in accordance with the outputting; analyzing the uttered voice obtained in the obtaining; estimating a state of oral organs of user <b>2</b> from a result of analysis of the uttered voice in the analyzing; and outputting, based on the state of the oral organs of user <b>2</b> estimated in the estimating, information for user <b>2</b> to achieve a state of the oral organs suitable for utterance of the predetermined voice.</p><p id="p-0097" num="0095">Accordingly, the recording medium can yield the same advantageous effects as those yielded by oral function visualization system <b>10</b> described above.</p><p id="p-0098" num="0096">[Other]</p><p id="p-0099" num="0097">Although an embodiment has been described above, the present disclosure is not limited to the above embodiment.</p><p id="p-0100" num="0098">For example, the oral function visualization system according to the above embodiment may be implemented by plural devices or may be implemented as a single device. For example, the oral function visualization system may be implemented as a client server system. Also, the oral function visualization system may be implemented as a discrete mobile terminal such as a smartphone or a tablet terminal. In the case of implementing the oral function visualization system using plural devices, the constituent elements of the oral function visualization system may be allocated to the plural devices in any manner.</p><p id="p-0101" num="0099">In the above embodiment, a process performed by a particular processing unit may be performed by another processing unit. The processing order of plural processes may be changed, and plural processes may be performed in parallel.</p><p id="p-0102" num="0100">In addition, in the above embodiment, each constituent element may be implemented through execution of a software program suitable for the constituent element. Each constituent element may also be implemented by a program executing unit such as CPU or a processor reading and executing a software program recorded on a recording medium such as a hard disk or semiconductor memory.</p><p id="p-0103" num="0101">Furthermore, each constituent element may be implemented in the form of a hardware product. For example, each constituent element may be a circuit (or an integrated circuit). These circuits may be configured as a single circuit or may be individual circuits. Moreover, these circuits may be general-purpose circuits, or may be dedicated circuits.</p><p id="p-0104" num="0102">Furthermore, general or specific aspects of the present disclosure may be implemented by a system, a device, a method, an integrated circuit, a computer program, or a computer-readable recording medium such CD-ROM. General or specific aspects of the present disclosure may also be implemented by any combination of systems, devices, methods, integrated circuits, computer programs, and recording media.</p><p id="p-0105" num="0103">For example, the present disclosure may be implemented as a program that causes a computer to execute the oral function visualization method according to the above embodiment. The present disclosure may be implemented as a non-transitory computer-readable recording medium having the program recorded thereon. Note that the program includes an application program for causing a general-purpose mobile terminal to operate as the oral function visualization system according to the above embodiment.</p><p id="p-0106" num="0104">The present disclosure also encompasses other forms achieved by making various modifications to the embodiment that are conceivable to those skilled in the art or forms resulting from any combination of the constituent elements and functions in the embodiment without departing from the essence of the present disclosure.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An oral function visualization system comprising:<claim-text>an outputter that outputs information for prompting a user to utter a predetermined voice;</claim-text><claim-text>an obtainer that obtains an uttered voice of the user uttered in accordance with the output;</claim-text><claim-text>an analyzer that analyzes a sound pressure difference and a speaking speed of the uttered voice obtained by the obtainer; and</claim-text><claim-text>an estimator that estimates a state of oral organs of the user from a result of analysis of the uttered voice by the analyzer,</claim-text><claim-text>wherein the outputter further outputs, based on the state of the oral organs of the user estimated by the estimator, information for the user to achieve a state of the oral organs suitable for utterance of the predetermined voice.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The oral function visualization system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, comprising:<claim-text>storage that stores (i) information indicating the state of the oral organs suitable for utterance of the predetermined voice and (ii) an uttered voice of the user uttered in past,</claim-text><claim-text>wherein the outputter reproduces a state of the oral organs of the user estimated by the estimator from the uttered voice of the user uttered in past and stored in the storage, reproduces a state of the oral organs of the user estimated by the estimator from an uttered voice uttered at present and obtained by the obtainer, and displays on a screen each of the states of the oral organs reproduced.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The oral function visualization system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>storage that stores an image showing the state of the oral organs suitable for utterance of the predetermined voice,</claim-text><claim-text>wherein the estimator generates an image showing the state of the oral organs of the user estimated, and</claim-text><claim-text>the outputter displays on a screen the image stored in the storage and the image generated by the estimator.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The oral function visualization system according to <claim-ref idref="CLM-00002">claim 2</claim-ref>,<claim-text>wherein the outputter shows, as a state of the oral organs, a position of a tongue in an oral cavity and an open or closed state of the oral cavity, using a cross-sectional view of an inside of the oral cavity in a lateral view of a person's face.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The oral function visualization system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein the outputter further outputs information that prompts the user to do training to achieve the state of the oral organs suitable for utterance of the predetermined voice.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The oral function visualization system according to <claim-ref idref="CLM-00002">claim 2</claim-ref>,<claim-text>wherein the storage further stores oral cavity state data items which are images each showing a state of the oral organs that is associated with a predetermined voice feature amount, and</claim-text><claim-text>the outputter outputs the oral cavity state data items each corresponding to a voice feature amount obtained as a result of analysis of the uttered voice by the analyzer.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The oral function visualization system according to <claim-ref idref="CLM-00006">claim 6</claim-ref>,<claim-text>wherein each of the images is video.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The oral function visualization system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>the outputter outputs on paper at least one of: information for the user to achieve the state of the oral organs suitable for utterance of the predetermined voice; information indicating the state of the oral organs suitable for utterance of the predetermined voice; or information that prompts the user to do training to achieve the state of the oral organs suitable for utterance of the predetermined voice.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. An oral function visualization method comprising:<claim-text>outputting information for prompting a user to utter a predetermined voice;</claim-text><claim-text>obtaining an uttered voice of the user uttered in accordance with the outputting;</claim-text><claim-text>analyzing a sound pressure difference and a speaking speed of the uttered voice obtained in the obtaining;</claim-text><claim-text>estimating a state of oral organs of the user from a result of analysis of the uttered voice in the analyzing; and</claim-text><claim-text>outputting, based on the state of the oral organs of the user estimated in the estimating, information for the user to achieve a state of the oral organs suitable for utterance of the predetermined voice.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. A non-transitory computer-readable recording medium having recorded thereon a program for causing a computer to execute the oral function visualization method according to <claim-ref idref="CLM-00009">claim 9</claim-ref>.</claim-text></claim></claims></us-patent-application>