<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005465A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005465</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17837684</doc-number><date>20220610</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="regional"><country>EP</country><doc-number>21182638.3</doc-number><date>20210630</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>13</main-group><subgroup>047</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>22</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>26</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>13</main-group><subgroup>033</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>13</main-group><subgroup>06</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>13</main-group><subgroup>047</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>22</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>26</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>13</main-group><subgroup>033</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>13</main-group><subgroup>06</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>L</subclass><main-group>47</main-group><subgroup>12</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">VOICE COMMUNICATION BETWEEN A SPEAKER AND A RECIPIENT OVER A COMMUNICATION NETWORK</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="obligated-assignee"><addressbook><orgname>Elektrobit Automotive GmbH</orgname><address><city>Erlangen</city><country>DE</country></address></addressbook><residence><country>DE</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Strassenburg-Kleciak</last-name><first-name>Marek</first-name><address><city>Erlangen</city><country>DE</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Kurumbudel</last-name><first-name>Prashanth Ram</first-name><address><city>Sullia</city><country>IN</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Voice communication, between a speaker and a recipient, either or both of which may be in a motor vehicle, is provided via a communication network. In a first step, an input speech utterance is received from the speaker. Optionally, a bandwidth of a connection to the communication network is evaluated at the side of the speaker. The input speech utterance is then converted to text. At least the text is transmitted over the communication network. In case of a sufficiently large bandwidth, the input speech utterance may be transmitted as voice and as text. The transmitted text is converted into an output speech utterance that simulates a voice of the speaker. Finally, the output speech utterance is provided to the recipient.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="136.57mm" wi="95.42mm" file="US20230005465A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="218.36mm" wi="147.91mm" file="US20230005465A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="164.93mm" wi="156.04mm" file="US20230005465A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="171.20mm" wi="158.24mm" file="US20230005465A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="227.67mm" wi="129.29mm" file="US20230005465A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="227.67mm" wi="129.29mm" file="US20230005465A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="242.57mm" wi="134.54mm" file="US20230005465A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">BACKGROUND</heading><p id="p-0002" num="0001">The present invention is related to a method, a computer program, and a system for voice communication between a speaker and a recipient over a communication network. The invention is further related to apparatuses for use of such a system and a vehicle comprising such apparatuses.</p><p id="p-0003" num="0002">With the broad availability of broadband Internet access, voice communication has shifted to IP telephony solutions, also known as Voice over Internet Protocol (VoIP). VoIP refers to technologies for the delivery of voice communications over Internet Protocol (IP) networks. While these technologies in general deliver a satisfactory service, sometimes people are difficult to understand during a voice call. A main reason is a low bandwidth or data rate of the connection. If the achievable data rate is too low, the connection is still available, but the quality of conversation is unsatisfactory.</p><p id="p-0004" num="0003">It is an object of the present invention to provide a solution for voice communication between a speaker and a recipient over a communication network, which delivers an improved quality of communication.</p><heading id="h-0002" level="1">BRIEF SUMMARY</heading><p id="p-0005" num="0004">This object is achieved by a method, a computer program, which implements this method, a system, and apparatuses according to the independent claims. The dependent claims include advantageous further developments and improvements of the present principles as described below.</p><p id="p-0006" num="0005">According to a first aspect, a method for voice communication between a speaker and a recipient over a communication network comprises the steps of:<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0000">    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0006">receiving an input speech utterance from the speaker;</li>        <li id="ul0002-0002" num="0007">converting the input speech utterance to text;</li>        <li id="ul0002-0003" num="0008">transmitting at least the text over the communication network;</li>        <li id="ul0002-0004" num="0009">converting the transmitted text into an output speech utterance that simulates a voice of the speaker; and</li>        <li id="ul0002-0005" num="0010">providing the output speech utterance to the recipient.</li>    </ul>    </li></ul></p><p id="p-0007" num="0011">Accordingly, a computer program comprises instructions, which, when executed by at least one processor, cause the at least one processor to perform the following steps for voice communication between a speaker and a recipient over a communication network:<ul id="ul0003" list-style="none">    <li id="ul0003-0001" num="0000">    <ul id="ul0004" list-style="none">        <li id="ul0004-0001" num="0012">receiving an input speech utterance from the speaker;</li>        <li id="ul0004-0002" num="0013">converting the input speech utterance to text;</li>        <li id="ul0004-0003" num="0014">transmitting at least the text over the communication network;</li>        <li id="ul0004-0004" num="0015">converting the transmitted text into an output speech utterance that simulates a voice of the speaker; and</li>        <li id="ul0004-0005" num="0016">providing the output speech utterance to the recipient.</li>    </ul>    </li></ul></p><p id="p-0008" num="0017">The term computer has to be understood broadly. In particular, it also includes workstations, distributed systems, and other processor-based or microcontroller-based data processing devices.</p><p id="p-0009" num="0018">The computer program can, for example, be made available for electronic retrieval or stored on a computer-readable storage medium. Amongst others, the computer program can be provided as an app for mobile devices.</p><p id="p-0010" num="0019">According to another aspect, a system for voice communication between a speaker and a recipient over a communication network comprises:<ul id="ul0005" list-style="none">    <li id="ul0005-0001" num="0000">    <ul id="ul0006" list-style="none">        <li id="ul0006-0001" num="0020">an input module configured to receive an input speech utterance from the speaker;</li>        <li id="ul0006-0002" num="0021">a speech-to-text conversion module configured to convert the input speech utterance to text;</li>        <li id="ul0006-0003" num="0022">a transmission module configured to transmit at least the text over the communication network;</li>        <li id="ul0006-0004" num="0023">a text-to-speech conversion module configured to convert the transmitted text into an output speech utterance that simulates a voice of the speaker; and</li>        <li id="ul0006-0005" num="0024">an output module configured to provide the output speech utterance to the recipient.</li>    </ul>    </li></ul></p><p id="p-0011" num="0025">According to another aspect, an apparatus for use in a system according to the invention comprises:<ul id="ul0007" list-style="none">    <li id="ul0007-0001" num="0000">    <ul id="ul0008" list-style="none">        <li id="ul0008-0001" num="0026">an input module configured to receive an input speech utterance from the speaker;</li>        <li id="ul0008-0002" num="0027">a speech-to-text conversion module configured to convert the input speech utterance to text; and</li>        <li id="ul0008-0003" num="0028">a transmission module configured to transmit at least the text over the communication network.</li>    </ul>    </li></ul></p><p id="p-0012" num="0029">According to another aspect, an apparatus for use in a system according to the invention comprises:<ul id="ul0009" list-style="none">    <li id="ul0009-0001" num="0000">    <ul id="ul0010" list-style="none">        <li id="ul0010-0001" num="0030">a receiving module configured to receive text generated from an input speech utterance of a speaker;</li>        <li id="ul0010-0002" num="0031">a text-to-speech conversion module configured to convert the transmitted text into an output speech utterance that simulates a voice of the speaker; and</li>        <li id="ul0010-0003" num="0032">an output module configured to provide the output speech utterance to the recipient.</li>    </ul>    </li></ul></p><p id="p-0013" num="0033">According to embodiments of the invention, the speech input of a speaker is converted into text by a speech-to-text conversion module and transmitted as text to the recipient, preferably together with additional information about the speech utterance. This additional information may include, for example, an intonation (e.g., ascending or descending), a speed of speech, detected emotions (e.g., excited, nervous, etc.), durations of the individual words, etc. At the side of the recipient, the received text and, if applicable, the additional information are then converted into a speech output by a text-to-speech conversion module. Speech-to-text and text-to-speech conversion modules are state of the art. This conversion of the received text is done in such way that the speech output resembles the voice of the speaker. Even though the voice of the speaker is synthesized, the recipient will have the feeling of listening to the speaker's voice. As the transmission of text has less requirements with regard to the connection to the communication network than the transmission of voice, a seamless voice call experience is achieved even in fluctuating network conditions. As a further advantage, the described solution allows removing noise stemming from the side of the speaker.</p><p id="p-0014" num="0034">In an advantageous embodiment, a bandwidth of a connection to the communication network is evaluated at the side of the speaker. In this way, the conversion of the speech input of the speaker into text can be omitted if the connection to the communication network is good enough for transmitting voice.</p><p id="p-0015" num="0035">In an advantageous embodiment, in case of a sufficiently large bandwidth, the input speech utterance is transmitted as voice and as text. In this way, depending on the data connection at the side of the recipient, the received text can be discarded or used for generating the speech output.</p><p id="p-0016" num="0036">In an advantageous embodiment, the transmitted text is converted into an output speech utterance by a text-to-speech algorithm. Text-to-speech algorithms are well established and have rather limited requirements with regard to the necessary processing power. Preferably, the text-to-speech algorithm uses a phoneme library suitable for simulating different speakers. In this way, by an appropriate choice of the phonemes the voice of the speaker can easily be simulated.</p><p id="p-0017" num="0037">In an advantageous embodiment, the transmitted text is converted into an output speech utterance by one or more trained artificial intelligence models. While trained artificial intelligence models typically require more processing power than text-to-speech algorithms, they will yield more natural speech outputs.</p><p id="p-0018" num="0038">In an advantageous embodiment, a first trained artificial intelligence model transforms the transmitted text into an intermediate speech utterance and a second trained artificial intelligence model transforms the intermediate speech utterance into the output speech utterance. In this way, the first trained artificial intelligence model converts the input data into another space and is broadly usable irrespective of a specific speaker. The second artificial intelligence model manipulates the data in the same space. Preferably, the second artificial intelligence model is trained with the voice of the individual specific speaker. In addition to the first artificial intelligence model and the second artificial intelligence model, a further artificial intelligence model may be provided, which is responsible for synthesizing the tone or emotion of the speaker. This further artificial intelligence model may make use of the additional information that is sent along with the text.</p><p id="p-0019" num="0039">In an advantageous embodiment, the second trained artificial intelligence model is selected from a bank of trained artificial intelligence models. The artificial intelligence models inside the bank are individual models trained with individual user voices. This allows simulating the voices of different speakers.</p><p id="p-0020" num="0040">In an advantageous embodiment, the second trained artificial intelligence model is selected from the bank of trained artificial intelligence models based on information about the speaker. In this way, an artificial intelligence model that is appropriate for simulating the voice of a specific speaker can easily be determined.</p><p id="p-0021" num="0041">In an advantageous embodiment, the information about the speaker is provided by the speaker or determined by a voice analysis algorithm. The information provided by the speaker may, for example, be a unique identifier, which is associated with an artificial intelligence model of the bank. Alternatively, the voice analysis algorithm may provide characteristics of the voice of the speaker. These characteristics may then be used for determining an artificial intelligence model in the bank that generates similar characteristics.</p><p id="p-0022" num="0042">Preferably, a vehicle comprises apparatuses for use in a system according to the invention. In this way, an improved quality of voice communication is achieved even in situations or locations with low connectivity. However, the described solutions are applicable to any VoIP system.</p><p id="p-0023" num="0043">Further features of the present invention will become apparent from the following description and the appended claims in conjunction with the figures.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0024" num="0044"><figref idref="DRAWINGS">FIG. <b>1</b></figref> schematically illustrates a method for voice communication between a speaker and a recipient over a communication network.</p><p id="p-0025" num="0045"><figref idref="DRAWINGS">FIG. <b>2</b></figref> schematically illustrates a system for voice communication between a speaker and a recipient over a communication network.</p><p id="p-0026" num="0046"><figref idref="DRAWINGS">FIG. <b>3</b></figref> schematically illustrates a first embodiment of an apparatus for use in the system of <figref idref="DRAWINGS">FIG. <b>2</b></figref> at the side of the speaker.</p><p id="p-0027" num="0047"><figref idref="DRAWINGS">FIG. <b>4</b></figref> schematically illustrates a second embodiment of an apparatus for use in the system of <figref idref="DRAWINGS">FIG. <b>2</b></figref> at the side of the speaker.</p><p id="p-0028" num="0048"><figref idref="DRAWINGS">FIG. <b>5</b></figref> schematically illustrates a first embodiment of an apparatus for use in the system of <figref idref="DRAWINGS">FIG. <b>2</b></figref> at the side of the recipient.</p><p id="p-0029" num="0049"><figref idref="DRAWINGS">FIG. <b>6</b></figref> schematically illustrates a second embodiment of an apparatus for use in the system of <figref idref="DRAWINGS">FIG. <b>2</b></figref> at the side of the recipient.</p><p id="p-0030" num="0050"><figref idref="DRAWINGS">FIG. <b>7</b></figref> depicts a system diagram of a first embodiment of a solution according to the invention.</p><p id="p-0031" num="0051"><figref idref="DRAWINGS">FIG. <b>8</b></figref> depicts a system diagram of a second embodiment of a solution according to the invention.</p><p id="p-0032" num="0052"><figref idref="DRAWINGS">FIG. <b>9</b></figref> shows details of a conversion from text to speech with trained artificial intelligence models.</p><p id="p-0033" num="0053"><figref idref="DRAWINGS">FIG. <b>10</b></figref> schematically illustrates a motor vehicle in which a solution according to the invention is implemented.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0004" level="1">DETAILED DESCRIPTION</heading><p id="p-0034" num="0054">The present description illustrates the principles of the present disclosure. It will thus be appreciated that those skilled in the art will be able to devise various arrangements that, although not explicitly described or shown herein, embody the principles of the disclosure.</p><p id="p-0035" num="0055">All examples and conditional language recited herein are intended for educational purposes to aid the reader in understanding the principles of the disclosure and the concepts contributed by the inventor to furthering the art, and are to be construed as being without limitation to such specifically recited examples and conditions.</p><p id="p-0036" num="0056">Moreover, all statements herein reciting principles, aspects, and embodiments of the disclosure, as well as specific examples thereof, are intended to encompass both structural and functional equivalents thereof. Additionally, it is intended that such equivalents include both currently known equivalents as well as equivalents developed in the future, i.e., any elements developed that perform the same function, regardless of structure.</p><p id="p-0037" num="0057">Thus, for example, it will be appreciated by those skilled in the art that the diagrams presented herein represent conceptual views of illustrative circuitry embodying the principles of the disclosure.</p><p id="p-0038" num="0058">The functions of the various elements shown in the figures may be provided through the use of dedicated hardware as well as hardware capable of executing software in association with appropriate software. When provided by a processor, the functions may be provided by a single dedicated processor, by a single shared processor, or by a plurality of individual processors, some of which may be shared. Moreover, explicit use of the term &#x201c;processor&#x201d; or &#x201c;controller&#x201d; should not be construed to refer exclusively to hardware capable of executing software, and may implicitly include, without limitation, digital signal processor (DSP) hardware, read only memory (ROM) for storing software, random access memory (RAM), and nonvolatile storage.</p><p id="p-0039" num="0059">Other hardware, conventional and/or custom, may also be included. Similarly, any switches shown in the figures are conceptual only. Their function may be carried out through the operation of program logic, through dedicated logic, through the interaction of program control and dedicated logic, or even manually, the particular technique being selectable by the implementer as more specifically understood from the context.</p><p id="p-0040" num="0060">In the claims hereof, any element expressed as a means for performing a specified function is intended to encompass any way of performing that function including, for example, a combination of circuit elements that performs that function or software in any form, including, therefore, firmware, microcode or the like, combined with appropriate circuitry for executing that software to perform the function. The disclosure as defined by such claims resides in the fact that the functionalities provided by the various recited means are combined and brought together in the manner which the claims call for. It is thus regarded that any means that can provide those functionalities are equivalent to those shown herein.</p><p id="p-0041" num="0061"><figref idref="DRAWINGS">FIG. <b>1</b></figref> schematically illustrates a method according to the invention for voice communication between a speaker and a recipient over a communication network. In a first step, an input speech utterance is received S<b>1</b> from the speaker. Optionally, a bandwidth of a connection to the communication network is evaluated S<b>2</b> at the side of the speaker. The input speech utterance is then converted S<b>3</b> to text. At least the text is transmitted S<b>4</b> over the communication network. In case of a sufficiently large bandwidth, the input speech utterance may be transmitted S<b>4</b> as voice and as text. The transmitted text is converted S<b>5</b> into an output speech utterance that simulates a voice of the speaker. For this purpose, a text-to-speech algorithm may be used. Preferably, such a text-to-speech algorithm uses a phoneme library suitable for simulating different speakers. Alternatively, the transmitted text is converted S<b>5</b> into an output speech utterance by one or more trained artificial intelligence models. For example, a first trained artificial intelligence model may transform the transmitted text into an intermediate speech utterance. A second trained artificial intelligence model then transforms the intermediate speech utterance into the output speech utterance. The second trained artificial intelligence model may be selected from a bank of trained artificial intelligence models, e.g. based on information about the speaker. Such information may be provided by the speaker or determined by a voice analysis algorithm. Finally, the output speech utterance is provided S<b>6</b> to the recipient.</p><p id="p-0042" num="0062"><figref idref="DRAWINGS">FIG. <b>2</b></figref> schematically illustrates a block diagram of a system for voice communication between a speaker S and a recipient R over a communication network N. The system comprises an input module <b>12</b> configured to receive an input speech utterance U<sub>i </sub>from the speaker S. An evaluation module <b>13</b> may be provided at the side of the speaker for evaluating a bandwidth of a connection to the communication network N. A speech-to-text conversion module <b>14</b> is configured to convert the input speech utterance U<sub>i </sub>to text T. A transmission module <b>15</b> is configured to transmit at least the text T over the communication network N, preferably together with additional information about the speech utterance. In case of a sufficiently large bandwidth, the input speech utterance U<sub>i </sub>may be transmitted by the transmission module <b>15</b> as voice V and as text T. A text-to-speech conversion module <b>33</b> is configured to convert the transmitted text T into an output speech utterance U<sub>o </sub>that simulates a voice of the speaker S. For this purpose, the text-to-speech conversion module <b>33</b> may use a text-to-speech algorithm. Preferably, such a text-to-speech algorithm uses a phoneme library suitable for simulating different speakers. Alternatively, the text-to-speech conversion module <b>33</b> may convert the transmitted text into the output speech utterance U<sub>o </sub>using one or more trained artificial intelligence models. For example, a first trained artificial intelligence model may transform the transmitted text into an intermediate speech utterance. A second trained artificial intelligence model then transforms the intermediate speech utterance into the output speech utterance. The second trained artificial intelligence model may be selected from a bank of trained artificial intelligence models, e.g. based on information about the speaker S. Such information may be provided by the speaker S or determined by a voice analysis algorithm. An output module <b>34</b> is configured to provide the output speech utterance U<sub>o </sub>to the recipient R.</p><p id="p-0043" num="0063"><figref idref="DRAWINGS">FIG. <b>3</b></figref> schematically illustrates a block diagram of a first embodiment of an apparatus <b>10</b> for use in the system of <figref idref="DRAWINGS">FIG. <b>2</b></figref> at the side of the speaker S. The apparatus <b>10</b> has an input <b>11</b> via which an input module <b>12</b> receives an input speech utterance U<sub>i </sub>from the speaker S. An evaluation module <b>13</b> may be provided for evaluating a bandwidth of a connection to a communication network N. The apparatus <b>10</b> further has a speech-to-text conversion module <b>14</b> configured to convert the input speech utterance U<sub>i </sub>to text T. A transmission module <b>15</b> is configured to transmit at least the text T over the communication network N via an output <b>18</b>, preferably together with additional information about the speech utterance, such as an intonation, a speed of speech, detected emotions, durations of the individual words, etc. In case of a sufficiently large bandwidth, the input speech utterance U<sub>i </sub>may be transmitted by the transmission module <b>15</b> as voice V and as text T. A local storage unit <b>17</b> is provided, e.g. for storing data during processing. The output <b>18</b> may also be combined with the input <b>11</b> into a single bidirectional interface.</p><p id="p-0044" num="0064">The various modules <b>12</b>-<b>15</b> may be controlled by a control module <b>16</b>. A user interface <b>19</b> may be provided for enabling a user to modify settings of the various modules <b>12</b>-<b>16</b>. The modules <b>12</b>-<b>16</b> of the apparatus <b>10</b> can be embodied as dedicated hardware units. Of course, they may likewise be fully or partially combined into a single unit or implemented as software running on a processor, e.g. a CPU or a GPU.</p><p id="p-0045" num="0065">A block diagram of a second embodiment of an apparatus <b>20</b> according to the invention for use in the system of <figref idref="DRAWINGS">FIG. <b>2</b></figref> at the side of the speaker is illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>. The apparatus <b>20</b> comprises a processing device <b>22</b> and a memory device <b>21</b>. For example, the apparatus <b>20</b> may be a computer, an embedded system, or part of a distributed system. The memory device <b>21</b> has stored instructions that, when executed by the processing device <b>22</b>, cause the apparatus <b>20</b> to perform steps according to one of the described methods. The instructions stored in the memory device <b>21</b> thus tangibly embody a program of instructions executable by the processing device <b>22</b> to perform program steps as described herein according to the present principles. The apparatus <b>20</b> has an input <b>23</b> for receiving data. Data generated by the processing device <b>22</b> are made available via an output <b>24</b>. In addition, such data may be stored in the memory device <b>21</b>. The input <b>23</b> and the output <b>24</b> may be combined into a single bidirectional interface.</p><p id="p-0046" num="0066">The processing device <b>22</b> as used herein may include one or more processing units, such as microprocessors, digital signal processors, or a combination thereof.</p><p id="p-0047" num="0067">The local storage unit <b>17</b> and the memory device <b>21</b> may include volatile and/or non-volatile memory regions and storage devices such as hard disk drives, optical drives, and/or solid-state memories.</p><p id="p-0048" num="0068"><figref idref="DRAWINGS">FIG. <b>5</b></figref> schematically illustrates a block diagram of a first embodiment of an apparatus <b>30</b> for use in the system of <figref idref="DRAWINGS">FIG. <b>2</b></figref> at the side of the recipient R. The apparatus <b>30</b> has an input <b>31</b> via which a receiving module <b>32</b> receives text T generated from an input speech utterance of a speaker. A text-to-speech conversion module <b>33</b> is configured to convert the transmitted text T into an output speech utterance U<sub>o </sub>that simulates a voice of the speaker. For this purpose, the text-to-speech conversion module <b>33</b> may use a text-to-speech algorithm. Preferably, such a text-to-speech algorithm uses a phoneme library suitable for simulating different speakers. Alternatively, the text-to-speech conversion module <b>33</b> may convert the transmitted text into an output speech utterance using one or more trained artificial intelligence models. For example, a first trained artificial intelligence model may transform the transmitted text into an intermediate speech utterance. A second trained artificial intelligence model then transforms the intermediate speech utterance into the output speech utterance. The second trained artificial intelligence model may be selected from a bank of trained artificial intelligence models, e.g. based on information about the speaker. Such information may be provided by the speaker or determined by a voice analysis algorithm. An output module <b>34</b> is configured to provide the output speech utterance U<sub>o </sub>to the recipient R via an output <b>37</b>. A local storage unit <b>36</b> is provided, e.g. for storing data during processing. The output <b>37</b> may also be combined with the input <b>31</b> into a single bidirectional interface.</p><p id="p-0049" num="0069">The various modules <b>32</b>-<b>34</b> may be controlled by a control module <b>35</b>. A user interface <b>38</b> may be provided for enabling a user to modify settings of the various modules <b>32</b>-<b>35</b>. The modules <b>32</b>-<b>35</b> of the apparatus <b>30</b> can be embodied as dedicated hardware units. Of course, they may likewise be fully or partially combined into a single unit or implemented as software running on a processor, e.g. a CPU or a GPU.</p><p id="p-0050" num="0070">A block diagram of a second embodiment of an apparatus <b>40</b> according to the invention for use in the system of <figref idref="DRAWINGS">FIG. <b>2</b></figref> at the side of the recipient is illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. The apparatus <b>40</b> comprises a processing device <b>42</b> and a memory device <b>41</b>. For example, the apparatus <b>40</b> may be a computer, an embedded system, or part of a distributed system. The memory device <b>41</b> has stored instructions that, when executed by the processing device <b>42</b>, cause the apparatus <b>40</b> to perform steps according to one of the described methods. The instructions stored in the memory device <b>41</b> thus tangibly embody a program of instructions executable by the processing device <b>42</b> to perform program steps as described herein according to the present principles. The apparatus <b>40</b> has an input <b>43</b> for receiving data. Data generated by the processing device <b>42</b> are made available via an output <b>44</b>. In addition, such data may be stored in the memory device <b>41</b>. The input <b>43</b> and the output <b>44</b> may be combined into a single bidirectional interface.</p><p id="p-0051" num="0071">The processing device <b>42</b> as used herein may include one or more processing units, such as microprocessors, digital signal processors, or a combination thereof.</p><p id="p-0052" num="0072">The local storage unit <b>36</b> and the memory device <b>41</b> may include volatile and/or non-volatile memory regions and storage devices such as hard disk drives, optical drives, and/or solid-state memories.</p><p id="p-0053" num="0073"><figref idref="DRAWINGS">FIG. <b>7</b></figref> depicts a system diagram of a first embodiment of a solution according to the invention. When the speaker S speaks, i.e. when an input speech utterance U<sub>i </sub>of the speaker S is received, a data connection of a VoIP device at the side of the speaker S is checked. In particular, the bandwidth or available data rate may be determined. If the connection is not good enough for transporting voice signals, the input speech utterance U<sub>i </sub>of the speaker S is converted to text T using a speech-to-text algorithm ASTT. The text T is transmitted over the communication network N. The received text T is then converted to voice with the help of a text-to-speech algorithm ATTS. The resulting output speech utterance U<sub>o </sub>is provided to the recipient R. For closely resembling the voice of the Speaker S, the text-to-speech algorithm ATTS makes use of a large phoneme library PL. As a result, even in case of a bad connection the recipient R will hear an output speech utterance U<sub>o </sub>that at least closely resembles the voice of the speaker S. The phoneme library PL may be located in the hardware used by the recipient R or in a cloud solution.</p><p id="p-0054" num="0074">If the connection at the side of the speaker S is good enough for voice transmission, the voice V is transmitted over the communication network N as VoIP. In this case, the input speech utterance U<sub>i </sub>may optionally still be converted to text T and transmitted in addition to the voice V. Depending on the data connection at the side of the recipient R, the system can make use of the received text T or discard it.</p><p id="p-0055" num="0075"><figref idref="DRAWINGS">FIG. <b>8</b></figref> depicts a system diagram of a second embodiment of a solution according to the invention. The solution is largely identical to the solution of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. However, in this case, the conversion of the text T into an output speech utterance U<sub>o </sub>is made using one or more trained artificial intelligence models AI. Details of the conversion from text T to speech are shown in FIG. <b>9</b>. The arrangement of the artificial intelligence models AIl, AI<b>2</b><sub>i </sub>in the figure constitutes a multimodal network. For any conversion from text to speech, processing is done by two artificial intelligence models. A first artificial intelligence model AIl converts the text T into an intermediate speech utterance U<sub>im </sub>in a digital format. The intermediate speech utterance U<sub>im </sub>is then provided to a second artificial intelligence model AI<b>2</b><sub>i</sub>, which is selected from a bank B of trained artificial intelligence models AI<b>2</b><sub>i</sub>. The artificial intelligence models AI<b>2</b><sub>i </sub>inside the bank B are individual models AI<b>21</b> trained with individual user voices. For the selection of a suitable artificial intelligence model AI<b>2</b><sub>i</sub>, information IS about the speaker is used. This information IS may, for example, be provided by the speaker or determined automatically using a voice analysis algorithm. The selected artificial intelligence model AI<b>2</b><sub>i </sub>manipulates the intermediate speech utterance U<sub>im </sub>created by the first artificial intelligence model AIl into another format in such a way that the resulting output speech utterance U<sub>o </sub>closely resembles the voice of the speaker. In other words, the first artificial intelligence model AIl converts the input data into another space, whereas the second artificial intelligence model AI<b>2</b><sub>i </sub>manipulates the data in the same space. In addition to the first artificial intelligence model AIl and the second artificial intelligence model AI<b>2</b><sub>i</sub>, a further artificial intelligence model (not shown) may be provided, which is responsible for synthesizing the tone or emotion of the speaker. This further artificial intelligence model may make use of tags that are sent along with the text. In this case, the speech-to-text algorithm on the sending side advantageously provides additional information about the speech utterance, such as an intonation, a speed of speech, detected emotions, durations of the individual words, etc.</p><p id="p-0056" num="0076"><figref idref="DRAWINGS">FIG. <b>10</b></figref> schematically shows a motor vehicle <b>50</b>, in which a solution in accordance with the invention is implemented. The motor vehicle <b>50</b> has an infotainment system <b>51</b>, which is able to establish a VoIP voice communication via a communication network. For this purpose, a data transmission unit <b>52</b> is provided. The motor vehicle <b>50</b> further has apparatuses <b>10</b>, <b>30</b> according to the invention, which are used for an improved voice communication. The apparatuses <b>10</b>, <b>30</b> may be provided as dedicated hardware units or included in the infotainment system <b>51</b>. A memory <b>53</b> is available for storing data. The data exchange between the different components of the motor vehicle <b>50</b> takes place via a network <b>54</b>.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method for voice communication between a speaker and a recipient over a communication network, the method comprising:<claim-text>receiving an input speech utterance from the speaker;</claim-text><claim-text>converting the input speech utterance to text;</claim-text><claim-text>transmitting at least the text over the communication network;</claim-text><claim-text>converting the transmitted text into an output speech utterance that simulates a voice of the speaker; and</claim-text><claim-text>providing the output speech utterance to the recipient.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising evaluating a bandwidth of a connection to the communication network at the side of the speaker.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein in case of a sufficiently large bandwidth, the input speech utterance is transmitted as voice and as text.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the transmitted text is converted into the output speech utterance by a text-to-speech algorithm.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the text-to-speech algorithm uses a phoneme library suitable for simulating different speakers.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the transmitted text is converted into the output speech utterance by one or more trained artificial intelligence models.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein a first trained artificial intelligence model transforms the transmitted text into an intermediate speech utterance and a second trained artificial intelligence model transforms the intermediate speech utterance into the output speech utterance.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the second trained artificial intelligence model is selected from a bank of trained artificial intelligence models.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the second trained artificial intelligence model is selected from the bank of trained artificial intelligence models based on information about the speaker.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the information about the speaker is provided by the speaker or determined by a voice analysis algorithm.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. A non-transitory computer-readable medium having stored thereon computer-executable instructions, which, when executed by at least one processor, cause the at least one processor to provide voice communication between a speaker and a recipient over a communication network by performing operations comprising:<claim-text>receiving an input speech utterance from the speaker;</claim-text><claim-text>converting the input speech utterance to text;</claim-text><claim-text>transmitting at least the text over the communication network;</claim-text><claim-text>converting the transmitted text into an output speech utterance that simulates a voice of the speaker; and</claim-text><claim-text>providing the output speech utterance to the recipient.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The non-transitory computer-readable medium according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, having stored thereon computer-executable instructions that, when executed, perform further operations comprising: evaluating a bandwidth of a connection to the communication network at the side of the speaker.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The non-transitory computer-readable medium according to <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein in case of a sufficiently large bandwidth, the input speech utterance is transmitted as voice and as text.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The non-transitory computer-readable medium according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the transmitted text is converted into the output speech utterance by a text-to-speech algorithm.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The non-transitory computer-readable medium according to <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the text-to-speech algorithm uses a phoneme library suitable for simulating different speakers.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The non-transitory computer-readable medium according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the transmitted text is converted into the output speech utterance by one or more trained artificial intelligence models.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The non-transitory computer-readable medium according to <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein a first trained artificial intelligence model transforms the transmitted text into an intermediate speech utterance and a second trained artificial intelligence model transforms the intermediate speech utterance into the output speech utterance.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The non-transitory computer-readable medium according to <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the second trained artificial intelligence model is selected from a bank of trained artificial intelligence models.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The non-transitory computer-readable medium according to <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the second trained artificial intelligence model is selected from the bank of trained artificial intelligence models based on information about the speaker.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The non-transitory computer-readable medium according to <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein the information about the speaker is provided by the speaker or determined by a voice analysis algorithm.</claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. A vehicle having a non-transitory computer-readable medium having stored thereon computer-executable instructions, which, when executed by at least one processor, cause the at least one processor to provide voice communication between a speaker and a recipient over a communication network by performing operations comprising:<claim-text>receiving an input speech utterance from the speaker;</claim-text><claim-text>converting the input speech utterance to text;</claim-text><claim-text>transmitting at least the text over the communication network;</claim-text><claim-text>converting the transmitted text into an output speech utterance that simulates a voice of the speaker; and</claim-text><claim-text>providing the output speech utterance to the recipient.</claim-text></claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. The vehicle according to <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the non-transitory computer-readable medium has stored thereon computer-executable instructions that, when executed, perform further operations comprising: evaluating a bandwidth of a connection to the communication network at the side of the speaker.</claim-text></claim><claim id="CLM-00023" num="00023"><claim-text><b>23</b>. The vehicle according to <claim-ref idref="CLM-00022">claim 22</claim-ref>, wherein in case of a sufficiently large bandwidth, the input speech utterance is transmitted as voice and as text.</claim-text></claim><claim id="CLM-00024" num="00024"><claim-text><b>24</b>. The vehicle according to <claim-ref idref="CLM-00023">claim 23</claim-ref>, wherein the transmitted text is converted into the output speech utterance by a text-to-speech algorithm.</claim-text></claim><claim id="CLM-00025" num="00025"><claim-text><b>25</b>. The vehicle according to <claim-ref idref="CLM-00024">claim 24</claim-ref>, wherein the text-to-speech algorithm uses a phoneme library suitable for simulating different speakers.</claim-text></claim><claim id="CLM-00026" num="00026"><claim-text><b>26</b>. The vehicle according to <claim-ref idref="CLM-00023">claim 23</claim-ref>, wherein the transmitted text is converted into the output speech utterance by one or more trained artificial intelligence models.</claim-text></claim><claim id="CLM-00027" num="00027"><claim-text><b>27</b>. The vehicle according to <claim-ref idref="CLM-00026">claim 26</claim-ref>, wherein a first trained artificial intelligence model transforms the transmitted text into an intermediate speech utterance and a second trained artificial intelligence model transforms the intermediate speech utterance into the output speech utterance.</claim-text></claim><claim id="CLM-00028" num="00028"><claim-text><b>28</b>. The vehicle according to <claim-ref idref="CLM-00027">claim 27</claim-ref>, wherein the second trained artificial intelligence model is selected from a bank of trained artificial intelligence models.</claim-text></claim><claim id="CLM-00029" num="00029"><claim-text><b>29</b>. The vehicle according to <claim-ref idref="CLM-00028">claim 28</claim-ref>, wherein the second trained artificial intelligence model is selected from the bank of trained artificial intelligence models based on information about the speaker.</claim-text></claim><claim id="CLM-00030" num="00030"><claim-text><b>30</b>. The vehicle according to <claim-ref idref="CLM-00029">claim 29</claim-ref>, wherein the information about the speaker is provided by the speaker or determined by a voice analysis algorithm.</claim-text></claim></claims></us-patent-application>