<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230000355A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221220" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230000355</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17870345</doc-number><date>20220721</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20180101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>16</class><subclass>H</subclass><main-group>50</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20170101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20180101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>16</class><subclass>H</subclass><main-group>70</main-group><subgroup>60</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>09</class><subclass>B</subclass><main-group>19</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20180101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>16</class><subclass>H</subclass><main-group>30</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20180101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>16</class><subclass>H</subclass><main-group>50</main-group><subgroup>30</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>R</subclass><main-group>33</main-group><subgroup>48</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20180101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>16</class><subclass>H</subclass><main-group>30</main-group><subgroup>40</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>055</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20180101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>16</class><subclass>H</subclass><main-group>50</main-group><subgroup>70</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20180101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>16</class><subclass>H</subclass><main-group>20</main-group><subgroup>30</subgroup><symbol-position>L</symbol-position><classification-value>N</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>0042</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20180101</date></cpc-version-indicator><section>G</section><class>16</class><subclass>H</subclass><main-group>50</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>0014</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20180101</date></cpc-version-indicator><section>G</section><class>16</class><subclass>H</subclass><main-group>70</main-group><subgroup>60</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>09</class><subclass>B</subclass><main-group>19</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20180101</date></cpc-version-indicator><section>G</section><class>16</class><subclass>H</subclass><main-group>30</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20180101</date></cpc-version-indicator><section>G</section><class>16</class><subclass>H</subclass><main-group>50</main-group><subgroup>30</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>R</subclass><main-group>33</main-group><subgroup>4806</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20180101</date></cpc-version-indicator><section>G</section><class>16</class><subclass>H</subclass><main-group>30</main-group><subgroup>40</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>055</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20180101</date></cpc-version-indicator><section>G</section><class>16</class><subclass>H</subclass><main-group>50</main-group><subgroup>70</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30016</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>2576</main-group><subgroup>026</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>2503</main-group><subgroup>10</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30104</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>10088</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20180101</date></cpc-version-indicator><section>G</section><class>16</class><subclass>H</subclass><main-group>20</main-group><subgroup>30</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">METHODS AND APPARATUS FOR DETECTING INJURY USING MULTIPLE TYPES OF MAGNETIC RESONANCE IMAGING DATA</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16756286</doc-number><date>20200415</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11419498</doc-number></document-id></parent-grant-document><parent-pct-document><document-id><country>WO</country><doc-number>PCT/IB2018/001307</doc-number><date>20181016</date></document-id></parent-pct-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17870345</doc-number></document-id></child-doc></relation></continuation><us-provisional-application><document-id><country>US</country><doc-number>62573027</doc-number><date>20171016</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Voxel AI, Inc.</orgname><address><city>Toronto</city><country>CA</country></address></addressbook><residence><country>CA</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Gallacher</last-name><first-name>Benjamin J.A.</first-name><address><city>Toronto</city><country>CA</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Cook</last-name><first-name>Douglas J.</first-name><address><city>Toronto</city><country>CA</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Murray</last-name><first-name>Christopher I.</first-name><address><city>Toronto</city><country>CA</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Ross</last-name><first-name>Andrew N.</first-name><address><city>Toronto</city><country>CA</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>Voxel AI, Inc.</orgname><role>03</role><address><city>Toronto</city><country>CA</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Methods and apparatus for predicting performance of an individual on a task, the method comprises receiving brain imaging data for the individual, wherein the brain imaging data comprises structural brain data, determining values for at least one characteristic of the structural brain data within regions of interest defined for a population of individuals having different performance levels, and predicting based on the determined values, a performance potential of the individual.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="105.49mm" wi="100.33mm" file="US20230000355A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="130.64mm" wi="102.36mm" file="US20230000355A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="131.91mm" wi="149.44mm" file="US20230000355A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="238.84mm" wi="116.08mm" file="US20230000355A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="124.38mm" wi="107.36mm" file="US20230000355A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="170.52mm" wi="124.38mm" file="US20230000355A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="132.00mm" wi="108.71mm" file="US20230000355A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="134.54mm" wi="105.16mm" file="US20230000355A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="141.99mm" wi="140.72mm" file="US20230000355A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="211.58mm" wi="145.20mm" orientation="landscape" file="US20230000355A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="213.78mm" wi="145.03mm" orientation="landscape" file="US20230000355A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="209.47mm" wi="53.34mm" orientation="landscape" file="US20230000355A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="216.75mm" wi="109.39mm" orientation="landscape" file="US20230000355A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="216.83mm" wi="112.61mm" orientation="landscape" file="US20230000355A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="225.21mm" wi="100.41mm" orientation="landscape" file="US20230000355A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="224.20mm" wi="107.02mm" orientation="landscape" file="US20230000355A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application is a Continuation of U.S. patent application Ser. No. 16/756,286, filed Apr. 15, 2020, entitled, &#x201c;METHODS AND APPARATUS FOR USING BRAIN IMAGING TO PREDICT PERFORMANCE,&#x201d; which is a national stage filing under 35 U.S.C. &#xa7; 371 of international application number PCT/IB2018/001307, filed Oct. 16, 2018, entitled &#x201c;METHODS AND APPARATUS FOR USING BRAIN IMAGING TO PREDICT PERFORMANCE,&#x201d; which claims the benefit under 35 U.S.C. &#xa7; 119(e) of U.S. Provisional Patent Application Ser. No. 62/573,027, filed Oct. 16, 2017, and entitled &#x201c;METHODS AND APPARATUS FOR USING BRAIN IMAGING TO PREDICT PERFORMANCE,&#x201d; the entire contents each of which is incorporated by reference herein.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">Evaluation of a person's performance on a task is often based in part on qualitative observations of the person as he/she performs the task. For example, scouts often observe the play of college athletes when considering whether they would be successful performing on a professional sports team. Quantitative measurements, such as performance statistics are also often used to evaluate how well a particular person has performed a task. For example, statistics describing how well a particular investor's investment choices yielded positive returns for his/her client may be used to evaluate how skilled the investor is developing successful investment strategies.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0004" num="0003">Some embodiments are directed to a computerized system for predicting performance. The system comprises at least one computer processor and at least one computer-readable medium encoded with a plurality of instructions that, when executed by the at least one computer processor, perform a method of predicting performance of an individual. The method comprises receiving brain imaging data for the individual, wherein the brain imaging data comprises structural brain data, determining first values for at least one characteristic of the structural brain data within regions of interest defined for a population of individuals having different performance levels, and predicting based, at least in part, on the first values, a performance potential of the individual.</p><p id="p-0005" num="0004">Some embodiments are directed to a computer-implemented method for predicting performance. The method comprises receiving brain imaging data for the individual, wherein the brain imaging data comprises structural brain data, determining first values for at least one characteristic of the structural brain data within regions of interest defined for a population of individuals having different performance levels, and predicting, based on the first values, a performance potential of the individual.</p><p id="p-0006" num="0005">Some embodiments are directed to a computerized system for identifying brain regions that predict differences in task performance. The system comprises at least one computer processor and at least one computer-readable medium encoded with a plurality of instructions that, when executed by the at least one computer processor, perform a method. The method comprises receiving brain imaging data for each of a plurality of individuals in a reference cohort, wherein the brain imaging data comprises structural brain data characterizing a static state of a brain and physiological brain data characterizing a dynamic state of the brain, receiving functional skill data for each of the plurality of individuals in the reference cohort, wherein the functional skill data indicates a performance score for each of the plurality of individuals on one or more tasks, determining, based, at least in part, on at least one first characteristic of the structural brain data, at least one second characteristic of the physiological brain data, and the functional skill data, brain regions associated with differences in performance on the one or more tasks, and outputting the determined brain regions as a set of predictive regions of interest.</p><p id="p-0007" num="0006">Some embodiments are directed to a computer-implemented method for identifying brain regions that predict differences in task performance. The method comprises receiving brain imaging data for each of a plurality of individuals in a reference cohort, wherein the brain imaging data comprises structural brain data characterizing a static state of a brain and physiological brain data characterizing a dynamic state of the brain, receiving functional skill data for each of the plurality of individuals in the reference cohort, wherein the functional skill data indicates a performance score for each of the plurality of individuals on one or more tasks, determining, based, at least in part, on at least one first characteristic of the structural brain data, at least one second characteristic of the physiological brain data, and the functional skill data, brain regions associated with differences in performance on the one or more tasks, and outputting the determined brain regions as a set of predictive regions of interest.</p><p id="p-0008" num="0007">It should be appreciated that all combinations of the foregoing concepts and additional concepts discussed in greater detail below (provided such concepts are not mutually inconsistent) are contemplated as being part of the inventive subject matter disclosed herein. In particular, all combinations of claimed subject matter appearing at the end of this disclosure are contemplated as being part of the inventive subject matter disclosed herein.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0009" num="0008">Various non-limiting embodiments of the technology will be described with reference to the following figures. It should be appreciated that the figures are not necessarily drawn to scale.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a flowchart of a process for generating a predictive model of performance in accordance with some embodiments;</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a schematic of a brain including a structural functional unit in accordance with some embodiments;</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flowchart of a process for generating a predictive model that includes multiple brain imaging data types in accordance with some embodiments;</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flowchart of a process for performing region of interest screening in accordance with some embodiments;</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flowchart of a process for iteratively identifying a set of regions of interest in accordance with some embodiments;</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flowchart of a process for predicting performance based on an analysis of structural and physiological brain data in accordance with some embodiments;</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart of a process for predicting performance based on an analysis of structural and physiological brain data within regions of interest in accordance with some embodiments;</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a block diagram of a computer system on which some embodiments may be implemented;</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a plot showing examples of corticospinal tracts in a variety of athletes;</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a plot showing examples of corpus callosum in a variety of athletes;</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>11</b>A</figref> schematically illustrates how the brain regions were defined to represent basic components of athletic performance that combine for overall performance in accordance with some embodiments;</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>11</b>B</figref> and <figref idref="DRAWINGS">FIG. <b>11</b>C</figref> show results of an example analysis for each component shown in <figref idref="DRAWINGS">FIG. <b>11</b>A</figref> and overall potential demonstrating the correlation between neurological feature selection and athlete performance; and</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>11</b>D</figref> and <figref idref="DRAWINGS">FIG. <b>11</b>E</figref> are plots is a plot showing the same analysis used to generate the results for <figref idref="DRAWINGS">FIG. <b>11</b>B</figref> and <figref idref="DRAWINGS">FIG. <b>11</b>C</figref> as applied to prospective professional hockey players.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0023" num="0022">The inventors have recognized and appreciated that conventional qualitative and quantitative techniques for evaluating the performance of an individual to perform a particular task or set of tasks may be improved by considering physiological and neurological data that is not typically used to evaluate task performance. Additionally, conventional quantitative measures used to evaluate an individual's performance typically only consider how well the person performs the task by, for example, assigning them a score (e.g., the individual scored 20/50 on the Wonderlic Personnel Test), without considering the individual's potential for performance on the task. To this end, some embodiments are directed to techniques for using brain imaging to estimate both an individual's potential for performance on one or more tasks and a current level of performance for the individual in performing the task. By estimating both potential and current performance, it is possible to assess whether the individual can be trained to improve performance (i.e., if there is room for improvement within their potential) or whether the individual is currently performing near their potential and does not have much room to improve through training. Based, at least in part, on a result of the analysis techniques described herein, individualized deficits can be identified for improvement and personalized training programs may be designed to improve an individual's performance within their potential level of performance.</p><p id="p-0024" num="0023">The inventors have recognized and appreciated that one or more measures of brain structure may be used to assess performance potential and one or more measures of brain physiology may be used to assess current performance within an individual's potential. To this end, some embodiments are directed to using structural and physiological brain data to predict an individual's performance on a task using a classifier trained to associate brain data with performance metrics. Some embodiments are directed to techniques for developing predictive models of performance by training a model (e.g., a neural network) using brain imaging data extracted from a cohort of high performing and lower performing subjects, as discussed in more detail below. Applications of estimating the performance using brain data in accordance with the techniques described herein include, but are not limited to assessing athletic performance, military performance, executive performance, and financial analysis performance.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a process <b>100</b> for generating a predictive model of performance (e.g., a trained classifier) in accordance with some embodiments. In act <b>102</b>, brain imaging data is acquired from a plurality of individuals having different skill levels. For example, the plurality of individuals may include a group of elite athletes (e.g., Olympic-level athletes, professional athlete all-stars) and a group of individuals who are not elite athletes (e.g., non-athletes, casual athletes).</p><p id="p-0026" num="0025">In some embodiments, the obtained brain imaging data includes structural brain data and physiological brain data associated with a &#x201c;structural-functional unit&#x201d; (SFU) that represents a functional network of connected nodes in the brain that are activated together to perform various cognitive, sensory, motor, or sensorimotor tasks. Examples of SFUs include, but are not limited to, a visuospatial network, a sensorimotor network, a default mode network, a working memory network, a salience network, an executive control network, a language network, and a motivation reward network. <figref idref="DRAWINGS">FIG. <b>2</b></figref> schematically illustrates the concept of an SFU that includes a plurality of gray matter regions of the brain and white matter tracts connecting the gray matter regions. For example, the illustrative SFU shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, includes three gray matter regions <b>202</b>, <b>204</b> and <b>206</b> and white matter tracts (e.g., white matter tract <b>210</b>) connecting the gray matter regions. Although shown as relatively large regions for illustration, it should be appreciated that the gray matter regions included in an SFU may be any size or shape. Additionally, although the white matter tracts in the illustrated SFU are shown as single lines connecting the gray matter regions, it should be appreciated that any suitable number of white matter tracts may connect gray matter regions in an SFU, and embodiments are not limited in this respect. SFUs may be defined, for example, based on the scientific literature, functional neuroimaging, or in any other suitable way.</p><p id="p-0027" num="0026">In some embodiments, the structural brain data and physiological brain data are acquired using magnetic resonance imaging (MRI). It should be appreciated, however, that in other embodiments one or both of the structural brain data and physiological brain data may be acquired using a brain imaging technique other than MRI including, but not limited to, computed tomography (CT), positron emission tomography (PET) and optical imaging (e.g., optical coherence tomography OCT)). Additionally, multiple scans using different imaging parameters (e.g., to achieve different contrasts) or experimental conditions (e.g., different tasks) may be used to obtain the structural brain data and/or the physiological brain data.</p><p id="p-0028" num="0027">In some embodiments, the structural brain data includes multiple types of structural brain data including, but not limited, to structural white matter connectivity data (e.g., obtained using diffusion tensor imaging (DTI)), white matter volume data, and gray matter volume data. One or more of the types of structural brain data may further include particular metrics used to characterize regions of the brain. For example, diffusion tensor imaging data may be used to evaluate for a voxel or group of voxels, one or more of fractional anisotropy (FA), mean diffusivity (MD), axial diffusivity (AD) and radial diffusivity (RD). The structural brain data may characterize a static state of the brain in that the structural connections do not change during performance of a task.</p><p id="p-0029" num="0028">In some embodiments, the physiological brain data includes multiple types of physiological brain data including, but not limited to, functional neuroimaging data (e.g., to assess resting state functional connectivity), cerebral blood flow data, cerebral vascular reactivity data, electroencephalography, function near infrared spectroscopy, near infrared spectroscopy, magnetoencephalography, computed tomography perfusion, transcranial Doppler, positron emission tomography and single photon emission computed tomography. In some embodiments, at least some of the physiological brain data may characterize a blood oxygen level dependent (BOLD) response within different regions of the brain (e.g., voxel or group of voxels) over time. In such a way, the physiological brain data captures dynamics of the brain in contrast to the structural brain data, described above.</p><p id="p-0030" num="0029">Brain data obtained from a plurality of individuals may be transformed into common brain space representation for analysis. For example, the obtained brain data may be transformed into the standardized Montreal Neurological Institute (MNI) brain coordinate space or the data may be transformed into another brain space representation (e.g., a brain space template defined by acquired brain data) that facilitates analysis in a common brain space. Additionally, at least some of the obtained brain data may be transformed to have a same voxel size. For example, brain data of a first type may be downsampled (or upsampled) to have a spatial resolution corresponding to brain data of a second type within the common brain space.</p><p id="p-0031" num="0030">Returning to process <b>100</b>, after the brain imaging data is obtained, the process proceeds to act <b>104</b>, where at least one structural measure is extracted from the structural brain data and at least one physiological measure is extracted from the physiological brain data. Examples of extracting structural measures and physiological measures from obtained brain data are described in more detail below. Process <b>100</b> then proceeds to act <b>106</b> where functional skill data is obtained from the plurality of individuals from whom the brain imaging data was obtained in act <b>102</b>. The functional skill data may be obtained, for example, by presenting the plurality of individuals with a set of standardized tasks and recording responses to the performance of the tasks. Process <b>100</b> then proceeds to act <b>108</b>, where a predictive model of performance is generated based, at least in part, on the obtained functional skill data and the structural and physiological measures extracted from the brain imaging data. As discussed in more detail below, in some embodiments, the predictive model is a classifier (e.g., a neural network) having parameters determined based on the functional skill data, the structural brain measures, and/or the physiological brain measures. In some embodiments, the predictive model may be generated based on multiple predictive models, each of which is generated for predicting performance on one or more particular tasks or sub-tasks, examples of which are described in more detail below.</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates a process <b>300</b> for analyzing obtained brain imaging data to extract structural and physiological brain measures used to generate one or more models for predicting performance in accordance with some embodiments. In act <b>302</b>, a structural functional unit (SFU) is selected for analysis. Rather than analyzing every voxel in the obtained brain imaging data volume, the selected SFU is used to constrain the analysis in process <b>300</b> to voxels represented within the gray matter nodes and/or white matter tracts connecting the gray matter nodes of the SFU.</p><p id="p-0033" num="0032">Process <b>300</b> then proceeds to act <b>304</b> where a functional skill task or sub-task is selected for analysis. As discussed above in connection with act <b>106</b> of process <b>100</b>, each of the plurality of individuals included in a reference cohort perform a plurality of functional skill tasks to obtain function skill data. In some embodiments, each of the individuals in the reference cohort performs a battery of standardized tests (e.g., the KINARM Standard Tests available from BKIN Products, Kingston, Ontario, Canada), and scores representing the individual's performance on each of the tests are determined and recorded. Some of the standardized tests may include multiple sub-tasks or metrics that capture different aspects of the individual's performance during the tests. Rather than selecting a particular standardized test in act <b>304</b>, one of the sub-tasks or metrics included in a test may instead be selected.</p><p id="p-0034" num="0033">Process <b>300</b> then proceeds to act <b>306</b>, where a particular data type from the obtained brain imaging data is selected. As discussed above, the brain imaging data may include structural brain data and physiological data, each of which may also comprise multiple data types. As an example, the structural brain data may include structural brain data obtained using diffusion tensor imaging, gray matter volume data, and white matter volume data, each corresponding to a different data type that may be selected in act <b>306</b>. Similarly, the physiological data may include multiple data types each of which may be selected for analysis in act <b>306</b>.</p><p id="p-0035" num="0034">Process <b>300</b> then proceeds to act <b>308</b>, where region of interest (ROI) screening is performed for the selected data type, structural functional unit, and task or sub-task or metric. The objective of the ROI screening is to identify regions of the brain included in the selected SFU that characterize differences in performance on the selected task, sub-task, or metric. The output of the ROI screening process in act <b>308</b> is a set of brain regions (e.g., voxels or groups of voxels) and associated values representing a prediction strength describing how strongly the voxels in each brain region are predictive of strong vs. poor performance on the selected task, sub-task or metric. An example of performing ROI screening in accordance with some embodiments is discussed in more detail with regard to <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0036" num="0035">Process <b>300</b> then proceeds to act <b>310</b>, where it is determined whether there are additional data types to analyze. If it is determined in act <b>310</b> that there are additional data types to analyze, process <b>300</b> returns to act <b>306</b>, where a new data type is selected and the ROI screening process in act <b>308</b> is repeated for the new data type. The resulting ROI screening process being a new set of brain regions within the SFU and associated values within each brain region for the selected brain data type that differentiate strong vs. poor performance on the selected task, sub-task, or metric. The process of acts <b>306</b>-<b>308</b> repeats until it is determined in act <b>310</b> that there are no other data types to analyze.</p><p id="p-0037" num="0036">When it is determined in act <b>310</b> that there are no other data types to analyze, process <b>300</b> proceeds to act <b>312</b>, where a multi-channel voxel map is created based on the output of the ROI screening process at each iteration of act <b>308</b> that is performed in process <b>300</b>. As described above, a structural functional unit defines a set of brain regions (e.g., gray matter regions) and connections (e.g., white matter tracts) between the brain regions. Within the brain regions defined by the SFU, each iteration through acts <b>306</b>-<b>308</b> results in a subset of those regions in the SFU being identified as predictive of strong vs. poor performance differences on a task, sub-task, or metric. Accordingly, the output of the ROI screening process in act <b>308</b> is a spatial map of brain regions within the SFU that characterize the differential task/sub-task/metric performance for a particular data type. For example, if the data types include structural connectivity, resting state functional connectivity, and cerebral blood flow, the output of the ROI screening process in act <b>308</b> would be a map for each of these data types. Conceptualized another way, each voxel within the SFU may be associated with multiple &#x201c;channels,&#x201d; where each channel represents a different data type. Continuing with the example above, each voxel in the SFU may be associated with a tuple of channels&#x2014;(structural connectivity, resting state functional connectivity, cerebral blood flow). As described in more detail below, the values ascribed to each channel for each voxel may be determined using a predictive model (e.g., a neural network) that reflects the strength of that voxel for predicting differences between strong and weak performers using the particular data type associated with the channel. The resultant map created in act <b>312</b> is then a spatial map of voxels in an SFU, where each of the voxels is associated with multiple channels for the different data types. In some embodiments, a visualization of the created multi-channel voxel map may be created and displayed (e.g., by using a color space (e.g., RGB, CMYK) to represent the different channels for each voxel.</p><p id="p-0038" num="0037">After creating a multi-channel voxel map for a particular SFU and task, sub-task, or metric, process <b>300</b> proceeds to act <b>314</b>, where a predictive model is used to weight the voxels within each identified ROI, such that the value for each channel in the multi-channel voxel map reflects the strength of that voxel for predicting differences between strong and weak performers using the particular data type associated with the channel trained based on the values in the multi-channel voxel map. Examples of predictive models that may be used in accordance with some embodiments include, but are not limited to neural networks (e.g., convolutional neural networks), support vector machines, and random forest classifiers. In one embodiments, a convolutional neural network regression analysis may be used to weight the ROI voxels that best predict task performance.</p><p id="p-0039" num="0038">Process <b>300</b> then proceeds to act <b>318</b>, where it is determined whether there are more tasks, sub-tasks, or metrics to analyze. If it is determined in act <b>318</b> that there are additional tasks, sub-tasks, or metrics to analyze, process <b>300</b> returns to act <b>304</b>. For example, if one of the standardized tests included ten different individual metrics, acts <b>304</b>-<b>316</b> may be performed for each of the ten different individual metrics, resulting in ten different predictive models of performance for the corresponding metrics. When it is determined in act <b>318</b> that all tasks, sub-tasks, or metrics have been analyzed, process <b>300</b> proceeds to act <b>320</b>, where it is determined whether there are additional SFUs to analyze. If it is determined in act <b>320</b> that there are additional SFUs to analyze, process <b>300</b> returns to act <b>302</b>, where a new SFU is selected, and acts <b>302</b>-<b>318</b> repeat until it is determined in act <b>320</b> that all SFUs have been analyzed, with the resultant output of process <b>300</b> being a plurality of predictive models of performance for each of a plurality of structural functional units. The predictive models of performance may be combined in any suitable way into a combined predictive model for performance that may be used to predict performance for a new individual not included in the reference cohort.</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates a process <b>400</b> for performing ROI screening in accordance with some embodiments. In act <b>402</b>, the ROI analysis is constrained based on the selected structural functional network being analyzed. As discussed above, each time that the ROI screening is performed, only voxels within brain regions included in a particular SFU are considered. Process <b>400</b> then proceeds to act <b>404</b>, where a volume slider is used to identify ROIs within the SFU that reflect differences in performance on a task, sub-task or metric as described above. The volume slider comprises a group of contiguous pixels over which a local transformation is performed to obtain a value (e.g., an average value) for the slider volume. For example, if the volume slider is a 3&#xd7;3&#xd7;3 cube that includes 27 pixels, an average value across the 27 pixels in the volume slider may be calculated and assigned to the voxel in the center of volume slider. The volume slider may then slide to a new position within the SFU and a new value for the volume slider may be calculated. Using a volume slider in accordance with some embodiments accommodates registration errors when individual brain data is registered to a standard brain space (e.g., the Montreal Neurological Institute (MNI) standard brain) and also accounts for variability in brain anatomy between individuals. Although the example above describes using a cube-shaped volume slider having 3&#xd7;3&#xd7;3 pixels, it should be appreciated that other size and shape volume sliders may alternatively be used, and embodiments are not limited in this respect.</p><p id="p-0041" num="0040">In some embodiments, the volume slider is used to determine an average value for each voxel (or sub-volume) within an SFU for each individual in the reference cohort. To determine how each voxel is predictive of performance on a selected task, sub-task or metric, a voxel-by-voxel numerical comparison (e.g., using a Student's t-test) is made between values for a first group of subjects that performed well on the task/sub-task/metric (e.g., the top 20% performers) and values for a second group of subjects that performed poorly on the task/sub-task/metric (e.g., the bottom 20% performers) to identify ROIs that are predictive of the task/sub-task/metric performance differences. For example, the ROIs may be identified by thresholding the values of the numerical comparison to omit those voxels (or sub-volumes) that are less than a threshold value.</p><p id="p-0042" num="0041">Process <b>400</b> then proceeds to act <b>406</b>, where a mask is created based on the identified ROIs that characterize the performance differences. Process <b>400</b> then proceeds to act <b>408</b> where data within each ROI is scored to produce the resultant spatial map for the particular data type, SFU, and task/sub-task/metric.</p><p id="p-0043" num="0042">The inventors have recognized that when identifying ROIs by analyzing brain imaging data that includes structural brain data and physiological brain data, it may be advantageous to iteratively refine the ROIs identified for a particular task by holding either structure constant or physiology constant while varying the other variable. <figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates a process <b>500</b> for iteratively identifying ROIs for a particular task in accordance with some embodiments. In act <b>502</b>, an ROI screening process is performed for each of multiple sets of structural brain data types (e.g., DTI data, gray matter volume). The scores for the multiple sets of structural brain data may then be combined. For example, the scores can be averaged or the scores may be combined in some other manner (e.g., a weighted average). Process <b>500</b> then proceeds to act <b>504</b>, where a subset of the individuals in the reference cohort are selected based on the ROI analysis of the structural data. For example, all individuals in the reference cohort may be ranked according to a measure of &#x201c;structural quality,&#x201d; and the top 50% of structural scoring subjects may be selected in act <b>504</b>. Process <b>500</b> then proceeds to act <b>506</b>, where an ROI screening process is performed for each of multiple sets of physiological brain data types (e.g., cerebral blood flow, resting state functional connectivity), as discussed above in connection with <figref idref="DRAWINGS">FIG. <b>4</b></figref>. Process <b>500</b> then proceeds to act <b>508</b> where it is determined whether the identified ROI masks output from act <b>506</b> are similar enough to the ROI masks output from act <b>502</b>. Any suitable measure of similarity may be used to determine whether the ROI masks have reached &#x201c;steady state&#x201d; in act <b>508</b>. If it is determined in act <b>508</b> that steady state of the output ROI masks has not been achieved, process <b>500</b> proceeds to act <b>510</b>, where a subset of the individuals in the reference cohort are selected based, at least in part, on their physiological scores. For example, the top 50% of physiological scoring subjects may be selected in act <b>510</b>. Process <b>500</b> then returns to act <b>502</b>, where an ROI screening process for structural brain data is performed on the subset of the individuals in the reference cohort (e.g., the top 50% physiological scoring subjects output from act <b>506</b>). Process <b>500</b> continues until it is determined in act <b>508</b> that the ROI mask output from the ROI screening process has reached a steady state.</p><p id="p-0044" num="0043">The inventors have recognized and appreciated that structural brain data (e.g., white matter organization in white matter tracts within an SFU), which reflects a static state of the brain, may be used to determine an individual's capacity, ceiling, or &#x201c;potential&#x201d; with respect to task performance, and that physiological brain data (e.g., cerebral blood flow), which reflects dynamics of the brain, may be used to determine a current performance level within the performance range established by the structural brain data analysis of performance potential. Differences between the performance potential and the current performance level may be referred to as a performance &#x201c;deficit,&#x201d; which represents a degree to which training the individual is likely to be effective in improving task performance up to the individual's measured potential. One or more predictive models generated in accordance with the techniques described herein may be used to predict a new individual's potential performance and current performance level relative to the potential performance level. To the extent that there is a difference between the individual's current performance level and potential performance level, a personalized training recommendation may be made to enable the individual to increase their current performance level to a level closer to their potential performance level by reducing their performance deficit.</p><p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates a process <b>600</b> for determining a personalized training recommendation for an individual in accordance with some embodiments. In act <b>602</b>, brain imaging data that includes structural data (e.g., DTI data) is obtained for an individual. The obtained brain imaging data may be transformed into a standard brain space (e.g., MNI coordinate space) and the ROI mask(s) determined based on the individuals in the reference cohort may be used to analyze the structural data. For example, within each of the ROIs defined by the ROI mask(s) values or scores (e.g., Z-scores) may be determined, and the scores may be used to determine a performance potential of the individual based on an analysis of the individual's structural data within the ROIs. Having established the individual's performance potential by analyzing the obtained structural data, process <b>600</b> proceeds to act <b>604</b>, where physiological brain data (e.g., cerebral blood flow) obtained from the individual is used to determine the individual's current performance level by, for example, analyzing the physiological brain data within the ROIs defined by the ROI mask(s) determined based on the individuals in the reference cohort. After determining the individual's current performance level, process <b>600</b> proceeds to act <b>606</b>, where a training recommendation is output based, at least in part, on the determined performance potential and current performance level. For example, a training recommendation may be made based, at least in part, on imbalances in structure, physiology, or both, and/or inefficiencies, for example, as detected in the individual's physiology relative to normalized control data.</p><p id="p-0046" num="0045">In some embodiments, separate predictive models are generated and subsequently used to analyze structural brain data and physiological brain data acquired from an individual to predict performance. In other embodiments, a single predictive model that takes as input both structural measures determined from structural brain data and physiological measures determined from physiological brain data, is used to predict one or more performance measures for an individual.</p><p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates a process <b>700</b> within which a single predictive model is used. In act <b>702</b>, brain imaging is performed on the individual, wherein the brain imaging includes at least one structural scan to obtain structural brain data and/or at least one physiological scan to obtain physiological brain data. Process <b>700</b> then proceeds to act <b>704</b>, where structural and physiological analyses are performed on the obtained structural and physiological brain data. For example, ROIs defined by analyzing the reference cohort of individuals, which represent brain regions within which differences between strong and weak performers were identified may be used to analyze the obtained structural and physiological brain data. The output of the ROI analysis may be a set of values or scores. Alternatively, the output of the ROI analysis may be a set of features that characterize the structural and physiological brain imaging data. Process <b>700</b> then proceeds to act <b>706</b>, where one or more performance measures are predicted based, at least in part, on the structural and/or physiological analysis output from act <b>704</b>. In some embodiments the predicted performance may be a classification of the individual into one of multiple performance groups (e.g., strong performer, average performer, poor performer). In other embodiments the predicted performance may be represented by one or more numerical values (e.g., on a scale of 0-100) that represent the predicted performance measures.</p><p id="p-0048" num="0047">An illustrative implementation of a computer system <b>800</b> that may be used in connection with any of the embodiments of the disclosure provided herein is shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>. The computer system <b>800</b> includes one or more computer hardware processors <b>810</b> and one or more articles of manufacture that comprise non-transitory computer-readable storage media (e.g., memory <b>820</b> and one or more non-volatile storage devices <b>830</b>). The processor(s) <b>810</b> may control writing data to and reading data from the memory <b>820</b> and the non-volatile storage device(s) <b>830</b> in any suitable manner. To perform any of the functionality described herein, the processor(s) <b>810</b> may execute one or more processor-executable instructions stored in one or more non-transitory computer-readable storage media (e.g., the memory <b>820</b>), which may serve as non-transitory computer-readable storage media storing processor-executable instructions for execution by the processor(s) <b>810</b>.</p><p id="p-0049" num="0048">In some embodiments, computer system <b>800</b> also includes a brain imaging system (e.g., an MRI system) <b>850</b> that provides brain imaging data to processor(s) <b>810</b>. Brain imaging system <b>850</b> may be communicatively coupled to processor(s) <b>810</b> using one or more wired or wireless communication networks. In some embodiments, processor(s) <b>810</b> may be integrated with the brain imaging system in an integrated device. For example, processor(s) <b>810</b> may be implemented on a chip arranged within a device that also includes brain imaging system <b>850</b>.</p><p id="p-0050" num="0049">Brain imaging system <b>850</b> may be configured to perform brain imaging on an individual to obtain structural brain data and physiological brain data. For example, the brain imaging system may be a magnetic resonance imaging system configured to acquire structural data from the individual using one more structural imaging scans and physiological data from the individual using one more functional imaging scans. The brain imaging data determined from the brain imaging system <b>850</b> may then be provided to the processor(s) <b>810</b> for inclusion in a performance prediction analysis, as described above.</p><p id="p-0051" num="0050">In some embodiments, computer system <b>800</b> also includes a user interface <b>840</b> in communication with processor(s) <b>810</b>. The user interface <b>800</b> may be configured to provide a training recommendation to a healthcare professional based, at least in part, on the results of performance prediction analysis output from processor(s) <b>810</b>.</p><heading id="h-0006" level="1">Illustrative Example</heading><p id="p-0052" num="0051">In this example, the left and right corticospinal tracts of several high performing elite athletes were examined. The corticospinal tract is a white matter tract that has a clear connection with athletics, as it comprises the primary bundle of axons that transmits neural signals developed in the primary motor cortex to the spinal cord and out to skeletal muscles through the peripheral nervous system. As a first step, fractional anisotropy (FA) values obtained from DTI scans of the left and right corticospinal tracts were examined. From a visual inspection it was possible to observe some distinct patterns. Highly elite athletes showed a very high FA values in their centrum semiovale as shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref>. The centrum semiovale is a brain region involved in coordinating multiple inputs from various areas of the motor cortex. High FA values were also observed in the cerebral peduncle, the site where several motor fibres exit the corticospinal tract. This pattern was also notable because elite athletes also displayed a higher degree of symmetry between the left and right tract than non-elite athletes and the greatest FA values were found in the tract associated with their dominant hand or dominant style of play.</p><p id="p-0053" num="0052">Based on these initial observations, ten brain regions and functional networks were selected to evaluate. The ten brain regions and functional networks were organized based on the principle that the primary components of athletic performance involve an athlete's ability to see a situation (Vision/See), determine a course of action (Process) and react based on that decision (Movement/Move). A total of 442 data points were extracted from the various brain regions. Examples of the average FA values extracted across 2 mm slices of the corpus callosum (a brain region for Process) can been see in <figref idref="DRAWINGS">FIG. <b>10</b></figref>. Interestingly, a distinct high FA band in the central posterior region of corpus callosum, known as the body, which was present more often in high performing elite athletes. This region includes the callosal motor and callosal premotor fibres that are involved in motor planning, execution and coordination. The premotor fibres are also connected to association networks in the parietal networks and play an important role of self-awareness in space.</p><p id="p-0054" num="0053">To predict athletic potential from the data extracted from the MRI scans of current NHL players, a set of algorithmic predictive models were generated for each aspect of performance in accordance with the techniques described above. To define each algorithm, a dataset in which players' MRI data for the individual brain regions was selected and compared against the players' overall NHL performance. Hockey performance was determined using a basket of advanced hockey specific metrics that are intended to evaluate a player's overall contribution. In this example, Wins Above Replacement (WAR), Game Score, CorsiFor % and Point Shares were used (advanced hockey analytics were obtained from Corsica.hockey and Hockey-Reference.com). Each advanced stat was calculated as a NHL career average and an average of a player's top three NHL seasons. Player metrics were assigned a percentile rank compared with all other active NHL players in their position over the same time period. For each brain region or network, a machine learning support vector approach was applied to perform the regression modeling, described above. The brain data was regressed against each hockey metric (a total of 8) and the top 5-6 were averaged and combined to derive each primary component score. An example of the performance for each of the See, Process and Move models as well as an overall score is shown in <figref idref="DRAWINGS">FIG. <b>11</b>B</figref> and <figref idref="DRAWINGS">FIG. <b>11</b>C</figref>.</p><p id="p-0055" num="0054">Once the predictive models were generated, the same data processing and extraction techniques were applied to a dataset of 25 prospective NHL players from a WHL team. The algorithms for each of the See, Process, and Move components were applied to the player's MRI data and scores were generated including an overall prediction of athletic potential. A summary of these results is presented in <figref idref="DRAWINGS">FIG. <b>11</b>D</figref> and <figref idref="DRAWINGS">FIG. <b>11</b>E</figref>. The individual scores were plotted against each player's WHL performance (avg. Game Score for the 2016-2017 season). While these metrics were not included in determining the underlying neurological scores, the currents stats can be a rough early guide for the quality of the predictions, although there is not a strong correlation between CHL performance and eventual NHL performance. Of note, two of the top three ranked players were selected in the first round of the 2017 NHL draft. High overall scores to other players selected in the later rounds of the draft (round 3-7). It should be noted that not all of the players were draft eligible yet, including three of the four players given higher performance scores.</p><p id="p-0056" num="0055">The player with the second highest overall score has also not been drafted (indicated by the arrow). This individual scored very highly in the Process and Move components but less well in the See component. More detailed analysis of his scan indicated some inefficiency in his visual system. Based on this data, custom tasks and training programs that an athlete or training staff can use to address any neurologic imbalance or inefficiency identified in the MRI scan were developed to improve performance.</p><p id="p-0057" num="0056">The above-described embodiments can be implemented in any of numerous ways. For example, the embodiments may be implemented using hardware, software or a combination thereof. When implemented in software, the software code can be executed on any suitable processor or collection of processors, whether provided in a single computer or distributed among multiple computers. It should be appreciated that any component or collection of components that perform the functions described above can be generically considered as one or more controllers that control the above-discussed functions. The one or more controllers can be implemented in numerous ways, such as with dedicated hardware or with one or more processors programmed using microcode or software to perform the functions recited above.</p><p id="p-0058" num="0057">In this respect, it should be appreciated that one implementation of the embodiments of the present invention comprises at least one non-transitory computer-readable storage medium (e.g., a computer memory, a portable memory, a compact disk, etc.) encoded with a computer program (i.e., a plurality of instructions), which, when executed on a processor, performs the above-discussed functions of the embodiments of the present invention. The computer-readable storage medium can be transportable such that the program stored thereon can be loaded onto any computer resource to implement the aspects of the present invention discussed herein. In addition, it should be appreciated that the reference to a computer program which, when executed, performs the above-discussed functions, is not limited to an application program running on a host computer. Rather, the term computer program is used herein in a generic sense to reference any type of computer code (e.g., software or microcode) that can be employed to program a processor to implement the above-discussed aspects of the present invention.</p><p id="p-0059" num="0058">Various aspects of the present invention may be used alone, in combination, or in a variety of arrangements not specifically discussed in the embodiments described in the foregoing and are therefore not limited in their application to the details and arrangement of components set forth in the foregoing description or illustrated in the drawings. For example, aspects described in one embodiment may be combined in any manner with aspects described in other embodiments.</p><p id="p-0060" num="0059">Also, embodiments of the invention may be implemented as one or more methods, of which an example has been provided. The acts performed as part of the method(s) may be ordered in any suitable way. Accordingly, embodiments may be constructed in which acts are performed in an order different than illustrated, which may include performing some acts simultaneously, even though shown as sequential acts in illustrative embodiments.</p><p id="p-0061" num="0060">Use of ordinal terms such as &#x201c;first,&#x201d; &#x201c;second,&#x201d; &#x201c;third,&#x201d; etc., in the claims to modify a claim element does not by itself connote any priority, precedence, or order of one claim element over another or the temporal order in which acts of a method are performed. Such terms are used merely as labels to distinguish one claim element having a certain name from another element having a same name (but for use of the ordinal term).</p><p id="p-0062" num="0061">The phraseology and terminology used herein is for the purpose of description and should not be regarded as limiting. The use of &#x201c;including,&#x201d; &#x201c;comprising,&#x201d; &#x201c;having,&#x201d; &#x201c;containing,&#x201d; &#x201c;involving,&#x201d; and variations thereof, is meant to encompass the items listed thereafter and additional items.</p><p id="p-0063" num="0062">Having described several embodiments of the invention in detail, various modifications and improvements will readily occur to those skilled in the art. Such modifications and improvements are intended to be within the spirit and scope of the invention. Accordingly, the foregoing description is by way of example only, and is not intended as limiting. The invention is limited only as defined by the following claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-01-23" num="01-23"><claim-text><b>1</b>-<b>23</b>. (canceled)</claim-text></claim><claim id="CLM-00024" num="00024"><claim-text><b>24</b>. A computerized system for identifying brain regions that predict differences in task performance, the system comprising:<claim-text>at least one computer processor; and</claim-text><claim-text>at least one computer-readable medium encoded with a plurality of instructions that, when executed by the at least one computer processor, perform a method, the method comprising:<claim-text>receiving brain imaging data for each of a plurality of individuals in a reference cohort, wherein the brain imaging data comprises structural brain data characterizing a static state of a brain and physiological brain data characterizing a dynamic state of the brain;</claim-text><claim-text>receiving functional skill data for each of the plurality of individuals in the reference cohort, wherein the functional skill data indicates a performance score for each of the plurality of individuals on one or more tasks;</claim-text><claim-text>determining, based, at least in part, on at least one first characteristic of the structural brain data, at least one second characteristic of the physiological brain data, and the functional skill data, brain regions associated with differences in performance on the one or more tasks; and</claim-text><claim-text>outputting the determined brain regions as a set of predictive regions of interest.</claim-text></claim-text></claim-text></claim><claim id="CLM-00025" num="00025"><claim-text><b>25</b>. The computerized system of <claim-ref idref="CLM-00024">claim 24</claim-ref>, wherein the method further comprises defining a set of brain regions and connections between the brain regions as a structural functional unit, and wherein<claim-text>determining the brain regions associated with differences in performance on the one or more tasks comprises identifying within the brain regions of the structural functional unit, brain regions associated with differences in performance on the one or more tasks.</claim-text></claim-text></claim><claim id="CLM-00026" num="00026"><claim-text><b>26</b>. The computerized system of <claim-ref idref="CLM-00025">claim 25</claim-ref> wherein determining the brain regions associated with differences in performance on the one or more tasks comprises:<claim-text>for each voxel of a brain region of the structural functional unit, determining a value indicating how well the brain data for the voxel characterizes differences in performance on the one or more tasks.</claim-text></claim-text></claim><claim id="CLM-00027" num="00027"><claim-text><b>27</b>. The computerized system of <claim-ref idref="CLM-00026">claim 26</claim-ref>, wherein determining a value indicating how well the brain data for the voxel characterizes difference in performance on the one or more tasks comprises:<claim-text>defining a first group of individuals in the reference cohort who performed well on the one or more tasks;</claim-text><claim-text>defining a second group of individuals in the reference cohort who performed poorly on the one or more tasks; and</claim-text><claim-text>performing, for the voxel, a numerical comparison between brain data for the first group of individuals and brain data for the second group of individuals to determine the value indicating how well the brain data for the voxel characterizes differences in performance on the one or more tasks.</claim-text></claim-text></claim><claim id="CLM-00028" num="00028"><claim-text><b>28</b>. The computerized system of <claim-ref idref="CLM-00027">claim 27</claim-ref>, wherein the method further comprises:<claim-text>performing a local transformation using a neighborhood of voxels around the voxel to determine a first value for the at least one first characteristic of the structural brain data and a second value for the at least one second characteristic of the physiological brain data, and</claim-text><claim-text>wherein performing, for the voxel, a numerical comparison between brain data for the first group of individuals and brain data for the second group of individuals comprises performing a first numerical comparison based on the first value and a second numerical comparison based on the second value.</claim-text></claim-text></claim><claim id="CLM-00029" num="00029"><claim-text><b>29</b>. A computer-implemented method for identifying brain regions that predict differences in task performance, the method comprising:<claim-text>receiving brain imaging data for each of a plurality of individuals in a reference cohort, wherein the brain imaging data comprises structural brain data characterizing a static state of a brain and physiological brain data characterizing a dynamic state of the brain;</claim-text><claim-text>receiving functional skill data for each of the plurality of individuals in the reference cohort, wherein the functional skill data indicates a performance score for each of the plurality of individuals on one or more tasks;</claim-text><claim-text>determining, based, at least in part, on at least one first characteristic of the structural brain data, at least one second characteristic of the physiological brain data, and the functional skill data, brain regions associated with differences in performance on the one or more tasks; and</claim-text><claim-text>outputting the determined brain regions as a set of predictive regions of interest.</claim-text></claim-text></claim><claim id="CLM-00030" num="00030"><claim-text><b>30</b>. The computer-implemented method of <claim-ref idref="CLM-00029">claim 29</claim-ref>, further comprising:<claim-text>defining a set of brain regions and connections between the brain regions as a structural functional unit, wherein</claim-text><claim-text>determining the brain regions associated with differences in performance on the one or more tasks comprises identifying within the brain regions of the structural functional unit, brain regions associated with differences in performance on the one or more tasks.</claim-text></claim-text></claim><claim id="CLM-00031" num="00031"><claim-text><b>31</b>. The computer-implemented method of <claim-ref idref="CLM-00030">claim 30</claim-ref> wherein determining the brain regions associated with differences in performance on the one or more tasks comprises:<claim-text>for each voxel of a brain region of the structural functional unit, determining a value indicating how well the brain data for the voxel characterizes differences in performance on the one or more tasks.</claim-text></claim-text></claim><claim id="CLM-00032" num="00032"><claim-text><b>32</b>. The computer-implemented method of <claim-ref idref="CLM-00031">claim 31</claim-ref>, wherein determining a value indicating how well the brain data for the voxel characterizes difference in performance on the one or more tasks comprises:<claim-text>defining a first group of individuals in the reference cohort who performed well on the one or more tasks;</claim-text><claim-text>defining a second group of individuals in the reference cohort who performed poorly on the one or more tasks; and</claim-text><claim-text>performing, for the voxel, a numerical comparison between brain data for the first group of individuals and brain data for the second group of individuals to determine the value indicating how well the brain data for the voxel characterizes differences in performance on the one or more tasks.</claim-text></claim-text></claim><claim id="CLM-00033" num="00033"><claim-text><b>33</b>. The computer-implemented method of <claim-ref idref="CLM-00032">claim 32</claim-ref>, further comprising:<claim-text>performing a local transformation using a neighborhood of voxels around the voxel to determine a first value for the at least one first characteristic of the structural brain data and a second value for the at least one second characteristic of the physiological brain data, and</claim-text><claim-text>wherein performing, for the voxel, a numerical comparison between brain data for the first group of individuals and brain data for the second group of individuals comprises performing a first numerical comparison based on the first value and a second numerical comparison based on the second value.</claim-text></claim-text></claim></claims></us-patent-application>