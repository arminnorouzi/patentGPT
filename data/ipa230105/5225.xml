<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005226A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005226</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17938957</doc-number><date>20220907</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>JP</country><doc-number>2011-235749</doc-number><date>20111027</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>19</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>70</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>73</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>19</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>7</main-group><subgroup>18</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>19</main-group><subgroup>006</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>70</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>73</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>19</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>7</main-group><subgroup>183</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2219</main-group><subgroup>004</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>10021</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30244</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">IMAGE PROCESSING APPARATUS, IMAGE PROCESSING METHOD, AND PROGRAM</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>17140144</doc-number><date>20210104</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11468647</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17938957</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16587070</doc-number><date>20190930</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>10902682</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17140144</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16051893</doc-number><date>20180801</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>10453266</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>16587070</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>15459711</doc-number><date>20170315</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>10068382</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>16051893</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>14994950</doc-number><date>20160113</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>9626806</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>15459711</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>13824140</doc-number><date>20130610</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>9292974</doc-number></document-id></parent-grant-document><parent-pct-document><document-id><country>WO</country><doc-number>PCT/JP2012/005582</doc-number><date>20120904</date></document-id></parent-pct-document></parent-doc><child-doc><document-id><country>US</country><doc-number>14994950</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Sony Group Corporation</orgname><address><city>Tokyo</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>KAINO</last-name><first-name>Akihiko</first-name><address><city>Kanagawa</city><country>JP</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>FUKUCHI</last-name><first-name>Masaki</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>KASHITANI</last-name><first-name>Tatsuki</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>OOI</last-name><first-name>Kenichiro</first-name><address><city>Kanagawa</city><country>JP</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>GUO</last-name><first-name>Jingjing</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>Sony Group Corporation</orgname><role>03</role><address><city>Tokyo</city><country>JP</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">An information processing system that acquires video data captured by an image pickup unit; detects an object from the video data; detects a condition corresponding to the image pickup unit; and controls a display to display content associated with the object at a position other than a detected position of the object based on the condition corresponding to the image pickup unit.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="103.38mm" wi="158.75mm" file="US20230005226A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="190.92mm" wi="134.20mm" orientation="landscape" file="US20230005226A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="224.62mm" wi="128.61mm" file="US20230005226A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="185.67mm" wi="118.03mm" orientation="landscape" file="US20230005226A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="204.05mm" wi="150.11mm" orientation="landscape" file="US20230005226A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="194.48mm" wi="154.26mm" file="US20230005226A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="196.00mm" wi="122.85mm" file="US20230005226A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="74.17mm" wi="140.29mm" file="US20230005226A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="167.22mm" wi="148.17mm" orientation="landscape" file="US20230005226A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="179.75mm" wi="138.77mm" orientation="landscape" file="US20230005226A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="178.22mm" wi="135.38mm" orientation="landscape" file="US20230005226A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="216.07mm" wi="123.87mm" file="US20230005226A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="211.67mm" wi="127.68mm" file="US20230005226A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="215.56mm" wi="142.83mm" file="US20230005226A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application is a continuation of U.S. application Ser. No. 17/140,144 filed Jan. 4, 2021, which is a continuation of U.S. application Ser. No. 16/587,070 filed Sep. 30, 2019 (now U.S. Pat. No. 10,902,682), which is a continuation of Ser. No. 16/051,893, filed Aug. 1, 2018 (now U.S. Pat. No. 10,453,266), which is a continuation of U.S. application Ser. No. 15/459,711, filed Mar. 15, 2017 (now U.S. Pat. No. 10,068,382), which is a continuation of U.S. application Ser. No. 14/994,950, filed Jan. 13, 2016, (now U.S. Pat. No. 9,626,806), which is a continuation of U.S. application Ser. No. 13/824,140, filed Jun. 10, 2013, (now U.S. Pat. No. 9,292,974), which is a National Stage of PCT/JP2012/005582, filed Sep. 4, 2012, which claims priority under 35 U.S.C. 119 to Japanese Application No. 2011-235749, filed Oct. 27, 2011, the entire contents of each are incorporated herein by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">The present disclosure relates to an image processing apparatus, an image processing method, and a program.</p><heading id="h-0003" level="1">BACKGROUND ART</heading><p id="p-0004" num="0003">In recent years, attention has been focused on a technology called augmented reality (AR) that presents virtual content to the user by overlaying such content onto a real space. The content presented to the user by AR technology may be visualized in a variety of forms, such as text, icons, and animations.</p><p id="p-0005" num="0004">In AR technology, content to be overlaid on an image may be selected according to a variety of criteria. One of such criteria is recognition of an object associated in advance with content. As one example, JP2010-170316A discloses a technique that detects a marker, which is an object on which a specified pattern is drawn, in an image and overlays content associated with the detected marker at the detected position of the marker.</p><heading id="h-0004" level="1">CITATION LIST</heading><heading id="h-0005" level="1">Patent Literature</heading><heading id="h-0006" level="2">[PTL 1]</heading><heading id="h-0007" level="2">JP 2010-170316A</heading><heading id="h-0008" level="1">SUMMARY</heading><heading id="h-0009" level="1">Technical Problem</heading><p id="p-0006" num="0005">However, with an AR technique based on the detection of markers as described above, it is normally difficult to continue the displaying of AR content once a marker has been lost from the image. Also, even if the displaying of AR content were continued after a marker was lost from the image, there would be a tendency for the displaying of AR content to not reflect the state of the real space and therefore appear unnatural.</p><p id="p-0007" num="0006">Accordingly, it would be desirable to realize an arrangement capable of continuing the displaying of AR content in a natural state even after an object that acts as a marker has been lost from the image.</p><heading id="h-0010" level="1">Solution to Problem</heading><p id="p-0008" num="0007">According to an embodiment of the present disclosure, there is provided an information processing system comprising: one or more processing units that: acquire video data captured by an image pickup unit; detect an object from the video data; detect a condition corresponding to the image pickup unit; and control a display to display content associated with the object at a position other than a detected position of the object based on the condition corresponding to the image pickup unit.</p><p id="p-0009" num="0008">According to another embodiment of the present disclosure, there is provided an information processing method performed by an information processing system, the method comprising: acquiring video data captured by an image pickup unit; detecting an object from the video data; detecting a condition corresponding to the image pickup unit; and controlling a display to display content associated with the object at a position other than a detected position of the object based on the condition corresponding to the image pickup unit.</p><p id="p-0010" num="0009">According to still another embodiment of the present disclosure, there is provided a non-transitory computer-readable medium including computer program instructions, which when executed by an information processing system, cause the information processing system to perform a method, the method comprising: acquiring video data captured by an image pickup unit; detecting an object from the video data; detecting a condition corresponding to the image pickup unit; and controlling a display to display content associated with the object at a position other than a detected position of the object based on the condition corresponding to the image pickup unit.</p><heading id="h-0011" level="1">Advantageous Effects of Invention</heading><p id="p-0011" num="0010">According to the above embodiments of the present disclosure, an arrangement capable of continuing the displaying of AR content in a natural state even after an object that acts as a marker has been lost from the image is realized.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0012" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram useful in explaining an overview of an information processing apparatus according to an embodiment of the present disclosure.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>2</b>A</figref> is a diagram useful in explaining one example of a marker that can be detected by the present embodiment.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>2</b>B</figref> is a diagram useful in explaining another example of a marker that can be detected by the present embodiment.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a block diagram showing one example of the hardware configuration of the information processing apparatus according to the present embodiment.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a block diagram showing one example of the logical functional configuration of the information processing apparatus according to the present embodiment.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flowchart showing one example of the flow of an analyzing process according to an analyzing unit illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a diagram useful in explaining one example of the configuration of feature point information.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a diagram useful in explaining one example of the configuration of camera position/posture information.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a diagram useful in explaining one example of the configuration of marker basic information.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a diagram useful in explaining one example of the configuration of marker detection information.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a diagram useful in explaining one example of the configuration of content information.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a diagram useful in explaining a first example of a removal condition of AR content.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a diagram useful in explaining a second example of a removal condition of AR content.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>13</b>A</figref> is a diagram useful in explaining a first example of displaying of AR content according to the present embodiment.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>13</b>B</figref> is a diagram useful in explaining a second example of displaying of AR content according to the present embodiment.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>13</b>C</figref> is a diagram useful in explaining a third example of displaying of AR content according to the present embodiment.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>13</b>D</figref> is a diagram useful in explaining a fourth example of displaying of AR content according to the present embodiment.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a flowchart showing one example of a flow of image processing according to the same embodiment.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0013" level="1">DESCRIPTION OF EMBODIMENTS</heading><p id="p-0030" num="0029">Hereinafter, preferred embodiments of the present disclosure will be described in detail with reference to the appended drawings. Note that, in this specification and the appended drawings, structural elements that have substantially the same function and structure are denoted with the same reference numerals, and repeated explanation of these structural elements is omitted.</p><p id="p-0031" num="0030">The following description is given in the order indicated below.</p><heading id="h-0014" level="2">1. Overview</heading><heading id="h-0015" level="2">2. Example Configuration of Image Processing Apparatus According to Embodiment of the Present Disclosure</heading><heading id="h-0016" level="2">2-1. Hardware Configuration</heading><heading id="h-0017" level="2">2-2. Functional Configuration</heading><heading id="h-0018" level="2">2-3. Example Displaying of AR Content</heading><heading id="h-0019" level="2">2-4. Flow of Processing</heading><heading id="h-0020" level="2">3. Conclusion</heading><heading id="h-0021" level="2">1. Overview</heading><p id="p-0032" num="0031">First, an overview of an image processing apparatus according to an embodiment of the present disclosure will be described with reference to <figref idref="DRAWINGS">FIGS. <b>1</b> to <b>2</b>B</figref>.</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram useful in explaining an overview of an image processing apparatus <b>100</b> according to an embodiment of the present disclosure. <figref idref="DRAWINGS">FIG. <b>1</b></figref> shows the image processing apparatus <b>100</b> in the possession of the user Ua. The image processing apparatus <b>100</b> is equipped with an image pickup unit <b>102</b> (hereinafter sometimes referred to simply as the &#x201c;camera&#x201d;) that picks up images of a real space <b>1</b> and a display unit <b>110</b>. In the example in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, a table <b>11</b>, a coffee cup <b>12</b>, a book <b>13</b>, and a poster <b>14</b> are present in the real space <b>1</b>. The image pickup unit <b>102</b> of the image processing apparatus <b>100</b> picks up a series of images that compose video images produced by image pickup of the real space <b>1</b>. The image processing apparatus <b>100</b> then carries out image processing with an image picked up by the image pickup unit <b>102</b> as an input image to generate an output image. In the present embodiment, the output image is typically generated by overlaying virtual content (hereinafter referred to as &#x201c;AR content&#x201d;) for augmented reality (AR) onto the input image. The display unit <b>110</b> of the image processing apparatus <b>100</b> successively displays the generated output images. Note that the real space <b>1</b> shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref> is merely one example. The input image processed by the image processing apparatus <b>100</b> may be any image in which a real space appears.</p><p id="p-0034" num="0033">The provision of AR content by the image processing apparatus <b>100</b> may start with detection of a marker appearing in an input image as a trigger. In this specification, the term &#x201c;marker&#x201d; typically refers to any kind of object present in the real space that has a known pattern. That is, the term &#x201c;marker&#x201d; may include a shape, symbol, character string or design shown on a real object, part of a real object, or the surface of a real object, or an image or the like displayed on a display. Although there are cases where as a narrow definition, the term &#x201c;marker&#x201d; refers to a special object provided for some kind of application, the technology according to the present disclosure is not limited to such a definition.</p><p id="p-0035" num="0034">Note that in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, a smartphone is shown as one example of the image processing apparatus <b>100</b>. However, the image processing apparatus <b>100</b> is not limited to this example. As examples, the image processing apparatus <b>100</b> may be a PC (Personal Computer), a PDA (Personal Digital Assistant), a game console, a PND (Portable Navigation Device), a content player, or a digital home appliance.</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>2</b>A</figref> is a diagram useful in explaining one example of a marker that can be detected in the present embodiment. <figref idref="DRAWINGS">FIG. <b>2</b>A</figref> shows an input image Im<b>01</b> as one example that can be acquired by the image processing apparatus <b>100</b> illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. The table <b>11</b>, the coffee cup <b>12</b>, and the poster <b>14</b> appear in the input image Im<b>01</b>. A marker <b>20</b><i>a </i>that is a known design is printed on the poster <b>14</b>. If the image processing apparatus <b>100</b> detects such a marker <b>20</b><i>a </i>in the input image Im<b>01</b>, content associated with the marker <b>20</b><i>a </i>may be overlaid on the input image Im<b>01</b>.</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>2</b>B</figref> is a diagram useful in explaining another example of a marker that may be detected in the present embodiment. <figref idref="DRAWINGS">FIG. <b>2</b>B</figref> shows an input image Im<b>02</b>. The table <b>11</b> and the book <b>13</b> appear in the input image Im<b>02</b>. A marker <b>20</b><i>b </i>that is a known design is printed on the book <b>13</b>. If the image processing apparatus <b>100</b> detects such a marker <b>20</b><i>b </i>in the input image Im<b>02</b>, content associated with the marker <b>20</b><i>b </i>may be overlaid on the input image Im<b>02</b>. In place of the marker <b>20</b><i>b </i>illustrated in <figref idref="DRAWINGS">FIG. <b>2</b>B</figref>, the image processing apparatus <b>100</b> may use a marker <b>20</b><i>c </i>that is a known character string.</p><p id="p-0038" num="0037">After a marker has been detected in the input image as described above, in some cases the marker will stop being detected from the input image due to the camera moving or the posture of the camera changing. In such case, with typical AR technology that is based on the detection of markers, it is difficult to continue displaying the AR content. If the displaying of AR content is continued even after a marker has been lost, the display will become unnatural, such as by having AR content displayed that is unrelated to the position or posture of the marker.</p><p id="p-0039" num="0038">For this reason, in the present embodiment, to eliminate or reduce the unnatural displaying of AR content, the image processing apparatus <b>100</b> tracks the position and posture of the camera in the three-dimensional real space and manages the positions and postures of the detected markers using a database. As described in detail later, the image processing apparatus <b>100</b> then controls the behavior of AR content based on at least one of the position and posture of the camera relative to the markers.</p><p id="p-0040" num="0039">2. Example Configuration of Image Processing Apparatus According to an Embodiment</p><p id="p-0041" num="0040">2-1. Hardware Configuration</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a block diagram showing one example of the hardware configuration of the image processing apparatus <b>100</b> according to the present embodiment. As shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the image processing apparatus <b>100</b> includes the image pickup unit <b>102</b>, a sensor unit <b>104</b>, an input unit <b>106</b>, a storage unit <b>108</b>, a display unit <b>110</b>, a communication unit <b>112</b>, a bus <b>116</b>, and a control unit <b>118</b>.</p><p id="p-0043" num="0042">(1) Image Pickup Unit</p><p id="p-0044" num="0043">The image pickup unit <b>102</b> is a camera module that picks up an image. The image pickup unit <b>102</b> picks up images of a real space using an image pickup element such as a CCD (Charge Coupled Device) or a CMOS (Complementary Metal Oxide Semiconductor) to generate a picked-up image. A series of the picked-up images generated by the image pickup unit <b>102</b> compose video images in which the real space appears. Note that the image pickup unit <b>102</b> does not need to be part of the image processing apparatus <b>100</b>. As one example, an image pickup apparatus connected to the image processing apparatus <b>100</b> wirelessly or using wires may be treated as the image pickup unit <b>102</b>.</p><p id="p-0045" num="0044">(2) Sensor Unit</p><p id="p-0046" num="0045">The sensor unit <b>104</b> may include a variety of sensors such as a positioning sensor, an acceleration sensor, and a gyrosensor. The position, posture, or movement of the image processing apparatus <b>100</b> that can be measured by the sensor unit <b>104</b> may be used for a variety of applications such as supporting recognition of the position and posture of a camera, described later, acquisition of data that specifies a global position, or recognition of instructions from the user. Note that the sensor unit <b>104</b> may be omitted from the configuration of the image processing apparatus <b>100</b>.</p><p id="p-0047" num="0046">(3) Input Unit</p><p id="p-0048" num="0047">The input unit <b>106</b> is an input device used by the user to operate the image processing apparatus <b>100</b> or to input information into the image processing apparatus <b>100</b>. As one example, the input unit <b>106</b> may include a touch sensor that detects touches made by the user on the screen of the display unit <b>110</b>. In place of (or in addition to) this, the input unit <b>106</b> may include a pointing device such as a mouse or a touch pad. In addition, the input unit <b>106</b> may include another type of input device such as a keyboard, a keypad, a button or buttons, or a switch or switches.</p><p id="p-0049" num="0048">(4) Storage Unit</p><p id="p-0050" num="0049">The storage unit <b>108</b> is constructed of a storage medium such as a semiconductor memory or a hard disk drive and stores programs and data for processing by the image processing apparatus <b>100</b>. The data stored by the storage unit <b>108</b> may include picked-up image data, sensor data, and data in a variety of databases (DB), described later. Note that instead of being stored in the storage unit <b>108</b>, some of the programs and data described in the present specification may be acquired from an external data source (as examples, a data server, network storage, or an external memory).</p><p id="p-0051" num="0050">(5) Display Unit</p><p id="p-0052" num="0051">The display unit <b>110</b> is a display module including a display such as an LCD (Liquid Crystal Display), an OLED (Organic Light-Emitting Diode), or a CRT (Cathode Ray Tube). As one example, the display unit <b>110</b> is used to display an output image generated by the image processing apparatus <b>100</b>. Note that the display unit <b>110</b> also does not need to be part of the image processing apparatus <b>100</b>. As one example, a display apparatus connected to the image processing apparatus <b>100</b> wirelessly or using wires may be treated as the display unit <b>110</b>.</p><p id="p-0053" num="0052">(6) Communication Unit</p><p id="p-0054" num="0053">The communication unit <b>112</b> is a communication interface that serves as a mediator for communication by the image processing apparatus <b>100</b> with other apparatuses. The communication unit <b>112</b> supports an arbitrary wireless communication protocol or wired communication protocol and establishes a communication connection with other apparatuses.</p><p id="p-0055" num="0054">(7) Bus</p><p id="p-0056" num="0055">The bus <b>116</b> connects the image pickup unit <b>102</b>, the sensor unit <b>104</b>, the input unit <b>106</b>, the storage unit <b>108</b>, the display unit <b>110</b>, the communication unit <b>112</b>, and the control unit <b>118</b> to one another.</p><p id="p-0057" num="0056">(8) Control Unit</p><p id="p-0058" num="0057">The control unit <b>118</b> corresponds to a processor such as a CPU (Central Processing Unit) or a DSP (Digital Signal Processor). By executing a program stored in the storage unit <b>108</b> or another storage medium, the control unit <b>118</b> causes the image processing apparatus <b>100</b> to function in a variety of ways as described later.</p><p id="p-0059" num="0058">2-2. Functional Configuration</p><p id="p-0060" num="0059"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a block diagram showing one example of a logical functional configuration realized by the storage unit <b>108</b> and the control unit <b>118</b> of the image processing apparatus <b>100</b> shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>. As shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the image processing apparatus <b>100</b> includes an image acquiring unit <b>120</b>, an analyzing unit <b>125</b>, a three-dimensional (3D) structure database (DB) <b>130</b>, a marker DB <b>135</b>, a marker detecting unit <b>140</b>, a marker managing unit <b>145</b>, a content DB <b>150</b>, a content control unit <b>155</b>, and a display control unit <b>160</b>.</p><p id="p-0061" num="0060">(1) Image Acquiring Unit</p><p id="p-0062" num="0061">The image acquiring unit <b>120</b> acquires the picked-up image generated by the image pickup unit <b>102</b> as an input image. The input image acquired by the image acquiring unit <b>120</b> may be an individual frame that composes video images produced by image pickup of a real space. The image acquiring unit <b>120</b> outputs the acquired input image to the analyzing unit <b>125</b>, the marker detecting unit <b>140</b>, and the display control unit <b>160</b>.</p><p id="p-0063" num="0062">(2) Analyzing Unit</p><p id="p-0064" num="0063">The analyzing unit <b>125</b> analyzes the input image inputted from the image acquiring unit <b>120</b> to recognize the three-dimensional position and posture in the real space of the apparatus that picked up the input image. The analyzing unit <b>125</b> also recognizes the three-dimensional structure of the peripheral environment of the image processing apparatus <b>100</b> and stores the recognized three-dimensional structure in the 3D structure DB <b>130</b>. In the present embodiment the analyzing process performed by the analyzing unit <b>125</b> is carried out according to SLAM (Simultaneous Localization And Mapping). The fundamental principles of SLAM are disclosed in &#x201c;Real-Time Simultaneous Localization and Mapping with a Single Camera&#x201d; (Andrew J. Davison, Proceedings of the 9<sup>th </sup>IEEE International Conference on Computer Vision Volume 2, 2003, pp. 1403-1410). Note that the present disclosure is not limited to this example and the analyzing unit <b>125</b> may analyze the input image using any other three-dimensional environment recognition technique.</p><p id="p-0065" num="0064">One characteristic of SLAM is that it is possible to dynamically recognize the three-dimensional structure of a real space appearing in an input image from a single (monocular) camera in parallel with the position and posture of such camera. <figref idref="DRAWINGS">FIG. <b>5</b></figref> shows one example of the analyzing process carried out by the analyzing unit <b>125</b>.</p><p id="p-0066" num="0065">In <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the analyzing unit <b>125</b> first initializes state variables (step S<b>101</b>). The state variables initialized here include at least the position and posture (rotational angle) of the camera and the movement speed and angular velocity of the camera, with the three-dimensional position of at least one feature point appearing in the input image also being added to the state variables. Input images acquired by the image acquiring unit <b>120</b> are successively inputted into the analyzing unit <b>125</b> (step S<b>102</b>). The processing in step S<b>103</b> to step S<b>105</b> may be repeated for each input image (that is, for each frame).</p><p id="p-0067" num="0066">In step S<b>103</b>, the analyzing unit <b>125</b> tracks the feature points appearing in the input image. For example, the analyzing unit <b>125</b> matches a patch (for example, a small image composed of nine pixels in a <b>3</b> by <b>3</b> grid centered on a feature point) for each feature point included in the state variables against a new input image. The analyzing unit <b>125</b> then detects the position of each patch in the input image, that is, the positions of the feature points. The positions of the feature points detected here are used when subsequently updating the state variables.</p><p id="p-0068" num="0067">In step S<b>104</b>, the analyzing unit <b>125</b> generates predicted values of the state variables of the next frame, for example, based on a specified prediction model. In step S<b>105</b>, the analyzing unit <b>125</b> uses the predicted values of the state variables generated in step S<b>104</b> and observed values in keeping with the positions of the feature points detected in step S<b>103</b> to update the state variables. The analyzing unit <b>125</b> carries out the processing in step S<b>104</b> and S<b>105</b> based on the principles of an extended Kalman filter. Note that such processing is described in detail in JP2011-159163A, for example.</p><p id="p-0069" num="0068">By carrying out such analyzing process, parameters included in the state variables are updated in each frame. The number of feature points included in the state variables may increase or decrease in each frame. That is, if the field of view of the camera changes, parameters of feature points in a region that has newly entered the frame may be added to the state variables and parameters of feature points in a region that has left the frame may be deleted from the state variables.</p><p id="p-0070" num="0069">The analyzing unit <b>125</b> stores the position and posture of the camera that are updated in this way for each frame in a time series in the 3D structure DB <b>130</b>. The analyzing unit <b>125</b> also stores the three-dimensional positions of the feature points included in the state variables for SLAM in the 3D structure DB <b>130</b>. Information on the feature points is gradually accumulated in the 3D structure DB <b>130</b> in keeping with movement of the field of view of the camera.</p><p id="p-0071" num="0070">Note that an example where the analyzing unit <b>125</b> uses SLAM to recognize both the position and the posture of the image pickup unit <b>102</b> is described here. However, the present disclosure is not limited to this example and it is also possible to recognize the position or the posture of the image pickup unit <b>102</b> based on sensor data from the sensor unit <b>104</b>, for example.</p><p id="p-0072" num="0071">(3) 3D Structure DB</p><p id="p-0073" num="0072">The 3D structure DB <b>130</b> is a database storing feature point information <b>131</b> used in the analyzing process by the analyzing unit <b>125</b> and camera position/posture information <b>132</b> recognized as the result of the analyzing process.</p><p id="p-0074" num="0073"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a diagram useful in explaining one example configuration of the feature point information <b>131</b>. As shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the feature point information <b>131</b> includes four data items, namely &#x201c;feature point ID&#x201d;, &#x201c;position&#x201d;, &#x201c;patch&#x201d;, and &#x201c;updated time&#x201d;. The &#x201c;feature point ID&#x201d; is an identifier for uniquely identifying each feature point. The &#x201c;position&#x201d; is a three-dimensional vector expressing the position of each feature point in the real space. The &#x201c;patch&#x201d; is image data of a small image used to detect each feature point in an input image. The &#x201c;updated time&#x201d; expresses the time when each record was updated. In the example in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, information on the two feature points FP<b>01</b> and FP<b>02</b> is shown. However, in reality, information on a larger number of feature points may be stored by the 3D structure DB <b>130</b> as the feature point information <b>131</b>.</p><p id="p-0075" num="0074"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a diagram useful in explaining one example of the configuration of the camera position/posture information <b>132</b>. As shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the camera position/posture information <b>132</b> includes the following three data items &#x201c;time&#x201d;, &#x201c;camera position&#x201d;, and &#x201c;camera posture&#x201d;. The &#x201c;time&#x201d; expresses the time at which each record was stored. The &#x201c;camera position&#x201d; is a three-dimensional vector showing the position of the camera recognized at each time as the result of the analyzing process. The &#x201c;camera posture&#x201d; is a rotational angle vector showing the posture of the camera recognized at each time as the result of the analyzing process. The camera position and posture tracked in this way are used by the content control unit <b>155</b>, described later, to control behavior of AR content and by the display control unit <b>160</b> to control the displaying of AR content.</p><p id="p-0076" num="0075">(4) Marker DB</p><p id="p-0077" num="0076">The marker DB <b>135</b> is a database storing information on at least one marker associated with content disposed in the AR space. In the present embodiment, the information stored by the marker DB <b>135</b> includes marker basic information <b>136</b> and marker detection information <b>137</b>.</p><p id="p-0078" num="0077"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a diagram useful in explaining one example of the configuration of the marker basic information <b>136</b>. As shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the marker basic information <b>136</b> includes three data items, namely &#x201c;marker ID&#x201d;, &#x201c;related content ID&#x201d; and &#x201c;size&#x201d;, in addition to a &#x201c;marker image&#x201d;. The &#x201c;marker ID&#x201d; is an identifier for uniquely identifying each marker. The &#x201c;related content ID&#x201d; is an identifier for identifying content associated with each marker. The &#x201c;marker image&#x201d; is image data of a known marker image used to detect each marker in an input image. Note that in place of a marker image, it is also possible to use a set of feature amounts extracted from each marker image to detect each marker. In the example in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, an image in which a lion is drawn is used as the marker image of the marker M<b>01</b> and an image in which an elephant is drawn is used as the marker image of the marker M<b>02</b>. The &#x201c;size&#x201d; expresses the assumed size of each marker image in the real space. Such marker basic information <b>136</b> may be stored in advance by the marker DB <b>135</b>. As an alternative, the marker basic information <b>136</b> may be stored in advance by an external server and selectively downloaded to the marker DB <b>135</b> in keeping with the position of the image processing apparatus <b>100</b> or the object of the provided AR application, for example.</p><p id="p-0079" num="0078">(5) Marker Detecting Unit</p><p id="p-0080" num="0079">The marker detecting unit <b>140</b> detects markers present in the real space from the input image. As a specific example, the marker detecting unit <b>140</b> extracts feature amounts of the input image and feature amounts of the respective marker images included in the marker basic information <b>136</b> in accordance with some kind of feature amount extraction algorithm. The marker detecting unit <b>140</b> then matches the extracted feature amounts of the input image against the feature amounts of each marker image. When a marker appears in the input image, this is indicated by a high matching score for the region in which such marker appears. By doing so, the marker detecting unit <b>140</b> is capable of detecting a marker that is present in the real space and appears in the input image. As examples, the feature amount extraction algorithm used by the marker detecting unit <b>140</b> may be Random Ferns described in &#x201c;Fast Keypoint Recognition using Random Ferns&#x201d; (Mustafa Oezuysal, IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. 32, Nr. 3, pp. 448-461, March 2010) or SURF described in &#x201c;SURF: Speeded Up Robust Features&#x201d; (H. Bay, A. Ess, T. Tuytelaars and L. V. Gool, Computer Vision and Image Understanding (CVIU), Vol. 110, No. 3, pp. 346-359, 2008).</p><p id="p-0081" num="0080">In addition, the marker detecting unit <b>140</b> estimates the three-dimensional position and posture of a marker in the real space based on the position of the detected marker in the input image (i.e., the two-dimensional position on the image pickup plane) and the marker size and form in the input image. The estimation carried out here may be part of the matching process for feature points described above. The marker detecting unit <b>140</b> then outputs the marker ID of the detected marker and also the estimated three-dimensional position and posture of the marker to the marker managing unit <b>145</b>.</p><p id="p-0082" num="0081">(6) Marker Managing Unit</p><p id="p-0083" num="0082">When a new marker appearing in the input image has been detected by the marker detecting unit <b>140</b>, the marker managing unit <b>145</b> stores the marker ID, the position and posture in the real space, and the detection time of the new marker in the marker DB <b>135</b>. Also, if a marker that has previously been detected is lost from the input image (due to a reason such as movement that places the marker outside the field of view or the marker being blocked by an obstacle), the marker managing unit <b>145</b> may also store a lost time of the marker that has been lost in the marker DB <b>135</b>.</p><p id="p-0084" num="0083"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a diagram useful in explaining one example of the configuration of the marker detection information <b>137</b> stored by the marker DB <b>135</b>. As shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref>, the marker detection information <b>137</b> has five data items, namely &#x201c;marker ID&#x201d;, &#x201c;position&#x201d;, &#x201c;posture&#x201d;, &#x201c;detection time&#x201d;, and &#x201c;lost time&#x201d;. The &#x201c;marker ID&#x201d; corresponds to the marker ID in the marker basic information <b>136</b> illustrated in <figref idref="DRAWINGS">FIG. <b>8</b></figref>. The &#x201c;position&#x201d; is a three-dimensional vector expressing the estimated position in the real space of each marker. The &#x201c;posture&#x201d; is a rotational angle vector expressing the estimated posture of each marker. The &#x201c;detection time&#x201d; expresses the time at which each marker was detected. The &#x201c;lost time&#x201d; expresses the time at which a marker that has previously been detected stops being detected. In the example in <figref idref="DRAWINGS">FIG. <b>9</b></figref>, the lost time L<b>1</b> is stored for the marker M<b>01</b>. This means that after the marker M<b>01</b> was detected, the marker M<b>01</b> was lost from the input image at time L<b>1</b>. Meanwhile, although a detection time D<b>2</b> is stored for the marker M<b>02</b>, no lost time is stored. This means that after being detected at time D<b>2</b>, the marker M<b>02</b> continues to appear in the input image as before. The parameters for respective markers that are managed in this way are used by the content control unit <b>155</b>, described later, to control the behavior of AR content.</p><p id="p-0085" num="0084">(7) Content DB</p><p id="p-0086" num="0085">The content DB <b>150</b> is a database storing content information <b>151</b> used to control and display at least one AR content item associated with the markers described above.</p><p id="p-0087" num="0086"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a diagram useful in explaining one example of the configuration of the content information <b>151</b>. As shown in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, the content information <b>151</b> includes a content ID and attributes, and also drawing data. The &#x201c;content ID&#x201d; is an identifier that uniquely identifies each AR content item. In the example in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, &#x201c;type&#x201d; and &#x201c;control parameter set&#x201d; are shown as the attributes of the AR content. The &#x201c;type&#x201d; is an attribute used to classify the AR content. The AR content may be classified according to a variety of viewpoints, such as the type of associated marker, the type of character expressed by the AR content, or the type of application providing the AR content. The &#x201c;control parameter set&#x201d; may include at least one control parameter used to control the behavior of AR content, described later.</p><p id="p-0088" num="0087">In the example in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, two types of drawing data, namely &#x201c;near&#x201d; and &#x201c;remote&#x201d;, are defined for each AR content item. As one example, such drawing data is CG (Computer Graphics) data for modeling AR content. The two types of drawing data differ in terms of display resolution. The display control unit <b>160</b>, described later, switches between which of such drawing data is to be used to display the AR content based on the position or posture of the camera relative to the detected marker.</p><p id="p-0089" num="0088">The content information <b>151</b> may be stored in advance in the content DB <b>150</b>. As an alternative, in the same way as the marker basic information <b>136</b> described earlier, the content information <b>151</b> may be stored in advance in an external server and selectively downloaded to the content DB <b>150</b> in keeping with the position of the image processing apparatus <b>100</b> or the object of the provided AR application, for example.</p><p id="p-0090" num="0089">(8) Content Control Unit</p><p id="p-0091" num="0090">The content control unit <b>155</b> controls the behavior of AR content associated with a detected marker in the AR space based on at least one of the camera position and the camera posture relative to the detected marker that is being tracked using the marker detection information <b>137</b> described above. In the present specification, the expression &#x201c;behavior of AR content&#x201d; includes the appearance and removal (disappearance) of AR content in the AR space and movement of the AR content.</p><p id="p-0092" num="0091">(8-1) Appearance of AR Content</p><p id="p-0093" num="0092">When a new marker appearing in the input image has been detected by the marker detecting unit <b>140</b> for example, the content control unit <b>155</b> has the AR content associated with such new marker in the marker basic information <b>136</b> appear in the AR space. The AR content may appear instantly in keeping with detection of the associated marker or may appear when a specified appearance condition has also been satisfied. As one example, the expression &#x201c;specified appearance condition&#x201d; may refer to a condition that a distance from the marker to the present camera position is below a specified distance threshold. In such case, even if a marker appears in the input image, the AR content will not appear if the distance from such marker to the camera position is far and the AR content will only appear when the camera position moves closer to the marker. Such distance threshold may be commonly defined for a plurality of AR content items or may be defined as a control parameter for each AR content item.</p><p id="p-0094" num="0093">(8-2) Movement of AR Content</p><p id="p-0095" num="0094">The content control unit <b>155</b> moves the AR content in the AR space in accordance with a change in at least one of the position and posture of the camera. For example, the content control unit <b>155</b> recognizes an operation such as panning or tilting of the camera by the user from a change in the camera posture (for example, a change in the angle of the optical axis that exceeds a specified amount of change.) As examples, the content control unit <b>155</b> may then change the orientation of the AR content in keeping with the panning and move the AR content forward or backward in keeping with the tilting. Note that the mapping between such types of operation and the movement of the AR content is not limited to this example.</p><p id="p-0096" num="0095">If a detected marker has moved outside the field of view of the input image, the content control unit <b>155</b> may move the AR content associated with such marker in the AR space so that the AR content is kept within the field of view of the new input image. The three-dimensional position to which the AR content is moved may be decided from the feature point positions stored by the 3D structure DB <b>130</b>.</p><p id="p-0097" num="0096">If the AR content is an image of a character capable of expressing a line of sight (i.e., looking in a certain direction) such as those illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, it is possible for the content control unit <b>155</b> to point the line of sight of the character toward the camera based on the position of the camera relative to the position of the character in the AR space.</p><p id="p-0098" num="0097">(8-3) Removal of AR Content</p><p id="p-0099" num="0098">In the present embodiment, as described earlier, the AR content is not necessarily removed (i.e., does not necessarily disappear) when the associated marker has moved out of the field of view of the input image. However, if AR content endlessly continued to be displayed regardless of the position and posture of the camera, this would conversely appear unnatural to the user. For this reason, in the present embodiment, the content control unit <b>155</b> removes AR content if at least one of the camera position and camera posture relative to a detected marker satisfies a specified removal condition. As examples, any of the following conditions A to D or a combination thereof may be used as the specified removal condition.</p><p id="p-0100" num="0099">Condition A: the distance from the marker to the camera position exceeds a specified distance threshold.</p><p id="p-0101" num="0100">Condition B: the angle made between the optical axis of the camera and the direction from the camera to the marker exceeds a specified angle threshold.</p><p id="p-0102" num="0101">Condition C: the time elapsed since the detection time of the marker exceeds a specified time threshold.</p><p id="p-0103" num="0102">Condition D: the time elapsed since the lost time of the marker exceeds a specified time threshold.</p><p id="p-0104" num="0103">The distance threshold, angle threshold, and time thresholds referred to here may be commonly defined for a plurality of AR content items or may be defined as control parameters for each AR content item.</p><p id="p-0105" num="0104"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a diagram useful in explaining the removal condition A for AR content. In <figref idref="DRAWINGS">FIG. <b>11</b></figref>, the real space <b>1</b> described earlier is shown again. In <figref idref="DRAWINGS">FIG. <b>11</b></figref>, a point P<b>1</b> is the detection position of a marker <b>20</b><i>a </i>and a broken line DL<b>1</b> shows a boundary where the distance from the point P<b>1</b> is equal to the distance threshold d<sub>th1</sub>. The distance of an image processing apparatus <b>100</b><i>a </i>from the marker <b>20</b><i>a </i>is below the distance threshold d<sub>t</sub>hl. In this case, the content control unit <b>155</b> of the image processing apparatus <b>100</b><i>a </i>does not remove the AR content <b>32</b><i>a </i>associated with the marker <b>20</b><i>a </i>and moves the AR content <b>32</b><i>a </i>within a field of view <b>30</b><i>a </i>of the image processing apparatus <b>100</b><i>a</i>. After this, assume that the apparatus has moved for example from the position of the image processing apparatus <b>100</b><i>a </i>to the position of the image processing apparatus <b>100</b><i>b</i>. The distance from the marker <b>20</b><i>a </i>to an image processing apparatus <b>100</b><i>b </i>exceeds the distance threshold d<sub>t</sub>hl. In this case, the content control unit <b>155</b> removes the AR content <b>32</b><i>a </i>associated with the marker <b>20</b><i>a</i>. That is, the AR content <b>32</b><i>a </i>does not appear in a field of view <b>30</b><i>b </i>of the image processing apparatus <b>100</b><i>b. </i></p><p id="p-0106" num="0105"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a diagram useful in explaining one example of the removal condition B for AR content. In <figref idref="DRAWINGS">FIG. <b>12</b></figref>, the real space <b>1</b> described earlier is shown again. In <figref idref="DRAWINGS">FIG. <b>12</b></figref>, point P<b>1</b> shows a detection position of the marker <b>20</b><i>a</i>. The distance from the marker <b>20</b><i>a </i>to an image processing apparatus <b>100</b><i>c </i>is assumed to be shorter than the specified distance threshold. However, the angle r<sub>opt </sub>between the optical axis V<sub>opt </sub>of the image pickup unit <b>102</b> of the image processing apparatus <b>100</b><i>c </i>and the direction V<sub>mark </sub>from the image pickup unit <b>102</b> to the marker <b>20</b><i>a </i>exceeds a specified angle threshold (not shown). In this case, the content control unit <b>155</b> of the image processing apparatus <b>100</b><i>c </i>removes the AR content <b>32</b><i>a </i>associated with the marker <b>20</b><i>a. </i></p><p id="p-0107" num="0106">Note that regardless of these removal conditions A and B, the content control unit <b>155</b> may remove the AR content associated with a marker when, as shown in removal conditions C and D given above, the time elapsed from the detection time of the marker or the time elapsed from the lost time of the marker exceeds a specified time threshold. Also, the AR content associated with a marker may be removed when removal condition A or B is satisfied and the time elapsed from the detection time of the marker or the time elapsed from the lost time of the marker exceeds a specified time threshold.</p><p id="p-0108" num="0107">By controlling the behavior of AR content in this way, an unnatural state where AR content endlessly continues to be displayed regardless of the position and posture of the camera is prevented. Overcrowding of AR content due to the displaying of a large number of AR content items is also avoided. In particular, in the present embodiment, the removal of AR content is controlled in keeping with the position or posture of the camera relative to a marker. This means that it is possible to remove AR content if the user has stopped being interested in such content (for example, if the user has moved away from the marker or is now picking up images in a completely different direction to the marker). That is, the life cycle from appearance to removal of AR content can be appropriately managed in keeping with the state of the user.</p><p id="p-0109" num="0108">(8-4) Coexistence of AR Content</p><p id="p-0110" num="0109">The content control unit <b>155</b> may control the coexistence of a plurality of AR content items associated with different markers based on the camera position or posture relative to such markers. For example, the content control unit <b>155</b> may select one of the two following control options when a second marker is newly detected in a state where a first AR content item associated with the first marker is already disposed in the AR space.</p><p id="p-0111" num="0110">Option A: dispose the second AR content item associated with the second marker in the AR space in addition to the first AR content item.</p><p id="p-0112" num="0111">Option B: dispose the second AR content item associated with the second marker in the AR space in place of the first AR content item.</p><p id="p-0113" num="0112">As one example, the content control unit <b>155</b> may select Option A if the distance from the first marker to the camera position is below a specified distance threshold when the second marker is detected and may select Option B if such distance is above the distance threshold. If Option A is selected, the first and second AR content items will coexist in the AR space. By doing so, as one example it is also possible to express interaction between the AR content items. In particular, in the present embodiment, since the displaying of an AR content item continues even after a marker has been lost from the image, even if a plurality of markers do not simultaneously appear in the input image, it is still possible to gradually add AR content items to the AR space. In this case, it is possible to avoid the coexistence of an excessive number of AR content items in the AR space and to have AR content items coexist in more natural conditions.</p><p id="p-0114" num="0113">Note that the content control unit <b>155</b> may control the coexistence of a plurality of AR content items based on the types (for example, the &#x201c;types&#x201d; illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref>) of the first and second AR content items. For example, the content control unit <b>155</b> may select Option A described above only when the first and second AR content items are the same type. The expression &#x201c;AR content items of the same type&#x201d; may refer for example to AR content items associated with the same type of marker, to AR content items that express the same type of character, or AR content items for an application with the same object. By doing so, it is possible to avoid having a mixture of AR content items that are incapable of interaction coexist with one another.</p><p id="p-0115" num="0114">(8-5) Output of Control Results</p><p id="p-0116" num="0115">By controlling the behavior of AR content in this way, the content control unit <b>155</b> selects the AR content to be overlaid on the input image. The content control unit <b>155</b> then decides the three-dimensional display position and display posture in the AR space of the selected AR content. The display position and display posture of the AR content are typically decided using the recognition results of the peripheral environment of the image processing apparatus <b>100</b> produced by the analyzing unit <b>125</b>. That is, the content control unit <b>155</b> decides the display position and display posture of the AR content using the feature point information <b>131</b> and the camera position/posture information <b>132</b> stored by the 3D structure DB <b>130</b>. The display position and display posture of the AR content may be decided so that the AR content is within the field of view of the camera and the respective AR content items stand on an object or on the ground in the field of view. If there is a sudden change in field of view, the display position(s) of the AR content may be decided so that the AR content moves slowly without completely tracking the change in the field of view. Note that the method of deciding the display position and display posture of the AR content is not limited to this example. The content control unit <b>155</b> then outputs drawing data, display positions, display postures, and other control parameters for the AR content to be overlaid on the input image to the display control unit <b>160</b>.</p><p id="p-0117" num="0116">The control parameters additionally outputted from the content control unit <b>155</b> to the display control unit <b>160</b> may include parameters including the line of sight of an AR content item, for example. Also, the control parameters may include a transparency parameter relating to the fading out of AR content. For example, during the determination of the removal condition A described earlier, the content control unit <b>155</b> may set the transparency of an AR content item higher as the distance from the marker to the camera position approaches the specified distance threshold. In the same way, during the determination of the removal condition B described earlier, the content control unit <b>155</b> may set the transparency of an AR content item higher as the angle between the optical axis of the camera and the direction from the camera to the marker approaches the specified angle threshold. By setting the transparency in this way, it is possible to have an AR content item gradually fade out before the AR content disappears. The content control unit <b>155</b> may also output a control parameter to the display control unit <b>160</b> indicating that a graphic indicating is to be displayed when the AR content is about to disappear from the display when one of the removal conditions is satisfied. This control parameter may cause the display to display a graphic indicia instructing a user to adjust the camera position such that a removal condition may no longer be satisfied. This instruction may, for example, be an arrow instructing the user to adjust a position of the camera and/or an instruction to move the camera closer to the marker. The graphic indicia may also simply be a warning indicating that the AR content is about to disappear from the display.</p><p id="p-0118" num="0117">(9) Display Control Unit</p><p id="p-0119" num="0118">The display control unit <b>160</b> generates an output image by overlaying the AR content associated with the marker(s) detected by the marker detecting unit <b>140</b> on the input image inputted from the image acquiring unit <b>120</b>. The display control unit <b>160</b> then displays the generated output image on the screen of the display unit <b>110</b>.</p><p id="p-0120" num="0119">More specifically, the drawing data, the display positions, the display posture, and the other control parameters for the AR content to be displayed are inputted from the content control unit <b>155</b> into the display control unit <b>160</b>. The display control unit <b>160</b> also acquires the present camera position and posture from the 3D structure DB <b>130</b>. The display control unit <b>160</b> then overlays the AR content at a rendering position on the image pickup plane based on the display position and display posture of the AR content and the present camera position and posture.</p><p id="p-0121" num="0120">The drawing data used for displaying by the display control unit <b>160</b> may be switched between the two types of drawing data illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref> based on the camera position and camera posture relative to the marker. By doing so, in a state where the user is close to a marker or is picking up images of the periphery of a marker, for example, the content associated with such marker may be displayed with a high display resolution. Also, the display control unit <b>160</b> may change the transparency of the AR content in keeping with a control parameter from the content control unit <b>155</b>.</p><p id="p-0122" num="0121">In the present embodiment, as described earlier, since the display position and display posture of the AR content are decided using the recognition result for the peripheral environment of the image processing apparatus <b>100</b>, the display control unit <b>160</b> is capable, even after a marker that was previously detected has moved out of the field of view of the input image, of overlaying AR content associated with such marker on the input image in a natural way. Also, since the recognition results for the peripheral environment are stored by the 3D structure DB <b>130</b>, even if recognition of the environment fails for a certain frame, for example, it is possible to continue recognition based on the previous recognition result without having to restart recognition of the environment from the beginning. Therefore, according to the present embodiment, it is possible to continue displaying AR content even if a marker no longer appears in the input image and recognition has temporarily failed. This means that the user can move the camera freely without having to worry about whether markers appear in the input image or whether the peripheral environment is being properly recognized.</p><p id="p-0123" num="0122">2-3. Example Displaying of AR Content</p><p id="p-0124" num="0123"><figref idref="DRAWINGS">FIG. <b>13</b>A</figref> is a diagram useful in explaining a first example of the displaying of AR content according to the present embodiment. <figref idref="DRAWINGS">FIG. <b>13</b>A</figref> shows an output image Iml<b>1</b> as one example. The table <b>11</b>, the coffee cup <b>12</b>, and the poster <b>14</b> appear in the output image IM<b>11</b>. Based on the positions of feature points of such real objects, the analyzing unit <b>125</b> of the image processing apparatus <b>100</b> recognizes the camera position and camera posture in three dimensions and also the three-dimensional structure (that is, the three-dimensional positions of such feature points) of the environment in accordance with SLAM. The marker <b>20</b><i>a </i>is printed on the poster <b>14</b>. The marker <b>20</b><i>a </i>is detected by the marker detecting unit <b>140</b> and the AR content <b>34</b><i>a </i>associated with the marker <b>20</b><i>a </i>is disposed in the AR space by the content control unit <b>155</b>. As a result, the AR content <b>34</b><i>a </i>is displayed in the output image Im<b>11</b>.</p><p id="p-0125" num="0124"><figref idref="DRAWINGS">FIG. <b>13</b>B</figref> is a diagram useful in explaining a second example of displaying of AR content according to the present embodiment. An output image Im<b>12</b> shown in <figref idref="DRAWINGS">FIG. <b>13</b>B</figref> is an image that may be displayed following the output image Iml<b>1</b> described above. Only part of the poster <b>14</b> appears in the output image Im<b>12</b>, so that the marker <b>20</b><i>a </i>cannot be detected by the marker detecting unit <b>140</b>. However, it is assumed that the camera position and camera posture relative to the marker <b>20</b><i>a </i>do not satisfy the removal condition described above. The content control unit <b>155</b> moves the AR content <b>34</b><i>a </i>within the field of view of the output image Im<b>12</b>. The display control unit <b>160</b> then overlays the AR content <b>34</b><i>a </i>at a position decided based on the camera position/posture information <b>132</b> stored in the 3D structure DB <b>130</b>. After this, if for example the image processing apparatus <b>100</b> moves further in a direction away from the marker <b>20</b><i>a</i>, the AR content <b>34</b><i>a </i>may fade out and finally disappear.</p><p id="p-0126" num="0125"><figref idref="DRAWINGS">FIG. <b>13</b>C</figref> is a diagram useful in explaining a third example of displaying of AR content according to the present embodiment. <figref idref="DRAWINGS">FIG. <b>13</b>C</figref> shows an output image Im<b>21</b> as one example. The table <b>11</b> and the book <b>13</b> appear in the output image Im<b>21</b>. The analyzing unit <b>125</b> of the image processing apparatus <b>100</b> recognizes the camera position and camera posture in three dimensions and also the three-dimensional structure of the environment based on the positions of the feature points of such real objects according to SLAM described earlier. The marker <b>20</b><i>b </i>is printed on the book <b>13</b>. The marker <b>20</b><i>b </i>is detected by the marker detecting unit <b>140</b> and the AR content <b>34</b><i>b </i>associated with the marker <b>20</b><i>b </i>is disposed in the AR space by the content control unit <b>155</b>. As a result, the AR content <b>34</b><i>b </i>is displayed in the output image Im<b>21</b>.</p><p id="p-0127" num="0126"><figref idref="DRAWINGS">FIG. <b>13</b>D</figref> is a diagram useful in explaining a fourth example of displaying of AR content according to the present embodiment. An output image Im<b>22</b> shown in <figref idref="DRAWINGS">FIG. <b>13</b>D</figref> is an image that may be displayed following the output image Im<b>21</b> described above. Although the marker <b>20</b><i>b </i>does not appear in the output image Im<b>22</b>, displaying of the AR content <b>34</b><i>b </i>is continued. The marker <b>20</b><i>a </i>additionally appears in the output image Im<b>22</b>. The marker <b>20</b><i>a </i>is detected by the marker detecting unit <b>140</b>. In the state in <figref idref="DRAWINGS">FIG. <b>13</b>D</figref>, since the distance from the marker <b>20</b><i>b </i>to the camera position is below the specified distance threshold, Option A described above is selected. As a result, the content control unit <b>155</b> disposes the AR content <b>34</b><i>a </i>associated with the newly detected marker <b>20</b><i>a </i>in the AR space in addition to the AR content <b>34</b><i>b. </i></p><p id="p-0128" num="0127">2-4. Flow of Processing</p><p id="p-0129" num="0128"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a flowchart showing one example of the flow of image processing by the image processing apparatus <b>100</b> according to the present embodiment.</p><p id="p-0130" num="0129">As shown in <figref idref="DRAWINGS">FIG. <b>14</b></figref>, the image acquiring unit <b>120</b> first acquires a picked-up image generated by the image pickup unit <b>102</b> as an input image (step S<b>110</b>). The image acquiring unit <b>120</b> then outputs the acquired input image to the analyzing unit <b>125</b>, the marker detecting unit <b>140</b>, and the display control unit <b>160</b>.</p><p id="p-0131" num="0130">Next, the analyzing unit <b>125</b> executes the analyzing process described above on the input image inputted from the image acquiring unit <b>120</b> (step S<b>120</b>). The analyzing process executed here may for example correspond to one frame out of the SLAM computation process described with reference to <figref idref="DRAWINGS">FIG. <b>5</b></figref>. As a result, the latest three-dimensional camera position and posture and the three-dimensional positions of the new feature points appearing in the input image are stored by the 3D structure DB <b>130</b>.</p><p id="p-0132" num="0131">After this, the marker detecting unit <b>140</b> searches the input image for a marker defined in the marker basic information <b>136</b> (step S<b>130</b>). If a new marker has been detected in the input image by the marker detecting unit <b>140</b> (step S<b>135</b>), the marker managing unit <b>145</b> stores the three-dimensional position and posture and detection time of the new marker in the marker DB <b>135</b> (step S<b>140</b>).</p><p id="p-0133" num="0132">Next, the content control unit <b>155</b> selects the AR content to be displayed (step S<b>150</b>). The AR content selected here may be markers that do not satisfy the removal condition described earlier out of the markers that have been detected and whose detection times are stored in the marker detection information <b>137</b>. The process hereafter branches in step S<b>150</b> according to whether AR content selected by the content control unit <b>155</b> is present (step S<b>155</b>).</p><p id="p-0134" num="0133">If no AR content has been selected by the content control unit <b>155</b>, that is, if there is no AR content to be displayed, the display control unit <b>160</b> sets the input image as it is as the output image (step S<b>160</b>). Meanwhile, if there is AR content to be displayed, the content control unit <b>155</b> decides the three-dimensional display position and display posture in the AR space of the selected AR content and the other control parameters (for example, the transparency) (step S<b>165</b>). The display control unit <b>160</b> then generates the output image by overlaying the AR content on the input image using the decided parameters and the position and posture of the camera (step S<b>170</b>).</p><p id="p-0135" num="0134">The display control unit <b>160</b> then displays the generated output image (which may be the same as the input image) on the screen of the display unit <b>110</b> (step S<b>180</b>). After this, the processing returns to step S<b>110</b> and the processing described above may be repeated for the next frame.</p><p id="p-0136" num="0135">3. Conclusion</p><p id="p-0137" num="0136">The image processing apparatus <b>100</b> according to an embodiment of the present disclosure has been described in detail above with reference to <figref idref="DRAWINGS">FIGS. <b>1</b> to <b>14</b></figref>. According to the above embodiment, markers associated with AR content to be disposed in an AR space are detected in an input image and information on the position and posture in the real space of each detected marker is managed using a storage medium. The position and posture of the camera relative to the detected marker(s) are tracked and the behavior of the AR content associated with such markers is controlled based on at least one of such position and posture. The laying out of AR content is carried out based on analysis results for the input image using an environment recognition technique such as SLAM. Accordingly, it is possible to continue displaying AR content even after a marker has been lost from the image and to maintain a natural displaying of AR content associated with markers. Note that it is not necessary to manage both the position and posture of detected markers in the real space and only one (for example, only the position) may be managed in a database.</p><p id="p-0138" num="0137">Note that some of the logical functions of the image processing apparatus <b>100</b> described earlier may be implemented at an apparatus present in a cloud computing environment instead of being implemented at the image processing apparatus itself. In this case, the information exchanged between the logical functions may be transmitted or received between apparatuses via the communication unit <b>112</b> illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0139" num="0138">The series of control processes carried out by the image processing apparatus <b>100</b> described in the present specification may be realized by software, hardware, or a combination of software and hardware. Programs that compose such software may be stored in advance for example on a storage medium provided inside or outside the image processing apparatus <b>100</b>. As one example, during execution, such programs are written into RAM (Random Access Memory) and executed by a processor such as a CPU.</p><p id="p-0140" num="0139">Although a preferred embodiment of the present disclosure has been described above with reference to the attached drawings, the technical scope of the present disclosure is not limited to such embodiment. It should be understood by those skilled in the art that various modifications, combinations, sub-combinations and alterations may occur depending on design requirements and other factors insofar as they are within the scope of the appended claims or the equivalents thereof.</p><p id="p-0141" num="0140">Additionally, the present technology may also be configured as below.</p><p id="p-0142" num="0141">(1) An information processing system comprising: one or more processing units that: acquire video data captured by an image pickup unit; detect an object from the video data; detect a condition corresponding to the image pickup unit; and control a display to display content associated with the object at a position other than a detected position of the object based on the condition corresponding to the image pickup unit.</p><p id="p-0143" num="0142">(2) The information processing system of (1), wherein the object detected from the video data is at least one of a shape, symbol, character string, design, object, part of an object and an image displayed on a display.</p><p id="p-0144" num="0143">(3) The information processing system of (1) or (2), wherein detecting a condition corresponding to the image pickup unit includes detecting at least one of a position and posture of the image pickup unit.</p><p id="p-0145" num="0144">(4) The information processing system of (3), wherein the one or more processing units detect at least one of a position and a posture of the detected object.</p><p id="p-0146" num="0145">(5) The information processing system of (4), wherein the one or more processors control the position on the display at which the content is displayed based on the at least one of a position and posture of the image pickup unit and the at least one of a position and a posture of the detected object.</p><p id="p-0147" num="0146">(6) The information processing system of any of (1) to (5), further comprising: a sensor unit that detects the condition corresponding to the image pickup unit.</p><p id="p-0148" num="0147">(7) The information processing system of (6), wherein the sensor unit includes at least one of a positioning sensor, an acceleration sensor and a gyrosensor.</p><p id="p-0149" num="0148">(8) The information processing system of any of (1) to (7), further comprising: a memory that stores information corresponding to at least one object in association with information corresponding to the content.</p><p id="p-0150" num="0149">(9) The information processing system of (8), wherein the information corresponding to the content includes at least one of a marker identifier, a related content identifier and a size corresponding to the content to be displayed.</p><p id="p-0151" num="0150">(10) The information processing system of (8), wherein the one or more processing units acquire the content based on a comparison between the detected object and the information corresponding to the at least one object stored in the memory.</p><p id="p-0152" num="0151">(11) The information processing system of any of (1) to (10), further comprising: a memory that stores at least one of identification information, attribute information and an image corresponding to the content.</p><p id="p-0153" num="0152">(12) The information processing system of (11), wherein the one or more processing units acquire the image corresponding to the content the memory based on the detected object.</p><p id="p-0154" num="0153">(13) The information processing system of any of (1) to (12), further comprising: a memory that stores information corresponding to the detection of the object.</p><p id="p-0155" num="0154">(14) The information processing system of (13), wherein the information corresponding to the detection of the object includes at least one of an object identifier, an object position, an object posture, an object detection time and a time corresponding to when the object was no longer detected in the acquired video data.</p><p id="p-0156" num="0155">(15) The information processing system of (14), wherein the one or more processing units control the position on the display at which the content is displayed based on the condition corresponding to the image pickup unit and the information corresponding to the detection of the object.</p><p id="p-0157" num="0156">(16) The information processing system of any of (1) to (15), wherein the one or more processing units control the display to move the content on the display based on a change in at least one of a position and a posture of the image pickup unit.</p><p id="p-0158" num="0157">(17) The information processing system of any of (1) to (16), wherein the one or more processing units control the display to display the content on the display when the object is no longer detected in the acquired video data.</p><p id="p-0159" num="0158">(18) The information processing system of any of (1) to (17), wherein the detected condition corresponding to the image pickup unit includes at least one of a position and a posture of the image pickup unit, and the one or more processing units control the display to stop displaying the content when at least one of the position and the posture of the image pickup unit satisfies a predetermined condition.</p><p id="p-0160" num="0159">(19) The information processing system of (18), wherein the one or more processing units controls the display to display an indication that the display is to stop displaying the content when at least one of the position and the posture of the image pickup unit satisfies the predetermined condition.</p><p id="p-0161" num="0160">(20) The information processing system of (18), wherein the predetermined condition corresponds to a distance between the image pickup unit and the detected object, and the one or more processing units control the display to stop displaying the content when the distance between image pickup unit and the detected object exceeds a predetermined threshold value.</p><p id="p-0162" num="0161">(21) The information processing system of (18), wherein the predetermined condition corresponds to a difference in an angle between an optical axis of the image pickup unit and an axis extending from the image pickup unit to the detected object, and the one or more processing units control the display to stop displaying the content when the difference in angle exceeds a predetermined threshold value.</p><p id="p-0163" num="0162">(22) The information processing system of (18), wherein the predetermined condition corresponds to a time elapsed from when the object was first detected in the acquired video data, and the one or more processing units control the display to stop displaying the content when time elapsed exceeds a predetermined threshold value.</p><p id="p-0164" num="0163">(23) The information processing system of (18), wherein the predetermined condition corresponds to a time elapsed from when the object was last detected in the acquired video data, and the one or more processing units control the display to stop displaying the content when time elapsed exceeds a predetermined threshold value.</p><p id="p-0165" num="0164">(24) An information processing method performed by an information processing system, the method comprising: acquiring video data captured by an image pickup unit; detecting an object from the video data; detecting a condition corresponding to the image pickup unit; and controlling a display to display content associated with the object at a position other than a detected position of the object based on the condition corresponding to the image pickup unit.</p><p id="p-0166" num="0165">(25) A non-transitory computer-readable medium including computer program instructions, which when executed by an information processing system, cause the information processing system to perform a method, the method comprising: acquiring video data captured by an image pickup unit; detecting an object from the video data; detecting a condition corresponding to the image pickup unit; and controlling a display to display content associated with the object at a position other than a detected position of the object based on the condition corresponding to the image pickup unit.</p><heading id="h-0022" level="1">REFERENCE SIGNS LIST</heading><p id="p-0167" num="0000"><ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0166"><b>1</b> Real space</li>    <li id="ul0001-0002" num="0167"><b>20</b><i>a</i>, <b>20</b><i>b</i>, <b>20</b><i>c </i>Marker (object)</li>    <li id="ul0001-0003" num="0168"><b>100</b> Image processing apparatus</li>    <li id="ul0001-0004" num="0169"><b>120</b> Image acquiring unit</li>    <li id="ul0001-0005" num="0170"><b>125</b> Analyzing unit</li>    <li id="ul0001-0006" num="0171"><b>140</b> Detecting unit</li>    <li id="ul0001-0007" num="0172"><b>145</b> Managing unit</li>    <li id="ul0001-0008" num="0173"><b>155</b> Content control unit</li>    <li id="ul0001-0009" num="0174"><b>160</b> Display control unit</li></ul></p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An information processing apparatus, comprising:<claim-text>at least one sensor;</claim-text><claim-text>at least one imaging device;</claim-text><claim-text>a display;</claim-text><claim-text>a memory; and</claim-text><claim-text>circuitry configured to<claim-text>capture an image including a real object;</claim-text><claim-text>detect at least one of a three-dimensional position information and a posture information;</claim-text><claim-text>control the display of an appearance of an augmented reality (AR) object based on the detected information; and</claim-text><claim-text>control the display of the information processing apparatus to continue displaying, on the display of the information processing apparatus, the AR object associated with the real object when the object is no longer detected on the display of the information processing apparatus.</claim-text></claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The information processing apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the circuitry is further configured to detect a status of the information processing apparatus.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The information processing apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the circuitry is further configured to control an appearance of the AR object based on the status of the information processing apparatus.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The information processing apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the circuitry is further configured to control a movement of the AR object based on the status of the information processing apparatus.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The information processing apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the AR object appears in response to a condition being satisfied.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The information processing apparatus of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the condition is determined based on a three-dimensional position and a posture of the at least one imaging device relative to the detected three-dimensional position information and the posture information.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The information processing apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the at least one sensor includes one or more of a positioning sensor, an acceleration sensor, and a gyro sensor.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The information processing apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the circuitry is further configured to<claim-text>control the display to continue displaying the AR object associated with the real object when the object is partially observable in acquired imaging data.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. A method, comprising:<claim-text>capturing, by an imaging device, an image including a real object;</claim-text><claim-text>detecting, by processing circuitry, at least one of a three-dimensional position information and a posture information;</claim-text><claim-text>controlling, by the processing circuitry, a display of an appearance of an augmented reality (AR) object based on the detected information; and</claim-text><claim-text>controlling, by the processing circuitry, a display to continue displaying, on the display, the AR object associated with the real object when the object is no longer detected on the display.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, further comprising:<claim-text>detecting a status of an information processing apparatus.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, further comprising:<claim-text>controlling an appearance of the AR object based on the status of the information processing apparatus.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, further comprising:<claim-text>controlling a movement of the AR object based on the status of the information processing apparatus.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, further comprising:<claim-text>displaying the AR object in response to a condition being satisfied.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, further comprising:<claim-text>determining the condition based on a three-dimensional position and a posture of an imaging device relative to the detected three-dimensional position information and the posture information.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, controlling the display to continue displaying the AR object associated with the real object when the object is partially observable in acquired imaging data.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. A non-transitory computer-readable storage medium storing computer-readable instructions thereon which, when executed by a computer, cause the computer to perform a method, the method comprising:<claim-text>capturing, by an imaging device, an image including a real object;</claim-text><claim-text>detecting, by processing circuitry, at least one of a three-dimensional position information and a posture information;</claim-text><claim-text>controlling, by the processing circuitry, a display of an appearance of an augmented reality (AR) object based on the detected information; and</claim-text><claim-text>controlling, by the processing circuitry, the display to continue displaying, on the display, the AR object associated with the real object when the object is no longer detected on the display.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00016">claim 16</claim-ref>, further comprising:<claim-text>detecting a status of an information processing apparatus;</claim-text><claim-text>controlling an appearance of the AR object based on the status of the information processing apparatus; and</claim-text><claim-text>controlling a movement of the AR object based on the status of the information processing apparatus.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00017">claim 17</claim-ref>, further comprising:<claim-text>displaying the AR object in response to a condition being satisfied.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00018">claim 18</claim-ref>, further comprising:<claim-text>determining the condition based on a three-dimensional position and a posture of an imaging device relative to the detected three-dimensional position information and the posture information.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00016">claim 16</claim-ref>, further comprising:<claim-text>controlling the display to continue displaying the AR object associated with the real object when the object is partially observable in acquired imaging data.</claim-text></claim-text></claim></claims></us-patent-application>