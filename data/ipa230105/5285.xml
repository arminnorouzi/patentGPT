<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005286A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005286</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17710649</doc-number><date>20220331</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>30</main-group><subgroup>416</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>30</main-group><subgroup>146</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>30</main-group><subgroup>148</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>30</main-group><subgroup>414</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>30</main-group><subgroup>413</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>Q</subclass><main-group>30</main-group><subgroup>02</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>30</main-group><subgroup>416</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>30</main-group><subgroup>147</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>30</main-group><subgroup>15</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>30</main-group><subgroup>414</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>30</main-group><subgroup>413</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>Q</subclass><main-group>30</main-group><subgroup>0283</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">METHODS, SYSTEMS, ARTICLES OF MANUFACTURE, AND APPARATUS FOR DECODING PURCHASE DATA USING AN IMAGE</invention-title><us-related-documents><us-provisional-application><document-id><country>US</country><doc-number>63214571</doc-number><date>20210624</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Nielsen Consumer LLC</orgname><address><city>New York</city><state>NY</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Yebes Torres</last-name><first-name>Jose Javier</first-name><address><city>Valladolid</city><country>ES</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Shina</last-name><first-name>Aditi</first-name><address><city>Weehawken</city><state>NY</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Lebrun</last-name><first-name>Christine</first-name><address><city>Paris</city><country>FR</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Oppini</last-name><first-name>Fabio</first-name><address><city>Milan</city><country>IT</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>Bansal</last-name><first-name>Atul</first-name><address><city>New York</city><state>NY</state><country>US</country></address></addressbook></inventor><inventor sequence="05" designation="us-only"><addressbook><last-name>Kumar</last-name><first-name>Mukul</first-name><address><city>New York</city><state>NY</state><country>US</country></address></addressbook></inventor><inventor sequence="06" designation="us-only"><addressbook><last-name>Chandramouli</last-name><first-name>Vignesh</first-name><address><city>New York</city><state>NY</state><country>US</country></address></addressbook></inventor><inventor sequence="07" designation="us-only"><addressbook><last-name>Sousa</last-name><first-name>Filipa</first-name><address><city>New York</city><state>NY</state><country>US</country></address></addressbook></inventor><inventor sequence="08" designation="us-only"><addressbook><last-name>Mercaldi</last-name><first-name>Gisella</first-name><address><city>Oxford</city><country>GB</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Methods, apparatus, systems, and articles of manufacture are disclosed that decode purchase data using an image. An example apparatus includes processor circuitry to execute machine readable instructions to at least crop an image of a receipt based on detected regions of interest, apply a first mask to a first cropped image to generate first bounding boxes corresponding to rows of the receipt, apply a second mask to a second cropped image to generate second bounding boxes corresponding to columns of the receipt, generate a structure of the receipt by mapping words detected by an optical character recognition engine to corresponding first bounding boxes and second bounding boxes based on a mapping criterion, classify the second bounding boxes by identifying an expression of interest in ones of the second bounding boxes, and generate purchase information by extracting text of interest from the structured receipt based on the classifications.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="191.18mm" wi="154.35mm" file="US20230005286A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="234.10mm" wi="177.63mm" file="US20230005286A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="230.04mm" wi="156.38mm" file="US20230005286A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="242.74mm" wi="172.89mm" file="US20230005286A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="227.67mm" wi="139.78mm" file="US20230005286A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="232.24mm" wi="103.72mm" file="US20230005286A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="205.99mm" wi="123.44mm" file="US20230005286A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="231.39mm" wi="144.78mm" file="US20230005286A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="209.38mm" wi="136.74mm" file="US20230005286A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="232.75mm" wi="173.40mm" file="US20230005286A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="235.97mm" wi="165.02mm" file="US20230005286A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="230.97mm" wi="175.01mm" file="US20230005286A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="191.01mm" wi="151.98mm" file="US20230005286A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="242.49mm" wi="109.05mm" file="US20230005286A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="130.73mm" wi="165.44mm" file="US20230005286A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="231.73mm" wi="126.24mm" file="US20230005286A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="232.33mm" wi="105.58mm" file="US20230005286A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="242.49mm" wi="178.48mm" orientation="landscape" file="US20230005286A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="240.71mm" wi="174.07mm" orientation="landscape" file="US20230005286A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00019" num="00019"><img id="EMI-D00019" he="210.40mm" wi="165.18mm" orientation="landscape" file="US20230005286A1-20230105-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00020" num="00020"><img id="EMI-D00020" he="235.54mm" wi="172.21mm" orientation="landscape" file="US20230005286A1-20230105-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00021" num="00021"><img id="EMI-D00021" he="186.94mm" wi="166.45mm" orientation="landscape" file="US20230005286A1-20230105-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00022" num="00022"><img id="EMI-D00022" he="216.92mm" wi="141.22mm" orientation="landscape" file="US20230005286A1-20230105-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00023" num="00023"><img id="EMI-D00023" he="246.63mm" wi="170.01mm" orientation="landscape" file="US20230005286A1-20230105-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00024" num="00024"><img id="EMI-D00024" he="246.89mm" wi="159.94mm" file="US20230005286A1-20230105-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00025" num="00025"><img id="EMI-D00025" he="242.06mm" wi="161.80mm" file="US20230005286A1-20230105-D00025.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00026" num="00026"><img id="EMI-D00026" he="235.54mm" wi="156.38mm" file="US20230005286A1-20230105-D00026.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00027" num="00027"><img id="EMI-D00027" he="239.78mm" wi="153.25mm" file="US20230005286A1-20230105-D00027.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00028" num="00028"><img id="EMI-D00028" he="242.15mm" wi="158.58mm" file="US20230005286A1-20230105-D00028.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00029" num="00029"><img id="EMI-D00029" he="245.19mm" wi="167.89mm" file="US20230005286A1-20230105-D00029.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00030" num="00030"><img id="EMI-D00030" he="218.95mm" wi="150.03mm" file="US20230005286A1-20230105-D00030.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00031" num="00031"><img id="EMI-D00031" he="244.35mm" wi="168.23mm" file="US20230005286A1-20230105-D00031.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00032" num="00032"><img id="EMI-D00032" he="215.56mm" wi="159.26mm" file="US20230005286A1-20230105-D00032.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00033" num="00033"><img id="EMI-D00033" he="242.57mm" wi="165.10mm" file="US20230005286A1-20230105-D00033.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00034" num="00034"><img id="EMI-D00034" he="242.57mm" wi="162.64mm" file="US20230005286A1-20230105-D00034.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00035" num="00035"><img id="EMI-D00035" he="231.56mm" wi="152.99mm" file="US20230005286A1-20230105-D00035.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00036" num="00036"><img id="EMI-D00036" he="246.21mm" wi="167.05mm" file="US20230005286A1-20230105-D00036.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00037" num="00037"><img id="EMI-D00037" he="244.86mm" wi="175.77mm" file="US20230005286A1-20230105-D00037.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00038" num="00038"><img id="EMI-D00038" he="168.83mm" wi="63.50mm" file="US20230005286A1-20230105-D00038.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">RELATED APPLICATION</heading><p id="p-0002" num="0001">This patent claims the benefit of U.S. Provisional Patent Application No. 63/214,571, which was filed on Jun. 24, 2021. U.S. Provisional Patent Application No. 63/214,571 is hereby incorporated herein by reference in its entirety. Priority to U.S. Provisional Patent Application No. 63/214,571 is hereby claimed.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">FIELD OF THE DISCLOSURE</heading><p id="p-0003" num="0002">This disclosure relates generally to computer-based image analysis and, more particularly, to methods, systems, articles of manufacture, and apparatus for decoding purchase data using an image.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">Artificial intelligence (AI) leverages computers and machines to mimic problem solving and decision making challenges that typically require human intelligence. Deep learning (DL), computer Vision (CV) and Natural Language Processing (NLP) are powerful AI techniques that can be combined to process an image. For example, these AI techniques can be applied to an image of a purchase document to extract and decode data from the purchase document.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0005" num="0004"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic illustration of an example environment constructed in accordance with the teachings of this disclosure for collecting purchase data from a consumer.</p><p id="p-0006" num="0005"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a schematic illustration of an example system for generating purchase data from a receipt image and corresponding barcodes constructed in accordance with the teachings of this disclosure.</p><p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a block diagram of the example extraction circuitry of <figref idref="DRAWINGS">FIGS. <b>1</b> and/or <b>2</b></figref>.</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>4</b>A</figref> is an illustration of an example receipt region of an example receipt image that can be output by the example regions detection circuitry of <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>4</b>B</figref> is an illustration of an example products region of the receipt image of <figref idref="DRAWINGS">FIG. <b>4</b>A</figref> that can be output by the example regions detection circuitry of <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>5</b>A</figref> is an illustration of an example pixel-wise mask on an example receipt region that can be output by an example row detection model.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>5</b>B</figref> is an illustration of the receipt region of <figref idref="DRAWINGS">FIG. <b>5</b>A</figref> after a post-processing block generated bounding boxes corresponding to text lines based on the pixel-wise mask in accordance with the teachings of this disclosure.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>6</b>A</figref> is an illustration of an example products region of an example receipt image including bounding boxes corresponding to lines of text with missed connections between the lines of text.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>6</b>B</figref> is an illustration of the products region of <figref idref="DRAWINGS">FIG. <b>6</b>A</figref> after a post-processing block merged the bounding boxes to connect the lines of text in accordance with the teachings of this disclosure.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>7</b>A</figref> is an illustration of an example pixel-wise classifier on a products regions of an example receipt image that can be output by an example column segmentation model.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>7</b>B</figref> is an illustration of the products region of <figref idref="DRAWINGS">FIG. <b>7</b>A</figref> after a post-processing block generated bounding boxes corresponding to columns based on the pixel-mask classifier in accordance with the teachings of this disclosure.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>8</b>A</figref> is an illustration of an example products portion of an example receipt image including bounding boxes corresponding to columns with a missed connection between two bounding boxes that belong to one column.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>8</b>B</figref> is an illustration of the example products region of <figref idref="DRAWINGS">FIG. <b>8</b>A</figref> after a post-processing block merged the two bounding boxes to generate the column in accordance with the teachings of this disclosure.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>9</b></figref> illustrates an example implementation of the example extraction circuitry of <figref idref="DRAWINGS">FIGS. <b>1</b>, <b>2</b></figref>, and/or <b>3</b> including outputs by different models for extracting purchase information from a receipt image.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a block diagram of an example system, including example machine learning circuitry, for training models for information extraction constructed in accordance with the teachings of this disclosure.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>11</b>A</figref> is an illustration of an example image pair that can be used to train an example row detection model.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>11</b>B</figref> is an illustration of an example image pair that can be used to train an example column segmentation model.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a schematic illustration of an example R-CNN architecture that can be used to train and implement an example regions detection model.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>13</b></figref> illustrates an example implementation of the example row detection architecture in accordance with the teachings of this disclosure.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a schematic illustration of an example U-Net architecture that can be used to train and implement an example column segmentation model.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>15</b>A</figref> is an illustration of example receipt image output by an example row detection model and example post-processing block, wherein the row detection model is based on a dhSegment architecture.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>15</b>B</figref> is an illustration of the example receipt image of <figref idref="DRAWINGS">FIG. <b>15</b>A</figref> output by an example row detection model and example post-processing block, wherein the row detection model is based on a U-Net architecture.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIGS. <b>16</b>A-<b>16</b>D</figref> are illustrations of example receipt regions output by example column segment models based on the U-Net and example post-processing blocks in accordance with the teachings of this disclosure.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is a block diagram of the example decoding circuitry of <figref idref="DRAWINGS">FIGS. <b>1</b> and/or <b>2</b></figref>.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>18</b></figref> is a block diagram of an example framework to match a purchased product and a barcode constructed in accordance with the teachings of this disclosure.</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>19</b></figref> is an illustration of an example search query generated to search an example internal dictionary.</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>20</b></figref> is an illustration of another example search query generated to search an example products datastore.</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>21</b></figref> is an illustration of an example matrix including matching scores for pairs of product descriptions and barcodes and an example list of the identified matches that can be output by the example decoding circuitry of <figref idref="DRAWINGS">FIGS. <b>1</b>, <b>2</b></figref>, and/or <b>18</b>.</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>22</b></figref> is a block diagram of example framework to process products having variable weights.</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>23</b></figref> is an illustration of another example search query generated to search an example previous jobs database.</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>24</b></figref> is a block diagram of another example implementation of the example extraction circuitry of <figref idref="DRAWINGS">FIGS. <b>1</b>,<b>2</b> and/or <b>3</b></figref>.</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>25</b></figref> is a block diagram of another example implementation of the example decoding circuitry of <figref idref="DRAWINGS">FIG. <b>1</b>, <b>2</b></figref>, and/or <b>17</b>.</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIGS. <b>26</b>-<b>34</b></figref> flowcharts representative of example machine readable instructions and/or example operations that may be executed by example processor circuitry to implement the document decode circuitry of <figref idref="DRAWINGS">FIGS. <b>1</b>, <b>2</b>, <b>3</b>, <b>9</b>, <b>10</b>, <b>17</b>, <b>18</b>, <b>24</b></figref>, and/or <b>25</b>.</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>35</b></figref> is a block diagram of an example processing platform including processor circuitry structured to execute the example machine readable instructions and/or the example operations of <figref idref="DRAWINGS">FIG. <b>3</b></figref> to implement the document decode circuitry of <figref idref="DRAWINGS">FIGS. <b>1</b>, <b>2</b>, <b>3</b>, <b>9</b>, <b>10</b>, <b>17</b>, <b>18</b>, <b>24</b></figref>, and/or <b>25</b>.</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>36</b></figref> is a block diagram of an example implementation of the processor circuitry of <figref idref="DRAWINGS">FIG. <b>35</b></figref>.</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>37</b></figref> is a block diagram of another example implementation of the processor circuitry of <figref idref="DRAWINGS">FIG. <b>35</b></figref>.</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>38</b></figref> is a block diagram of an example software distribution platform (e.g., one or more servers) to distribute software (e.g., software corresponding to the example machine readable instructions of <figref idref="DRAWINGS">FIGS. <b>26</b>-<b>34</b></figref>) to client devices associated with end users and/or consumers (e.g., for license, sale, and/or use), retailers (e.g., for sale, re-sale, license, and/or sub-license), and/or original equipment manufacturers (OEMs) (e.g., for inclusion in products to be distributed to, for example, retailers and/or to other end users such as direct buy customers).</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0042" num="0041">In general, the same reference numbers will be used throughout the drawing(s) and accompanying written description to refer to the same or like parts. The figures are not to scale. Instead, the thickness of the layers or regions may be enlarged in the drawings. Although the figures show layers and regions with clean lines and boundaries, some or all of these lines and/or boundaries may be idealized. In reality, the boundaries and/or lines may be unobservable, blended, and/or irregular.</p><p id="p-0043" num="0042">As used herein, connection references (e.g., attached, coupled, connected, and joined) may include intermediate members between the elements referenced by the connection reference and/or relative movement between those elements unless otherwise indicated. As such, connection references do not necessarily infer that two elements are directly connected and/or in fixed relation to each other. As used herein, stating that any part is in &#x201c;contact&#x201d; with another part is defined to mean that there is no intermediate part between the two parts.</p><p id="p-0044" num="0043">Unless specifically stated otherwise, descriptors such as &#x201c;first,&#x201d; &#x201c;second,&#x201d; &#x201c;third,&#x201d; etc., are used herein without imputing or otherwise indicating any meaning of priority, physical order, arrangement in a list, and/or ordering in any way, but are merely used as labels and/or arbitrary names to distinguish elements for ease of understanding the disclosed examples. In some examples, the descriptor &#x201c;first&#x201d; may be used to refer to an element in the detailed description, while the same element may be referred to in a claim with a different descriptor such as &#x201c;second&#x201d; or &#x201c;third.&#x201d; In such instances, it should be understood that such descriptors are used merely for identifying those elements distinctly that might, for example, otherwise share a same name.</p><p id="p-0045" num="0044">As used herein, &#x201c;approximately&#x201d; and &#x201c;about&#x201d; refer to dimensions that may not be exact due to manufacturing tolerances and/or other real world imperfections. As used herein &#x201c;substantially real time&#x201d; refers to occurrence in a near instantaneous manner recognizing there may be real world delays for computing time, transmission, etc. Thus, unless otherwise specified, &#x201c;substantially real time&#x201d; refers to real time +/&#x2212;1 second.</p><p id="p-0046" num="0045">As used herein, the phrase &#x201c;in communication,&#x201d; including variations thereof, encompasses direct communication and/or indirect communication through one or more intermediary components, and does not require direct physical (e.g., wired) communication and/or constant communication, but rather additionally includes selective communication at periodic intervals, scheduled intervals, aperiodic intervals, and/or one-time events.</p><p id="p-0047" num="0046">As used herein, &#x201c;processor circuitry&#x201d; is defined to include (i) one or more special purpose electrical circuits structured to perform specific operation(s) and including one or more semiconductor-based logic devices (e.g., electrical hardware implemented by one or more transistors), and/or (ii) one or more general purpose semiconductor-based electrical circuits programmed with instructions to perform specific operations and including one or more semiconductor-based logic devices (e.g., electrical hardware implemented by one or more transistors). Examples of processor circuitry include programmed microprocessors, Field Programmable Gate Arrays (FPGAs) that may instantiate instructions, Central Processor Units (CPUs), Graphics Processor Units (GPUs), Digital Signal Processors (DSPs), XPUs, or microcontrollers and integrated circuits such as Application Specific Integrated Circuits (ASICs). For example, an XPU may be implemented by a heterogeneous computing system including multiple types of processor circuitry (e.g., one or more FPGAs, one or more CPUs, one or more GPUs, one or more DSPs, etc., and/or a combination thereof) and application programming interface(s) (API(s)) that may assign computing task(s) to whichever one(s) of the multiple types of the processing circuitry is/are best suited to execute the computing task(s).</p><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0048" num="0047">Marketing intelligence entities provide manufacturers and retailers with a complete picture of the complex marketplace and actionable information that brands need to grow their businesses. To do so, some marketing research entities collect and analyze purchase data to extract insights. A common source of such purchase data includes consumer panels, which are groups of individuals that have agreed to provide their purchase data and/or other types of data, such as demographic data, to the entity. A consumer panel member (e.g., a panelist) can record individual and/or household purchases (e.g., purchase data) from various retailers and transmit the purchase data to the marketing research entity for analysis. Typically, the panelist represents at least one demographic characteristic such as, but not limited to geographic location, household income, presence of children, etc., enabling the marketing research entity to extract insights about consumer purchase behavior beyond just a sale of a product. Consequently, this data source can be very important for the marketing intelligence entity to facilitate, for example, marketing initiatives that target consumers more likely to purchase goods and/or services of interest.</p><p id="p-0049" num="0048">Panelists around the world share their purchased baskets with marketing research entities using different collection methods. As disclosed herein, a basket refers to a group of items purchased by the panelist and identified on a receipt. Thus, sharing the basket with the entity includes providing purchase information to the entity such as purchase details for each product purchased (e.g., product description, price of product, quantity purchased, etc.), promotions applied, etc. as well as barcodes corresponding to each of the purchased products. In some examples, the panelist can use an electronic device (e.g., a smartphone, a tablet, etc.) to manually enter purchase information for the purchased products and to scan the barcodes of the purchased products (e.g., using a camera of the electronic device and/or a barcode scanner). However, this task can be quite time-consuming and is often burdensome for the panelist, especially for large baskets with numerous items. In some examples, these burdens diminish the panelists' willingness to collaborate with the marketing research entity long term, resulting in reduced data capture by the entity. The reduced data capture can result in missed valuable data that can limit an ability of the research marketing entity to collect actionable consumer behavior data such as consumer consumption and/or impulse purchases, etc. Some entities incorporate and/or rely on reward programs to recruit and/or retain panelists, which can be costly for the entity. For example, the marketing intelligence entity may incorporate a rewards program to maintain a consistent (e.g., dependable) level of data capture and/or increase a level of data capture.</p><p id="p-0050" num="0049">In some examples, the marketing research entity can shift the burden of recording purchase data from the panelist to a workforce. For example, the panelist can save a receipt(s) for a purchase(s) and/or the purchased products as listed in the receipt in a bin (e.g., container, etc.). The workforce can collect the purchase data by periodically and/or aperiodically visiting the panelist's bin to manually enter the purchase data from the receipt(s) (e.g., by scanning the purchased product and entering purchase information). While this collection method eases the burden on the panelist, the process is still manual and may require the entity to train and/or maintain a field of personnel uniquely qualified to perform such tasks. As such, this collection method is typically resource intensive, time-consuming, and costly for the marketing research entity.</p><p id="p-0051" num="0050">In yet another collection method, the marketing research entity can collect the purchase data from purchase documents. Purchase documents, such as receipts and/or invoices, memorialize a transaction between a consumer and a retailer. Such documents can include different information. For example, the receipt typically includes a list of purchased goods and/or services, prices paid, details associated with the point of sale, the total price, and any tax assessed on the transaction. The panel member can capture an image (e.g., via a camera, scanner, etc.) of the receipt and transmit the image to the entity for processing. A workforce can manually process the purchase document to extract purchase-related information corresponding to the purchased basket. For example, the workforce can transcribe, digitize, and store the receipt and/or extracted purchase data in a database. In some examples, the workforce can extract information from relevant fields of the receipt, such as retailer name, product names, item codes, item prices, price total, date, and time, etc. However, this collection method is still manual and may require the entity to train and/or maintain a workforce, which can be resource intensive, time consuming, and costly for the marketing research entity.</p><p id="p-0052" num="0051">Based at least on the foregoing issues, there is a need to transform the manual data collection by auditors and panelists and to provide new tools that can revolutionize current processes towards technology driven product organization. Examples disclosed herein facilitate technology (e.g., automation) to the collection of purchase data from panelists and provide a large improvement on the productivity, error reduction, and digitalization of marketing intelligence companies. Further, technological (e.g., automatic) examples disclosed herein facilitate extraction of purchase data from the receipt to boost the entity's throughput by enabling the entity to collect more data with improved accuracy and, consequently, increase the entity's profits.</p><p id="p-0053" num="0052">Recent advances in artificial intelligence (AI) enable marketing intelligence entities to solve new and challenging business use cases, such as the automatic extraction of information from an image. For example, applying AI techniques to receipt processing can result in more efficient and cost effective processes. Deep learning (DL), computer vision (CV), and natural language processing (NLP) are powerful AI techniques that can be combined to process an image. Deep learning is subset of machine learning in which a neural network attempts to simulate the human brain by &#x201c;learning&#x201d; from collections of data. Computer vision is a field of AI that trains computers and machines to interpret and understand an image and to act accordingly. Natural language processing is a field of AI concerned with giving computers the ability to understand human language as it is written. Further, progress in the areas of data science, cloud technology, and DevOps can be applied to receipt processing. DevOps (e.g., software development and information technology (IT) operations) refers to a methodology or practice that aims to increase an efficiency and speed of software development by implementing collaboration between software developers and IT teams.</p><p id="p-0054" num="0053">Disclosed herein are example methods, systems, articles of manufacture, and apparatus for extracting and decoding (e.g., automatically) purchase data using images of receipts. Examples disclosed herein can reduce a burden on a panelist in providing purchase-related information by automatically detecting the information from a receipt uploaded by the panelist. By uploading the receipt, the panelist no longer has to manually enter purchase information such as a product(s) purchased, a price(s), a quantity(ies), and/or a promotion(s) applied to the basket. Additionally, because the panelist burden is relieved via a technological process, erroneous human behaviors are eliminated. Further, the task of providing basket information to the entity that would take, for example, 20 minutes to manually enter can take significantly less of the panelist's time (e.g., only 5 minutes). In some examples, the decoding process reduces an entity's reliance on rewards programs to recruit and/or retain panelists by reducing the burden on the panelists. In some examples, reducing the burdens on the panelist reduces the churn of panelists recruited to participate in consumer panels. Further, automating the extraction of information from a receipt facilitates the collection of more information (e.g., more details of baskets of purchased goods and/or more baskets) by the entity.</p><p id="p-0055" num="0054">In examples disclosed herein, cooperating consumers transmit images of receipts and corresponding barcodes (e.g., universal product code (UPC), European article number (EAN), etc.) to the entity via an electronic device such as a smartphone or tablet. For example, the panelist can download a software application (e.g., application) onto the electronic device and transmit the receipt image and barcodes to the entity via the application. As disclosed herein, the barcode is a unique code that represents a specific product in a machine-readable form. For example, the barcode can include a unique composition of symbols such as numbers, letters, shapes, and/or a specific pattern of stripes.</p><p id="p-0056" num="0055">In some examples, the receipt image is a digital receipt provided to the panelist by a corresponding retailer (e.g., via email, a website, an application, etc.). In some examples, the panelist captures an image of a paper receipt via a camera and/or other image capture device and transmits the captured image to the entity. For example, the panelist can capture the image of a receipt (e.g., receipt image) using the application and a camera on the electronic device. The panelist can also use the electronic device to scan barcodes of barcoded products and/or select barcodes (e.g., for non-barcoded products such fruits, vegetables, fish, etc.) from a codebook, list, library, or other database to provide information about the purchase. In some examples, the codebook or database is also included in the application. In some examples, the panelist also provides a store (e.g., retailer, vendor, merchant, etc.) from which the basket was purchased. For example, the panelist can select the store from a storelist in the application and/or from a favorite's store list (e.g., with store ID of physical location) associated with the panelist.</p><p id="p-0057" num="0056">In examples disclosed herein, collected basket information (e.g., image receipt, barcodes, and retailer) entered into the application by the panelist can be compiled and uploaded to the entity for processing. Examples disclosed herein automatically process the panelists' basket information to collect (e.g., generate, gather, harvest, etc.) purchase data. For example, the entity can apply a ubiquitous and transparent decoding process that extracts purchase information contained in the receipt image and decodes the basket. For example, the decoding process includes associating barcodes uploaded by the panelist with purchase details extracted from the receipt. In some examples, the basket information is uploaded to a backend service that is structured to decode the purchased basket. For example, the decoding process can be deployed as a cloud service (e.g., Software as a Service, Function as a Service, etc.) by a cloud service provider. In some examples, the decoded purchase data can be forwarded to the entity's back office (e.g., via a backend server) for trend analysis and insight extraction.</p><p id="p-0058" num="0057">Automating the receipt decoding process poses several technological, analytical, and/or real-world challenges. In some examples, there can be inherent challenges in processing receipt images based on the nature of the collection process. For example, receipt images are often captured by mobile devices, which means they can be taken in less controlled conditions. Such a collection process can result in issues with a perspective and/or viewpoint of the receipt image, image quality, receipt defects, etc. Moreover, different retailers in different countries have different printing layouts. All this variance in the appearance of the receipt images necessitates a solution that can generalize well to new formats based on large collections of receipt images.</p><p id="p-0059" num="0058">To overcome the foregoing challenges, examples disclosed herein apply AI techniques such as natural language processing, deep learning, and computer vision to the decoding process, which play a significant role in understanding, learning, and modeling specific processing modules that can automatically detect targeted information (e.g., product purchased, price paid, quantity purchased, etc.) on the receipt images. In some examples, the decoding process includes two main phases (e.g., parts, modules, etc.), including an extraction phase in which information is extracted from the image of the receipt and a decoding phase in which detected items (e.g., purchased products) are matched with corresponding barcodes. During the extraction phase, disclosed examples analyze the image to obtain text, detect regions of interest, identify structure in the detected regions of interest, map the obtained text to the identified structure to generate a digitized receipt, and interpret the text in the digitized receipt to generate purchase-related information, such as a list of items (e.g., products) purchased, quantities, prices, applied promotions, and/or a total spent. During the decoding phase, a list (e.g., a first list) of purchased products extracted during the extraction phase and a list (e.g., a second list) of barcodes provided by the panelist can be used to generate matches (e.g., associations) among the two lists. In other words, the decoding process includes detecting and generating associations between product descriptions and provided barcodes. Examples disclosed herein can thus report decoded purchase data that includes prices and quantities associated with specific purchased barcodes.</p><p id="p-0060" num="0059">Examples disclosed herein extract text from the receipt images using optical character recognition (OCR) techniques, which include aspects of computer vision and natural language processing. Techniques based on OCR examine images pixel by pixel, looking for shapes that match character traits. A standard out-of-the-box OCR engine can detect text, generate text boxes corresponding to the text, determine locations (e.g., coordinates) of the text boxes, and transcribe the text. While OCR engines are generally capable of recognizing, detecting, and transcribing text, the OCR output does not guarantee a strict top-to-bottom, left-to-right ordering in the list of obtained words. Further, OCR engines tend to struggle to properly align and arrange detected words in receipts because receipts are often wrinkled (e.g., resulting in non-flat deformations), worn, and/or otherwise difficult for the OCR engine to read. Also, receipts vary in layout (e.g., based on country, store, etc.) and can be captured with differing viewpoints and/or perspectives. For example, the captured receipt images can be skewed, tilted in any direction, squeezed, rotated by some angle, etc. While layouts of receipt can vary, receipts are typically constructed to contain ordered information. For example, information corresponding to purchased products and their respective prices are often horizontally aligned across a receipt. In some examples, failure of an OCR engine to properly align the text in the receipt can result in improperly associated products and prices during the decoding phase, which can reduce the usefulness of the receipt image.</p><p id="p-0061" num="0060">Systems, methods, apparatus, and articles of manufacture disclosed herein correct the above-noted deficiencies by detecting a structural layout of the receipt in a manner that is independent of the OCR output. For example, the structural layout can be detected by identifying regions of interest and using those regions of interest to detect rows and columns of the receipt from the receipt image. In other words, examples disclosed herein detect the rows and columns from the image of the receipt rather than from text of the receipt. Certain disclosed examples apply object detection and recognition techniques to detect the regions of interest, row, and columns. Examples disclosed herein can combine the OCR output with the detected structural layout to generate a digitized receipt from the receipt image. In some examples, the digital receipt is a symbolic representation of a corresponding paper receipt that can be understood and/or processed by computers.</p><p id="p-0062" num="0061">Certain examples detect the regions of interest by applying a trained AI model to the receipt image. In some examples, the AI model is based on a region-based convolutional neural network (R-CNN). However, other neural networks can be used additionally or alternatively, such as faster R-CNNs, deep neural networks (DNN), etc. In some examples, there are two main regions of interest in receipt image including a first region, which is an orthogonal (e.g., rectangular) area in the receipt image where the receipt is present, and a second region, which is a rectangular area around printed text that contains the purchase details (e.g., product description, price, quantity, etc.). Receipt images uploaded by panelists tend to include clutter in a background of the image. In some examples, the background clutter can include irrelevant and/or unwanted text, visual texture, etc. For example, the receipt image can include the receipt and another document adjacent the receipt that can contribute noise and/or undesired text detection during an OCR process. In some examples, the first region (e.g., the receipt region) is needed to segment out (e.g., separate) the receipt from such background clutter. In some examples, the second region (e.g., the products region) is the area containing a list of purchased products with product descriptions (e.g., item descriptions), quantities, prices, promotions, discounts and is thus a main targeted region for extracting data from the receipt. Therefore, detecting the regions of interest can strengthen the extraction process by focusing on a specific region of the receipt image. In some examples, detecting the regions of interest can speed up the decoding process by focusing on data that needs to be processed. In some examples, detecting the regions of interest can reduce instances of falsely identifying other non-product related text in the receipt.</p><p id="p-0063" num="0062">Examples disclosed herein utilize the receipt region to detect rows representing the receipt's horizontal structure. In some examples, row detection can enable structuring of scattered words on the receipt by grouping together words that belong to the same line. Certain examples detect the rows by applying a CNN-based pixel-wise predictor (e.g., an AI model) to the receipt image. In some examples, the pixel-wise predictor outputs a pixel-wise mask that identifies each pixel in the image as belonging to part of a text line (e.g., a row) or part of the background (e.g., the rest of the receipt). In some examples, the output of the pixel-wise mask is post-processed (e.g., refined) by detecting polygons representing rows of the receipt. For example, the polygons can be detected by identifying groups of pixels that belong to the same line. In some examples, a merging process is applied to the detected polygons to merge unconnected polygons belonging to the same row. In some examples, the row detection techniques can be used to generate a list of polygonal regions (e.g., bounding boxes) representing the rows of the receipt including locations (e.g., coordinates) of the polygonal regions.</p><p id="p-0064" num="0063">Examples disclosed herein utilize the products region (e.g., purchase region, etc.) to detect columns representing the receipt's vertical structure. In some examples, column detection can be used to identify tabulated lines that include purchase details and/or lines that include promotions and/or discounts applied to the purchased products. In other words, detecting the columns enables identification of elements listed in the detected lines (e.g., rows). Certain examples detect the columns by applying a CNN-based AI model to the products region of the receipt image. For example, the CNN-based AI model can be based on semantic pixel-wise segmentation techniques that generate a pixel-wise classification of the receipt image. In some examples, the CNN-based segmentation model identifies pixels of the image that belong to a column region. The CNN-based segmentation model is a generic detection method that can work on any type of column. For example, the CNN-based segmentation model identifies a pixel as belonging to a column, but does not identify the type of column. That is, the columns are detected in a manner that is independent of OCR output. In some examples, the detected columns are classified at another step of the decoding process. In some examples, an output of the CNN-based segmentation model is post-processed by detecting polygons representing columns of the receipt and/or merging polygons that are determined to belong to the same column. For example, the post-processing steps can be used to generate a list of polygonal regions (e.g., bounding boxes) representing the columns of the receipt that include coordinates for the polygonal regions. In some examples, the post-processing steps are similar to the post-processing steps applied during row detection.</p><p id="p-0065" num="0064">Examples disclosed herein combine the detected columns, the detected rows, and the detected text from the OCR engine to determine and form a structure (e.g., layout, table) of the receipt. In some examples, coordinates of the detected bounding boxes are transformed (e.g., adjusted, corrected, etc.) to reference coordinates of the original receipt image. Disclosed examples map the words and/or characters detected by the OCR engine to corresponding columns and rows. In some examples, Intersection over Union (IoU) calculations are used to map (e.g., assign) each of the words to their respective columns and rows. IoU is a metric for measuring overlap between two bounding boxes by comparing a ratio of an overlap area of the two bounding boxes to a total area of the two bounding boxes. In some examples, the words are assigned to a column and/or row if the IoU ratio reaches a threshold value. In some examples, the threshold value is approximately 0.5 (e.g., 50%). However, the threshold value can be higher or lower in additional or alternative examples. In some examples, the receipt can be substantially fully structured after the words generated by the OCR engine are assigned to the detected rows and/or columns. For example, mapping the word to the detected rows and/or columns generates an example data frame. As disclosed herein, a data frame refers to data displayed in a format as a table. While the data frame can include different types of columns, each column in the data frame should have the same type of data.</p><p id="p-0066" num="0065">Examples disclosed herein digitize the receipt by detecting rows and columns and assigning words detected by OCR to the rows and columns. Examples disclosed herein can utilize the digitized receipt to extract purchase information from the digitized receipt, such as purchase details (e.g., product descriptions, prices, quantities, etc.) and promotion information (e.g., discounts, multi-buy, etc.). Examples disclosed herein apply post-processing methods to the data frame to extract purchase details to be used during the decoding phase. For example, the extracted purchase details are used during the decoding phase to match extracted product descriptions to respective barcodes. In some examples, the purchase details are extracted and transmitted to a decoding module. In some examples, additional purchase information is extracted and associated with the decoded basket (e.g., matched product descriptions and barcodes), such as promotions applied to the basket.</p><p id="p-0067" num="0066">In some examples, NLP techniques are applied to the detected columns to classify (e.g., identify) the columns. For example, the NLP techniques can be used to classify column headers that identify targeted text in the rows. For example, the columns can be classified using generic regular expressions (e.g., regex)) that can be implemented by a regex engine. As disclosed herein, a regex is a sequence of characters that specifies a search pattern that can be used to match and locate text. Disclosed examples include regex for targeted purchase details such as product description, price, and/or product code. For example, if a string of characters in a column matches the product description regex, the column can be classified as a product description column. In some examples, columns that do not match a defined regex are classified as an unknown column type. After the columns are identified, the product description, price, and/or product code columns are extracted as purchase details to be used for the decoding process.</p><p id="p-0068" num="0067">Examples disclosed herein apply a refining process to the extracted purchase details to remove words or elements that are not needed during the decoding phase. Certain examples apply the regex engine over each row to validate and/or drop elements or rows that do not qualify a condition defined by a regex. For example, a word in a row that does not qualify a condition defined in the regex of a respective column can be removed (e.g., erased, dropped, etc.) from the extracted purchase details. For example, a price word can be removed from an product description column, letters can be removed from a price column, etc. Certain examples include a regex for quantity. For example, some receipts include a quantity in the product description column (e.g., before the product description). Accordingly, some examples disclosed herein include a product description regex specifying that a number followed by a spaced followed by a product description is determined to be a quantity. In some examples, the quantity is extracted and associated with a respective purchased product, but is removed from the product description. Certain examples include a collection of words that indicate that a particular row can be determined as invalid for the extraction process. In some examples, the collection of words are referred to as stop words (e.g., stopwords) and are stored in a database. In some examples, the stopwords are a collection of words that are determined to not correspond to purchase details such as, but not limited to total, promotion, multibuy, etc. Examples disclosed herein remove rows that include stopwords from the extracted purchase details. However, certain examples extract such information from such rows to be associated with decoded basket.</p><p id="p-0069" num="0068">Examples disclosed herein utilize the extracted purchase details to decode the purchased basket. In some examples, the information extracted from the receipt can include a list of purchased products that includes, for each purchased product, a product description, a price, and a quantity. In some examples, the panelist provided barcodes for each of the purchased products. However, it may not be known which purchased product corresponds to which barcode. Examples disclosed herein decode the purchased basket by matching (e.g., associating) an extracted product description and corresponding price and quantity with a respective barcode provided by the panelist. In some examples, each purchased product is matched with a respective barcode. In some examples, however, less than all purchased products are matched with a respective barcode. For example, the decoding phase may not be able to match at least one extracted product description with a barcode.</p><p id="p-0070" num="0069">Examples disclosed herein incorporate data from different sources to perform a matching process between extracted product descriptions and uploaded barcodes. Example systems, apparatus, and articles of manufacture disclosed herein generate a search query that includes a first product description of the purchased products detected in the extraction phase as well as a list of the barcodes provided by the panelist. In some examples, the query also includes a store identifier (ID) corresponding to a store from which the receipt was obtained. In some examples, duplicate barcodes are removed so the list of barcodes is a list of unique barcodes. As used herein, a list of unique barcodes represents a list that does not contain duplicate barcodes (e.g., two or more of the same barcodes). For example, if two barcodes provided by the panelist are identical, one of the barcodes can be removed from the query and the other barcode can remain in the query. In some examples, the query is searched against an internal dictionary to identify a match between the first product description and a barcode. In some examples, the internal dictionary is a database that includes previously matched product descriptions and barcodes. For example, the internal dictionary can be generated by the monitoring entity by decoding a plurality of receipts. The internal dictionary can be built and expanded upon over time as more receipts are processed. In some examples, the internal dictionary includes pre-associated barcodes and product descriptions provided by specific retailers who share such information with the document decoding system. For example, the internal dictionary can include information from external dictionaries provided by various retailers.</p><p id="p-0071" num="0070">If the first product description is matched with a barcode, the result can be saved and/or a second product description can be searched. If the first product description is not matched, certain examples disclosed herein generate second query and search the second query against a second database. In some examples, the second database is quite large compared to the internal database. For example, the second database can be a products datastore that includes numerous products, descriptions, barcodes, and other attributes of the products. In some examples, the second query can include the same information as the initial query, including the first product description, the list of barcodes, and the store ID. However, the second query can include different conditions. For example, the initial query can include a condition that demands a substantially exact match of product description for a response and the second query can include a condition for an n-gram match. An n-gram match is a criterion that identifies a match if n elements are shared. For example, if n is 2 and the element is a word, a match can be determined if any two words are the same. In some examples, the second query is searched against the second database to detect a match between the first product description and a barcode. If the first product description is matched with a barcode, the results can be saved and/or a second product can be searched. If the first product is not matched, the second query can be adjusted and searched against the second databased a second time. For example, the second query can be adjusted to remove the store ID. If the first product description is matched with a barcode, the results can be saved and/or a second product can be searched.</p><p id="p-0072" num="0071">In some examples, a third query is generated if the first product description is not matched after searching the second database. The third query can be searched against a third database. For example, the third database can be a historical database that includes &#x201c;previous jobs.&#x201d; In some examples, the historical database is known as &#x201c;previous jobs&#x201d; because each processed receipt can be designated as a job to be served by a cloud service. In some examples, the third query can be searched against the historical database by comparing the third query against historical receipts with the same store ID. If the first product description is matched with a barcode, the results can be saved and/or a second product can be searched. If the first product description is not matched with a barcode, the corresponding receipt can be uploaded to the previous jobs database to potentially be identified at a later time.</p><p id="p-0073" num="0072">In some examples, artificial intelligence and machine learning are leveraged to build and supplement the internal dictionary by associating item descriptions and barcodes based on the analyzed receipt images. In some examples, further details are extracted from receipts to enabling monitoring of trends that provide a greater level of granularity regarding consumer behavior than receipt-only monitoring. Item descriptions on receipts tend to be abbreviated, variable, interchangeable, inconsistent within or across retailers, and missing information such as, for example, barcodes. In examples disclosed herein, actional insights based on data related to consumer brand switch, consumer preferences, efficacy of promotions, online/offline purchasing activity, path to purchase, market penetration, purchase occasion, and market segmentation can be determined from the data analyzed and created in accordance with the details of this disclosure.</p><p id="p-0074" num="0073">Examples disclosed herein may be part of a larger document decoding service (DDS) that can extract and/or decode various types of documents. While examples disclosed herein are applied to receipts, it is understood that examples disclosed herein can be applied to other documents as well, such as invoices and/or other purchase documents. Further, examples disclosed herein can be applied to extraction and decoding of images in other industries or applications, such as historical document digitization, banking and commercial operations, mail sorting, etc. In other words, the example DDS as disclosed herein can be designed to support several types of documents, such as invoices, receipts, historical documents, bank notes, etc.</p><p id="p-0075" num="0074">Artificial intelligence (AI), including machine learning (ML), deep learning (DL), and/or other artificial machine-driven logic, enables machines (e.g., computers, logic circuits, etc.) to use a model to process input data to generate an output based on patterns and/or associations previously learned by the model via a training process. For instance, the model may be trained with data to recognize patterns and/or associations and follow such patterns and/or associations when processing input data such that other input(s) result in output(s) consistent with the recognized patterns and/or associations.</p><p id="p-0076" num="0075">Many different types of machine learning models and/or machine learning architectures exist. In examples disclosed herein, different types of model architectures are used. In general, machine learning models/architectures that are suitable to use in the example approaches disclosed herein will be convolution neural networks, a residual neural network (ResNet), etc. However, other types of machine learning models could additionally or alternatively be used such as deep neural networks, other types of neural networks, etc.</p><p id="p-0077" num="0076">In general, implementing a ML/AI system involves two phases, a learning/training phase and an inference phase. In the learning/training phase, a training algorithm is used to train a model to operate in accordance with patterns and/or associations based on, for example, training data. In general, the model includes internal parameters that guide how input data is transformed into output data, such as through a series of nodes and connections within the model to transform input data into output data. Additionally, hyperparameters are used as part of the training process to control how the learning is performed (e.g., a learning rate, a number of layers to be used in the machine learning model, etc.). Hyperparameters are defined to be training parameters that are determined prior to initiating the training process.</p><p id="p-0078" num="0077">Different types of training may be performed based on the type of ML/AI model and/or the expected output. For example, supervised training uses inputs and corresponding expected (e.g., labeled) outputs to select parameters (e.g., by iterating over combinations of select parameters) for the ML/AI model that reduce model error. As used herein, labelling refers to an expected output of the machine learning model (e.g., a classification, an expected output value, etc.) Alternatively, unsupervised training (e.g., used in deep learning, a subset of machine learning, etc.) involves inferring patterns from inputs to select parameters for the ML/AI model (e.g., without the benefit of expected (e.g., labeled) outputs).</p><p id="p-0079" num="0078">Algorithms disclosed herein are used to train ML/AI models. However, any other training algorithm may additionally or alternatively be used. Training is performed using hyperparameters that control how the learning is performed (e.g., a learning rate, a number of layers to be used in the machine learning model, etc.). In some examples re-training may be performed.</p><p id="p-0080" num="0079">Training is performed using training data. In examples disclosed herein, the training data originates from locally generated data. Because supervised training is used, the training data is labeled. Labeling is applied to the training data by a workforce. In some examples, the training data is sub-divided into training data and testing data.</p><p id="p-0081" num="0080">Once training is complete, the model is deployed for use as an executable construct that processes an input and provides an output based on the network of nodes and connections defined in the model. In some examples, the model is saved with the DDS. The model may then be executed by extraction circuitry and/or decoding circuitry.</p><p id="p-0082" num="0081">Once trained, the deployed model may be operated in an inference phase to process data. In the inference phase, data to be analyzed (e.g., live data) is input to the model, and the model executes to create an output. This inference phase can be thought of as the AI &#x201c;thinking&#x201d; to generate the output based on what it learned from the training (e.g., by executing the model to apply the learned patterns and/or associations to the live data). In some examples, input data undergoes pre-processing before being used as an input to the machine learning model. Moreover, in some examples, the output data may undergo post-processing after it is generated by the AI model to transform the output into a useful result (e.g., a display of data, an instruction to be executed by a machine, etc.).</p><p id="p-0083" num="0082">In some examples, output of the deployed model may be captured and provided as feedback. By analyzing the feedback, an accuracy of the deployed model can be determined. If the feedback indicates that the accuracy of the deployed model is less than a threshold or other criterion, training of an updated model can be triggered using the feedback and an updated training data set, hyperparameters, etc., to generate an updated, deployed model.</p><p id="p-0084" num="0083"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram of an example environment <b>100</b> constructed in accordance with the teachings of this disclosure for collecting purchase data from a consumer. In some examples, the purchase data includes purchased product descriptions, quantities, and prices associated with specific purchased barcodes. For example, purchase data refers to processed (e.g., decoded) basket information provided by the panelist. The example environment <b>100</b> includes an example monitoring entity <b>102</b>, which can be, for example, a marketing research entity that enlists consumer panelists from which to collect purchase data. In the illustrated example of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the example monitoring entity <b>102</b> is in communication with at least one example electronic device <b>104</b> that can be associated with a cooperating consumer(s) (e.g., panelist). The electronic device <b>104</b> can be, for example, a personal computing (PC) device such as a laptop, a smartphone, an electronic tablet, a hybrid or convertible PC, etc. used by a panelist to provide basket information to the entity.</p><p id="p-0085" num="0084">In the illustrated example of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the entity <b>102</b> and electronic device(s) <b>104</b> are communicatively coupled via an example network <b>106</b>. The example network <b>106</b> may be implemented using any network over which data can be transferred, such as the Internet. The example network <b>106</b> may be implemented using any suitable wired and/or wireless network(s) including, for example, one or more data buses, one or more Local Area Networks (LANs), one or more wireless LANs, one or more cellular networks, one or more private networks, one or more public networks, among others. In additional or alternative examples, the network <b>106</b> is an enterprise network (e.g., within businesses, corporations, etc.), a home network, among others.</p><p id="p-0086" num="0085">The panelist can use the example electronic device <b>104</b> to obtain an image(s) <b>108</b> of a receipt that memorializes a transaction between the panelist and a retailer. In some examples, the receipt image <b>108</b> is a digital receipt the panelist obtained, for example, via email, a website, an application, etc. In some examples, the receipt image <b>108</b> is an image of a receipt captured by the panelist (e.g., via the electronic device <b>104</b>). For example, the panelist can use an application installed on the electronic device <b>104</b> that accesses an image sensor (e.g., a camera) to capture the receipt image <b>108</b>. In examples disclosed herein, the panelist also obtains barcodes <b>110</b> corresponding to purchased products as memorialized in the receipt image <b>108</b>. In some examples, the panelist captures the barcode(s) <b>110</b> using the camera of the electronic device <b>104</b>. In some examples, the panelist uses a barcode scanner that is communicatively coupled to the electronic device <b>104</b>. In some examples, the panelist selects a barcode from a list of barcodes via the application on the electronic device <b>104</b>.</p><p id="p-0087" num="0086">In some examples, the panelist provides additional basket information, such as a retailer from which the basket was purchased. In some examples, the panelist provides the retailer's name, a store code identifying the specific store of the retailer, and a country in which the store is located. In some examples, the application includes three lists of stores from which the panelist can select the retailer, including generic stores, banner stores, and favorite stores. In additional or alternative examples, the retailer is extracted from the receipt image <b>108</b>.</p><p id="p-0088" num="0087">The panelist can use the electronic device <b>104</b> to transmit the basket information (e.g., the receipt images <b>108</b>, barcodes <b>110</b>, retailer, etc.) to the entity <b>102</b> via the network <b>106</b>. Put another way, the monitoring entity <b>102</b> receives or obtains the basket information captured and/or uploaded by panelist via the electronic device <b>104</b>. In the illustrated example of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the monitoring entity <b>102</b> includes an example basket datastore <b>112</b>, which is structured to store the basket information. In some examples, the basket datastore <b>112</b> stores the basket information until the receipts images <b>108</b> and barcodes <b>110</b> can be processed by example document decode circuitry <b>114</b>. The example basket datastore <b>112</b> of the illustrated example of <figref idref="DRAWINGS">FIG. <b>1</b></figref> is implemented by any memor(ies), storage device(s) and/or storage disc(s) for storing data such as, for example, flash memory, magnetic media, optical media, etc. Furthermore, the data stored in the basket datastore <b>112</b> may be in any data format such as, for example, binary data, comma delimited data, tab delimited data, structured query language (SQL) structures, image data, etc.</p><p id="p-0089" num="0088">The example document decode circuitry <b>114</b> is structured to process (e.g., apply a decoding process to) receipt images <b>108</b> and barcodes <b>110</b> obtained by the monitoring entity <b>102</b> from any number of panelists using any number of electronic devices <b>104</b> to collect purchase data. In the illustrated example of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the example document decode circuitry <b>114</b> implements at least a portion of a document decode service (DDS). In some examples, the monitoring entity <b>102</b> serves as a host machine for the document decode circuitry <b>114</b> to process of the receipt image <b>108</b> and/or the barcodes <b>110</b>. In additional or alternative examples, the monitoring entity <b>102</b> enlists a cloud service provider to execute the document decode circuitry <b>114</b> as a cloud service (e.g., infrastructure as a service, system as a service , etc.). In some examples, the document decode circuitry <b>114</b> is implemented by an Edge network.</p><p id="p-0090" num="0089">The example document decode circuitry <b>114</b> processes the receipt image <b>108</b> to extract purchase information from the receipt, such as purchased products (e.g., items), quantities, prices, promotions, etc. The document decode circuitry <b>114</b> can then decode the receipt by matching the purchased products with respective barcodes <b>110</b>. In this manner, the example document decode circuitry <b>114</b> can report quantities and prices associated with specific purchased barcodes. To process the receipt image <b>108</b> and barcodes <b>110</b>, the document decode circuitry <b>114</b> includes example OCR circuitry <b>116</b>, example extraction circuitry <b>118</b>, an example database <b>120</b>, example decoding circuitry <b>122</b>, and example report generating circuitry <b>134</b>.</p><p id="p-0091" num="0090">The example OCR circuitry <b>116</b> is structured to perform optical character recognition (OCR) on the receipt image <b>108</b> to convert text in the receipt image <b>108</b> into machine-readable text. For example, the example OCR circuitry <b>116</b> can apply an OCR-based algorithm over the receipt image <b>108</b> to obtain text data. After applying an OCR-based algorithm over receipt image <b>108</b>, the OCR circuitry <b>116</b> can return the characters and words (e.g., text) obtained from the receipt image <b>108</b> as well as their locations. For example, the OCR circuitry <b>116</b> can output bounding boxes (e.g., text boxes) corresponding to strings of characters (e.g., transcribed text) and locations (e.g., coordinates) of the bounding boxes within the receipt image <b>108</b>. As disclosed herein, a word detected by the OCR circuitry <b>116</b> can include a word, an abbreviation, a partial word, a number, a symbol, etc. For example, the word can include a price of a purchased product. In some examples, the OCR circuitry <b>116</b> is a component of the extraction circuitry <b>118</b>.</p><p id="p-0092" num="0091">The example extraction circuitry <b>118</b> is structured to extract the purchase information from the receipt image <b>108</b>. The extraction circuitry <b>118</b> receives and/or retrieves the receipt image <b>108</b> and identifies regions of interest in the receipt. In some examples, there are two main regions of interest in receipt image <b>108</b>, including a receipt region (e.g., a rectangular area in which the receipt is present) and a product region (e.g., a rectangular area around text that contains purchase details). The receipt region and the products regions are both substantially rectangular regions. In some examples, the regions of interest can thus be detected using object detection techniques. In some examples, the extraction circuitry <b>118</b> applies an object detection AI model based on computer vision and deep learning to the receipt image <b>108</b> to identify the two main regions of interest. For example, the regions of interest can be detected by applying a faster R-CNN based AI model to the receipt image <b>108</b>.</p><p id="p-0093" num="0092">In some examples, the detected regions of interest are used to detect rows and columns from the receipt image <b>108</b>. For example, the example extraction circuitry <b>118</b> can use the receipt region to detect rows within the receipt image <b>108</b>. The rows refer to individual text lines that contain purchase information such as purchased products, volume and unit prices, promotions and/or discounts applied, etc. Further, the extraction circuitry <b>118</b> can use the products region to detect columns within the receipt image <b>108</b>. That is, the extraction circuitry <b>118</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> detects columns of tables instead of a whole table. While receipt formats can vary depending on a corresponding country, retailer, documents type, etc., most receipts contain tabulated information that mimic a column-wise organization of the purchase details. In some examples, segmenting out the products region enables the extraction circuitry <b>118</b> to identify tabulated lines that include the purchase details and/or lines having promotions. For example, detecting columns enables the extraction circuitry <b>118</b> to segregate details present in the rows, which can include product description, price, unit of measure, quantity, product code, tax notations, promotions, etc. In some examples, the extraction circuitry <b>118</b> applies AI models based on computer vision and deep learning techniques to detect the rows and/or the columns.</p><p id="p-0094" num="0093">In some examples, the extraction circuitry <b>118</b> generates a digitized receipt from the receipt image <b>108</b> by mapping words generated by the OCR circuitry <b>116</b> to detected rows and/or columns. For example, the detected rows and columns can be used to form the structure of a receipt that corresponds to the receipt image <b>108</b>. Text detected by the OCR engine can be mapped to respective positions within the receipt based on coordinates of the words, the rows, and/or the columns. In some examples, the mapping is based on IoU calculations. In some examples, the digitized receipt can be used to extract purchase information, such as purchase details (e.g., product descriptions, quantities, prices), promotions, etc.</p><p id="p-0095" num="0094">The document decode circuitry <b>114</b> includes an example database <b>120</b>, which is structured to store data related to the DDS. For example, the database <b>120</b> can be used to store AI models for detecting regions of interest, rows, and/or columns. In some examples, digitized receipts are stored in an example database <b>120</b>. The example database <b>120</b> can be implemented by any memor(ies), storage device(s) and/or storage disc(s) for storing data such as, for example, flash memory, magnetic media, optical media, etc. Furthermore, the data stored in the database <b>120</b> may be in any data format such as, for example, binary data, comma delimited data, tab delimited data, SQL structures, image data, etc.</p><p id="p-0096" num="0095">The example decoding circuitry <b>122</b> is structured to decode a basket by matching (e.g., associating) extracted purchase details with specific barcodes <b>110</b>. To facilitate the matching, the document decode circuitry <b>114</b> is communicatively coupled to at least one data source. In the illustrated example of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the document decode circuitry <b>114</b> is communicatively coupled to an example internal dictionary <b>124</b>, an example products datastore <b>126</b>, an example retailer database <b>128</b>, and an example previous jobs database <b>130</b>. In some examples, the monitoring entity <b>102</b> is communicatively coupled to an example retailer dictionary(ies) <b>132</b>. For example, the retailer dictionary <b>132</b> can be provided to the monitoring entity <b>102</b> by a retailer and can include files that store lists of products sold by the retailer and associated barcodes. In some examples, the monitoring entity <b>102</b> can add information from the retailer dictionary <b>132</b> to the internal dictionary <b>124</b> to help facilitate the matching process.</p><p id="p-0097" num="0096">The internal dictionary <b>124</b> is a database that includes associations between product descriptions and barcodes. In some examples, the internal dictionary <b>124</b> records associations between item descriptions and barcodes at a retailer level. For example, the internal dictionary <b>124</b> can include a plurality of entries in the form of documents and each document can include a retailer, an item description(s), and a corresponding barcode(s) corresponding to a specific product. In some examples, more than one barcode <b>110</b> can be associated with the item description. For example, the item description can be associated with different types of barcodes, such as a UPC, an EAN, and/or other types of barcodes <b>110</b>. Further, a barcode <b>110</b> can be associated with more than one product description. For example, different retailers may include different product descriptions for the same product. Accordingly, the internal dictionary <b>124</b> can include multiple item descriptions and multiple barcodes <b>110</b> for a single product. In some examples, the internal dictionary grows (e.g., expands) over time by recording associations generated by the decoding circuitry <b>122</b>. In some examples, the internal dictionary <b>124</b> is used as a first step in the decoding process. That is, the decoding circuitry <b>122</b> searches the internal dictionary <b>124</b> first to detect a match between a purchased product and a barcode <b>110</b>.</p><p id="p-0098" num="0097">In some examples, the internal dictionary <b>124</b> is implemented as a standalone full-text search server or platform. For example, the internal dictionary <b>124</b> can serve as both a search engine and a distributed document database with SQL support. The internal dictionary <b>124</b> can store various information. For example, entries in the internal dictionary <b>124</b> can include different fields such as a &#x201c;brand,&#x201d; &#x201c;price,&#x201d; &#x201c;counter,&#x201d; &#x201c;created on,&#x201d; &#x201c;EAN,&#x201d; &#x201c;entity description,&#x201d; &#x201c;entity description_n-gram,&#x201d; &#x201c;receipt description,&#x201d; &#x201c;receipt description_n-gram,&#x201d; &#x201c;source,&#x201d; &#x201c;store ID,&#x201d; &#x201c;storename,&#x201d; and/or &#x201c;upid.&#x201d; The brand can refer to a brand of an item and can be obtained from the product datastore <b>126</b>. The price can refer to a price obtained from the products datastore <b>126</b> and associated with a respective barcode (e.g., EAN). The counter can refer to a number of times that a specific product description and storelD have been matched in a receipt. The created_on can refer to a creation date of an entry in the internal dictionary <b>124</b>. The EAN can refer to an EAN (e.g., barcode) corresponding to a product. The entity description can refer to an item description obtained from the products datastore <b>126</b> for a respective barcode. The entity description_n-gram can refer to an n-gram tokenization of a respective entity description. The receipt description can refer to an item description extracted from a receipt image <b>108</b>. The receipt description_n-gram can refer to an n-gram tokenization of a respective receipt description. The source can refer to a source of information, such as the products datastore <b>126</b>, the previous jobs database <b>130</b>, a retailer, etc. The storelD can refer to a unique identifier of a respective store. The upid can refer to a unique identifier for a processed receipt image <b>108</b>. Below is an example record that can be saved in the internal dictionary <b>124</b>:</p><p id="p-0099" num="0000"><tables id="TABLE-US-00001" num="00001"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="offset" colwidth="7pt" align="left"/><colspec colname="1" colwidth="210pt" align="left"/><thead><row><entry/><entry namest="offset" nameend="1" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/><entry>&#x201c;brand&#x201d;:&#x201c;KAHVIKEKSIT&#x201d;,</entry></row><row><entry/><entry>&#x201c;price&#x201d;:&#x201c;0&#x201d;,</entry></row><row><entry/><entry>&#x201c;counter&#x201d;:0,</entry></row><row><entry/><entry>&#x201c;created on&#x201d;:&#x201c;2020-06-07T19:56:52Z&#x201d;, &#x201c;ean&#x201d;:&#x201c;5010204791500&#x201d;,</entry></row><row><entry/><entry>&#x201c;entitydescription&#x201d;:&#x201c;QUADRUPLE CHOCO COOKIES&#x201d;,</entry></row><row><entry/><entry>&#x201c;entitydescription_n-gram&#x201d;:&#x201c;QUADRUPLE CHOCO COOKIES&#x201d;,</entry></row><row><entry/><entry>&#x201c;receiptdescription&#x201d;:&#x201c;QUADRUPLE CHOCO COOKIES&#x201d;,</entry></row><row><entry/><entry>&#x201c;receiptdescription_n-gram&#x201d;/&#x201c;QUADRUPLE CHOCO COOKIES&#x201d;,</entry></row><row><entry/><entry>&#x201c;source&#x201d;:&#x201c;RETAILER-PROVIDED&#x201d;,</entry></row><row><entry/><entry>&#x201c;storeid&#x201d;:&#x201c;0000001022&#x201d;,</entry></row><row><entry/><entry>&#x201c;storename&#x201d;:&#x201c;PRISMA&#x201d;,</entry></row><row><entry/><entry>&#x201c;upid&#x201d;:&#x201c;433053c7-a3cb-4ca2-9e12-3ec68e16dbaa&#x201d;,</entry></row><row><entry/><entry>&#x201c;_version_&#x201d;:1668871422566465540},</entry></row><row><entry/><entry namest="offset" nameend="1" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0100" num="0098">The example products datastore <b>126</b> is structured to store product information for a plurality of products. The product information can include, for example, product descriptions, product images, barcodes, etc. and/or other attributes of the products. In some examples, the products datastore <b>126</b> is a copy of a larger products database that is periodically and/or aperiodically obtained by the decoding circuitry <b>122</b>. For example, the decoding circuitry <b>122</b> can retrieve the copy of the products datastore <b>126</b> that includes a plurality of the products for a specific market or country and selected attributes of the plurality of products that may be less than all available attributes. In some examples, the copy is retrieved on a weekly basis. In some examples, the copies of the products datastore <b>126</b> are obtained because the large products database is too large and/or does not include an interface for searching. In additional or alternative examples, the products datastore <b>126</b> includes an interface that allows the document decode circuitry <b>114</b> to request information.</p><p id="p-0101" num="0099">The products datastore <b>126</b> can store various information including different fields such as a &#x201c;EAN,&#x201d; &#x201c;store code,&#x201d; &#x201c;product ID,&#x201d; &#x201c;description,&#x201d; &#x201c;price,&#x201d; &#x201c;type,&#x201d; &#x201c;BSDSCRID,&#x201d; &#x201c;data source,&#x201d; and/or other information. The brand can refer to a brand of an item and can be obtained from the product datastore <b>126</b>. The EAN can refer to an EAN (e.g., barcode) corresponding to a product. The store code can refer to a group code obtain from the products datastore <b>126</b> that matches a mapping in the retailer database <b>128</b>. The product ID can refer to a Not a Number (NAN) key in the products datastore <b>126</b>. The description can refer to a description as listed in the products datastore <b>126</b>. The price can refer to a unitary price of the product as found in products datastore <b>126</b> for a specific barcodes (e.g., EAN). The type can refers to a barcode type, such a EAN, UPC, local assigned code (LAC), etc. The BSDSCRID can refer to a best description ID attribute from the products datastore <b>126</b>, which can be the same as the product ID. In some examples, the products datastore <b>126</b> stores other fields such as DEPCODE (e.g., department code), DEPDESCR (e.g., department description), SUPPLIER CODE (e.g., supplier code for the product), BRAND (e.g., brand of the product), OWNER (e.g., brand owner of the product), CATEGORY (e.g., product category), PNU (e.g., a number of units in a package of the product), PACKAGING (e.g., product packaging description), PACKAGING MATERIAL (e.g., product packaging material), SUPER GROUP (e.g., a code and/or identifier to group products), IS BEST DESCRIPTION (e.g., a yes or no indication), and/or other information. Below is an example entry that can be saved in the products datastore <b>126</b>:</p><p id="p-0102" num="0000"><tables id="TABLE-US-00002" num="00002"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="offset" colwidth="14pt" align="left"/><colspec colname="1" colwidth="203pt" align="left"/><thead><row><entry/><entry namest="offset" nameend="1" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/><entry>{</entry></row><row><entry/><entry>&#x2018;ean&#x2019;: &#x2018;6412612460112&#x2019;,</entry></row><row><entry/><entry>&#x2018;storecode&#x2019;: &#x2018;FI001&#x2019;, &#x2018;bsdscrid&#x2019;: &#x2018;1600752112.0&#x2019;,</entry></row><row><entry/><entry>&#x2018;productid&#x2019;: &#x2018;4422746&#x2019;, &#x2018;type&#x2019;: &#x2018;EAN&#x2019;,</entry></row><row><entry/><entry>&#x2018;description&#x2019;: &#x2018;NALLE 100G 75%VILLA/25%POLYAMIDI&#x2019;,</entry></row><row><entry/><entry>&#x2018;depcode&#x2019;: &#x201c;,</entry></row><row><entry/><entry>&#x2018;depdscr&#x2019;: &#x201d;,</entry></row><row><entry/><entry>&#x2018;suplrcode&#x2019;: &#x201c;, &#x2018;price&#x2019;: 0,</entry></row><row><entry/><entry>&#x2018;isBestDescription&#x2019;: &#x2018;Y&#x2019;, &#x2018;datasource&#x2019;:, &#x2018;brand&#x2019;: &#x201d;,</entry></row><row><entry/><entry>&#x2018;brandowner&#x2019;: &#x201c;,</entry></row><row><entry/><entry>&#x2018;category&#x2019;: &#x201d;,</entry></row><row><entry/><entry>&#x2018;pnu&#x2019;: &#x201c;,</entry></row><row><entry/><entry>&#x2018;packaging&#x2019;: &#x201d;, &#x2018;packagingmaterial&#x2019;: &#x201c;,</entry></row><row><entry/><entry>&#x2018;module&#x2019;: &#x2018;SEWNG/HBRDSHRY&#x2019;,</entry></row><row><entry/><entry>&#x2018;supergroup&#x2019;: &#x201d;</entry></row><row><entry/><entry>}</entry></row><row><entry/><entry namest="offset" nameend="1" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0103" num="0100">The example retailer database <b>128</b> is structured to store information regarding various retailers or stores for various countries. For example, the retailer database <b>128</b> can include information such as a store code (e.g., store_code) , a store name (e.g., store_name), a global store code (e.g., global_store), a favorite flag (e.g., is favorite_store), and/or a products datastore mapping identifier (e.g., products_mappings). In some examples, the decoding circuitry <b>122</b> obtains a copy of the retailer database <b>128</b>, which can be stored in the example database <b>120</b>. In some examples, the decoding circuitry <b>122</b> includes an interface to the retailer database <b>128</b>. In some examples, the retailer database <b>128</b> can be used during decoding to translate a favorite store into a &#x201c;global store code&#x201d;, which can be useful during a search of the products datastore <b>126</b>. For example, to limit a search in the products datastore <b>126</b> to products belonging to a specific store, the decoding circuitry <b>122</b> can use the group code and/or a global store code in a search query. Below are example entries that can be saved in the retailer database <b>128</b> and/or stored in the database <b>120</b>:</p><p id="p-0104" num="0000"><tables id="TABLE-US-00003" num="00003"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="offset" colwidth="21pt" align="left"/><colspec colname="1" colwidth="35pt" align="left"/><colspec colname="2" colwidth="161pt" align="left"/><thead><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/><entry>1.</entry><entry>store_code: 0000001999</entry></row><row><entry/><entry/><entry>store_name: MUU SUOMEN L&#xc4;HIKAUPPA</entry></row><row><entry/><entry/><entry>global_store: 0000001015</entry></row><row><entry/><entry/><entry>is_favorite_store: true</entry></row><row><entry/><entry/><entry>products_mappings: [&#x201c;FI016&#x201d;]</entry></row><row><entry/><entry>2.</entry><entry>store_code: 0000001044</entry></row><row><entry/><entry/><entry>store_name: STOCKMANN</entry></row><row><entry/><entry/><entry>global_store: NULL</entry></row><row><entry/><entry/><entry>is_favorite_store: false</entry></row><row><entry/><entry/><entry>products_mappings: [&#x201c;FI016&#x201d;]</entry></row><row><entry/><entry>3.</entry><entry>store_code: 0000001045</entry></row><row><entry/><entry/><entry>store_name: TARMO</entry></row><row><entry/><entry/><entry>global_store: NULL</entry></row><row><entry/><entry/><entry>is_favorite_store: false</entry></row><row><entry/><entry/><entry>products_mappings: [&#x201c;FI016&#x201d;]</entry></row><row><entry/><entry>4</entry><entry>store_code: 0000001046</entry></row><row><entry/><entry/><entry>store_name: ELINTARVIKE ERIKOISML&#xc4;</entry></row><row><entry/><entry/><entry>global_store: NULL</entry></row><row><entry/><entry/><entry>is_favorite_store: false</entry></row><row><entry/><entry/><entry>products_mappings: [&#x201c;FI016&#x201d;]</entry></row><row><entry/><entry>5.</entry><entry>store_code: 0000001051</entry></row><row><entry/><entry/><entry>store_name: ANTTILA/KODINYKK</entry></row><row><entry/><entry/><entry>global_store: NULL</entry></row><row><entry/><entry/><entry>is_favorite_store: false</entry></row><row><entry/><entry/><entry>products_mappings: [&#x201c;FI016&#x201d;]</entry></row><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0105" num="0101">In some examples, the store code and store name may match the store details provided by the panelist. As illustrated above, the first example corresponds to a favorite store and, thus, includes a global store identifier. The second example on the other hand does not corresponding to a favorite store and, thus, does not include a global store identifier.</p><p id="p-0106" num="0102">The example previous jobs database <b>130</b> stores product descriptions and barcodes <b>110</b> that were unable to be matched. For example, the decoding circuitry <b>122</b> can search the available data sources to identify a match between the product description and barcode <b>110</b>, but fail to identify such a match. The unmatched item description(s) and barcode(s) <b>110</b> can be stored in the previous jobs database <b>130</b>. In some examples, another product description and barcode(s) <b>110</b> can be searched against the previous jobs database <b>130</b> to determine whether a similar product description and barcode <b>110</b> combination has been previously observed. In other words, the previous jobs database <b>130</b> can be used to track previous processed receipts and increase chances of recognizing associations using this historical information.</p><p id="p-0107" num="0103">The previous jobs database <b>130</b> can store various information including fields such as &#x201c;job_id,&#x201d; &#x201c;description,&#x201d; &#x201c;EAN,&#x201d; &#x201c;store_ID,&#x201d; and/or &#x201c;created_on.&#x201d; In some examples, the description and EAN fields can include an array and/or list of item descriptions as extracted from the extraction circuitry <b>118</b> and barcodes <b>110</b> provided by the panelist, respectively. In some examples, the previous jobs database <b>130</b> can include a record for each processed receipt image <b>108</b> that includes un-matched items after searching the available data sources.</p><p id="p-0108" num="0104">The example decoding circuitry <b>122</b> is structured to receive and/or retrieve purchase information extracted by the extraction circuitry <b>118</b> and respective barcodes <b>110</b> corresponding to a receipt image <b>108</b>. The decoding circuitry <b>122</b> generates at least one search query that is used to search a data source, such as the internal dictionary <b>124</b>, the products datastore <b>126</b>, and/or the previous jobs database <b>130</b>. In some examples, the decoding circuitry <b>122</b> removes duplicate barcodes <b>110</b> provided by the panelist to search with a list of unique barcodes. In some examples, the decoding circuitry <b>122</b> matches one extracted purchased product at a time. For example, the decoding circuitry <b>122</b> can generate a query that includes a product description for a first product, a store ID, and the unique list of the barcodes <b>110</b>. In some examples, a barcode <b>110</b> that is matched to a product description can be removed from the list of unique barcodes <b>110</b> before searching another product.</p><p id="p-0109" num="0105">In some examples, the decoding circuitry <b>122</b> may need to search more than one data source to identify a match between a product description and a barcode <b>110</b>. For example, if a search of a first data source does not yield a match, the decoding circuitry <b>122</b> may need to search one or more additional data sources. In some examples, the decoding circuitry <b>122</b> searches the internal dictionary <b>124</b> first. If no match is identified for a product, the decoding circuitry <b>122</b> can search the products datastore <b>126</b>. In some examples, the decoding circuitry <b>122</b> can search the previous jobs database <b>130</b> if the products datastore <b>126</b> search(es) does not yield a match. If no match is identifies after the previous jobs database <b>130</b> search, the un-matched product(s) and barcode(s) <b>110</b> can be stored in the previous jobs database <b>130</b>.</p><p id="p-0110" num="0106">In some examples, the decoding circuitry <b>122</b> searches one data source at a time for a specific basket. For example, the decoding circuitry <b>122</b> may search each detected product extracted by the extraction circuitry <b>118</b> against the internal dictionary <b>124</b> (e.g., one product at a time). Products that are matched to barcodes <b>110</b> can be saved to a response. Products that were not matched using the internal dictionary <b>124</b> can be searched against the products datastore <b>126</b> (e.g., one product at a time). Products that are matched to barcodes <b>110</b> using the products datastore <b>126</b> can be saved to a response. Products that were not matched using the products datastore <b>126</b> can then be searched against the previous jobs database <b>130</b> (e.g., one product at a time). Unmatched products after the previous jobs database <b>130</b> can be saved to the previous jobs database <b>130</b> to potentially be matched at a later point in time. In other examples, a first product can be searched against the data source(s) until a match is determined or until all data sources have been searched before searching a second product.</p><p id="p-0111" num="0107">An identified match between a purchased product identified by the extraction circuitry <b>118</b> and a respective barcode <b>110</b> provided by the panelist can be stored in the internal dictionary <b>124</b>. Accordingly, the internal dictionary <b>124</b> is built (e.g., expanded, added to, grown, etc.) over time. Product descriptions and barcodes <b>110</b> that are not matched after the decoding process can be saved in the previous jobs database <b>130</b>. In some examples, the decoding circuitry <b>122</b> generates a response that includes product descriptions, quantities, prices, etc. and associated barcodes <b>110</b>. In some examples, the response is transmitted to example report generating circuitry <b>134</b>.</p><p id="p-0112" num="0108">The example report generating circuitry <b>134</b> is structured to compile results of receipt extraction to generate a report of the results. In some examples, the report includes decoded purchase data corresponding to the receipt image <b>108</b> and respective barcodes <b>110</b>. In some examples, the report is transmitted to the monitoring entity <b>102</b> to be further analyzed.</p><p id="p-0113" num="0109">In the illustrated example of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the document decode circuitry <b>114</b> includes example machine learning circuitry <b>136</b>. The machine learning circuitry <b>136</b> is structured to train AI models for use by the document decode circuitry <b>114</b> and/or other portions of a DDS. For example, the machine learning circuitry <b>136</b> can be used to train AI models to detect regions of interest, rows, and/or columns. In some examples, the machine learning circuitry <b>136</b> can train an AI model to save and/or store results of the decoding circuitry <b>122</b>. In additional or alternative example, the machine learning circuitry <b>136</b> is separate from the document decode circuitry <b>114</b> in a location that is accessible by the document decode circuitry <b>114</b>. For example, the machine learning circuitry <b>136</b> may located with the monitoring entity <b>102</b>, where AI models can be trained. In some such examples, the trained models can be transmitted to the document decode circuitry <b>114</b>, which may be deployed as cloud service.</p><p id="p-0114" num="0110">In some examples, the document decode circuitry <b>114</b> includes means for generating text data from an image. For example, the means for generating text data from the image may be implemented by OCR circuitry <b>116</b>. In some examples, the OCR circuitry <b>116</b> may be instantiated by processor circuitry such as the example processor circuitry <b>3512</b> of <figref idref="DRAWINGS">FIG. <b>35</b></figref>. For instance, the OCR circuitry <b>116</b> may be instantiated by the example general purpose processor circuitry <b>3600</b> of <figref idref="DRAWINGS">FIG. <b>36</b></figref> executing machine executable instructions such as that implemented by at least blocks <b>2704</b> of <figref idref="DRAWINGS">FIG. <b>27</b></figref>. In some examples, the OCR circuitry <b>116</b> may be instantiated by hardware logic circuitry, which may be implemented by an ASIC or the FPGA circuitry <b>3700</b> of <figref idref="DRAWINGS">FIG. <b>37</b></figref> structured to perform operations corresponding to the machine readable instructions. Additionally or alternatively, the OCR circuitry <b>116</b> may be instantiated by any other combination of hardware, software, and/or firmware. For example, the OCR circuitry <b>116</b> may be implemented by at least one or more hardware circuits (e.g., processor circuitry, discrete and/or integrated analog and/or digital circuitry, an FPGA, an Application Specific Integrated Circuit (ASIC), a comparator, an operational-amplifier (op-amp), a logic circuit, etc.) structured to execute some or all of the machine readable instructions and/or to perform some or all of the operations corresponding to the machine readable instructions without executing software or firmware, but other structures are likewise appropriate.</p><p id="p-0115" num="0111">In some examples, the document decode circuitry <b>114</b> includes means for extracting purchase information. For example, the means for extracting purchase information may be implemented by extraction circuitry <b>118</b>. In some examples, the extraction circuitry <b>118</b> may be instantiated by processor circuitry such as the example processor circuitry <b>3512</b> of <figref idref="DRAWINGS">FIG. <b>35</b></figref>. For instance, the extraction circuitry <b>118</b> may be instantiated by the example general purpose processor circuitry <b>3600</b> of <figref idref="DRAWINGS">FIG. <b>36</b></figref> executing machine executable instructions such as that implemented by at least blocks <b>2706</b>-<b>2712</b> of <figref idref="DRAWINGS">FIGS. <b>27</b>-<b>32</b></figref>. In some examples, the extraction circuitry <b>118</b> may be instantiated by hardware logic circuitry, which may be implemented by an ASIC or the FPGA circuitry <b>3700</b> of <figref idref="DRAWINGS">FIG. <b>37</b></figref> structured to perform operations corresponding to the machine readable instructions. Additionally or alternatively, the extraction circuitry <b>118</b> may be instantiated by any other combination of hardware, software, and/or firmware. For example, the extraction circuitry <b>118</b> may be implemented by at least one or more hardware circuits (e.g., processor circuitry, discrete and/or integrated analog and/or digital circuitry, an FPGA, an Application Specific Integrated Circuit (ASIC), a comparator, an operational-amplifier (op-amp), a logic circuit, etc.) structured to execute some or all of the machine readable instructions and/or to perform some or all of the operations corresponding to the machine readable instructions without executing software or firmware, but other structures are likewise appropriate.</p><p id="p-0116" num="0112">In some examples, the document decode circuitry <b>114</b> includes means for decoding a basket. For example, the means for decoding the basket may be implemented by decoding circuitry <b>122</b>. In some examples, the decoding circuitry <b>122</b> may be instantiated by processor circuitry such as the example processor circuitry <b>3512</b> of <figref idref="DRAWINGS">FIG. <b>35</b></figref>. For instance, the decoding circuitry <b>122</b> may be instantiated by the example general purpose processor circuitry <b>3600</b> of <figref idref="DRAWINGS">FIG. <b>36</b></figref> executing machine executable instructions such as that implemented by at least blocks <b>2714</b> of <figref idref="DRAWINGS">FIGS. <b>27</b>, <b>33</b>, and <b>34</b></figref>. In some examples, the decoding circuitry <b>122</b> may be instantiated by hardware logic circuitry, which may be implemented by an ASIC or the FPGA circuitry <b>3700</b> of <figref idref="DRAWINGS">FIG. <b>37</b></figref> structured to perform operations corresponding to the machine readable instructions. Additionally or alternatively, the decoding circuitry <b>122</b> may be instantiated by any other combination of hardware, software, and/or firmware. For example, the decoding circuitry <b>122</b> may be implemented by at least one or more hardware circuits (e.g., processor circuitry, discrete and/or integrated analog and/or digital circuitry, an FPGA, an Application Specific Integrated Circuit (ASIC), a comparator, an operational-amplifier (op-amp), a logic circuit, etc.) structured to execute some or all of the machine readable instructions and/or to perform some or all of the operations corresponding to the machine readable instructions without executing software or firmware, but other structures are likewise appropriate.</p><p id="p-0117" num="0113"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram of an example system <b>200</b> for generating purchase data from a receipt image <b>108</b> and barcodes <b>110</b> constructed in accordance with the teachings of this disclosure. In some examples, the system <b>200</b> implements a decoding process for receipts, which can be part of a larger DDS. In some examples, the system <b>200</b> implements the document decode circuitry <b>114</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. In some examples, the system <b>200</b> provides a simpler technique for consumer panelists to record individual and/or household purchases from any number of retailers by allowing the panelists to upload an image of a receipt rather than manually input purchase data contained in the receipt. Further, the system <b>200</b> reduces the panelists' burden without losing a granularity and/or accuracy of afforded by other collection methods.</p><p id="p-0118" num="0114">As noted above, a panelist provides a receipt image <b>108</b> (e.g., corresponding to a receipt) that includes purchase information and barcodes <b>110</b> corresponding to purchased products. In examples disclosed herein, receipt processing is divided into an extraction phase, which can be implemented by the example extraction circuitry <b>118</b>, and a decoding phase, which can be implemented by the example decoding circuitry <b>122</b>.</p><p id="p-0119" num="0115">In some examples, the receipt image <b>108</b> is provided to the example extraction circuitry <b>118</b> to extract purchase information from the receipt image. That is, the extraction circuitry <b>118</b> obtains information from the receipt image <b>108</b>. The extraction circuitry <b>118</b> of <figref idref="DRAWINGS">FIGS. <b>1</b> and/or <b>2</b></figref> does not rely on the barcodes <b>110</b> to process the receipt image <b>108</b>. Thus, in some examples, the barcodes <b>110</b> are provided directly to the example decoding circuitry <b>122</b> to be matched with purchase details extracted by the extraction circuitry <b>118</b>. In some examples, the receipt images <b>108</b> and barcodes <b>110</b> are provided to the extraction circuitry <b>118</b> and the decoding circuitry <b>122</b>, respectively, from a basket datastore (e.g., basket datastore <b>112</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>).</p><p id="p-0120" num="0116">In some examples, the system <b>200</b> relies on a complex variety of data sources that can include useful information but can also provide variance, noise, and uncertainty that the system <b>200</b> needs to manage to deliver expected outcomes (e.g., outputs). For example, the receipt image <b>108</b> can belong to any of several retailers that employ several printing layouts and formats. The extraction circuitry <b>118</b> is structured to leverage OCR text extraction techniques applied by the OCR circuitry <b>116</b> and a detected structural layout of the receipt image <b>108</b> to automatically extract text from the images. Accordingly, the system <b>200</b> can be generalized and still provide accurate data.</p><p id="p-0121" num="0117">The extraction circuitry <b>118</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref> generates a list of items <b>202</b> corresponding to purchased products listed on the receipt image <b>108</b>. The list of items <b>202</b> can include, for each purchase product, a product description, a price, a quantity, and/or a discount applied. The list of items <b>202</b> is provided to the decoding circuitry <b>122</b> along with a list of barcodes <b>110</b> uploaded by the panelist.</p><p id="p-0122" num="0118">The decoding circuitry <b>122</b> is structured to generate associations between products in the list of items <b>202</b> and corresponding barcodes <b>110</b>. In other words, the decoding circuitry <b>122</b> associates a single barcode to a single extracted product description. To facilitate the matching, the decoding circuitry <b>122</b> is coupled to one or more databases (e.g., products datastore <b>126</b>, retailer database <b>128</b>, previous jobs database <b>130</b>) and one or more dictionaries (e.g., an internal dictionary <b>124</b>, a retail dictionary <b>132</b>). In some examples, the decoding circuitry <b>122</b> matches, one product at a time, the list of items <b>202</b> to respective barcodes <b>110</b>. As a product of the list of items <b>202</b> is matched, the matched barcode <b>110</b> is removed from the list of barcodes <b>110</b>. A next product of the list of items <b>202</b> is searched against at least one database with the list of barcodes <b>110</b> less any matched barcodes <b>110</b>.</p><p id="p-0123" num="0119">In some examples, the system <b>200</b> relies on document analysis and recognition techniques that include image processing solutions. In some examples, the image processing solutions include four main steps including pre-processing, object detect, object recognition, and post-processing. The first step (e.g., pre-processing) typically includes applications of techniques such as image enhancement, distortion corrections and/or noise reduction. The second step (e.g., object detection) typically includes layout analysis techniques, which aim to is to obtain the structure of the document such as textual, layout, and image components. The third step (e.g., object recognition) typically aims to recognize specific information contained on the objects detected in the second step. The fourth step (e.g., post-processing) typically includes establishing relationships between the objects of interest recognized in the document.</p><p id="p-0124" num="0120">In some examples, the system <b>200</b> incorporates the four main modules or steps. In some examples, the first step (e.g., pre-processing) can correspond to detecting regions of interest within a receipt image <b>108</b>. For example, the detecting of the regions of interest reduces noise within the receipt image <b>108</b> and prepare the receipt image <b>108</b> for further processing. In some examples, second step (e.g., object detection) can correspond to detecting rows and columns within the receipt image <b>108</b>. For example, detecting rows and columns within the receipt image <b>108</b> is used to detect the structure of the receipt. In some examples, third step (e.g., object recognition) can correspond to extracting purchase information from the receipt image using the OCR output. For example, by mapping words output by the OCR circuitry <b>116</b> to detected rows and columns, a meaning of the words can be determined based on their place in the receipt. In some examples, the fourth step (e.g., post-processing) can correspond to decoding the basket by associated extracted purchase information with provided barcodes <b>110</b>. That is, the recognized words are post-processed to generate associations between extracted product descriptions and barcodes.</p><p id="p-0125" num="0121"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a block diagram of the example extraction circuitry <b>118</b> of <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>2</b></figref> to extract purchase information from a receipt image <b>108</b>. The extraction circuitry <b>118</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref> may be instantiated (e.g., creating an instance of, bring into being for any length of time, materialize, implement, etc.) by processor circuitry such as a central processing unit executing instructions. Additionally or alternatively, extraction circuitry <b>118</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref> may be instantiated (e.g., creating an instance of, bring into being for any length of time, materialize, implement, etc.) by an ASIC or an FPGA structured to perform operations corresponding to the instructions. It should be understood that some or all of the extraction circuitry <b>118</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref> may, thus, be instantiated at the same or different times. Some or all of the circuitry may be instantiated, for example, in one or more threads executing concurrently on hardware and/or in series on hardware. Moreover, in some examples, some or all of the extraction circuitry <b>118</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref> may be implemented by one or more virtual machines and/or containers executing on the microprocessor.</p><p id="p-0126" num="0122">The extraction circuitry <b>118</b> includes example datastore interface circuitry <b>302</b>, which is structured to provide an interface between the extraction circuitry <b>118</b> and the example basket datastore <b>112</b>. For example, the extraction circuitry <b>118</b> can receive and/or retrieve a receipt image <b>108</b> from the basket datastore <b>112</b> via the example datastore interface circuitry <b>302</b>. In some examples, the datastore interface circuitry <b>302</b> transmits the receipt image <b>108</b> to example regions detection circuitry <b>304</b>.</p><p id="p-0127" num="0123">The example regions detection circuitry <b>304</b> is structured to detect regions of interest from the receipt image <b>108</b>. In some examples, the regions of interest include a receipt region and a products region. In the illustrated example of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the regions detection circuitry <b>304</b> applies an AI-based recognition model to the receipt image <b>108</b> to detect the regions of interest. Accordingly, the regions detection circuitry <b>304</b> includes an example regions detection model(s) <b>306</b>.</p><p id="p-0128" num="0124">In some examples, the regions detection model <b>306</b> is based on computer vision algorithms that focus on object detection and classification techniques. For example, the regions detection model <b>306</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref> is based on a faster R-CNN architecture. Whereas a standard CNN is mainly based on image classification, the faster R-CNN is based on object detection, which combines localization (e.g., detection) and classification. The faster R-CNN architecture is forced to focus on a single region of the receipt image <b>108</b> at a time because it is expected that only a single object of interest will dominate in a given region. In some examples, the regions detection model <b>306</b> detects the regions using a selective search algorithm followed by a resizing operation. For example, the selective search algorithm can be used to detect the regions and to generate bounding boxes corresponding to the detected regions. The regions detection model <b>306</b> can input the bounding boxes into a CNN to be classified. In some examples, the bounding boxes are re-sized such that each bounding box is of substantially equal size before being fed to the CNN for classification and bounding box regression. In other words, the regions detection model <b>306</b> can take the receipt image <b>108</b> as an input and produce a set of bounding boxes as an output. The bounding boxes can contain an object (e.g., the region) and a category (e.g., the receipt region or the product region). In some examples, an output of the regions detection model <b>306</b> is input to example image cropping circuitry <b>308</b>.</p><p id="p-0129" num="0125">The regions detection circuitry <b>304</b> includes the example image cropping circuitry <b>308</b>, which is structured to crop the receipt image <b>108</b> based on the detected regions of interest. For example, the image cropping circuitry <b>308</b> crops the receipt image <b>108</b> to generate a first cropped image corresponding to the receipt region and a second cropped image corresponding to the products region. In some examples, the image cropping circuitry <b>308</b> crops the receipt image <b>108</b> based on the bounding boxes generated by the regions detection model <b>306</b>. In the illustrated example of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the image cropping circuitry <b>308</b> transmits the first cropped image corresponding to the receipt region to example row detection circuitry <b>310</b> and transmits the second cropped image corresponding to the products region to example column detection circuitry <b>312</b>.</p><p id="p-0130" num="0126"><figref idref="DRAWINGS">FIG. <b>4</b>A</figref> illustrates an example receipt region <b>402</b>. The receipt region <b>402</b> includes a receipt portion of an example receipt image <b>108</b> uploaded by a panelist. The receipt region of <figref idref="DRAWINGS">FIG. <b>4</b>A</figref> corresponds to a cropped receipt image <b>108</b> (e.g., by the image cropping circuitry <b>308</b>). In some examples, the receipt region <b>402</b> is an output of the example regions detection circuitry <b>304</b>. For example, the receipt region <b>402</b> can be output by the regions detection circuitry <b>304</b> and provided as an input to example row detection circuitry <b>310</b>.</p><p id="p-0131" num="0127"><figref idref="DRAWINGS">FIG. <b>4</b>B</figref> illustrates an example products region <b>404</b> of the example receipt image <b>108</b> of <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>. The products region <b>404</b> includes a listing of purchased products <b>406</b> and a listing of prices <b>408</b> of the purchased products <b>406</b>. In some examples, the purchased products <b>406</b> portion of the products region <b>404</b> includes product descriptions of the purchase products. In some examples, a quantity of a purchased product can be determined by the products region <b>404</b>. For example, <figref idref="DRAWINGS">FIG. <b>4</b>B</figref> shows two brioche-knotens purchased. Thus, the brioche-knoten has a quantity of two. The products region <b>404</b> of <figref idref="DRAWINGS">FIG. <b>4</b>B</figref> corresponds to another cropped receipt image <b>108</b> (e.g., by the image cropping circuitry <b>308</b>). In some examples, the products region <b>404</b> is another output of the example regions detection circuitry <b>304</b>. For example, the products region <b>404</b> can be output by the regions detection circuitry <b>304</b> and provided as an input to example column detection circuitry <b>312</b>.</p><p id="p-0132" num="0128">Referring again to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the example row detection circuitry <b>310</b> is structured to detect rows of text within a receipt region of a receipt image <b>108</b>. In the illustrated example of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the row detection circuitry <b>310</b> applies a CNN-based model to the receipt region to detect the rows. Accordingly, the row detection circuitry <b>310</b> includes an example row segmentation model(s) <b>314</b>. In some examples, the row segmentation model <b>314</b> is a pixel-wise predictor. For example, when the row detection circuitry <b>310</b> applies the row segmentation model <b>314</b> to the receipt region, each pixel within the region can be identified as belonging to a first class or a second class. The first class can be a line (e.g., row) class and the second class can be a background class, a non-line class, other class, etc.</p><p id="p-0133" num="0129">In some examples, the row segmentation model <b>314</b> outputs a pixel-wise mask. For example, the pixel-wise mask can include a matrix corresponding to pixels in the receipt region that includes binary (e.g., is and 0 s) corresponding to light space (e.g., 1 s) and dark space (e.g., 0 s). The pixel-wise mask can thus indicate which pixels are text (e.g., irrespective of a color of the text) and which pixels are not text (e.g., background pixels). In some examples, the pixel-wise mask includes pixels classified as row pixels grouped into clusters. Each of the clusters can correspond to at least part of a line of text. In the illustrated example of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the pixel-wise mask output by the row segmentation model <b>314</b> may need refinement to yield a desired outcome of bounding boxes corresponding to text lines.</p><p id="p-0134" num="0130">The row detection circuitry <b>310</b> includes example bounding box generating circuitry <b>316</b>, which is structured to generate bounding boxes corresponding to pixel clusters output by the row segmentation model <b>314</b>. To generate the bounding boxes, the bounding box generating circuitry <b>316</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref> identifies contours around the clusters of pixels that were identified as belonging to a text line. As disclosed herein, a contour is a free connection of dots that forms a convex closed form. In some examples, the detected contours can be converted into a minimal geometrical representation that can be projected into a set of intersecting lines. For example, a detected contour can be projected into a polygon. In some examples, the polygons are projected using a minimum rotated rectangle (e.g., &#x201c;min rotated rectangle&#x201d;). However, other projection can be used in additional or alternative examples, such as rectangle without rotation , quadrilateral, etc.</p><p id="p-0135" num="0131"><figref idref="DRAWINGS">FIG. <b>5</b>A</figref> illustrates an example receipt region <b>500</b> that includes an example pixel-wise mask <b>502</b>. For example, the receipt region <b>500</b> having the pixel-wise mask can be output by an example row segmentation model <b>314</b>. The pixel-wise mask <b>502</b> identifies each pixel of the receipt region <b>500</b> as belonging to a text line <b>504</b> or as background <b>506</b>. The pixel-wise mask <b>502</b> includes clusters <b>508</b> corresponding to groups of pixels identified as belonging to text line <b>504</b>.</p><p id="p-0136" num="0132"><figref idref="DRAWINGS">FIG. <b>5</b>B</figref> illustrates example bounding boxes (e.g., polygons, polygonal regions, etc.) <b>510</b> that can be detected and output by example bounding box generating circuitry <b>316</b>. For example, the bounding boxes <b>510</b> can correspond to cluster <b>508</b> of pixels identified as belonging to the text lines <b>504</b>. In some examples, the bounding boxes <b>510</b> correspond to rows of the receipt region <b>500</b>.</p><p id="p-0137" num="0133">Referring again to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the bounding box generating circuitry <b>316</b> outputs a plurality of bounding boxes including respective coordinates corresponding to lines of the receipt region. In some examples, however, two or more bounding boxes that belong to the same text line are not connected. For example, if there is a wide blank space between words, the bounding box generating circuitry <b>316</b> may miss such a connection and generate separate bounding boxes for the line. Accordingly, the row detection circuitry <b>310</b> includes example line merging circuitry <b>318</b>.</p><p id="p-0138" num="0134">The example line merging circuitry <b>318</b> is structured to identify and connect bounding boxes that belong to the same line. In some examples, the line merging circuitry <b>318</b> first identifies bounding boxes that are possible candidates for a merging process. For example, the line merging circuitry <b>318</b> can identify candidates for the merging processing by segregating bounding boxes based on example equation 1, where hL is a horizontal length of a particular bounding box, n is a number of total detected bounding boxes, and a refers to the specific bounding box.</p><p id="p-0139" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>hL</i><sub>a</sub>&#x3c;max(<i>hL</i><sub>1</sub><i>, hL</i><sub>2</sub><i>, hL</i><sub>3</sub><i>, . . . , hL</i><sub>n</sub>)*0.9 &#x2003;&#x2003;Eq. 1<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0140" num="0135">Horizontal lengths of each bounding box can be determined based on coordinates of the bounding boxes. In some examples, a bounding box that has an example largest horizontal length is multiplied by a value of 0.9 (e.g., 90%). However, other values between 0 and 1 can be used additionally or alternatively. In some examples, an output of the formula is a criterion (e.g., a length criterion) for identifying merging candidates and horizontal lengths of each bounding box can be compared to the criterion. Bounding boxes that satisfy the criterion are placed in an example scrutiny list (e.g., candidate list). Bounding boxes that do not satisfy the criterion are determined to not be candidates for the merging process.</p><p id="p-0141" num="0136">In some examples, the line merging circuitry <b>318</b> initializes a graph that includes a plurality of nodes. For example, the graph can be a graphical structure used to model pairwise relations between objects. In some examples, each node of the plurality of nodes can represent a bounding box from scrutiny list. After initializing the graph with nodes, edges (e.g., lines, links, etc.) can be added between two nodes whose respective bounding boxes satisfy two conditions. In some examples, conditions are a column connection criterion. For example, adding edges between nodes indicates a connection between respective bounding boxes. In some examples, the first condition is that the two bounding boxes share a positive vertical coordinate (e.g., a y-axis coordinate). In some examples, the second condition is that the two bounding boxes do not have an overlapping horizontal coordinate (e.g., a-axis coordinate).</p><p id="p-0142" num="0137">In some examples, once edges have been added between nodes that satisfy the conditions, connected components (e.g., bounding boxes) can be identified. In some examples, the line merging circuitry <b>318</b> applies a depth-first search (DFS) approach to identify connected components in the graph. However, the line merging circuitry <b>318</b> can apply other approaches in additional or alternative examples, such as a breadth-first search. The DFS approach is an algorithm for searching graphical data structures that starts at an arbitrary node and explores as far as possible along a branch before backtracking. In the illustrated example of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the line merging circuitry <b>318</b> iterates over the connected components (e.g., one by one) and arranges bounding boxes corresponding nodes in a left to right order. In some examples, the arrangement of the bounding boxes can enable the line merging circuitry <b>318</b> to merge a leftmost and a rightmost bounding boxes and ignore other (e.g., intermediate) bounding boxes. In some examples, the row detection circuitry <b>310</b> outputs a list of polygonal regions (e.g., bounding boxes) that are fitted to printed text lines (e.g., rows) within the receipt region and include coordinates.</p><p id="p-0143" num="0138"><figref idref="DRAWINGS">FIG. <b>6</b>A</figref> illustrates an example output <b>602</b> of example bounding box generating circuitry <b>316</b>. The output <b>602</b> includes a plurality of bounding boxes <b>604</b> identified from a pixel-wise mask. In the illustrated example of <figref idref="DRAWINGS">FIG. <b>6</b>A</figref>, the output <b>602</b> includes missed connections <b>606</b> between bounding boxes <b>604</b> that belong the same row. Accordingly, the output <b>602</b> can be provided to example line merging circuitry <b>318</b> to identify and connect bounding boxes <b>604</b> that belong to the same row.</p><p id="p-0144" num="0139">As noted above, the line merging circuitry <b>318</b> identifies lengths <b>608</b> of each bounding box <b>604</b>. The line merging circuitry <b>318</b> can then identify a bounding box <b>604</b> that has the maximum length (e.g., the largest length value) <b>610</b>. In some examples, the line merging circuitry <b>318</b> multiplies the maximum length <b>610</b> by a value (e.g., 0.9) to generate a merging candidate criterion value. In some examples, the line merging circuitry <b>318</b> compares each bounding box <b>604</b> to the merging candidate criterion value to identify which bounding boxes <b>604</b> are shorter than the merging candidate criteria value. Bounding boxes <b>604</b> that are shorter than the merging candidate criterion value are determined to be line merging candidates. Bounding boxes <b>604</b> that are longer than the merging candidate criterion value are determined to not be merging candidates.</p><p id="p-0145" num="0140"><figref idref="DRAWINGS">FIG. <b>6</b>B</figref> illustrates an example output <b>612</b> of the line merging circuitry <b>318</b>. The output <b>612</b> includes a plurality of bounding boxes <b>614</b> corresponding to lines (e.g., rows) of a receipt. In other words, the missed connections <b>606</b> from <figref idref="DRAWINGS">FIG. <b>6</b>A</figref> have been connected and/or otherwise fused to generate a text line.</p><p id="p-0146" num="0141">Referring again to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the example column detection circuitry <b>312</b> is structured to detect columns using the products region of the receipt image <b>108</b>. The products region typically includes multiple sub-regions of interest, which are divided or organized based on columns. For example, the products region can include details such as product description, quantity, and price in a column-type organization. In some examples, each column corresponds to one or more purchase details.</p><p id="p-0147" num="0142">In some examples, the column detection circuitry <b>312</b> applies a generic column detector that detects columns without having to learn or identify the specific type of column. For example, the generic column detector can be used because the column detection circuitry <b>312</b> is concerned with detecting a structure of the receipt, not with text of the receipt. In the illustrated example of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the column detection circuitry <b>312</b> applies a CNN-based segmentation model to the products region of the receipt image <b>108</b>. Accordingly, the column detection circuitry <b>312</b> includes an example column segmentation model(s) <b>320</b>. In some examples, the column segmentation model <b>320</b> is a pixel-wise classifier that splits the products region into columns, but does not identify (e.g., classify) types of columns (e.g., price column, products column, etc.). In some examples, when the column detection circuitry <b>312</b> applies the column segmentation model <b>320</b> to the products region, each pixel within the products region is identified as belonging to a first class or a second class. For example, the first class can be a column class and the second class can be another class, etc. In some examples, the column segmentation model <b>320</b> outputs a pixel-wise mask (e.g., classifier) segments pixels. In some examples, the pixel-wise mask includes groups of pixels classified as belonging to a column in a cluster. In the illustrated example of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the pixel-wise mask output by the column segmentation model <b>320</b> needs refinement to yield a desired outcome of bounding boxes corresponding to text columns.</p><p id="p-0148" num="0143">The column detection circuitry <b>312</b> includes example bounding box generating circuitry <b>322</b>, which is structured to generate bounding boxes corresponding to pixel clusters output by the column segmentation model <b>320</b>. To generate the bounding boxes, the bounding box generating circuitry <b>322</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref> identifies contours around the clusters of pixels that were identified as belonging to a column. In some examples, the detected contours can be converted into polygons (e.g., bounding boxes) using example bounding box generating circuitry <b>322</b>. In some examples, the bounding box generating circuitry <b>322</b> is structured to generate a plurality of bounding boxes including respective coordinates corresponding to columns of the products region. In some examples, the bounding box generating circuitry <b>322</b> applies similar methods to the example bounding box generating circuitry <b>316</b> of the row detection circuitry <b>310</b>.</p><p id="p-0149" num="0144"><figref idref="DRAWINGS">FIG. <b>7</b>A</figref> is an example products region <b>700</b> that includes an example pixel-wise classifier <b>702</b>. For example, the products region <b>700</b> having the pixel-wise classifier <b>702</b> can be output by the example column segmentation model <b>320</b>. The pixel-wise segmentation includes segmented regions classified as belonging to a columns <b>704</b> and a segment region classified as belonging to background <b>706</b>.</p><p id="p-0150" num="0145"><figref idref="DRAWINGS">FIG. <b>7</b>B</figref> illustrates example bounding boxes (e.g., polygons, polygonal regions, etc.) <b>708</b> that can be detected and output by example bounding box generating circuitry <b>322</b>. For example, the bounding boxes <b>708</b> can correspond to segmented regions classified as belonging to a columns <b>704</b>. In some examples, the bounding boxes <b>708</b> correspond to columns of the products region <b>700</b>.</p><p id="p-0151" num="0146">Referring again to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the bounding box generating circuitry <b>322</b> outputs a plurality of bounding boxes corresponding to columns. In some examples, however, two or more bounding boxes that belong to the same text column are not connected. For example, the bounding box generating circuitry <b>322</b> may miss a connection if a wide blank space is present between words. Accordingly, the column detection circuitry <b>312</b> includes example column merging circuitry <b>324</b>.</p><p id="p-0152" num="0147">The example column merging circuitry <b>324</b> is structured to connect bounding boxes that belong to the same column but were not connected during by the bounding box generating circuitry <b>322</b>. In some examples, the column merging circuitry <b>324</b> applies similar methods as the line merging circuitry <b>318</b> of the row detection circuitry <b>310</b>. For example, the column merging circuitry <b>324</b> can initialize a graph with node representing each detected column and add edges between two nodes that meet two conditions. In some examples, conditions are a column connection criterion. In some examples, first condition is that the two nodes share a positive horizontal coordinate (e.g., a x-axis coordinate). In some examples, the second condition is that the two nodes do not have any overlapping vertical coordinates (e.g., y-axis coordinates). In some examples, connected components in the graph can be identified using a depth-first search (DFS) approach. In the illustrated example of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the column merging circuitry <b>324</b> iterates over the connected components one by one and arranges columns to corresponding nodes in a top to bottom to merge a topmost and a bottommost column and ignore other (e.g., intermediate) columns. In some examples, the column detection circuitry <b>312</b> outputs a list of polygonal regions that are fitted to columns within the products region and include coordinates.</p><p id="p-0153" num="0148"><figref idref="DRAWINGS">FIG. <b>8</b>A</figref> illustrates an example products region <b>800</b> of an example receipt image <b>108</b>. The products region <b>800</b> includes a first column <b>802</b> and a second column <b>804</b>. In the illustrated example of <figref idref="DRAWINGS">FIG. <b>8</b>A</figref>, a column segmentation model <b>320</b> has been applied to the products regions <b>800</b> and bounding box generating circuitry <b>322</b> has generated a plurality of bounding boxes corresponding to columns. However, the bounding box generating circuitry <b>322</b> missed a connection between a first purchase price and a second purchase price, both of which correspond to the second column <b>804</b>. That first purchase price has a first bounding box <b>806</b> and the second purchase price has a second bounding box <b>808</b>. However, the first price and the second price should share a bounding box since they both belong to the same column <b>804</b>.</p><p id="p-0154" num="0149"><figref idref="DRAWINGS">FIG. <b>8</b>B</figref> illustrates the price column <b>804</b> of <figref idref="DRAWINGS">FIG. <b>8</b>A</figref> after algorithms were applied to merge and/or otherwise fuse the broken column. For example, the column merging circuitry <b>324</b> applied algorithms to connect the first bounding box <b>806</b> and the second bounding box <b>808</b>. The result is a bounding box <b>810</b> corresponding to the column <b>804</b>.</p><p id="p-0155" num="0150">Referring again to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the extraction circuitry <b>118</b> includes example data extracting circuitry <b>326</b>, which is structured to extract purchase information from the receipt image <b>108</b>. The example OCR circuitry <b>116</b> transmits detected bounding boxes and coordinates corresponding to detected words to the data extracting circuitry <b>326</b>. Further, the example row detection circuitry <b>310</b> and the example column detection circuitry <b>312</b> transmit row bounding boxes and coordinates and column bounding boxes and coordinates, respectively, to the data extracting circuitry <b>326</b>. The data extracting circuitry <b>326</b> inputs the text bounding boxes, row bounding boxes, and column bounding boxes into example structuring circuitry <b>330</b>.</p><p id="p-0156" num="0151">The example structuring circuitry <b>330</b> is structured to form (e.g., generate, build, etc.) a structural layout of at least a targeted portion the receipt. For example, the structuring circuitry <b>330</b> can generate a table-like structure corresponding to the products regions of the receipt image <b>108</b>. In some examples, the structuring circuitry <b>330</b> transforms coordinates of the text bounding boxes, the row bounding boxes, and/or the column bounding boxes based on reference coordinates of the original receipt image <b>108</b>. For example, the coordinates of the bounding boxes may need to be transformed because text bounding boxes were generated using the receipt image <b>108</b> while the row bounding boxes were generated using the cropped receipt region and the column bounding boxes were generated using the cropped products region. In some examples, the row bounding boxes are transformed by comparing their coordinates within the receipt region with the receipt region's coordinates with the receipt image <b>108</b>. For examples, the structuring circuitry <b>330</b> can transform the row bounding boxes by adding their coordinates within the receipt regions with the receipt region's coordinates within the receipt image <b>108</b>. Similarly, in some examples, the column bounding boxes are transformed by comparing their coordinates within the products region with the products region's coordinates with the receipt image <b>108</b>. For examples, the structuring circuitry <b>330</b> can transform the column bounding boxes by adding their coordinates within the products regions with the products region's coordinates within the receipt image <b>108</b>.</p><p id="p-0157" num="0152">In some examples, the structuring circuitry <b>330</b> extends the row bounding boxes and/or the column bounding boxes to an intersection with a products region boundary. For example, the extensions can enable inclusion of words or characters near a borders that may have been missed by the row bounding boxes and/or the column bounding boxes. Once the structure of the receipt is determined, the structure is transmitted to example mapping circuitry <b>332</b>.</p><p id="p-0158" num="0153">The example mapping circuitry <b>332</b> is structured to map words detected by the OCR circuitry <b>116</b> to corresponding rows and columns (e.g., to generate a data frame). In some examples, an area of a word that is intersected with a row bounding box and/or a column bounding box is calculated to determine a highest overlapping bounding box in which to position the word. For example IoU calculations between bounding boxes can be calculated to determine the highest overlapping bounding boxes. In some examples, the text bounding boxes are assigned to rows and/or columns that have the highest IoU calculations. In some examples, a digitized receipt corresponding to a receipt image <b>108</b> is generated once the mapping is complete. In some examples, the mapping circuitry <b>332</b> can transmit the digitized receipt to the database <b>120</b> and/or another database for storage.</p><p id="p-0159" num="0154">The extraction circuitry <b>118</b> includes example column identifying circuitry <b>334</b>, which is structured to identify column types by detecting an expression of interest. For example, the column identifying circuitry <b>334</b> can classify column headers that identify expressions of interest that correspond to targeted facts (e.g., item description, quantity, and/or price) and/or supplier identification. In some examples, the column identifying circuitry <b>334</b> applies techniques based on natural language processing algorithms and tools for manipulation and interpretation of detected text. In some examples, the column identifying circuitry <b>334</b> implements a regex engine that applies example regex to identify the column types. For example, the regex can identify strings of characters that specify a search pattern that can be used to match and locate text that corresponds to targeted information such as product description, price, and product code. In some examples, columns that do not include information that falls into a defined regex are classified as an &#x201c;unknown&#x201d; column type. In some examples, regex such as quantity, unit of measure, etc. can be present in any column or as a separate line. In some such examples, another regex is generated to account for such regex. For example, the column identifying circuitry <b>334</b> can apply a regex that searches for a number followed by a space followed by a product description. In additional or alternative examples, FastText can be used to identify columns. FastText is a technique based on an efficient learning of word representations and sentence classification. This technique allows for supervised and unsupervised representations of words and sentences. In some examples, the representations are named embeddings that can be used for numerous applications from data compression, as features into additional models, for candidate selection, or as initializers for transfer learning. In some examples, a data frame corresponding to a receipt is generated once the columns are identified. In some examples, the column identifying circuitry <b>334</b> can transmit the data frame to the database <b>120</b> and/or another database for storage.</p><p id="p-0160" num="0155">The example purchase information extracting circuitry <b>336</b> is structured to detect purchase information from the digitized receipt. For example the purchase information extracting circuitry <b>336</b> extracts purchase details to transmit to the decoding circuitry <b>122</b> for the decoding process. In some examples, the purchase information extracting circuitry <b>336</b> extracts additional information, such as a promotion applied to the basket. For example, the receipt image <b>108</b> can include promotions, such as discounts, multi-buys promotions, etc. Accordingly, in some examples, the column identifying circuitry <b>334</b> inputs the digitized receipt into example promotion identifying circuitry <b>328</b>.</p><p id="p-0161" num="0156">In some examples, the purchase information extracting circuitry <b>336</b> is structured to extract purchase details from the data frame. For example, the purchase details include product descriptions for purchased products and prices of the purchased products. In some examples, the purchase information extracting circuitry <b>336</b> validates and/or drops elements or rows that do not qualify a condition defined in a respective regex (e.g., removing price from item description column, removing a letter present in a price column, etc.). In some examples, a collection of words is stored in the example database <b>120</b> for various retailers, which are referred to as stop words. In some examples, the stop words indicate that a particular row is invalid for the extraction process. For example, stop words can include words such as total, promotion, multibuy, and/or words that cannot be considered as a purchase detail. In some examples, the purchase information extracting circuitry <b>336</b> removes rows or elements based on stop words.</p><p id="p-0162" num="0157">After classifying columns and dropping rows of elements that do not include text of interest, the purchase information extracting circuitry <b>336</b> has extracted text of interest for the decoding process from the receipt. In some examples, the purchase information extracting circuitry <b>336</b> can output a list of purchased products, including product descriptions, prices, quantities, etc. In some examples, the purchase information extracting circuitry <b>336</b> transmits the purchase details to the decoding circuitry <b>122</b> for decoding.</p><p id="p-0163" num="0158">The promotion identifying circuitry <b>328</b> is structured to detect various types of promotions. In some examples, the promotion identifying circuitry <b>328</b> can extract a price paid for a specific purchased product (e.g., after a discount is applied), a discount amount, and an original price. In some examples, the price paid, original price, and discount amount are determined for each purchased product detected by data extracting circuitry <b>326</b>. In some such examples, the price paid and the original price can be the same and the discount amount can be zero (e.g., if no discount was applied). In some examples, the promotion identifying circuitry <b>328</b> first identifies a price present in the products region and a price present in a discounting section. For example, the price present in the discounting section can be a discount amount or a paid price. In some examples, the promotion identifying circuitry <b>328</b> can calculate a third price based on the two types of prices present in the receipt. In some examples, the promotion identifying circuitry <b>328</b> is structured to flag purchased products as including a promotion, multi-buy, etc.</p><p id="p-0164" num="0159">In some examples, the extraction circuitry <b>118</b> includes means for generating row bounding boxes. For example, the means for generating row bounding boxes may be implemented by row detection circuitry <b>310</b>. In some examples, the row detection circuitry <b>310</b> may be instantiated by processor circuitry such as the example processor circuitry <b>3512</b> of <figref idref="DRAWINGS">FIG. <b>35</b></figref>. For instance, the row detection circuitry <b>310</b> may be instantiated by the example general purpose processor circuitry <b>3600</b> of <figref idref="DRAWINGS">FIG. <b>36</b></figref> executing machine executable instructions such as that implemented by at least blocks <b>2708</b> of <figref idref="DRAWINGS">FIGS. <b>27</b> and <b>29</b></figref>. In some examples, the row detection circuitry <b>310</b> may be instantiated by hardware logic circuitry, which may be implemented by an ASIC or the FPGA circuitry <b>3700</b> of <figref idref="DRAWINGS">FIG. <b>37</b></figref> structured to perform operations corresponding to the machine readable instructions. Additionally or alternatively, the row detection circuitry <b>310</b> may be instantiated by any other combination of hardware, software, and/or firmware. For example, the row detection circuitry <b>310</b> may be implemented by at least one or more hardware circuits (e.g., processor circuitry, discrete and/or integrated analog and/or digital circuitry, an FPGA, an Application Specific Integrated Circuit (ASIC), a comparator, an operational-amplifier (op-amp), a logic circuit, etc.) structured to execute some or all of the machine readable instructions and/or to perform some or all of the operations corresponding to the machine readable instructions without executing software or firmware, but other structures are likewise appropriate.</p><p id="p-0165" num="0160">In some examples, the extraction circuitry <b>118</b> includes means for generating column bounding boxes. For example, the means for generating column bounding boxes may be implemented by column detection circuitry <b>312</b>. In some examples, the column detection circuitry <b>312</b> may be instantiated by processor circuitry such as the example processor circuitry <b>3512</b> of <figref idref="DRAWINGS">FIG. <b>35</b></figref>. For instance, the column detection circuitry <b>312</b> may be instantiated by the example general purpose processor circuitry <b>3600</b> of <figref idref="DRAWINGS">FIG. <b>36</b></figref> executing machine executable instructions such as that implemented by at least blocks <b>2710</b> of <figref idref="DRAWINGS">FIGS. <b>27</b> and <b>30</b></figref>. In some examples, the column detection circuitry <b>312</b> may be instantiated by hardware logic circuitry, which may be implemented by an ASIC or the FPGA circuitry <b>3700</b> of <figref idref="DRAWINGS">FIG. <b>37</b></figref> structured to perform operations corresponding to the machine readable instructions. Additionally or alternatively, the column detection circuitry <b>312</b> may be instantiated by any other combination of hardware, software, and/or firmware. For example, the column detection circuitry <b>312</b> may be implemented by at least one or more hardware circuits (e.g., processor circuitry, discrete and/or integrated analog and/or digital circuitry, an FPGA, an Application Specific Integrated Circuit (ASIC), a comparator, an operational-amplifier (op-amp), a logic circuit, etc.) structured to execute some or all of the machine readable instructions and/or to perform some or all of the operations corresponding to the machine readable instructions without executing software or firmware, but other structures are likewise appropriate.</p><p id="p-0166" num="0161">In some examples, the extraction circuitry <b>118</b> includes means for extracting purchase information. For example, the means for extracting purchase information may be implemented by data extracting circuitry <b>326</b>. In some examples, the data extracting circuitry <b>326</b> may be instantiated by processor circuitry such as the example processor circuitry <b>3512</b> of <figref idref="DRAWINGS">FIG. <b>35</b></figref>. For instance, the data extracting circuitry <b>326</b> may be instantiated by the example general purpose processor circuitry <b>3600</b> of <figref idref="DRAWINGS">FIG. <b>36</b></figref> executing machine executable instructions such as that implemented by at least blocks <b>2712</b> of <figref idref="DRAWINGS">FIGS. <b>27</b> and <b>31</b></figref>. In some examples, the data extracting circuitry <b>326</b> may be instantiated by hardware logic circuitry, which may be implemented by an ASIC or the FPGA circuitry <b>3700</b> of <figref idref="DRAWINGS">FIG. <b>37</b></figref> structured to perform operations corresponding to the machine readable instructions. Additionally or alternatively, the data extracting circuitry <b>326</b> may be instantiated by any other combination of hardware, software, and/or firmware. For example, the data extracting circuitry <b>326</b> may be implemented by at least one or more hardware circuits (e.g., processor circuitry, discrete and/or integrated analog and/or digital circuitry, an FPGA, an Application Specific Integrated Circuit (ASIC), a comparator, an operational-amplifier (op-amp), a logic circuit, etc.) structured to execute some or all of the machine readable instructions and/or to perform some or all of the operations corresponding to the machine readable instructions without executing software or firmware, but other structures are likewise appropriate.</p><p id="p-0167" num="0162">While an example manner of implementing the example extraction circuitry <b>118</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> is illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, one or more of the elements, processes, and/or devices illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref> may be combined, divided, re-arranged, omitted, eliminated, and/or implemented in any other way. Further, the example datastore interface circuitry <b>302</b>, example regions detection circuitry <b>304</b>, example image cropping circuitry <b>308</b>, example row detection circuitry <b>310</b>, example bounding box generating circuitry <b>316</b>, example line merging circuitry <b>318</b>, example column detection circuitry <b>312</b>, example bounding box generating circuitry <b>322</b>, example column merging circuitry <b>324</b>, example data extracting circuitry <b>326</b>, example promotion identifying circuitry <b>328</b>, example structuring circuitry <b>330</b>, example mapping circuitry <b>332</b>, example column identifying circuitry <b>334</b>, example purchase information extracting circuitry <b>336</b>, and/or, more generally, the example extraction circuitry <b>118</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, may be implemented by hardware alone or by hardware in combination with software and/or firmware. Thus, for example, any of the example datastore interface circuitry <b>302</b>, example regions detection circuitry <b>304</b>, example image cropping circuitry <b>308</b>, example row detection circuitry <b>310</b>, example bounding box generating circuitry <b>316</b>, example line merging circuitry <b>318</b>, example column detection circuitry <b>312</b>, example bounding box generating circuitry <b>322</b>, example column merging circuitry <b>324</b>, example data extracting circuitry <b>326</b>, example promotion identifying circuitry <b>328</b>, example structuring circuitry <b>330</b>, example mapping circuitry <b>332</b>, example column identifying circuitry <b>334</b>, example purchase information extracting circuitry <b>336</b>, and/or, more generally, the example extraction circuitry <b>118</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, could be implemented by processor circuitry, analog circuit(s), digital circuit(s), logic circuit(s), programmable processor(s), programmable microcontroller(s), graphics processing unit(s) (GPU(s)), digital signal processor(s) (DSP(s)), application specific integrated circuit(s) (ASIC(s)), programmable logic device(s) (PLD(s)), and/or field programmable logic device(s) (FPLD(s)) such as Field Programmable Gate Arrays (FPGAs). Further still, the example extraction circuitry <b>118</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> may include one or more elements, processes, and/or devices in addition to, or instead of, those illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, and/or may include more than one of any or all of the illustrated elements, processes and devices.</p><p id="p-0168" num="0163"><figref idref="DRAWINGS">FIG. <b>9</b></figref> illustrates an example outputs of different models for application by the example extraction circuitry <b>118</b> for performing document analysis and recognition. Generally, document analysis and recognition aims to automatically extract information from digital documents. In other words, <figref idref="DRAWINGS">FIG. <b>9</b></figref> illustrates steps for detecting targeted regions and text from images of receipts. In some examples, the extraction circuitry <b>118</b> relies on AI models to detect the target regions and text. For example, the AI models may implement different aspects of ML and AI including, for example, DL, CV, and NLP.</p><p id="p-0169" num="0164">In the illustrated example of <figref idref="DRAWINGS">FIG. <b>9</b></figref>, the example extraction circuitry <b>118</b> receives an example receipt image <b>108</b> (e.g., from the basket datastore <b>112</b>). The extraction circuitry <b>118</b> transmits the receipt image <b>108</b> to the example OCR circuitry <b>116</b> to detect and extract text. The extraction circuitry <b>118</b> applies an example regions detection model <b>306</b> to the receipt image <b>108</b> to detect an example receipt region <b>902</b> and an example products region <b>904</b>. In some examples, the extraction circuitry <b>118</b> applies a cropping operation (e.g., via the image cropping circuitry <b>308</b>) to the receipt image <b>108</b> based on the detected regions <b>902</b>, <b>904</b>. In some examples, the extraction circuitry <b>118</b> applies a row segmentation model <b>314</b> to a cropped receipt region <b>902</b> to detect rows within the receipt region <b>902</b>. In some examples, the extraction circuitry <b>118</b> also applies post-processing steps to the output of the row segmentation model <b>314</b> to generate bounding boxes corresponding to the detected row. In some examples, the extraction circuitry <b>118</b> applies a column segmentation model <b>320</b> to a cropped products region <b>404</b> to detect columns within the products region <b>904</b>. In some examples, the extraction circuitry <b>118</b> also applies post-processing steps to the output of the column segmentation model <b>320</b> to generate bounding boxes corresponding to the detected columns.</p><p id="p-0170" num="0165">The extraction circuitry <b>118</b> aggregates the text extracted by the OCR circuitry <b>116</b>, the detect columns in the form of bounding boxes to represent the vertical structure, and the detected rows in the form of bounding boxes to represent the horizontal structure. In some examples, the extraction circuitry <b>118</b> can combine these outputs with algorithms to generate a unified result that includes rows, columns, and detected text within respective rows and columns. For example, the unified result can correspond to a digitized receipt of the respective receipt image <b>108</b>. In other words, the extraction circuitry <b>118</b> receives an image of a receipt and outputs a digitized receipt and/or a data frame.</p><p id="p-0171" num="0166"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a block diagram of example system <b>1000</b> for training models including example machine learning circuitry (e.g., machine learning circuitry <b>136</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>). The system <b>1000</b> includes example training images <b>1002</b>, which can be labeled receipt images. In some examples, the models include an example regions detection model(s) <b>306</b>, an example row segmentation model(s) <b>314</b>, and/or an example column segmentation model(s) <b>320</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>. In some examples, additional or alternative models may be trained and/or utilized.</p><p id="p-0172" num="0167">In some examples, the training images <b>1002</b> are stored in the example database <b>120</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> and obtained by the system <b>1000</b>. The training images <b>1002</b> can be transmitted to example regions detection model training circuitry <b>1004</b>, which is structured to train an example regions detection model(s) <b>306</b>. In examples disclosed herein, the regions detection model(s) <b>306</b> is a faster R-CNN model. The faster R-CNN technique localizes and classifies objects over images. However, other model architectures can be use in additional or alternative examples, such as YOLO, SSD, etc. In some examples, the training images <b>1002</b> used to train the regions detection model <b>306</b> include previously labeled receipt and product detail regions to provide knowledge to the model during training.</p><p id="p-0173" num="0168">In some examples, an output of the regions detection model(s) <b>306</b> during training is compared to example ground truth annotations <b>1006</b>. For example, an IoU can be calculated between detected and annotated regions. In some examples, the IoU calculation should meet a threshold value of 0.8 to be considered an accurate result. In some examples, the regions detection model(s) <b>306</b> can be trained with a Resnet-101 as a backbone feature extractor network. For example, the Resnet-101 feature extractor network can be pre-trained with an ImageNet dataset. In other words, a transfer learning strategy can be implemented by using the pre-trained weights on the ImageNet dataset.</p><p id="p-0174" num="0169">In some examples, the training images <b>1002</b> can be cropped by example cropping circuitry <b>1008</b>. For example, the cropping circuitry <b>1008</b> can crop the training image <b>1002</b> based on the ground truth annotation <b>1006</b> to generate example row training images <b>1010</b>. In some examples, the row training image(s) <b>1010</b> are provided to example row detection model training circuitry <b>1012</b>, which is structured to train example row segmentation model(s) <b>314</b>. In some examples, the row training image(s) <b>1010</b> include pairs of images <b>1010</b>, including an image and a labeled pixel-wise mask. For example, the training image(s) <b>1010</b> used to train the row segmentation model <b>314</b> can be collected receipts with labeled polygonal regions. The row detection model training circuitry <b>1012</b> can train the row segmentation model(s) <b>314</b> using any suitable architecture, such as a dhSement architecture, U-Net architecture, etc. Examples disclosed herein use the dhSegment architecture to train and implement the row segmentation model(s) <b>314</b>. The dhSegment architecture was originally proposed as a Deep Learning framework to process handwritten historical documents. In that use case, text is dense and organized in several lines per page. In some examples, the dhSegment techniques can be applied to the detection of rows in receipts to help overcome challenges introduced in the captured images by classifying each pixel as belonging to one of two classes (e.g., a row or background). Further, dhSegment techniques can be applied regardless of purchased item lines in receipts being tabulated and/or having blank spaces longer than one character (e.g., to separate product description from price and/or quantity).</p><p id="p-0175" num="0170">In some examples, an output of the row segmentation model <b>314</b> during training is compared to a labeled counterpart. For example, an IoU can be calculated between detected row and the label row. In some examples, the IoU calculation should meet a threshold value of 0.8 to be considered an accurate results. In some examples, the row segmentation model <b>314</b> can be trained with a Resnet-<b>101</b> as a backbone feature extractor network. For example, the Resnet-<b>101</b> feature extractor network can be pre-trained with an ImageNet dataset.</p><p id="p-0176" num="0171">In some examples, the cropping circuitry <b>1008</b> can crop the training images <b>1002</b> based on the ground truth annotations <b>1006</b> to generate example column training images <b>1014</b>. In some examples, the column training image(s) <b>1014</b> are provided to example column detection model training circuitry <b>1016</b>, which is structured to train example column segmentation model(s) <b>320</b>. In some examples, the column training images <b>1014</b> include pairs of training images <b>1014</b> that include an image and a labeled pixel-wise classifier. The column detection model training circuitry <b>1016</b> can train the column segmentation model(s) <b>320</b> using any suitable architecture, such as a dhSement architecture, U-Net architecture, etc. Examples disclosed herein train the column segmentation model <b>320</b> using U-Net, which is a semantic segmentation model. U-Net is CNN that was originally applied to biomedical images for segmentation of brain neuronal structures in electron microscopic stacks. U-Net can be used to train the column segmentation model <b>320</b> to split a products region into multiple parts based on spatial arrangements of texts inside it. In some examples, the column segmentation model <b>320</b> is trained to split the products region into column regions and background.</p><p id="p-0177" num="0172">U-Net can be used as an application of data augmentation strategies to train a CNN with an image dataset of limited size. While dhSegment could be used to train and implement a column segmentation model <b>320</b>, in some examples, U-Net tends to give better output in terms of precision. In some examples, the U-Net model is used for column detection because higher precision can be needed to avoid unwanted regions in the detected column region.</p><p id="p-0178" num="0173"><figref idref="DRAWINGS">FIG. <b>11</b>A</figref> is an example image pair <b>1100</b>, including an example receipt region <b>1102</b> and a labeled receipt region <b>1104</b>, that can be used to train a row segmentation model <b>314</b>. The receipt region <b>1102</b> is a receipt region of an example receipt image. The labeled receipt region <b>1104</b> can be labeled pixel-wise mask. In other words, the labeled receipt region <b>1104</b> classifies each pixel of the receipt region <b>1102</b> as belonging to either a text line or background. In some examples, the receipt region <b>1102</b> can be used to train the row segmentation model <b>314</b> and the labeled receipt region <b>1104</b> can be used to verify an output of the receipt region <b>1102</b> after being run through the row segmentation model <b>314</b>.</p><p id="p-0179" num="0174"><figref idref="DRAWINGS">FIG. <b>11</b>B</figref> illustrates an example image pair <b>1106</b>, including a products region <b>1108</b> of a receipt image and a labeled products region <b>1110</b> of the receipt image, used to train a column segmentation model <b>320</b>. In the illustrated example of <figref idref="DRAWINGS">FIG. <b>11</b>B</figref>, the labeled products region <b>1110</b> is labeled as a pixel-wise classifier. In other words, the labeled products region <b>1108</b> classifies each pixel of the products region <b>1108</b> as belonging to either a column or background. In some examples, the products region <b>1108</b> can be used to train the column segmentation model <b>320</b> and the labeled products region <b>1110</b> can be used to verify an output of the products region <b>1108</b> after being run through the column segmentation model <b>320</b>.</p><p id="p-0180" num="0175"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is an illustration of an example faster R-CNN architecture <b>1200</b> for detection and classification of a receipt region and a product regions. In some examples, the architecture <b>1200</b> can be used to implement and/or train models disclosed herein. In some examples, the architecture <b>1200</b> is used to detect regions of interest. In some examples, the architecture <b>1200</b> can be applied for detecting the regions of interest based on an R-CNN approach.</p><p id="p-0181" num="0176">In a faster R-CNN architecture <b>1200</b>, an example image classification CNN <b>1202</b> can be applied to an receipt image <b>108</b>. In some examples, the CNN <b>1202</b> generates an example feature map <b>1204</b> that includes projections of example regions of interest. In some examples, the feature map <b>1204</b> is used as input to an example region proposal network (RPN) <b>1206</b>. For example, instead of applying a selective search algorithm on the feature map <b>1204</b> to identify region proposals as with a R-CNN architecture, the faster R-CNN uses the RPN <b>1206</b> to predict example region proposals <b>1208</b>. The predicted region proposals <b>1208</b> can be re-sized using an example regions of interest (RoI) pooling layer <b>1210</b>, which is then used to classify the image within the proposed region <b>1208</b> and predict coordinates for the bounding boxes.</p><p id="p-0182" num="0177"><figref idref="DRAWINGS">FIG. <b>13</b></figref> illustrates an example dhSegment architecture <b>1300</b>, including an example dhSegment block <b>1302</b> and output <b>1304</b> and an example line post-processing block <b>1306</b> and output <b>1308</b>. In some examples, the dhSegment <b>1300</b> is an example machine learning framework. In some examples, the dhSement architecture <b>1300</b> is used to detect rows within a receipt image <b>108</b>. Row detection enables structuring scattered words on the receipt canvas. In some examples, the dhSegment block <b>1302</b> implements a row segmentation model <b>314</b>. In some such examples, the output <b>1304</b> can be an output of the row segmentation model <b>314</b>. In some examples, the line post-processing block <b>1306</b> can implement example bounding box generating circuitry <b>316</b>. In some such examples, the output <b>1308</b> can be an output of the bounding box generating circuitry <b>316</b>.</p><p id="p-0183" num="0178">The example dhSegment block <b>1302</b> can group words together that belong to the same line in a scanned receipt image as a human would read them. This process can be challenging because the captured images can be skewed, tilted in any direction, squeezed, or rotated by some angle. In addition, the physical receipt may be crumpled and have non-flat surface deformations. In some examples, the dhSegment block <b>1302</b> can be used to overcome such challenges because the dhSegment block <b>1302</b> classifies each pixel as either belonging to a line or not. As such, imperfections in the receipt image <b>108</b> can be ignored by the dhSegment block <b>1302</b>.</p><p id="p-0184" num="0179"><figref idref="DRAWINGS">FIG. <b>14</b></figref> illustrates an example U-Net block diagram <b>1400</b>, which is an encoder-decoder based architecture. The example architecture <b>1400</b> may be implemented with an example column detection model(s) <b>320</b>. In some examples, the products region includes multiple subregions of interest that are segmented into columns. In some examples, column detection is used to segregate words present in rows based on a column type, which can include item description, price, unit of measure, quantity, UPC/LAC/product code, tax notations, discount/promotions, etc. However, column detection as disclosed herein is managed in a manner that is independent of OCR output. As a result, examples disclosed herein apply the U-Net architecture <b>1400</b> to implement example column segmentation models <b>320</b>.</p><p id="p-0185" num="0180">The U-Net architecture <b>1400</b> receives an image as an input and applies an encoder network (e.g., contracting path) followed by a decoder network (e.g., expanding path). In some examples, the encoder network is a pre-trained classification network such as a ResNet. The encoder network can apply a plurality of convolution blocks, each of which can be followed by a maxpool downsampling block. In some examples, the encoder network encodes the input image into feature representations at multiple different levels. A goal of the decoder network is to semantically project the feature representation that the encoder network leaned onto a pixel space to generate a dense classification. In some examples, the decoder network applies upsampling blocks and concatenation followed by regular convolution operations. In other words, the U-Net architecture <b>1400</b> combines location information from encoder network with contextual information in the decoder network to obtain a pixel-wise classifier that combines localization and context.</p><p id="p-0186" num="0181"><figref idref="DRAWINGS">FIG. <b>15</b>A</figref> illustrates example detected rows using a dhSegment model. That is, <figref idref="DRAWINGS">FIG. <b>15</b>A</figref> includes an output <b>1502</b> of the dhSegment model and a corresponding output <b>1504</b> of bounding box generating circuitry <b>316</b>.</p><p id="p-0187" num="0182"><figref idref="DRAWINGS">FIG. <b>15</b>B</figref> illustrates example detected rows using a U-NET model. That is, <figref idref="DRAWINGS">FIG. <b>15</b>B</figref> includes an output <b>1506</b> of the U-NET model and a corresponding output <b>1508</b> of the bounding box generating circuitry <b>316</b>. In some examples, as illustrated in <figref idref="DRAWINGS">FIGS. <b>15</b>A and <b>15</b>B</figref>, the dhSegment model can be better at detecting rows. For example, <figref idref="DRAWINGS">FIG. <b>15</b>B</figref> illustrated numerous bounding boxes that do not include text and/or do not belong to a text line.</p><p id="p-0188" num="0183"><figref idref="DRAWINGS">FIGS. <b>16</b>A-<b>16</b>D</figref> illustrated example column detections output by the example U-Net model <b>1400</b> and post-processed by example column detection circuitry <b>312</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>. <figref idref="DRAWINGS">FIG. <b>16</b>A</figref> illustrates a first output <b>1602</b>. <figref idref="DRAWINGS">FIG. <b>16</b>B</figref> illustrates a first output <b>1604</b>. <figref idref="DRAWINGS">FIG. <b>16</b>C</figref> illustrates a first output <b>1606</b>. <figref idref="DRAWINGS">FIG. <b>16</b>D</figref> illustrates a first output <b>1608</b>. As illustrated in <figref idref="DRAWINGS">FIGS. <b>16</b>A-<b>16</b>D</figref>, the example U-Net model and post-processing block</p><p id="p-0189" num="0184"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is a block diagram of example decoding circuitry <b>122</b> to generate purchase data by decoding purchase information extracted from a receipt image <b>108</b> and barcodes <b>110</b> uploaded by a panelist. The decoding circuitry <b>122</b> of <figref idref="DRAWINGS">FIG. <b>17</b></figref> may be instantiated (e.g., creating an instance of, bring into being for any length of time, materialize, implement, etc.) by processor circuitry such as a central processing unit executing instructions. Additionally or alternatively, the decoding circuitry <b>122</b> of <figref idref="DRAWINGS">FIG. <b>17</b></figref> may be instantiated (e.g., creating an instance of, bring into being for any length of time, materialize, implement, etc.) by an ASIC or an FPGA structured to perform operations corresponding to the instructions. It should be understood that some or all of the decoding circuitry <b>122</b> of <figref idref="DRAWINGS">FIG. <b>17</b></figref> may, thus, be instantiated at the same or different times. Some or all of the circuitry may be instantiated, for example, in one or more threads executing concurrently on hardware and/or in series on hardware. Moreover, in some examples, some or all of the decoding circuitry <b>122</b> of <figref idref="DRAWINGS">FIG. <b>17</b></figref> may be implemented by one or more virtual machines and/or containers executing on the microprocessor.</p><p id="p-0190" num="0185">The decoding circuitry <b>122</b> includes example extraction interface circuitry <b>1702</b>, which is structured to provide an interface between the decoding circuitry <b>122</b> and example extraction circuitry <b>118</b>. For example, the decoding circuitry <b>122</b> can obtain (e.g., receive and/or retrieve) purchase information corresponding to an example receipt image <b>108</b> that was extracted by the extraction circuitry <b>118</b>. In some examples, the purchase information includes a list of products purchased (e.g., items) by the panelist as listed in the receipt image <b>108</b> and corresponding prices and quantities. In some examples the purchase information includes a store ID (e.g., retailer ID), store name (e.g., retailer name), and/or other retailer-related information. In some examples, the extraction interface circuitry <b>1702</b> notifies example datastore interface circuitry <b>1704</b> of the obtained purchase information. In some examples, the extraction interface circuitry <b>1702</b> transmits the purchase information to example matching circuitry <b>1706</b>.</p><p id="p-0191" num="0186">The example datastore interface circuitry <b>1704</b> is structured to provide an interface between the decoding circuitry <b>122</b> and the example basket datastore <b>112</b>. For example, the decoding circuitry <b>122</b> can receive and/or retrieve barcodes <b>110</b> from the basket datastore <b>112</b> via the example datastore interface circuitry <b>1704</b>. In some examples, the decoding circuitry <b>122</b> obtains the barcodes <b>110</b> that correspond to the receipt image <b>108</b> from which the obtained purchase information was extracted. In some examples, the datastore interface circuitry <b>302</b> transmits the barcodes <b>110</b> to the example matching circuitry <b>1706</b>.</p><p id="p-0192" num="0187">The example matching circuitry <b>1706</b> is structured to match items listed in the extracted purchase information with respective barcodes <b>110</b>. For example, the matching circuitry <b>1706</b> can search one or more data sources to identify a match between an item and a respective barcode <b>110</b> and associate the item and corresponding price and quantity with the matched barcode <b>110</b>. In some examples, the matching is based on a similarity search between an item description extracted by the extraction circuitry <b>118</b> and a database description as listed in a data source. In some examples, the matching circuitry <b>1706</b> receives a list of items extracted by the extraction circuitry <b>118</b> and a list of barcodes <b>110</b> provided by the panelist.</p><p id="p-0193" num="0188">The matching circuitry <b>1706</b> includes example barcode filtering circuitry <b>1708</b>, which is structured to remove barcodes <b>110</b> from the list of barcodes. In some examples, the barcode filtering circuitry <b>1708</b> removes duplicate barcodes <b>110</b>. For example, the list of barcodes corresponding to the purchase data can include two or more identical barcodes <b>110</b>. For example, the panelist may have purchased a plurality of an item (e.g., two cans of soup) and scanned a barcode <b>110</b> on each of the plurality of items (e.g., the barcode <b>110</b> on each of the two cans of soup). The barcode filtering circuitry <b>1708</b> can remove the duplicates to generate a list of unique barcodes.</p><p id="p-0194" num="0189">In some examples, the barcode filtering circuitry <b>1708</b> can remove additional or alternative barcodes. In some examples, panelists are inconsistent about whether they scan a barcode <b>110</b> of plastic bag and/or whether the plastic bag is listed in the receipt image <b>108</b>. Accordingly, in some examples, the barcode filtering circuitry <b>1708</b> removes barcodes <b>110</b> that correspond to purchase plastic bags. In some examples, the barcode filtering circuitry <b>1708</b> also removes the plastic bag from the list of items. For example, the barcode filtering circuitry <b>1708</b> can detect and remove the plastic bag from the list of items based on text matching in the product descriptions extracted by the extraction circuitry <b>118</b>.</p><p id="p-0195" num="0190">The matching circuitry <b>1706</b> of <figref idref="DRAWINGS">FIG. <b>17</b></figref> includes example text correcting circuitry <b>1710</b>. In some examples, text (e.g., product descriptions, etc.) extracted by the example OCR circuitry <b>116</b> can include typos, such as mis-identified or unidentified characters. For example, some languages include special characters that can be incorrectly detected by the OCR circuitry <b>116</b>. For example, a character that includes a specific (e.g., small) symbol can be incorrectly detected and/or missed by the OCR circuitry <b>116</b>. These failures of the OCR circuitry <b>116</b> can be difficult to predict because they happen can randomly within the same receipt. Further, this type of error can affect product searches in the data sources, causing inaccurate product matching. While such failures can be easy for a human to resolve when looking at the original receipt image, the failures can be difficult for a machine to handle. Accordingly, the example text correcting circuitry <b>1710</b> is structured to manage and/or correct such typos.</p><p id="p-0196" num="0191">In some examples, the matching circuitry <b>1706</b> includes an example language dictionary(ies) <b>1712</b>. For example, the matching circuitry <b>1706</b> can include a language dictionary <b>1712</b> corresponding to a main language of a country in which the DDS is executing. In some examples, the matching circuitry <b>1706</b> can include more than one language dictionary <b>1712</b>. In some examples, the text correcting circuitry <b>1710</b> can utilize the language dictionary <b>1712</b> and implement a spell checker. For example, the text correcting circuitry <b>1710</b> can compare the list of items to the language dictionary <b>1712</b> to detect and/or correct mis-identified characters.</p><p id="p-0197" num="0192">In some examples, the text correcting circuitry <b>1710</b> can apply text normalization techniques to the list of items to remove a dependency on special characters. For example, the text correcting circuitry <b>1710</b> can apply text normalization techniques to text extracted by the extraction circuitry <b>118</b> and to the data sources (e.g., the internal dictionary <b>124</b>, products datastore <b>126</b>, etc.) such that all sources of data used by the decoding circuitry <b>122</b> are normalized for the text similarity calculations and product searches. In some examples, the text normalization techniques aim to increase product matching accuracy for this particular type of error without decreasing the overall matching performance.</p><p id="p-0198" num="0193">The matching circuitry <b>1706</b> includes example search manager circuitry <b>1714</b>, which is structured to manage searching against a data source(s) (e.g., the internal dictionary <b>124</b>, the products datastore <b>126</b>, the previous jobs database <b>130</b>, etc.). In some examples, the search manager circuitry <b>1714</b> generates input for a search query. For example, the input for the search query can include a single product description of a purchased item from the list of items extracted by the extraction circuitry <b>118</b> and the list of unique barcodes <b>110</b>. In some examples, the input for the search query also includes a store ID and/or retailer ID.</p><p id="p-0199" num="0194">In some examples, the matching circuitry <b>1706</b> includes example memory <b>1716</b>, which is structured to store data. For example, the memory <b>1716</b> can store a list of unique barcodes, purchase information extracted by the extraction circuitry <b>118</b>, a search response, etc. In some examples, the memory <b>1716</b> can store data until the data has been processed. For example, the search manager circuitry <b>1714</b> can store the list of unique barcodes, corresponding purchase information, and corresponding search results until each product listed in the purchase information has been matched or a search for a match has been exhausted.</p><p id="p-0200" num="0195">In some examples, the memory <b>1716</b> can be one or more memory systems that include various types of computer memory. In some examples, the memory <b>1716</b> may be implemented by a volatile memory (e.g., a Synchronous Dynamic Random Access Memory (SDRAM), a Dynamic Random Access Memory (DRAM), a RAMBUS Dynamic Random Access Memory (RDRAM), a double data rate (DDR) memory, such as DDR, DDR2, DDR3, DDR4, mobile DDR (mDDR), etc.)) and/or a non-volatile memory (e.g., flash memory, a hard disk drive (HDD), etc.).</p><p id="p-0201" num="0196">The matching circuitry <b>1706</b> includes example dictionary searching circuitry <b>1718</b>, which is structured to search a dictionary such as the internal dictionary <b>124</b>. In some examples, the dictionary searching circuitry <b>1718</b> is coupled to example internal dictionary interface circuitry <b>1720</b>, which is structured to provide an interface between the dictionary searching circuitry <b>1718</b> and the internal dictionary <b>124</b>. For example, the dictionary searching circuitry <b>1718</b> can generate a search query based on input received from the search manager circuitry <b>1714</b>, and search the query against the internal dictionary <b>124</b> via the internal dictionary interface circuitry <b>1720</b>. In some examples, the internal dictionary <b>124</b> is searched first (e.g., before the products datastore <b>126</b>, the previous jobs database <b>130</b>, etc.). If a match is identified, the match can be saved to a final response. If a match is not identified, the input can be used to search another data source, such as the products datastore <b>126</b>.</p><p id="p-0202" num="0197">In some examples, the search query for the search of the internal dictionary <b>124</b> can include a first item description, the list of unique barcodes, the retailer ID, and a search condition(s). In some examples, the search query can include three search conditions, which aim to return a substantially exact match between the item description and a barcode <b>110</b> based on the data in the internal dictionary <b>124</b>. For example, a first example condition can be that a response(s) should match at least one barcode <b>110</b>. A second example condition can include that the response(s) needs to match the provided store ID. A third example condition can include that the response(s) must match the item description as extracted by the extraction circuitry <b>118</b>.</p><p id="p-0203" num="0198">As noted above, the internal dictionary <b>124</b> includes records of previously associated product descriptions and barcodes <b>110</b>. The records can include information for each previously recognized product, such as a receipt description (e.g., as extracted by extraction circuitry <b>118</b>), a product description as listed in the products datastore <b>126</b>, a store ID, a counter, a matched barcode(s) <b>110</b>, etc. Thus, by searching one item description, the list of unique barcodes <b>110</b>, and the store ID with the search conditions against the internal dictionary <b>124</b>, the dictionary searching circuitry <b>1718</b> can determine whether a match can be identified based on the match being previously identified. For example, by searching the search query against the internal dictionary <b>124</b>, the dictionary searching circuitry <b>1718</b> can identify barcodes in the internal dictionary <b>124</b> that correspond to the unique list of barcodes and to the store ID in the search query. If any such barcodes exist, the dictionary searching circuitry <b>1718</b> can determine whether one of the barcodes includes the first item description. In some examples, if one of the identified barcodes includes the first item description, the dictionary searching circuitry <b>1718</b> can identify a match and associate the barcode and the item description.</p><p id="p-0204" num="0199">The matching circuitry <b>1706</b> includes example products datastore searching circuitry <b>1722</b>, which is structured to search a products datastore (e.g., products datastore <b>126</b>). In some examples, the products datastore searching circuitry <b>1722</b> is coupled to example products datastore interface circuitry <b>1724</b>, which is structed to provide an interface between the products datastore searching circuitry <b>1722</b> and the products datastore <b>126</b>. In some examples, the products datastore <b>126</b> is searched if the search against the internal dictionary <b>124</b> does not yield a match. For example, the products datastore <b>126</b> is searched to recognize a match between a product description and a barcode <b>110</b> if the product description not matched to a barcode <b>110</b> during the internal dictionary <b>124</b> search. In some examples, the search manager circuitry <b>1714</b> transmits the input to the products datastore searching circuitry <b>1722</b>. In some examples, the products datastore searching circuitry <b>1722</b> searches the products datastore <b>126</b> a first time using a first search query that includes the retailer ID. If the first search does not yield a match, the products datastore searching circuitry <b>1722</b> can search the products datastore <b>126</b> a second time using a second search query that excludes the retailer ID.</p><p id="p-0205" num="0200">As noted above, in some examples, the products datastore <b>126</b> is copy of a large database that is obtained on a weekly basis. The products datastore <b>126</b> includes a plurality of products and corresponding attributes for a given country and/or market. In some examples, the products datastore <b>126</b> is implemented with example ElasticSearch technology, which is an open-source full-text search and analytics engine. In some examples, the ElasticSearch technology as applied to the products datastore <b>126</b> enables the products datastore searching circuitry <b>1722</b> to search and analyze large volumes of data quickly.</p><p id="p-0206" num="0201">In some examples, the first search query for the search of the products datastore <b>126</b> can include a first item description (e.g., of an item not matched during the internal dictionary <b>124</b> search), the list of unique barcodes, the retailer ID, and a search condition(s). In some examples, the search query can include numerous search conditions, which aim to return a return a candidate result(s) and corresponding similarity score(s) for matches between the first item description and a barcode <b>110</b>. In some results, the products datastore searching circuitry <b>1722</b> can replace a generic description extracted by the extraction circuitry <b>118</b> with an assigned description to increase chances of identifying a match.</p><p id="p-0207" num="0202">In some examples, the first search of the products datastore <b>126</b> that includes a retailer ID can be limited to products belonging to the retailer ID. To do so, in some examples, the search manager circuitry <b>1714</b> can obtain a &#x201c;group code&#x201d; and/or a &#x201c;global store code&#x201d; (e.g., for a favorite store) from a database (e.g., the database <b>120</b> and/or the retailer database <b>128</b> via an interface) to add to the first search query. If the retailer ID provided by the panelist is from a favorite list, the search manager circuitry <b>1714</b> can add the global store identifier to the first search query. In some examples, the products datastore searching circuitry <b>1722</b> searches the query against the products datastore <b>126</b> via the products datastore interface circuitry <b>1724</b>. In some examples, searching the query against the products datastore <b>126</b> includes identifying barcodes in the products datastore <b>126</b> that correspond to the unique list of barcodes in the query and comparing product descriptions associated with the barcodes in the products datastore <b>126</b> with the first item description. In some such examples, the products datastore searching circuitry <b>1724</b> generates a similarity value (e.g., similarity score, etc.) for the product descriptions associated with the barcodes identified in the products datastore <b>126</b> based on the comparisons. In some examples, the products datastore searching circuitry <b>1724</b> selects one of the barcodes identified in the products datastore <b>126</b> based on the similarity values. For example, the products datastore searching circuitry <b>1724</b> may selected the barcode that receives that highest similarity value above a threshold value.</p><p id="p-0208" num="0203">If a match is identified, the match can be saved to the final response. If a match is not identified, the products datastore searching circuitry <b>1722</b> can generate a second search query to be searched against the products datastore <b>126</b>. For example, the products datastore searching circuitry <b>1722</b> can generate the second search query that includes the first item description (e.g., of the item not matched during the internal dictionary <b>124</b> search or the first products datastore <b>126</b> search), the list of unique barcodes, and a search condition(s). In some examples, item descriptions and barcodes that remain unmatched after a second search of the products datastore <b>126</b> can be added to a list of unassociated products.</p><p id="p-0209" num="0204">The matching circuitry <b>1706</b> includes example previous jobs searching circuitry <b>1726</b>, which is structured to search a previous jobs database (e.g., previous jobs database <b>130</b>). In some examples, the previous jobs database <b>130</b> is implemented with example ElasticSearch technology, enabling the previous jobs searching circuitry <b>1726</b> to search and analyze large volumes of data quickly. In some examples, the previous jobs searching circuitry <b>1726</b> can search the previous jobs database <b>130</b> with a plurality of un-matched product descriptions and barcodes <b>110</b>. In some examples, the previous jobs searching circuitry <b>1726</b> can add matched items to the final response and put any un-matched product descriptions and barcodes <b>110</b> into another list. For example, the other list can be a list of items and barcodes to be added to the previous jobs database <b>130</b>.</p><p id="p-0210" num="0205">In some examples, the matching circuitry <b>1706</b> includes the example previous jobs database <b>130</b>. In additional or alternative examples, the previous jobs database <b>130</b> can be stored in another location that is accessible by the previous jobs database <b>130</b>. The example previous jobs database <b>130</b> can be implemented by any memor(ies), storage device(s) and/or storage disc(s) for storing data such as, for example, flash memory, magnetic media, optical media, etc. Furthermore, the data stored in the example previous jobs database <b>130</b> may be in any data format such as, for example, binary data, comma delimited data, tab delimited data, structured query language (SQL) structures, image data, etc.</p><p id="p-0211" num="0206">In some examples, one item description and one barcode <b>110</b> can remain unmatched after the internal dictionary <b>124</b>, products datastore <b>126</b>, previous jobs database <b>130</b> and/or other data sources have been searched. This can sometimes be referred to as a &#x201c;single match scenario.&#x201d; In such a single match scenario, the search manager circuitry <b>1714</b> is structured to associate the one remaining item description and one remaining barcode <b>110</b> and add the association to the final response.</p><p id="p-0212" num="0207">The decoding circuitry <b>122</b> includes example updating circuitry <b>1730</b>, which is structured to update a data source such as the internal dictionary <b>124</b> and/or the previous jobs database <b>130</b>. In some examples, the updating circuitry <b>1730</b> saves unmatched item descriptions and barcodes <b>110</b> in the previous jobs database <b>130</b> to keep track of previous processed receipts. In some examples, the previous jobs searching circuitry <b>1726</b> updates the previous jobs database <b>130</b>.</p><p id="p-0213" num="0208">In some examples, the updating circuitry <b>1730</b> updates the internal dictionary <b>124</b> to leverage historic data for recognized items in previous processed receipts. In some examples, the internal dictionary <b>124</b> is updated after a basket has been processed. If the recognized items came directly from the dictionary search, only the counter inside the dictionary entry is incremented by one. If the items were recognized in the other search steps, a new entry will be added per recognized receipt description. In some examples, products matched using the products datastore <b>126</b> can be recorded (e.g., added) to the internal dictionary <b>124</b> such that in the future, product searches can yield results faster.</p><p id="p-0214" num="0209">The decoding circuitry <b>122</b> includes examples response generating circuitry <b>1732</b>, which is structured to generate a final response. For example, after the search manager circuitry <b>1722</b> has matched each item in the list of items and/or exhausted searches of available data source, the response generating circuitry <b>1732</b>is structured to aggregate and format the matched items to as the final response. In some examples, the final response is transmitted to example report generating circuitry (e.g., report generating circuitry <b>134</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>) corresponding to a document decoding service (e.g., document decode circuitry <b>114</b>).</p><p id="p-0215" num="0210">In some examples, the decoding circuitry <b>122</b> includes means for matching a product description and a barcode. For example, the means for matching a product description and a barcode may be implemented by matching circuitry <b>1706</b>. In some examples, the matching circuitry <b>1706</b> may be instantiated by processor circuitry such as the example processor circuitry <b>3512</b> of <figref idref="DRAWINGS">FIG. <b>35</b></figref>. For instance, the matching circuitry <b>1706</b> may be instantiated by the example general purpose processor circuitry <b>3600</b> of <figref idref="DRAWINGS">FIG. <b>36</b></figref> executing machine executable instructions such as that implemented by at least blocks <b>2714</b> of FIGS. <b>27</b> <b>33</b>, and <b>34</b>. In some examples, the matching circuitry <b>1706</b> may be instantiated by hardware logic circuitry, which may be implemented by an ASIC or the FPGA circuitry <b>3700</b> of <figref idref="DRAWINGS">FIG. <b>37</b></figref> structured to perform operations corresponding to the machine readable instructions. Additionally or alternatively, the matching circuitry <b>1706</b> may be instantiated by any other combination of hardware, software, and/or firmware. For example, the matching circuitry <b>1706</b> may be implemented by at least one or more hardware circuits (e.g., processor circuitry, discrete and/or integrated analog and/or digital circuitry, an FPGA, an Application Specific Integrated Circuit (ASIC), a comparator, an operational-amplifier (op-amp), a logic circuit, etc.) structured to execute some or all of the machine readable instructions and/or to perform some or all of the operations corresponding to the machine readable instructions without executing software or firmware, but other structures are likewise appropriate.</p><p id="p-0216" num="0211">In some examples, the matching circuitry <b>1706</b> includes means for searching. For example, the means for searching may be implemented by example dictionary searching circuitry <b>1718</b>, example products datastore searching circuitry <b>1722</b>, and/or example previous jobs searching circuitry <b>1726</b>. In some examples, the dictionary searching circuitry <b>1718</b>, products datastore searching circuitry <b>1722</b>, and/or previous jobs searching circuitry <b>1726</b> may be instantiated by processor circuitry such as the example processor circuitry <b>3512</b> of <figref idref="DRAWINGS">FIG. <b>35</b></figref>. For instance, the dictionary searching circuitry <b>1718</b>, products datastore searching circuitry <b>1722</b>, and/or previous jobs searching circuitry <b>1726</b> may be instantiated by the example general purpose processor circuitry <b>3600</b> of <figref idref="DRAWINGS">FIG. <b>36</b></figref> executing machine executable instructions such as that implemented by at least blocks <b>3312</b> of <figref idref="DRAWINGS">FIGS. <b>33</b>, <b>3406</b> and <b>6412</b></figref> of <figref idref="DRAWINGS">FIG. <b>34</b></figref>, and/or <b>3422</b> of <figref idref="DRAWINGS">FIG. <b>34</b></figref>, respectively. In some examples, the dictionary searching circuitry <b>1718</b>, products datastore searching circuitry <b>1722</b>, and/or previous jobs searching circuitry <b>1726</b> may be instantiated by hardware logic circuitry, which may be implemented by an ASIC or the FPGA circuitry <b>3700</b> of <figref idref="DRAWINGS">FIG. <b>37</b></figref> structured to perform operations corresponding to the machine readable instructions. Additionally or alternatively, the dictionary searching circuitry <b>1718</b>, products datastore searching circuitry <b>1722</b>, and/or previous jobs searching circuitry <b>1726</b> may be instantiated by any other combination of hardware, software, and/or firmware. For example, the dictionary searching circuitry <b>1718</b>, products datastore searching circuitry <b>1722</b>, and/or previous jobs searching circuitry <b>1726</b> may be implemented by at least one or more hardware circuits (e.g., processor circuitry, discrete and/or integrated analog and/or digital circuitry, an FPGA, an Application Specific Integrated Circuit (ASIC), a comparator, an operational-amplifier (op-amp), a logic circuit, etc.) structured to execute some or all of the machine readable instructions and/or to perform some or all of the operations corresponding to the machine readable instructions without executing software or firmware, but other structures are likewise appropriate.</p><p id="p-0217" num="0212">While an example manner of implementing the decoding circuitry <b>122</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> is illustrated in <figref idref="DRAWINGS">FIG. <b>17</b></figref>, one or more of the elements, processes, and/or devices illustrated in <figref idref="DRAWINGS">FIG. <b>17</b></figref> may be combined, divided, re-arranged, omitted, eliminated, and/or implemented in any other way. Further, the example extraction interface circuitry <b>1702</b>, example datastore interface circuitry <b>1704</b>, example matching circuitry <b>1706</b>, example barcode filtering circuitry <b>1708</b>, example text correcting circuitry <b>1710</b>, example search manager circuitry <b>1714</b>, example dictionary searching circuitry <b>1718</b>, example internal dictionary interface circuitry <b>1720</b>, example products datastore searching circuitry <b>1722</b>, example products datastore interface circuitry <b>1724</b>, example previous jobs searching circuitry <b>1726</b>, example updating circuitry <b>1730</b>, example response generating circuitry <b>1732</b>, and/or, more generally, the example decoding circuitry <b>122</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, may be implemented by hardware alone or by hardware in combination with software and/or firmware. Thus, for example, any of the example extraction interface circuitry <b>1702</b>, example datastore interface circuitry <b>1704</b>, example matching circuitry <b>1706</b>, example barcode filtering circuitry <b>1708</b>, example text correcting circuitry <b>1710</b>, example search manager circuitry <b>1714</b>, example dictionary searching circuitry <b>1718</b>, example internal dictionary interface circuitry <b>1720</b>, example products datastore searching circuitry <b>1722</b>, example products datastore interface circuitry <b>1724</b>, example previous jobs searching circuitry <b>1726</b>, example updating circuitry <b>1730</b>, example response generating circuitry <b>1732</b>, and/or, more generally, the example decoding circuitry <b>122</b>, could be implemented by processor circuitry, analog circuit(s), digital circuit(s), logic circuit(s), programmable processor(s), programmable microcontroller(s), graphics processing unit(s) (GPU(s)), digital signal processor(s) (DSP(s)), application specific integrated circuit(s) (ASIC(s)), programmable logic device(s) (PLD(s)), and/or field programmable logic device(s) (FPLD(s)) such as Field Programmable Gate Arrays (FPGAs). Further still, the example decoding circuitry <b>122</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> may include one or more elements, processes, and/or devices in addition to, or instead of, those illustrated in FIG. <b>17</b>, and/or may include more than one of any or all of the illustrated elements, processes and devices.</p><p id="p-0218" num="0213"><figref idref="DRAWINGS">FIG. <b>18</b></figref> is a block diagram of an example framework <b>1800</b> to implement the example matching circuitry <b>1706</b> of <figref idref="DRAWINGS">FIG. <b>17</b></figref>. In some examples, the framework <b>1800</b> is implemented by the example search manager circuitry <b>1714</b>. In some examples, the framework <b>1800</b> includes the search manager circuitry <b>1714</b> generating an input <b>1802</b> that includes a list of items (e.g., including product descriptions, quantity, price, etc.) corresponding to an example receipt image <b>108</b> that includes any number of purchased items and corresponding barcodes <b>110</b>. The framework <b>1800</b> can be used to search the items against the internal dictionary <b>124</b>, the products datastore <b>126</b>, and/or the previous jobs database <b>130</b> to associate the items with respective barcodes <b>110</b>. In the illustrated example of <figref idref="DRAWINGS">FIG. <b>18</b></figref>, the framework <b>1800</b> is structured to search each item in the list of items (e.g., one at a time) against a first data source before searching a second data source. For example, the framework <b>1800</b> is structured to search each item against the internal dictionary <b>124</b> first. In some examples, associated items are stored in a final response. In some examples, un-associated items are searched against the products datastore <b>126</b>.</p><p id="p-0219" num="0214">In some examples, the framework <b>1800</b> begins with the example search manager circuitry <b>1714</b> providing the input <b>1802</b> to the dictionary searching circuitry <b>1718</b>. In some examples, the dictionary searching circuitry <b>1718</b> generates a first search query that includes a portion of the input <b>1802</b>. For example, the search query can include a product description for a first item of the list of items, a unique list of barcodes (e.g., with duplicates and/or matched barcodes removed), and a retailer ID corresponding to a retailer from which the items were purchased. In some examples, dictionary searching circuitry <b>1718</b> can then search the first search query against the internal dictionary <b>124</b> via the example internal dictionary interface circuitry <b>1720</b>. The dictionary searching circuitry <b>1718</b> can obtain an example dictionary response <b>1804</b>. For example, the dictionary response <b>1804</b> can include an indication of a match <b>1806</b> or no-match <b>1808</b>. In some examples, the dictionary searching circuitry <b>1718</b> saves the match <b>1806</b> response to a final response <b>1810</b> and removes a matched barcode <b>110</b> from the list of unique barcodes.</p><p id="p-0220" num="0215">In some examples, the dictionary searching circuitry <b>1718</b> can generate a second search query that includes a product description for a second item of the list of items, a unique list of barcodes (e.g., with duplicates and/or matched barcodes removed), and the retailer ID. The dictionary searching circuitry <b>1718</b> can then search the second search query against the internal dictionary to obtain another dictionary response <b>1804</b>. In some examples, the dictionary searching circuitry <b>1718</b> can iterate this process until each item in the list of items has been searched.</p><p id="p-0221" num="0216">In some examples, the search manager circuitry <b>1714</b> transmits items of the list of items that received a no-match <b>1808</b> response from the internal dictionary <b>124</b> search to the products datastore searching circuitry <b>1722</b>. In some examples, the products datastore searching circuitry <b>1722</b> generates a first search query that includes a product description for a first un-matched item, a unique list of barcodes, and the retailer ID. The products datastore searching circuitry <b>1722</b> can then search the first search query against the products datastore <b>126</b> via the example products datastore interface circuitry <b>1724</b>. For example, the products datastore searching circuitry <b>1722</b> can look up the barcodes in the products datastore <b>126</b> and compare corresponding products datastore <b>126</b> descriptions with the product description extracted by the extraction circuitry <b>118</b>.</p><p id="p-0222" num="0217">In some examples, the products datastore searching circuitry <b>1722</b> obtains an example products datastore response <b>1804</b> that can include an indication of a match <b>1812</b> or no-match <b>1808</b>. In some examples, the products datastore searching circuitry <b>1722</b> saves the match <b>1806</b> response to the final response <b>1810</b> and removes a matched barcode <b>110</b> from the list of unique barcodes. If the products datastore response <b>1804</b> includes a no-match <b>1808</b> response, the products datastore searching circuitry <b>1722</b> can remove the retailer ID from the first search query and re-search the products datastore <b>126</b>. A match <b>1806</b> response can be saved to the final response <b>1810</b>. In some examples, the products datastore searching circuitry <b>1722</b> can iterate this process until each item in the list of no-match items has been twice searched (e.g., with the retailer ID and without the retailer ID).</p><p id="p-0223" num="0218">In some examples, the search manager circuitry <b>1714</b> transmits items of the list that received a no-match <b>1808</b> response from the products datastore <b>126</b> search to the previous jobs searching circuitry <b>1726</b>. In some examples, the previous jobs searching circuitry <b>1726</b> generates a search query that includes any un-matched product descriptions, any unmatched barcodes, and the retailer ID. In some examples, the search query includes a condition to obtain an intersection of the product descriptions and barcodes between the current basket and past baskets. The previous jobs searching circuitry <b>1726</b> can search the search query against the previous jobs database <b>130</b>. In some examples, the previous jobs searching circuitry <b>1726</b> obtains an example previous jobs response <b>1814</b>. If one barcode and one receipt description are minimum matched (e.g., min-matched), the search manager circuitry <b>1714</b> may assign the barcode <b>110</b> to the item description and send the match <b>1806</b> to the final response.</p><p id="p-0224" num="0219">In some examples, if only one barcode and one receipt description are left un-matched, the search manager circuitry <b>1714</b> associates the barcode and receipt description. In some such examples, all items of the receipt are recognized. Once the recognized items are formatted and aggregated to the final response, they can be transmitted to example report generating circuitry <b>134</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. Further, the example updating circuitry <b>1730</b> can add matched item descriptions and barcodes <b>110</b> to the internal dictionary <b>124</b>.</p><p id="p-0225" num="0220"><figref idref="DRAWINGS">FIG. <b>19</b></figref> illustrates an example search query <b>1900</b> generated by the example dictionary searching circuitry <b>1718</b> to search the example internal dictionary <b>124</b>. The search query <b>1900</b> includes a request <b>1902</b>, which corresponds to an input (e.g., input <b>1802</b>). Accordingly, the search query <b>1900</b> includes a list of unique barcodes <b>1904</b>, a store ID <b>1906</b>, and an example product description <b>1908</b> as extracted by the extraction circuitry <b>118</b>. The search query <b>1900</b> also include example conditions <b>1910</b>. In the illustrated example of <figref idref="DRAWINGS">FIG. <b>19</b></figref>, the conditions include (1) a result should match at least one barcode, (2) the result should match the store ID <b>1906</b>, and (3) the results should match the receipt description exactly. However, additional or alternative conditions can be used in additional or alternative examples. In some examples, as in the illustrated example of <figref idref="DRAWINGS">FIG. <b>19</b></figref>, the results can be sorted by a score (e.g., a similarity score) and a counter (e.g., a number of time the same record added to the internal dictionary <b>124</b>).</p><p id="p-0226" num="0221">In the illustrated example of <figref idref="DRAWINGS">FIG. <b>19</b></figref>, the dictionary searching circuitry <b>1718</b> searches the search query <b>1900</b> against the internal dictionary <b>124</b> to obtain a dictionary response <b>1912</b>. In some examples, the dictionary response <b>1912</b> can include several matches and/or suggestions. In some such examples, the search manager circuitry <b>1714</b> can select one result based on the score returned by the search query for each result and/or based on the counter for each suggested entry. In some examples, the dictionary response <b>1912</b> can include a single matched entry. In some such examples, the search manager circuitry <b>1714</b> determines the single match as a match and associates the product description and matched barcode. In some examples, the dictionary response <b>1912</b> can include no matches at all. In some such examples, the search manager circuitry <b>1714</b> can mark the item description as unrecognized and add the item description to a list of unrecognized items that will be used to search the products datastore <b>126</b>.</p><p id="p-0227" num="0222"><figref idref="DRAWINGS">FIG. <b>20</b></figref> illustrates an example search query <b>2000</b> generated by the example products datastore searching circuitry <b>1722</b> to search the example products datastore <b>126</b>. The search query <b>2000</b> includes an example product description region <b>2002</b>, an example store ID region <b>2004</b>, and an example a barcodes region <b>2006</b> that includes a list of unique barcodes. The product description region <b>2002</b> includes example conditions <b>2008</b>. In some examples, the conditions <b>2008</b> create a complex query that contributes to scoring of each candidate result. For example, the conditions <b>2008</b> can include conditions for a plurality of similar matches such as n-gram matches, tokenized matches, etc. In some examples, as in the illustrated example of <figref idref="DRAWINGS">FIG. <b>20</b></figref>, the results can be filtered by barcodes (e.g., based on a similarity score). Below is an example search query <b>2000</b> that includes a store ID:</p><p id="p-0228" num="0000"><tables id="TABLE-US-00004" num="00004"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="offset" colwidth="7pt" align="left"/><colspec colname="1" colwidth="210pt" align="left"/><thead><row><entry/><entry namest="offset" nameend="1" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/><entry>{</entry></row><row><entry/><entry>&#x2003;&#x2032;size&#x2032;: 5,</entry></row><row><entry/><entry>&#x2003;&#x2032;explain&#x2032;: False,</entry></row><row><entry/><entry>&#x2003;&#x2032;query&#x2032;: {</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2032;bool&#x2032;: {</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2032;filter&#x2032;: { &#x2032;terms&#x2032;: { &#x2032;ean&#x2032;: [&#x2032;UPC1&#x2032;, &#x2032;UPC2&#x2032;, ... &#x2032;UPCN&#x2032;, ]&#x2003;} },</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2032;must&#x2032;: [{</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2032;bool&#x2032;: {</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2032;should&#x2032;: [{</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2032;match&#x2032;: {</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2032;description.keyword&#x2032;: {</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2032;query&#x2032;: &#x2032;RECEIPT DESCRIPTION&#x2032;,</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2032;fuzziness&#x2032;: 2,</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2032;boost&#x2032;: 2</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;}</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;}</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;}, {</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2032;match&#x2032;: {</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2032;description.lowercase_keyword&#x2032;: {</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2032;query&#x2032;: &#x2032;RECEIPT DESCRIPTION&#x2032;,</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2032;fuzziness&#x2032;: 2,</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2032;boost&#x2032;: 1</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;}</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;}</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;}, {</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2032;match&#x2032;: {</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2032;description.lowercase_text&#x2032;: {</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2032;query&#x2032;: &#x2032;RECEIPT DESCRIPTION&#x2032;,</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2032;fuzziness&#x2032;: 2,</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2032;boost&#x2032;: 1</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;}</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;}</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;}, {</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2032;match&#x2032;: {</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2032;description.ngram&#x2032;: {</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2032;query&#x2032;: &#x2032;RECEIPT DESCRIPTION&#x2032;,</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2032;fuzziness&#x2032;: 2,</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2032;boost&#x2032;: 1</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;}</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;}</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;}, {</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2032;match&#x2032;: {</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2032;description&#x2032;: {</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2032;query&#x2032;: &#x2032;RECEIPT DESCRIPTION&#x2032;,</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2032;fuzziness&#x2032;: 2,</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2032;boost&#x2032;: 1</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;}</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;}</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;}, {</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2032;match&#x2032;: {</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2032;description&#x2032;: {</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2032;query&#x2032;: &#x2032;RECEIPT DESCRIPTION tokenized&#x2032;,</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2032;fuzziness&#x2032;: 2,</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2032;boost&#x2032;: 1</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;}</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;}</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;}, {</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2032;match&#x2032;: {</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2032;description&#x2032;: {</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2032;query&#x2032;: &#x2032;RECEIPT DESC. without special chars&#x2032;,</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2032;fuzziness&#x2032;: 2,</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2032;boost&#x2032;: 1</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;}</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;}</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;}</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;],</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2032;minimum_should_match&#x2032;: 1</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;}</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;}, {</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2032;bool&#x2032;: { &#x2032;filter&#x2032;: { &#x2032;terms&#x2032;: { &#x2032;storecode&#x2032;: [&#x2032;STORE_CODE&#x2032;,</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2032;all&#x2032;] } } }</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;}, {</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2032;terms&#x2032;: { &#x2032;type&#x2032;: [&#x2032;ean&#x2032;, &#x2032;upc&#x2032;, &#x2032;rac&#x2032;,&#x2019;bvw&#x2019;,&#x2018;pzn&#x2019;] }</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;}</entry></row><row><entry/><entry>&#x2003;&#x2003;], // close must section</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2032;should&#x2032;: [{</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2032;range&#x2032;: {</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2032;price&#x2032;: {</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2032;gte&#x2032;: 0.80 * ITEM_PRICE,</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2032;lte&#x2032;: 1.20 * ITEM_PRICE,</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2032;boost&#x2019;&#x2019;&#x2032;: 1</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;}</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2003;}</entry></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;} ]</entry></row><row><entry/><entry>&#x2003;&#x2003;}</entry></row><row><entry/><entry>&#x2003;}</entry></row><row><entry/><entry>}</entry></row><row><entry/><entry namest="offset" nameend="1" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0229" num="0223">In some examples, the store ID region <b>2004</b> can be removed from the search query <b>2000</b>. In some examples, the products datastore search circuitry <b>1722</b> can remove RAC type barcodes because these are retailer specific codes that are not targeted when searching without store codes. Below is an example search query <b>2000</b> without a store ID:</p><p id="p-0230" num="0000"><tables id="TABLE-US-00005" num="00005"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="left"/><thead><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry>{</entry></row><row><entry>&#x2003;&#x2018;size&#x2019;: 5,</entry></row><row><entry>&#x2003;&#x2018;explain&#x2019;: False,</entry></row><row><entry>&#x2003;&#x2018;query&#x2019;: {</entry></row><row><entry>&#x2003;&#x2003;&#x2018;bool&#x2019;: {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2018;filter&#x2019;: { &#x2018;terms&#x2019;: { &#x2018;ean&#x2019;: [&#x2018;UPC1&#x2019;, &#x2018;UPC2&#x2019;, ... &#x2018;UPCN&#x2019;, ]&#x2003;} },</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2018;must&#x2019;: [{</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2018;bool&#x2019;: {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2018;should&#x2019;: [{</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2018;match&#x2019;: {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2018;description.keyword&#x2019;: {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2018;query&#x2019;: &#x2018;RECEIPT DESCRIPTION&#x2019;,</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2018;fuzziness&#x2019;: 2,</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2018;boost&#x2019;: 2</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;}, {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2018;match&#x2019;: {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2018;description.lowercase_keyword&#x2019;: {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2018;query&#x2019;: &#x2018;RECEIPT DESCRIPTION&#x2019;,</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2018;fuzziness&#x2019;: 2,</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2018;boost&#x2019;: 1</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;},{</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2018;match&#x2019;: {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2018;description.lowercase_text&#x2019;: {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2018;query&#x2019;: &#x2018;RECEIPT DESCRIPTION&#x2019;,</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2018;fuzziness': 2,</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2018;boost&#x2019;: 1</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;}, {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2018;match&#x2019;: {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2018;description.ngram&#x2019;: {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2018;query&#x2019;: &#x2018;RECEIPT DESCRIPTION&#x2019;,</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2018;fuzziness&#x2019;: 2,</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2018;boost&#x2019;: 1</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;}, {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2018;match&#x2019;: {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2018;description&#x2019;: {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2018;query&#x2019;: &#x2018;RECEIPT DESCRIPTION&#x2019;,</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2018;fuzziness&#x2019;: 2,</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2018;boost&#x2019;: 1</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;}, {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2018;match&#x2019;: {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2018;description&#x2019;: {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2018;query&#x2019;: &#x2018;RECEIPT DESCRIPTION tokenized&#x2019;, &#x2018;</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;fuzziness&#x2019;: 2,</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2018;boost&#x2019;: 1</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;}, {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2018;match&#x2019;: {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2018;description&#x2019;: {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2018;query&#x2019;: &#x2018;RECEIPT DESC. without special chars&#x2019;,</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2018;fuzziness&#x2019;: 2,</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2018;boost&#x2019;: 1</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;],</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x201c;minimum should match&#x201d;: 1,</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x201c;must&#x201d;: [ { &#x201c;terms&#x201d;: {&#x201c;type&#x201d;: [ &#x201c;ean&#x201d;,&#x201c;upc&#x201d;,&#x201c;bvw&#x201d;,&#x201c;pzn&#x201d;] }</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;} ]</entry></row><row><entry>&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;}</entry></row><row><entry>}</entry></row><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0231" num="0224"><figref idref="DRAWINGS">FIG. <b>21</b></figref> illustrates an example matrix <b>2100</b> including example matching scores <b>2102</b> for pairs of product descriptions and barcodes and a list <b>2104</b> of the identified matches that can be output by the example products datastore searching circuitry <b>1722</b>. For example, the matrix <b>2100</b> can be used to resolve matching between the barcodes and descriptions. The example matrix <b>2100</b> of <figref idref="DRAWINGS">FIG. <b>21</b></figref> includes a row <b>2106</b> having receipt descriptions, a column <b>2108</b> having barcodes, and cells <b>2110</b> that include values with scores for each pair (row, column).</p><p id="p-0232" num="0225">The products datastore searching circuitry <b>1722</b> generates a search query using the list of unique barcodes and one item description detected from the receipt. As a result, the products datastore searching circuitry <b>1722</b> fills one row of the matrix on each query. In some examples, the products datastore searching circuitry <b>1722</b> matches a barcode with an item description based on a highest score above a threshold. For example, the threshold can be a score of at least <b>90</b>. However, the threshold can be high or lower is additional or alternative examples. Thus, the products datastore searching circuitry <b>1722</b> can identify a match between a barcode and item description that receive the highest score above a threshold. In the illustrated example of <figref idref="DRAWINGS">FIG. <b>21</b></figref>, this process can resulted in a single match scenario. That is, the &#x201c;Sieria Toilet wC-paperi 8rl kelt&#x201d; did not receive a score above the threshold value of 90. However, the other product descriptions did receive scores above the threshold value. Accordingly, each product description searched by the products datastore searching circuitry <b>1722</b> received a matched barcode. The results are illustrated in the list <b>2104</b> of the identified matches</p><p id="p-0233" num="0226"><figref idref="DRAWINGS">FIG. <b>22</b></figref> is a block diagram of example framework to process products having variable weights. In some examples, the framework is implemented by the example document decoding circuitry <b>114</b>. In some examples, there are two main types of Variable Weighted Products including Variable Weight Products (VWP) and Branded Variable Weight Products (BVWP). In some examples, VWPs and BVWPs barcode's are treated differently inside a data source, such as the products datastore <b>126</b>. In some examples, the decoding circuitry <b>122</b> can rely on the products datastore <b>126</b> to perform product matching and, thus, the decoding circuitry <b>122</b> can be structured to implement specific rules to correctly handle these special barcodes.</p><p id="p-0234" num="0227">In some examples, the VWPs can be codebook items that may have special characteristics (e.g. organic banana vs non-organic, etc.). In some examples, the products datastore <b>126</b> stores this type of product with an identifier made of the main barcode plus concatenated characteristics. However, in some examples, rules for the concatenation are defined inside by a system that is external to the products datastore <b>126</b>. In some examples, an application a panelist used to upload basket information can implement a set of pop-ups for the panelist to pick the characteristics that apply to the purchased VWP. However, in some examples, the application does not concatenate the characteristics to produce the final barcode. Thus, in some examples, the document decode circuity <b>114</b> needs to replicate the external system's rules to find the correct product in the products datastore <b>126</b>. In some examples, the rules can vary by country.</p><p id="p-0235" num="0228">The framework of <figref idref="DRAWINGS">FIG. <b>22</b></figref> includes an example codebook <b>2202</b>, which is structured to store barcodes for a plurality of products. The example codebook <b>2202</b> of the illustrated example of <figref idref="DRAWINGS">FIG. <b>22</b></figref> is implemented by any memor(ies), storage device(s) and/or storage disc(s) for storing data such as, for example, flash memory, magnetic media, optical media, etc. Furthermore, the data stored in the codebook <b>2202</b> may be in any data format such as, for example, binary data, comma delimited data, tab delimited data, structured query language (SQL) structures, image data, etc.</p><p id="p-0236" num="0229">In some examples, the framework includes example modifying circuitry <b>2204</b>, which is structured to modify a barcode based on rules. For example, a panelist may use the codebook <b>2202</b> to select an example item <b>2206</b> and an example special character <b>2208</b>. The modifying circuitry <b>2204</b> can be used to modify the barcode by applying the rules. Thus, in some examples, the modifying circuitry <b>2204</b> includes an example rule(s) <b>2210</b>. In some examples, the rules are stored in the application. In some examples, the rules are stored in a database, such as the database <b>120</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. In some examples, a first rule <b>2210</b> can be designated for a barcode that does not include characteristics. For example, the rule <b>2210</b> can be to add double zero at the end of the code. In some examples, a check digit can be calculated and attached to the end of the barcode. The check digit is used to verify the VWP. In some examples, a second rule <b>2210</b> can be designated for a barcode that includes a single characteristic. For example, the rule <b>2210</b> can be to add a zero at the end of the barcode, concatenate with the digits of the characteristic from position <b>14</b> of the barcode, and add a check digit. In some examples, a third rule <b>2210</b> can be designated for a barcode that includes several characteristics. For example, the rule <b>2210</b> can be to concatenate with the digits of the characteristics from position <b>14</b> of the barcode and add the check digit. Below is an example of a codebook prefix and the rules as applied for no characteristics, a single characteristic, and several characteristics:</p><p id="p-0237" num="0000"><tables id="TABLE-US-00006" num="00006"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="left"/><thead><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry>Example:</entry></row><row><entry>&#x2003;codebook barcode prefix = [&#x2018;6438161&#x2019;]</entry></row><row><entry>&#x2003;codebook start position = 13</entry></row><row><entry>&#x2003;codebook concatenate digit = 1</entry></row><row><entry>No characteristics:</entry></row><row><entry>&#x2003;UPC: 6438161001</entry></row><row><entry>&#x2003;&#x2003;6438161001 + 0 + 0 =&#x3e; 643816100100</entry></row><row><entry>&#x2003;calculated check digit is 2 =&#x3e; 6438161001002</entry></row><row><entry>Single characteristic:</entry></row><row><entry>&#x2003;UPC: 6438161001,</entry></row><row><entry>&#x2003;Characteristics: [{&#x201c;id&#x201d;: &#x201c;01&#x201d;,&#x201c;selectionNumber&#x201d;: &#x201c;00000000010104&#x201d;}]</entry></row><row><entry>&#x2003;6438161001 + 0 + 4 =643816100104</entry></row><row><entry>&#x2003;calculated check digit is 0 =&#x3e; 6438161001040</entry></row><row><entry>Several characteristics:</entry></row><row><entry>&#x2003;UPC: 6438161001,</entry></row><row><entry>&#x2003;Characteristics: [{&#x201c;id&#x201d;: &#x201c;01&#x201d;,&#x201c;selectionNumber&#x201d;: &#x201c;00000000010104&#x201d;},</entry></row><row><entry>&#x2003;&#x2003;{&#x201c;id&#x201d;: &#x201c;02&#x201d;,&#x201c;selectionNumber&#x201d;: &#x201c;00000000010103&#x201d;}]</entry></row><row><entry>&#x2003;6438161001+ 4 + 3 =643816100143</entry></row><row><entry>&#x2003;calculated check digit is 9 =&#x3e; 6438161001439</entry></row><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0238" num="0230">In some examples, an example modified barcode <b>2212</b> (e.g., as it is expected to appear in the products datastore <b>126</b>) can also be stored in the example internal dictionary <b>124</b> when the item is recognized. In some such examples, the internal dictionary <b>124</b> search can output a dictionary result that includes the original barcode as provided without any replacement of digits.</p><p id="p-0239" num="0231">In some examples, a BVWP barcode as input into the application by the panelist can include a barcode with a length of 13 digits and a prefix digit of &#x201c;23&#x201d;. The decoding circuitry <b>122</b> can identify digits from position <b>8</b> to position <b>12</b> as the variable weight of the purchase product. In some examples, before using this barcode for searching the products datastore <b>126</b>, the decoding circuitry <b>122</b> can replace the weight digits with zeros, which can be a format used by the monitoring entity <b>102</b> when this type of product is incorporated into the products datastore <b>126</b>. In some examples, a last digit can be a check digit for consistency purposes that is calculated using the other positions from 1 to 12. Therefore, when replacing the zeroes, the decoding circuitry <b>122</b> may need to recalculate the check digit. For example, an example BVWP uploaded by a panelist can be 2396274906008. After replacing the weight and recalculating the checked digit, the modified barcode can be 2396274900000. In some examples, a modified barcode (e.g., as it is expected to appear in the products datastore <b>126</b>) can also be stored in the example internal dictionary <b>124</b> when the item is recognized. In some such examples, the internal dictionary <b>124</b> search can output a dictionary result that includes the original barcode as provided without any replacement of digits.</p><p id="p-0240" num="0232"><figref idref="DRAWINGS">FIG. <b>23</b></figref> illustrates another example search query <b>2300</b> generated by the example previous jobs searching circuitry <b>1726</b> to search the example previous jobs database <b>130</b>. In some examples, the search query <b>2300</b> can be used to search the previous jobs database <b>130</b> and to upload un-matched product descriptions and barcodes to the previous jobs database <b>130</b>. The search query <b>2300</b> includes an example combination region <b>2302</b> into which the previous jobs searching circuitry <b>1726</b> inputs the un-matched products descriptions and barcodes. The search query <b>2300</b> also includes an example store ID region <b>2304</b> and an example conditions regions <b>2306</b>. In some examples, the conditions <b>2306</b> include a minimum matching condition. For example, the minimum matching condition can be used such that during the search, a result that includes a match between at least one product description and barcode can be returned.</p><p id="p-0241" num="0233">In the illustrated example of <figref idref="DRAWINGS">FIG. <b>23</b></figref>, example results region <b>2308</b> can compare an example previous job <b>2310</b> to the current job <b>2312</b>. A match can be determine based on at least one common product description and one common barcode. In some examples, the matched product descriptions and barcodes can be added to a final response and un-matched product descriptions and barcodes can be transmitted to an example upload section <b>2314</b> to be uploaded to the previous jobs database <b>130</b>.</p><p id="p-0242" num="0234"><figref idref="DRAWINGS">FIG. <b>24</b></figref> is a block diagram of another example implementation of the example extraction circuitry <b>118</b> of <figref idref="DRAWINGS">FIGS. <b>1</b>, <b>2</b></figref>, and/or <b>3</b>. <figref idref="DRAWINGS">FIG. <b>24</b></figref> illustrates example steps applied to a receipt image <b>108</b> that is to be processed by example document decode circuitry <b>114</b> and/or another DDS. The example extracting circuitry <b>118</b> of <figref idref="DRAWINGS">FIG. <b>24</b></figref> includes example extraction flow determining circuitry <b>2402</b>, which is structured to identify a path on which to send the receipt image <b>108</b>. In the illustrated example of <figref idref="DRAWINGS">FIG. <b>24</b></figref>, there are two possible paths (e.g., scenarios, etc.), including a &#x201c;receipt only&#x201d; flow <b>2404</b> and a &#x201c;receipt and barcode(s)&#x201d; flow <b>2406</b>. In some examples, the path on which the receipt image <b>108</b> should be sent to is determined based on which information the panelist uploaded, such as the receipt image <b>108</b> and metadata or the receipt image <b>108</b>, metadata, and barcodes <b>110</b>. In some examples, the receipt only flow <b>2404</b> can be used when barcodes <b>110</b> are unavailable. For example, the panelist may not be asked to provide barcodes <b>110</b> to save a burden (e.g., time, work, etc.) of the panelist.</p><p id="p-0243" num="0235">In the illustrated example of <figref idref="DRAWINGS">FIG. <b>24</b></figref>, the receipt image <b>108</b> initially takes the same path in both the receipt only flow <b>2404</b> and the receipt and barcodes flow <b>2406</b>. That is, the extraction circuitry <b>118</b> applies the same or similar technique to a receipt only and/or a receipt and barcodes upload. For example, the regions detection circuitry <b>304</b> can detect regions of interest on either receipt image <b>108</b>. In some examples, the extraction circuitry <b>118</b> can include example overlapping region removal circuitry <b>2408</b>, which is structured to remove overlapping regions (e.g., overlapping bounding boxes corresponding to products). For example, the overlapping region removal circuitry <b>2408</b> may be applied to a receipt image <b>108</b> that was uploaded in multiple parts (e.g., for a long receipt). In some examples, other post-processing steps can be applied by example post-processing circuitry <b>2410</b>.</p><p id="p-0244" num="0236"><figref idref="DRAWINGS">FIG. <b>25</b></figref> is a block diagram of another example implementation of the example decoding circuitry of <figref idref="DRAWINGS">FIG. <b>1</b>, <b>2</b></figref>, and/or <b>17</b>. <figref idref="DRAWINGS">FIG. <b>25</b></figref> illustrates example primary steps applied to a basket that is to be processed by example document decode circuitry <b>114</b> and/or another DDS. The example decoding circuitry <b>122</b> of <figref idref="DRAWINGS">FIG. <b>25</b></figref> includes example decoding flow determining circuitry <b>2500</b>, which is structured to identify a path on which to send the basket. In the illustrated example of <figref idref="DRAWINGS">FIG. <b>25</b></figref>, there are two possible paths (e.g., scenarios, etc.), including a &#x201c;receipt only&#x201d; flow <b>2502</b> and a &#x201c;receipt and barcode(s)&#x201d; flow <b>2504</b>. In some examples, the path on which the basket should be sent to is determined based on which basket information the panelist uploaded, such as the receipt image <b>108</b> and metadata or the receipt image <b>108</b>, metadata, and barcodes <b>110</b>.</p><p id="p-0245" num="0237">In receipt only flow <b>2502</b>, the decoding circuitry <b>122</b> is structured to use purchase information obtained from the extraction circuitry <b>118</b> and other metadata provided by the panelist. In some examples, the receipt only flow <b>2502</b> can be used when barcodes <b>110</b> are unavailable. For example, the panelist may not be asked to provide barcodes <b>110</b> to save a burden (e.g., time, work, etc.) of the panelist. In some examples, the receipt only flow <b>2502</b> is not ideal because an available dictionary (e.g., internal dictionary <b>124</b>) does not include a sufficient amount of receipt descriptions mapped to barcodes. In the receipt and barcodes flow <b>2504</b>, in addition to the purchase information and metadata, the decoding circuitry <b>122</b> can receive a list of barcodes <b>110</b> (e.g., scanned barcodes and/or manually selected codebook barcodes) from the panelist.</p><p id="p-0246" num="0238">In the illustrated example of <figref idref="DRAWINGS">FIG. <b>25</b></figref>, baskets are searched against the internal dictionary <b>124</b> using the dictionary searching circuitry <b>1718</b> first, regardless of whether the basket takes the receipt only flow <b>2502</b> or the receipt and barcodes flow <b>2504</b>. In the receipt only flow <b>2502</b>, the internal dictionary <b>124</b> search can yield multiple matches <b>2506</b>, a single match <b>2508</b>, or no matches <b>2510</b>. The receipt and barcode flow <b>2504</b> internal dictionary <b>124</b> search can yield the single match <b>2508</b> or the no match <b>2510</b>. In either flow, the multiple match <b>2508</b> response and the single match response <b>2508</b> can be transmitted to the example response generating circuitry <b>1732</b> to be saved to a final response.</p><p id="p-0247" num="0239">In either flow, product descriptions and barcodes that received the no-match response <b>2510</b> can be searched against the products datastore <b>126</b> using the products datastore searching circuitry <b>1722</b>. However, algorithms applied to generate the search query can be different depending on which path the basket took. For example, the products datastore searching circuitry <b>1722</b> can generate the search query based on an example receipt only algorithm <b>2512</b> for the receipt only path <b>2502</b>. Further, the products datastore searching circuitry <b>1722</b> can generate the search query based on an example receipt and barcode algorithm <b>2514</b> for the receipt and barcodes path <b>2504</b>.</p><p id="p-0248" num="0240">In the illustrated example of <figref idref="DRAWINGS">FIG. <b>25</b></figref>, either flow can results in a recognized element <b>2516</b> or an unrecognized element <b>2518</b> for the search query. In some examples, the recognized element <b>2516</b> can be transmitted to the updating circuitry <b>1730</b> to update the internal dictionary <b>124</b>. In some examples, the recognized element <b>2516</b> can also be transmitted to the response generating circuitry <b>1732</b> to be added to the final response. Further, in either flow, the unrecognized element <b>2518</b> can be transmitted to the previous jobs searching circuitry <b>1726</b> to search the previous jobs database <b>130</b> and/or to upload the unrecognized element <b>2518</b> to the previous jobs database <b>130</b>.</p><p id="p-0249" num="0241">Flowcharts representative of example hardware logic circuitry, machine readable instructions, hardware implemented state machines, and/or any combination thereof for implementing the document decoding circuitry <figref idref="DRAWINGS">FIGS. <b>1</b>, <b>2</b>, <b>3</b>, <b>9</b>, <b>10</b>, <b>17</b>, <b>24</b></figref>, and/or <b>25</b> is shown in <figref idref="DRAWINGS">FIGS. <b>26</b>-<b>34</b></figref>. The machine readable instructions may be one or more executable programs or portion(s) of an executable program for execution by processor circuitry, such as the processor circuitry <b>3512</b> shown in the example processor platform <b>3500</b> discussed below in connection with <figref idref="DRAWINGS">FIG. <b>35</b></figref> and/or the example processor circuitry discussed below in connection with <figref idref="DRAWINGS">FIGS. <b>36</b> and/or <b>37</b></figref>. The program may be embodied in software stored on one or more non-transitory computer readable storage media such as a compact disk (CD), a floppy disk, a hard disk drive (HDD), a solid-state drive (SSD), a digital versatile disk (DVD), a Blu-ray disk, a volatile memory (e.g., Random Access Memory (RAM) of any type, etc.), or a non-volatile memory (e.g., electrically erasable programmable read-only memory (EEPROM), FLASH memory, an HDD, an SSD, etc.) associated with processor circuitry located in one or more hardware devices, but the entire program and/or parts thereof could alternatively be executed by one or more hardware devices other than the processor circuitry and/or embodied in firmware or dedicated hardware. The machine readable instructions may be distributed across multiple hardware devices and/or executed by two or more hardware devices (e.g., a server and a client hardware device). For example, the client hardware device may be implemented by an endpoint client hardware device (e.g., a hardware device associated with a user) or an intermediate client hardware device (e.g., a radio access network (RAN)) gateway that may facilitate communication between a server and an endpoint client hardware device). Similarly, the non-transitory computer readable storage media may include one or more mediums located in one or more hardware devices. Further, although the example program is described with reference to the flowcharts illustrated in <figref idref="DRAWINGS">FIGS. <b>26</b>-<b>34</b></figref>, many other methods of implementing the example document decode circuitry <b>114</b> may alternatively be used. For example, the order of execution of the blocks may be changed, and/or some of the blocks described may be changed, eliminated, or combined. Additionally or alternatively, any or all of the blocks may be implemented by one or more hardware circuits (e.g., processor circuitry, discrete and/or integrated analog and/or digital circuitry, an FPGA, an ASIC, a comparator, an operational-amplifier (op-amp), a logic circuit, etc.) structured to perform the corresponding operation without executing software or firmware. The processor circuitry may be distributed in different network locations and/or local to one or more hardware devices (e.g., a single-core processor (e.g., a single core central processor unit (CPU)), a multi-core processor (e.g., a multi-core CPU), etc.) in a single machine, multiple processors distributed across multiple servers of a server rack, multiple processors distributed across one or more server racks, a CPU and/or a FPGA located in the same package (e.g., the same integrated circuit (IC) package or in two or more separate housings, etc.).</p><p id="p-0250" num="0242">The machine readable instructions described herein may be stored in one or more of a compressed format, an encrypted format, a fragmented format, a compiled format, an executable format, a packaged format, etc. Machine readable instructions as described herein may be stored as data or a data structure (e.g., as portions of instructions, code, representations of code, etc.) that may be utilized to create, manufacture, and/or produce machine executable instructions. For example, the machine readable instructions may be fragmented and stored on one or more storage devices and/or computing devices (e.g., servers) located at the same or different locations of a network or collection of networks (e.g., in the cloud, in edge devices, etc.). The machine readable instructions may require one or more of installation, modification, adaptation, updating, combining, supplementing, configuring, decryption, decompression, unpacking, distribution, reassignment, compilation, etc., in order to make them directly readable, interpretable, and/or executable by a computing device and/or other machine. For example, the machine readable instructions may be stored in multiple parts, which are individually compressed, encrypted, and/or stored on separate computing devices, wherein the parts when decrypted, decompressed, and/or combined form a set of machine executable instructions that implement one or more operations that may together form a program such as that described herein.</p><p id="p-0251" num="0243">In another example, the machine readable instructions may be stored in a state in which they may be read by processor circuitry, but require addition of a library (e.g., a dynamic link library (DLL)), a software development kit (SDK), an application programming interface (API), etc., in order to execute the machine readable instructions on a particular computing device or other device. In another example, the machine readable instructions may need to be configured (e.g., settings stored, data input, network addresses recorded, etc.) before the machine readable instructions and/or the corresponding program(s) can be executed in whole or in part. Thus, machine readable media, as used herein, may include machine readable instructions and/or program(s) regardless of the particular format or state of the machine readable instructions and/or program(s) when stored or otherwise at rest or in transit.</p><p id="p-0252" num="0244">The machine readable instructions described herein can be represented by any past, present, or future instruction language, scripting language, programming language, etc. For example, the machine readable instructions may be represented using any of the following languages: C, C++, Java, C#, Perl, Python, JavaScript, HyperText Markup Language (HTML), Structured Query Language (SQL), Swift, etc.</p><p id="p-0253" num="0245">As mentioned above, the example operations of <figref idref="DRAWINGS">FIGS. <b>26</b>-<b>34</b></figref> may be implemented using executable instructions (e.g., computer and/or machine readable instructions) stored on one or more non-transitory computer and/or machine readable media such as optical storage devices, magnetic storage devices, an HDD, a flash memory, a read-only memory (ROM), a CD, a DVD, a cache, a RAM of any type, a register, and/or any other storage device or storage disk in which information is stored for any duration (e.g., for extended time periods, permanently, for brief instances, for temporarily buffering, and/or for caching of the information). As used herein, the terms non-transitory computer readable medium and non-transitory computer readable storage medium are expressly defined to include any type of computer readable storage device and/or storage disk and to exclude propagating signals and to exclude transmission media.</p><p id="p-0254" num="0246">&#x201c;Including&#x201d; and &#x201c;comprising&#x201d; (and all forms and tenses thereof) are used herein to be open ended terms. Thus, whenever a claim employs any form of &#x201c;include&#x201d; or &#x201c;comprise&#x201d; (e.g., comprises, includes, comprising, including, having, etc.) as a preamble or within a claim recitation of any kind, it is to be understood that additional elements, terms, etc., may be present without falling outside the scope of the corresponding claim or recitation. As used herein, when the phrase &#x201c;at least&#x201d; is used as the transition term in, for example, a preamble of a claim, it is open-ended in the same manner as the term &#x201c;comprising&#x201d; and &#x201c;including&#x201d; are open ended. The term &#x201c;and/or&#x201d; when used, for example, in a form such as A, B, and/or C refers to any combination or subset of A, B, C such as (1) A alone, (2) B alone, (3) C alone, (4) A with B, (5) A with C, (6) B with C, or (7) A with B and with C. As used herein in the context of describing structures, components, items, objects and/or things, the phrase &#x201c;at least one of A and B&#x201d; is intended to refer to implementations including any of (1) at least one A, (2) at least one B, or (3) at least one A and at least one B. Similarly, as used herein in the context of describing structures, components, items, objects and/or things, the phrase &#x201c;at least one of A or B&#x201d; is intended to refer to implementations including any of (1) at least one A, (2) at least one B, or (3) at least one A and at least one B. As used herein in the context of describing the performance or execution of processes, instructions, actions, activities and/or steps, the phrase &#x201c;at least one of A and B&#x201d; is intended to refer to implementations including any of (1) at least one A, (2) at least one B, or (3) at least one A and at least one B. Similarly, as used herein in the context of describing the performance or execution of processes, instructions, actions, activities and/or steps, the phrase &#x201c;at least one of A or B&#x201d; is intended to refer to implementations including any of (1) at least one A, (2) at least one B, or (3) at least one A and at least one B.</p><p id="p-0255" num="0247">As used herein, singular references (e.g., &#x201c;a&#x201d;, &#x201c;an&#x201d;, &#x201c;first&#x201d;, &#x201c;second&#x201d;, etc.) do not exclude a plurality. The term &#x201c;a&#x201d; or &#x201c;an&#x201d; object, as used herein, refers to one or more of that object. The terms &#x201c;a&#x201d; (or &#x201c;an&#x201d;), &#x201c;one or more&#x201d;, and &#x201c;at least one&#x201d; are used interchangeably herein. Furthermore, although individually listed, a plurality of means, elements or method actions may be implemented by, e.g., the same entity or object. Additionally, although individual features may be included in different examples or claims, these may possibly be combined, and the inclusion in different examples or claims does not imply that a combination of features is not feasible and/or advantageous.</p><p id="p-0256" num="0248"><figref idref="DRAWINGS">FIG. <b>26</b></figref> is a flowchart representative of example machine readable instructions and/or example operations <b>2600</b> that may be executed and/or instantiated by processor circuitry to train document decode models. The machine readable instructions and/or the operations <b>2600</b> of <figref idref="DRAWINGS">FIG. <b>26</b></figref> begin at block <b>2602</b>, at which the example machine learning circuity <b>136</b> receives example training images (e.g., training images <b>1002</b>) and example ground truth annotations (e.g., ground truth annotations <b>1006</b>). For example, the machine learning circuitry <b>136</b> can receive and/or retrieve the training images <b>1002</b> and ground truth annotations <b>1006</b> from an example database (e.g., database <b>120</b>). In some examples, the training images <b>1002</b> and ground truth annotations <b>1006</b> can correspond to pairs of trainings images in which one image is an original image and the other image is a labeled image.</p><p id="p-0257" num="0249">At block <b>2604</b>, example regions detection model training circuitry <b>1004</b> trains an example regions detection model (e.g., regions detection model <b>306</b>) to detect regions of interest. For example, the regions detection model training circuitry <b>1004</b> trains the regions detection model <b>306</b> to detect a receipt region and a column region. In some examples, the regions detection model training circuitry <b>1004</b> trains the regions detection model <b>306</b> using a faster R-CNN architecture. In some examples, the regions detection model training circuitry <b>1004</b> trains the regions detection model <b>306</b> using the training images <b>1002</b> and determines a performance of the trained regions detection model <b>306</b> using the ground truth annotations. For example, the ground truth annotations <b>1006</b> can include labeled receipt regions and labeled products regions corresponding to the training images <b>1002</b>.</p><p id="p-0258" num="0250">At block <b>2606</b>, example cropping circuitry (e.g., cropping circuitry <b>1008</b>) crops the training images <b>1002</b> based on the regions of interest using the ground truth annotations <b>1006</b>. In some examples, the cropping circuitry <b>1008</b> can crop the training images <b>1002</b> using the receipt regions in the ground truth annotations <b>1006</b> to generate example row training images (e.g., row training images <b>1010</b>). In some examples, the cropping circuitry <b>1008</b> can crop the training images <b>1002</b> using the products regions in the ground truth annotations <b>1006</b> to generate example column training images (e.g., column training images <b>1014</b>).</p><p id="p-0259" num="0251">At block <b>2608</b>, example row training circuitry (e.g., row detection model training circuitry <b>1012</b>) trains an example row detection model (e.g., row segmentation model(s) <b>314</b>). For example, the row detection model training circuitry <b>1012</b> trains the row segmentation model <b>314</b> to classify each pixel in an image as belonging to a row class or a background class. In some examples, the row detection model training circuitry <b>1012</b> trains the row segmentation model <b>314</b> using an example dhSegment architecture.</p><p id="p-0260" num="0252">At block <b>2610</b>, example column detection model training circuitry (e.g., column detection model training circuitry <b>1016</b>) trains an example column segmentation model (e.g., column segmentation model(s) <b>320</b>). For example, the column detection model training circuitry <b>1016</b> trains the column segmentation model <b>320</b> to classify each pixel in an image as belonging to a column class or a background class. In some examples, the column detection model training circuitry <b>1016</b> trains the column segmentation model <b>320</b> using an example U-Net architecture.</p><p id="p-0261" num="0253">At block <b>2612</b>, the example machine learning circuitry <b>136</b> determines whether to perform additional training. For example, the machine learning circuitry <b>136</b> may decide to continue training the regions detection model(s) <b>306</b>, the row segmentation model(s) <b>314</b>, and/or the column segmentation model(s) <b>320</b>. In some examples, the machine learning circuitry <b>136</b> may decide to train additional or alternative regions detection model(s) <b>306</b>, row segmentation model(s) <b>314</b>, and/or column segmentation model(s) <b>320</b>. In some examples, the machine learning circuitry <b>136</b> may decide to train other models, such as models to structure a receipt layout, to store information in a database, etc. If the answer to block <b>2612</b> is YES, and the machine learning circuitry <b>136</b> decides to train more models, control advances back to block <b>2602</b> at which the machine learning circuitry <b>136</b> receives training images and ground truth annotations. If the answer to block <b>2612</b> is NO, and the machine learning circuitry <b>136</b> decides not to perform additional training, the model training circuitry <b>2614</b> saves the models (Block <b>2614</b>). For example, the machine learning circuitry <b>136</b> may save the models <b>306</b>, <b>314</b>, <b>320</b> to the example database <b>120</b> and/or another storage device (e.g., with a cloud service provider).</p><p id="p-0262" num="0254"><figref idref="DRAWINGS">FIG. <b>27</b></figref> is a flowchart representative of example machine readable instructions and/or example operations <b>2700</b> that may be executed and/or instantiated by processor circuitry to decode a purchased basket provided by a panelist or other consumer/participant. The machine readable instructions and/or the operations <b>2700</b> of <figref idref="DRAWINGS">FIG. <b>27</b></figref> begin at block <b>2702</b>, at which the document decode circuitry <b>114</b> receives a receipt image (e.g., receipt image <b>108</b>) and corresponding barcodes (e.g., barcodes <b>110</b>) for processing. For example, the document decode circuitry <b>114</b> can receive and/or retrieve the receipt image <b>108</b> and corresponding barcodes <b>110</b> from an example basket datastore (e.g., basket datastore <b>112</b>).</p><p id="p-0263" num="0255">At block <b>2704</b>, example OCR circuitry (e.g., OCR circuitry <b>116</b>) performs OCR analysis of the receipt image <b>108</b>. For example, the OCR circuitry <b>116</b> applies an OCR algorithm over the image to detect text. In some examples, the OCR circuitry <b>116</b> outputs a plurality of bounding boxes corresponding to text, coordinates of the bounding boxes, and transcribed text within the bounding boxes. In some examples, the OCR circuitry <b>116</b> transmits the OCR output to example extraction circuitry (e.g., extraction circuitry <b>118</b>).</p><p id="p-0264" num="0256">At block <b>2706</b>, example regions detection circuitry (e.g., regions detection circuitry <b>304</b>) processes the receipt image <b>108</b> with a regions detection model <b>306</b> to detect regions of interest. For example, the regions detection circuity <b>304</b> can apply the regions detection model <b>306</b> to the receipt image <b>108</b> to detect a receipt region and a products regions, as described above and in further detail below. In some examples, example image cropping circuitry (e.g., image cropping circuitry <b>308</b>) applies a cropping operation on the receipt image <b>108</b> based on the detected regions of interest, and transmits the cropped images to example row detection circuitry (e.g., row detection circuitry <b>310</b>) and example column detection circuitry (e.g., column detection circuitry <b>312</b>).</p><p id="p-0265" num="0257">At block <b>2708</b>, the example row detection circuitry <b>310</b> processes the receipt region of the receipt image <b>108</b> to detect rows that correspond to lines of text, as described above and in further detail below. For example, the row detection circuitry <b>310</b> can apply an example row segmentation model <b>314</b> to the receipt region to output a pixel-wise mask. In some examples, the row detection circuitry <b>310</b> applies post-processing techniques to detect bounding boxes corresponding to the rows.</p><p id="p-0266" num="0258">At block <b>2710</b>, the example column detection circuitry <b>312</b> processes the products region of the receipt image <b>108</b> to detect columns, as described above and in further detail below. For example, the column detection circuitry <b>312</b> can apply an example column segmentation model <b>320</b> to the products region to output a pixel-wise classifier. In some examples, the column detection circuitry <b>312</b> applies post techniques to detect bounding boxes corresponding to the columns.</p><p id="p-0267" num="0259">At block <b>2712</b>, example data extracting circuitry (e.g., data extracting circuitry <b>326</b>) detects and builds a structure a receipt corresponding to the receipt image <b>108</b> to extract purchase information from the receipt, as described above and in further detail below. For example, the purchase information can include purchase products, prices, quantities, discounts, etc. In some examples, the data extracting circuitry <b>326</b> can receive and/or retrieve the text data output by the OCR circuitry <b>116</b>, row data output by the row detection circuitry <b>310</b>, and column data output by the column detection circuitry <b>312</b> to facilitate building of the receipt's structure. For example, the data extracting circuitry <b>326</b> organizes the detected row and columns based on coordinates of their respective bounding boxes and maps words output by the OCR circuitry <b>116</b> to respective rows and columns based on IoU calculations. In some examples, forming the receipt's structure generates a digitized receipt from which the data extracting circuitry <b>326</b> can extract the purchase information.</p><p id="p-0268" num="0260">At block <b>2714</b>, example decoding circuitry (e.g., decoding circuitry <b>122</b>) decodes the receipt by matching extracted purchase information with respective barcodes <b>110</b>, as described above and in further detail below. For example, the decoding circuitry <b>122</b> can receive and/or retrieve the extracted purchase information from the extraction circuitry <b>118</b> and corresponding barcodes <b>110</b> uploaded by the panelist (e.g., from the basket datastore <b>112</b>). In some examples, the decoding circuitry <b>122</b> generates a search query and searches the query against a first data source. For example, the first data source can be an example internal dictionary (e.g., internal dictionary <b>124</b>). In some examples, the decoding circuitry <b>122</b> generates a second search query and searches the query against a second data source (e.g., if the first search does not yield a match). For example, the second data source can be an example products datastore (e.g., products datastore <b>126</b>). In some examples, the decoding circuitry <b>122</b> generates a third search query and searches the query against a third data source (e.g., if the second search does not yield a match). For example, the third data source can be an example previous jobs database (e.g., previous jobs database <b>130</b>).</p><p id="p-0269" num="0261">At block <b>2716</b>, example report generating circuitry (e.g., report generating circuitry <b>134</b>) generates a report of the results. For example, the report can include decoded purchase data that includes prices and quantities associated with specific purchased barcodes. In some examples, the report generating circuitry <b>134</b> transmits the report to an example monitoring entity (e.g., monitoring entity <b>102</b>). In some examples, the monitoring entity <b>102</b> can be a marketing research entity that further analyzing the results to extract insights.</p><p id="p-0270" num="0262"><figref idref="DRAWINGS">FIG. <b>28</b></figref> is a flowchart representative of example machine readable instructions and/or example operations <b>2706</b> that may be executed and/or instantiated by processor circuitry to detect regions of interest of a receipt image <b>108</b>. The machine readable instructions and/or the operations <b>2706</b> of <figref idref="DRAWINGS">FIG. <b>28</b></figref> begin at block <b>2802</b>, at which the regions detection circuitry <b>304</b> applies the regions detection model <b>306</b> to the receipt image <b>108</b>. For example, the regions detection model <b>306</b> can be an R-CNN that detects and classifies the regions of interest.</p><p id="p-0271" num="0263">At block <b>2804</b>, the regions detection circuitry <b>304</b> (e.g., via the regions detection model <b>306</b>) detects the receipt region, which is a rectangular region of the receipt image <b>108</b> in which the corresponding receipt is positioned. For example, the regions detection model <b>306</b> can apply a selective search algorithm to detect the receipt region. At block <b>2806</b>, the regions detection circuitry <b>304</b> (e.g., via the regions detection model <b>306</b>) generates a bounding box around the receipt region. In some examples, the bounding box can include reference coordinates that correspond to the receipt region's position within the receipt image <b>108</b>. At block <b>2808</b>, the example image cropping circuitry <b>308</b> crops an area outside the bounding box to generate a first cropped image that corresponds to the receipt region.</p><p id="p-0272" num="0264">At block <b>2810</b>, the regions detection circuitry <b>304</b> (e.g., via the regions detection model <b>306</b>) detects the products region, which is a rectangular region around text of the receipt image <b>108</b> that contains purchase details. For example, the regions detection model <b>306</b> can apply a selective search algorithm to detect the products region. At block <b>2812</b>, the regions detection circuitry <b>304</b> (e.g., via the regions detection model <b>306</b>) generates a bounding box around the products region. In some examples, the bounding box can include reference coordinates that correspond to the products region's position within the receipt image <b>108</b>. At block <b>2814</b>, the example image cropping circuitry <b>308</b> crops an area outside the bounding box to generate a second cropped image that corresponds to the products region.</p><p id="p-0273" num="0265">At block <b>2816</b>, the regions detection circuitry <b>304</b> transmits the first cropped image to the example row detection circuitry <b>310</b> and transmits the second cropped image to the example column detection circuitry <b>312</b>. For example, the regions detection circuitry <b>304</b> transmits the receipt region to the row detection circuitry <b>310</b> and the products region to the column detection circuitry <b>312</b>.</p><p id="p-0274" num="0266"><figref idref="DRAWINGS">FIG. <b>29</b></figref> is a flowchart representative of example machine readable instructions and/or example operations <b>2708</b> that may be executed and/or instantiated by processor circuitry to detect rows from the receipt region of the receipt image <b>108</b>. The machine readable instructions and/or the operations <b>2708</b> of <figref idref="DRAWINGS">FIG. <b>29</b></figref> begin at block <b>2902</b>, at which the example row detection circuitry <b>310</b> receives the cropped receipt region of the receipt image <b>108</b>. At block <b>2904</b>, the row detection circuitry <b>310</b> applies a row segmentation model <b>314</b> on the receipt region to generate an example pixel-wise mask. For example, the row segmentation model <b>314</b> can be CNN-based model trained as a pixel-wise predictor (e.g., using an example dhSegment architecture). In some examples, the row segmentation model <b>314</b> outputs the pixel-wise mask that classifies each pixel within the receipt region as belonging to either a text line or background. In some examples, the pixel-wise mask includes groups of pixels classified as row pixels in clusters.</p><p id="p-0275" num="0267">At block <b>2906</b>, example bounding box generating circuitry (e.g., bounding box generating circuitry <b>316</b>) identifies contours around the clusters of pixels belonging to a text line. For example, the contour can be a free connection of dots that form a convex closed form. At block <b>2908</b>, the bounding box generating circuitry <b>316</b> converts each contour into a minimal geometric representation that can be projected into a set of intersecting lines (e.g., four intersecting lines). For example, the set of intersecting lines can form a polygon that includes four angles (e.g., 90 degree angles). For example, the polygon can corresponding to a bounding box. Accordingly, the bounding box generating circuitry <b>316</b> can generate a plurality of bounding boxes that represent lines of text within the receipt region.</p><p id="p-0276" num="0268">At block <b>2910</b>, example line merging circuitry <b>318</b> identifies lengths of ones of the bounding boxes generated by the bounding box generating circuitry <b>316</b>. For example, the line merging circuitry <b>318</b> can identify the lengths of the bounding boxes using coordinates of each of the bounding boxes. At block <b>2912</b>, the line merging circuitry <b>318</b> generates a merging criterion using a largest length. For example, the line merging circuitry <b>318</b> compares lengths of the ones of the bounding boxes and identifies the longest bounding box. The line merging circuitry <b>318</b> generates the merging criterion by multiplying the largest length by a value between 0 and 1 (e.g., 0.9). Accordingly, the merging criterion is a threshold value.</p><p id="p-0277" num="0269">At block <b>2914</b>, the line merging circuitry <b>318</b> selects a bounding box to determine whether the bounding box is a candidate for line merging. The bounding box can be any bounding box generated by the example bounding box generating circuitry <b>316</b>. At block <b>2916</b>, the line merging circuitry <b>318</b> determines whether the bounding box is a candidate for line merging. For example, the line merging circuitry <b>318</b> compares a length of the bounding box to the merging criterion. If the length of the bounding box is greater than or equal to the threshold value of the merging criterion, the line merging circuitry <b>318</b> determines the bounding box is not a candidate for merging (e.g., block <b>2916</b>: NO). If the answer to block <b>2916</b> is NO, control advances to block <b>2918</b> at which the line merging circuitry <b>318</b> decides to ignore the bounding box during the line merging process. If the length of the bounding box is less that the threshold value of the merging criterion, the line merging circuitry <b>318</b> determines the bounding box is a candidate for merging (e.g., block <b>2916</b>: YES). If the answer to block <b>1916</b> is YES, control advances to block <b>2920</b>, at which the line merging circuitry <b>318</b> adds the bounding box to a list of merging candidates.</p><p id="p-0278" num="0270">At block <b>2922</b>, the line merging circuitry <b>318</b> determines whether to select another bounding box. For example, if another bounding box has not been compared to the merging criterion, the line merging circuitry <b>318</b> can determine to select another bounding box (e.g., block <b>2922</b>: YES). If the answer to block <b>2922</b> is YES, control advances to block <b>2914</b> at which the lines merging circuitry <b>2914</b> selects a bounding box. If the line merging circuitry <b>318</b> has compared each bounding box to the merging criterion, the line merging circuitry <b>318</b> can determine not to select another bounding box (e.g., block <b>2922</b>: NO).</p><p id="p-0279" num="0271">If the answer to block <b>2922</b> is NO, control advances to block <b>2924</b> at which the line merging circuitry <b>318</b> initializes a graph with nodes representing the bounding boxes from the list of merging candidates to identify components that belong to a same row. For example, the line merging circuitry <b>318</b> can initialize the graph with a plurality of nodes, wherein each of the plurality of nodes represents a respective bounding boxes in the list of merging candidates. In some examples, the line merging circuitry <b>318</b> identifies the bounding boxes that belong to the same row by adding edges between any two nodes that satisfy two conditions. For example, the first condition can be that the bounding boxes share a positive vertical coordinate and the second condition can be that the bounding boxes do not have an overlapping horizontal coordinate.</p><p id="p-0280" num="0272">At block <b>2926</b>, the line merging circuitry <b>318</b> identifies and arranges connected components (e.g., nodes) within the graph. In some examples, the line merging circuitry <b>318</b> applies a depth-first search (DFS) algorithm to the graph to identify the connected components within the graph. In some examples, the line merging circuitry <b>318</b> arranges the connected components corresponding to the nodes in a left to right order. For example, arranging the connected components in a left to right order enables the line merging circuitry <b>318</b> to merge a leftmost and a rightmost bounding boxes and ignore other (e.g., intermediate) bounding boxes.</p><p id="p-0281" num="0273">At block <b>2928</b>, the line merging circuitry <b>318</b> merges the connected components. That is, the line merging circuitry <b>318</b> merges bounding boxes that have been identified as belonging to the same row. In some examples, the row detection circuitry <b>310</b> outputs a list of bounding boxes that are fitted to printed text lines (e.g., rows) within the receipt region (e.g., and coordinates of the bounding boxes).</p><p id="p-0282" num="0274"><figref idref="DRAWINGS">FIG. <b>30</b></figref> is a flowchart representative of example machine readable instructions and/or example operations <b>2710</b> that may be executed and/or instantiated by processor circuitry to detect columns from the products regions of the receipt image <b>108</b>. The machine readable instructions and/or the operations <b>2710</b> of <figref idref="DRAWINGS">FIG. <b>30</b></figref> begin at block <b>3002</b>, at which the example column detection circuitry <b>312</b> receives the cropped products region of the receipt image <b>108</b>. At block <b>3004</b>, the column detection circuitry <b>312</b> applies a column segmentation model <b>320</b> on the products region to generate an example pixel-wise classifier. For example, the column segmentation model <b>320</b> can be CNN-based model trained as a pixel-wise predictor (e.g., using an example U-Net architecture). In some examples, the column segmentation model <b>320</b> outputs the pixel-wise classifier that classifies each pixel within the products region as belonging to either a text area or background. In some examples, the pixel-wise classifier includes groups of pixels classified as columns pixels in clusters.</p><p id="p-0283" num="0275">At block <b>3006</b>, example bounding box generating circuitry (e.g., bounding box generating circuitry <b>322</b>) identifies contours around the clusters of pixels belonging to a column. For example, the contour can be a free connection of dots that form a convex closed form. At block <b>3008</b>, the bounding box generating circuitry <b>322</b> converts each contour into a minimal geometric representation that can be projected into a set of intersecting lines (e.g., four intersecting lines). For example, the set of intersecting lines can form a polygon that includes four angles (e.g., 90 degree angles). For example, the polygon can corresponding to a bounding box. Accordingly, the bounding box generating circuitry <b>322</b> can generate a plurality of bounding boxes that represent columns of text within the products region.</p><p id="p-0284" num="0276">At block <b>3010</b>, example column merging circuitry (e.g., column merging circuitry <b>324</b>) initializes a graph with nodes representing the bounding boxes from plurality of bounding boxes generated by the bounding box generating circuitry <b>322</b>. For example, the column merging circuitry <b>324</b> can initialize the graph with a plurality of nodes, wherein each of the plurality of nodes represents a respective bounding boxes in of the plurality of bounding boxes. In some examples, the column merging circuitry <b>324</b> identifies the bounding boxes that belong to the same column by adding edges between any two nodes that satisfy two conditions. For example, the first condition can be that the bounding boxes share a positive horizontal coordinate and the second condition can be that the bounding boxes do not have an overlapping vertical coordinate.</p><p id="p-0285" num="0277">At block <b>3012</b>, the column merging circuitry <b>324</b> identifies and arranges connected components (e.g., nodes) within the graph. In some examples, the column merging circuitry <b>324</b> applies a depth-first search (DFS) algorithm to the graph to identify the connected components within the graph. In some examples, the column merging circuitry <b>324</b> arranges the connected components corresponding to the nodes in a top to bottom order. For example, arranging the connected components in a top to bottom order enables the column merging circuitry <b>324</b> to merge a topmost and a bottommost bounding boxes and ignore other (e.g., intermediate) bounding boxes.</p><p id="p-0286" num="0278">At block <b>3014</b>, the column merging circuitry <b>324</b> merges the connected components. That is, the column merging circuitry <b>324</b> merges bounding boxes that have been identified as belonging to the same column. In some examples, the column detection circuitry <b>312</b> outputs a list of bounding boxes that are fitted to columns within the products region (e.g., and coordinates of the bounding boxes).</p><p id="p-0287" num="0279"><figref idref="DRAWINGS">FIG. <b>31</b></figref> is a flowchart representative of example machine readable instructions and/or example operations <b>2712</b> that may be executed and/or instantiated by processor circuitry to determine and form a structure of a receipt corresponding to the receipt image <b>108</b> to extract purchase information. The machine readable instructions and/or the operations <b>2712</b> of <figref idref="DRAWINGS">FIG. <b>31</b></figref> begin at block <b>3102</b>, at which the data extracting circuitry <b>326</b> receives the OCR text data, detected rows, and detected columns. For example, the data extracting circuitry <b>326</b> receives text data, text bounding boxes, and respective coordinates from the OCR circuitry <b>116</b>, the row bounding boxes and respective coordinates from the row detection circuitry <b>310</b>, and the column bounding boxes and respective coordinates from the column detection circuitry <b>312</b>.</p><p id="p-0288" num="0280">At block <b>3104</b>, example structuring circuitry (e.g., structuring circuitry <b>330</b>) transforms the coordinates of the bounding boxes to reference coordinate of the receipt image <b>108</b>. For example, the coordinates of the bounding boxes may need to be transformed because text bounding boxes were generated using the receipt image <b>108</b> while the row bounding boxes were generated using the cropped receipt region and the column bounding boxes were generated using the cropped products region. In some examples, the structuring circuitry <b>330</b> transforms the row bounding boxes based on the receipt region's location within the receipt image <b>108</b> and transforms the column bounding boxes based on the products region's location within the receipt image <b>108</b>.</p><p id="p-0289" num="0281">At block <b>3106</b>, the structing circuitry <b>330</b> extends the row bounding boxes and the column bounding boxes to form a grid. For example, the structuring circuitry <b>330</b> can extend the row bounding boxes to an intersection with a products region's horizontal boundary(ies). Further, the structuring circuitry <b>330</b> can extend the column bounding boxes to an intersection with the products region's vertical boundary(ies). In some examples, the extensions can enable inclusion of words or characters near a borders that may have been missed by the row bounding boxes and/or the column bounding boxes.</p><p id="p-0290" num="0282">At block <b>3108</b>, example mapping circuitry (e.g., mapping circuitry <b>332</b>) determines and forms the structure the receipt by mapping words detected by the OCR circuitry <b>116</b> to corresponding rows and columns on the grid. In some examples, the mapping circuitry <b>332</b> determines IoU calculations for the text bounding boxes and the row bounding boxes and/or column bounding boxes. For example, the mapping circuitry <b>332</b> can compare determine an IoU calculation for each text bounding boxes with each row bounding box to determine whether the IoU meets a threshold value (e.g., 0.5). Similarly, the mapping circuitry <b>332</b> can compare determine an IoU calculation for each text bounding boxes with each column bounding box to determine whether the IoU meets the threshold value. If an IoU calculation meets the threshold value, the text bounding box can be associated with the respective row bounding box and/or column bounding box.</p><p id="p-0291" num="0283">At block <b>3110</b>, example column identifying circuitry (e.g., column identifying circuitry <b>334</b>) selects a column to be identified. For example, the column identifying circuitry <b>334</b> may select a column bounding box corresponding to a respective column to identify the column. At block <b>3112</b>, the column identifying circuitry <b>334</b> implements a regex engine to determine whether the column can be identified by a regular expression (e.g., regex). For example, the regex can include item description (e.g., product description), price, product code, etc. In other words, the column identifying circuitry <b>334</b> determines whether the column can be identified by a regex based on text within the column. If the column contains text that is defined by one of the regex, the column identifying circuitry <b>334</b> determines that the column follows the regex and classifies the column based on the regex. If the column identifying circuitry <b>334</b> determines that the column can be identified by a regex (e.g., block <b>3112</b>: YES), the column identifying circuitry <b>334</b> labels the column with the respective identifier (e.g., the regex) and control advances to block <b>3118</b>. If the column identifying circuitry <b>334</b> determines that the column cannot be identified by a regex (e.g., block <b>3112</b>: NO), the column identifying circuitry <b>334</b> labels the column with an &#x201c;unknown&#x201d; identifier and control advances to block <b>3118</b>.</p><p id="p-0292" num="0284">At block <b>3118</b>, the column identifying circuitry <b>334</b> determines whether to select another column. For example, if each column bounding box has been identified by a regex or an &#x201c;unknown&#x201d; identifier, the column identifying circuitry <b>334</b> may determine not to select another column (e.g., block <b>3118</b>: NO). If the answer to block <b>3118</b> is NO, control advances to block <b>3120</b>. If a column bounding box has not yet been identified by a regex or an &#x201c;unknown&#x201d; identifier, the column identifying circuitry <b>334</b> may determine to select another column (e.g., block <b>3118</b>: YES). If the answer to block <b>3118</b> is YES, control advances to block <b>3110</b> at which the column identifying circuitry <b>334</b> selects another column.</p><p id="p-0293" num="0285">At block <b>3120</b>, example purchase data identifying circuitry (e.g., purchase information extracting circuitry <b>336</b>) selects a row for validation and data extraction. For example, the purchase information extracting circuitry <b>336</b> can select a row that includes text data and compare the text data to a corresponding regex and/or dictionary of stopwords. At block <b>3122</b>, the purchase information extracting circuitry <b>336</b> validates the row based on the corresponding regex and stopwords. For example, the purchase information extracting circuitry <b>336</b> may compare the words in the row to the regex to determine whether the words in the row match the regex. Further, the purchase information extracting circuitry <b>336</b> can determine whether the row includes a stopword, such as total, promotion, and/or another word that does not correspond to a purchased item.</p><p id="p-0294" num="0286">At block <b>3124</b>, the purchase information extracting circuitry <b>336</b> removes words determined to not qualify the regex. For example, the purchase information extracting circuitry <b>336</b> may remove a price from an item description column and/or a letter in a price column. Further, the purchase information extracting circuitry <b>336</b> may remove the row if the row contains a stopword because the row is not needed for the decoding process.</p><p id="p-0295" num="0287">At block <b>3126</b>, the purchase information extracting circuitry <b>336</b> extracts purchase information from the row. For example, the purchase information extracting circuitry <b>336</b> can extract the purchase information that includes a product description, price, and quantity. In some examples, the price is in the product description column. Accordingly, the purchase information extracting circuitry <b>336</b> is constructed to extract the quantity from the product description.</p><p id="p-0296" num="0288">At block <b>3128</b>, the purchase information extracting circuitry <b>336</b> determines whether to select another row. For example, if each has been validated and extracted, the purchase information extracting circuitry <b>336</b> may determine not to select another row (e.g., block <b>3128</b>: NO). If the answer to block <b>3128</b> is NO, control advances to block <b>3130</b>. If a row has been validated or extracted, purchase information extracting circuitry <b>336</b> may determine to select another row (e.g., block <b>3128</b>: YES). If the answer to block <b>3128</b> is YES, control advances to block <b>3120</b> at which the purchase information extracting circuitry <b>336</b> selects another row.</p><p id="p-0297" num="0289">At block <b>3130</b>, example promotion identifying circuitry (e.g., promotion identifying circuitry <b>328</b>) identifies promotions and/or multi-buys in the receipt. For example, the promotion identifying circuitry <b>328</b> can identify various types of promotions including multibuy using a product description and respective price.</p><p id="p-0298" num="0290"><figref idref="DRAWINGS">FIG. <b>32</b></figref> is a flowchart representative of example machine readable instructions and/or example operations <b>3130</b> that may be executed and/or instantiated by processor circuitry to identify and flag promotions. The machine readable instructions and/or the operations <b>3130</b> of <figref idref="DRAWINGS">FIG. <b>32</b></figref> begin at block <b>3202</b>, at which the promotion identifying circuitry <b>328</b> selects an extracted row. For example, the extracted row can include a product description for a purchased product and a price. At block <b>3204</b>, the promotion identifying circuitry <b>328</b>, identifies a first price as listed in the price column. For example, the price may be an original price of the product or a discounted price of the product.</p><p id="p-0299" num="0291">At block <b>3206</b>, the promotion identifying circuitry <b>328</b> determines whether a promotion is listed that corresponds to the row. For example, the promotion can be a discount, a multi-buy, a coupon, etc. In some examples, the promotion identifying circuitry <b>328</b> can analyze text within the product description to identify a key word that indicates the promotion was applied to the product. For example, the product description can include a word such as buy-one-get-one (e.g., BOGO). In some examples, the promotion identifying circuitry <b>328</b> determines whether the promotion is listed in a discount section. For example, some receipts include a section of the receipt that lists the discounts. If the answer to block <b>3206</b> is NO, and a promotion is not listed that corresponds to the row, control advances to block <b>3214</b> at which the promotion identifying circuitry <b>328</b> saves the first price corresponding to the product. In some examples, the first price is the original price and the price paid and the discount amount is zero. If the answer to block <b>3206</b> is YES, a promotion is listed that corresponds to the row, control advances to block <b>3208</b></p><p id="p-0300" num="0292">At block <b>3208</b>, the promotion identifying circuitry <b>328</b> determines a second price as listed in the promotion. For example, the second price may be the price paid for the discount amount. At block <b>3210</b>, the promotion identifying circuitry <b>328</b> calculate a third price based on the first price and the second price. For example, if the first price is the original price and the second price is the discount amount, the third price is the price paid for the product. If the first price is the price paid for the product and the second price is the discount amount, the third price is the original price of the product.</p><p id="p-0301" num="0293">At block <b>3212</b>, the promotion identifying circuitry <b>328</b> flags the product description as including a promotion. For example, the promotion identifying circuitry <b>328</b> can apply an indication, such as a code, to the row that indicates a promotion was applied to the product. At block <b>3212</b>, the promotion identifying circuitry <b>328</b> saves the prices (e.g., the first price, second price, and third price) corresponding to the purchased product.</p><p id="p-0302" num="0294">At block <b>3216</b>, the promotion identifying circuitry <b>328</b> determines whether to select another row. For example, the promotion identifying circuitry <b>328</b> may be structured to extract a paid price after discounting, a discount amount, and an original price for each extracted purchased product. If the prices have been determines for each product, the promotion identifying circuitry <b>328</b> may determine not to select another row (e.g., block <b>3216</b>: NO). If the answer to block <b>3216</b> is NO, control advances to block <b>3218</b>. If a purchased product has not been analyzed to determine the three prices, the promotion identifying circuitry <b>328</b> may determine to select another row (e.g., block <b>3216</b>: YES). If the answer to block <b>3216</b> is YES, control advances to block <b>3218</b> at which the promotion identifying circuitry <b>328</b> saves the results. For example, the promotion identifying circuitry <b>328</b> save the three prices for each purchased product with the purchase information extracted by the purchase information extracting circuitry <b>336</b>.</p><p id="p-0303" num="0295"><figref idref="DRAWINGS">FIG. <b>33</b></figref> is a flowchart representative of example machine readable instructions and/or example operations <b>2714</b> that may be executed and/or instantiated by processor circuitry to match extracted information with barcodes <b>110</b>. The machine readable instructions and/or the operations <b>2714</b> of <figref idref="DRAWINGS">FIG. <b>33</b></figref> begin at block <b>3302</b>, at which the decoding circuitry <b>122</b> receives extracted purchase information, barcodes, and a store ID corresponding to a receipt image <b>108</b>. For example, the decoding circuitry <b>122</b> can receive the extracted purchase information from the extraction circuitry <b>118</b> via example extraction interface circuitry (e.g., extraction information circuitry <b>1702</b>). Further, the decoding circuitry <b>122</b> can receive the barcodes (e.g., barcodes <b>110</b>) and store ID from the basket datastore <b>112</b> via example datastore interface circuitry (e.g., datastore interface circuitry <b>1704</b>. In some examples, the decoding circuitry <b>122</b> retrieves the barcodes and store ID from the basket datastore <b>112</b> in response to receiving the extracted purchase information from the extraction circuitry <b>118</b>.</p><p id="p-0304" num="0296">At block <b>3304</b>, example barcode filtering circuitry (e.g., barcode filtering circuitry <b>1708</b>) filters the barcodes <b>110</b> to generate a unique list of barcodes. For example, the barcode filtering circuitry <b>1708</b> may remove duplicate barcodes uploaded by a panelist, barcodes corresponding to plastic bags, and/or other types of barcodes that may not be needed during the decoding process.</p><p id="p-0305" num="0297">At block <b>3306</b>, example text correcting circuitry (e.g., text correcting circuitry <b>1710</b>) recognizes and corrects error in product description text. For example, the text correcting circuitry <b>1710</b> can search for typos such as mis-identified or un-identified character (e.g., special characters) using an example language dictionary (e.g., language dictionary <b>1712</b>). In some examples, the text correcting circuitry <b>1710</b> may perform a text normalization on the text of the product descriptions.</p><p id="p-0306" num="0298">At block <b>3308</b>, example search manager circuitry (e.g., search manager circuitry <b>1714</b>) selects a product description from the extracted purchase information and generates search query input. For example, the search query input includes the first product description, the list of unique barcodes, and the store ID. In some examples, the search manager circuitry <b>1714</b> removes matched barcodes to update the list of unique barcodes. In some examples, the search manager circuitry <b>1714</b> transmits the input to example dictionary searching circuitry (e.g., dictionary searching circuitry <b>1718</b>).</p><p id="p-0307" num="0299">At block <b>3310</b>, the dictionary searching circuitry <b>1718</b> generates a search query that includes the product description, the list of unique barcodes, the store ID, and search conditions. For example, the search conditions can include a condition that a response(s) should match at least one barcode, a condition that the response(s) needs to match the provided store ID, a condition that the response(s) should match the item description as extracted by the extraction circuitry <b>118</b>, and/or another searching condition not disclosed herein.</p><p id="p-0308" num="0300">At block <b>3312</b>, the dictionary searching circuitry <b>1718</b> searches a first data source using the generated search query. For example, the dictionary searching circuitry <b>1718</b> can search an example internal dictionary (e.g., internal dictionary <b>124</b>) via example internal dictionary interface circuitry (e.g., internal dictionary interface circuitry <b>1720</b>). At block <b>3312</b>, the dictionary searching circuitry <b>3312</b> determines whether a match is identified. For example, if a dictionary response includes a recognized matched between the product description and a barcode from the list of unique barcodes, the dictionary searching circuitry <b>3312</b> determines that the matched has been identified (e.g., the answer to block <b>3312</b> is YES). If the answer to block <b>3312</b> is YES, control advances to block <b>3318</b> at which the search manager circuitry <b>1714</b> saves the match to a final response. If the answer to block <b>3312</b> is NO, and a match was no identified, the search manager circuitry <b>1714</b> adds the product description to a list of product descriptions to be searched against another data source (e.g., block <b>3316</b>).</p><p id="p-0309" num="0301">At block <b>3320</b>, the search manager circuitry <b>1714</b> determines whether to select another product description. For example, the search manager circuitry <b>1714</b> may determines whether any product descriptions extracted by the extraction circuitry <b>118</b> have yet to be searched against the first data source. If the answer to block <b>3320</b> is YES, control advances back to block <b>3308</b> at which the search manager circuitry <b>1714</b> selects a product description from the extracted purchase information and generates search query input. If the answer to block <b>3320</b> is NO, control advances to block <b>3322</b>.</p><p id="p-0310" num="0302">At block <b>3322</b>, the search manager circuitry <b>1714</b> determines whether to search another data source. For example, the search manager circuitry <b>1714</b> may decide to search another data source if there are product descriptions in the list of product descriptions to be searched against another data source. In some examples, the search manager circuitry <b>1714</b> decides not to search another data source if each product description extracted by the extraction circuitry <b>118</b> has been matched. If the answer to block <b>3322</b> is YES, and the search manager circuitry <b>1714</b> decides to search another data source, control advances to block <b>3324</b>. If the answer to block <b>3322</b> is NO, and the search manager circuitry <b>1714</b> decides not to search another data source, control advances to block <b>3326</b>.</p><p id="p-0311" num="0303">At block <b>3324</b>, the matching circuitry <b>1706</b> searches un-matched product descriptions against least one additional data source. For example, the matching circuitry <b>1706</b> may search an example products datastore (e.g., products datastore <b>126</b>) and/or an example previous jobs database (e.g., previous jobs database <b>130</b>).</p><p id="p-0312" num="0304">At block <b>3326</b>, example updating circuitry (e.g., updating circuitry <b>1730</b>) updates the first data source. For example, the updating circuitry <b>1730</b> can update the internal dictionary <b>124</b> with matched product descriptions and barcodes.</p><p id="p-0313" num="0305"><figref idref="DRAWINGS">FIG. <b>34</b></figref> is a flowchart representative of example machine readable instructions and/or example operations <b>3318</b> that may be executed and/or instantiated by processor circuitry to search another database. The machine readable instructions and/or the operations <b>3318</b> of <figref idref="DRAWINGS">FIG. <b>34</b></figref> begin at block <b>3402</b>, at which the search manager circuitry <b>1714</b> selects a product description from the list of un-matched product descriptions and generates search query input. For example, the searching query input can include the selected product description, an updated list of unique barcodes (e.g., that does not include previously matched barcodes), and the store ID.</p><p id="p-0314" num="0306">At block <b>3404</b>, example products datastore search circuitry (e.g., products datastore searching circuitry <b>1722</b>) generates a first search query that includes the product description, the list of unique barcodes, the store ID, and search conditions. For example, the search conditions can include a variety search conditions that aim to return a candidate result(s) and corresponding similarity score(s) for matches between the product description and a barcode. In some examples, the search query also includes a group store code and/or a global store code to limits a search of the products datastore <b>126</b> to products belonging to the store ID.</p><p id="p-0315" num="0307">At block <b>3406</b>, the products datastore searching circuitry <b>1722</b> searches a second data source using the generated search query. For example, the products datastore searching circuitry <b>1722</b> can search the products datastore <b>126</b> via example products datastore interface circuitry (e.g., products datastore interface circuitry <b>1724</b>). In some examples, the products datastore searching circuitry <b>1722</b> searches against the products datastore <b>126</b> by lookup up the barcodes in the list of unique barcodes and comparing products description corresponding to the barcodes as listed in the products datastore <b>126</b> and comparing those products descriptions to the product description in the search query. The result can include candidates that are similar and corresponding similarity scores.</p><p id="p-0316" num="0308">At block <b>3408</b>, the products datastore searching circuitry <b>1722</b> determines whether a match is identified. For example, if a response includes a candidate result with a corresponding similarity score that reaches a threshold value, the products datastore searching circuitry <b>1722</b> determines that the matched has been identified (e.g., the answer to block <b>3312</b> is YES). If the query returns more than one candidate result with corresponding similarity scores that reaches the threshold value, the products datastore searching circuitry <b>1722</b> selects the barcode with highest (e.g., largest) similarity score. If the answer to block <b>3408</b> is YES, control advances to block <b>3428</b> at which the search manager circuitry <b>1714</b> saves the match to the final response. If the answer to block <b>3408</b> is NO, and a match was not identified, control advances to block <b>3410</b>.</p><p id="p-0317" num="0309">At block <b>3410</b>, the product datastore searching circuitry <b>1722</b> generates another search query. For example, the product datastore searching circuitry <b>1722</b> can generate the second search query by removing the store ID from the first search query. At block <b>3412</b>, the product datastore searching circuitry <b>1722</b> searches the second search query against the products datastore <b>126</b> via the products datastore interface circuitry <b>1724</b>. For example, the products datastore searching circuitry <b>1722</b> searches the query against the products datastore <b>126</b> to identify barcodes with similar products descriptions to the product description in the query.</p><p id="p-0318" num="0310">At block <b>3412</b>, the product datastore searching circuitry <b>1722</b> determines whether a match is identified. For example, if a response includes a candidate result with a corresponding similarity score that reaches a threshold value, the products datastore searching circuitry <b>1722</b> determines that the matched has been identified (e.g., the answer to block <b>3410</b> is YES). If the answer to block <b>3410</b> is YES, control advances to block <b>3428</b> at which the search manager circuitry <b>1714</b> saves the match to the final response. If the answer to block <b>3410</b> is NO, and a match was not identified, control advances to block <b>3416</b>. At block <b>3416</b>, the search manager circuitry <b>1714</b> add the product description to a list of un-matched product descriptions.</p><p id="p-0319" num="0311">At block <b>3418</b>, the search manager circuitry <b>1714</b> determines whether to select another product description. For example, the search manager circuitry <b>1714</b> may determines whether any product descriptions in the list of un-matched have yet to be searched against the second data source. If the answer to block <b>3418</b> is YES, control advances back to block <b>3402</b> at which the search manager circuitry <b>1714</b> selects a product description from the list of un-matched product descriptions and generates search query input. If the answer to block <b>3418</b> is NO, control advances to block <b>3420</b>.</p><p id="p-0320" num="0312">At block <b>3420</b>, example previous jobs searching circuitry (e.g., previous jobs searching circuitry <b>1726</b>) generates a search query that includes the list of un-product descriptions (e.g., generated after the products datastore <b>126</b> searches), the unmatched barcodes, the store ID, and search conditions. For example, the search condition can be to obtain an intersection of the product descriptions and barcodes between the current basket and past baskets. At block <b>3422</b>, the previous jobs searching circuitry <b>1726</b> searches a third data source using the generated search query. For example, the previous jobs searching circuitry <b>1726</b> can search an example previous jobs database (e.g., previous jobs database <b>130</b>).</p><p id="p-0321" num="0313">At block <b>3424</b>, the previous jobs searching circuitry <b>1726</b> determines whether any match(es) is identified. For example, the previous jobs searching circuitry <b>1726</b> may identify a match if one barcode and one receipt description are minimum matched (e.g., min-matched). If the answer to block <b>3424</b> is YES, control advances to block <b>3428</b> at which the search manager circuitry <b>1714</b> saves the match to the final response. If the answer to block <b>3424</b> is NO, control advances to block <b>3436</b>. At block <b>3426</b>, the previous jobs searching circuitry <b>1726</b> uploads the un-matched products descriptions, barcodes, and store ID to the previous jobs database <b>130</b>. For example, the previous jobs searching circuitry <b>1726</b> can add an entry into the previous jobs database <b>130</b> that includes the un-matched purchase information and/or other information or data related to the receipt image <b>108</b>.</p><p id="p-0322" num="0314"><figref idref="DRAWINGS">FIG. <b>35</b></figref> is a block diagram of an example processor platform <b>3500</b> structured to execute and/or instantiate the machine readable instructions and/or the operations of <figref idref="DRAWINGS">FIGS. <b>26</b>-<b>34</b></figref> to implement the document decode circuitry <b>114</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. The processor platform <b>3500</b> can be, for example, a server, a personal computer, a workstation, a self-learning machine (e.g., a neural network), a mobile device (e.g., a cell phone, a smart phone, a tablet such as an iPad), a personal digital assistant (PDA), an Internet appliance, a gaming console, or any other type of computing device.</p><p id="p-0323" num="0315">The processor platform <b>3500</b> of the illustrated example includes processor circuitry <b>3512</b>. The processor circuitry <b>3512</b> of the illustrated example is hardware. For example, the processor circuitry <b>3512</b> can be implemented by one or more integrated circuits, logic circuits, FPGAs, microprocessors, CPUs, GPUs, DSPs, and/or microcontrollers from any desired family or manufacturer. The processor circuitry <b>3512</b> may be implemented by one or more semiconductor based (e.g., silicon based) devices. In this example, the processor circuitry <b>3512</b> implements example OCR circuitry <b>116</b>, example extraction circuitry <b>118</b>, example datastore interface circuitry <b>302</b>, example regions detection circuitry <b>304</b>, example image cropping circuitry <b>308</b>, example row detection circuitry <b>310</b>, example bounding box generating circuitry <b>316</b>, example line merging circuitry <b>318</b>, example column detection circuitry <b>312</b>, example bounding box generating circuitry <b>322</b>, example column merging circuitry <b>324</b>, example data extracting circuitry <b>326</b>, example promotion identifying circuitry <b>328</b>, example structuring circuitry <b>330</b>, example mapping circuitry <b>332</b>, example column identifying circuitry <b>334</b>, example purchase information extracting circuitry <b>336</b>, example decoding circuitry <b>122</b>, example extraction interface circuitry <b>1702</b>, example datastore interface circuitry <b>1704</b>, example matching circuitry <b>1706</b>, example barcode filtering circuitry <b>1708</b>, example text correcting circuitry <b>1710</b>, example search manager circuitry <b>1714</b>, example dictionary searching circuitry <b>1718</b>, example internal dictionary interface circuitry <b>1720</b>, example products datastore searching circuitry <b>1722</b>, example products datastore interface circuitry <b>1724</b>, example previous jobs searching circuitry <b>1726</b>, example updating circuitry <b>1730</b>, example response generating circuitry <b>1732</b>, example report generating circuitry <b>134</b>, example machine learning circuitry <b>136</b>, and/or, more generally, example document decode circuitry <b>114</b>.</p><p id="p-0324" num="0316">The processor circuitry <b>3512</b> of the illustrated example includes a local memory <b>3513</b> (e.g., a cache, registers, etc.). The processor circuitry <b>3512</b> of the illustrated example is in communication with a main memory including a volatile memory <b>3514</b> and a non-volatile memory <b>3516</b> by a bus <b>3518</b>. The volatile memory <b>3514</b> may be implemented by Synchronous Dynamic Random Access Memory (SDRAM), Dynamic Random Access Memory (DRAM), RAMBUS&#xae; Dynamic Random Access Memory (RDRAM&#xae;), and/or any other type of RAM device. The non-volatile memory <b>3516</b> may be implemented by flash memory and/or any other desired type of memory device. Access to the main memory <b>3514</b>, <b>3516</b> of the illustrated example is controlled by a memory controller <b>3517</b>.</p><p id="p-0325" num="0317">The processor platform <b>3500</b> of the illustrated example also includes interface circuitry <b>3520</b>. The interface circuitry <b>3520</b> may be implemented by hardware in accordance with any type of interface standard, such as an Ethernet interface, a universal serial bus (USB) interface, a Bluetooth&#xae; interface, a near field communication (NFC) interface, a Peripheral Component Interconnect (PCI) interface, and/or a Peripheral Component Interconnect Express (PCIe) interface.</p><p id="p-0326" num="0318">In the illustrated example, one or more input devices <b>3522</b> are connected to the interface circuitry <b>3520</b>. The input device(s) <b>3522</b> permit(s) a user to enter data and/or commands into the processor circuitry <b>3512</b>. The input device(s) <b>3522</b> can be implemented by, for example, an audio sensor, a microphone, a camera (still or video), a keyboard, a button, a mouse, a touchscreen, a track-pad, a trackball, an isopoint device, and/or a voice recognition system.</p><p id="p-0327" num="0319">One or more output devices <b>3524</b> are also connected to the interface circuitry <b>3520</b> of the illustrated example. The output device(s) <b>3524</b> can be implemented, for example, by display devices (e.g., a light emitting diode (LED), an organic light emitting diode (OLED), a liquid crystal display (LCD), a cathode ray tube (CRT) display, an in-place switching (IPS) display, a touchscreen, etc.), a tactile output device, a printer, and/or speaker. The interface circuitry <b>3520</b> of the illustrated example, thus, typically includes a graphics driver card, a graphics driver chip, and/or graphics processor circuitry such as a GPU.</p><p id="p-0328" num="0320">The interface circuitry <b>3520</b> of the illustrated example also includes a communication device such as a transmitter, a receiver, a transceiver, a modem, a residential gateway, a wireless access point, and/or a network interface to facilitate exchange of data with external machines (e.g., computing devices of any kind) by a network <b>3526</b>. The communication can be by, for example, an Ethernet connection, a digital subscriber line (DSL) connection, a telephone line connection, a coaxial cable system, a satellite system, a line-of-site wireless system, a cellular telephone system, an optical connection, etc.</p><p id="p-0329" num="0321">The processor platform <b>3500</b> of the illustrated example also includes one or more mass storage devices <b>3528</b> to store software and/or data. Examples of such mass storage devices <b>3528</b> include magnetic storage devices, optical storage devices, floppy disk drives, HDDs, CDs, Blu-ray disk drives, redundant array of independent disks (RAID) systems, solid state storage devices such as flash memory devices and/or SSDs, and DVD drives.</p><p id="p-0330" num="0322">The machine executable instructions <b>3532</b>, which may be implemented by the machine readable instructions of <figref idref="DRAWINGS">FIGS. <b>26</b>-<b>34</b></figref>, may be stored in the mass storage device <b>3528</b>, in the volatile memory <b>3514</b>, in the non-volatile memory <b>3516</b>, and/or on a removable non-transitory computer readable storage medium such as a CD or DVD.</p><p id="p-0331" num="0323"><figref idref="DRAWINGS">FIG. <b>36</b></figref> is a block diagram of an example implementation of the processor circuitry <b>3512</b> of <figref idref="DRAWINGS">FIG. <b>35</b></figref>. In this example, the processor circuitry <b>3512</b> of <figref idref="DRAWINGS">FIG. <b>35</b></figref> is implemented by a general purpose microprocessor <b>3600</b>. The general purpose microprocessor circuitry <b>3600</b> executes some or all of the machine readable instructions of the flowchart of <figref idref="DRAWINGS">FIGS. <b>26</b>-<b>34</b></figref> to effectively instantiate the document decode circuitry of <figref idref="DRAWINGS">FIG. <b>1</b></figref> as logic circuits to perform the operations corresponding to those machine readable instructions. In some such examples, the circuitry of <figref idref="DRAWINGS">FIG. <b>1</b></figref> is instantiated by the hardware circuits of the microprocessor <b>3600</b> in combination with the instructions. For example, the microprocessor <b>3600</b> may implement multi-core hardware circuitry such as a CPU, a DSP, a GPU, an XPU, etc. Although it may include any number of example cores <b>3602</b> (e.g., 1 core), the microprocessor <b>3600</b> of this example is a multi-core semiconductor device including N cores. The cores <b>3602</b> of the microprocessor <b>3600</b> may operate independently or may cooperate to execute machine readable instructions. For example, machine code corresponding to a firmware program, an embedded software program, or a software program may be executed by one of the cores <b>3602</b> or may be executed by multiple ones of the cores <b>3602</b> at the same or different times. In some examples, the machine code corresponding to the firmware program, the embedded software program, or the software program is split into threads and executed in parallel by two or more of the cores <b>3602</b>. The software program may correspond to a portion or all of the machine readable instructions and/or operations represented by the flowcharts of <figref idref="DRAWINGS">FIGS. <b>26</b>-<b>34</b></figref>.</p><p id="p-0332" num="0324">The cores <b>3602</b> may communicate by a first example bus <b>3604</b>. In some examples, the first bus <b>3604</b> may implement a communication bus to effectuate communication associated with one(s) of the cores <b>3602</b>. For example, the first bus <b>3604</b> may implement at least one of an Inter-Integrated Circuit (I2C) bus, a Serial Peripheral Interface (SPI) bus, a PCI bus, or a PCIe bus. Additionally or alternatively, the first bus <b>3604</b> may implement any other type of computing or electrical bus. The cores <b>3602</b> may obtain data, instructions, and/or signals from one or more external devices by example interface circuitry <b>3606</b>. The cores <b>3602</b> may output data, instructions, and/or signals to the one or more external devices by the interface circuitry <b>3606</b>. Although the cores <b>3602</b> of this example include example local memory <b>3620</b> (e.g., Level 1 (L1) cache that may be split into an L1 data cache and an L1 instruction cache), the microprocessor <b>3600</b> also includes example shared memory <b>3610</b> that may be shared by the cores (e.g., Level 2 (L2_cache)) for high-speed access to data and/or instructions. Data and/or instructions may be transferred (e.g., shared) by writing to and/or reading from the shared memory <b>3610</b>. The local memory <b>3620</b> of each of the cores <b>3602</b> and the shared memory <b>3610</b> may be part of a hierarchy of storage devices including multiple levels of cache memory and the main memory (e.g., the main memory <b>3514</b>, <b>3516</b> of <figref idref="DRAWINGS">FIG. <b>35</b></figref>). Typically, higher levels of memory in the hierarchy exhibit lower access time and have smaller storage capacity than lower levels of memory. Changes in the various levels of the cache hierarchy are managed (e.g., coordinated) by a cache coherency policy.</p><p id="p-0333" num="0325">Each core <b>3602</b> may be referred to as a CPU, DSP, GPU, etc., or any other type of hardware circuitry. Each core <b>3602</b> includes control unit circuitry <b>3614</b>, arithmetic and logic (AL) circuitry (sometimes referred to as an ALU) <b>3616</b>, a plurality of registers <b>3618</b>, the L1 cache <b>3620</b>, and a second example bus <b>3622</b>. Other structures may be present. For example, each core <b>3602</b> may include vector unit circuitry, single instruction multiple data (SIMD) unit circuitry, load/store unit (LSU) circuitry, branch/jump unit circuitry, floating-point unit (FPU) circuitry, etc. The control unit circuitry <b>3614</b> includes semiconductor-based circuits structured to control (e.g., coordinate) data movement within the corresponding core <b>3602</b>. The AL circuitry <b>3616</b> includes semiconductor-based circuits structured to perform one or more mathematic and/or logic operations on the data within the corresponding core <b>3602</b>. The AL circuitry <b>3616</b> of some examples performs integer based operations. In other examples, the AL circuitry <b>3616</b> also performs floating point operations. In yet other examples, the AL circuitry <b>3616</b> may include first AL circuitry that performs integer based operations and second AL circuitry that performs floating point operations. In some examples, the AL circuitry <b>3616</b> may be referred to as an Arithmetic Logic Unit (ALU). The registers <b>3618</b> are semiconductor-based structures to store data and/or instructions such as results of one or more of the operations performed by the AL circuitry <b>3616</b> of the corresponding core <b>3602</b>. For example, the registers <b>3618</b> may include vector register(s), SIMD register(s), general purpose register(s), flag register(s), segment register(s), machine specific register(s), instruction pointer register(s), control register(s), debug register(s), memory management register(s), machine check register(s), etc. The registers <b>3618</b> may be arranged in a bank as shown in <figref idref="DRAWINGS">FIG. <b>36</b></figref>. Alternatively, the registers <b>3618</b> may be organized in any other arrangement, format, or structure including distributed throughout the core <b>3602</b> to shorten access time. The second bus <b>3622</b> may implement at least one of an I2C bus, a SPI bus, a PCI bus, or a PCIe bus</p><p id="p-0334" num="0326">Each core <b>3602</b> and/or, more generally, the microprocessor <b>3600</b> may include additional and/or alternate structures to those shown and described above. For example, one or more clock circuits, one or more power supplies, one or more power gates, one or more cache home agents (CHAs), one or more converged/common mesh stops (CMSs), one or more shifters (e.g., barrel shifter(s)) and/or other circuitry may be present. The microprocessor <b>3600</b> is a semiconductor device fabricated to include many transistors interconnected to implement the structures described above in one or more integrated circuits (ICs) contained in one or more packages. The processor circuitry may include and/or cooperate with one or more accelerators. In some examples, accelerators are implemented by logic circuitry to perform certain tasks more quickly and/or efficiently than can be done by a general purpose processor. Examples of accelerators include ASICs and FPGAs such as those discussed herein. A GPU or other programmable device can also be an accelerator. Accelerators may be on-board the processor circuitry, in the same chip package as the processor circuitry and/or in one or more separate packages from the processor circuitry.</p><p id="p-0335" num="0327"><figref idref="DRAWINGS">FIG. <b>37</b></figref> is a block diagram of another example implementation of the processor circuitry <b>3512</b> of <figref idref="DRAWINGS">FIG. <b>35</b></figref>. In this example, the processor circuitry <b>3512</b> is implemented by FPGA circuitry <b>3700</b>. The FPGA circuitry <b>3700</b> can be used, for example, to perform operations that could otherwise be performed by the example microprocessor <b>3600</b> of <figref idref="DRAWINGS">FIG. <b>36</b></figref> executing corresponding machine readable instructions. However, once configured, the FPGA circuitry <b>3700</b> instantiates the machine readable instructions in hardware and, thus, can often execute the operations faster than they could be performed by a general purpose microprocessor executing the corresponding software.</p><p id="p-0336" num="0328">More specifically, in contrast to the microprocessor <b>3600</b> of <figref idref="DRAWINGS">FIG. <b>36</b></figref> described above (which is a general purpose device that may be programmed to execute some or all of the machine readable instructions represented by the flowcharts of <figref idref="DRAWINGS">FIGS. <b>26</b>-<b>34</b></figref> but whose interconnections and logic circuitry are fixed once fabricated), the FPGA circuitry <b>3700</b> of the example of <figref idref="DRAWINGS">FIG. <b>37</b></figref> includes interconnections and logic circuitry that may be configured and/or interconnected in different ways after fabrication to instantiate, for example, some or all of the machine readable instructions represented by the flowcharts of <figref idref="DRAWINGS">FIGS. <b>26</b>-<b>34</b></figref>. In particular, the FPGA <b>3700</b> may be thought of as an array of logic gates, interconnections, and switches. The switches can be programmed to change how the logic gates are interconnected by the interconnections, effectively forming one or more dedicated logic circuits (unless and until the FPGA circuitry <b>3700</b> is reprogrammed). The configured logic circuits enable the logic gates to cooperate in different ways to perform different operations on data received by input circuitry. Those operations may correspond to some or all of the software represented by the flowcharts of <figref idref="DRAWINGS">FIGS. <b>26</b>-<b>34</b></figref>. As such, the FPGA circuitry <b>3700</b> may be structured to effectively instantiate some or all of the machine readable instructions of the flowcharts of <figref idref="DRAWINGS">FIGS. <b>26</b>-<b>34</b></figref> as dedicated logic circuits to perform the operations corresponding to those software instructions in a dedicated manner analogous to an ASIC. Therefore, the FPGA circuitry <b>3700</b> may perform the operations corresponding to the some or all of the machine readable instructions of <figref idref="DRAWINGS">FIGS. <b>26</b>-<b>34</b></figref> faster than the general purpose microprocessor can execute the same.</p><p id="p-0337" num="0329">In the example of <figref idref="DRAWINGS">FIG. <b>37</b></figref>, the FPGA circuitry <b>3700</b> is structured to be programmed (and/or reprogrammed one or more times) by an end user by a hardware description language (HDL) such as Verilog. The FPGA circuitry <b>3700</b> of <figref idref="DRAWINGS">FIG. <b>37</b></figref>, includes example input/output (I/O) circuitry <b>3702</b> to obtain and/or output data to/from example configuration circuitry <b>3704</b> and/or external hardware (e.g., external hardware circuitry) <b>3706</b>. For example, the configuration circuitry <b>3704</b> may implement interface circuitry that may obtain machine readable instructions to configure the FPGA circuitry <b>3700</b>, or portion(s) thereof. In some such examples, the configuration circuitry <b>3704</b> may obtain the machine readable instructions from a user, a machine (e.g., hardware circuitry (e.g., programmed or dedicated circuitry) that may implement an Artificial Intelligence/Machine Learning (AI/ML) model to generate the instructions), etc. In some examples, the external hardware <b>3706</b> may implement the microprocessor <b>3600</b> of <figref idref="DRAWINGS">FIG. <b>36</b></figref>. The FPGA circuitry <b>3700</b> also includes an array of example logic gate circuitry <b>3708</b>, a plurality of example configurable interconnections <b>3710</b>, and example storage circuitry <b>3712</b>. The logic gate circuitry <b>3708</b> and interconnections <b>3710</b> are configurable to instantiate one or more operations that may correspond to at least some of the machine readable instructions of <figref idref="DRAWINGS">FIGS. <b>26</b>-<b>34</b></figref> and/or other desired operations. The logic gate circuitry <b>3708</b> shown in <figref idref="DRAWINGS">FIG. <b>37</b></figref> is fabricated in groups or blocks. Each block includes semiconductor-based electrical structures that may be configured into logic circuits. In some examples, the electrical structures include logic gates (e.g., And gates, Or gates, Nor gates, etc.) that provide basic building blocks for logic circuits. Electrically controllable switches (e.g., transistors) are present within each of the logic gate circuitry <b>3708</b> to enable configuration of the electrical structures and/or the logic gates to form circuits to perform desired operations. The logic gate circuitry <b>3708</b> may include other electrical structures such as look-up tables (LUTs), registers (e.g., flip-flops or latches), multiplexers, etc.</p><p id="p-0338" num="0330">The interconnections <b>3710</b> of the illustrated example are conductive pathways, traces, vias, or the like that may include electrically controllable switches (e.g., transistors) whose state can be changed by programming (e.g., using an HDL instruction language) to activate or deactivate one or more connections between one or more of the logic gate circuitry <b>3708</b> to program desired logic circuits.</p><p id="p-0339" num="0331">The storage circuitry <b>3712</b> of the illustrated example is structured to store result(s) of the one or more of the operations performed by corresponding logic gates. The storage circuitry <b>3712</b> may be implemented by registers or the like. In the illustrated example, the storage circuitry <b>3712</b> is distributed amongst the logic gate circuitry <b>3708</b> to facilitate access and increase execution speed.</p><p id="p-0340" num="0332">The example FPGA circuitry <b>3700</b> of <figref idref="DRAWINGS">FIG. <b>37</b></figref> also includes example Dedicated Operations Circuitry <b>3714</b>. In this example, the Dedicated Operations Circuitry <b>3714</b> includes special purpose circuitry <b>3716</b> that may be invoked to implement commonly used functions to avoid the need to program those functions in the field. Examples of such special purpose circuitry <b>3716</b> include memory (e.g., DRAM) controller circuitry, PCIe controller circuitry, clock circuitry, transceiver circuitry, memory, and multiplier-accumulator circuitry. Other types of special purpose circuitry may be present. In some examples, the FPGA circuitry <b>3700</b> may also include example general purpose programmable circuitry <b>3718</b> such as an example CPU <b>3720</b> and/or an example DSP <b>3722</b>. Other general purpose programmable circuitry <b>3718</b> may additionally or alternatively be present such as a GPU, an XPU, etc., that can be programmed to perform other operations.</p><p id="p-0341" num="0333">Although <figref idref="DRAWINGS">FIGS. <b>36</b> and <b>37</b></figref> illustrate two example implementations of the processor circuitry <b>3512</b> of <figref idref="DRAWINGS">FIG. <b>35</b></figref>, many other approaches are contemplated. For example, as mentioned above, modern FPGA circuitry may include an on-board CPU, such as one or more of the example CPU <b>3720</b> of <figref idref="DRAWINGS">FIG. <b>37</b></figref>. Therefore, the processor circuitry <b>3512</b> of <figref idref="DRAWINGS">FIG. <b>35</b></figref> may additionally be implemented by combining the example microprocessor <b>3600</b> of <figref idref="DRAWINGS">FIG. <b>36</b></figref> and the example FPGA circuitry <b>3700</b> of <figref idref="DRAWINGS">FIG. <b>37</b></figref>. In some such hybrid examples, a first portion of the machine readable instructions represented by the flowcharts of <figref idref="DRAWINGS">FIGS. <b>26</b>-<b>34</b></figref> may be executed by one or more of the cores <b>3602</b> of <figref idref="DRAWINGS">FIG. <b>36</b></figref>, a second portion of the machine readable instructions represented by the flowcharts of <figref idref="DRAWINGS">FIGS. <b>26</b>-<b>34</b></figref> may be executed by the FPGA circuitry <b>3700</b> of <figref idref="DRAWINGS">FIG. <b>37</b></figref>, and/or a third portion of the machine readable instructions represented by the flowcharts of <figref idref="DRAWINGS">FIGS. <b>26</b>-<b>34</b></figref> may be executed by an ASIC. It should be understood that some or all of the circuitry of <figref idref="DRAWINGS">FIG. <b>1</b></figref> may, thus, be instantiated at the same or different times. Some or all of the circuitry may be instantiated, for example, in one or more threads executing concurrently and/or in series. Moreover, in some examples, some or all of the circuitry of <figref idref="DRAWINGS">FIG. <b>1</b></figref> may be implemented within one or more virtual machines and/or containers executing on the microprocessor.</p><p id="p-0342" num="0334">In some examples, the processor circuitry <b>3512</b> of <figref idref="DRAWINGS">FIG. <b>35</b></figref> may be in one or more packages. For example, the processor circuitry <b>3600</b> of <figref idref="DRAWINGS">FIG. <b>36</b></figref> and/or the FPGA circuitry <b>3700</b> of <figref idref="DRAWINGS">FIG. <b>37</b></figref> may be in one or more packages. In some examples, an XPU may be implemented by the processor circuitry <b>3512</b> of <figref idref="DRAWINGS">FIG. <b>35</b></figref>, which may be in one or more packages. For example, the XPU may include a CPU in one package, a DSP in another package, a GPU in yet another package, and an FPGA in still yet another package.</p><p id="p-0343" num="0335">A block diagram illustrating an example software distribution platform <b>3805</b> to distribute software such as the example machine readable instructions <b>3532</b> of <figref idref="DRAWINGS">FIG. <b>35</b></figref> to hardware devices owned and/or operated by third parties is illustrated in <figref idref="DRAWINGS">FIG. <b>38</b></figref>. The example software distribution platform <b>3805</b> may be implemented by any computer server, data facility, cloud service, etc., capable of storing and transmitting software to other computing devices. The third parties may be customers of the entity owning and/or operating the software distribution platform <b>3805</b>. For example, the entity that owns and/or operates the software distribution platform <b>3805</b> may be a developer, a seller, and/or a licensor of software such as the example machine readable instructions <b>3532</b> of <figref idref="DRAWINGS">FIG. <b>35</b></figref>. The third parties may be consumers, users, retailers, OEMs, etc., who purchase and/or license the software for use and/or re-sale and/or sub-licensing. In the illustrated example, the software distribution platform <b>3805</b> includes one or more servers and one or more storage devices. The storage devices store the machine readable instructions <b>3532</b>, which may correspond to the example machine readable instructions <b>2600</b> and/or <b>2700</b> of <figref idref="DRAWINGS">FIGS. <b>26</b>-<b>34</b></figref>, as described above. The one or more servers of the example software distribution platform <b>3805</b> are in communication with a network <b>3810</b>, which may correspond to any one or more of the Internet and/or any of the example networks <b>106</b> described above. In some examples, the one or more servers are responsive to requests to transmit the software to a requesting party as part of a commercial transaction. Payment for the delivery, sale, and/or license of the software may be handled by the one or more servers of the software distribution platform and/or by a third party payment entity. The servers enable purchasers and/or licensors to download the machine readable instructions <b>3532</b> from the software distribution platform <b>3805</b>. For example, the software, which may correspond to the example machine readable instructions <b>2600</b> and/or <b>2700</b> of <figref idref="DRAWINGS">FIGS. <b>26</b>-<b>34</b></figref>, may be downloaded to the example processor platform <b>3500</b>, which is to execute the machine readable instructions <b>3532</b> to implement the document decode circuitry <b>114</b>. In some example, one or more servers of the software distribution platform <b>3805</b> periodically offer, transmit, and/or force updates to the software (e.g., the example machine readable instructions <b>3532</b> of <figref idref="DRAWINGS">FIG. <b>35</b></figref>) to ensure improvements, patches, updates, etc., are distributed and applied to the software at the end user devices.</p><p id="p-0344" num="0336">From the foregoing, it will be appreciated that example systems, methods, apparatus, and articles of manufacture have been disclosed that decode purchase data using an image of a receipt and a plurality of barcodes uploaded by a cooperating consumer. Disclosed systems, methods, apparatus, and articles of manufacture improve the efficiency of using a computing device by extracting and decoding (e.g., automatically) purchase data using images of receipts. In some examples, the automated decoding process can eliminate or otherwise reduce erroneous human behaviors, collect more information e.g., more details of baskets of purchased goods and/or more baskets), detect a structural layout of a receipt in a manner that is independent of OCR output to reduce deficiencies in OCR techniques, speed up the decoding process by focusing on data that needs to be processed, reduce instances of falsely identifying other non-product related text in the receipt, etc. Disclosed systems, methods, apparatus, and articles of manufacture are accordingly directed to one or more improvement(s) in the operation of a machine such as a computer or other electronic and/or mechanical device.</p><p id="p-0345" num="0337">Example methods, systems, and apparatus to extract and decode information from a receipt image have been disclosed herein. The following paragraphs provide various examples and example combinations of the examples disclosed herein.</p><p id="p-0346" num="0338">Example 1 includes an apparatus comprising memory, interface circuitry, and processor circuitry to execute machine readable instructions to at least crop an image of a receipt based on detected regions of interest, apply a first mask to a first cropped image of the receipt to generate first bounding boxes, the first bounding boxes corresponding to rows of the receipt, apply a second mask to a second cropped image of the receipt to generate second bounding boxes, the second bounding boxes corresponding to columns of the receipt, generate a structure of the receipt by mapping words detected by an optical character recognition (OCR) engine to corresponding first bounding boxes and second bounding boxes based on a mapping criterion, classify the second bounding boxes corresponding to the columns by identifying an expression of interest in ones of the second bounding boxes, and generate purchase information by detecting and extracting text of interest from the structured receipt based on the classifications, wherein the purchase information includes purchased products listed in the receipt.</p><p id="p-0347" num="0339">Example 2 includes the apparatus of example 1, wherein the regions of interest include (1) a receipt region corresponding to the receipt, and (2) a purchase region corresponding to an area of the receipt that includes information about the purchased products in the receipt.</p><p id="p-0348" num="0340">Example 3 includes the apparatus of any of examples 1-2, wherein the first cropped image is the receipt region and the second cropped image is the purchase region.</p><p id="p-0349" num="0341">Example 4 includes the apparatus of any of examples 1-3, wherein the first mask identifies ones of a plurality of pixels of the first cropped image as belonging to a first class or a second class.</p><p id="p-0350" num="0342">Example 5 includes the apparatus of any of examples 1-4, wherein the first class is a text line and the second class is background.</p><p id="p-0351" num="0343">Example 6 includes the apparatus of any of examples 1-5, wherein the second mask identifies ones of a plurality of pixels of the second cropped image as belonging to a first class or a second class.</p><p id="p-0352" num="0344">Example 7 includes the apparatus of any of examples 1-6, wherein the first class is a column and the second class is another class.</p><p id="p-0353" num="0345">Example 8 includes the apparatus of any of examples 1-7, wherein the processor circuitry is to execute the machine readable instructions to merge a first one of the first bounding boxes and a second one of the first bounding boxes based on a row connection criterion, the first one and the second one of the first bounding boxes to satisfy a length criterion.</p><p id="p-0354" num="0346">Example 9 includes the apparatus of any of examples 1-10, wherein the processor circuitry is to execute the machine readable instructions to merge a first one of the second bounding boxes and a second one of the second bound boxes based on a column connection criterion.</p><p id="p-0355" num="0347">Example 10 includes the apparatus of any of examples 1-9, wherein the expression of interest corresponds to a targeted fact.</p><p id="p-0356" num="0348">Example 11 includes the apparatus of any of examples 1-10, wherein the targeted fact includes at least one of a product description, a quantity, or a price.</p><p id="p-0357" num="0349">Example 12 includes the apparatus of any of examples 1-11, wherein the purchase information includes purchase details and promotion information, and wherein the processor circuitry is to execute the machine readable instructions to remove words from the first bounding boxes that do not correspond to the classification of respective second bounding boxes prior to generating the purchase details.</p><p id="p-0358" num="0350">Example 13 includes the apparatus of any of examples 1-12, wherein the purchase details include, for ones of the purchased products in the receipt, a product description, a price, and a quantity.</p><p id="p-0359" num="0351">Example 14 includes the apparatus of any of examples 1-13, wherein the processor circuitry executes the machine readable instructions to transmit the purchase details to decoding circuitry.</p><p id="p-0360" num="0352">Example 15 includes the apparatus of any of examples 1-14, wherein to generate the promotion information, the processor circuitry is to execute the machine readable instructions to determine whether a product is associated with a promotion, in response to determining the product is associated with the promotion: identify a first price associated with the product, the first price corresponding to a price column, identify a second price associated with the product, the second price corresponding to the promotion, and calculate a third price based on the first price and the second price.</p><p id="p-0361" num="0353">Example 16 includes the apparatus of any of examples 1-15, wherein the first price is an original price of the product, the second price is a discount amount, and the third price is the price paid for the product.</p><p id="p-0362" num="0354">Example 17 includes at least one non-transitory computer readable storage medium comprising instructions that, when executed, cause processor circuitry to at least crop an image of a receipt based on detected regions of interest, apply a first mask to a first cropped image of the receipt to generate first bounding boxes, the first bounding boxes corresponding to rows of the receipt, apply a second mask to a second cropped image of the receipt to generate second bounding boxes, the second bounding boxes corresponding to columns of the receipt, form a structure of the receipt by mapping words detected by an optical character recognition (OCR) engine to corresponding first bounding boxes and second bounding boxes based on a mapping criterion, classify the second bounding boxes corresponding to the columns by identifying an expression of interest in ones of the second bounding boxes, and generate purchase information by detecting and extracting text of interest from the structured receipt based on the classifications, wherein the purchase information includes purchased items listed in the receipt.</p><p id="p-0363" num="0355">Example 18 includes the at least one non-transitory computer readable storage medium of example <b>17</b>, wherein the regions of interest include (1) a receipt region corresponding to the receipt, and (2) a purchase region corresponding to an area of the receipt that includes information about the purchased items in the receipt.</p><p id="p-0364" num="0356">Example 19 includes the at least one non-transitory computer readable storage medium of any of examples 17-18, wherein the first cropped image is the receipt region and the second cropped image is the purchase region.</p><p id="p-0365" num="0357">Example 20 includes the at least one non-transitory computer readable storage medium of any of examples 17-19, wherein the first mask identifies ones of a plurality of pixels of the first cropped image as belonging to a first class or a second class.</p><p id="p-0366" num="0358">Example 21 includes the at least one non-transitory computer readable storage medium of any of examples 17-20, wherein the first class is a text line and the second class is background.</p><p id="p-0367" num="0359">Example 22 includes the at least one non-transitory computer readable storage medium of any of examples 17-21, wherein the second mask identifies ones of a plurality of pixels of the second cropped image as belonging to a first class or a second class.</p><p id="p-0368" num="0360">Example 23 includes the at least one non-transitory computer readable storage medium of any of examples 17-22, wherein the first class is a column and the second class is another class.</p><p id="p-0369" num="0361">Example 24 includes the at least one non-transitory computer readable storage medium of any of examples 17-23, wherein the instructions, when executed, cause the processor circuitry to merge a first one of the first bounding boxes and a second one of the first bounding boxes based on a row connection criterion, the first one and the second one of the first bounding boxes to satisfy a length criterion.</p><p id="p-0370" num="0362">Example 25 includes the at least one non-transitory computer readable storage medium of any of examples 17-24, wherein the instructions, when executed, cause the processor circuitry to merge a first one of the second bounding boxes and a second one of the second bound boxes based on a column connection criterion.</p><p id="p-0371" num="0363">Example 26 includes the at least one non-transitory computer readable storage medium of any of examples 17-25, wherein the expression of interest corresponds to a targeted fact.</p><p id="p-0372" num="0364">Example 27 includes the at least one non-transitory computer readable storage medium of any of examples 17-26, wherein the targeted fact includes at least one of an item description, a quantity, or a price.</p><p id="p-0373" num="0365">Example 28 includes the at least one non-transitory computer readable storage medium of any of examples 17-27, wherein the instructions, when executed, cause the processor circuitry to the purchase information includes purchase details and promotion information, and wherein the processor circuitry is to execute the machine readable instructions to remove words from the first bounding boxes that do not correspond to the classification of respective second bounding boxes prior to generating the purchase details.</p><p id="p-0374" num="0366">Example 29 includes the at least one non-transitory computer readable storage medium of any of examples 17-28, wherein the purchase details include, for ones of the purchased products in the receipt, a product description, a price, and a quantity.</p><p id="p-0375" num="0367">Example 30 includes the at least one non-transitory computer readable storage medium of any of examples 17-29, wherein the instructions, when executed, cause the processor circuitry to transmit the purchase details to decoding circuitry.</p><p id="p-0376" num="0368">Example 31 includes the at least one non-transitory computer readable storage medium of any of examples 17-30, wherein the instructions, when executed, cause the processor circuitry to generate the promotion information by determining whether an item is associated with a promotion, in response to determining the item is associated with the promotion: identifying a first price associated with the item, the first price corresponding to a price column, identifying a second price associated with the item, the second price corresponding to the promotion, and calculating a third price based on the first price and the second price.</p><p id="p-0377" num="0369">Example 32 includes the at least one non-transitory computer readable storage medium of any of examples 17-31, wherein the first price is an original price of the item, the second price is a discount amount, and the third price is the price paid for the item.</p><p id="p-0378" num="0370">Example 33 includes a method comprising cropping, by executing machine readable instructions with at least one processor, an image of a receipt based on detected regions of interest, applying, by executing machine readable instructions with the least one processor, a first mask to a first cropped image of the receipt to generate first bounding boxes, the first bounding boxes corresponding to rows of the receipt, applying, by executing machine readable instructions with the least one processor, a second mask to a second cropped image of the receipt to generate second bounding boxes, the second bounding boxes corresponding to columns of the receipt, generating, by executing machine readable instructions with the least one processor, a structure of the receipt by mapping words detected by an optical character recognition (OCR) engine to corresponding first bounding boxes and second bounding boxes based on a mapping criterion, classifying, by executing machine readable instructions with the least one processor, the second bounding boxes corresponding to the columns by identifying an expression of interest in ones of the second bounding boxes, and generating, by executing machine readable instructions with the least one processor, purchase information by detecting and extracting text of interest from the structured receipt based on the classifications, wherein the purchase information includes purchased products listed in the receipt.</p><p id="p-0379" num="0371">Example 34 includes the method of example 33, wherein the regions of interest include (1) a receipt region corresponding to the receipt, and (2) a purchase region corresponding to an area of the receipt that includes information about the purchased products in the receipt.</p><p id="p-0380" num="0372">Example 35 includes the method of any of examples 33-34, wherein the first cropped image is the receipt region and the second cropped image is the purchase region.</p><p id="p-0381" num="0373">Example 36 includes the method of any of examples 33-35, wherein the first mask identifies ones of a plurality of pixels of the first cropped image as belonging to a first class or a second class.</p><p id="p-0382" num="0374">Example 37 includes the method of any of examples 33-36, wherein the first class is a text line and the second class is background.</p><p id="p-0383" num="0375">Example 38 includes the method of any of examples 33-37, wherein the second mask identifies ones of a plurality of pixels of the second cropped image as belonging to a first class or a second class.</p><p id="p-0384" num="0376">Example 39 includes the method of any of examples 33-38, wherein the first class is a column and the second class is another class.</p><p id="p-0385" num="0377">Example 40 includes the method of any of examples 33-39, further including merging a first one of the first bounding boxes and a second one of the first bounding boxes based on a row connection criterion, the first one and the second one of the first bounding boxes to satisfy a length criterion.</p><p id="p-0386" num="0378">Example 41 includes the method of any of examples 33-40, further including merging a first one of the second bounding boxes and a second one of the second bound boxes based on a column connection criterion.</p><p id="p-0387" num="0379">Example 42 includes the method of any of examples 33-41, wherein the expression of interest corresponds to a targeted fact.</p><p id="p-0388" num="0380">Example 43 includes the method of any of examples 33-42, wherein the targeted fact includes at least one of a product description, a quantity, or a price.</p><p id="p-0389" num="0381">Example 44 includes the method of any of examples 33-43, wherein the purchase information includes purchase details and promotion information, the method further including removing words from the first bounding boxes that do not correspond to the classification of respective second bounding boxes prior to generating the purchase details.</p><p id="p-0390" num="0382">Example 45 includes the method of any of examples 33-44, wherein the purchase details include, for ones of the purchased products in the receipt, a product description, a price, and a quantity.</p><p id="p-0391" num="0383">Example 46 includes the method of any of examples 33-45, further including transmitting the purchase details to decoding circuitry.</p><p id="p-0392" num="0384">Example 47 includes the method of any of examples 33-46, wherein the generating the promotion information includes determining whether a product is associated with a promotion, in response to determining the product is associated with the promotion: identifying a first price associated with the product, the first price corresponding to a price column, identifying a second price associated with the product, the second price corresponding to the promotion, and calculating a third price based on the first price and the second price.</p><p id="p-0393" num="0385">Example 48 includes the method of any of examples 33-47, wherein the first price is an original price of the product, the second price is a discount amount, and the third price is the price paid for the product.</p><p id="p-0394" num="0386">The following claims are hereby incorporated into this Detailed Description by this reference. Although certain example systems, methods, apparatus, and articles of manufacture have been disclosed herein, the scope of coverage of this patent is not limited thereto. On the contrary, this patent covers all systems, methods, apparatus, and articles of manufacture fairly falling within the scope of the claims of this patent.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An apparatus comprising:<claim-text>memory;</claim-text><claim-text>interface circuitry; and</claim-text><claim-text>processor circuitry to execute machine readable instructions to at least:<claim-text>crop an image of a receipt based on detected regions of interest;</claim-text><claim-text>apply a first mask to a first cropped image of the receipt to generate first bounding boxes, the first bounding boxes corresponding to rows of the receipt;</claim-text><claim-text>apply a second mask to a second cropped image of the receipt to generate second bounding boxes, the second bounding boxes corresponding to columns of the receipt;</claim-text><claim-text>generate a structure of the receipt by mapping words detected by an optical character recognition (OCR) engine to corresponding first bounding boxes and second bounding boxes based on a mapping criterion;</claim-text><claim-text>classify the second bounding boxes corresponding to the columns by identifying an expression of interest in ones of the second bounding boxes; and</claim-text><claim-text>generate purchase information by detecting and extracting text of interest from the structured receipt based on the classifications, wherein the purchase information includes purchased products listed in the receipt.</claim-text></claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the regions of interest include (1) a receipt region corresponding to the receipt, and (2) a purchase region corresponding to an area of the receipt that includes information about the purchased products in the receipt.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The apparatus of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the first cropped image is the receipt region and the second cropped image is the purchase region.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first mask identifies ones of a plurality of pixels of the first cropped image as belonging to a first class or a second class.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The apparatus of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the first class is a text line and the second class is background.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the second mask identifies ones of a plurality of pixels of the second cropped image as belonging to a first class or a second class.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The apparatus of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the first class is a column and the second class is another class.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the processor circuitry is to execute the machine readable instructions to merge a first one of the first bounding boxes and a second one of the first bounding boxes based on a row connection criterion, the first one and the second one of the first bounding boxes to satisfy a length criterion.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the processor circuitry is to execute the machine readable instructions to merge a first one of the second bounding boxes and a second one of the second bound boxes based on a column connection criterion.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the expression of interest corresponds to a targeted fact.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The apparatus of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the targeted fact includes at least one of a product description, a quantity, or a price.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the purchase information includes purchase details and promotion information, and wherein the processor circuitry is to execute the machine readable instructions to remove words from the first bounding boxes that do not correspond to the classification of respective second bounding boxes prior to generating the purchase details.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The apparatus of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the purchase details include, for ones of the purchased products in the receipt, a product description, a price, and a quantity.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The apparatus of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the processor circuitry executes the machine readable instructions to transmit the purchase details to decoding circuitry.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The apparatus of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein to generate the promotion information, the processor circuitry is to execute the machine readable instructions to:<claim-text>determine whether a product is associated with a promotion;</claim-text><claim-text>in response to determining the product is associated with the promotion:<claim-text>identify a first price associated with the product, the first price corresponding to a price column;</claim-text><claim-text>identify a second price associated with the product, the second price corresponding to the promotion; and</claim-text><claim-text>calculate a third price based on the first price and the second price.</claim-text></claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The apparatus of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the first price is an original price of the product, the second price is a discount amount, and the third price is the price paid for the product.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. At least one non-transitory computer readable storage medium comprising instructions that, when executed, cause processor circuitry to at least:<claim-text>crop an image of a receipt based on detected regions of interest;</claim-text><claim-text>apply a first mask to a first cropped image of the receipt to generate first bounding boxes, the first bounding boxes corresponding to rows of the receipt;</claim-text><claim-text>apply a second mask to a second cropped image of the receipt to generate second bounding boxes, the second bounding boxes corresponding to columns of the receipt;</claim-text><claim-text>form a structure of the receipt by mapping words detected by an optical character recognition (OCR) engine to corresponding first bounding boxes and second bounding boxes based on a mapping criterion;</claim-text><claim-text>classify the second bounding boxes corresponding to the columns by identifying an expression of interest in ones of the second bounding boxes; and</claim-text><claim-text>generate purchase information by detecting and extracting text of interest from the structured receipt based on the classifications, wherein the purchase information includes purchased items listed in the receipt.</claim-text></claim-text></claim><claim id="CLM-18-23canceled" num="18-23canceled"><claim-text><b>18</b>.-<b>23</b>.(canceled)</claim-text></claim><claim id="CLM-00024" num="00024"><claim-text><b>24</b>. The at least one non-transitory computer readable storage medium as defined in <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the instructions, when executed, cause the processor circuitry to merge a first one of the first bounding boxes and a second one of the first bounding boxes based on a row connection criterion, the first one and the second one of the first bounding boxes to satisfy a length criterion.</claim-text></claim><claim id="CLM-00025" num="00025"><claim-text><b>25</b>. (canceled)</claim-text></claim><claim id="CLM-00026" num="00026"><claim-text><b>26</b>. The at least one non-transitory computer readable storage medium as defined in <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the expression of interest corresponds to a targeted fact.</claim-text></claim><claim id="CLM-27-32" num="27-32"><claim-text><b>27</b>.-<b>32</b>. (canceled)</claim-text></claim><claim id="CLM-00033" num="00033"><claim-text><b>33</b>. A method comprising:<claim-text>cropping, by executing machine readable instructions with at least one processor, an image of a receipt based on detected regions of interest;</claim-text><claim-text>applying, by executing machine readable instructions with the least one processor, a first mask to a first cropped image of the receipt to generate first bounding boxes, the first bounding boxes corresponding to rows of the receipt;</claim-text><claim-text>applying, by executing machine readable instructions with the least one processor, a second mask to a second cropped image of the receipt to generate second bounding boxes, the second bounding boxes corresponding to columns of the receipt;</claim-text><claim-text>generating, by executing machine readable instructions with the least one processor, a structure of the receipt by mapping words detected by an optical character recognition (OCR) engine to corresponding first bounding boxes and second bounding boxes based on a mapping criterion;</claim-text><claim-text>classifying, by executing machine readable instructions with the least one processor, the second bounding boxes corresponding to the columns by identifying an expression of interest in ones of the second bounding boxes; and</claim-text><claim-text>generating, by executing machine readable instructions with the least one processor, purchase information by detecting and extracting text of interest from the structured receipt based on the classifications, wherein the purchase information includes purchased products listed in the receipt.</claim-text></claim-text></claim><claim id="CLM-34-48" num="34-48"><claim-text><b>34</b>.-<b>48</b>. (canceled)</claim-text></claim></claims></us-patent-application>