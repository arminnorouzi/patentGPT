<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005264A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005264</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17932360</doc-number><date>20220915</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>40</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>776</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>82</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>774</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>41</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>776</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>82</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>774</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">METHOD FOR VIDEO RECOGNITION AND RELATED PRODUCTS</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>PCT/CN2021/083326</doc-number><date>20210326</date></document-id><parent-status>PENDING</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>17932360</doc-number></document-id></child-doc></relation></continuation><us-provisional-application><document-id><country>US</country><doc-number>63000011</doc-number><date>20200326</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Guangdong Oppo Mobile Telecommunications Corp., Ltd.</orgname><address><city>Dongguan</city><country>CN</country></address></addressbook><residence><country>CN</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Hsiao</last-name><first-name>Jenhao</first-name><address><city>Palo Alto</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Chen</last-name><first-name>Jiawei</first-name><address><city>Palo Alto</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A method for video recognition and related products are provided. The method includes the following. An original set of clip descriptors is obtained by providing multiple clips of a video as an input of a 3D CNN of a neural network, where the neural network includes the 3D CNN and at least one first fully connected layer, and each of the multiple clips includes at least one frame. An attention vector corresponding to the original set of clip descriptors is determined. An enhanced set of clip descriptors is obtained based on the original set of clip descriptors and the attention vector. The enhanced set of clip descriptors is input into the at least one first fully connected layer and video recognition is performed based on an output of the at least one first fully connected layer.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="183.73mm" wi="158.16mm" file="US20230005264A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="193.29mm" wi="160.19mm" file="US20230005264A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="139.11mm" wi="141.31mm" file="US20230005264A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="241.05mm" wi="59.52mm" orientation="landscape" file="US20230005264A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="174.41mm" wi="131.32mm" file="US20230005264A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="131.15mm" wi="165.69mm" file="US20230005264A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="131.15mm" wi="165.78mm" file="US20230005264A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION(S)</heading><p id="p-0002" num="0001">This application is a continuation of International Application No. PCT/CN2021/083326, filed Mar. 26, 2021, which claims priority to U.S. Provisional Application No. 63/000,011, filed Mar. 26, 2020, the entire disclosures of which are incorporated herein by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">This disclosure relates to the field of video recognition technology, and more particularly to a method for video recognition and related products.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">Most existing video recognition techniques rely on trimmed videos as their inputs. However, most videos, even for short videos, in the real world are untrimmed and contain large numbers of irrelevant frames. One of popular methods is to directly use a central clip in a video as the input for video recognition, which assume that the central clip is the most related event and can be served as a cleanly-trimmed video. While the other method uniformly segments a video and take average of all output for video class prediction, which is usually complex and needs to process all video segments in the video.</p><p id="p-0005" num="0004">How to extracting salient information in the video accurately and quickly has been a problem to be solved.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0006" num="0005">In a first aspect, a method for video recognition is provided. The method for video recognition includes the following. An original set of clip descriptors is obtained by providing multiple clips of a video as an input of a three-dimensional (3D) convolutional neural network (CNN) of a neural network, where the neural network includes the 3D CNN and at least one first fully connected layer, and each of the multiple clips includes at least one frame. An attention vector corresponding to the original set of clip descriptors is determined. An enhanced set of clip descriptors is obtained based on the original set of clip descriptors and the attention vector. The enhanced set of clip descriptors is input into the at least one first fully connected layer and video recognition is performed based on an output of the at least one first fully connected layer.</p><p id="p-0007" num="0006">In a second aspect, a method for training a neural network is provided. The method for training a neural network includes the following. An original set of clip descriptors is obtained by providing multiple clips of a video as an input of a 3D CNN of a neural network, where the neural network includes the 3D CNN and at least one first fully connected layer, the 3D CNN includes at least one convolutional layer and at least one second fully connected layer, and each of the multiple clips includes at least one frame. An attention vector corresponding to the original set of clip descriptors is determined. An enhanced set of clip descriptors is obtained based on the original set of clip descriptors and the attention vector. The enhanced set of clip descriptors is input into the at least one first fully connected layer and an output of the neural network is obtained. The neural network is trained by updating parameters of the neural network based on a loss of the neural network, where the parameters of the network include a weight of the at least one first fully connected layer and a weight of the at least one second fully connected layer.</p><p id="p-0008" num="0007">In a third aspect, an apparatus for video recognition is provided. The apparatus is based on a neural network. The apparatus includes at least one processor and a memory coupled with the at least one processor. The memory is configured to store instructions which, when executed by the at least one processor, are operable with the processor to implement the neural network to: obtain an original set of clip descriptors by providing multiple clips of a video as an input of a 3D CNN of a neural network, where the neural network includes the 3D CNN and at least one first fully connected layer, and each of the multiple clips includes at least one frame; determine an attention vector corresponding to the original set of clip descriptors; obtain an enhanced set of clip descriptors based on the original set of clip descriptors and the attention vector; input the enhanced set of clip descriptors into the at least one first fully connected layer and perform video recognition based on an output of the at least one first fully connected layer.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0009" num="0008">In order to describe technical solutions of implementations more clearly, the following will give a brief description of accompanying drawings used for describing the implementations. Apparently, accompanying drawings described below are merely some implementations. Those of ordinary skill in the art can also obtain other accompanying drawings based on the accompanying drawings described below without creative efforts.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram of a framework of a neural network according to implementations.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a schematic flowchart of a method for video recognition according to implementations.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates an example recognition result according to implementations.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a schematic flowchart of a method for training a network according to implementations.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is schematic structural diagram of an apparatus for video recognition according to implementations.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is schematic structural diagram of an apparatus for training a neural network according to implementations.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0016" num="0015">In order for those skilled in the art to better understand technical solutions of implementations, technical solutions of implementations will be described clearly and completely with reference to accompanying drawings in the implementations. Apparently, implementations hereinafter described are merely some implementations, rather than all implementations, of the disclosure. All other implementations obtained by those of ordinary skill in the art based on the implementations herein without creative efforts shall fall within the protection scope of the disclosure.</p><p id="p-0017" num="0016">The terms &#x201c;first&#x201d;, &#x201c;second&#x201d;, &#x201c;third&#x201d;, and the like used in the specification, the claims, and the accompany drawings of the disclosure are used to distinguish different objects rather than describe a particular order. The terms &#x201c;include&#x201d;, &#x201c;comprise&#x201d;, and &#x201c;have&#x201d; as well as variations thereof are intended to cover non-exclusive inclusion. For example, a process, method, system, product, or apparatus including a series of steps or units is not limited to the listed steps or units. Instead, it can optionally include other steps or units that are not listed; alternatively, other steps or units inherent to the process, method, product, or apparatus can also be included.</p><p id="p-0018" num="0017">The term &#x201c;implementation&#x201d; referred to herein means that a particular feature, structure, or character described in conjunction with the implementation may be contained in at least one implementation of the disclosure. The phrase appearing in various places in the specification does not necessarily refer to the same implementation, nor does it refer to an independent or alternative implementation that is mutually exclusive with other implementations. It is explicitly and implicitly understood by those skilled in the art that an implementation described herein may be combined with other implementations.</p><p id="p-0019" num="0018">A mobile terminal referred to herein may include various handheld devices, in-vehicle devices, wearable devices, computing devices that have wireless communication functions or other processing devices connected to a wireless modem, as well as various forms of user equipment (UE), mobile stations (MS), terminal devices, and the like. For ease of description, the above- mentioned devices are collectively referred to as a mobile terminal.</p><p id="p-0020" num="0019">Hereinafter, detailed description of implementations of the present disclosure will be given below.</p><p id="p-0021" num="0020">The neural network in the present disclosure includes a three-dimensional (3D) convolutional neural network (CNN) and at least one first fully connected layer, where the 3D CNN includes at least one convolutional layer and at least one second fully connected layer. In other words, the neural network includes one or more convolutional layers and one or more fully connected layers, the one or more convolutional layers and part of the one or more fully connected layers form the 3D CNN, and the 3D CNN and the other part of the one or more fully connected layers form the entire neural network.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram of a framework of a neural network according to implementations. As illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, an original set of clip descriptors is obtained by providing multiple clips of a video as an input of a 3D CNN of a neural network. An attention vector corresponding to the original set of clip descriptors is determined. An enhanced set of clip descriptors is obtained based on the original set of clip descriptors and the attention vector. The enhanced set of clip descriptors is input into the at least one first fully connected layer and video recognition is performed based on an output of the at least one first fully connected layer. Detailed description of the framework of the neural network will be described in detail with reference of following method implementations.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a schematic flowchart of a method for video recognition according to implementations. As illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, an original set of clip descriptors is obtained by providing multiple clips of a video as an input of a 3D CNN of a neural network, an attention vector corresponding to the original set of clip descriptors is determined, an enhanced set of clip descriptors is obtained based on the original set of clip descriptors and the attention vector and input into the at least one first fully connected layer, and video recognition is performed based on an output of the at least one first fully connected layer. The following are described in detail.</p><p id="p-0024" num="0023">S<b>202</b>, an original set of clip descriptors is obtained by providing multiple clips of a video as an input of a 3D CNN of a neural network.</p><p id="p-0025" num="0024">The neural network includes the 3D CNN and at least one fully connected layer, and each of the multiple clips includes at least one frame. The at least one fully connected layer includes at least one first fully connected layer and at least one second fully connected layer. The at least one convolutional layer and the at least one second fully connected layer form the 3D CNN. The 3D CNN and the at least one first fully connected layer form entire the neural network in the present disclosure. For example, multiple clips X={x<sub>1</sub>, x<sub>2</sub>, . . . , x<sub>c</sub>} can be set as an input of the neural network and each clip x may contain at least one frame, such as 16 stacked frames. When the multiple clips are input into the neural network, each clip will be firstly processed by the 3D CNN of the neural network. The 3D CNN may contain a set of 3D convolutional layers, which are used to extract clip features corresponding to the multiple clips, and a set of first fully connected layers. The input shape for one batch data (i.e., the shape for the input of the 3D CNN of the neural network) can be represented as C&#xd7;T&#xd7;H&#xd7;W&#xd7;ch, where C denotes the number of clips, T frames are stacked together to form a clip, each of the T frames has a height H and a width W. ch denotes the channel number, and in the present disclosure, ch is 3 for RGB images. A convolutional kernel for each 3D convolutional layer in the 3D CNN is in three dimensions, being k&#xd7;k&#xd7;k. Then for each 3D convolutional layer, data will be computed among three dimensions simultaneously. The output of 3D CNN is the original set of clip descriptors V={v<sub>1</sub>, v<sub>2</sub>, . . . , v<sub>c</sub>}, where v &#x2208; R<sup>D </sup>and v is the output of last second fully connected layer of 3D CNN. D denotes the number of convolution kernels, for example, D=2048.</p><p id="p-0026" num="0025">S<b>204</b>, an attention vector corresponding to the original set of clip descriptors is determined.</p><p id="p-0027" num="0026">Since each clip descriptor are produced by the 3D CNN (i.e., a 3D CNN module) separately, inter-clip relationships modelled by convolution are inherently implicit and local. That is, each clip descriptor can only observe an extremely limited local event and there are no inter-clip relationships. However, duration of different actions in the video are variant and complex actions could across multiple video segments (i.e., multiple clips can be involved), no inter-clip relationships among clip descriptors produced by 3D CNN becomes a performance bottleneck for video recognition. To mitigate this problem that each clip descriptor is unable to exploit contextual information outside of its scope, an attention mechanism is used to alleviate this limitation. By means of the attention vector in the present disclosure, clip interdependencies can be explicitly modelled, the learning of convolutional features can be enhanced, so that the neural network is able to increase its sensitivity to informative features across segments and can deliver a better decision of video recognition based on a global view. The attention vector includes multiple attention weights. Each of the multiple clips correspond to each clip descriptor in the original set of clip descriptors in one-to-one correspondence, and each of the attention weights correspond to each clip descriptor in the original set of clip descriptors in one-to-one correspondence. For example, the input is N clips of a video, then the original set of clip descriptors has N clip descriptors, and the attention vector includes N elements (i.e., N attention weights). There is a one-to-one-to-one mapping relationship among N clips, N clip descriptors, and N attention weights.</p><p id="p-0028" num="0027">S<b>205</b>, an enhanced set of clip descriptors is obtained based on the original set of clip descriptors and the attention vector.</p><p id="p-0029" num="0028">The enhanced set of clip descriptors is obtained by rescaling the original set of clip descriptors with the attention vector (i.e., activations). The attention vector intrinsically introduces dynamics conditioned on the original set of clip descriptors. The attention vector can be regarded as a self-attention function on clips whose relationships are not confined to the local receptive field 3D convolutional filters are responsive to. By means of the attention vector, each clip descriptor in the original set of clip descriptors corresponds to an attention weight, therefore by processing each clip descriptor in the original set of clip descriptors with a corresponding attention weight, uniformly averaging all clips can be avoided and video accuracy can be improved.</p><p id="p-0030" num="0029">S<b>208</b>, the enhanced set of clip descriptors is inputting into the at least one first fully connected layer and video recognition is performed based on an output of the at least one first fully connected layer.</p><p id="p-0031" num="0030">By means of the implementation of the present disclosure, multiple clips of a video are obtained as an input, original set of clip descriptors is obtained by inputting the multiple clips into a 3D CNN of a neural network, an attention vector corresponding to the original set of clip descriptors is determined, an enhanced set of clip descriptors is obtained based on the original set of clip descriptors and the attention vector and then input into the at least one first fully connected layer, and finally video recognition is performed based on an output of the at least one first fully connected layer. By means of the attention vector, each clip descriptor in the original set of clip descriptors corresponds to an attention weight, therefore by processing each clip descriptor in the original set of clip descriptors with a corresponding attention weight, uniformly averaging all clips can be avoided and video accuracy can be improved.</p><p id="p-0032" num="0031">As an implementation, the attention vector corresponding to the original set of clip descriptors is determined as follows.</p><p id="p-0033" num="0032">A first vector is obtained by performing a global average pooling on the original set of clip descriptors. The attention vector is obtained by employing a gating mechanism on the first vector based on a weight of the at least one second fully connected layer, where the 3D CNN includes at least one convolutional layer and the at least one second fully connected layer.</p><p id="p-0034" num="0033">In order to obtain the attention vector, clip-wise statistics that is the first vector is first generated by global average pooling. The pooled output can be interpreted as a collection of local descriptors (i.e., the original set of clip descriptors) whose statistics are expressive for the whole clip. The first vector can be defined as:</p><p id="p-0035" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mrow>  <mrow>   <mi>g</mi>   <mo>=</mo>   <mrow>    <mo>[</mo>    <mrow>     <mrow>      <mi>g</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <msub>       <mi>v</mi>       <mn>1</mn>      </msub>      <mo>)</mo>     </mrow>     <mo>,</mo>     <mrow>      <mi>g</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <msub>       <mi>v</mi>       <mn>1</mn>      </msub>      <mo>)</mo>     </mrow>     <mo>,</mo>     <mo>&#x2026;</mo>     <mtext>   </mtext>     <mo>,</mo>     <mrow>      <mi>g</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <msub>       <mi>v</mi>       <mi>c</mi>      </msub>      <mo>)</mo>     </mrow>    </mrow>    <mo>]</mo>   </mrow>  </mrow>  <mo>,</mo>  <mtext></mtext>  <mrow>   <mrow>    <mi>g</mi>    <mo>&#x2061;</mo>    <mo>(</mo>    <mi>v</mi>    <mo>)</mo>   </mrow>   <mo>=</mo>   <mrow>    <mfrac>     <mn>1</mn>     <mi>D</mi>    </mfrac>    <mo>&#x2062;</mo>    <mrow>     <munder>      <mo>&#x2211;</mo>      <mi>i</mi>     </munder>     <msup>      <mi>v</mi>      <mi>i</mi>     </msup>    </mrow>   </mrow>  </mrow> </mrow></math></maths></p><p id="p-0036" num="0034">where v<sup>i </sup>is the i-th element of a clip descriptor. As stated above, D denotes the number of convolution kernels, when D=2048, the first vector can be defined as:</p><p id="p-0037" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mrow>  <mrow>   <mi>g</mi>   <mo>&#x2061;</mo>   <mo>(</mo>   <mi>v</mi>   <mo>)</mo>  </mrow>  <mo>=</mo>  <mrow>   <mfrac>    <mn>1</mn>    <mrow>     <mn>2</mn>     <mo>&#x2062;</mo>     <mn>0</mn>     <mo>&#x2062;</mo>     <mn>4</mn>     <mo>&#x2062;</mo>     <mn>8</mn>    </mrow>   </mfrac>   <mo>&#x2062;</mo>   <mrow>    <munder>     <mo>&#x2211;</mo>     <mi>i</mi>    </munder>    <msup>     <mi>v</mi>     <mi>i</mi>    </msup>   </mrow>  </mrow> </mrow></math></maths></p><p id="p-0038" num="0035">Then a second operation which aims to fully capture clip-wise dependencies is performed. To fulfil this objective, a function used in the second operation shall meet two criteria: first, it shall be capable of learning a nonlinear interaction between clips; second, it shall learn a non-mutually-exclusive relationship since it needs to be ensured that multiple clips are allowed to be emphasized. To meet these criteria, a gating mechanism with a sigmoid activation is employed.</p><p id="p-0039" num="0036">As an implementation, the attention vector is obtained by employing the gating mechanism on the first vector based on the weight of the at least one second fully connected layer as follows.</p><p id="p-0040" num="0037">The first vector (i.e., g) is multiplied by a first weight (i. e., w<sub>1</sub>) of the at least one second fully connected layer to obtain a second vector (i. e., w<sub>1</sub>g). The second vector is processed based on a rectified linear unit (ReLU) function to obtain a third vector (i.e., &#x3c3;<sub>ReLU</sub>(w<sub>1</sub>g)). The third vector is multiplied by a second weight of the at least one second fully connected layer (i.e., w<sub>2</sub>) to obtain a fourth vector (i.e., w<sub>2</sub>&#x3c3;<sub>ReLU</sub>(w<sub>1</sub>g)). The fourth vector is processed based on an activation function to obtain the attention vector (i.e., &#x3c3;<sub>sigmoid</sub>(w<sub>2</sub>&#x3c3;<sub>ReLU</sub>(w<sub>1</sub>g))).</p><p id="p-0041" num="0038">The attention vector can be defined as:</p><p id="p-0042" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>att=&#x3c3;<sub>sigmoid</sub>(<i>w</i><sub>2</sub>&#x3c3;<sub>ReLU</sub>(<i>w</i><sub>1</sub><i>g</i>)),<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0043" num="0039">where &#x3c3;<sub>ReLU </sub>refers to a rectified linear unit (ReLU) function, w<sub>1 </sub>and w<sub>2 </sub>are weights of second fully connected layer of 3D CNN, and &#x3c3;<sub>sigmoid </sub>is a sigmoid function. In this way, the attention vector is defined in a clip-agnostic way, which is useful to identify segments relevant to the action of interest and estimate temporal intervals of detected actions.</p><p id="p-0044" num="0040">The enhanced set of clip descriptors can be obtained based on the original set of clip descriptors and the attention vector in many ways.</p><p id="p-0045" num="0041">As an implementation, the enhanced set of clip descriptors is obtained based on the original set of clip descriptors and the attention vector as follows.</p><p id="p-0046" num="0042">The enhanced set of clip descriptors is obtained by multiplying the original set of clip descriptors by the attention vector.</p><p id="p-0047" num="0043">The enhanced set of clip descriptors can be defined as:</p><p id="p-0048" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>S={s<sub>1</sub>, s<sub>2</sub>, . . . , s<sub>c</sub>},<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0049" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>s</i><sub>c</sub>=att<sub>c</sub><i>&#xd7;v</i><sub>c </sub><?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0050" num="0044">As an implementation, the enhanced set of clip descriptors can also be obtained based on the original set of clip descriptors and the attention vector as follows.</p><p id="p-0051" num="0045">A first set of clip descriptors is obtained by multiplying the original set of clip descriptors by the attention vector S. The enhanced set of clip descriptors R is obtained by adding the first set of clip descriptors to the original set of clip descriptors (i.e., R=V+S).</p><p id="p-0052" num="0046">To make the learning more robust and effective, the enhanced set of clip descriptors can be defined as:</p><p id="p-0053" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>R=V+S, </i><?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0054" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>S={s<sub>1</sub>, s<sub>2</sub>, . . . , s<sub>c</sub>},<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0055" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>s</i><sub>c</sub>=att<sub>c</sub><i>&#xd7;v</i><sub>c</sub>,<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0056" num="0047">where R={r<sub>1</sub>, r<sub>2</sub>, . . . , r<sub>c</sub>} can be considered as the enhanced set of clip descriptors in other words, R can be considered as refined clip descriptors. As an implementation, the enhanced set of clip descriptors is input into the at least one first fully connected layer and video recognition is performed based on the output of the at least one first fully connected layer as follows.</p><p id="p-0057" num="0048">A fifth vector denoted by v&#x2032;, is determined based on the enhanced set of clip descriptors. The fifth vector v&#x2032; can be defined as:</p><p id="p-0058" num="0000"><maths id="MATH-US-00003" num="00003"><math overflow="scroll"> <mrow>  <msup>   <mi>v</mi>   <mo>&#x2032;</mo>  </msup>  <mo>=</mo>  <mrow>   <mo>&#x2211;</mo>   <mrow>    <msub>     <mi>r</mi>     <mi>i</mi>    </msub>    <mo>/</mo>    <mi>C</mi>   </mrow>  </mrow> </mrow></math></maths></p><p id="p-0059" num="0049">The output of the at least one first fully connected layer is obtained by multiplying the fifth vector by a weight of the at least one first fully connected layer (i.e., w<sub>3</sub>v&#x2032;). An output of the neural network which is used for video recognition is obtained by processing the output of the at least one first fully connected layer based on a SoftMax function. The output of the neural network, based on v&#x2032;, can be defined as:</p><p id="p-0060" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>output=&#x3c3;<sub>softmax</sub>(<i>w</i><sub>3</sub><i>v&#x2032;</i>).<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0061" num="0050">where w<sub>3 </sub>is the weight of the at least one first fully connected layer, and &#x3c3;<sub>softmax </sub>is a SoftMax function.</p><p id="p-0062" num="0051">As an implementation, the method further includes the following.</p><p id="p-0063" num="0052">Parameters of the neural network are obtained based on a loss, where parameters of the neural network include the weight of the at least one first fully connected layer and the weight of the at least one second fully connected layer, the weight of the at least one second fully connected layer includes the first weight of the at least one second fully connected layer and the second weight of the at least one second fully connected layer, the loss includes a classification loss corresponding to an output of the neural network and a sparsity loss corresponding to the attention vector.</p><p id="p-0064" num="0053">As an implementation, the classification loss is based on a standard cross-entropy loss between a ground truth corresponding to the input and the output of the neural network corresponding to the input, and the sparsity loss is obtained by performing L1 norm on the attention vector.</p><p id="p-0065" num="0054">A loss function in the proposed neural network is composed of two terms, the classification loss and the sparsity loss, which is given by:</p><p id="p-0066" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>L=L</i><sub>c</sub><i>+&#x3b2;L</i><sub>s </sub><?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0067" num="0055">where L<sub>c </sub>(i.e., L<sub>class</sub>) denotes the classification loss computed on video-level class labels, L<sub>s </sub>(i.e., L<sub>sparsitr</sub>) is the sparsity loss on the attention vector, and &#x3b2; is a constant to control the trade-off between the two terms. The classification loss is based on the standard cross-entropy loss between ground truth and the output (after passing through the proposed neural network as illustrated in previous section), while the sparsity loss is given by the L1 norm on attention vector att, and the attention vector can be defined as:</p><p id="p-0068" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>L<sub>s</sub>=&#x2225;att&#x2225;<sub>1 </sub><?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0069" num="0056">Because of the use of the sigmoid function and the L1 loss, all attention weights in the attention vector tend to have values close to either 0 or 1. In this case, an action can be recognized with a sparse subset of key segments in a video, which will help locating relevant clips for action detection.</p><p id="p-0070" num="0057">In the present disclosure, a novel weakly supervised temporal action recognition and localization algorithm based on an end-to-end deep neural network are proposed. Comparing to other method in the related art, the present disclosure provides the following advantages.</p><p id="p-0071" num="0058">First, recognition accuracy is improved.</p><p id="p-0072" num="0059">Classification is performed by evaluating a video-level representation given by a carefully designed neural network architecture that fully employ inter-clip relationships to better describe an action event with variant length (e.g., across several clips) and thus boost the overall recognition accuracy.</p><p id="p-0073" num="0060">To prove the effectiveness of the present disclosure, Table 1 shows accuracy comparison of different methods in Kinetics-600, which consists 600 action classes and contains around 20k videos for validation. As can be seen that in traditional method, which assume that the central clip is the most related event and directly use it as the input, can achieve the poorest 58.58% top-1 accuracy. The poor accuracy is mainly due to the lack of fully utilizing the information in the video (e.g., the rest relevant clips). Naive average of clips is another popular method, but it can only achieve 65.3% top-1 accuracy. Since an action is usually complex and across video segments, uniformly average all clips is obviously not the best strategy and can only achieve limited accuracy. Our method achieves the best 67.46% top-1 accuracy due to the introduction of inter-clip interdependencies, which in turn proves that the inter-clip relationships is a key to model a complex action event in a real-world untrimmed video.</p><p id="p-0074" num="0000"><tables id="TABLE-US-00001" num="00001"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="center"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE 1</entry></row></thead><tbody valign="top"><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>Accuracy comparison of different methods in Kinetics-600</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="offset" colwidth="14pt" align="left"/><colspec colname="1" colwidth="126pt" align="left"/><colspec colname="2" colwidth="77pt" align="center"/><tbody valign="top"><row><entry/><entry>Method</entry><entry>Top-1 Accuracy (%)</entry></row><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row><row><entry/><entry>3D ResNet-101 + Central clip</entry><entry>58.58</entry></row><row><entry/><entry>3D ResNet-101 + 10 clips average</entry><entry>65.30</entry></row><row><entry/><entry>Our method (back bone: 3D ResNet-101)</entry><entry>67.46</entry></row><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0075" num="0061">Second, the method for video recognition according to implementations can achieve action detection without the need of clip- or frame-level annotations. As can been seen in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the action (in this case, blowing candles) can be easily located by using the attention vector with a threshold (e.g., &#x3e;0.7). In other words, a clip of a video corresponds to a clip descriptor, and a clip descriptor corresponds to an attention weight in the attention vector. When the threshold is set to 0.7, one or more attention weights the values of which are greater than 0.7 are determined, thus one or more clips of the video that correspond to the or more attention weights the values of which are greater than 0.7 are determined, and these determined one or more clips of the video can be used for video recognition, such as for action recognition.</p><p id="p-0076" num="0062"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a schematic flowchart of a method for training a neural network according to implementations. The neural network can be used for video recognition as described in the implementation as illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. As illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the method includes the following.</p><p id="p-0077" num="0063">S<b>402</b>, an original set of clip descriptors is obtained by providing multiple clips of a video as an input of a 3D CNN of a neural network.</p><p id="p-0078" num="0064">The neural network includes the 3D CNN and at least one first fully connected layer, the 3D CNN includes at least one convolutional layer and at least one second fully connected layer, and each of the multiple clips includes at least one frame.</p><p id="p-0079" num="0065">S<b>404</b>, an attention vector corresponding to the original set of clip descriptors is determined.</p><p id="p-0080" num="0066">S<b>406</b>, an enhanced set of clip descriptors is obtained based on the original set of clip descriptors and the attention vector.</p><p id="p-0081" num="0067">S<b>408</b>, the enhanced set of clip descriptors is input into the at least one first fully connected layer and an output of the neural network is obtained.</p><p id="p-0082" num="0068">S<b>410</b>, the neural network is trained by updating parameters of the neural network based on a loss of the neural network.</p><p id="p-0083" num="0069">The parameters of the network include a weight of the at least one first fully connected layer and a weight of the at least one second fully connected layer.</p><p id="p-0084" num="0070">As an implementation, the attention vector corresponding to the original set of clip descriptors is determined as follows.</p><p id="p-0085" num="0071">A first vector is obtained by performing a global average pooling on the original set of clip descriptors. The attention vector is obtained by employing a gating mechanism on the first vector based on a weight of the at least one second fully connected layer, where the 3D CNN includes at least one convolutional layer and the at least one second fully connected layer.</p><p id="p-0086" num="0072">As an implementation, the attention vector is obtained by employing the gating mechanism on the first vector based on the weight of the at least one second fully connected layer as follows.</p><p id="p-0087" num="0073">The first vector is multiplied by a first weight of the at least one second fully connected layer to obtain a second vector. The second vector is processed based on a rectified linear unit (ReLU) function to obtain a third vector. The third vector is multiplied by a second weight of the at least one second fully connected layer to obtain a fourth vector. The fourth vector is processed based on an activation function to obtain the attention vector.</p><p id="p-0088" num="0074">As an implementation, the enhanced set of clip descriptors is obtained based on the original set of clip descriptors and the attention vector as follows.</p><p id="p-0089" num="0075">The enhanced set of clip descriptors is obtained by multiplying the original set of clip descriptors by the attention vector.</p><p id="p-0090" num="0076">As an implementation, the enhanced set of clip descriptors is obtained based on the original set of clip descriptors and the attention vector as follows.</p><p id="p-0091" num="0077">A first set of clip descriptors is obtained by multiplying the original set of clip descriptors by the attention vector. The enhanced set of clip descriptors is obtained by adding the first set of clip descriptors to the original set of clip descriptors.</p><p id="p-0092" num="0078">As an implementation, the enhanced set of clip descriptors is input into the at least one first fully connected layer and an output of the neural network is obtained as follows.</p><p id="p-0093" num="0079">A fifth vector is determined based on the enhanced set of clip descriptors. The output of the at least one first fully connected layer is obtained by multiplying the fifth vector by a weight of the at least one first fully connected layer. The output of the neural network which is used for video recognition is obtained by processing the output of the at least one first fully connected layer based on a SoftMax function.</p><p id="p-0094" num="0080">As an implementation, the loss includes a classification loss corresponding to the output of the neural network and a sparsity loss corresponding to the attention vector.</p><p id="p-0095" num="0081">As an implementation, the classification loss is based on a standard cross-entropy loss between a ground truth corresponding to the input and the output of the neural network corresponding to the input, and the sparsity loss is obtained by performing L1 norm on the attention vector.</p><p id="p-0096" num="0082">For detailed descriptions of operations of the method for training a neural network, reference can be made to the related operations of the method for video recognition, which will not be described in detail.</p><p id="p-0097" num="0083"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a schematic structural diagram of a neural network based apparatus <b>50</b> for video recognition. As illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the apparatus includes at least one processor <b>501</b>, a memory <b>502</b>, and a neural network <b>503</b>. The neural network <b>503</b> includes at least one convolutional layer and at least one fully connected layer. The at least one fully connected layer may further include at least one first fully connected layer and at least one second fully connected layer. The memory <b>502</b> is coupled with the at least one processor and configured to store instructions which, when executed by the at least one processor, are operable with the processor to implement a neural network <b>503</b> to perform the following.</p><p id="p-0098" num="0084">An original set of clip descriptors is obtained by providing multiple clips of a video as an input of a 3D CNN of a neural network, where the neural network includes the 3D CNN and at least one first fully connected layer, and each of the multiple clips includes at least one frame. An attention vector corresponding to the original set of clip descriptors is determined. An enhanced set of clip descriptors is obtained based on the original set of clip descriptors and the attention vector. The enhanced set of clip descriptors is input into the at least one first fully connected layer and video recognition is performed based on an output of the at least one first fully connected layer.</p><p id="p-0099" num="0085">As an implementation, the instructions being operable with the at least one processor <b>501</b> to implement the neural network <b>503</b> to determine the attention vector corresponding to the original set of clip descriptors are operable with the at least one processor to implement the neural network to: obtain a first vector by performing a global average pooling on the original set of clip descriptors; obtain the attention vector by employing a gating mechanism on the first vector based on a weight of the at least one second fully connected layer, where the 3D CNN includes at least one convolutional layer and the at least one second fully connected layer.</p><p id="p-0100" num="0086">As an implementation, the instructions being operable with the at least one processor <b>501</b> to implement the neural network <b>503</b> to obtain the attention vector by employing the gating mechanism on the first vector based on the weight of the at least one second fully connected layer are operable with the at least one processor to implement the neural network to: multiply the first vector by a first weight of the at least one second fully connected layer to obtain a second vector; process the second vector based on a rectified linear unit (ReLU) function to obtain a third vector; multiply the third vector by a second weight of the at least one second fully connected layer to obtain a fourth vector; process the fourth vector based on an activation function to obtain the attention vector.</p><p id="p-0101" num="0087">As an implementation, the instructions being operable with the at least one processor <b>501</b> to implement the neural network <b>503</b> to obtain the enhanced set of clip descriptors based on the original set of clip descriptors and the attention vector are operable with the at least one processor to implement the neural network to: obtain the enhanced set of clip descriptors by multiplying the original set of clip descriptors by the attention vector.</p><p id="p-0102" num="0088">As an implementation, the instructions being operable with the at least one processor <b>501</b> to implement the neural network <b>503</b> to obtain the enhanced set of clip descriptors based on the original set of clip descriptors and the attention vector are operable with the at least one processor to implement the neural network to: obtain a first set of clip descriptors by multiplying the original set of clip descriptors by the attention vector; obtain the enhanced set of clip descriptors by adding the first set of clip descriptors to the original set of clip descriptors.</p><p id="p-0103" num="0089">As an implementation, the instructions being operable with the at least one processor <b>501</b> to implement the neural network <b>503</b> to input the enhanced set of clip descriptors into the at least one first fully connected layer and performing video recognition based on the output of the at least one first fully connected layer are operable with the at least one processor to implement the neural network to: determine a fifth vector based on the enhanced set of clip descriptors; obtain the output of the at least one first fully connected layer by multiplying the fifth vector by a weight of the at least one first fully connected layer; obtain an output of the neural network which is used for video recognition by processing the output of the at least one first fully connected layer based on a SoftMax function.</p><p id="p-0104" num="0090">As an implementation, the memory <b>502</b> is further configured to store instructions which, when executed by the at least one processor <b>501</b>, are operable with the at least one processor <b>501</b> to implement the neural network <b>503</b> to: obtain parameters of the neural network based on a loss, where parameters of the neural network include the weight of the at least one first fully connected layer and the weight of the at least one second fully connected layer and, the loss includes a classification loss corresponding to an output of the neural network and a sparsity loss corresponding to the attention vector.</p><p id="p-0105" num="0091">As an implementation, the classification loss is based on a standard cross-entropy loss between a ground truth corresponding to the input and the output of the neural network corresponding to the input, and the sparsity loss is obtained by performing L1 norm on the attention vector.</p><p id="p-0106" num="0092"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a schematic structural diagram of an apparatus for training a neural network. As illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the apparatus includes at least one processor <b>601</b>, a memory <b>602</b>, and a neural network <b>603</b>. The neural network <b>603</b> includes at least one convolutional layer and at least one fully connected layer. The at least one fully connected layer may further include at least one first fully connected layer and at least one second fully connected layer. The memory <b>602</b> is coupled with the at least one processor <b>601</b> and configured to store instructions which, when executed by the at least one processor, are operable with the processor to implement a neural network <b>603</b> to perform the following.</p><p id="p-0107" num="0093">An original set of clip descriptors is obtained by providing multiple clips of a video as an input of a 3D CNN of a neural network, where the neural network includes the 3D CNN and at least one first fully connected layer, the 3D CNN includes at least one convolutional layer and at least one second fully connected layer, and each of the multiple clips includes at least one frame. An attention vector corresponding to the original set of clip descriptors is determined. An enhanced set of clip descriptors is obtained based on the original set of clip descriptors and the attention vector. The enhanced set of clip descriptors is input into the at least one first fully connected layer and an output of the neural network is obtained. The neural network is trained by updating parameters of the neural network based on a loss of the neural network, where the parameters of the network include a weight of the at least one first fully connected layer and a weight of the at least one second fully connected layer.</p><p id="p-0108" num="0094">As an implementation, the instructions being operable with the at least one processor <b>601</b> to implement the neural network <b>603</b> to determine the attention vector corresponding to the original set of clip descriptors are operable with the at least one processor <b>601</b> to implement the neural network <b>603</b> to: obtain a first vector by performing a global average pooling on the original set of clip descriptors; obtain the attention vector by employing a gating mechanism on the first vector based on a weight of the at least one second fully connected layer, where the 3D CNN includes at least one convolutional layer and the at least one second fully connected layer.</p><p id="p-0109" num="0095">As an implementation, the instructions being operable with the at least one processor <b>601</b> to implement the neural network <b>603</b> to obtain the attention vector by employing the gating mechanism on the first vector based on the weight of the at least one second fully connected layer are operable with the at least one processor <b>601</b> to implement the neural network <b>603</b> to: multiply the first vector by a first weight of the at least one second fully connected layer to obtain a second vector; process the second vector based on a rectified linear unit (ReLU) function to obtain a third vector; multiply the third vector by a second weight of the at least one second fully connected layer to obtain a fourth vector; process the fourth vector based on an activation function to obtain the attention vector.</p><p id="p-0110" num="0096">As an implementation, the instructions being operable with the at least one processor <b>601</b> to implement the neural network <b>603</b> to obtain the enhanced set of clip descriptors based on the original set of clip descriptors and the attention vector are operable with the at least one processor <b>601</b> to implement the neural network <b>603</b> to: obtain the enhanced set of clip descriptors by multiplying the original set of clip descriptors by the attention vector.</p><p id="p-0111" num="0097">As an implementation, the instructions being operable with the at least one processor <b>601</b> to implement the neural network <b>603</b> to obtain the enhanced set of clip descriptors based on the original set of clip descriptors and the attention vector are operable with the at least one processor <b>601</b> to implement the neural network <b>603</b> to: obtain a first set of clip descriptors by multiplying the original set of clip descriptors by the attention vector; obtain the enhanced set of clip descriptors by adding the first set of clip descriptors to the original set of clip descriptors.</p><p id="p-0112" num="0098">As an implementation, the instructions being operable with the at least one processor <b>601</b> to implement the neural network <b>603</b> to input the enhanced set of clip descriptors into the at least one first fully connected layer and obtain an output of the neural network are operable with the at least one processor <b>601</b> to implement the neural network <b>603</b> to: determine a fifth vector based on the enhanced set of clip descriptors; obtain the output of the at least one first fully connected layer by multiplying the fifth vector by a weight of the at least one first fully connected layer; obtain an output of the neural network which is used for video recognition by processing the output of the at least one first fully connected layer based on a SoftMax function.</p><p id="p-0113" num="0099">As an implementation, the loss includes a classification loss corresponding to the output of the neural network and a sparsity loss corresponding to the attention vector.</p><p id="p-0114" num="0100">As an implementation, the classification loss is based on a standard cross-entropy loss between a ground truth corresponding to the input and the output of the neural network corresponding to the input, and the sparsity loss is obtained by performing L1 norm on the attention vector.</p><p id="p-0115" num="0101">Implementations further provide a non-transitory computer storage medium. The computer storage medium is configured to store computer programs for electronic data interchange (EDI) which, when executed, are operable with a computer to perform some or all operations of any one of the foregoing method implementations. The computer includes a terminal.</p><p id="p-0116" num="0102">Implementations further provide a computer program product. The computer program product includes a non-transitory computer-readable storage medium that stores computer programs. The computer programs are operable with a computer to execute some or all operations of any one of the foregoing method implementations. The computer program product may be a software installation package. The computer includes a terminal.</p><p id="p-0117" num="0103">It is to be noted that, for the sake of simplicity, the foregoing method implementations are described as a series of action combinations, however, it will be appreciated by those skilled in the art that the present disclosure is not limited by the sequence of actions described. According to implementations, certain steps or operations may be performed in other order or simultaneously. Besides, it will be appreciated by those skilled in the art that the implementations described in the specification are exemplary implementations and the actions and modules involved are not necessarily essential to the present disclosure.</p><p id="p-0118" num="0104">In the above implementations, description of each implementation has its own emphasis. For details not described in one implementation, reference can be made to related part in other implementations.</p><p id="p-0119" num="0105">It will be appreciated that the apparatuses disclosed in implementations herein may also be implemented in various other manners. For example, the above apparatus implementations are merely illustrative, i.e., the division of units is only a division of logical functions, and there may exist other manners of division in practice, i.e., multiple units or assemblies may be combined or may be integrated into another system, or some features may be ignored or skipped. In other respects, the coupling or direct coupling or communication connection as illustrated or discussed may be an indirect coupling or communication connection through some interface, device or unit, and may be electrical, or otherwise.</p><p id="p-0120" num="0106">Separated units as illustrated may or may not be physically separated. Components or parts displayed as units may or may not be physical units, and may reside at one location or may be distributed to multiple networked units. Some or all of the units may be selectively adopted according to practical needs to achieve desired objectives of the disclosure.</p><p id="p-0121" num="0107">Various functional units described in implementations herein may be integrated into one processing unit or may be present as a number of physically separated units, and two or more units may be integrated into one. The integrated unit may take the form of hardware or a software functional unit.</p><p id="p-0122" num="0108">If the integrated units are implemented as software functional units and sold or used as standalone products, they may be stored in a computer readable storage medium. Based on such an understanding, the essential technical solution, or the portion that contributes to the prior art, or all or part of the technical solution of the disclosure may be embodied as software products. The computer software products can be stored in a storage medium and may include multiple instructions that, when executed, can cause a computing device, i.e., a personal computer, a server, a network device, etc, to execute some or all operations of the methods described in various implementations. The above storage medium may include various kinds of media that can store program codes, such as a universal serial bus (USB) flash disk, a read only memory (ROM), a random access memory (RAM), a mobile hard drive, a magnetic disk, or an optical disk.</p><p id="p-0123" num="0109">It will be understood by those of ordinary skill in the art that all or part of the various methods of the implementations described above may be accomplished by means of a program to instruct associated hardware. The program may be stored in a computer-readable memory, which may include a flash memory, a ROM, a RAM, a magnetic disk, an optical disk, and so on.</p><p id="p-0124" num="0110">While the disclosure has been described in connection with certain embodiments, it is to be understood that the disclosure is not to be limited to the disclosed embodiments but, on the contrary, is intended to cover various modifications and equivalent arrangements included within the scope of the appended claims, which scope is to be accorded the broadest interpretation so as to encompass all such modifications and equivalent structures as is permitted under the law.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230005264A1-20230105-M00001.NB"><img id="EMI-M00001" he="10.92mm" wi="76.20mm" file="US20230005264A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002" nb-file="US20230005264A1-20230105-M00002.NB"><img id="EMI-M00002" he="7.03mm" wi="76.20mm" file="US20230005264A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00003" nb-file="US20230005264A1-20230105-M00003.NB"><img id="EMI-M00003" he="3.89mm" wi="76.20mm" file="US20230005264A1-20230105-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method for video recognition, comprising:<claim-text>obtaining an original set of clip descriptors by providing a plurality of clips of a video as an input of a three-dimensional (3D) convolutional neural network (CNN) of a neural network, wherein the neural network comprises the 3D CNN and at least one first fully connected layer, and each of the plurality of clips comprises at least one frame;</claim-text><claim-text>determining an attention vector corresponding to the original set of clip descriptors;</claim-text><claim-text>obtaining an enhanced set of clip descriptors based on the original set of clip descriptors and the attention vector; and</claim-text><claim-text>inputting the enhanced set of clip descriptors into the at least one first fully connected layer and performing video recognition based on an output of the at least one first fully connected layer.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein determining the attention vector corresponding to the original set of clip descriptors comprises:<claim-text>obtaining a first vector by performing a global average pooling on the original set of clip descriptors; and</claim-text><claim-text>obtaining the attention vector by employing a gating mechanism on the first vector based on a weight of at least one second fully connected layer, wherein the 3D CNN comprises at least one convolutional layer and the at least one second fully connected layer.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein obtaining the attention vector by employing the gating mechanism on the first vector based on the weight of the at least one second fully connected layer comprises:<claim-text>multiplying the first vector by a first weight of the at least one second fully connected layer to obtain a second vector;</claim-text><claim-text>processing the second vector based on a rectified linear unit (ReLU) function to obtain a third vector;</claim-text><claim-text>multiplying the third vector by a second weight of the at least one second fully connected layer to obtain a fourth vector; and</claim-text><claim-text>processing the fourth vector based on an activation function to obtain the attention vector.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein obtaining the enhanced set of clip descriptors based on the original set of clip descriptors and the attention vector comprises:<claim-text>obtaining a first set of clip descriptors as the enhanced set of clip descriptors by multiplying the original set of clip descriptors by the attention vector.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein obtaining the enhanced set of clip descriptors based on the original set of clip descriptors and the attention vector comprises:<claim-text>obtaining a first set of clip descriptors by multiplying the original set of clip descriptors by the attention vector; and</claim-text><claim-text>obtaining the enhanced set of clip descriptors by adding the first set of clip descriptors to the original set of clip descriptors.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein inputting the enhanced set of clip descriptors into the at least one first fully connected layer and performing video recognition based on the output of the at least one first fully connected layer comprises:<claim-text>determining a fifth vector based on the enhanced set of clip descriptors;</claim-text><claim-text>obtaining the output of the at least one first fully connected layer by multiplying the fifth vector by a weight of the at least one first fully connected layer; and</claim-text><claim-text>obtaining an output of the neural network which is used for video recognition by processing the output of the at least one first fully connected layer based on a SoftMax function.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>obtaining parameters of the neural network based on a loss, wherein parameters of the neural network comprise a weight of the at least one first fully connected layer and a weight of at least one second fully connected layer, the loss comprises a classification loss corresponding to an output of the neural network and a sparsity loss corresponding to the attention vector.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the classification loss is based on a standard cross-entropy loss between a ground truth corresponding to the input and the output of the neural network corresponding to the input, and the sparsity loss is obtained by performing L1 norm on the attention vector.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. A method for training a neural network, comprising,<claim-text>obtaining an original set of clip descriptors by providing a plurality of clips of a video as an input of a three-dimensional (3D) convolutional neural network (CNN) of a neural network, wherein the neural network comprises the 3D CNN and at least one first fully connected layer, the 3D CNN comprises at least one convolutional layer and at least one second fully connected layer, and each of the plurality of clips comprises at least one frame;</claim-text><claim-text>determining an attention vector corresponding to the original set of clip descriptors;</claim-text><claim-text>obtaining an enhanced set of clip descriptors based on the original set of clip descriptors and the attention vector;</claim-text><claim-text>inputting the enhanced set of clip descriptors into the at least one first fully connected layer and obtaining an output of the neural network; and</claim-text><claim-text>training the neural network by updating parameters of the neural network based on a loss of the neural network, wherein the parameters of the neural network comprise a weight of the at least one first fully connected layer and a weight of the at least one second fully connected layer.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the loss comprises a classification loss corresponding to the output of the neural network and a sparsity loss corresponding to the attention vector.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the classification loss is based on a standard cross-entropy loss between a ground truth corresponding to the input and the output of the neural network corresponding to the input, and the sparsity loss is obtained by performing L1 norm on the attention vector.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. A neural network based apparatus for video recognition, comprising:<claim-text>at least one processor;</claim-text><claim-text>a memory coupled with the at least one processor and configured to store instructions which, when executed by the at least one processor, are operable with the processor to implement a neural network to:<claim-text>obtain an original set of clip descriptors by providing a plurality of clips of a video as an input of a three-dimensional (3D) convolutional neural network (CNN) of a neural network, wherein the neural network comprises the 3D CNN and at least one first fully connected layer, and each of the plurality of clips comprises at least one frame;</claim-text><claim-text>determine an attention vector corresponding to the original set of clip descriptors;</claim-text><claim-text>obtain an enhanced set of clip descriptors based on the original set of clip descriptors and the attention vector; and</claim-text><claim-text>input the enhanced set of clip descriptors into the at least one first fully connected layer and perform video recognition based on an output of the at least one first fully connected layer.</claim-text></claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The apparatus of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the instructions being operable with the at least one processor to implement the neural network to determine the attention vector corresponding to the original set of clip descriptors are operable with the at least one processor to implement the neural network to:<claim-text>obtain a first vector by performing a global average pooling on the original set of clip descriptors; and</claim-text><claim-text>obtain the attention vector by employing a gating mechanism on the first vector based on a weight of at least one second fully connected layer, wherein the 3D CNN comprises at least one convolutional layer and the at least one second fully connected layer.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The apparatus of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the instructions being operable with the at least one processor to implement the neural network to obtain the attention vector by employing the gating mechanism on the first vector based on the weight of the at least one second fully connected layer are operable with the at least one processor to implement the neural network to:<claim-text>multiply the first vector by a first weight of the at least one second fully connected layer to obtain a second vector;</claim-text><claim-text>process the second vector based on a rectified linear unit (ReLU) function to obtain a third vector;</claim-text><claim-text>multiply the third vector by a second weight of the at least one second fully connected layer to obtain a fourth vector; and</claim-text><claim-text>process the fourth vector based on an activation function to obtain the attention vector.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The apparatus of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the instructions being operable with the at least one processor to implement the neural network to obtain the enhanced set of clip descriptors based on the original set of clip descriptors and the attention vector are operable with the at least one processor to implement the neural network to:<claim-text>obtain a first set of clip descriptors as the enhanced set of clip descriptors by multiplying the original set of clip descriptors by the attention vector.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The apparatus of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the instructions being operable with the at least one processor to implement the neural network to obtain the enhanced set of clip descriptors based on the original set of clip descriptors and the attention vector are operable with the at least one processor to implement the neural network to:<claim-text>obtain a first set of clip descriptors by multiplying the original set of clip descriptors by the attention vector; and</claim-text><claim-text>obtain the enhanced set of clip descriptors by adding the first set of clip descriptors to the original set of clip descriptors.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The apparatus of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the instructions being operable with the at least one processor to implement the neural network to input the enhanced set of clip descriptors into the at least one first fully connected layer and performing video recognition based on the output of the at least one first fully connected layer are operable with the at least one processor to implement the neural network to:<claim-text>determine a fifth vector based on the enhanced set of clip descriptors;</claim-text><claim-text>obtain the output of the at least one first fully connected layer by multiplying the fifth vector by a weight of the at least one first fully connected layer; and</claim-text><claim-text>obtain an output of the neural network which is used for video recognition by processing the output of the at least one first fully connected layer based on a SoftMax function.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The apparatus of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the memory is further configured to store instructions which, when executed by the at least one processor, are operable with the at least one processor to implement the neural network to:<claim-text>obtain parameters of the neural network based on a loss, wherein parameters of the neural network comprise a weight of the at least one first fully connected layer and a weight of at least one second fully connected layer, the loss comprises a classification loss corresponding to an output of the neural network and a sparsity loss corresponding to the attention vector.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The apparatus of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the classification loss is based on a standard cross-entropy loss between a ground truth corresponding to the input and the output of the neural network corresponding to the input, and the sparsity loss is obtained by performing L1 norm on the attention vector.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The apparatus of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the instructions being operable with the at least one processor to implement the neural network to obtain the enhanced set of clip descriptors based on the original set of clip descriptors and the attention vector are operable with the at least one processor to implement the neural network to:<claim-text>obtain a first set of clip descriptors as the enhanced set of clip descriptors by multiplying the original set of clip descriptors by the attention vector.</claim-text></claim-text></claim></claims></us-patent-application>