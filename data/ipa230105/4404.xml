<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004405A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004405</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17903763</doc-number><date>20220906</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20180101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>445</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>11</main-group><subgroup>34</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20180101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>8</main-group><subgroup>76</subgroup><symbol-position>L</symbol-position><classification-value>N</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>44505</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>11</main-group><subgroup>3419</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>11</main-group><subgroup>3442</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>11</main-group><subgroup>3452</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>2201</main-group><subgroup>88</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>8</main-group><subgroup>76</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">RUNNING A LEGACY APPLICATION ON A NON-LEGACY DEVICE WITH APPLICATION-SPECIFIC OPERATING PARAMETERS FOR BACKWARDS COMPATIBILITY</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>17170692</doc-number><date>20210208</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11474833</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17903763</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16408054</doc-number><date>20190509</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>10915333</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17170692</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>15466769</doc-number><date>20170322</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>10303488</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>16408054</doc-number></document-id></child-doc></relation></continuation><us-provisional-application><document-id><country>US</country><doc-number>62315345</doc-number><date>20160330</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Sony Interactive Entertainment Inc.</orgname><address><city>Tokyo</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Simpson</last-name><first-name>David</first-name><address><city>Los Angeles</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Cerny</last-name><first-name>Mark Evan</first-name><address><city>Burbank</city><state>MA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A method, system and computer readable medium for running a legacy application on a non-legacy device. Operating parameters of the non-legacy device when running the legacy application are set based on one or more pre-determined heuristics for adjustment of operating parameters of the newer system when running the legacy application on the non-legacy device from one or more performance metrics and other performance information.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="216.15mm" wi="156.63mm" file="US20230004405A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="233.26mm" wi="100.41mm" file="US20230004405A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="237.66mm" wi="158.67mm" file="US20230004405A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="229.95mm" wi="169.08mm" file="US20230004405A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="231.99mm" wi="151.47mm" file="US20230004405A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="235.20mm" wi="149.01mm" file="US20230004405A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CLAIM OF PRIORITY</heading><p id="p-0002" num="0001">This application is a continuation of U.S. patent application Ser. No. 17/170,692 to David Simpson and Mark Evan Cerny entitled &#x201c;DERIVING APPLICATION-SPECIFIC OPERATING PARAMETERS FOR BACKWARDS COMPATIBILITY&#x201d; (Attorney Docket No.: SCEA14059US03), filed Feb. 8, 2021, the entire contents of which are incorporated herein by reference.</p><p id="p-0003" num="0002">U.S. patent application Ser. No. 17/170,692 is a continuation of U.S. patent application Ser. No. 16/408,054 to David Simpson and Mark Evan Cerny entitled &#x201c;DERIVING APPLICATION-SPECIFIC OPERATING PARAMETERS FOR BACKWARDS COMPATIBILITY&#x201d; (Attorney Docket No.: SCEA14059US02), filed May 9, 2019, the entire contents of which are incorporated herein by reference.</p><p id="p-0004" num="0003">U.S. patent application Ser. No. 16/408,054 is a continuation of U.S. patent application Ser. No. 15/466,769 to David Simpson and Mark Evan Cerny entitled &#x201c;REAL TIME ADJUSTMENT OF APPLICATION-SPECIFIC OPERATING PARAMETERS FOR BACKWARDS COMPATIBILITY&#x201d; (Attorney Docket No.: SCEA14060US01), filed Mar. 22, 2017, the entire contents of which are incorporated herein by reference.</p><p id="p-0005" num="0004">U.S. patent application Ser. No. 15/466,769 claims the benefit of U.S. Provisional Patent Application No. 62/315,345 filed Mar. 30, 2016, the entire contents of which are incorporated herein by reference.</p><heading id="h-0002" level="1">CROSS-REFERENCE TO RELATED APPLICATION</heading><p id="p-0006" num="0005">This application is related to U.S. Provisional Patent Application No. 62/315,315 to Mark Evan Cerny and David Simpson entitled &#x201c;DERIVING APPLICATION-SPECIFIC OPERATING PARAMETERS FOR BACKWARDS COMPATIBILITY&#x201d; (Attorney Docket No.: SCEA14059US01), filed Mar. 30, 2016, the entire contents of which are incorporated herein by reference.</p><p id="p-0007" num="0006">This application is related to U.S. patent application Ser. No. 15/466,759 to David Simpson and Mark Evan Cerny entitled &#x201c;DERIVING APPLICATION-SPECIFIC OPERATING PARAMETERS FOR BACKWARDS COMPATIBILITY&#x201d; (Attorney Docket No.: SCEA14059US01), filed Mar. 22, 2017 (now U.S. Pat. No. 10,275,239 issued Apr. 30, 2019, the entire contents of which are incorporated herein by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0003" level="1">FIELD OF THE DISCLOSURE</heading><p id="p-0008" num="0007">Aspects of the present disclosure are related to execution of a computer application on a computer system. In particular, aspects of the present disclosure are related to a system or a method that provides backward compatibility for applications/titles designed for older versions of a computer system.</p><heading id="h-0004" level="1">BACKGROUND</heading><p id="p-0009" num="0008">When new computer architecture is released it is desirable for the applications written for a previous version of the architecture to run flawlessly on the new architecture. This capability is often referred to as &#x201c;backwards compatibility.&#x201d; Implementing backwards compatibility involves emulating a target legacy device on the new host architecture so that the new architecture can execute the instructions of programs written for the legacy device. Computer architectures change over time to take advantage of technological advances in busses, clock speed, processor architecture, caching, standards, etc. When one computer architecture is replaced by a newer architecture the older architecture becomes what is called legacy architecture. Over the course of its development software applications, such as network protocols, user interfaces, audio processing, device drivers, graphics processing, messaging, word processors, spreadsheets, database programs, games, and other applications are written for a legacy architecture. Such legacy software still has value to its users even if they upgrade to a new architecture. A need therefore exists to be able to run legacy software on the new architecture.</p><p id="p-0010" num="0009">Differences in performance of the hardware components of a new device and a legacy device can cause errors in synchronization on the new device, which may cause a legacy application to crash or produce incorrect output when running on a new device architecture. Such differences in performance can arise, e.g., from differences in hardware architecture between the new and legacy devices. It is within this context that aspects of the present disclosure arise.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>1</b>A</figref> is a flow diagram that illustrates derivation of application-specific operating parameters in accordance with aspects of the present disclosure.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>1</b>B</figref> is a flow diagram that illustrates real-time adjustment of application-specific operating parameters in accordance with aspects of the present disclosure.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>2</b>A</figref> is a block diagram illustrating an example of a central processing unit (CPU) core that may be configured to operate in a backwards compatibility mode in accordance with aspects of the present disclosure.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>2</b>B</figref> is a block diagram illustrating an example of a possible multi-core architecture for a CPU in accordance with aspects of the present disclosure.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a block diagram of a device having a CPU configured to operate in a backwards compatibility mode in accordance with aspects of the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">INTRODUCTION</heading><p id="p-0016" num="0015">To address problems that arise due to differences in hardware behavior when running a legacy application on a new device the new hardware may be tuned to run the legacy application.</p><p id="p-0017" num="0016">During a testing stage the legacy application is run on a legacy device with legacy architecture and performance information is collected. Examples of performance information include the number of ALU instructions or memory operations per unit time, and average parallel processing hardware scheduling unit (e.g., wavefront) occupancy or lifetime. The performance information may be directly measured (ALU and memory operations) by running games and application on a legacy device and reading counters. Alternatively, performance information may be derived from reading such counters or other data output as part of the measuring process. As an example of such derivation, average wavefront occupancy and lifetime may be derived from measurements of when wavefronts start and stop. Combined performance data for a particular application, e.g., a particular video game, is referred to herein as the performance characteristics for that application. The performance characteristics determined for the application in the testing stage can be used as a baseline for running the same application on a new system to ensure backwards compatibility.</p><p id="p-0018" num="0017">The performance of an application on a new device may be closely matched to the performance of that same application on the legacy device by tuning the operating parameters of the new device. Examples of operating parameters include, among other things, the clock frequencies of the new device, the number of available general purpose registers (GPRs), instruction launch rates, and the like. The application may be run repeatedly on the new system while tuning its operating parameters to adjust the application-specific performance characteristics. After a sufficient number of tests on the new system one can analyze how the performance characteristics of the application on the new system converge as the operating parameters change. A new set of operating parameters can be created based on the convergence analysis. This process may be repeated until the operating parameters are set optimally for the application on the new system. To further optimize, one can adjust the execution of the new hardware to see if the application can be run faster on the new hardware without causing it to fail.</p><heading id="h-0007" level="1">DETAILED DESCRIPTION</heading><p id="p-0019" num="0018">Application-Specific Performance Characteristic Determination</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>1</b>A</figref> shows a method <b>10</b> for deriving application-specific operating parameters during a testing phase in which the application runs on the legacy system. The application is run on a legacy system <b>12</b> and for each code block <b>14</b>, performance information is recorded or derived <b>16</b>. Running the application may include, e.g., loading a capture and letting it run without input or playing through a specific area of a game. Performance information includes Key Performance Metrics and Other Performance Information. Key Performance Metrics refers to that subset of performance information that is most important when the application is run on the new system. A Key Performance Metric is one that must be met when the application is run on the new hardware. Examples of Key Performance Metrics include, but are not limited to frames per second (e.g., in the case of video intensive applications, such as video games) and instructions per cycle (IPC) binned to a program counter (PC) range.</p><p id="p-0021" num="0020">Other Performance Information includes, but is not limited to PC block residence, number of arithmetic logic unit (ALU) instructions issued per unit time (CPU and GPU), number of memory operations issued per unit time (CPU and GPU), average parallel processing hardware scheduling unit (e.g., wavefront, warp, or vector width) occupancy per unit time, average parallel processing hardware scheduling unit lifetime, average latency for memory operations, count of pixels output to render targets per unit time, and total cycles active during a frame (ALU count is a specific example of this).</p><p id="p-0022" num="0021">Performance information may include values that are directly read from counters or derived from such values and other information, such as counting clock cycles between events during program execution. The performance information may be further analyzed at <b>18</b> and selected performance information may be combined to determine a set of performance characteristics <b>19</b>, which may then be saved or transferred <b>20</b>.</p><p id="p-0023" num="0022">Certain performance information values may be stored in dedicated processor registers that keep track of information related to execution of the application. Examples of such values include, but are not limited to counter values, such as the program counter and counters for memory cycles, arithmetic logic unit (ALU) cycles, and pixels, among others. The program counter (PC), also called the instruction pointer (IP) in Intel x86 and Itanium microprocessors, and sometimes called the instruction address register (IAR), or the instruction counter, is a processor register that indicates where a computer is in its program sequence.</p><p id="p-0024" num="0023">As noted above, certain other performance information, such as average parallel processing hardware scheduling unit (e.g., wavefront, warp, or vector width) occupancy per unit time, average parallel processing hardware scheduling unit lifetime, average latency for memory operations, count of pixels output to render targets per unit time may be derived indirectly. By way of example, and not by way of limitation, the number of instructions per cycle (IPC) may be derived by dividing a difference between initial and final program counter values by a number of clock cycles between the initial and final program counter values. Also, determining the average parallel processing hardware scheduling unit lifetime may involve detecting the launch and completion of such scheduling units and counting clock cycles therebetween. Similarly, determining average occupancy per unit time of parallel processing hardware scheduling units is a matter of recording launch and completion during a given window of time and determining how many, on average, are executing at any given time within that window of time.</p><p id="p-0025" num="0024">The term &#x201c;parallel processing scheduling unit&#x201d; is used herein as a generic term to cover several different terms used by manufacturers of different processor hardware to describe the concept of the smallest executable unit for parallel processing of code. For example, in the context of GPUs, parallel processing threads are bunched in what is sometimes called a &#x201c;warp&#x201d; (for NVIDIA hardware) or a &#x201c;wavefront&#x201d; (for AMD hardware) as the most basic unit of scheduling, the difference primarily being the number of threads that are grouped together. Other equivalent definitions include: &#x201c;the smallest executable unit code can operate upon&#x201d; or &#x201c;unit of processing by a single instruction over all of the threads in it at the same time&#x201d; or &#x201c;minimum size of the data processed in SIMD fashion&#x201d;. For CPU hardware the concept of a most basic level of parallelism is often called a &#x201c;vector width&#x201d; (for example when using the SSE instructions on Intel and AMD processors). For the sake of simplicity, the term &#x201c;wavefront&#x201d; will be used herein as a substitute for &#x201c;parallel processing scheduling unit&#x201d;. All the threads in a wavefront execute the same instruction in lock-step, the only difference being the data operated on by that instruction.</p><p id="p-0026" num="0025">Other operating information can be derived from operating register values in a number of different ways. For example, IPC may be derived by sampling the counter containing the total number of instructions that have been executed as the program executes. By way of example, this counter may be sampled every N cycles. The IPC value may be derived from an initial total instructions executed value (TIE) and a subsequent value N cycles later (TIE<sub>i+N</sub>) from (TIE<sub>i+N</sub>&#x2212;TIE<sub>i</sub>)/N. As a practical matter, the IPC value for a given section of an application (e.g., block of code) may be binned by the PC range for that particular section. Furthermore, each PC range within an application may therefore have different potential behavior and correspondingly different IPC values. It is therefore useful to associate IPC values with identified sections of program code, e.g., by code block number.</p><p id="p-0027" num="0026">PC block residence, which refers to the block of application code currently being executed, may be more relevant for the CPU than the GPU since the GPU typically runs multiple pieces of code simultaneously. PC block residence may be derived by sampling the PC every N cycles and counting the number of times the sample falls in the same block of code.</p><p id="p-0028" num="0027">Frequency of issuing ALU or Memory Operations may be derived by detecting the issuance of such operations and counting the number of such operations issued over a given window of time. Similarly, the count of pixels output to render targets per unit time may be derived by counting pixels output over a given window of time. Latencies, such as cache latency or memory operation latency may be derived by detecting issuance and completion of cache read/write and/or memory access instructions and counting clock cycles between issuance and completion.</p><p id="p-0029" num="0028">Recording/deriving performance information at <b>16</b> may include detecting busy waits. Busy waiting is typically implemented as a short loop. From a counter perspective this will look like the PC is staying in a very small range (and repeating) and there will be some type of memory read or IO read operation that happens every time through the loop. It is possible IPC could be high because of the loop, but more practically IPC will likely be low as the time in the loop will be dominated by waiting for the results of the memory or IO operation to return. Busy waits may be detected by looking for times when the PC stays in a very small range and the time is dominated by waiting for memory or IO operations to complete. The busy waits tend to skew the IPC and other performance information measurements. Since the time spent on busy waits is unpredictable, measurements taken while busy waiting may be removed from the performance information as part of the process of determining the performance characteristics at <b>18</b>. By doing so, the subsequent process of adjusting the operating parameters won't be influenced by the presence of busy waits.</p><p id="p-0030" num="0029">Analyzing recorded or derived performance information at <b>18</b> generally involves narrowing down the performance information to a useful set of performance characteristics <b>19</b> that generally characterizes the behavior of the application during execution. The performance characteristics <b>19</b> include, but are not limited to one or more key performance metrics and other performance information that is useful for later determination of operating parameters, as discussed below.</p><p id="p-0031" num="0030">The performance characteristic determination stage <b>18</b> may determine which performance information values are useful for tuning operating parameters, e.g., by determining correlations between changes in key performance information values and operating parameters through multivariate analysis as many different performance information values may change in response to changes in a given operating parameter.</p><p id="p-0032" num="0031">Parameter Adjustment Process</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>1</b>B</figref> describes a method <b>30</b> for adjusting execution of a legacy application to optimize the operating parameters when the application runs on the new system. The new system may use the performance characteristics <b>19</b> to adjust one or more operating parameters in real time when running the legacy application (<b>32</b>). For each code block <b>34</b> performance information is derived <b>36</b>, e.g., as discussed above. If one or more key performance metrics are not within bounds at <b>38</b>, one or more operating parameters may be iteratively adjusted <b>40</b> until they are. Once the key performance metrics are within bounds at <b>38</b>, operating parameters may be updated <b>42</b> and optimized by further adjustment <b>40</b>. Updated/optimized operating parameter data <b>43</b> may be saved or transferred <b>44</b>.</p><p id="p-0034" num="0033">The term &#x201c;operating parameters&#x201d; generally refers to aspects of the execution of application on the new system that can be adjusted to affect performance information including key performance metrics. Examples of operating parameters may include, but are not limited to: clock frequencies, e.g., for CPU, GPU, or memory, launch rate of instructions, launch rate of ALU and/or memory operations, resources e.g., general purpose registers (GPRs), wavefront slots, read and store queue sizes, etc., feature disablement, cache parameters (e.g., cache size, number of ways, number of banks, etc.), wavefront launch rate, pixel output rate from render backends, memory operation stalling.</p><p id="p-0035" num="0034">Algorithm matching refers to performing certain operations on the new system using algorithms from the legacy system architecture instead of new and improved algorithms written for the new system architecture. An example of such algorithm matching would be to use the branch predictor for the legacy system to perform branch prediction on the new system. In this example, the algorithm matching parameters would include parameters used in the legacy algorithm.</p><p id="p-0036" num="0035">Other operating parameters may also include parameters related to resource restriction (e.g., as described in U.S. patent application Ser. No. 14/810,361, filed Jul. 27, 2015 and published as U.S. Patent Application Publication Number 20170031834, both of which are incorporated herein by reference, and parameters related to algorithm matching, feature disablement, and matching latency or throughput (e.g., as described in U.S. patent application Ser. No. 14/810,334, filed Jul. 27, 2015 and published as U.S. Patent Application Publication Number 20170031732, now U.S. Pat. No. 10,235,219 all of which are which is incorporated herein by reference.</p><p id="p-0037" num="0036">Adjustment of operating parameters at <b>40</b> can be simple, e.g., setting the number of general purpose registers (GPRs) on the new hardware to the same number as the legacy hardware. Alternatively the new hardware may use a legacy algorithm for certain operations or features of the new hardware may be disabled for operation of the legacy application. The execution may be adjusted to match a latency on the new system to a legacy hardware latency.</p><p id="p-0038" num="0037">Adjustment of operating parameters can be more complex due to architectural differences between the legacy and new hardware. In some cases more resources may be allocated on the new hardware than the original hardware, e.g., by setting a slightly larger number of GPRs.</p><p id="p-0039" num="0038">Table I below lists some non-limiting examples of application-specific operating parameters, how to derive them, and how to adjust them.</p><p id="p-0040" num="0000"><tables id="TABLE-US-00001" num="00001"><table frame="none" colsep="0" rowsep="0" pgwide="1"><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="1" colwidth="63pt" align="left"/><colspec colname="2" colwidth="119pt" align="left"/><colspec colname="3" colwidth="98pt" align="left"/><thead><row><entry namest="1" nameend="3" rowsep="1">TABLE I</entry></row><row><entry namest="1" nameend="3" align="center" rowsep="1"/></row><row><entry>Performance</entry><entry/><entry>How to vary by Adjusting</entry></row><row><entry>Information</entry><entry>How to Measure/Derive</entry><entry>Operating Parameter</entry></row><row><entry namest="1" nameend="3" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry>Instructions per</entry><entry>Measure all instructions issued with</entry><entry>Adjust instruction launch rate</entry></row><row><entry>Cycle (IPC)</entry><entry>counter during frame (CPU and GPU)</entry><entry>(need to do in HW)</entry></row><row><entry>Frequency of</entry><entry>Measure ALU cycles with counter</entry><entry>Don't allow ALU Operation</entry></row><row><entry>issuing ALU</entry><entry>during frame (CPU and GPU)</entry><entry>every N cycles (need to do in</entry></row><row><entry>Operations</entry><entry/><entry>HW)</entry></row><row><entry>Frequency of</entry><entry>Measure Memory cycles with counter</entry><entry>Don't allow Memory Operation</entry></row><row><entry>issuing Memory</entry><entry>during frame (CPU and GPU)</entry><entry>every N cycles (need to do in</entry></row><row><entry>Operations</entry><entry/><entry>HW)</entry></row><row><entry>Average</entry><entry>Can sample this with a counter or</entry><entry>Selectively Allocate GPRs</entry></row><row><entry>Wavefront</entry><entry>replay capture and look at when</entry><entry>(could do in SW) or throttle</entry></row><row><entry>Occupancy per</entry><entry>wavefronts start and stop. (GPU)</entry><entry>wavefront launch rate (need to</entry></row><row><entry>unit time</entry><entry/><entry>do in HW)</entry></row><row><entry>Average</entry><entry>Replay capture and look at when</entry><entry>Selectively Allocate GPRs</entry></row><row><entry>Wavefront</entry><entry>wavefronts start and stop. (GPU)</entry><entry>(could do in SW) or throttle</entry></row><row><entry>Lifetime</entry><entry/><entry>wavefront launch rate (need to</entry></row><row><entry/><entry/><entry>do in HW)</entry></row><row><entry>Pixels output to</entry><entry>Look at pixel count per unit time</entry><entry>Throttle output rate (# pixels)</entry></row><row><entry>render targets per</entry><entry>w/existing counter. (GPU)</entry><entry>from render backends (these</entry></row><row><entry>unit time</entry><entry/><entry>write pixels out to render targets</entry></row><row><entry/><entry/><entry>at the bottom of the graphics</entry></row><row><entry/><entry/><entry>pipeline). (need to do in HW)</entry></row><row><entry>Average Memory</entry><entry>Determine when a memory instruction</entry><entry>Stall memory operations from</entry></row><row><entry>operation latency</entry><entry>is issued and when it's executed and</entry><entry>finishing (need to do in HW) or</entry></row><row><entry/><entry>count clock cycles in between (CPU</entry><entry>run the clock at a different rate</entry></row><row><entry/><entry>and GPU)</entry><entry>(can do in SW).</entry></row><row><entry>PC block residence</entry><entry>Read Program Counter (CPU)</entry><entry>Useful information when</entry></row><row><entry/><entry/><entry>adjusting operating parameters if</entry></row><row><entry/><entry/><entry>operating parameter values are</entry></row><row><entry/><entry/><entry>strongly correlated to block</entry></row><row><entry/><entry/><entry>residence</entry></row><row><entry namest="1" nameend="3" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0041" num="0039">The resulting updated operating parameters <b>43</b> may include a lower bound for each item of application-specific performance information above where the performance metrics of the legacy application running on the legacy hardware are consistently met. With further testing on the new hardware, each item of application-specific performance information may further include an upper bound, above which the legacy application no longer functions properly or the key performance metrics of the legacy application are no longer met on the new system. The application-specific performance information may correspond to the information in Table II below.</p><p id="p-0042" num="0000"><tables id="TABLE-US-00002" num="00002"><table frame="none" colsep="0" rowsep="0" pgwide="1"><tgroup align="left" colsep="0" rowsep="0" cols="4"><colspec colname="1" colwidth="49pt" align="left"/><colspec colname="2" colwidth="133pt" align="left"/><colspec colname="3" colwidth="42pt" align="left"/><colspec colname="4" colwidth="35pt" align="left"/><thead><row><entry namest="1" nameend="4" rowsep="1">TABLE II</entry></row><row><entry namest="1" nameend="4" align="center" rowsep="1"/></row><row><entry>Code Block</entry><entry>Performance Information</entry><entry>Min</entry><entry>Max</entry></row><row><entry namest="1" nameend="4" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry>CB1</entry><entry>Instructions per Cycle (IPC)</entry><entry>IPC1<sub>min</sub></entry><entry>IPC1<sub>max</sub></entry></row><row><entry>CB1</entry><entry>Frequency of issuing ALU Operations</entry><entry>AOF1<sub>min</sub></entry><entry>AOF1<sub>max</sub></entry></row><row><entry>CB1</entry><entry>Frequency of issuing Memory Operations</entry><entry>MOF1<sub>min</sub></entry><entry>MOF1<sub>max</sub></entry></row><row><entry>CB1</entry><entry>Average Wavefront Occupancy per unit</entry><entry>AWO1<sub>min</sub></entry><entry>AWO1<sub>max</sub></entry></row><row><entry/><entry>time</entry></row><row><entry>CB1</entry><entry>Pixels output to render targets per unit</entry><entry>PORT1<sub>min</sub></entry><entry>PORT1<sub>max</sub></entry></row><row><entry/><entry>time</entry></row><row><entry>CB1</entry><entry>Average Memory operation latency</entry><entry>AML1<sub>min</sub></entry><entry>AML1<sub>max</sub></entry></row><row><entry>CB1</entry><entry>PC block residence</entry><entry>PB1<sub>min</sub></entry><entry>PB1<sub>max</sub></entry></row><row><entry>CB2</entry><entry>Instructions per Cycle (IPC)</entry><entry>IPC2<sub>min</sub></entry><entry>IPC2<sub>max</sub></entry></row><row><entry>CB2</entry><entry>Frequency of issuing ALU Operations</entry><entry>AOF2<sub>min</sub></entry><entry>AOF2<sub>max</sub></entry></row><row><entry>CB2</entry><entry>Frequency of issuing Memory Operations</entry><entry>MOF2<sub>min</sub></entry><entry>MOF2<sub>max</sub></entry></row><row><entry>CB2</entry><entry>Average Wavefront Occupancy per unit</entry><entry>AWO2<sub>min</sub></entry><entry>AWO2<sub>max</sub></entry></row><row><entry/><entry>time</entry></row><row><entry>CB2</entry><entry>Pixels output to render targets per unit</entry><entry>PORT2<sub>min</sub></entry><entry>PORT2<sub>max</sub></entry></row><row><entry/><entry>time</entry></row><row><entry>CB2</entry><entry>Average Memory operation latency</entry><entry>AML2<sub>min</sub></entry><entry>AML2<sub>max</sub></entry></row><row><entry>CB2</entry><entry>PC block residence</entry><entry>PB2<sub>min</sub></entry><entry>PB2<sub>max</sub></entry></row><row><entry namest="1" nameend="4" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0043" num="0040">In the example shown in Table II, there is a set of upper and lower bounds for performance information for each code block in a legacy program. This information may be used in subsequent operation of the legacy game on the new hardware. Such subsequent operation may proceed as described in <figref idref="DRAWINGS">FIG. <b>1</b>B</figref> with the new hardware adjusting execution to keep the performance information between the minimum and maximum values.</p><p id="p-0044" num="0041">Operating parameter derivation and adjustment may be related to features of hardware shown in <figref idref="DRAWINGS">FIGS. <b>2</b>A-<b>2</b>B</figref> and <figref idref="DRAWINGS">FIG. <b>3</b></figref>. <figref idref="DRAWINGS">FIG. <b>2</b>A</figref> depicts a generalized architecture of a CPU core <b>100</b>. The CPU core <b>100</b> typically includes a branch prediction unit <b>102</b>, that attempts to predict whether a branch will be taken or not, and also attempts (in the event that the branch is taken) to predict the destination address of the branch. To the extent that these predictions are correct the efficiency of speculatively executed code will be increased; highly accurate branch prediction is therefore extremely desirable. The branch prediction unit <b>102</b> may include highly specialized sub-units such as a return address stack <b>104</b> that tracks return addresses from subroutines, an indirect target array <b>106</b> that tracks the destinations of indirect branches, and a branch target buffer <b>108</b> and its associated prediction logic that track past history of branches in order to more accurately predict their resulting addresses.</p><p id="p-0045" num="0042">The CPU core <b>100</b> typically includes an instruction fetch and decode unit <b>110</b>, which includes an instruction fetch unit <b>112</b>, an instruction byte buffer <b>114</b>, and an instruction decode unit <b>116</b>. The CPU core <b>100</b> also typically includes a number of instruction related caches and instruction translation lookaside buffers (ITLBs) <b>120</b>. These may include an ITLB cache hierarchy <b>124</b> that caches virtual address to physical address translation information such as page table entries, page directory entries, and the like. This information is used to transform the virtual address of the instruction into a physical address so that the instruction fetch unit <b>112</b> can load the instructions from the cache hierarchy. By way of example, and not by way of limitation, the program instructions may be cached according to a cache hierarchy that includes a level 1 instruction cache (L1 I-Cache) <b>122</b> residing in the core, as well as other cache levels <b>176</b> external to the CPU core <b>100</b>; using the physical address of the instruction, these caches are first searched for the program instructions. If the instructions are not found, then they are loaded from a system memory <b>101</b>. Depending on the architecture, there may also be a micro-op cache <b>126</b> that contains the decoded instructions, as described below.</p><p id="p-0046" num="0043">Once the program instructions have been fetched, they are typically placed in the instruction byte buffer <b>114</b> awaiting processing by the instruction fetch and decode unit <b>110</b>. Decoding can be a very complex process; it is difficult to decode multiple instructions each cycle, and there may be restrictions on instruction alignment or type of instruction that limit how many instructions may be decoded in a cycle. Decoded instructions may, depending on architecture, be placed in the micro-op cache <b>126</b> (if one is present on the new CPU) so that the decode stage can be bypassed for subsequent use of the program instructions.</p><p id="p-0047" num="0044">Decoded instructions are typically passed to other units for dispatch and scheduling <b>130</b>. These units may use retirement queues <b>132</b> to track the status of the instructions throughout the remainder of the CPU pipeline. Also, due to the limited number of general purpose and SIMD registers available on many CPU architectures, register renaming may be performed, in which as logical (also known as architectural) registers are encountered in stream of instructions being executed, physical registers <b>140</b> are assigned to represent them. The physical registers <b>140</b> may include Single Instruction Multiple Data (SIMD) register banks <b>142</b> and General Purpose (GP) register banks <b>144</b>, which can be much larger in size than the number of logical registers available on the particular CPU architecture, and as a result the performance can be considerably increased. After register renaming <b>134</b> is performed, instructions are typically placed in scheduling queues <b>136</b>, from which a number of instructions may be selected each cycle (based on dependencies) for execution by execution units <b>150</b>.</p><p id="p-0048" num="0045">The execution units <b>150</b> typically include SIMD pipes <b>152</b> that perform a number of parallel operations on multiple data fields contained in 128-bit or wider SIMD registers contained in the SIMD register bank <b>142</b>, arithmetic and logic units (ALUs) <b>154</b> that perform a number of logical, arithmetic, and miscellaneous operations on GPRs contained in the GP register bank <b>144</b>, and address generation units (AGUs) <b>156</b> that calculate the address from which memory should be stored or loaded. There may be multiple instances of each type of execution unit, and the instances may have differing capabilities, for example a specific SIMD pipe <b>152</b> may be able to perform floating point multiply operations but not floating point add operations.</p><p id="p-0049" num="0046">Stores and loads are typically buffered in a store queue <b>162</b> and a load queue <b>164</b> so that many memory operations can be performed in parallel. To assist in memory operations, the CPU core <b>100</b> usually includes a number of data related caches and data translation lookaside buffers (DTLBs) <b>170</b>. A DTLB cache hierarchy <b>172</b> caches virtual address to physical address translation such as page table entries, page directory entries, and the like; this information is used to transform the virtual address of the memory operation into a physical address so that data can be stored or loaded from system memory. The data is typically cached in a level 1 data cache (L1 D-Cache) <b>174</b> residing in the core, as well as other cache levels <b>176</b> external to the core <b>100</b>.</p><p id="p-0050" num="0047">According to certain aspects of the disclosure, a CPU may include a plurality of cores. By way of example and not by way of limitation, <figref idref="DRAWINGS">FIG. <b>2</b>B</figref> depicts an example of a possible multi-core CPU <b>200</b> that may be used in conjunction with aspects of the present disclosure. Specifically, the architecture of the CPU <b>200</b> may include M clusters <b>201</b>-<b>1</b> . . . <b>201</b>-M, where M is an integer greater than zero. Each cluster may have N cores <b>202</b>-<b>1</b>, <b>202</b>-<b>2</b> . . . <b>202</b>-N, where N is an integer greater than 1. Aspects of the present disclosure include implementations in which different clusters have different numbers of cores. Each core may include one or more corresponding dedicated local caches (e.g., L1 instruction, L1 data, or L2 caches). Each of the local caches may be dedicated to a particular corresponding core in the sense that it is not shared with any other cores. Each cluster may also include a cluster-level cache <b>203</b>-<b>1</b> . . . <b>203</b>-M that may be shared between the cores in the corresponding cluster. In some implementations the cluster-level caches are not shared by cores associated with different caches. Furthermore, the CPU <b>200</b> may include one or more higher-level caches <b>204</b>, which may be shared between the clusters. To facilitate communication among the cores in a cluster, the clusters <b>201</b>-<b>1</b> . . . <b>202</b>-M may include corresponding local busses <b>205</b>-<b>1</b> . . . <b>205</b>-M coupled to each of the cores and the cluster-level cache for the cluster. Likewise, to facilitate communication among the clusters, the CPU <b>200</b> may include one or more higher-level busses <b>206</b> coupled to the clusters <b>201</b>-<b>1</b> . . . <b>201</b>-M and to the higher level cache <b>204</b>. In some implementations the higher-level bus or busses <b>206</b> may also be coupled to other devices, e.g., a GPU, memory, or memory controller. In still other implementations, the higher-level bus or busses <b>206</b> may be connected to a device-level bus that connects to different devices within a system. In yet other implementations, the higher level bus or busses <b>206</b> may couple the clusters <b>201</b>-<b>1</b> . . . <b>201</b>-M to the higher level cache <b>204</b>, and a device-level bus <b>208</b> may couple the higher level cache <b>204</b> to other devices, e.g., a GPU, memory, or memory controller. By way of example, and not by way of limitation, an implementation with such a device-level bus <b>208</b> may arise, e.g., where the higher level cache <b>204</b> is an L3 for all CPU cores, but not for GPU use.</p><p id="p-0051" num="0048">In the CPU <b>200</b> OS processing may occur predominantly on a certain core, or a certain subset of the cores. Similarly, application-level processing may occur predominantly on a particular core or subset of the cores. Individual application threads may be designated by the application to run on a certain core, or a certain subset of the cores. As caches and buses are shared, speed of processing by a given application thread may vary depending on the processing occurring by other threads (e.g., application threads or OS threads) running in the same cluster as the given application thread. Depending on the specifics of the CPU <b>200</b>, a core may be capable of executing only one thread at once, or may be capable of executing multiple threads simultaneously (&#x201c;hyperthreading&#x201d;). In the case of a hyperthreaded CPU, an application may also designate which threads may be executed simultaneously with which other threads. Performance of a thread is impacted by the specific processing performed by any other threads being executed by the same core.</p><p id="p-0052" num="0049">Turning now to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, an illustrative example of a device <b>300</b> configured to operate in accordance with aspects of the present disclosure is depicted. According to aspects of the present disclosure, the device <b>300</b> may be an embedded system, mobile phone, personal computer, tablet computer, portable game device, workstation, game console, and the like.</p><p id="p-0053" num="0050">The device <b>300</b> generally includes a central processor unit (CPU) <b>320</b> which may include one or more CPU cores <b>323</b> of the type depicted in <figref idref="DRAWINGS">FIG. <b>1</b></figref> and discussed above. The CPU <b>320</b> may include a plurality of such cores <b>323</b> and one or more caches <b>325</b> in a configuration like that shown in the CPU <b>200</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>. By way of example and not by way of limitation, the CPU <b>320</b> may be part of an accelerated processing unit (APU) <b>310</b> that includes the CPU <b>320</b>, and a graphics processing unit (GPU) <b>330</b> on a single chip. In alternative implementations, the CPU <b>320</b> and GPU <b>330</b> may be implemented as separate hardware components on separate chips. The GPU <b>330</b> may also include two or more cores <b>332</b> and two or more caches <b>334</b> and (in some implementations) one or more busses to facilitate communication among the cores and caches and other components of the system. The busses may include an internal bus or busses <b>317</b> for the APU <b>310</b>, and an external data bus <b>390</b>.</p><p id="p-0054" num="0051">The device <b>300</b> may also include memory <b>340</b>. The memory <b>340</b> may optionally include a main memory unit that is accessible to the CPU <b>320</b> and GPU <b>330</b>. The CPU <b>320</b> and GPU <b>330</b> may each include one or more processor cores, e.g., a single core, two cores, four cores, eight cores, or more. The CPU <b>320</b> and GPU <b>330</b> may be configured to access one or more memory units using the external data bus <b>390</b>, and, in some implementations, it may be useful for the device <b>300</b> to include two or more different buses.</p><p id="p-0055" num="0052">The memory <b>340</b> may include one or more memory units in the form of integrated circuits that provides addressable memory, e.g., RAM, DRAM, and the like. The memory may contain executable instructions configured to implement a method like the method of <figref idref="DRAWINGS">FIG. <b>5</b></figref> upon execution for determining operate the device <b>300</b> in a timing testing mode when running applications originally created for execution on a legacy CPU. In addition, the memory <b>340</b> may include a dedicated graphics memory for temporarily storing graphics resources, graphics buffers, and other graphics data for a graphics rendering pipeline.</p><p id="p-0056" num="0053">The CPU <b>320</b> may be configured to execute CPU code, which may include operating system (OS) <b>321</b> or an application <b>322</b> (e.g., a video game). The operating system may include a kernel that manages input/output (I/O) requests from software (e.g., application <b>322</b>) and translates them into data processing instructions for the CPU <b>320</b>, GPU <b>330</b> or other components of the device <b>300</b>. The OS <b>321</b> may also include firmware, which may be stored in non-volatile memory. The OS <b>321</b> may be configured to implement certain features of operating the CPU <b>320</b> in a timing testing mode, as discussed in detail below. The CPU code may include a graphics application programming interface (API) <b>324</b> for issuing draw commands or draw calls to programs implemented by the GPU <b>330</b> based on a state of the application <b>322</b>. The CPU code may also implement physics simulations and other functions. Portions of the code for one or more of the OS <b>321</b>, application <b>322</b>, or API <b>324</b> may be stored in the memory <b>340</b>, caches internal or external to the CPU or in a mass storage device accessible to the CPU <b>320</b>.</p><p id="p-0057" num="0054">The device <b>300</b> may include a memory controller <b>315</b>. The memory controller <b>315</b> may be a digital circuit that manages the flow of data going to and from the memory <b>340</b>. By way of example and not by way of limitation, the memory controller may be an integral part of the APU <b>310</b>, as in the example depicted in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, or may be a separate hardware component.</p><p id="p-0058" num="0055">The device <b>300</b> may also include well-known support functions <b>350</b>, which may communicate with other components of the system, e.g., via the bus <b>390</b>. Such support functions may include, but are not limited to, input/output (I/O) elements <b>352</b>, one or more clocks <b>356</b>, which may include separate clocks for the CPU <b>320</b>, GPU <b>330</b>, and memory <b>340</b>, respectively, and one or more levels of cache <b>358</b>, which may be external to the CPU <b>320</b> and GPU <b>330</b>. The device <b>300</b> may optionally include a mass storage device <b>360</b> such as a disk drive, CD-ROM drive, flash memory, tape drive, Blu-ray drive, or the like to store programs and/or data. In one example, the mass storage device <b>360</b> may receive a computer readable medium <b>362</b> containing a legacy application originally designed to run on a system having a legacy CPU. Alternatively, the legacy application <b>362</b> (or portions thereof) may be stored in memory <b>340</b> or partly in the cache <b>358</b>.</p><p id="p-0059" num="0056">The device <b>300</b> may also include a display unit <b>380</b> to present rendered graphics <b>382</b> prepared by the GPU <b>330</b> to a user. The device <b>300</b> may also include a user interface unit <b>370</b> to facilitate interaction between the system <b>100</b> and a user. The display unit <b>380</b> may be in the form of a flat panel display, cathode ray tube (CRT) screen, touch screen, head mounted display (HMD) or other device that can display text, numerals, graphical symbols, or images. The display <b>380</b> may display rendered graphics <b>382</b> processed in accordance with various techniques described herein. The user interface <b>370</b> may contain one or more peripherals, such as a keyboard, mouse, joystick, light pen, game controller, touch screen, and/or other device that may be used in conjunction with a graphical user interface (GUI). In certain implementations, the state of the application <b>322</b> and the underlying content of the graphics may be determined at least in part by user input through the user interface <b>370</b>, e.g., where the application <b>322</b> includes a video game or other graphics intensive application.</p><p id="p-0060" num="0057">The device <b>300</b> may also include a network interface <b>372</b> to enable the device to communicate with other devices over a network. The network may be, e.g., a local area network (LAN), a wide area network such as the internet, a personal area network, such as a Bluetooth network or other type of network. Various ones of the components shown and described may be implemented in hardware, software, or firmware, or some combination of two or more of these.</p><p id="p-0061" num="0058">Table III below lists some non-limiting examples of how specific hardware elements described above with respect to <figref idref="DRAWINGS">FIG. <b>2</b>A</figref>, <figref idref="DRAWINGS">FIG. <b>2</b>B</figref>, and <figref idref="DRAWINGS">FIG. <b>3</b></figref> may be used to determine performance information and corresponding operating parameters to adjust.</p><p id="p-0062" num="0000"><tables id="TABLE-US-00003" num="00003"><table frame="none" colsep="0" rowsep="0" pgwide="1"><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="1" colwidth="63pt" align="left"/><colspec colname="2" colwidth="98pt" align="left"/><colspec colname="3" colwidth="119pt" align="left"/><thead><row><entry namest="1" nameend="3" rowsep="1">TABLE III</entry></row><row><entry namest="1" nameend="3" align="center" rowsep="1"/></row><row><entry>Performance</entry><entry/><entry/></row><row><entry>Information</entry><entry>How to Record/Derive</entry><entry>Operating Parameter to Adjust</entry></row><row><entry namest="1" nameend="3" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry>Instructions per</entry><entry>Read counter of instructions</entry><entry>Adjust instruction launch rate (FIG.</entry></row><row><entry>Cycle (IPC)</entry><entry>executed (FIG. 2A, part of</entry><entry>2A, part of Scheduling Queues 136 for</entry></row><row><entry/><entry>Retirement Queues 132)</entry><entry>CPU and FIG. 3, part of GPU Core 332</entry></row><row><entry/><entry/><entry>for GPU)</entry></row><row><entry>Frequency of</entry><entry>Read ALU counters (FIG. 2A,</entry><entry>Don't allow ALU Operation every N</entry></row><row><entry>issuing ALU</entry><entry>part of ALUs 154 and SIMD</entry><entry>cycles (FIG. 2A, part of Scheduling</entry></row><row><entry>Operations</entry><entry>Pipes 152 for CPU and FIG. 3,</entry><entry>Queues 136 for CPU and FIG. 3, part</entry></row><row><entry/><entry>part of the GPU Core 332 for</entry><entry>of GPU Core 332 for GPU)</entry></row><row><entry/><entry>GPU)</entry></row><row><entry>Frequency of</entry><entry>Read memory operation</entry><entry>Don't allow Memory Operation every</entry></row><row><entry>issuing Memory</entry><entry>counter(s) (FIG. 2A, part of</entry><entry>N cycles (FIG. 2A, part of Scheduling</entry></row><row><entry>Operations</entry><entry>AGUs 156 for CPU and FIG. 3,</entry><entry>Queues 136 for CPU and FIG. 3, part</entry></row><row><entry/><entry>part of GPU Core 332 for GPU)</entry><entry>of GPU Core 332 for GPU)</entry></row><row><entry>Average</entry><entry>Record wavefront start and</entry><entry>Selectively Allocate GPRs (FIG. 3,</entry></row><row><entry>Wavefront</entry><entry>completion events generated by</entry><entry>GPRs 336) or Throttle wavefront</entry></row><row><entry>Occupancy per</entry><entry>GPU core (FIG. 3, part of GPU</entry><entry>launch rate (FIG. 3, part of GPU Core</entry></row><row><entry>unit time</entry><entry>Core 332)</entry><entry>332)</entry></row><row><entry>Average</entry><entry>Record wavefront start and</entry><entry>Selectively Allocate GPRs (FIG. 3,</entry></row><row><entry>Wavefront</entry><entry>completion events generated by</entry><entry>GPRs 336) or Throttle wavefront</entry></row><row><entry>Lifetime</entry><entry>GPU core (FIG. 3, part of GPU</entry><entry>launch rate (FIG. 3, part of GPU Core</entry></row><row><entry/><entry>Core 332) or read wavefront</entry><entry>332)</entry></row><row><entry/><entry>lifetime counters core (FIG. 3,</entry></row><row><entry/><entry>part of GPU Core 332)</entry></row><row><entry>Pixels output to</entry><entry>Read pixel counters (FIG. 3,</entry><entry>Throttle output rate (# pixels) from</entry></row><row><entry>render targets per</entry><entry>part of GPU Core 332)</entry><entry>render backends (FIG. 3, part of GPU</entry></row><row><entry>unit time</entry><entry/><entry>Core 332)</entry></row><row><entry>Average Memory</entry><entry>Track length of outstanding</entry><entry>Stall memory operations from finishing</entry></row><row><entry>operation latency</entry><entry>memory operations (FIG. 2A,</entry><entry>(FIG. 2A, part of Store Queue 162 and</entry></row><row><entry/><entry>part of Store Queue 162 and</entry><entry>Load Queue 164 for CPU and FIG. 3,</entry></row><row><entry/><entry>Load Queue 164 for CPU and</entry><entry>part of GPU Core 332 for GPU) or run</entry></row><row><entry/><entry>FIG. 3, part of GPU Core 332</entry><entry>the clock at a different rate (FIG. 3,</entry></row><row><entry/><entry>for GPU)</entry><entry>CLK 356)</entry></row><row><entry>Branch Predict hits</entry><entry>Read counters of branch</entry><entry>Match legacy branch prediction</entry></row><row><entry>and misses</entry><entry>prediction hits and misses (FIG.</entry><entry>algorithm (FIG. 2A, part of Branch</entry></row><row><entry/><entry>2A, part of Branch Predict 102)</entry><entry>Predict 102) or run the clock at a</entry></row><row><entry/><entry/><entry>different rate (FIG. 3, CLK 356)</entry></row><row><entry>PC block residence</entry><entry>Read Program Counter (FIG.</entry><entry>No operating parameters to directly</entry></row><row><entry/><entry>2A, part of Fetch and Decode</entry><entry>adjust, but useful information if</entry></row><row><entry/><entry>Unit 110)</entry><entry>operating parameter values are strongly</entry></row><row><entry/><entry/><entry>correlated to block residence</entry></row><row><entry namest="1" nameend="3" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0063" num="0059">Note that it may not be possible for performance information recorded or derived when running the legacy application on the new device to meet or exceed corresponding performance information for running the legacy application on the legacy device. For example, average memory latency on a new device may be higher than the average memory latency measured on the legacy device, regardless of how the operating parameters are adjusted. Knowing this type of performance information for the new system may be useful when adjusting the operating parameters, but shouldn't be used when comparing the performance characteristics of an application running on a legacy and new device. Only key performance metrics like, but not limited to, frames per second (FPS) and instructions per cycle (IPC) should actually be used.</p><p id="p-0064" num="0060">The process of adjusting the operating parameters when the application runs on a new device in order to meet the key performance metrics of the same application run on a legacy device may be understood from the following example involving video games. First, performance data is collected for a game running on the legacy device in order to determine its key performance metrics. Next the game is run on the new device while adjusting the operating parameters of the new device. Performance of the new device can be measured by collecting the same performance information on the new device as was done when the game ran on the legacy device and then comparing the key performance metrics of the game running on those two devices. While it may be desirable for the performance information for the new device to perfectly match the performance data from the legacy device, this may not be practically possible. It is sufficient for the performance information on the new device to match the performance information on the legacy device as closely as possible. However, it is unacceptable for key performance metrics on the new device to be worse than key performance metrics on the legacy device, as is an application or game that crashes (typically due to synchronization problems), or produces incorrect outputs (for the same reasons).</p><p id="p-0065" num="0061">Now practically speaking, the first several times games are run on a new device the operating parameters will likely be set to be the same as on the legacy device. Once enough games have been run on the new device and their operating parameters have been tuned, that experience and data can be used to build a heuristic that can be used for additional games. The heuristic can be used to set the initial values of the operating parameters on the new device based upon the performance characteristics of the game. The game would then be run on the new device and the operating parameters may be modified to better match the key performance metrics. All performance data as measured on the new device can be used to help adjust the operating parameters, not just the key performance metrics. Any adjustments made to the operating parameters can also be used to further refine the heuristic.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method for running a legacy application on a non-legacy device, comprising:<claim-text>setting operating parameters of the non-legacy device when running the legacy application based on one or more pre-determined heuristics for adjustment of operating parameters of the newer system when running the legacy application on the non-legacy device from one or more performance metrics and other performance information.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one or more pre-determined heuristics use one or more performance metrics determined from counting clock cycles between events during program execution.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one or more pre-determined heuristics use one or more performance metrics determined from values stored in one or more dedicated processor registers that keep track of information related to execution of the application.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the information stored in the one or more dedicated processor registers includes a counter value.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the counter value is a program counter value.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the counter value is a value of a counter for memory cycles, arithmetic logic unit (ALU) cycles, or pixels.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one or more pre-determined heuristics use one or more performance metrics are determined from detected busy waits.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one or more pre-determined heuristics use one or more key performance metrics include to frames per second.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one or more pre-determined heuristics use one or more key performance metrics include instructions per cycle (IPC) binned to a program counter (PC) range.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the other performance information is derived indirectly.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the other performance information includes an average parallel processing hardware scheduling unit occupancy per unit time, an average parallel processing hardware scheduling unit lifetime, an average latency for memory operations, or a count of pixels output to render targets per unit time.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising determining one or more performance information values that are useful for tuning operating parameters of the newer system.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein determining the one or more performance information values that are useful for tuning operating parameters of the newer system includes determining one or more correlations between changes in key performance information values and operating parameters.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein determining one or more performance information values that are useful for tuning operating parameters of the newer system includes determining one or more correlations between changes in key performance information values and operating parameters through multivariate analysis.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the non-legacy device is a video game system.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. A system, comprising:<claim-text>a processor;</claim-text><claim-text>a memory; and</claim-text><claim-text>processor executable instructions embodied in the memory, the instructions being configured to implement a method for running a legacy application on a non-legacy device, the method comprising;</claim-text><claim-text>setting one or more operating parameters of the system when running the legacy application based on one or more pre-determined heuristics for adjustment of operating parameters of the system when running the legacy application on the system from one or more performance metrics and other performance information.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. A non-transitory computer readable medium having computer readable instructions embodied therein, the instructions being configured to implement a method for running a legacy application on a non-legacy device, the method comprising;<claim-text>setting operating parameters of the non-legacy device based on one or more pre-determined heuristics for adjustment of operating parameters of the non-legacy device when running the legacy application on the non-legacy device from one or more performance metrics and other information.</claim-text></claim-text></claim></claims></us-patent-application>