<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005466A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005466</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17820339</doc-number><date>20220817</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>CN</country><doc-number>202110949472.1</doc-number><date>20210818</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>13</main-group><subgroup>08</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>13</main-group><subgroup>047</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>13</main-group><subgroup>08</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>13</main-group><subgroup>047</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">SPEECH SYNTHESIS METHOD, AND ELECTRONIC DEVICE</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>BEIJING BAIDU NETCOM SCIENCE TECHNOLOGY CO., LTD.</orgname><address><city>Beijing</city><country>CN</country></address></addressbook><residence><country>CN</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>GAO</last-name><first-name>Zhengkun</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>ZHANG</last-name><first-name>Junteng</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>SUN</last-name><first-name>Tao</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>JIA</last-name><first-name>Lei</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">The disclosure provides a speech synthesis method, and an electronic device. The technical solution is described as follows. A text to be synthesized and speech features of a target user are obtained. Predicted first acoustic features based on the text to be synthesized and the speech features are obtained. A target template audio is obtained from a template audio library based on the text to be synthesized. Second acoustic features of the target template audio are extracted. Target acoustic features are generated by splicing the first acoustic features and the second acoustic features. Speech synthesis is performed on the text to be synthesized based on the target acoustic features and the speech features, to generate a target speech of the text to be synthesized.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="179.66mm" wi="107.10mm" file="US20230005466A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="232.41mm" wi="121.24mm" file="US20230005466A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="189.65mm" wi="109.14mm" file="US20230005466A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="217.17mm" wi="63.33mm" file="US20230005466A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="106.34mm" wi="158.24mm" file="US20230005466A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">CROSS REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application claims priority to Chinese Patent Application No. 202110949472.1, filed on Aug. 18, 2021, the entire disclosure of which is incorporated herein by reference.</p><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">The disclosure relates to the field of computer technology, in particular to a speech synthesis method, and an electronic device.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">The existing speech synthesis methods are capable of converting text into audio having speech features of a target user, and have been widely used in fields such as speech chat and smart home. For example, in a speech chat scenario, after the user chat speech is received, a chat text matching the chat speech can be obtained, and the chat text can be converted into an audio having the speech features of the target user in real time, and then the audio can be played or fed back to a user terminal. However, the authenticity and naturalness of the synthesized speech obtained according to the speech synthesis methods in the related art are poor.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0005" num="0004">According to a first aspect of the disclosure, a speech synthesis method is provided. The method includes: obtaining a text to be synthesized and speech features of a target user, and obtaining predicted first acoustic features based on the text to be synthesized and the speech features; obtaining a target template audio from a template audio library based on the text to be synthesized, and extracting second acoustic features of the target template audio; generating target acoustic features by splicing the first acoustic features and the second acoustic features; and performing speech synthesis on the text to be synthesized based on the target acoustic features and the speech features, to generate a target speech of the text to be synthesized.</p><p id="p-0006" num="0005">According to a second aspect of the disclosure, an electronic device is provided. The electronic device includes: at least one processor and a memory communicatively coupled to the at least one processor. The memory stores instructions executable by the at least one processor, and when the instructions are executed by the at least one processor, the at least one processor is enabled to implement the speech synthesis method.</p><p id="p-0007" num="0006">It should be understood that the content described in this section is not intended to identify key or important features of the embodiments of the disclosure, nor is it intended to limit the scope of the disclosure. Additional features of the disclosure will be easily understood based on the following description.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0008" num="0007">The drawings are used to better understand the solution and do not constitute a limitation to the disclosure, in which:</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a flowchart of a speech synthesis method according to an embodiment of the disclosure.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a flowchart of a speech synthesis method according to another embodiment of the disclosure.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flowchart of a speech synthesis method according to a further embodiment of the disclosure.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a schematic diagram of a target speech synthesis model according to an embodiment of the disclosure.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a block diagram of a speech synthesis apparatus according to an embodiment of the disclosure.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a block diagram of an electronic device used to implement the speech synthesis method according to an embodiment of the disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0015" num="0014">The following describes the exemplary embodiments of the disclosure with reference to the accompanying drawings, which includes various details of the embodiments of the disclosure to facilitate understanding, which shall be considered merely exemplary. Therefore, those of ordinary skill in the art should recognize that various changes and modifications can be made to the embodiments described herein without departing from the scope and spirit of the disclosure. For clarity and conciseness, descriptions of well-known functions and structures are omitted in the following description.</p><p id="p-0016" num="0015">Speech includes technical fields such as speech recognition, speech separation, speech interaction and speech synthesis. Speech is an important direction in the field of artificial intelligence (AI).</p><p id="p-0017" num="0016">Speech synthesis is a technology that enables machines to convert textual information into outputable speech, and involves fields such as acoustics, linguistics, digital signal processing, and computer science.</p><p id="p-0018" num="0017">AI is a technical science that studies and develops theories, methods, technologies and application systems for simulating, extending and expanding human intelligence. Currently, AI technology has been widely used due to the advantages of high degree of automation, high accuracy and low cost</p><p id="p-0019" num="0018">Natural Language Processing (NLP) is a science that studies computer systems that can effectively realize natural language communication, especially software systems. NLP is an important direction in the fields of computer science and AI.</p><p id="p-0020" num="0019">Deep learning, as a new research direction in the field of machine learning, is a science that learns the inherent laws and representation levels of sample data, so that machines can analyze and learn like human and recognize data such as texts, images and sounds. Deep learning has been widely used in speech and image recognition.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a flowchart of a speech synthesis method according to an embodiment of the disclosure.</p><p id="p-0022" num="0021">As illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, a speech synthesis method according to an embodiment of the disclosure includes the following blocks.</p><p id="p-0023" num="0022">At block S<b>101</b>, a text to be synthesized and speech features of a target user are obtained, and predicted first acoustic features are obtained based on the text to be synthesized and the speech features.</p><p id="p-0024" num="0023">It should be noted that the executive body of the speech synthesis method in the embodiment of the disclosure may be a hardware device having data information processing capability and/or a necessary software for driving the hardware device to operate. Optionally, the executive body may include a workstation, a server, a computer, a user terminal and other intelligent devices. The user terminal includes but not limited to a mobile phone, a computer, an intelligent speech interaction device, a smart home appliance, and a vehicle-mounted terminal.</p><p id="p-0025" num="0024">In the embodiment of the disclosure, the text to be synthesized is obtained. It should be noted that the language and type of the text to be synthesized are not limited. For example, the language of the text to be synthesized includes but not limited to Chinese and English, and the type of the text to be synthesized includes but not limited to chat text and speech text.</p><p id="p-0026" num="0025">In the embodiment of the disclosure, the speech features of the target user can be obtained. It is understood that different target users may have different speech features. It should be noted that the types of speech features are not limited herein.</p><p id="p-0027" num="0026">In an embodiment, the speech features of the target user can be stored locally in advance, so that the speech features of the target user can be obtained from the local storage space during speech synthesis, that is, the speech features of the target user can be obtained offline, which effectively reduces the amount of computation of speech synthesis.</p><p id="p-0028" num="0027">In the embodiment of the disclosure, the predicted first acoustic features are obtained based on the text to be synthesized and the speech features. Since the influence of the text to be synthesized and of the speech features of the target user on the predicted first acoustic features is comprehensively considered, the predicted first acoustic features contain the features of the text to be synthesized and the speech features, and thus personalized speech synthesis can be realized.</p><p id="p-0029" num="0028">In an embodiment, obtaining the predicted first acoustic features based on the text to be synthesized and the speech features includes: inputting the text to be synthesized and the speech features into a feature prediction model, so that the feature prediction model outputs the predicted first acoustic features. The feature prediction model can be set according to the actual situation, which is not limited here.</p><p id="p-0030" num="0029">At block S<b>102</b>, a target template audio is obtained from a template audio library based on the text to be synthesized, and second acoustic features of the target template audio are extracted.</p><p id="p-0031" num="0030">In the embodiment of the disclosure, the template audio library is obtained. The template audio library includes a plurality of template audios. It is understood that the template audio library can be set according to the actual situation, which is not limited here.</p><p id="p-0032" num="0031">In the embodiment of the disclosure, the target template audio is obtained from the template audio library based on the text to be synthesized. Further, the second acoustic features of the target template audio are extracted.</p><p id="p-0033" num="0032">In an embodiment, extracting the second acoustic features of the target template audio includes: inputting the target template audio into a feature extraction model, so that the feature extraction model outputs the second acoustic features. The feature extraction model can be set according to the actual situation, which is not limited here.</p><p id="p-0034" num="0033">At block S<b>103</b>, target acoustic features are generated by splicing the first acoustic features and the second acoustic features.</p><p id="p-0035" num="0034">In the embodiment of the disclosure, the target acoustic features are generated by splicing the first acoustic features and the second acoustic features. The generated target acoustic features have both the first acoustic features of the text to be synthesized and the second acoustic features of the target template audio, and thus a good feature representing effect is achieved.</p><p id="p-0036" num="0035">In an embodiment, generating the target acoustic features by splicing the first acoustic features and the second acoustic features includes: obtaining a target template text corresponding to the target template audio; determining splicing positions between the first acoustic features and the second acoustic features based on the target template text and the text to be synthesized; and generating the target acoustic features by splicing the first acoustic features and the second acoustic features according to the splicing positions.</p><p id="p-0037" num="0036">For example, if the text to be synthesized is &#x201c;Your call charges this month is 16 yuan&#x201d;, and the target template text corresponding to the target template audio is &#x201c;Your call charges this month is 100 yuan&#x201d;, the splicing position of feature 2 corresponding to &#x201c;16&#x201d; in the first acoustic features is determined as the feature position corresponding to &#x201c;100&#x201d; in the second acoustic features, and then feature 1 corresponding to &#x201c;Your call charges this month is&#x201d; in the second acoustic features, feature 2 corresponding to &#x201c;16&#x201d; in the first acoustic features, and feature 3 corresponding to &#x201c;yuan&#x201d; in the second acoustic features are spliced, in which, feature 1 is prior to feature 2 in sequence, and feature 2 is prior to feature 3 in sequence.</p><p id="p-0038" num="0037">At block S<b>104</b>, speech synthesis is performed on the text to be synthesized based on the target acoustic features and the speech features, to generate a target speech of the text to be synthesized.</p><p id="p-0039" num="0038">In the embodiment of the disclosure, the target speech of the text to be synthesized is generated by performing the speech synthesis on the text to be synthesized based on the target acoustic features and the speech features.</p><p id="p-0040" num="0039">In an embodiment, performing speech synthesis on the text to be synthesized based on the target acoustic features and the speech features, to generate the target speech of the text to be synthesized includes: inputting the target acoustic features, the speech features and the text to be synthesized into a speech synthesis model, so that the speech synthesis model outputs the target speech of the text to be synthesized. The speech synthesis model can be set according to the actual situation, which is not limited here.</p><p id="p-0041" num="0040">In conclusion, according to the speech synthesis method, the predicted first acoustic features are obtained based on the text to be synthesized and the speech features, and the second acoustic features of the target template audio are extracted. The target acoustic features are obtained by splicing the first acoustic features and the second acoustic features. Then speech synthesis is performed on the text to be synthesized based on the target acoustic features and the speech features, to generate the target speech of the text to be synthesized. Thus, the target acoustic features contain the first acoustic features of the text to be synthesized and the second acoustic features of the target template audio, so that a good feature representing effect is achieved, which facilitates the improvement of the authenticity and naturalness of the target speech, and achieves a better speech synthesis effect.</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a flowchart of a speech synthesis method according to another embodiment of the disclosure.</p><p id="p-0043" num="0042">As illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the speech synthesis method according to the second embodiment of the disclosure includes the following blocks.</p><p id="p-0044" num="0043">At block S<b>201</b>, a text to be synthesized and speech features of a target user are obtained, and the speech features include style features and timbre features, and the predicted first acoustic features are obtained based on the text to be synthesized and the style features.</p><p id="p-0045" num="0044">In the embodiment of the disclosure, there is a correspondence between the identification information of the target user and the speech features of the target user. The identification information can be set according to the actual situation, which is not limited here. For example, the identification information of the target user can be set as &#x201c;Zhang San&#x201d; or &#x201c;Li Si&#x201d;.</p><p id="p-0046" num="0045">In the embodiment of the disclosure, obtaining the speech features of the target user includes: obtaining identification information of the target user; and obtaining the speech features of the target user based on the identification information. Therefore, the speech features of the target user can be obtained based on the identification information of the target user.</p><p id="p-0047" num="0046">For example, a mapping relation or a mapping table between the identification information and the speech features of the target user can be generated in advance. After the identification information of the target user is obtained, the speech features mapped by the identification information are obtained by querying the identification information in the above mapping relation or the mapping table, and the speech features are determined as the speech features of the target user.</p><p id="p-0048" num="0047">In the embodiment of the disclosure, the speech features may include the style features and the timbre features. The style features are used to distinguish different styles of a user, and the timbre features can be used to distinguish different users.</p><p id="p-0049" num="0048">In the embodiment of the disclosure, the predicted first acoustic features are obtained based on the text to be synthesized and the style features.</p><p id="p-0050" num="0049">In an embodiment, obtaining the first acoustic features based on the text to be synthesized and the style features includes: obtaining vector features by performing vectorization processing on the text to be synthesized; obtaining text features of the text to be synthesized by performing convolution processing and bi-directional time loop processing on the vector features; obtaining first splicing features by splicing the text features and the style features; and obtaining the first acoustic features by performing the convolution processing, the bidirectional time loop processing and linear processing sequentially on the first splicing features.</p><p id="p-0051" num="0050">Optionally, obtaining the first splicing features by splicing the text features and the style features includes: determining a sum of the text features and the style features as the first splicing features.</p><p id="p-0052" num="0051">In the embodiment of the disclosure, the acoustic features include at least one of fundamental frequency features, energy features and time duration features. It should be noted that the granularity of the acoustic features is not limited too much, for example, the granularity of the acoustic features may be phoneme granularity.</p><p id="p-0053" num="0052">At block S<b>202</b>, a target template audio is obtained from a template audio library based on the text to be synthesized, and second acoustic features of the target template audio are extracted.</p><p id="p-0054" num="0053">In the embodiment of the disclosure, obtaining the target template audio from the template audio library based on the text to be synthesized includes: obtaining template texts corresponding to template audios in the template audio library; obtaining a similarity between the text to be synthesized and the template texts; and determining the template audio corresponding to the template text with the highest similarity as the target template audio. Therefore, according to the method, the template audio corresponding to the template text with the highest similarity is selected from the template audio library as the target template audio. Since the selected target template audio has the highest matching degree with the text to be synthesized, the effect of speech synthesis is good.</p><p id="p-0055" num="0054">For example, in the intelligent customer service scene, the template audio library includes template audios A, B, C and D, and template audios A, B, C and D correspond to template texts a, b, c and d respectively, in which template text a is &#x201c;Your call charges this month is 100 yuan&#x201d;, template text b is &#x201c;You have successfully added 100 yuan to your credit&#x201d;, template text c is &#x201c;Your account balance is 100 yuan&#x201d;, and template text d is &#x201c;Your remaining universal data for this month is 5 GB&#x201d;. The text to be synthesized is &#x201c;Your call charges this month is 16 yuan&#x201d;, and it can be known that the text to be synthesized has the highest similarity with template text a, and the target template audio can be obtained from the template audio library as template audio A.</p><p id="p-0056" num="0055">At block S<b>203</b>, target acoustic features are generated by splicing the first acoustic features and the second acoustic features.</p><p id="p-0057" num="0056">At block S<b>204</b>, speech synthesis is performed on the text to be synthesized based on the target acoustic features and the speech features, to generate a target speech of the text to be synthesized.</p><p id="p-0058" num="0057">For the relevant content of steps S<b>203</b>-S<b>204</b>, reference may be made to the above embodiments, which will not be repeated here.</p><p id="p-0059" num="0058">In conclusion, according to the speech synthesis method of the embodiment of the disclosure, the speech features include style features and timbre features, and the predicted first acoustic features can be obtained according to the text to be synthesized and the style features. The influence of the text to be synthesized and of the style features of the target user on the predicted first acoustic features can be comprehensively considered, so that the predicted first acoustic features contain the style features of the target user, which helps to improve the personalized speech synthesis.</p><p id="p-0060" num="0059"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flowchart of a speech synthesis method according to a further embodiment of the disclosure.</p><p id="p-0061" num="0060">As illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the speech synthesis method according to a further embodiment of the disclosure includes the following blocks.</p><p id="p-0062" num="0061">At block S<b>301</b>, a text to be synthesized and speech features of a target user are obtained, and predicted first acoustic features are obtained based on the text to be synthesized and the speech features.</p><p id="p-0063" num="0062">At block S<b>302</b>, a target template audio is obtained from a template audio library based on the text to be synthesized, and second acoustic features of the target template audio are extracted.</p><p id="p-0064" num="0063">For the relevant content of steps S<b>301</b>-S<b>302</b>, reference may be made to the foregoing embodiments, and details are not repeated here.</p><p id="p-0065" num="0064">At block S<b>303</b>, a target template text corresponding to the target template audio is obtained. In the embodiment of the disclosure, there is a correspondence between the template audios and the template texts, and different template audios correspond to different template texts, and the target template text corresponding to the target template audio can be obtained.</p><p id="p-0066" num="0065">In an embodiment, the mapping relation or mapping table between the template audios and the template texts can be generated in advance. After obtaining the target template audio, the template text mapped by the target template audio and obtained by querying the above mapping relation or mapping table, is determined as the target template text. It should be noted that, the above mapping relation or mapping table can be set according to the actual situation, which is not limited here.</p><p id="p-0067" num="0066">For example, in the intelligent customer service scene, the template audio library includes template audios A, B, C and D, and template audios A, B, C and D correspond to template texts a, b, c and d respectively, in which template text a is &#x201c;Your call charges this month is 100 yuan&#x201d;, template text b is &#x201c;You have successfully added 100 yuan to your credit&#x201d;, template text c is &#x201c;Your account balance is 100 yuan&#x201d;, and template text d is &#x201c;Your remaining universal data for this month is 5 GB&#x201d;. If the target template audio is template audio A, template text a &#x201c;Your call charges this month is 100 yuan&#x201d; is determined as the target template text.</p><p id="p-0068" num="0067">At block S<b>304</b>, an overlapping text and a difference text between the text to be synthesized and the target template text are obtained.</p><p id="p-0069" num="0068">In the embodiment of the disclosure, the text to be synthesized and the target template text may have identical parts and different parts. The text to be synthesized is compared with the target template text, to obtain the overlapping texts and the difference texts between the text to be synthesized and the target template text.</p><p id="p-0070" num="0069">It is understood that the text to be synthesized and the target template text both include the overlapping text.</p><p id="p-0071" num="0070">In an embodiment, the difference text refers to the text in the text to be synthesized that differs from the target template text.</p><p id="p-0072" num="0071">For example, if the text to be synthesized is &#x201c;Your call charges this month is 16 yuan&#x201d;, and the target template text is &#x201c;Your call charges this month is 100 yuan&#x201d;, the overlapping texts are &#x201c;Your call charges this month is&#x201d; and &#x201c;yuan&#x201d;, and the difference text is &#x201c;16&#x201d;.</p><p id="p-0073" num="0072">At block S<b>305</b>, target first acoustic features corresponding to the difference text are extracted from the first acoustic features, and target second acoustic features corresponding to the overlapping text are extracted from the second acoustic features.</p><p id="p-0074" num="0073">In the embodiments of the disclosure, the text to be synthesized may include the difference text, and the target first acoustic features corresponding to the difference text may be extracted from the first acoustic features corresponding to the text to be synthesized. The target template text corresponding to the target template audio includes the overlapping text, and the target second acoustic features corresponding to the overlapping text may be extracted from the second acoustic features corresponding to the target template audio.</p><p id="p-0075" num="0074">For example, if the text to be synthesized is &#x201c;Your call charges this month is 16 yuan&#x201d; and the target template text is &#x201c;Your call charges this month is 100 yuan&#x201d;, the overlapping texts are &#x201c;Your call charges this month is&#x201d; and &#x201c;yuan&#x201d;, and the difference text may be &#x201c;16&#x201d;, and then the target first acoustic features corresponding to &#x201c;16&#x201d; are extracted from the first acoustic features, and the target second acoustic features corresponding to &#x201c;Your call charges this month is&#x201d; and &#x201c;yuan&#x201d; are extracted from the second acoustic features.</p><p id="p-0076" num="0075">In an embodiment, extracting the target first acoustic features corresponding to the difference text from the first acoustic features includes: obtaining a first start position and a first end position of the difference text in the text to be synthesized; and extracting acoustic features corresponding to positions between the first start position and the first end position from the first acoustic features, and determining the extracted acoustic features as the target first acoustic features. Therefore, the target first acoustic features are accurately extracted from the first acoustic features based on the first start position and the first end position.</p><p id="p-0077" num="0076">For example, if the text to be synthesized is &#x201c;Your call charges this month is 16 yuan&#x201d;, and the difference text is &#x201c;16&#x201d;, the first start position of the difference text &#x201c;16&#x201d; in the text to be synthesized is the text position corresponding to &#x201c;1&#x201d;, and the first end position is the text position corresponding to &#x201c;6&#x201d;. The acoustic features corresponding to positions between the first start position and the first end position are extracted from the first acoustic features and determined as the target first acoustic features.</p><p id="p-0078" num="0077">In an embodiment, extracting the target second acoustic features corresponding to the overlapping text from the second acoustic features includes: obtaining a second start position and a second end position of the overlapping text in the target template text; extracting acoustic features corresponding to positions between the second start position and the second end position from the second acoustic features; and determining the extracted acoustic features as the target second acoustic features. Therefore, the target second acoustic features are extracted from the second acoustic features accurately based on the second start position and the second end position.</p><p id="p-0079" num="0078">For example, if the target template text is &#x201c;Your call charges this month is 100 yuan&#x201d;, and the overlapping texts are &#x201c;Your call charges this month is&#x201d; and &#x201c;yuan&#x201d;, the second start position of the overlapping text &#x201c;Your call charges this month is&#x201d; in the target template text is the text position corresponding to &#x201c;Your&#x201d; and the second end position is the text position corresponding to &#x201c;is&#x201d;, and the second start position and the second end position of the overlapping text &#x201c;yuan&#x201d; in the target template text are both the text position corresponding to &#x201c;yuan&#x201d;, and thus the acoustic features corresponding to the positions between the second start position and the second end position are extracted from the second acoustic features, and the extracted acoustic features may be determined as the target second acoustic features.</p><p id="p-0080" num="0079">In an embodiment, extracting the acoustic features corresponding to the positions between the first start position and the first end position from the first acoustic features include: obtaining a correspondence between the text positions of the text to be synthesized and the feature positions of the first acoustic features; obtaining a third start position and a third end position corresponding to the first start position and the first end position in the first acoustic features based on the correspondence; and extracting acoustic features corresponding to the positions between the third start position and the third end position from the first acoustic features.</p><p id="p-0081" num="0080">It should be noted that, the relevant content of extracting the acoustic features corresponding to the positions between the second start position and the second end position from the second acoustic features, refer to the relevant content of extracting the acoustic features corresponding to the positions between the first start position and the first end position from the first acoustic features, which is not repeated here.</p><p id="p-0082" num="0081">At block S<b>306</b>, target acoustic features are generated by splicing the first acoustic features and the second acoustic features.</p><p id="p-0083" num="0082">For the relevant content of step S<b>306</b>, reference may be made to the above embodiments, which will not be repeated here.</p><p id="p-0084" num="0083">At block S<b>307</b>, second splicing features are generated by splicing the text features of the text to be synthesized, the timbre features and the target acoustic features.</p><p id="p-0085" num="0084">It should be noted that, for the relevant content of the text features of the text to be synthesized, reference may be made to the above embodiments, which will not be repeated here.</p><p id="p-0086" num="0085">In the embodiment of the disclosure, the second splicing features are generated by splicing the text features of the text to be synthesized, the timbre features and the target acoustic features, so that the second splicing features contain the text features of the text to be synthesized, the timbre features and the target acoustic features at the same time, and the feature representing effect is good.</p><p id="p-0087" num="0086">In an embodiment, generating the second splicing features by splicing the text features of the text to be synthesized, the timbre features and the target acoustic features include: determining a sum of the text features, the timbre features and the target acoustic features as the second splicing features.</p><p id="p-0088" num="0087">At block S<b>308</b>, the target speech is synthesized based on the second splicing features.</p><p id="p-0089" num="0088">In an embodiment, synthesizing the target speech based on the second splicing features include: inputting the second splicing features into a speech synthesis model, so that the speech synthesis model outputs the target speech of the text to be synthesized. The speech synthesis model can be set according to the actual situation, which is not limited here.</p><p id="p-0090" num="0089">In conclusion, according to the speech synthesis method of the embodiments of the disclosure, the target first acoustic features corresponding to the difference text are extracted from the first acoustic features based on the overlapping text and the difference text between the text to be synthesized and the target template text. The target second acoustic features corresponding to the overlapping text are extracted from the second acoustic features. The target first acoustic features and the target second acoustic features are spliced to generate the target acoustic features. The text features of the text to be synthesized, the timbre features and the target acoustic features are spliced to generate the second splicing features. The target speech is synthesized based on the second splicing features, so that the target speech has the timbre features of the target user, which is helpful to improve the personalized speech synthesis.</p><p id="p-0091" num="0090">In the embodiment of the disclosure, a pre-trained target speech synthesis model may be obtained, the text to be synthesized, the target template audio and the speech features are input into the target speech synthesis model, so that the target speech synthesis model outputs the target speech of the text to be synthesized. Therefore, speech synthesis performed by the target speech synthesis model is automatically realized, which helps to improve the speech synthesis efficiency.</p><p id="p-0092" num="0091">It is understood that the target speech synthesis model can be set according to the actual situation, which is not limited here. For example, the target speech synthesis model may be a neural network model.</p><p id="p-0093" num="0092">As illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the target speech synthesis model <b>400</b> includes a feature prediction layer <b>401</b>, a first feature extraction layer <b>402</b>, a second feature extraction layer <b>403</b>, a feature splicing layer <b>404</b> and a speech synthesis layer <b>405</b>.</p><p id="p-0094" num="0093">The feature prediction layer <b>401</b> obtains the predicted first acoustic features based on the text to be synthesized and the speech features.</p><p id="p-0095" num="0094">The first feature extraction layer <b>402</b> extracts the second acoustic features of the target template audio.</p><p id="p-0096" num="0095">The second feature extraction layer <b>403</b> extracts the text features of the text to be synthesized.</p><p id="p-0097" num="0096">The feature splicing layer <b>404</b> generates the target acoustic features by splicing the first acoustic features and the second acoustic features, and generates the second splicing features by splicing the text features, the timbre features and the target acoustic features.</p><p id="p-0098" num="0097">The speech synthesis layer <b>405</b> synthesizes the target speech based on the second splicing features.</p><p id="p-0099" num="0098">In an embodiment, training samples may be obtained. The training samples include sample texts to be synthesized, sample target template audios, speech features and sample target speeches of sample target users. It can be understood that the training samples can be set according to the actual situation, which are not limited here.</p><p id="p-0100" num="0099">The speech synthesis model is trained based on the training samples. In response to not satisfying model training end conditions, the next training sample is used to continue training the speech synthesis model and adjust the model parameters until the model training end conditions are satisfied, and then the target speech synthesis model is obtained. The model training end conditions can be set according to the actual situation, which is not limited here. For example, the model training end conditions may include: a number of times of model training reaching a preset threshold value, and a model accuracy reaching a preset accuracy threshold value.</p><p id="p-0101" num="0100"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a block diagram of a speech synthesis apparatus according to an embodiment of the disclosure.</p><p id="p-0102" num="0101">As illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the speech synthesis apparatus <b>500</b> includes: a predicting module <b>501</b>, an extracting module <b>502</b>, a splicing module <b>503</b> and a synthesizing module <b>504</b>.</p><p id="p-0103" num="0102">The predicting module <b>501</b> is configured to obtain a text to be synthesized and speech features of a target user, and obtain predicted first acoustic features based on the text to be synthesized and the speech features.</p><p id="p-0104" num="0103">The extracting module <b>502</b> is configured to obtain a target template audio from a template audio library based on the text to be synthesized, and extract second acoustic features of the target template audio.</p><p id="p-0105" num="0104">The splicing module <b>503</b> is configured to generate target acoustic features by splicing the first acoustic features and the second acoustic features.</p><p id="p-0106" num="0105">The synthesizing module <b>504</b> is configured to perform speech synthesis on the text to be synthesized based on the target acoustic features and the speech features, to generate a target speech of the text to be synthesized.</p><p id="p-0107" num="0106">In the embodiment of the disclosure, the splicing module <b>503</b> includes: a first obtaining unit, a second obtaining unit, an extracting unit and a splicing unit. The first obtaining unit is configured to obtain a target template text corresponding to the target template audio. The second obtaining unit is configured to obtain at least one overlapping text and at least one difference text between the text to be synthesized and the target template text. The extracting unit is configured to extract target first acoustic features corresponding to the difference text from the first acoustic features, and extract target second acoustic features corresponding to the overlapping text from the second acoustic features. The splicing unit is configured to generate the target acoustic features by splicing the target first acoustic features and the target second acoustic features.</p><p id="p-0108" num="0107">In the embodiment of the disclosure, the extracting unit is further configured to: obtain a first start position and a first end position of the difference text in the text to be synthesized; extract acoustic features corresponding to positions between the first start position and the first end position from the first acoustic features, and determine the extracted acoustic features as the target first acoustic features; obtain a second start position and a second end position of the overlapping text in the target template text; and extract acoustic features corresponding to positions between the second start position and the second end position from the second acoustic features, and determine the extracted acoustic features as the target second acoustic features.</p><p id="p-0109" num="0108">In the embodiment of the disclosure, the predicting module <b>501</b> is further configured to: obtain identification information of the target user; and obtain the speech features of the target user based on the identification information.</p><p id="p-0110" num="0109">In the embodiment of the disclosure, the speech features include style features and timbre features, and the predicting module <b>501</b> is further configured to: obtain the first acoustic features based on the text to be synthesized and the style features.</p><p id="p-0111" num="0110">In the embodiment of the disclosure, the predicting module <b>501</b> is further configured to: obtain vector features by performing vectorization processing on the text to be synthesized; obtain text features of the text to be synthesized by performing convolution processing and bi-directional time loop processing on the vector features; obtain first splicing features by splicing the text features and the style features; and obtain the first acoustic features by performing the convolution processing, the bidirectional time loop processing and linear processing sequentially on the first splicing features.</p><p id="p-0112" num="0111">In the embodiment of the disclosure, the synthesizing module <b>504</b> is further configured to: generate second splicing features by splicing the text features of the text to be synthesized, the timbre features and the target acoustic features; and synthesize the target speech based on the second splicing features.</p><p id="p-0113" num="0112">In the embodiment of the disclosure, the apparatus further includes: an inputting module, and the inputting module is configured to: input the text to be synthesized, the target template audio and the speech features into a target speech synthesis model, in which the target speech synthesis model includes a feature prediction layer, a first feature extraction layer, a second feature extraction layer, a feature splicing layer and a speech synthesis layer; obtain the first acoustic features by the feature prediction layer based on the text to be synthesized and the speech features; extract the second acoustic features of the target template audio by the first feature extraction layer; extract the text features of the text to be synthesized by the second feature extraction layer; generate the target acoustic features by splicing the first acoustic features and the second acoustic features using the feature splicing layer, and generate the second splicing features by splicing the text features, the timbre features and the target acoustic features using the feature splicing layer; and synthesize the target speech based on the second splicing features by the speech synthesis layer.</p><p id="p-0114" num="0113">In the embodiment of the disclosure, the extracting module <b>502</b> is further configured to: obtain template texts corresponding to template audios in the template audio library; obtain a similarity between the text to be synthesized and the template texts; and determine the template audio corresponding to the template text with the highest similarity as the target template audio.</p><p id="p-0115" num="0114">In the embodiment of the disclosure, the acoustic features include at least one of fundamental frequency features, energy features and time duration features.</p><p id="p-0116" num="0115">In conclusion, with the speech synthesis apparatus according to the embodiments of the disclosure, the predicted first acoustic features are obtained based on the text to be synthesized and the speech features. The second acoustic features of the target template audio are extracted. The target acoustic features are obtained by splicing the first acoustic features and the second acoustic features. Then, speech synthesis is performed on the text to be synthesized based on the target acoustic features and the speech features, to generate the target speech of the text to be synthesized. Therefore, the target acoustic features contain the first acoustic features of the text to be synthesized and the second acoustic features of the target template audio at the same time, so that a good feature representing effect is achieved, which facilitates to improve the authenticity and naturalness of the target speech, and achieve a better speech synthesis effect.</p><p id="p-0117" num="0116">In the technical solution of the disclosure, the collection, storage, use, processing, transmission, provision and disclosure of the user's personal information involved are all in compliance with relevant laws and regulations, and do not violate public order and good customs.</p><p id="p-0118" num="0117">According to the embodiments of the disclosure, the disclosure provides an electronic device, a readable storage medium and a computer program product.</p><p id="p-0119" num="0118"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a block diagram of an example electronic device <b>600</b> used to implement the embodiments of the disclosure. Electronic devices are intended to represent various forms of digital computers, such as laptop computers, desktop computers, workbenches, personal digital assistants, servers, blade servers, mainframe computers, and other suitable computers. Electronic devices may also represent various forms of mobile devices, such as personal digital processing, cellular phones, smart phones, wearable devices, and other similar computing devices. The components shown here, their connections and relations, and their functions are merely examples, and are not intended to limit the implementation of the disclosure described and/or required herein.</p><p id="p-0120" num="0119">As illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the electronic device <b>600</b> includes: a computing unit <b>601</b> performing various appropriate actions and processes based on computer programs stored in a read-only memory (ROM) <b>602</b> or computer programs loaded from the storage unit <b>608</b> to a random access memory (RAM) <b>603</b>. In the RAM <b>603</b>, various programs and data required for the operation of the device <b>600</b> are stored. The computing unit <b>601</b>, the ROM <b>602</b>, and the RAM <b>603</b> are connected to each other through a bus <b>604</b>. An input/output (I/O) interface <b>605</b> is also connected to the bus <b>604</b>.</p><p id="p-0121" num="0120">Components in the device <b>600</b> are connected to the I/O interface <b>605</b>, including: an inputting unit <b>606</b>, such as a keyboard, a mouse; an outputting unit <b>607</b>, such as various types of displays, speakers; a storage unit <b>608</b>, such as a disk, an optical disk; and a communication unit <b>609</b>, such as network cards, modems, and wireless communication transceivers. The communication unit <b>609</b> allows the device <b>600</b> to exchange information/data with other devices through a computer network such as the Internet and/or various telecommunication networks.</p><p id="p-0122" num="0121">The computing unit <b>601</b> may be various general-purpose and/or dedicated processing components with processing and computing capabilities. Some examples of computing unit <b>601</b> include, but are not limited to, a central processing unit (CPU), a graphics processing unit (GPU), various dedicated AI computing chips, various computing units that run machine learning model algorithms, and a digital signal processor (DSP), and any appropriate processor, controller and microcontroller. The computing unit <b>601</b> executes the various methods and processes described above, such as the speech synthesis method. For example, in some embodiments, the speech synthesis method may be implemented as a computer software program, which is tangibly contained in a machine-readable medium, such as the storage unit <b>608</b>. In some embodiments, part or all of the computer program may be loaded and/or installed on the device <b>600</b> via the ROM <b>602</b> and/or the communication unit <b>609</b>. When the computer program is loaded on the RAM <b>603</b> and executed by the computing unit <b>601</b>, one or more steps of the speech synthesis method described above may be executed. Alternatively, in other embodiments, the computing unit <b>601</b> may be configured to perform the speech synthesis method in any other suitable manner (for example, by means of firmware).</p><p id="p-0123" num="0122">Various implementations of the systems and techniques described above may be implemented by a digital electronic circuit system, an integrated circuit system, Field Programmable Gate Arrays (FPGAs), Application Specific Integrated Circuits (ASICs), Application Specific Standard Products (ASSPs), System on Chip (SOCs), Load programmable logic devices (CPLDs), computer hardware, firmware, software, and/or a combination thereof. These various embodiments may be implemented in one or more computer programs, the one or more computer programs may be executed and/or interpreted on a programmable system including at least one programmable processor, which may be a dedicated or general programmable processor for receiving data and instructions from the storage system, at least one input device and at least one output device, and transmitting the data and instructions to the storage system, the at least one input device and the at least one output device.</p><p id="p-0124" num="0123">The program code configured to implement the method of the disclosure may be written in any combination of one or more programming languages. These program codes may be provided to the processors or controllers of general-purpose computers, dedicated computers, or other programmable data processing devices, so that the program codes, when executed by the processors or controllers, enable the functions/operations specified in the flowchart and/or block diagram to be implemented. The program code may be executed entirely on the machine, partly executed on the machine, partly executed on the machine and partly executed on the remote machine as an independent software package, or entirely executed on the remote machine or server.</p><p id="p-0125" num="0124">In the context of the disclosure, a machine-readable medium may be a tangible medium that may contain or store a program for use by or in combination with an instruction execution system, apparatus, or device. The machine-readable medium may be a machine-readable signal medium or a machine-readable storage medium. A machine-readable medium may include, but is not limited to, an electronic, magnetic, optical, electromagnetic, infrared, or semiconductor system, apparatus, or device, or any suitable combination of the foregoing. More specific examples of machine-readable storage media include electrical connections based on one or more wires, portable computer disks, hard disks, random access memories (RAM), read-only memories (ROM), electrically programmable read-only-memory (EPROM), flash memory, fiber optics, compact disc read-only memories (CD-ROM), optical storage devices, magnetic storage devices, or any suitable combination of the foregoing.</p><p id="p-0126" num="0125">In order to provide interaction with a user, the systems and techniques described herein may be implemented on a computer having a display device (e.g., a Cathode Ray Tube (CRT) or a Liquid Crystal Display (LCD) monitor for displaying information to a user); and a keyboard and pointing device (such as a mouse or trackball) through which the user can provide input to the computer. Other kinds of devices may also be used to provide interaction with the user. For example, the feedback provided to the user may be any form of sensory feedback (e.g., visual feedback, auditory feedback, or haptic feedback), and the input from the user may be received in any form (including acoustic input, speech input, or tactile input).</p><p id="p-0127" num="0126">The systems and technologies described herein can be implemented in a computing system that includes background components (for example, a data server), or a computing system that includes middleware components (for example, an application server), or a computing system that includes front-end components (for example, a user computer with a graphical user interface or a web browser, through which the user can interact with the implementation of the systems and technologies described herein), or a computing system that includes any combination of such background components, intermediate computing components, or front-end components. The components of the system may be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include: a local area network (LAN), a wide area network (WAN), and the Internet.</p><p id="p-0128" num="0127">The computer system may include a client and a server. The client and server are generally remote from each other and interacting through a communication network. The client-server relation is generated by computer programs running on the respective computers and having a client-server relation with each other. The server may be a cloud server, a server of a distributed system, or a server combined with a block-chain.</p><p id="p-0129" num="0128">According to the embodiment of the disclosure, the disclosure also provides a computer program product including computer programs. When the computer programs are executed by a processor, the speech synthesis method according to the embodiments of the disclosure is implemented.</p><p id="p-0130" num="0129">It should be understood that the various forms of processes shown above can be used to reorder, add or delete steps. For example, the steps described in the disclosure could be performed in parallel, sequentially, or in a different order, as long as the desired result of the technical solution disclosed in the disclosure is achieved, which is not limited herein.</p><p id="p-0131" num="0130">The above specific embodiments do not constitute a limitation on the protection scope of the disclosure. Those skilled in the art should understand that various modifications, combinations, sub-combinations and substitutions can be made according to design requirements and other factors. Any modification, equivalent replacement and improvement made within the spirit and principle of this application shall be included in the protection scope of this application.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A speech synthesis method, comprising:<claim-text>obtaining predicted first acoustic features based on a text to be synthesized and speech features of a target user;</claim-text><claim-text>obtaining a target template audio from a template audio library based on the text to be synthesized, and extracting second acoustic features of the target template audio;</claim-text><claim-text>generating target acoustic features by splicing the first acoustic features and the second acoustic features; and</claim-text><claim-text>performing speech synthesis on the text to be synthesized based on the target acoustic features and the speech features, to generate a target speech of the text to be synthesized.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein generating the target acoustic features by splicing the first acoustic features and the second acoustic features, comprises:<claim-text>obtaining a target template text corresponding to the target template audio;</claim-text><claim-text>obtaining an overlapping text and a difference text between the text to be synthesized and the target template text;</claim-text><claim-text>extracting target first acoustic features corresponding to the difference text from the first acoustic features, and extracting target second acoustic features corresponding to the overlapping text from the second acoustic features; and</claim-text><claim-text>generating the target acoustic features by splicing the target first acoustic features and the target second acoustic features.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein extracting the target first acoustic features corresponding to the difference text from the first acoustic features, comprises:<claim-text>obtaining a first start position and a first end position of the difference text in the text to be synthesized; and</claim-text><claim-text>extracting acoustic features corresponding to positions between the first start position and the first end position from the first acoustic features, and determining the acoustic features extracted as the target first acoustic features,</claim-text><claim-text>wherein extracting the target second acoustic features corresponding to the overlapping text from the second acoustic features, comprises:</claim-text><claim-text>obtaining a second start position and a second end position of the overlapping text in the target template text; and</claim-text><claim-text>extracting acoustic features corresponding to positions between the second start position and the second end position from the second acoustic features, and determining the acoustic features extracted as the target second acoustic features.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the speech features of the target user are obtained by:<claim-text>obtaining identification information of the target user; and</claim-text><claim-text>obtaining the speech features of the target user based on the identification information.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the speech features comprise style features and timbre features, and obtaining the predicted first acoustic features based on the text to be synthesized and the speech features, comprises:<claim-text>obtaining the first acoustic features based on the text to be synthesized and the style features.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein obtaining the first acoustic features based on the text to be synthesized and the style features, comprises:<claim-text>obtaining vector features by performing vectorization processing on the text to be synthesized;</claim-text><claim-text>obtaining text features of the text to be synthesized by performing convolution processing and bi-directional time loop processing on the vector features;</claim-text><claim-text>obtaining first splicing features by splicing the text features and the style features; and</claim-text><claim-text>obtaining the first acoustic features by performing the convolution processing, the bi-directional time loop processing and linear processing sequentially on the first splicing features.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein performing the speech synthesis on the text to be synthesized based on the target acoustic features and the speech features, to generate the target speech of the text to be synthesized, comprises:<claim-text>generating second splicing features by splicing the text features of the text to be synthesized, the timbre features and the target acoustic features; and</claim-text><claim-text>synthesizing the target speech based on the second splicing features.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, further comprising:<claim-text>inputting the text to be synthesized, the target template audio and the speech features into a target speech synthesis model, wherein the target speech synthesis model comprises a feature prediction layer, a first feature extraction layer, a second feature extraction layer, a feature splicing layer and a speech synthesis layer;</claim-text><claim-text>obtaining the first acoustic features by the feature prediction layer based on the text to be synthesized and the speech features;</claim-text><claim-text>extracting the second acoustic features of the target template audio by the first feature extraction layer;</claim-text><claim-text>extracting the text features of the text to be synthesized by the second feature extraction layer;</claim-text><claim-text>generating the target acoustic features by splicing the first acoustic features and the second acoustic features by the feature splicing layer, and generating the second splicing features by splicing the text features, the timbre features and the target acoustic features by the feature splicing layer; and</claim-text><claim-text>synthesizing the target speech based on the second splicing features by the speech synthesis layer.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein obtaining the target template audio from the template audio library based on the text to be synthesized, comprises:<claim-text>obtaining template texts corresponding to template audios in the template audio library;</claim-text><claim-text>obtaining a similarity between the text to be synthesized and the template texts; and</claim-text><claim-text>determining a template audio corresponding to the template text with the highest similarity as the target template audio.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the acoustic features comprise at least one of fundamental frequency features, energy features and time duration features.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. An electronic device, comprising:<claim-text>at least one processor; and</claim-text><claim-text>a memory communicatively coupled to the at least one processor;</claim-text><claim-text>wherein the memory stores instructions executable by the at least one processor, when the instructions are executed by the at least one processor, the at least one processor is enabled to perform the following:</claim-text><claim-text>obtaining predicted first acoustic features based on a text to be synthesized and speech features of a target user;</claim-text><claim-text>obtaining a target template audio from a template audio library based on the text to be synthesized, and extracting second acoustic features of the target template audio;</claim-text><claim-text>generating target acoustic features by splicing the first acoustic features and the second acoustic features; and</claim-text><claim-text>performing speech synthesis on the text to be synthesized based on the target acoustic features and the speech features, to generate a target speech of the text to be synthesized.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The device of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein generating the target acoustic features by splicing the first acoustic features and the second acoustic features, comprises:<claim-text>obtaining a target template text corresponding to the target template audio;</claim-text><claim-text>obtaining an overlapping text and a difference text between the text to be synthesized and the target template text;</claim-text><claim-text>extracting target first acoustic features corresponding to the difference text from the first acoustic features, and extracting target second acoustic features corresponding to the overlapping text from the second acoustic features; and</claim-text><claim-text>generating the target acoustic features by splicing the target first acoustic features and the target second acoustic features.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The device of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein extracting the target first acoustic features corresponding to the difference text from the first acoustic features, comprises:<claim-text>obtaining a first start position and a first end position of the difference text in the text to be synthesized; and</claim-text><claim-text>extracting acoustic features corresponding to positions between the first start position and the first end position from the first acoustic features, and determining the acoustic features extracted as the target first acoustic features,</claim-text><claim-text>wherein extracting the target second acoustic features corresponding to the overlapping text from the second acoustic features, comprises:</claim-text><claim-text>obtaining a second start position and a second end position of the overlapping text in the target template text; and</claim-text><claim-text>extracting acoustic features corresponding to positions between the second start position and the second end position from the second acoustic features, and determining the acoustic features extracted as the target second acoustic features.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The device of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the speech features of the target user are obtained by:<claim-text>obtaining identification information of the target user; and</claim-text><claim-text>obtaining the speech features of the target user based on the identification information.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The device of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the speech features comprise style features and timbre features, and obtaining the predicted first acoustic features based on the text to be synthesized and the speech features, comprises:<claim-text>obtaining the first acoustic features based on the text to be synthesized and the style features.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The device of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein obtaining the first acoustic features based on the text to be synthesized and the style features, comprises:<claim-text>obtaining vector features by performing vectorization processing on the text to be synthesized;</claim-text><claim-text>obtaining text features of the text to be synthesized by performing convolution processing and bi-directional time loop processing on the vector features;</claim-text><claim-text>obtaining first splicing features by splicing the text features and the style features; and</claim-text><claim-text>obtaining the first acoustic features by performing the convolution processing, the bi-directional time loop processing and linear processing sequentially on the first splicing features.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The device of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein performing the speech synthesis on the text to be synthesized based on the target acoustic features and the speech features, to generate the target speech of the text to be synthesized, comprises:<claim-text>generating second splicing features by splicing the text features of the text to be synthesized, the timbre features and the target acoustic features; and</claim-text><claim-text>synthesizing the target speech based on the second splicing features.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The device of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the at least one processor is further configured to perform the following:<claim-text>inputting the text to be synthesized, the target template audio and the speech features into a target speech synthesis model, wherein the target speech synthesis model comprises a feature prediction layer, a first feature extraction layer, a second feature extraction layer, a feature splicing layer and a speech synthesis layer;</claim-text><claim-text>obtaining the first acoustic features by the feature prediction layer based on the text to be synthesized and the speech features;</claim-text><claim-text>extracting the second acoustic features of the target template audio by the first feature extraction layer;</claim-text><claim-text>extracting the text features of the text to be synthesized by the second feature extraction layer;</claim-text><claim-text>generating the target acoustic features by splicing the first acoustic features and the second acoustic features by the feature splicing layer, and generating the second splicing features by splicing the text features, the timbre features and the target acoustic features by the feature splicing layer; and</claim-text><claim-text>synthesizing the target speech based on the second splicing features by the speech synthesis layer.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The device of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein obtaining the target template audio from the template audio library based on the text to be synthesized, comprises:<claim-text>obtaining template texts corresponding to template audios in the template audio library;</claim-text><claim-text>obtaining a similarity between the text to be synthesized and the template texts; and</claim-text><claim-text>determining a template audio corresponding to the template text with the highest similarity as the target template audio.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. A non-transitory computer-readable storage medium having computer instructions stored thereon, wherein the computer instructions are configured to cause a computer to implement the method of <claim-ref idref="CLM-00001">claim 1</claim-ref>.</claim-text></claim></claims></us-patent-application>