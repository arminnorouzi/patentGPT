<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004414A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004414</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17367470</doc-number><date>20210705</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>455</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>50</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>L</subclass><main-group>12</main-group><subgroup>24</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>45558</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>5083</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>L</subclass><main-group>41</main-group><subgroup>0893</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>L</subclass><main-group>41</main-group><subgroup>0806</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>2009</main-group><subgroup>45595</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>2009</main-group><subgroup>4557</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">AUTOMATED INSTANTIATION AND MANAGEMENT OF MOBILE NETWORKS</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>VNware, Inc.</orgname><address><city>Palo Alto</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Lin</last-name><first-name>Xiaojun</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Cui</last-name><first-name>Leon</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Pannem</last-name><first-name>Hemanth Kumar</first-name><address><city>Palo Alto</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Tie</last-name><first-name>Xiaoli</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>VNware, Inc.</orgname><role>02</role><address><city>Palo Alto</city><state>CA</state><country>US</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">The current document is directed to methods and subsystems that instantiate and manage mobile-network computational infrastructure. The currently disclosed improved mobile-network-computational-infrastructure orchestration system employs several layers of containerized-application orchestration and management systems. For increased efficiency and security, mobile-network-specific operators are added to the containerized-application orchestration layers in order to extend the functionalities of the containerized-application orchestration layers and move virtualization-layer dependencies from the mobile-network-computational-infrastructure orchestration system down into the containerized-application orchestration layers. The improved mobile-network-computational-infrastructure orchestration system is responsible for generating, from an input mobile-network computational-infrastructure specification, one or more workload resource specifications and a node policy that are input to a containerized-application-orchestration layer. The containerized-application-orchestration layers instantiate and manage worker nodes that execute mobile-network application instances that implement VNFs and CNFs according to the one or more workload resource specifications and the node policy.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="197.61mm" wi="158.75mm" file="US20230004414A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="211.50mm" wi="170.26mm" file="US20230004414A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="219.96mm" wi="149.44mm" orientation="landscape" file="US20230004414A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="217.00mm" wi="154.26mm" orientation="landscape" file="US20230004414A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="192.62mm" wi="157.06mm" orientation="landscape" file="US20230004414A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="176.02mm" wi="157.99mm" orientation="landscape" file="US20230004414A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="177.55mm" wi="167.13mm" orientation="landscape" file="US20230004414A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="215.98mm" wi="165.95mm" orientation="landscape" file="US20230004414A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="209.63mm" wi="168.06mm" orientation="landscape" file="US20230004414A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="201.34mm" wi="158.75mm" orientation="landscape" file="US20230004414A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="227.67mm" wi="158.33mm" orientation="landscape" file="US20230004414A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="209.89mm" wi="160.53mm" orientation="landscape" file="US20230004414A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="227.16mm" wi="145.37mm" orientation="landscape" file="US20230004414A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="225.64mm" wi="159.09mm" orientation="landscape" file="US20230004414A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="221.83mm" wi="148.76mm" orientation="landscape" file="US20230004414A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="224.28mm" wi="161.71mm" orientation="landscape" file="US20230004414A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="213.70mm" wi="159.77mm" file="US20230004414A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="183.30mm" wi="162.14mm" orientation="landscape" file="US20230004414A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="222.84mm" wi="139.70mm" orientation="landscape" file="US20230004414A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00019" num="00019"><img id="EMI-D00019" he="212.60mm" wi="113.88mm" orientation="landscape" file="US20230004414A1-20230105-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00020" num="00020"><img id="EMI-D00020" he="215.22mm" wi="146.47mm" orientation="landscape" file="US20230004414A1-20230105-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00021" num="00021"><img id="EMI-D00021" he="133.60mm" wi="145.54mm" orientation="landscape" file="US20230004414A1-20230105-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00022" num="00022"><img id="EMI-D00022" he="171.53mm" wi="144.70mm" orientation="landscape" file="US20230004414A1-20230105-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00023" num="00023"><img id="EMI-D00023" he="231.31mm" wi="150.71mm" orientation="landscape" file="US20230004414A1-20230105-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00024" num="00024"><img id="EMI-D00024" he="202.86mm" wi="145.46mm" orientation="landscape" file="US20230004414A1-20230105-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00025" num="00025"><img id="EMI-D00025" he="203.03mm" wi="157.23mm" orientation="landscape" file="US20230004414A1-20230105-D00025.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00026" num="00026"><img id="EMI-D00026" he="214.04mm" wi="145.37mm" orientation="landscape" file="US20230004414A1-20230105-D00026.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00027" num="00027"><img id="EMI-D00027" he="214.21mm" wi="147.91mm" orientation="landscape" file="US20230004414A1-20230105-D00027.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00028" num="00028"><img id="EMI-D00028" he="162.90mm" wi="157.48mm" orientation="landscape" file="US20230004414A1-20230105-D00028.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00029" num="00029"><img id="EMI-D00029" he="201.42mm" wi="158.67mm" orientation="landscape" file="US20230004414A1-20230105-D00029.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00030" num="00030"><img id="EMI-D00030" he="225.81mm" wi="170.35mm" orientation="landscape" file="US20230004414A1-20230105-D00030.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00031" num="00031"><img id="EMI-D00031" he="228.09mm" wi="171.28mm" file="US20230004414A1-20230105-D00031.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00032" num="00032"><img id="EMI-D00032" he="203.54mm" wi="166.71mm" file="US20230004414A1-20230105-D00032.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00033" num="00033"><img id="EMI-D00033" he="151.47mm" wi="145.12mm" file="US20230004414A1-20230105-D00033.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00034" num="00034"><img id="EMI-D00034" he="185.25mm" wi="164.51mm" file="US20230004414A1-20230105-D00034.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00035" num="00035"><img id="EMI-D00035" he="189.99mm" wi="167.05mm" file="US20230004414A1-20230105-D00035.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00036" num="00036"><img id="EMI-D00036" he="184.15mm" wi="168.66mm" file="US20230004414A1-20230105-D00036.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00037" num="00037"><img id="EMI-D00037" he="184.07mm" wi="164.68mm" file="US20230004414A1-20230105-D00037.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00038" num="00038"><img id="EMI-D00038" he="183.90mm" wi="164.59mm" file="US20230004414A1-20230105-D00038.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00039" num="00039"><img id="EMI-D00039" he="184.74mm" wi="164.59mm" file="US20230004414A1-20230105-D00039.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00040" num="00040"><img id="EMI-D00040" he="184.23mm" wi="180.93mm" file="US20230004414A1-20230105-D00040.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00041" num="00041"><img id="EMI-D00041" he="184.23mm" wi="164.59mm" file="US20230004414A1-20230105-D00041.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00042" num="00042"><img id="EMI-D00042" he="191.52mm" wi="164.51mm" file="US20230004414A1-20230105-D00042.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00043" num="00043"><img id="EMI-D00043" he="207.52mm" wi="165.61mm" file="US20230004414A1-20230105-D00043.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00044" num="00044"><img id="EMI-D00044" he="203.03mm" wi="164.68mm" file="US20230004414A1-20230105-D00044.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00045" num="00045"><img id="EMI-D00045" he="202.52mm" wi="164.59mm" file="US20230004414A1-20230105-D00045.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00046" num="00046"><img id="EMI-D00046" he="213.44mm" wi="169.59mm" file="US20230004414A1-20230105-D00046.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00047" num="00047"><img id="EMI-D00047" he="213.28mm" wi="163.83mm" file="US20230004414A1-20230105-D00047.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00048" num="00048"><img id="EMI-D00048" he="93.64mm" wi="156.55mm" file="US20230004414A1-20230105-D00048.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00049" num="00049"><img id="EMI-D00049" he="233.93mm" wi="166.20mm" file="US20230004414A1-20230105-D00049.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00050" num="00050"><img id="EMI-D00050" he="189.31mm" wi="97.28mm" orientation="landscape" file="US20230004414A1-20230105-D00050.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00051" num="00051"><img id="EMI-D00051" he="221.57mm" wi="108.46mm" orientation="landscape" file="US20230004414A1-20230105-D00051.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0002" num="0001">The current document is directed to distributed-computer-systems and, in particular, to methods and subsystems that automatically and efficiently instantiate and manage cloud-based mobile networks.</p><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">During the past seven decades. electronic computing has evolved from primitive, vacuum-tube-based computer systems, initially developed during the 1940s, to modern electronic computing systems in which large numbers of multi-processor servers, work stations. and other individual computing systems are networked together with large-capacity data-storage devices and other electronic devices to produce geographically distributed computing systems with hundreds of thousands, millions, or more components that provide enormous computational bandwidths and data-storage capacities. These large, distributed computing systems are made possible by advances in computer networking, distributed operating systems and applications, data-storage appliances, computer hardware, and software technologies. However. despite all of these advances, the rapid increase in the size and complexity of computing systems has been accompanied by numerous scaling issues and technical challenges, including technical challenges associated with distributed-system management. As new distributed-computing technologies are developed, and as general hardware and software technologies continue to advance, the current trend towards ever-larger and more complex distributed computing systems appears likely to continue well into the future.</p><p id="p-0004" num="0003">The 5G mobile-network architecture is an example of a complex distributed computing system. 5G mobile networks are rapidly moving towards cloud implementations based on cloud-native network functions (&#x201c;CNFs&#x201d;) and virtual network functions (&#x201c;VNFs&#x201d;) for many reasons, including reduction of latencies associated with transmission of packets between base-station controllers and core functionalities implemented in centralized data centers, increased flexibility in distributing functionalities among local, regional, and national data centers, and increased ability to rapidly update mobile-network functionalities and implementations. However, due to the great complexity of mobile-and networking systems, instantiation and management of such systems are associated with many technical difficulties and challenges. As cloud-based 5G mobile networks increasingly replace older technologies, vendors, developers, managers, and. ultimately, users of cloud-based 5G mobile networks continue to seek more time-efficient, cost-efficient, and reliable implementations, particularly from the standpoint of mobile-network computational-infrastructure instantiation and subsequent management.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0005" num="0004">The current document is directed to methods and subsystems that instantiate and manage mobile-network computational infrastructure. The currently disclosed improved mobile-network-computational-infrastructure orchestration system employs several layers of containerized-application orchestration and management systems. For increased efficiency and security, mobile-network-specific operators are added to the containerized-application orchestration layers in order to extend the functionalities of the containerized-application orchestration layers and move virtualization-layer dependencies from the mobile-network-computational-infrastructure orchestration system down into the containerized-application orchestration layers. The improved mobile-network-computational-infrastructure orchestration system is responsible for generating. from an input mobile-network computational-infrastructure specification, one or more workload resource specifications and a node policy that are input to a containerized-application-orchestration layer. The containerized-application-orchestration layers instantiate and manage worker nodes that execute mobile-network application instances that implement VNFs and CNFs according to the one or more workload resource specifications and the node policy.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0006" num="0005"><figref idref="DRAWINGS">FIG. <b>1</b></figref> provides a general architectural diagram for various types of computers.</p><p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates an Internet-connected distributed computing system.</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates cloud computing.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates generalized hardware and software components of a general-purpose computer system, such as a general-purpose computer system having an architecture similar to that shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIGS. <b>5</b>A-D</figref> illustrate two types of virtual machine and virtual-machine execution environments.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates an OVF package.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates virtual data centers provided as an abstraction of underlying physical-data-center hardware components.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates virtual-machine components of a VI-management-server and physical servers of a physical data center above which a virtual-data-center interface is provided by the VI-management-server.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>9</b></figref> illustrates a cloud-director level of abstraction.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>10</b></figref> illustrates virtual-cloud-connector nodes (&#x201c;VCC nodes&#x201d;) and a VCC server, components of a distributed system that provides multi-cloud aggregation and that includes a cloud-connector server and cloud-connector nodes that cooperate to provide services that are distributed across multiple clouds.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>11</b></figref> illustrates the Open Systems Interconnection model (&#x201c;OSI model&#x201d;) that characterizes many modern approaches to implementation of communications systems that interconnect computers.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIGS. <b>12</b>A-B</figref> illustrate a layer-2-over-layer-3 encapsulation technology on which virtualized networking can be based.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>13</b></figref> illustrates virtualization of two communicating servers.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>14</b></figref> illustrates a virtual distributed computer system based on one or more distributed computer systems.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>15</b></figref> illustrates a fundamental Kubernetes abstraction.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>16</b></figref> illustrates a next level of abstraction provided by Kubernetes, referred to as a &#x201c;Kubernetes cluster.&#x201d;</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>17</b></figref> illustrates the logical contents of a pod.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>18</b></figref> illustrates the logical contents of a Kubernetes management node and a Kubernetes worker node.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIGS. <b>19</b>A-E</figref> illustrate operation of a Kubernetes cluster.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>20</b></figref> illustrates the Tanzu Kubernetes Grid (&#x201c;TKG&#x201d;) containerized-application orchestration system.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>21</b></figref> illustrates an older-technology mobile network.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>22</b></figref> illustrates newer-technology mobile network based largely on packet-based-network communications.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>23</b></figref> provides a block diagram for the various logical components of a 5G mobile network.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>24</b></figref> illustrates the nature of VNF and CNF implementations.</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>25</b></figref> illustrates the nature of certain mobile-network-application-execution-environment requirements.</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIGS. <b>26</b>A-H</figref> illustrate a current approach to instantiating and managing a mobile network implemented as CNFs and VNFs in multiple distributed computing systems, data centers, and/or cloud-computing facilities.</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIGS. <b>27</b>A-D</figref> illustrate operation of the improved TCA using illustration conventions employed in <figref idref="DRAWINGS">FIGS. <b>26</b>A-H</figref>, discussed in the previous subsection of this document.</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIGS. <b>28</b>A-B</figref> show an example VMConfig custom resource definition.</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIGS. <b>29</b>A-D</figref> show an example of a NodeConfig custom resource definition.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0035" num="0034">The current document is directed to methods and subsystems that that instantiate and manage mobile-network computational infrastructure. In a first subsection. below, a detailed description of computer hardware, complex computational systems, and virtualization is provided with reference to <figref idref="DRAWINGS">FIGS. <b>1</b>-<b>14</b></figref>. In a second subsection. the Kubernetes orchestration system is discussed, with reference to <figref idref="DRAWINGS">FIGS. <b>15</b>-<b>20</b></figref>. Mobile-network infrastructure is discussed in a third subsection, with reference to <figref idref="DRAWINGS">FIGS. <b>21</b>-<b>24</b></figref>. A currently available mobile-network-computational-infrastructure instantiation and management subsystem is discussed in a fourth subsection, with reference to <figref idref="DRAWINGS">FIGS. <b>25</b>-<b>26</b>H</figref>. The currently disclosed methods and systems are discussed with reference to <figref idref="DRAWINGS">FIGS. <b>27</b>A-D</figref>.</p><heading id="h-0006" level="1">Computer Hardware, Complex Computational Systems, and Virtualization</heading><p id="p-0036" num="0035">The term &#x201c;abstraction&#x201d; is not, in any way, intended to mean or suggest an abstract idea or concept. Computational abstractions are tangible, physical interfaces that are implemented, ultimately, using physical computer hardware, data-storage devices, and communications systems. Instead, the term &#x201c;abstraction&#x201d; refers, in the current discussion, to a logical level of functionality encapsulated within one or more concrete, tangible. physically-implemented computer systems with defined interfaces through which electronically-encoded data is exchanged, process execution launched, and electronic services are provided. Interfaces may include graphical and textual data displayed on physical display devices as well as computer programs and routines that control physical computer processors to carry out various tasks and operations and that are invoked through electronically implemented application programming interfaces (&#x201c;APIs&#x201d;) and other electronically implemented interfaces. There is a tendency among those unfamiliar with modern technology and science to misinterpret the terms &#x201c;abstract&#x201d; and &#x201c;abstraction,&#x201d; when used to describe certain aspects of modern computing. For example. one frequently encounters assertions that, because a computational system is described in terms of abstractions, functional layers, and interfaces, the computational system is somehow different from a physical machine or device. Such allegations are unfounded. One only needs to disconnect a computer system or group of computer systems from their respective power supplies to appreciate the physical, machine nature of complex computer technologies. One also frequently encounters statements that characterize a computational technology as being &#x201c;only software,&#x201d; and thus not a machine or device. Software is essentially a sequence of encoded symbols, such as a printout of a computer program or digitally encoded computer instructions sequentially stored in a file on an optical disk or within an electromechanical mass-storage device. Software alone can do nothing. It is only when encoded computer instructions are loaded into an electronic memory within a computer system and executed on a physical processor that so-called &#x201c;software implemented&#x201d; functionality is provided. The digitally encoded computer instructions are an essential and physical control component of processor-controlled machines and devices, no less essential and physical than a cam-shaft control system in an internal-combustion engine. Multi-cloud aggregations, cloud-computing services, virtual-machine containers and virtual machines, communications interfaces. and many of the other topics discussed below are tangible, physical components of physical, electro-optical-mechanical computer systems.</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>1</b></figref> provides a general architectural diagram for various types of computers. The computer system contains one or multiple central processing units (&#x201c;CPUs&#x201d;) <b>102</b>-<b>105</b>, one or more electronic memories <b>108</b> interconnected with the CPUs by a CPU/memory-subsystem bus <b>110</b> or multiple busses, a first bridge <b>112</b> that interconnects the CPU/memory-subsystem bus <b>110</b> with additional busses <b>114</b> and <b>116</b>, or other types of high-speed interconnection media. including multiple, high-speed serial interconnects. These busses or serial interconnections, in turn, connect the CPUs and memory with specialized processors, such as a graphics processor <b>118</b>. and with one or more additional bridges <b>120</b>, which are interconnected with high-speed serial links or with multiple controllers <b>122</b>-<b>127</b>. such as controller <b>127</b>, that provide access to various different types of mass-storage devices <b>128</b>, electronic displays, input devices, and other such components. subcomponents. and computational resources. It should be noted that computer-readable data-storage devices include optical and electromagnetic disks, electronic memories, and other physical data-storage devices. Those familiar with modern science and technology appreciate that electromagnetic radiation and propagating signals do not store data for subsequent retrieval and can transiently &#x201c;store&#x201d; only a byte or less of information per mile, far less information than needed to encode even the simplest of routines.</p><p id="p-0038" num="0037">Of course. there are many different types of computer-system architectures that differ from one another in the number of different memories, including different types of hierarchical cache memories, the number of processors and the connectivity of the processors with other system components, the number of internal communications busses and serial links, and in many other ways. However, computer systems generally execute stored programs by fetching instructions from memory and executing the instructions in one or more processors. Computer systems include general-purpose computer systems, such as personal computers (&#x201c;PCs&#x201d;), various types of servers and workstations, and higher-end mainframe computers. but may also include a plethora of various types of special-purpose computing devices, including data-storage systems, communications routers, network nodes, tablet computers. and mobile telephones.</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates an Internet-connected distributed computing system. As communications and networking technologies have evolved in capability and accessibility, and as the computational bandwidths, data-storage capacities, and other capabilities and capacities of various types of computer systems have steadily and rapidly increased, much of modern computing now generally involves large distributed systems and computers interconnected by local networks, wide-area networks. wireless communications, and the Internet. <figref idref="DRAWINGS">FIG. <b>2</b></figref> shows a typical distributed system in which a large number of PCs <b>202</b>-<b>205</b>, a high-end distributed mainframe system <b>210</b> with a large data-storage system <b>212</b>. and a large computer center <b>214</b> with large numbers of rack-mounted servers or blade servers all interconnected through various communications and networking systems that together comprise the Internet <b>216</b>. Such distributed computing systems provide diverse arrays of functionalities. For example, a PC user sitting in a home office may access hundreds of millions of different web sites provided by hundreds of thousands of different web servers throughout the world and may access high-computational-bandwidth computing services from remote computer facilities for running complex computational tasks.</p><p id="p-0040" num="0039">Until recently, computational services were generally provided by computer systems and data centers purchased. configured, managed, and maintained by service-provider organizations. For example, an e-commerce retailer generally purchased, configured. managed, and maintained a data center including numerous web servers, back-end computer systems, and data-storage systems for serving web pages to remote customers, receiving orders through the web-page interface, processing the orders, tracking completed orders, and other myriad different tasks associated with an e-commerce enterprise.</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates cloud computing. In the recently developed cloud-computing paradigm, computing cycles and data-storage facilities are provided to organizations and individuals by cloud-computing providers. In addition, larger organizations may elect to establish private cloud-computing facilities in addition to. or instead of, subscribing to computing services provided by public cloud-computing service providers. In <figref idref="DRAWINGS">FIG. <b>3</b></figref>, a system administrator for an organization, using a PC <b>302</b>, accesses the organization's private cloud <b>304</b> through a local network <b>306</b> and private-cloud interface <b>308</b> and also accesses, through the Internet <b>310</b>, a public cloud <b>312</b> through a public-cloud services interface <b>314</b>. The administrator can. in either the case of the private cloud <b>304</b> or public cloud <b>312</b>, configure virtual computer systems and even entire virtual data centers and launch execution of application programs on the virtual computer systems and virtual data centers in order to carry out any of many different types of computational tasks. As one example, a small organization may configure and run a virtual data center within a public cloud that executes web servers to provide an e-commerce interface through the public cloud to remote customers of the organization, such as a user viewing the organization's e-commerce web pages on a remote user system <b>316</b>.</p><p id="p-0042" num="0041">Cloud-computing facilities are intended to provide computational bandwidth and data-storage services much as utility companies provide electrical power and water to consumers. Cloud computing provides enormous advantages to small organizations without the resources to purchase. manage. and maintain in-house data centers. Such organizations can dynamically add and delete virtual computer systems from their virtual data centers within public clouds in order to track computational-bandwidth and data-storage needs, rather than purchasing sufficient computer systems within a physical data center to handle peak computational-bandwidth and data-storage demands. Moreover, small organizations can completely avoid the overhead of maintaining and managing physical computer systems, including hiring and periodically retraining information-technology specialists and continuously paying for operating-system and database-management-system upgrades. Furthermore, cloud-computing interfaces allow for easy and straightforward configuration of virtual computing facilities. flexibility in the types of applications and operating systems that can be configured, and other functionalities that are useful even for owners and administrators of private cloud-computing facilities used by a single organization.</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates generalized hardware and software components of a general-purpose computer system, such as a general-purpose computer system having an architecture similar to that shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. The computer system <b>400</b> is often considered to include three fundamental layers: (1) a hardware layer or level <b>402</b>; (2) an operating-system layer or level <b>404</b>: and (3) an application-program layer or level <b>406</b>. The hardware layer <b>402</b> includes one or more processors <b>408</b>, system memory <b>410</b>, various different types of input-output (&#x201c;I/O&#x201d;) devices <b>410</b> and <b>412</b>, and mass-storage devices <b>414</b>. Of course, the hardware level also includes many other components, including power supplies, internal communications links and busses, specialized integrated circuits, many different types of processor-controlled or microprocessor-controlled peripheral devices and controllers, and many other components. The operating system <b>404</b> interfaces to the hardware level <b>402</b> through a low-level operating system and hardware interface <b>416</b> generally comprising a set of non-privileged computer instructions <b>418</b>, a set of privileged computer instructions <b>420</b>, a set of non-privileged registers and memory addresses <b>422</b>, and a set of privileged registers and memory addresses <b>424</b>. In general, the operating system exposes non-privileged instructions, non-privileged registers, and non-privileged memory addresses <b>426</b> and a system-call interface <b>428</b> as an operating-system interface <b>430</b> to application programs <b>432</b>-<b>436</b> that execute within an execution environment provided to the application programs by the operating system. The operating system, alone, accesses the privileged instructions. privileged registers, and privileged memory addresses. By reserving access to privileged instructions, privileged registers, and privileged memory addresses, the operating system can ensure that application programs and other higher-level computational entities cannot interfere with one another's execution and cannot change the overall state of the computer system in ways that could deleteriously impact system operation. The operating system includes many internal components and modules, including a scheduler <b>442</b>. memory management <b>444</b>. a file system <b>446</b>, device drivers <b>448</b>, and many other components and modules. To a certain degree. modern operating systems provide numerous levels of abstraction above the hardware level, including virtual memory. which provides to each application program and other computational entities a separate, large, linear memory-address space that is mapped by the operating system to various electronic memories and mass-storage devices. The scheduler orchestrates interleaved execution of various different application programs and higher-level computational entities, providing to each application program a virtual, stand-alone system devoted entirely to the application program. From the application program's standpoint. the application program executes continuously without concern for the need to share processor resources and other system resources with other application programs and higher-level computational entities. The device drivers abstract details of hardware-component operation. allowing application programs to employ the system-call interface for transmitting and receiving data to and from communications networks, mass-storage devices, and other I/O devices and subsystems. The file system <b>436</b> facilitates abstraction of mass-storage-device and memory resources as a high-level, easy-to-access, file-system interface. Thus, the development and evolution of the operating system has resulted in the generation of a type of multi-faceted virtual execution environment for application programs and other higher-level computational entities.</p><p id="p-0044" num="0043">While the execution environments provided by operating systems have proved to be an enormously successful level of abstraction within computer systems. the operating-system-provided level of abstraction is nonetheless associated with difficulties and challenges for developers and users of application programs and other higher-level computational entities. One difficulty arises from the fact that there are many different operating systems that run within various different types of computer hardware. In many cases, popular application programs and computational systems are developed to run on only a subset of the available operating systems and can therefore be executed within only a subset of the various different types of computer systems on which the operating systems are designed to run. Often, even when an application program or other computational system is ported to additional operating systems, the application program or other computational system can nonetheless run more efficiently on the operating systems for which the application program or other computational system was originally targeted. Another difficulty arises from the increasingly distributed nature of computer systems. Although distributed operating systems are the subject of considerable research and development efforts, many of the popular operating systems are designed primarily for execution on a single computer system. In many cases, it is difficult to move application programs, in real time, between the different computer systems of a distributed computing system for high-availability, fault-tolerance, and load-balancing purposes. The problems are even greater in heterogeneous distributed computing systems which include different types of hardware and devices running different types of operating systems. Operating systems continue to evolve, as a result of which certain older application programs and other computational entities may be incompatible with more recent versions of operating systems for which they are targeted. creating compatibility issues that are particularly difficult to manage in large distributed systems.</p><p id="p-0045" num="0044">For all of these reasons. a higher level of abstraction. referred to as the &#x201c;virtual machine,&#x201d; has been developed and evolved to further abstract computer hardware in order to address many difficulties and challenges associated with traditional computing systems, including the compatibility issues discussed above. <figref idref="DRAWINGS">FIGS. <b>5</b>A-D</figref> illustrate several types of virtual machine and virtual-machine execution environments. <figref idref="DRAWINGS">FIGS. <b>5</b>A-B</figref> use the same illustration conventions as used in <figref idref="DRAWINGS">FIG. <b>4</b></figref>. <figref idref="DRAWINGS">FIG. <b>5</b>A</figref> shows a first type of virtualization. The computer system <b>500</b> in <figref idref="DRAWINGS">FIG. <b>5</b>A</figref> includes the same hardware layer <b>502</b> as the hardware layer <b>402</b> shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>. However, rather than providing an operating system layer directly above the hardware layer. as in <figref idref="DRAWINGS">FIG. <b>4</b></figref>. the virtualized computing environment illustrated in <figref idref="DRAWINGS">FIG. <b>5</b>A</figref> features a virtualization layer <b>504</b> that interfaces through a virtualization-layer/hardware-layer interface <b>506</b>, equivalent to interface <b>416</b> in <figref idref="DRAWINGS">FIG. <b>4</b></figref>. to the hardware. The virtualization layer provides a hardware-like interface <b>508</b> to a number of virtual machines, such as virtual machine <b>510</b>, executing above the virtualization layer in a virtual-machine layer <b>512</b>. Each virtual machine includes one or more application programs or other higher-level computational entities packaged together with an operating system, referred to as a &#x201c;guest operating system,&#x201d; such as application <b>514</b> and guest operating system <b>516</b> packaged together within virtual machine <b>510</b>. Each virtual machine is thus equivalent to the operating-system layer <b>404</b> and application-program layer <b>406</b> in the general-purpose computer system shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>. Each guest operating system within a virtual machine interfaces to the virtualization-layer interface <b>508</b> rather than to the actual hardware interface <b>506</b>. The virtualization layer partitions hardware resources into abstract virtual-hardware layers to which each guest operating system within a virtual machine interfaces. The guest operating systems within the virtual machines. in general. are unaware of the virtualization layer and operate as if they were directly accessing a true hardware interface. The virtualization layer ensures that each of the virtual machines currently executing within the virtual environment receive a fair allocation of underlying hardware resources and that all virtual machines receive sufficient resources to progress in execution. The virtualization-layer interface <b>508</b> may differ for different guest operating systems. For example, the virtualization layer is generally able to provide virtual hardware interfaces for a variety of different types of computer hardware. This allows, as one example, a virtual machine that includes a guest operating system designed for a particular computer architecture to run on hardware of a different architecture. The number of virtual machines need not be equal to the number of physical processors or even a multiple of the number of processors.</p><p id="p-0046" num="0045">The virtualization layer includes a virtual-machine-monitor module <b>518</b> (&#x201c;VMM&#x201d;) that virtualizes physical processors in the hardware layer to create virtual processors on which each of the virtual machines executes. For execution efficiency, the virtualization layer attempts to allow virtual machines to directly execute non-privileged instructions and to directly access non-privileged registers and memory. However, when the guest operating system within a virtual machine accesses virtual privileged instructions. virtual privileged registers. and virtual privileged memory through the virtualization-layer interface <b>508</b>, the accesses result in execution of virtualization-layer code to simulate or emulate the privileged resources. The virtualization layer additionally includes a kernel module <b>520</b> that manages memory, communications, and data-storage machine resources on behalf of executing virtual machines (&#x201c;VM kernel&#x201d;). The VM kernel, for example, maintains shadow page tables on each virtual machine so that hardware-level virtual-memory facilities can be used to process memory accesses. The VM kernel additionally includes routines that implement virtual communications and data-storage devices as well as device drivers that directly control the operation of underlying hardware communications and data-storage devices. Similarly, the VM kernel virtualizes various other types of I/O devices, including keyboards, optical-disk drives, and other such devices. The virtualization layer essentially schedules execution of virtual machines much like an operating system schedules execution of application programs, so that the virtual machines each execute within a complete and fully functional virtual hardware layer.</p><p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. <b>5</b>B</figref> illustrates a second type of virtualization. In <figref idref="DRAWINGS">FIG. <b>5</b>B</figref>, the computer system <b>540</b> includes the same hardware layer <b>542</b> and software layer <b>544</b> as the hardware layer <b>402</b> shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>. Several application programs <b>546</b> and <b>548</b> are shown running in the execution environment provided by the operating system. In addition, a virtualization layer <b>550</b> is also provided, in computer <b>540</b>. but, unlike the virtualization layer <b>504</b> discussed with reference to <figref idref="DRAWINGS">FIG. <b>5</b>A</figref>. virtualization layer <b>550</b> is layered above the operating system <b>544</b>, referred to as the &#x201c;host OS,&#x201d; and uses the operating system interface to access operating-system-provided functionality as well as the hardware. The virtualization layer <b>550</b> comprises primarily a VMM and a hardware-like interface <b>552</b>, similar to hardware-like interface <b>508</b> in <figref idref="DRAWINGS">FIG. <b>5</b>A</figref>. The virtualization-layer/hardware-layer interface <b>552</b>, equivalent to interface <b>416</b> in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, provides an execution environment for a number of virtual machines <b>556</b>-<b>558</b>. each including one or more application programs or other higher-level computational entities packaged together with a guest operating system.</p><p id="p-0048" num="0047">While the traditional virtual-machine-based virtualization layers, described with reference to <figref idref="DRAWINGS">FIGS. <b>5</b>A-B</figref>. have enjoyed widespread adoption and use in a variety of different environments, from personal computers to enormous distributed computing systems, traditional virtualization technologies are associated with computational overheads. While these computational overheads have been steadily decreased, over the years, and often represent ten percent or less of the total computational bandwidth consumed by an application running in a virtualized environment, traditional virtualization technologies nonetheless involve computational costs in return for the power and flexibility that they provide. Another approach to virtualization is referred to as operating-system-level virtualization (&#x201c;OSL virtualization&#x201d;). <figref idref="DRAWINGS">FIG. <b>5</b>C</figref> illustrates the OSL-virtualization approach. In <figref idref="DRAWINGS">FIG. <b>5</b>C</figref>, as in previously discussed <figref idref="DRAWINGS">FIG. <b>4</b></figref>, an operating system <b>404</b> runs above the hardware <b>402</b> of a host computer. The operating system provides an interface for higher-level computational entities, the interface including a system-call interface <b>428</b> and exposure to the non-privileged instructions and memory addresses and registers <b>426</b> of the hardware layer <b>402</b>. However, unlike in <figref idref="DRAWINGS">FIG. <b>5</b>A</figref>, rather than applications running directly above the operating system, OSL virtualization involves an OS-level virtualization layer <b>560</b> that provides an operating-system interface <b>562</b>-<b>564</b> to each of one or more containers <b>566</b>-<b>568</b>. The containers, in turn, provide an execution environment for one or more applications, such as application <b>570</b> running within the execution environment provided by container <b>566</b>. The container can be thought of as a partition of the resources generally available to higher-level computational entities through the operating system interface <b>430</b>. While a traditional virtualization layer can simulate the hardware interface expected by any of many different operating systems, OSL virtualization essentially provides a secure partition of the execution environment provided by a particular operating system. As one example, OSL virtualization provides a file system to each container, but the file system provided to the container is essentially a view of a partition of the general file system provided by the underlying operating system. In essence. OSL virtualization uses operating-system features, such as name space support, to isolate each container from the remaining containers so that the applications executing within the execution environment provided by a container are isolated from applications executing within the execution environments provided by all other containers. As a result, a container can be booted up much faster than a virtual machine, since the container uses operating-system-kernel features that are already available within the host computer. Furthermore, the containers share computational bandwidth, memory, network bandwidth, and other computational resources provided by the operating system, without resource overhead allocated to virtual machines and virtualization layers. Again, however, OSL virtualization does not provide many desirable features of traditional virtualization. As mentioned above, OSL virtualization does not provide a way to run different types of operating systems for different groups of containers within the same host system, nor does OSL-virtualization provide for live migration of containers between host computers, as does traditional virtualization technologies,</p><p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. <b>5</b>D</figref> illustrates an approach to combining the power and flexibility of traditional virtualization with the advantages of OSL virtualization. <figref idref="DRAWINGS">FIG. <b>5</b>D</figref> shows a host computer similar to that shown in <figref idref="DRAWINGS">FIG. <b>5</b>A</figref>, discussed above. The host computer includes a hardware layer <b>502</b> and a virtualization layer <b>504</b> that provides a simulated hardware interface <b>508</b> to an operating system <b>572</b>. Unlike in <figref idref="DRAWINGS">FIG. <b>5</b>A</figref>, the operating system interfaces to an OSL-virtualization layer <b>574</b> that provides container execution environments <b>576</b>-<b>578</b> to multiple application programs. Running containers above a guest operating system within a virtualized host computer provides many of the advantages of traditional virtualization and OSL virtualization. Containers can be quickly booted in order to provide additional execution environments and associated resources to new applications. The resources available to the guest operating system are efficiently partitioned among the containers provided by the OSL-virtualization layer <b>574</b>. Many of the powerful and flexible features of the traditional virtualization technology can be applied to containers running above guest operating systems including live migration from one host computer to another, various types of high-availability and distributed resource sharing, and other such features. Containers provide share-based allocation of computational resources to groups of applications with guaranteed isolation of applications in one container from applications in the remaining containers executing above a guest operating system. Moreover, resource allocation can be modified at run time between containers. The traditional virtualization layer provides flexible and easy scaling and a simple approach to operating-system upgrades and patches. Thus, the use of OSL virtualization above traditional virtualization, as illustrated in <figref idref="DRAWINGS">FIG. <b>5</b>D</figref>. provides much of the advantages of both a traditional virtualization layer and the advantages of OSL virtualization. Note that, although only a single guest operating system and OSL virtualization layer as shown in <figref idref="DRAWINGS">FIG. <b>5</b>D</figref>, a single virtualized host system can run multiple different guest operating systems within multiple virtual machines. each of which supports one or more containers.</p><p id="p-0050" num="0049">A virtual machine or virtual application, described below, is encapsulated within a data package for transmission, distribution, and loading into a virtual-execution environment. One public standard for virtual-machine encapsulation is referred to as the &#x201c;open virtualization format&#x201d; (&#x201c;OVF&#x201d;). The OVF standard specifies a format for digitally encoding a virtual machine within one or more data files. <figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates an OVF package. An OVF package <b>602</b> includes an OVF descriptor <b>604</b>, an OVF manifest <b>606</b>, an OVF certificate <b>608</b>. one or more disk-image files <b>610</b>-<b>611</b>, and one or more resource files <b>612</b>-<b>614</b>. The OVF package can be encoded and stored as a single file or as a set of files. The OVF descriptor <b>604</b> is an XML document <b>620</b> that includes a hierarchical set of elements, each demarcated by a beginning tag and an ending tag. The outermost, or highest-level, element is the envelope element, demarcated by tags <b>622</b> and <b>623</b>. The next-level element includes a reference element <b>626</b> that includes references to all files that are part of the OVF package, a disk section <b>628</b> that contains meta information about all of the virtual disks included in the OVF package, a networks section <b>630</b> that includes meta information about all of the logical networks included in the OVF package, and a collection of virtual-machine configurations <b>632</b> which further includes hardware descriptions of each virtual machine <b>634</b>. There are many additional hierarchical levels and elements within a typical OVF descriptor. The OVF descriptor is thus a self-describing XML file that describes the contents of an OVF package. The OVF manifest <b>606</b> is a list of cryptographic-hash-function-generated digests <b>636</b> of the entire OVF package and of the various components of the OVF package. The OVF certificate <b>608</b> is an authentication certificate <b>640</b> that includes a digest of the manifest and that is cryptographically signed. Disk image files, such as disk image file <b>610</b>, are digital encodings of the contents of virtual disks and resource files <b>612</b> are digitally encoded content, such as operating-system images. A virtual machine or a collection of virtual machines encapsulated together within a virtual application can thus be digitally encoded as one or more files within an OVF package that can be transmitted, distributed, and loaded using well-known tools for transmitting. distributing, and loading files. A virtual appliance is a software service that is delivered as a complete software stack installed within one or more virtual machines that is encoded within an OVF package.</p><p id="p-0051" num="0050">The advent of virtual machines and virtual environments has alleviated many of the difficulties and challenges associated with traditional general-purpose computing. Machine and operating-system dependencies can be significantly reduced or entirely eliminated by packaging applications and operating systems together as virtual machines and virtual appliances that execute within virtual environments provided by virtualization layers running on many different types of computer hardware. A next level of abstraction. referred to as virtual data centers which are one example of a broader virtual-infrastructure category, provide a data-center interface to virtual data centers computationally constructed within physical data centers. <figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates virtual data centers provided as an abstraction of underlying physical-data-center hardware components. In <figref idref="DRAWINGS">FIG. <b>7</b></figref>, a physical data center <b>702</b> is shown below a virtual-interface plane <b>704</b>. The physical data center consists of a virtual-infrastructure management server (&#x201c;VI-management-server&#x201d;) <b>706</b> and any of various different computers, such as PCs <b>708</b>, on which a virtual-data-center management interface may be displayed to system administrators and other users. The physical data center additionally includes generally large numbers of server computers, such as server computer <b>710</b>, that are coupled together by local area networks, such as local area network <b>712</b> that directly interconnects server computer <b>710</b> and <b>714</b>-<b>720</b> and a mass-storage array <b>722</b>. The physical data center shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref> includes three local area networks <b>712</b>, <b>724</b>, and <b>726</b> that each directly interconnects a bank of eight servers and a mass-storage array. The individual server computers, such as server computer <b>710</b>, each includes a virtualization layer and runs multiple virtual machines. Different physical data centers may include many different types of computers, networks, data-storage systems and devices connected according to many different types of connection topologies. The virtual-data-center abstraction layer <b>704</b>, a logical abstraction layer shown by a plane in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, abstracts the physical data center to a virtual data center comprising one or more resource pools, such as resource pools <b>730</b>-<b>732</b>, one or more virtual data stores, such as virtual data stores <b>734</b>-<b>736</b>. and one or more virtual networks. In certain implementations, the resource pools abstract banks of physical servers directly interconnected by a local area network.</p><p id="p-0052" num="0051">The virtual-data-center management interface allows provisioning and launching of virtual machines with respect to resource pools, virtual data stores, and virtual networks, so that virtual-data-center administrators need not be concerned with the identities of physical-data-center components used to execute particular virtual machines. Furthermore, the VI-management-server includes functionality to migrate running virtual machines from one physical server to another in order to optimally or near optimally manage resource allocation, provide fault tolerance, and high availability by migrating virtual machines to most effectively utilize underlying physical hardware resources, to replace virtual machines disabled by physical hardware problems and failures, and to ensure that multiple virtual machines supporting a high-availability virtual appliance are executing on multiple physical computer systems so that the services provided by the virtual appliance are continuously accessible, even when one of the multiple virtual appliances becomes compute bound. data-access bound, suspends execution, or fails. Thus, the virtual data center layer of abstraction provides a virtual-data-center abstraction of physical data centers to simplify provisioning. launching. and maintenance of virtual machines and virtual appliances as well as to provide high-level, distributed functionalities that involve pooling the resources of individual physical servers and migrating virtual machines among physical servers to achieve load balancing, fault tolerance. and high availability.</p><p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates virtual-machine components of a VI-management-server and physical servers of a physical data center above which a virtual-data-center interface is provided by the VI-management-server. The VI-management-server <b>802</b> and a virtual-data-center database <b>804</b> comprise the physical components of the management component of the virtual data center. The VI-management-server <b>802</b> includes a hardware layer <b>806</b> and virtualization layer <b>808</b> and runs a virtual-data-center management-server virtual machine <b>810</b> above the virtualization layer. Although shown as a single server in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the VI-management-server (&#x201c;VI management server&#x201d;) may include two or more physical server computers that support multiple VI-management-server virtual appliances. The virtual machine <b>810</b> includes a management-interface component <b>812</b>, distributed services <b>814</b>, core services <b>816</b>, and a host-management interface <b>818</b>, The management interface is accessed from any of various computers, such as the PC <b>708</b> shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>. The management interface allows the virtual-data-center administrator to configure a virtual data center, provision virtual machines, collect statistics and view log files for the virtual data center, and to carry out other, similar management tasks. The host-management interface <b>818</b> interfaces to virtual-data-center agents <b>824</b>, <b>825</b>, and <b>826</b> that execute as virtual machines within each of the physical servers of the physical data center that is abstracted to a virtual data center by the VI management server.</p><p id="p-0054" num="0053">The distributed services <b>814</b> include a distributed-resource scheduler that assigns virtual machines to execute within particular physical servers and that migrates virtual machines in order to most effectively make use of computational bandwidths, data-storage capacities, and network capacities of the physical data center. The distributed services further include a high-availability service that replicates and migrates virtual machines in order to ensure that virtual machines continue to execute despite problems and failures experienced by physical hardware components. The distributed services also include a live-virtual-machine migration service that temporarily halts execution of a virtual machine, encapsulates the virtual machine in an OVF package, transmits the OVF package to a different physical server, and restarts the virtual machine on the different physical server from a virtual-machine state recorded when execution of the virtual machine was halted. The distributed services also include a distributed backup service that provides centralized virtual-machine backup and restore.</p><p id="p-0055" num="0054">The core services provided by the VI management server include host configuration, virtual-machine configuration. virtual-machine provisioning, generation of virtual-data-center alarms and events, ongoing event logging and statistics collection, a task scheduler. and a resource-management module. Each physical server <b>820</b>-<b>822</b> also includes a host-agent virtual machine <b>828</b>-<b>830</b> through which the virtualization layer can be accessed via a virtual-infrastructure application programming interface (&#x201c;API&#x201d;). This interface allows a remote administrator or user to manage an individual server through the infrastructure API. The virtual-data-center agents <b>824</b>-<b>826</b> access virtualization-layer server information through the host agents. The virtual-datacenter agents are primarily responsible for offloading certain of the virtual-data-center management-server functions specific to a particular physical server to that physical server. The virtual-data-center agents relay and enforce resource allocations made by the VI management server, relay virtual-machine provisioning and configuration-change commands to host agents. monitor and collect performance statistics, alarms, and events communicated to the virtual-data-center agents by the local host agents through the interface API, and to carry out other. similar virtual-data-management tasks.</p><p id="p-0056" num="0055">The virtual-data-center abstraction provides a convenient and efficient level of abstraction for exposing the computational resources of a cloud-computing facility to cloud-computing-infrastructure users. A cloud-director management server exposes virtual resources of a cloud-computing facility to cloud-computing-infrastructure users. In addition. the cloud director introduces a multi-tenancy layer of abstraction, which partitions virtual data centers (&#x201c;VDCs&#x201d;) into tenant-associated VDCs that can each be allocated to a particular individual tenant or tenant organization, both referred to as a &#x201c;tenant.&#x201d; A given tenant can be provided one or more tenant-associated VDCs by a cloud director managing the multi-tenancy layer of abstraction within a cloud-computing facility. The cloud services interface (<b>308</b> in <figref idref="DRAWINGS">FIG. <b>3</b></figref>) exposes a virtual-data-center management interface that abstracts the physical data center.</p><p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. <b>9</b></figref> illustrates a cloud-director level of abstraction. In <figref idref="DRAWINGS">FIG. <b>9</b></figref>, three different physical data centers <b>902</b>-<b>904</b> are shown below planes representing the cloud-director layer of abstraction <b>906</b>-<b>908</b>. Above the planes representing the cloud-director level of abstraction, multi-tenant virtual data centers <b>910</b>-<b>912</b> are shown. The resources of these multi-tenant virtual data centers are securely partitioned in order to provide secure virtual data centers to multiple tenants, or cloud-services-accessing organizations. For example, a cloud-services-provider virtual data center <b>910</b> is partitioned into four different tenant-associated virtual-data centers within a multi-tenant virtual data center for four different tenants <b>916</b>-<b>919</b>. Each multi-tenant virtual data center is managed by a cloud director comprising one or more cloud-director servers <b>920</b>-<b>922</b> and associated cloud-director databases <b>924</b>-<b>926</b>. Each cloud-director server or servers runs a cloud-director virtual appliance <b>930</b> that includes a cloud-director management interface <b>932</b>, a set of cloud-director services <b>934</b>, and a virtual-data-center management-server interface <b>936</b>. The cloud-director services include an interface and tools for provisioning multi-tenant virtual data center virtual data centers on behalf of tenants. tools and interfaces for configuring and managing tenant organizations, tools and services for organization of virtual data centers and tenant-associated virtual data centers within the multi-tenant virtual data center, services associated with template and media catalogs. and provisioning of virtualization networks from a network pool. Templates are virtual machines that each contains an OS and/or one or more virtual machines containing applications. A template may include much of the detailed contents of virtual machines and virtual appliances that are encoded within OVF packages, so that the task of configuring a virtual machine or virtual appliance is significantly simplified, requiring only deployment of one OVF package. These templates are stored in catalogs within a tenant's virtual-data center. These catalogs are used for developing and staging new virtual appliances and published catalogs are used for sharing templates in virtual appliances across organizations. Catalogs may include OS images and other information relevant to construction, distribution, and provisioning of virtual appliances.</p><p id="p-0058" num="0057">Considering <figref idref="DRAWINGS">FIGS. <b>7</b> and <b>9</b></figref>, the VI management server and cloud-director layers of abstraction can be seen, as discussed above, to facilitate employment of the virtual-data-center concept within private and public clouds. However, this level of abstraction does not fully facilitate aggregation of single-tenant and multi-tenant virtual data centers into heterogeneous or homogeneous aggregations of cloud-computing facilities.</p><p id="p-0059" num="0058"><figref idref="DRAWINGS">FIG. <b>10</b></figref> illustrates virtual-cloud-connector nodes (&#x201c;VCC nodes&#x201d;) and a VCC server. components of a distributed system that provides multi-cloud aggregation and that includes a cloud-connector server and cloud-connector nodes that cooperate to provide services that are distributed across multiple clouds. VMware vCloud&#x2122; VCC servers and nodes are one example of VCC server and nodes. In <figref idref="DRAWINGS">FIG. <b>10</b></figref>, seven different cloud-computing facilities are illustrated <b>1002</b>-<b>1008</b>. Cloud-computing facility <b>1002</b> is a private multi-tenant cloud with a cloud director <b>1010</b> that interfaces to a VI management server <b>1012</b> to provide a multi-tenant private cloud comprising multiple tenant-associated virtual data centers. The remaining cloud-computing facilities <b>1003</b>-<b>1008</b> may be either public or private cloud-computing facilities and may be single-tenant virtual data centers. such as virtual data centers <b>1003</b> and <b>1006</b>, multi-tenant virtual data centers, such as multi-tenant virtual data centers <b>1004</b> and <b>1007</b>-<b>1008</b>. or any of various different kinds of third-party cloud-services facilities, such as third-party cloud-services facility <b>1005</b>. An additional component, the VCC server <b>1014</b>, acting as a controller is included in the private cloud-computing facility <b>1002</b> and interfaces to a VCC node <b>1016</b> that runs as a virtual appliance within the cloud director <b>1010</b>. A VCC server may also run as a virtual appliance within a VI management server that manages a single-tenant private cloud. The VCC server <b>1014</b> additionally interfaces, through the Internet, to VCC node virtual appliances executing within remote VI management servers, remote cloud directors, or within the third-party cloud services <b>1018</b>-<b>1023</b>. The VCC server provides a VCC server interface that can be displayed on a local or remote terminal, PC, or other computer system <b>1026</b> to allow a cloud-aggregation administrator or other user to access VCC-server-provided aggregate-cloud distributed services. In general. the cloud-computing facilities that together form a multiple-cloud-computing aggregation through distributed services provided by the WC server and VCC nodes are geographically and operationally distinct.</p><p id="p-0060" num="0059">The current document discusses migration of a virtual-network subsystem within a virtual distributed computer system from a first version and/or implementation to a second version and/or implementation as an example of migration of a virtual subsystem within a distributed computer system to which implementations of the currently disclosed methods and systems can be applied. However, the currently disclosed methods and systems can be generally applied to the migration of various different types of virtual subsystems, in addition to virtual-network subsystems.</p><p id="p-0061" num="0060"><figref idref="DRAWINGS">FIG. <b>11</b></figref> illustrates the Open Systems Interconnection model (&#x201c;OSI model&#x201d;) that characterizes many modern approaches to implementation of communications systems that interconnect computers. In <figref idref="DRAWINGS">FIG. <b>11</b></figref>, two processor-controlled network devices, or computer systems, are represented by dashed rectangles <b>1102</b> and <b>1104</b>. Within each processor-controlled network device, a set of communications layers are shown, with the communications layers both labeled and numbered. For example, the first communications level <b>1106</b> in network device <b>1102</b> represents the physical layer which is alternatively designated as layer <b>1</b>. The communications messages that are passed from one network device to another at each layer are represented by divided rectangles in the central portion of <figref idref="DRAWINGS">FIG. <b>11</b></figref>. such as divided rectangle <b>1108</b>. The largest rectangular division <b>1110</b> in each divided rectangle represents the data contents of the message. Smaller rectangles. such as rectangle <b>1111</b>. represent message headers that are prepended to a message by the communications subsystem in order to facilitate routing of the message and interpretation of the data contained in the message, often within the context of an interchange of multiple messages between the network devices. Smaller rectangle <b>1112</b> represents a footer appended to a message to facilitate data-link-layer frame exchange. As can be seen by the progression of messages down the stack of corresponding communications-system layers, each communications layer in the OSI model generally adds a header or a header and footer specific to the communications layer to the message that is exchanged between the network devices.</p><p id="p-0062" num="0061">It should be noted that while the OSI model is a useful conceptual description of the modern approach to electronic communications, particular communications-systems implementations may depart significantly from the seven-layer OSI model. However, in general. the majority of communications systems include at least subsets of the functionality described by the OSI model. even when that functionality is alternatively organized and layered.</p><p id="p-0063" num="0062">The physical layer. or layer <b>1</b>. represents the physical transmission medium and communications hardware. At this layer. signals <b>1114</b> are passed between the hardware communications systems of the two network devices <b>1102</b> and <b>1104</b>. The signals may be electrical signals, optical signals. or any other type of physically detectable and transmittable signal. The physical layer defines how the signals are interpreted to generate a sequence of bits <b>1116</b> from the signals. The second data-link layer <b>1118</b> is concerned with data transfer between two nodes, such as the two network devices <b>1102</b> and <b>1104</b>. At this layer, the unit of information exchange is referred to as a &#x201c;data frame&#x201d; <b>1120</b>. The data-link layer is concerned with access to the communications medium, synchronization of data-frame transmission, and checking for and controlling transmission errors. The third network layer <b>1120</b> of the OSI model is concerned with transmission of variable-length data sequences between nodes of a network. This layer is concerned with networking addressing, certain types of routing of messages within a network. and disassembly of a large amount of data into separate frames that are reassembled on the receiving side. The fourth transport layer <b>1122</b> of the OSI model is concerned with the transfer of variable-length data sequences from a source node to a destination node through one or more networks while maintaining various specified thresholds of service quality. This may include retransmission of packets that fail to reach their destination, acknowledgement messages and guaranteed delivery, error detection and correction, and many other types of reliability. The transport layer also provides for node-to-node connections to support multi-packet and multi-message conversations, which include notions of message sequencing. Thus. layer <b>4</b> can be considered to be a connections-oriented layer. The fifth session layer of the OSI model <b>1124</b> involves establishment. management, and termination of connections between application programs running within network devices. The sixth presentation layer <b>1126</b> is concerned with communications context between application-layer entities, translation and mapping of data between application-layer entities, data-representation independence, and other such higher-level communications services. The final seventh application layer <b>1128</b> represents direct interaction of the communications systems with application programs. This layer involves authentication, synchronization, determination of resource availability, and many other services that allow particular applications to communicate with one another on different network devices. The seventh layer can thus be considered to be an application-oriented layer.</p><p id="p-0064" num="0063">In the widely used TCP/IP communications protocol stack. the seven OSI layers are generally viewed as being compressed into a data-frame layer, which includes OSI layers 1 and 2, a transport layer, corresponding to OSI layer 4, and an application layer, corresponding to OSI layers 5-7. These layers are commonly referred to as &#x201c;layer 2,&#x201d; &#x201c;layer 4,&#x201d; and &#x201c;layer 7.&#x201d; to be consistent with the OSI terminology.</p><p id="p-0065" num="0064"><figref idref="DRAWINGS">FIGS. <b>12</b>A-B</figref> illustrate a layer-2-over-layer-3 encapsulation technology on which virtualized networking can be based. <figref idref="DRAWINGS">FIG. <b>12</b>A</figref> shows a traditional network communications between two applications running on two different computer systems. Representations of components of the first computer system are shown in a first column <b>1202</b> and representations of components of the second computer system shown in a second column <b>1204</b>. An application <b>1206</b> running on the first computer system calls an operating-system function, represented by arrow <b>1208</b>, to send a message <b>1210</b> stored in application-accessible memory to an application <b>1212</b> running on the second computer system. The operating system on the first computer system <b>1214</b> moves the message to an output-message queue <b>1216</b> from which it is transferred <b>1218</b> to a network-interface-card (&#x201c;NIC&#x201d;) <b>1220</b>, which decomposes the message into frames that are transmitted over a physical communications medium <b>1222</b> to a NIC <b>1224</b> in the second computer system. The received frames are then placed into an incoming-message queue <b>1226</b> managed by the operating system <b>1228</b> on the second computer system, which then transfers <b>1230</b> the message to an application-accessible memory <b>1232</b> for reception by the second application <b>1212</b> running on the second computer system. In general, communications are bidirectional. so that the second application can similarly transmit messages to the first application. In addition, the networking protocols generally return acknowledgment messages in response to reception of messages. As indicated in the central portion of <figref idref="DRAWINGS">FIG. <b>12</b>A</figref> <b>1234</b>. the NIC-to-NIC transmission of data frames over the physical communications medium corresponds to layer-2 (&#x201c;L2&#x201d;) network operations and functionality, layer-4 (&#x201c;L4&#x201d;) network operations and functionality are carried out by a combination of operating-system and NIC functionalities, and the system-call-based initiation of a message transmission by the application program and operating system represents layer-7 (&#x201c;L7&#x201d;) network operations and functionalities. The actual precise boundary locations between the layers may vary depending on particular implementations.</p><p id="p-0066" num="0065"><figref idref="DRAWINGS">FIG. <b>12</b>B</figref> shows use of a layer-2-over-layer-3 encapsulation technology in a virtualized network communications scheme. <figref idref="DRAWINGS">FIG. <b>12</b>B</figref> uses similar illustration conventions as used in <figref idref="DRAWINGS">FIG. <b>12</b>A</figref>. The first application <b>1206</b> again employs an operating-system call <b>1208</b> to send a message <b>1210</b> stored in local memory accessible to the first application. However, the system call, in this case. is received by a guest operating system <b>1240</b> running within a virtual machine. The guest operating system queues the message for transmission to a virtual NIC <b>1242</b> (&#x201c;vNIC&#x201d;), which transmits L2 data frames <b>1244</b> to a virtual communications medium. What this means. in the described implementation, is that the L2 data frames are received by a hypervisor <b>1246</b>. which packages the L2 data frames into L3 data packets and then either directly, or via an operating system, provides the L3 data packets to a physical NIC <b>1220</b> for transmission to a receiving physical NIC <b>1224</b> via a physical communications medium. In other words, the L2 data frames produced by the virtual NIC are encapsulated in higher-level-protocol packets or messages that are then transmitted through a normal communications protocol stack and associated devices and components. The receiving physical NIC reconstructs the L3 data packets and provides them to a hypervisor and/or operating system <b>1248</b> on the receiving computer system, which unpackages the L2 data frames <b>1250</b> and provides the L2 data frames to a vNIC <b>1252</b>. The vNIC, in turn. reconstructs a message or messages from the L2 data frames and provides a message to a guest operating system <b>1254</b>, which reconstructs the original application-layer message <b>1256</b> in application-accessible memory. Of course, the same process can be used by the application <b>1212</b> on the second computer system to send messages to the application <b>1206</b> and the first computer system.</p><p id="p-0067" num="0066">The layer-2-over-layer-3 encapsulation technology provides a basis for generating complex virtual networks and associated virtual-network elements, such as firewalls, routers, edge routers, and other virtual-network elements within a virtual data centers, discussed above, with reference to <figref idref="DRAWINGS">FIGS. <b>7</b>-<b>10</b></figref>, in the context of a preceding discussion of virtualization technologies that references <figref idref="DRAWINGS">FIGS. <b>4</b>-<b>6</b></figref>. Virtual machines and vNICs are implemented by a virtualization layer. and the layer-2-over-layer-3 encapsulation technology allows the L2 data frames generated by a vNIC implemented by the virtualization layer to be physically transmitted, over physical communications facilities, in higher-level protocol messages or. in some cases. over internal buses within a server, providing a relatively simple interface between virtualized networks and physical communications networks.</p><p id="p-0068" num="0067"><figref idref="DRAWINGS">FIG. <b>13</b></figref> illustrates virtualization of two communicating servers. A first physical server <b>1302</b> and a second physical server <b>1304</b> are interconnected by physical communications network <b>1306</b> in the lower portion of <figref idref="DRAWINGS">FIG. <b>13</b></figref>. Virtualization layers running on both physical servers together compose a distributed virtualization layer <b>1308</b>, which can then implement a first virtual machine (&#x201c;VM&#x201d;) <b>1310</b> and a second VM <b>1312</b> that are interconnected by a virtual communications network <b>1314</b>. The first VM and the second VM may both execute on the first physical server, may both execute on the second physical server, or one VM may execute on one of the two physical servers and the other VM may execute on another of the two physical servers. The VMs may move from one physical server to another while executing applications and guest operating systems. The characteristics of the VMs, including computational bandwidths. memory capacities, instruction sets. and other characteristics. may differ from the characteristics of the underlying servers. Similarly, the characteristics of the virtual communications network <b>1314</b> may differ from the characteristics of the physical communications network <b>1306</b>. As one example, the virtual communications network <b>1314</b> may provide for interconnection of 10, 20, or more virtual machines, and may include multiple local virtual networks bridged by virtual switches or virtual routers, while the physical communications network <b>1306</b> may be a local area network (&#x201c;LAN&#x201d;) or point-to-point data exchange medium that connects only the two physical servers to one another. In essence, the virtualization layer <b>1308</b> can construct any number of different virtual machines and virtual communications networks based on the underlying physical servers and physical communications network. Of course, the virtual machines' operational capabilities, such as computational bandwidths, are constrained by the aggregate operational capabilities of the two physical servers and the virtual networks' operational capabilities are constrained by the aggregate operational capabilities of the underlying physical communications network, but the virtualization layer can partition the operational capabilities in many different ways among many different virtual entities, including virtual machines and virtual networks.</p><p id="p-0069" num="0068"><figref idref="DRAWINGS">FIG. <b>14</b></figref> illustrates a virtual distributed computer system based on one or more distributed computer systems. The one or more physical distributed computer systems <b>1402</b> underlying the virtual/physical boundary <b>1403</b> are abstracted, by virtualization layers running within the physical servers, as a virtual distributed computer system <b>1404</b> shown above the virtual physical boundary. In the virtual distributed computer system <b>1404</b>, there are numerous virtual local area networks (&#x201c;LANs&#x201d;) <b>1410</b>-<b>1414</b> interconnected by virtual switches (&#x201c;vSs&#x201d;) <b>1416</b> and <b>1418</b> to one another and to a virtual router (&#x201c;vR&#x201d;) <b>1421</b> that interconnects the virtual router through a virtual edge-router firewall (&#x201c;vEF&#x201d;) <b>1422</b> to a virtual edge router (&#x201c;vER&#x201d;) <b>1424</b> that, in turn, interconnects the virtual distributed computer system with external data centers, external computers, and other external network-communications-enable devices and systems. A large number of virtual machines, such as virtual machine <b>1426</b>, are connected to the LANs through virtual firewalls (&#x201c;vFs&#x201d;). such as vF <b>1428</b>. The VMs, vFs, vSs, vR, vEF, and vER are implemented largely by execution of stored computer instructions by the hypervisors within the physical servers, and while underlying physical resources of the one or more physical distributed computer systems are employed to implement the virtual distributed computer system, the components, topology, and organization of the virtual distributed computer system is largely independent from the underlying one or more physical distributed computer systems.</p><p id="p-0070" num="0069">Virtualization provides many important and significant advantages, Virtualized distributed computer systems can be configured and launched in time frames ranging from seconds to minutes, while physical distributed computer systems often require weeks or months for construction and configuration. Virtual machines can emulate many different types of physical computer systems with many different types of physical computer-system architectures, so that a virtual distributed computer system can run many different operating systems, as guest operating systems, that would otherwise not be compatible with the physical servers of the underlying one or more physical distributed computer systems. Similarly. virtual networks can provide capabilities that are not available in the underlying physical networks. As one example, the virtualized distributed computer system can provide firewall security to each virtual machine using vFs. as shown in <figref idref="DRAWINGS">FIG. <b>14</b></figref>. This allows a much finer granularity of network-communications security, referred to as &#x201c;microsegmentation,&#x201d; than can be provided by the underlying physical networks. Additionally, virtual networks allow for partitioning of the physical resources of an underlying physical distributed computer system into multiple virtual distributed computer systems, each owned and managed by different organizations and individuals, that are each provided full security through completely separate internal virtual LANs connected to virtual edge routers. Virtualization thus provides capabilities and facilities that are unavailable in non-virtualized distributed computer systems and that provide enormous improvements in the computational services that can be obtained from a distributed computer system.</p><heading id="h-0007" level="1">Kubernetes</heading><p id="p-0071" num="0070">Kubernetes is an open-source containerized-application orchestration system that provides an abstraction layer above virtual and physical computational resources within a data center or cloud-computing facility. Containers are a type of virtualized application-execution environment discussed above with reference to <figref idref="DRAWINGS">FIGS. <b>5</b>C-D</figref>. Containerized applications are applications that packaged for execution within containers. Kubernetes automatically distributes and schedules containerized applications across physical and virtual computational resources of a data center or cloud-computing facility. As one example, modern service-oriented applications are generally implemented by distributed applications running on the multiple virtual machines or containers within multiple physical servers of a data center or cloud-computing facility. Rather than manually installing and managing all of these different virtual machines and/or containers, a user can develop Kubernetes workload-resource specifications and supply the workload-resource specifications along with references to containerized applications to a Kubernetes orchestration system, which instantiates and manages operation of the service-oriented application.</p><p id="p-0072" num="0071"><figref idref="DRAWINGS">FIG. <b>15</b></figref> illustrates a fundamental Kubernetes abstraction. A data center, cloud-computing facility, or other distributed computer system is represented, in <figref idref="DRAWINGS">FIG. <b>15</b></figref>, as a large number of physical computational resources, such as servers <b>1502</b>. Kubernetes abstracts a portion of the physical and virtual computational resources provided by the underlying data center, cloud-computing facility, or other distributed computer system as a set of Kubernetes nodes <b>1504</b>, where horizontal plane <b>1506</b> represents the fundamental Kubernetes abstraction of the underlying physical and virtual computational resources of the data center or cloud-computing facility. Kubernetes nodes may be virtual machines. physical computers, or other such computational entities that provide execution environments for containerized applications. The Kubernetes orchestration system is responsible for mapping Kubernetes nodes to the physical and virtual computational resources, including physical and virtual data-storage facilities and communications networks in addition to containerized-application execution environments.</p><p id="p-0073" num="0072"><figref idref="DRAWINGS">FIG. <b>16</b></figref> illustrates a next level of abstraction provided by Kubernetes, referred to as a &#x201c;Kubernetes cluster.&#x201d; A Kubernetes cluster comprises a set of highly available, interconnected Kubernetes nodes that are managed by Kubernetes as a computational entity. The nodes in a cluster are partitioned into worker nodes <b>1602</b>, often simply referred to as &#x201c;nodes,&#x201d; and master nodes <b>1604</b> that together implement a Kubernetes-cluster control plane. In general, only one of the masters nodes is active at any given time, with the inactive master nodes providing for immediate failover in the case that the active master node fails. The control plane is responsible for distributing containerized applications among the worker nodes and scheduling execution of the containerized applications. In addition, the control plane manages operation of the nodes and containerized applications executing within the nodes. The control plane provides a Kubernetes application programming interface (&#x201c;API&#x201d;) <b>1606</b> through which the control plane communicates with the nodes and through which Kubernetes services and facilities are accessed by users, often via the Kubectl command line interface <b>1608</b>. An additional Kubernetes layer of abstraction <b>1610</b> provides a set of pods <b>1612</b> that are deployed to. and that provide execution environments within, the nodes <b>1602</b>. A pod is the smallest computational unit in Kubernetes. A pod supports execution of a single container or two or more tightly coupled containers. including shared data-storage and networking resources. that are scheduled and deployed together by the cluster control plane. In many cases, a pod includes only a single container that provides an execution environment for a single instance of a containerized application. Pods are created and managed by controllers for workload resources, discussed below, and are each associated with a pod template. or pod specification.</p><p id="p-0074" num="0073"><figref idref="DRAWINGS">FIG. <b>17</b></figref> illustrates the logical contents of a pod. The pod <b>1702</b> includes one or more containers <b>1704</b>-<b>1705</b>. shared storage and networking resources <b>1706</b>, and various types of metadata <b>1708</b>, including operational parameters and resource requirements. A pod is assigned a set of unique network addresses that is shared, along with a set of ports. by all of the containers in the pod. Containers within a pod can communicate with one another via shared memory, semaphores, and localhost.</p><p id="p-0075" num="0074"><figref idref="DRAWINGS">FIG. <b>18</b></figref> illustrates the logical contents of a Kubernetes management node and a Kubernetes worker node. A Kubernetes management node <b>1802</b> includes an API server <b>1804</b> that exposes the Kubernetes API to remote entities and that implements the control-plane front-end. in addition, a Kubernetes management node includes a scheduler <b>1806</b> that is responsible for distributing newly created pods among worker nodes. matching pod requirements, constraints, affinities and parameters to the parameters and characteristics of the worker nodes to which a pod is distributed. A Kubernetes management node additionally includes a controller manager <b>1808</b> comprising multiple processes that implement multiple controllers, including a node controller, a replication controller, an endpoints controller, and a service-account-and-token controller. Controllers monitor the operational status of pods within the cluster and attempt to ameliorate any detected departures from the specified operational behaviors of the pods. For example, the node controller detects failed nodes and attempt to mitigate node failures. As another example, the replication controller monitors replication objects to ensure that the proper number of pods are running for each replication object. A Kubernetes management node further includes an etcd key-value data store <b>1810</b> and a cloud-controller manager <b>1812</b>, which includes multiple controllers that manage cloud-hosted Kubernetes cluster components. The above-discussed logical components of a master node are implemented above the computational resources <b>1814</b> provided by a virtual machine or physical server. A worker node <b>1820</b> includes a Kubelet agent <b>1822</b> that manages pods running within the worker node in cooperation with the control plate, with which the Kubelet agent communicates via the Kubernetes API. as indicated by dashed arrow <b>1824</b>. In addition, a worker node includes a container run time <b>1826</b>, such as the Docker container runtime, and one or more pods <b>1828</b>-<b>1830</b> that execute using the computational resources <b>1832</b> provided by a virtual machine or physical server.</p><p id="p-0076" num="0075"><figref idref="DRAWINGS">FIGS. <b>19</b>A-E</figref> illustrate operation of a Kubernetes cluster. While there are many ways for a user to access a Kubernetes cluster and Kubernetes-cluster services through the Kubernetes API, a common approach to instantiating containerized applications is to develop a specification, referred to as a &#x201c;configuration file.&#x201d; that specifies one or more of various types of workload resources <b>1902</b> and to submit the configuration file, along with references to containerized applications <b>1904</b>-<b>1906</b>, via the Kubectl command line interface <b>1908</b> to the Kubernetes API <b>1910</b> provided by a Kubernetes-cluster control plane <b>1912</b>. The Kubernetes-cluster control plane distributes and schedules execution of a set of pods containing containerized-application instances of the containerized applications according to the workload-resource specification. The Kubernetes-cluster control plane then monitors the operational behaviors of the distributed pods over an execution lifetime specified in the workload-resource specification. Thus. the Kubernetes cluster automatically instantiates and manages executable instances of supplied containerized applications according to a workload-resource specification.</p><p id="p-0077" num="0076">There are a number of different types of workload resources. A deployment-and-replicaSet workload resource <b>1914</b> is often used for instantiating and managing stateless applications. The Kubernetes control plane manages this type of workload resource, in part. by ensuring that a specified number of pods remain operational for each different type of containerized-application instance specified in the deployment. A statefulSet workload resource <b>1916</b> can be used to specify instantiation and management of a set of related pods associated with states. Additional types of workload resources include daemonSets <b>1918</b> and jobs <b>1920</b>. In addition, Kubernetes supports specifying a service abstraction layer that includes a logical set of pods that are exposed to external communications and provided with service-related functionalities, including load-balancing and service discovery.</p><p id="p-0078" num="0077">When, in the example shown in <figref idref="DRAWINGS">FIGS. <b>19</b>A-E</figref>, the configuration file is input to a Kubernetes system via the Kubectl command line interface <b>1908</b>. the active master node of the control plane invokes the scheduler to create and distribute pods containing the specified number of containerized-application instances among worker nodes of the cluster as well as to provide additional facilities for sets of pods defined to compose a service. In the example shown in <figref idref="DRAWINGS">FIG. <b>19</b>A</figref>, two pods containing instances of application a <b>1822</b>-<b>1923</b>, two pods containing instances of application h <b>1924</b>-<b>1925</b>, and three pods containing instances of application c <b>1926</b>-<b>1928</b>, which together compose a service, as indicated by dashed contour <b>1930</b>, are created according to the input configuration file. As shown in <figref idref="DRAWINGS">FIG. <b>19</b>B</figref>, the Kubernetes control plate then invokes the controller manager to launch controllers <b>1932</b>-<b>1935</b> to monitor operation of the distributed pods which, in turn, launch execution of the containerized applications within the pods according to specifications contained in the configuration file.</p><p id="p-0079" num="0078"><figref idref="DRAWINGS">FIGS. <b>19</b>C-E</figref> illustrate various types of management operations carried out by the Kubernetes control plate during the lifetime of the workload resources instantiated in <figref idref="DRAWINGS">FIGS. <b>19</b>A-B</figref>. As shown in <figref idref="DRAWINGS">FIG. <b>19</b>C</figref>, when a node <b>1940</b> that originally hosted an instance of application a fails, as indicated by the &#x201c;X&#x201d; symbol <b>1942</b>, a controller within the Kubernetes control plane detects the failure, after which the Kubernetes control plane creates a new pod to execute an instance of application a <b>1944</b> and distributes the new pod to a different. functioning node <b>1946</b>. As shown in <figref idref="DRAWINGS">FIG. <b>19</b>D</figref>, when a user supplies a reference to a new version of application b <b>1948</b> to the Kubernetes control plane via the Kubectl command line interface <b>1908</b>. the Kubernetes control plate arranges for two replacement pods <b>1850</b> and <b>1952</b> containing instances of the new version of application b to be distributed to nodes <b>1954</b> and <b>1956</b>, following which the original pods containing the older version of application b are terminated. As shown in <figref idref="DRAWINGS">FIG. <b>19</b>E</figref>. when the Kubernetes control plane determines that the current workload associated with the service comprising three pods containing instances of application c (<b>1930</b> in <figref idref="DRAWINGS">FIG. <b>19</b>A</figref>) has increased above a specified threshold workload, the Kubernetes control plane automatically scales up this service to include three new pods <b>1960</b>-<b>1962</b> to which portions of the excessively high workload can be distributed. Detecting and ameliorating node failures, carrying out updates and upgrades of executing containerized applications. and automatically scaling up and scaling down a deployed workload resource are examples of the many different types of management services and operations provided by a Kubernetes cluster via a set of controllers running within the active management node. Controllers monitor pod operations for occurrences of various types of events and invoke event handlers to handle the events. with each different type of controller monitoring and handling different types of events, The control plane thus dynamically controls the worker nodes in accordance with the configuration file or files that define the configuration and operational behaviors of each workload resource.</p><p id="p-0080" num="0079"><figref idref="DRAWINGS">FIG. <b>20</b></figref> illustrates the Tanzu Kubernetes Grid (&#x201c;TKG&#x201d;) containerized-application orchestration system. TKG is a higher-level orchestration system that automatically instantiates and manages Kubernetes clusters across multiple data centers and clouds. TKG <b>2002</b> provides, through a TKG API <b>2004</b>. similar services and functionality to those provided by Kubernetes. In fact, TKG is layered on top of Kubernetes <b>2006</b>. However. TKG is also layered above the multi-data-center and multi-cloud virtualization layer <b>2008</b>. such as the multi-cloud aggregation distributed system discussed above with reference to <figref idref="DRAWINGS">FIG. <b>10</b></figref>. This allows TKG to support Kubernetes-like clusters across multiple data centers and cloud-computing facilities <b>2010</b>-<b>2012</b>. This also allows TKG to migrate nodes among different data centers and cloud-computing facilities and provide additional functionalities that are possible because of TKG's access to services and functionalities provided by the multi-data-center and multi-cloud virtualization layer. In essence. TKG is a meta-level Kubernetes system. Like Kubernetes, TKG uses both a control plane comprising specialized control-plane nodes as well as a set of worker Kubernetes clusters across which TKG distributes workload resources.</p><heading id="h-0008" level="1">Mobile Network Infrastructure</heading><p id="p-0081" num="0080"><figref idref="DRAWINGS">FIG. <b>21</b></figref> illustrates an older-technology mobile network. A mobile network provides radio-frequency voice and data transmissions between user cell phones as well as interconnection of cell phones to the public switched telephone network (&#x201c;PSTN&#x201d;) and to packet-based networks on which the Internet is implemented. Mobile networks are complex systems with many different electronic components, including routers, bridges, firewall appliances, and other such components as well as computer-system-implemented components. Mobile networks have steadily evolved to incorporate new, more capable technologies, and different types and generations of mobile networks are currently in service around the world. A typical older-technology mobile network includes a large number of geographical areas, referred to as &#x201c;cells,&#x201d; such as hexagonally shaped area <b>2102</b>, each served by a cellular tower, such as cellular tower <b>2104</b>. Cells are often hexagonally shaped. but can have other regular shapes, such as squares and circles. The cellular tower is a radio-frequency transceiver that sends radio-frequency signals to, and receives radio-frequency signals from, user cell phones within a relatively small geographical area surrounding the cellular tower, often including the cell containing the cellular tower and adjacent cells. The cellular towers are each connected to a base transceiver station (&#x201c;BTS&#x201d;), such as BTS <b>2106</b>, which act as aggregators or collectors for signals received by the cellular towers connected to the BTS and as distributors of signals forwarded to the BTS by higher-level components of the mobile network for distribution to user cell phones via cellular towers connected to the BTS.</p><p id="p-0082" num="0081">Each BTS is connected to a base station controller (&#x201c;BSC&#x201d;). such as BSC <b>2108</b>. The BSC allocates radio channels, controls handovers between BTSs connected to the BSC when users move from accessing the mobile network through a cellular tower connected to a first BTS to a cellular tower connected to a second BTS, and controls forwarding of signals from the connected BTSs to higher-level components of the mobile network and distribution of signals from the higher-level components of the mobile network to user cell phones via the connected BTSs and cellular towers. The BSC is often implemented using a distributed computing system, including data-storage appliances, along with many types of electrical components. including power supplies, routers, switching circuitry, and many other types of components.</p><p id="p-0083" num="0082">Each BSC is connected to a mobile switching center (&#x201c;MSC&#x201d;), such as MSC <b>2110</b>. An MSC provides circuit-switched calling mobility management, interconnecting mobile calls to the PSTN, interconnecting user mobile devices with the Internet, implementing handovers at the BSC-to-BSC level and facilitating handovers at the MSC level, providing connection to additional services, such as conference calling, generation of billing information, distribution of calls from the PSTN and the mobile network to called user devices, routing and delivering short message service (&#x201c;SMS&#x201d;) messages, and accessing various types of stored information related to mobile-network users. mobile-network-user devices, and other types of information. This information may be centrally stored in databases in one or more data centers. such as data center <b>2112</b>. The dashed circle <b>2114</b> in <figref idref="DRAWINGS">FIG. <b>21</b></figref> indicates that. in older-technology mobile networks, signals are generally transmitted through circuit-switched communications networks up to the MSCs and between the mobile network and PSTN while signals are transferred through packet-based networks between MSCs and between MSCs and data centers within the dashed circle.</p><p id="p-0084" num="0083"><figref idref="DRAWINGS">FIG. <b>22</b></figref> illustrates newer-technology mobile network based largely on packet-based-network communications. Newer-technology mobile networks extend packet-based-network communications at least as far down as the BSCs <b>2202</b>-<b>2206</b> and. in certain cases, even lower. Much of the older-technology electrical components from the BTS/BSC level upwards are implemented as virtual components within data centers and cloud-computing facilities <b>2208</b>-<b>2211</b>. In essence, much of the complexity of newer-technology mobile networks is implemented in software rather than as discrete electrical and electromechanical appliances and components used in older-technology mobile networks. This provides many advantages to mobile-network-service providers. Data transfer, including digitally-encoded voice data as well as digital data exchanged through the Internet, can be carried out with significantly reduced latencies in packet-based network communications in comparison to circuit-switched network communications. Maintenance costs can be significantly reduced, since most of the complexity of the newer-technology mobile networks resides in mobile-network applications executing within distributed computing systems rather than in large numbers of geographically distributed hardware appliances and electrical components. Incorporation of technology improvements and updating newer-technology mobile networks is far more cost-effective and time efficient for computationally implemented components. In addition, newer-technology mobile networks provide for greater flexibility with respect to the location of virtualized components. It is even possible to dynamically aggregate functionality at higher levels and to disperse aggregated functionality to lower levels in order to optimize use of computational resources and to optimally decrease network latencies. Because of the decreased cell sizes and greatly increased communications bandwidths in fifth-generation (5G) mobile networks, transition to computational implemented components and subsystems is necessary to provide desired levels of performance and service quality.</p><p id="p-0085" num="0084"><figref idref="DRAWINGS">FIG. <b>23</b></figref> provides a block diagram for the various logical components of a 5G mobile network. These logical components are implemented as CNFs within data centers and/or as CNFs and cloud-computing facilities. A first set of vertical brackets <b>2302</b> indicates the levels of logical components that may be included in the newer-technology BTS BSC base station layer, a second set of vertical brackets <b>2304</b> indicates the levels of logical components that may be included in newer-technology regional data centers, and a third set of vertical brackets <b>2306</b> indicates the levels of logical components that may be included in a national data center. The range of levels reflects the flexibility with which computationally-implemented mobile-network components can be distributed among national data centers, regional data centers, and BSCs. In <figref idref="DRAWINGS">FIG. <b>23</b></figref>. logical components are represented as rectangles, some of which include smaller rectangles representing protocols used for data exchange between component layers and levels. Interfaces between components are indicated by double headed arrows, such as double headed arrow <b>2308</b>.</p><p id="p-0086" num="0085">The lowest level component shown in <figref idref="DRAWINGS">FIG. <b>23</b></figref> is the remote radio head (&#x201c;RRH&#x201d;) <b>2310</b>. This component connects a radio-frequency transceiver with the lower-level mobile-network protocol stack. A distributed unit <b>2312</b> is the next lowest level component. and implements a protocol stack including the medium access control (&#x201c;MAC&#x201d;) and radio link control (&#x201c;RLC&#x201d; protocols. The next level component is referred to as the central unit (&#x201c;CU&#x201d;). The central unit includes a control-plane protocol stack and a user-plane protocol stack that interface to the access mobility management functions (&#x201c;AMF&#x201d;) <b>2314</b> and the user plane functions (&#x201c;UPF&#x201d;) <b>2316</b> logical components. respectively. The CU and DU together provide the functionality of the base station and the higher-level components together compose the 5G core functionality implemented as VNFs and CNFs within regional and national data centers. The AMF <b>2314</b> is responsible for many different functionalities. including registration management, mobility management, SM message transport, authentication and authorization. SMS-message transport, and many other functionalities. The UPF <b>2316</b> is responsible for packet routing and forwarding, traffic uses reporting. packet buffering, and other such functionality. The AMF and UPF logical components interface to numerous additional logical components <b>2318</b>-<b>2323</b>.</p><p id="p-0087" num="0086"><figref idref="DRAWINGS">FIG. <b>24</b></figref> illustrates the nature of VNF and CNF implementations. A virtual function <b>2402</b>, such as an access mobility management function, may be implemented as multiple instances of a containerized mobile-network application running within multiple virtual machines <b>2404</b>-<b>2410</b> or physical servers. The virtual machines or physical servers may be distributed across one or more data centers or cloud-computing facilities <b>2412</b>-<b>2414</b>. A given instance of a mobile-network application running within a virtual machine <b>2420</b> may interface to many additional virtual functions implemented as mobile-network-application instances within virtual machines <b>2422</b>-<b>2423</b>, each of which may, in turn, interface to yet more virtual functions and virtual-network components implemented as mobile-network-application instances within virtual machines <b>2424</b>-<b>2426</b>. Thus, newer-technology mobile networks are implemented as complex meshes of many different types of containerized-mobile-network-application instances distributed across many different data centers and/or cloud-computing facilities as well as distributed computing systems located within base stations. Depending on the particular implementation, a given data center or cloud-computing facility may include a very different set of VNFs and CNFs than other of the data centers and cloud-computing facilities that together implement a mobile network. Furthermore, because many of the VNFs and CNFs are directly concerned with receiving and transmitting very large volumes of digital voice-message packets and data packets from user cell phones to mobile-network components and from mobile-network components to user cell phones, and because the transmission of data packets are associated with high-throughput and low-latency performance requirements and constraints, many of the mobile-application instances are required to execute on physical computing platforms, such as servers. with specific hardware, operating-system, and hypervisor configurations and facilities. These specific hardware. operating-system, and hypervisor configurations and the facilities are obtained both by customization through software installation and configuration as well as by instantiating mobile-application instances within physical servers or virtual machines running on physical servers that meet the specific hardware requirements of the mobile-application instances. The scale and complexity of a mobile-network implementation therefore represents difficult technical challenges with respect to instantiating a mobile network within multiple distributed-computing facilities as well as managing the mobile-network implementation. over time.</p><heading id="h-0009" level="1">Current Cloud-Based Mobile-Network-Infrastructure Instantiation and Management</heading><p id="p-0088" num="0087">As mentioned in the preceding subsection, mobile-application instances that implement VNFs and CNFs are often associated with specific requirements for the execution environments in which they are deployed. <figref idref="DRAWINGS">FIG. <b>25</b></figref> illustrates the nature of certain mobile-network-application-execution-environment requirements. The outer rectangle <b>2502</b> in <figref idref="DRAWINGS">FIG. <b>25</b></figref> represents a server or other physical computer system that includes a hardware layer <b>2504</b>, a firmware level <b>2506</b>, a virtualization layer <b>2508</b>. a guest-operating-system layer <b>2510</b>, and an application layer <b>2512</b>. The application layer and guest-operating-system layer together represent an application-execution environment provided by a virtual machine, as discussed in preceding subsections. Execution of a particular containerized-mobile-network application instance <b>2514</b> may require post-deployment installation of a particular plug-in <b>2516</b> to extend the functionality of the application instance. In addition, proper execution of the application may depend on the guest operating system including one or more specific operating-system features <b>2518</b> and/or a particular configuration of the guest operating system via parameter settings <b>2520</b> or other types of customizations. Similarly, proper execution of the application may depend on particular virtualization-layer features <b>2522</b> and/or configurations <b>2524</b> as well as firmware configurations <b>2526</b>, such as a specific basic input-output system (&#x201c;BIOS&#x201d;) configuration. Examples include named data networking forwarding (&#x201c;NFD&#x201d;) daemons, Huge Pages virtual-memory-management features, single-route I/O virtualization (&#x201c;SR-IOV&#x201d;) features, real-time-kernel OS features. and virtualization-layer features that allow reservation of CPU and memory resources for particular virtual machines. Finally, proper execution of the application instance may require particular hardware components and features <b>2528</b>, such as field programmable gate arrays (&#x201c;FPGAs&#x201d;), graphical processing units (&#x201c;GPUs&#x201d;), and precision-time-protocol (&#x201c;PTP&#x201d;) real-time clocks, and may also require virtualization-layer pass-throughs <b>2530</b> that allow exclusive access by the guest operating system to particular hardware components <b>2532</b>.</p><p id="p-0089" num="0088"><figref idref="DRAWINGS">FIGS. <b>26</b>A-H</figref> illustrate a current approach to instantiating and managing a mobile network implemented as CNFs and VNFs in multiple distributed computing systems, data centers, and/or cloud-computing facilities. <figref idref="DRAWINGS">FIG. <b>26</b>A</figref> shows the logical components of a telco-cloud-automation (&#x201c;TCA&#x201d;) mobile-network orchestration system. The TCA <b>2602</b> is responsible for using input mobile-network-infrastructure specifications to instantiate the VNFs, CNFs, and additional computational infrastructure that together compose a mobile network. In addition, the TCA monitors the mobile-network infrastructure. over the lifetime of the mobile networking, to ensure that the infrastructure continues to operate according to the input mobile-network-infrastructure specifications as well as to update the mobile network in order to employ latest versions of mobile-network applications and to adjust the mobile-network infrastructure in order to maintain specified levels of service and cost-effective operation. The TCA operates as a meta-level orchestration system with specific functionality to address the operational requirements and complexities of mobile-network infrastructure.</p><p id="p-0090" num="0089">The TCA employs the above-described TKG orchestration system <b>2604</b> to instantiate workloads across multiple data centers and cloud-computing facilities. As discussed above, the TKG, in turn, employs the Kubernetes orchestration system <b>2606</b> as well as a multi-data-center and multi-cloud virtualization layer <b>2608</b>. The TCA, along with the TKG orchestration system, virtualization layer. and the Kubernetes orchestration system. instantiates and manages a mobile network based on the VNFs and CNFs <b>2610</b> distributed across multiple distributed computer systems, data centers, and/or cloud-computing facilities <b>2612</b>-<b>2614</b>.</p><p id="p-0091" num="0090"><figref idref="DRAWINGS">FIGS. <b>26</b>B-G</figref> illustrates instantiation of a mobile network by the TCA. As shown in <b>26</b>B, the TCA receives a mobile-network specification <b>2616</b> from which it generates a set of one or more workload-resource specifications <b>2618</b>, or configuration files, that the TCA users to invoke the TKG <b>2604</b> to instantiate a set of Kubernetes clusters <b>2620</b> distributed across the multiple distributed-computing systems. data centers, and/or cloud-computing facilities. Arrows <b>2622</b>-<b>2629</b> indicates that the TKG relies on the services and functionalities of the virtualization layer <b>2608</b> and the Kubernetes orchestration system <b>2606</b> to instantiate the Kubernetes clusters.</p><p id="p-0092" num="0091">Next, as shown in <figref idref="DRAWINGS">FIG. <b>26</b>C</figref>, the TCA employs services and functionalities of the TKG and virtualization layer. as indicated by arrows <b>2630</b> and <b>2631</b>. to determine whether the Kubernetes clusters can provide worker nodes with the mobile-network-specific hardware. firmware, virtualization-layer, operating-system. and other configurations. components, and facilities, discussed above with reference to <figref idref="DRAWINGS">FIG. <b>25</b></figref>. Determining whether the Kubernetes clusters can provide worker nodes with the required mobile-network-specific configurations, components, and facilities involves direct access, by the TCA, to virtualization-layer functionalities. While it is possible to specify various high-level requirements through the TKG and Kubernetes APIs, such as persistent-storage capacities, networking bandwidth, and processing bandwidths, the TKG and Kubernetes APIs do not provide functionality for specifying detailed mobile-network-specific configurations, components, and facilities required for hosting mobile-network application instances. The TCA may need to carry out a lengthy interaction with the TKG to obtain a suitable set of Kubernetes clusters.</p><p id="p-0093" num="0092">Next, as indicated by arrow <b>2632</b> in <figref idref="DRAWINGS">FIG. <b>26</b>D</figref>, the TCA instructs the TKG to provision worker nodes <b>2634</b> among the Kubernetes clusters. The worker nodes are provisioned to meet specified requirements for CPU bandwidth, memory capacity. and other such requirements via constraints and node affinities that can be imposed through the TKG API, but, as shown in <figref idref="DRAWINGS">FIG. <b>26</b>E</figref>. the TCA again needs to directly interact with the virtualization layer, as indicated by arrow <b>2636</b>. with the TKG, as indicated by arrow <b>2638</b>, and directly with worker nodes, as indicated by arrows <b>2640</b>-<b>2642</b>, in order to ensure that the provisioned worker nodes have the mobile-network-specific configurations, facilities, and components required for mobile-network operation. When the TCA directly interact with the virtualization layer and provisioned worker nodes in order to ascertain whether or not they have the mobile-network-specific configurations. facilities, and components, the TCA may need to employ secure communications connections to the worker nodes. In view of the fact that there may be thousands, tens of thousands. or more worker nodes provisioned for a mobile network. direct access, by the TCA, to the virtualization-layer and provisioned worker nodes represents a significant computational and temporal overhead.</p><p id="p-0094" num="0093">Next, as shown in <figref idref="DRAWINGS">FIG. <b>26</b>F</figref>. the TCA interacts directly with the virtualization Byer, as represented by arrow <b>2644</b> and <b>2646</b>-<b>2648</b>, to customize worker nodes to have the configurations and facilities required for operation of the mobile networks. This may involve downloading and installing plug-ins and pass-throughs, and interfacing to the virtualization layer and guest operating system in order to configure the worker nodes. Again, these operations may involve establishment of secure connections between the TCA and worker nodes and may involve multiple operations and at least temporarily storing data returned from operations needed to carry out subsequent operations. Worker-node customization represents an even greater computational and temporal overhead than the initial verification of worker-node capabilities discussed above with reference to <figref idref="DRAWINGS">FIG. <b>26</b>E</figref>. Finally. as shown in <figref idref="DRAWINGS">FIG. <b>26</b>G</figref>. the mobile-network application instances that together implement the VNFs, CNFs. and other computational support for the mobile-network infrastructure are launched via the TKG.</p><p id="p-0095" num="0094">Once the mobile-network infrastructure is up and running, the TCA cooperates with the TKG and a virtualization layer to monitor and manage the mobile-network computational infrastructure. As one example, when, due to increased bandwidth requirements, certain of the mobile-network applications are scaled up to include additional worker nodes, the TCA needs to cooperate with the TKG and virtualization layer to make sure that the new worker nodes are properly customized and provide the required hardware components, configurations, and facilities to implement mobile-network components. This again represents a very large and ongoing temporal and computational overhead, requiring establishment of secure communications connections and often requiring multiple, successive operations and interactions between the TCA, virtualization layer, and worker nodes. Thus, while the current TCA implementations provide highly useful and desirable orchestration functions, the very tight coupling between the TCA, TKG, and virtualization layer introduces significant complexities and the implementation of the TCA and involves very large, ongoing computational and temporal overheads.</p><heading id="h-0010" level="1">Currently Disclosed Methods and Systems</heading><p id="p-0096" num="0095">The currently disclosed methods and systems are directed to an improved TCA that instantiates and manages mobile-network infrastructure without tight coupling and interdependencies with the underlying TKG and virtualization layers. <figref idref="DRAWINGS">FIGS. <b>27</b>A-D</figref> illustrate operation of the improved TCA using illustration conventions employed in <figref idref="DRAWINGS">FIGS. <b>26</b>A-H</figref>, discussed above in the previous subsection of this document. As shown in <figref idref="DRAWINGS">FIG. <b>27</b>A</figref>, a mobile-network specification <b>2702</b> is input to the improved TCA <b>2704</b>, as to the original TCA in <figref idref="DRAWINGS">FIG. <b>26</b>A</figref>. However, the improved TCA prepares both a node policy <b>2706</b> as well as one or more workload-resource specifications <b>2708</b> based on the input mobile-network specification <b>2702</b>. The node policy <b>2706</b> specifies the various mobile-network-specific configurations, facilities. and components required for different types of worker nodes to be provisioned in order to implement the mobile-network computational infrastructure. The workload-resource specifications <b>2708</b> provide the information, along with the node policy, that is used by the TKG to instantiate and manage the mobile-network computational infrastructure. In addition, as represented by arrow <b>2710</b>, the TCA uses TKG operator-provisioning facilities to extend TKG functionality by introducing two new operators into the TKG. These include a VmConfig operator that is provisioned into the TKG control plane and a NodeConfig operator that is provisioned into the TKG workload clusters. The VmConfig operator includes logic for processing the node policy <b>2706</b> in order to generate custom-resource definitions and custom resources that extend the workload resources specified by the workload-resource specifications <b>2708</b>. VmConfig operator contains the logic that implements the custom-resource extensions. including logic for interacting with the virtualization layer to provision worker nodes within worker nodes having hardware components specified in the node policy, logic for interacting with the virtualization layer to customize provisioned worker nodes to have the mobile-network-specific configurations and facilities specified in the node policy. logic for interacting with a virtualization layer in order to carry out various types of management operations provided by the TKG with respect to the custom resources, including scale out, worker-node deployment, version updates, and other such management operations. In addition, the VmConfig operator extends the TKG to persist configuration data related to the custom resources and provide the data, as needed, to the NodeConfig operators provisioned within TKG workload clusters. The NodeConfig operators perform node-customization operations related to virtual machines during instantiation and during management operations carried out by the TKG. The extended TKG then receives the node policy <b>2706</b> and workload resource specifications and provisions TKG manager nodes <b>2712</b> across the distributed-computer systems, data centers, and/or cloud-computing facilities, each TKG manager node extended via inclusion of the VmConfig operator.</p><p id="p-0097" num="0096">As shown in <figref idref="DRAWINGS">FIG. <b>2713</b></figref>. the TKG manager nodes generate custom resources <b>2714</b>-<b>2717</b>, provision TKG workload clusters <b>2718</b>-<b>2721</b> across the distributed-computing systems. data centers, and/or cloud-computing facilities, each TKG workload cluster provisioned with the NodeConfig operator. The TKG managers then direct the TKG clusters, using the custom resources, to provision the worker nodes <b>2730</b> needed for implementation of the mobile-network computational infrastructure. The VmConfig and NodeConfig operators extend the TKG to access virtualization-layer functionality in order to ensure that the provisioned worker nodes have the mobile-network-specific components, configurations, and facilities, specified in the node policy, to support the mobile-network application instances that they are provisioned to execute. Thus, unlike in the currently available TCA implementations, the improved TCA is not involved in deploying and scheduling TKG workload clusters and worker nodes. Instead, the TKG has been extended, by incorporation of the VmConfig and NodeConfig operators, to provision and customize the mobile-network computational infrastructure, without participation of the improved TCA. The improved TCA therefore acts as a meta-level orchestrator that first extends the underlying TKG for orchestration of mobile-network-specific deployments and then generates the configurations and node policy needed by the extended TKG to instantiate and manage a mobile-network computational infrastructure. The extended TKG carries out the instantiation and management tasks independently from the TCA. As shown in <figref idref="DRAWINGS">FIG. <b>27</b>D</figref>, once the mobile-network computational infrastructure is instantiated and operating, ongoing management operations are carried out entirely by the extended TKG. In essence, the currently disclosed methods and systems provide an improved TCA that carries out TKG extension. via TKG operators, in an initial set of operations that allow the TCA to avoid the large computational and temporal overheads incurred by current TCA implementations, which interoperate with the TKG virtualization layer during mobile-network-computational-infrastructure instantiation and management.</p><p id="p-0098" num="0097"><figref idref="DRAWINGS">FIGS. <b>28</b>A-B</figref> show an example VMConfig custom resource definition. As discussed above, the VMConfig operator is associated with one or more controllers that facilitate instantiation mobile-network-specific pods for execution of mobile-network-specific applications, by mobile-network-specific worker nodes, that implement many different VNFs, CNFs. and/or virtual network components. In addition, the one or more controllers associated with the VMConfig operator facilitate monitoring execution of the mobile-network-specific pods during their execution life times, detecting and responding to various different events. The controllers are responsible for ensuring that mobile-network-specific worker nodes are mapped, by the TKG virtualization layer, to physical computer systems having the necessary hardware components and configuring and provisioning the mobile-network-specific worker nodes. The VMConfig operator processes one or more node policies input by the improved TCA to generate one or more custom resource definitions specify one or more custom resources corresponding to mobile-network-specific applications that implement the VNFs, CNFs, and/or virtual network components of a mobile-network infrastructure.</p><p id="p-0099" num="0098">Line <b>2802</b> in <figref idref="DRAWINGS">FIG. <b>28</b>A</figref> specifies a particular VM hardware version. A controller associated with the VMConfig operator determines whether or not a TKG worker node provisioned by TKG for execution of a mobile-network-specific application is configured to the specified VM hardware version. If not, the controller determines whether the TKG worker node is mapped to a server that supports the specified VM hardware version. If so, then the TKG worker node is upgraded to the specified VM hardware version. If not, then steps are taken to identify another available worker node that supports the specified VM hardware version. Lines <b>2804</b> in <figref idref="DRAWINGS">FIG. <b>28</b>A</figref> specify the number of required PCI bridges and the number of functions per PCI bridge. A controller associated with the VMConfig operator ensures that a TKG worker node provisioned by TKG for execution of a mobile-network-specific application is properly configured with the specified number of PCI bridges and properly configured. Lines <b>2806</b> specify parameter settings for a VMkernel process that manages <b>110</b> to and from certain classes of devices. Line <b>2808</b> specifies memory pinning and, along with lines <b>2806</b>, which ensures that a mobile-network-specific VM is non-uniform-memory-access (&#x201c;NUMA aligned&#x201d;) without CPU pinning. Lines <b>2810</b> specified various required network adapters and lines <b>2012</b> specify various pass-through devices. There is other parameters, constraints, and requirements can be found in the example VMConfig custom resource definition.</p><p id="p-0100" num="0099"><figref idref="DRAWINGS">FIGS. <b>29</b>A-D</figref> show an example of a NodeConfig custom resource definition. <figref idref="DRAWINGS">FIG. <b>29</b>A</figref> shows a custom resource definition, <figref idref="DRAWINGS">FIGS. <b>29</b>B-C</figref> show a profile definition for nodes. and <figref idref="DRAWINGS">FIG. <b>29</b>D</figref> shows a NodeConfig status custom resource definition. As discussed above, each TKG workload cluster is provisioned with a NodeConfig operator. The NodeConfig operator facilitates proper configuration and monitoring of mobile-network-specific pods and mobile-network-specific nodes on which they run.</p><p id="p-0101" num="0100">The present invention has been described in terms of particular embodiments, it is not intended that the invention be limited to these embodiments. Modifications within the spirit of the invention will be apparent to those skilled in the art. For example. any of many different implementations of the mobile-network-infrastructure orchestration system can be obtained by varying various design and implementation parameters, including modular organization, control structures, data structures. hardware. operating system, and virtualization layers, and other such design and implementation parameters. Alternative implementations of the mobile-network-infrastructure orchestration system receive and process mobile-network-computational-infrastructure specifications with different formats, vocabularies, and syntaxes. Node policies may also have different formats, vocabularies, and syntaxes, depending on the implementation. The currently disclosed mobile-network-infrastructure orchestration system can be implemented to incorporate a variety of different types of orchestration subsystems and virtualization systems that aggregate multiple distributed computer systems and facilities.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A mobile-network-infrastructure orchestration system comprising:<claim-text>one or more processors;</claim-text><claim-text>one or more memories;</claim-text><claim-text>one or more data-storage devices; and</claim-text><claim-text>processor instructions, contained in executable files stored in one or more of the one or more data-storage devices, that when executed by one or more of the one or more processors, control the mobile-network-infrastructure orchestration system to<claim-text>receive a mobile-network-computational-infrastructure specification;</claim-text><claim-text>extend a containerized-application orchestration system to include functionality needed to instantiate and manage mobile-network-specific worker nodes within workload clusters distributed across multiple distributed-computer systems aggregated by a virtualization layer;</claim-text><claim-text>generate one or more workload-resource specifications and a node policy from the mobile-network-computational-infrastructure specification; and</claim-text><claim-text>input the one or more workload-resource specifications and node policy to launch instantiation and subsequent management of a mobile-network computational infrastructure by the extended containerized-application orchestration system.</claim-text></claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The mobile-network-infrastructure orchestration system of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the mobile-network computational infrastructure comprises virtual network functions and/or cloud-native network functions implemented on worker nodes provisioned on worker nodes within workload clusters distributed across multiple distributed-computer systems.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The mobile-network-infrastructure orchestration system of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the multiple distributed-computer systems include distributed computer systems that implement mobile-network base stations, regional data venters, and national datacenters.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The mobile-network-infrastructure orchestration system of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the multiple distributed-computer systems include data centers and cloud-computing facilities.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The mobile-network-infrastructure orchestration system of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the mobile-network-infrastructure orchestration system extends the containerized-application orchestration system to include functionality needed to instantiate and manage mobile-network-specific worker nodes within workload clusters distributed across multiple distributed-computer systems by provisioning the containerized-application orchestration system with one or more control-plane operators and one or more workload-cluster operators.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The mobile-network-infrastructure orchestration system of <claim-ref idref="CLM-00005">claim 5</claim-ref> wherein the functionality needed to instantiate and manage mobile-network-specific worker nodes within workload clusters distributed across multiple distributed-computer systems includes:<claim-text>worker-node-information-extraction routines that access worker nodes through the virtualization layer to determine characteristics and parameters of the worker nodes, including the hardware components, hardware configuration, firmware configuration, operating-system configuration, and installed applications; and</claim-text><claim-text>worker-node customization routines that configure worker nodes.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The mobile-network-infrastructure orchestration system of <claim-ref idref="CLM-00006">claim 6</claim-ref> wherein the worker-node customization routines:<claim-text>install plugins specified in the node policy;</claim-text><claim-text>install passthroughs specified in the node policy;</claim-text><claim-text>download downloadable components specified in the node policy; and</claim-text><claim-text>alter operating-system, firmware. and local-virtualization-layer settings. parameters, and configurations as specified in the node policy.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The mobile-network-infrastructure orchestration system of <claim-ref idref="CLM-00006">claim 6</claim-ref> wherein the one or more control-plane operators:<claim-text>process the node policy to determine mobile-network-specific components. functionalities, and configurations needed by worker nodes; and</claim-text><claim-text>create custom resources corresponding to worker nodes that embody the mobile-network-specific components. functionalities. and configurations needed by worker nodes;</claim-text><claim-text>call worker-node-information-extraction routines to match workload cluster nodes to custom resources during management operations carried out by the extended containerized-application orchestration system, including scheduling and deployment of mobile-application instances and node-replacement. scaling, and update operations;</claim-text><claim-text>call worker-node customization routines to customize hardware, firmware, local-virtualization-layer, and operating-systems within worker nodes during management operations carried out by the extended containerized-application orchestration system, including scheduling and deployment of mobile-application instances and node-replacement and scaling operations; and</claim-text><claim-text>persist worker-node configuration information and pass worker-node configuration information to workload-cluster operators.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The mobile-network-infrastructure orchestration system of <claim-ref idref="CLM-00006">claim 6</claim-ref> wherein the one or more workload-cluster operators call worker-node customization routines to customize virtual machines within worker-nodes during management operations carried out by the extended containerized-application orchestration system.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. A method that instantiates and manages a mobile-network computational infrastructure. the method comprising:<claim-text>receiving a mobile-network-computational-infrastructure specification:</claim-text><claim-text>extending a containerized-application orchestration system to include functionality needed to instantiate and manage mobile-network-specific worker nodes within workload clusters distributed across multiple distributed-computer systems aggregated by a virtualization layer;</claim-text><claim-text>generating one or more workload-resource specifications and a node policy from the mobile-network-computational-infrastructure specification; and</claim-text><claim-text>inputting the one or more workload-resource specifications and node policy to launch instantiation and subsequent management of a mobile-network computational infrastructure by the extended containerized-application orchestration system.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref> wherein the mobile-network computational infrastructure comprises virtual network functions and/or cloud-native network functions implemented on worker nodes provisioned on worker nodes within workload clusters distributed across multiple distributed-computer systems.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref> wherein the multiple distributed-computer systems include distributed computer systems that implement mobile-network base stations, regional data venters, and national datacenters.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref> wherein the multiple distributed-computer systems include data centers and cloud-computing facilities.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method system of <claim-ref idref="CLM-00010">claim 10</claim-ref> wherein extending the containerized-application orchestration system to include functionality needed to instantiate and manage mobile-network-specific worker nodes within workload clusters distributed across multiple distributed-computer systems further comprises provisioning the containerized-application orchestration system with one or more control-plane operators and one or more workload cluster operators.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref> wherein the functionality needed to instantiate and manage mobile-network-specific worker nodes within workload clusters distributed across multiple distributed-computer systems includes:<claim-text>worker-node-information-extraction routines that access worker nodes through the virtualization layer to determine characteristics and parameters of the worker nodes, including the hardware components, hardware configuration, firmware configuration, operating-system configuration, and installed applications; and</claim-text><claim-text>worker-node customization routines that configure worker nodes.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref> wherein the worker-node customization routines:<claim-text>install plugins specified in the node policy:</claim-text><claim-text>install passthroughs specified in the node policy:</claim-text><claim-text>download downloadable components specified in the node policy: and</claim-text><claim-text>alter operating-system, firmware, and local-virtualization-layer settings, parameters, and configurations as specified in the node policy.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref> wherein the one or more control-plane operators:<claim-text>process the node policy to determine mobile-network-specific components. functionalities. and configurations needed by worker nodes; and</claim-text><claim-text>create custom resources corresponding to worker nodes that embody the mobile-network-specific components, functionalities, and configurations needed by worker nodes:</claim-text><claim-text>call worker-node-information-extraction routines to match workload cluster nodes to custom resources during management operations carried out by the extended containerized-application orchestration system, including scheduling and deployment of mobile-application instances and node-replacement, scaling. and update operations;</claim-text><claim-text>call worker-node customization routines to customize hardware, firmware, local-virtualization-layer, and operating-systems within worker nodes during management operations carried out by the extended containerized-application orchestration system, including scheduling and deployment of mobile-application instances and node-replacement and scaling operations; and</claim-text><claim-text>persist worker-node configuration information and pass worker-node configuration information to workload-cluster operators.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The method of <claim-ref idref="CLM-00017">claim 17</claim-ref> wherein the one or more workload-cluster operators call worker-node customization routines to customize virtual machines within worker-nodes during management operations carried out by the extended containerized-application orchestration system.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. A physical data-storage device that stores computer instructions that, when executed by processors within a computer system. control the computer system to instantiate and manage a mobile-network computational infrastructure by:<claim-text>receiving a mobile-network-computational-infrastructure specification;</claim-text><claim-text>extending a containerized-application orchestration system to include functionality needed to instantiate and manage mobile-network-specific worker nodes within workload clusters distributed across multiple distributed-computer systems aggregated by a virtualization layer;</claim-text><claim-text>generating one or more workload-resource specifications and a node policy from the mobile-network-computational-infrastructure specification; and</claim-text><claim-text>inputting the one or more workload-resource specifications and node policy to launch instantiation and subsequent management of a mobile-network computational infrastructure by the extended containerized-application orchestration system.</claim-text></claim-text></claim></claims></us-patent-application>