<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230007272A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230007272</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17894309</doc-number><date>20220824</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>159</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>567</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>147</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>109</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>176</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>513</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>70</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20141101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>159</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20141101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>567</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20141101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>147</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20141101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>109</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20141101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>176</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20141101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>513</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20141101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>70</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">TECHNIQUES OF MULTI-HYPOTHESIS MOTION COMPENSATION</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16879007</doc-number><date>20200520</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11463707</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17894309</doc-number></document-id></child-doc></relation></continuation><division><relation><parent-doc><document-id><country>US</country><doc-number>16257904</doc-number><date>20190125</date></document-id><parent-status>ABANDONED</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>16879007</doc-number></document-id></child-doc></relation></division><us-provisional-application><document-id><country>US</country><doc-number>62626276</doc-number><date>20180205</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Apple Inc.</orgname><address><city>Cupertino</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>TOURAPIS</last-name><first-name>Alexandros Michael</first-name><address><city>Cupertino</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>SU</last-name><first-name>Yeping</first-name><address><city>Sunnyvale</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>SINGER</last-name><first-name>David</first-name><address><city>San Francisco</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>WU</last-name><first-name>Hsi-Jung</first-name><address><city>San Jose</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">The present disclosure describes techniques for coding and decoding video in which a plurality of coding hypotheses are developed for an input pixel block of frame content. Each coding hypothesis may include generation of prediction data for the input pixel block according to a respective prediction search. The input pixel block may be coded with reference to a prediction block formed from prediction data derived according to plurality of hypotheses. Data of the coded pixel block may be transmitted to a decoder along with data identifying a number of the hypotheses used during the coding to a channel. At a decoder, an inverse process may be performed, which may include generation of a counterpart prediction block from prediction data derived according to the hypothesis identified with the coded pixel block data, then decoding of the coded pixel block according to the prediction data.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="108.12mm" wi="158.75mm" file="US20230007272A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="146.13mm" wi="167.89mm" file="US20230007272A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="199.47mm" wi="171.70mm" file="US20230007272A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="88.90mm" wi="156.89mm" file="US20230007272A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="230.21mm" wi="161.71mm" orientation="landscape" file="US20230007272A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="228.77mm" wi="156.55mm" orientation="landscape" file="US20230007272A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CLAIM FOR PRIORITY</heading><p id="p-0002" num="0001">The present application benefits from priority conferred by application Ser. No. 62/625,547, entitled &#x201c;Techniques of Multi-Hypothesis Motion Compensation&#x201d; and filed on Feb. 2, 2018, as well as application Ser. No. 62/626,276, also entitled &#x201c;Techniques of Multi-Hypothesis Motion Compensation&#x201d; and filed on Feb. 5, 2018, the disclosures of which are incorporated herein in their entirety.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">The present disclosure relates to video coding and, in particular, to video coding techniques that employ multi-hypothesis motion compensation coding (also, &#x201c;MHMC&#x201d; coding).</p><p id="p-0004" num="0003">Modern video compression systems, such as MPEG-4 AVC, HEVC, VP9, VP8, and AV1, often employ block-based strategies for video compression. In particular, a rectangular video region, (e.g. an image, a tile, and rows within such regions), are partitioned into rectangular or square blocks (called &#x201c;pixel blocks&#x201d; for convenience), and, for each pixel block, a different prediction mode is specified.</p><p id="p-0005" num="0004">MHMC is an inter prediction method where prediction of pixel blocks within a region such as a frame, tile, or slice, may be coded several different ways (called &#x201c;hypotheses&#x201d;) and the hypotheses are combined together. Thus, a prediction block &#x15d; for an input pixel block s can be generated using a set of N hypotheses h<sub>i </sub>as follows:</p><p id="p-0006" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mover accent="true">      <mi>s</mi>      <mi>&#x2c6;</mi>     </mover>     <mo>=</mo>     <mrow>      <mfrac>       <mn>1</mn>       <mi>N</mi>      </mfrac>      <mo>&#x2062;</mo>      <mrow>       <msubsup>        <mo>&#x2211;</mo>        <mrow>         <mi>i</mi>         <mo>=</mo>         <mn>0</mn>        </mrow>        <mrow>         <mi>N</mi>         <mo>-</mo>         <mn>1</mn>        </mrow>       </msubsup>       <mrow>        <msub>         <mi>w</mi>         <mi>i</mi>        </msub>        <mo>&#xd7;</mo>        <msub>         <mi>h</mi>         <mi>i</mi>        </msub>       </mrow>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mrow>      <mi>Eq</mi>      <mo>.</mo>      <mtext>   </mtext>      <mn>1</mn>     </mrow>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0007" num="0000">Each hypothesis commonly is associated with a reference index, motion vector parameters, and, in some instances, illumination compensation (weighted prediction) parameters. Motion vectors indicate displacement information, i.e. the displacement of the current block in relationship to the reference frame identified for their respective hypotheses. Commonly, such displacement is limited to only translational information. Current MHMC coding systems merge hypotheses via linear models as shown in Eq. 1.</p><p id="p-0008" num="0005">Modern codecs, such as AVC and HEVC, do not force the number of hypotheses for a block in a multihypothesis-predicted frame. Instead, a block can use a single or more hypotheses, or even be predicted using intra prediction. Currently, the number of maximum hypotheses that could be used is limited to two. This is mostly due to the cost of signalling any associated parameters relating to the prediction process but also due to complexity and bandwidth of reading data and reconstructing the final prediction. At lower bitrates, for example, mode and motion information can dominate the overall bitrate of an encoded stream. On the other hand, increasing the number of hypotheses even slightly, i.e. from two to three could result, in considerable, e.g. a 33%, increase in bandwidth in a system, and therefore impact power consumption. Data prefetching and memory management can become more complex and costly. Current coding protocols do not always provide efficient mechanisms to communicate video coding parameters between encoders and decoders.</p><p id="p-0009" num="0006">For example, in many video coding systems, the indication of prediction lists to be used in B slices is performed using a single parameter, which is sent at the prediction unit level of the coding syntax. Such a parameter may indicate whether a block will be using the List0 prediction, List1 prediction, or Bi-prediction. This type of signalling may also include indication of other coding modes as well, such as SKIP and DIRECT modes, Intra, etc. This method, therefore, incurs cost in terms of the signalling overhead to communicate these parameters and the cost can be expected to increase if MHMC coding techniques were employed that expand the number of hypothesis to levels greater than two. Similarly the overhead cost of signalling other coding parameters, such as prediction mode and motion vectors, would increase if MHMC coding technique expand the number of hypotheses to levels greater than two.</p><p id="p-0010" num="0007">Conventionally, modern MHMC coding systems are designed to use exactly the same partitioning and are not permitted to change within the said partitioning. That is, if a block of size 16&#xd7;16 is said to be utilizing bi-prediction, then a single set of parameters (e.g., reference indices, motion vectors, and weighted parameters), for each hypothesis is signalled and used.</p><p id="p-0011" num="0008">The inventors perceive a need for techniques to improve MHMC coding operations that provide greater flexibility to coding system to represent image data in coding.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0012" num="0009"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a simplified block diagram of a video delivery system <b>100</b> according to an aspect of the present disclosure.</p><p id="p-0013" num="0010"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a functional block diagram illustrating components of an encoding terminal.</p><p id="p-0014" num="0011"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a functional block diagram illustrating components of a decoding terminal according to an aspect of the present disclosure.</p><p id="p-0015" num="0012"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates exemplary application of MHMC coding.</p><p id="p-0016" num="0013"><figref idref="DRAWINGS">FIGS. <b>5</b>(<i>a</i>) and <b>5</b>(<i>b</i>)</figref>, respectively, illustrate exemplary operation of multi-hypothesis motion compensation coding according to an aspect of the present disclosure.</p><p id="p-0017" num="0014"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a functional block diagram of a coding system according to an aspect of the present disclosure.</p><p id="p-0018" num="0015"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a functional block diagram of a decoding system according to an aspect of the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0004" level="1">DETAILED DESCRIPTION</heading><p id="p-0019" num="0016">Aspects of the present disclosure provide techniques for coding and decoding video in which a plurality of coding hypotheses are developed for an input pixel block of frame content. Each coding hypothesis may include generation of prediction data for the input pixel block according to a respective prediction search. The input pixel block may be coded with reference to a prediction block formed from prediction data derived according to plurality of hypotheses. Data of the coded pixel block may be transmitted to a decoder along with data identifying a number of the hypotheses used during the coding to a channel. At a decoder, an inverse process may be performed, which may include generation of a counterpart prediction block from prediction data derived according to the hypothesis identified with the coded pixel block data, then decoding of the coded pixel block according to the prediction data.</p><p id="p-0020" num="0017"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a simplified block diagram of a video delivery system <b>100</b> according to an aspect of the present disclosure. The system <b>100</b> may include a plurality of terminals <b>110</b>, <b>120</b> interconnected via a network. The terminals <b>110</b>, <b>120</b> may code video data for transmission to their counterparts via the network. Thus, a first terminal <b>110</b> may capture video data locally, code the video data and transmit the coded video data to the counterpart terminal <b>120</b> via a channel. The receiving terminal <b>120</b> may receive the coded video data, decode it, and render it locally, for example, on a display at the terminal <b>120</b>. If the terminals are engaged in bidirectional exchange of video data, then the terminal <b>120</b> may capture video data locally, code the video data and transmit the coded video data to the counterpart terminal <b>110</b> via another channel. The receiving terminal <b>110</b> may receive the coded video data transmitted from terminal <b>120</b>, decode it, and render it locally, for example, on its own display. The processes described can operate on both frame and field frame coding but, for simplicity, the present discussion will describe the techniques in the context of integral frames.</p><p id="p-0021" num="0018">A video coding system <b>100</b> may be used in a variety of applications. In a first application, the terminals <b>110</b>, <b>120</b> may support real time bidirectional exchange of coded video to establish a video conferencing session between them. In another application, a terminal <b>110</b> may code pre-produced video (for example, television or movie programming) and store the coded video for delivery to one or, often, many downloading clients (e.g., terminal <b>120</b>). Thus, the video being coded may be live or pre-produced, and the terminal <b>110</b> may act as a media server, delivering the coded video according to a one-to-one or a one-to-many distribution model. For the purposes of the present discussion, the type of video and the video distribution schemes are immaterial unless otherwise noted.</p><p id="p-0022" num="0019">In <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the terminals <b>110</b>, <b>120</b> are illustrated as a personal computer and a smart phone, respectively, but the principles of the present disclosure are not so limited. Aspects of the present disclosure also find application with various types of computers (desktop, laptop, and tablet computers), computer servers, media players, dedicated video conferencing equipment and/or dedicated video encoding equipment.</p><p id="p-0023" num="0020">The network <b>130</b> represents any number of networks that convey coded video data between the terminals <b>110</b>, <b>120</b>, including for example wireline and/or wireless communication networks. The communication network may exchange data in circuit-switched or packet-switched channels. Representative networks include telecommunications networks, local area networks, wide area networks, and/or the Internet. For the purposes of the present discussion, the architecture and topology of the network are immaterial to the operation of the present disclosure unless otherwise noted.</p><p id="p-0024" num="0021"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a functional block diagram illustrating components of an encoding terminal according to an aspect of the present disclosure. The encoding terminal may include a video source <b>210</b>, an image processor <b>220</b>, a coding system <b>230</b>, and a transmitter <b>240</b>. The video source <b>210</b> may supply video to be coded. The video source <b>210</b> may be provided as a camera that captures image data of a local environment, a storage device that stores video from some other source or a network connection through which source video data is received. The image processor <b>220</b> may perform signal conditioning operations on the video to be coded to prepare the video data for coding. For example, the preprocessor <b>220</b> alter the frame rate, frame resolution, and/or other properties of the source video. The image processor <b>220</b> also may perform filtering operations on the source video.</p><p id="p-0025" num="0022">The coding system <b>230</b> may perform coding operations on the video to reduce its bandwidth. Typically, the coding system <b>230</b> exploits temporal and/or spatial redundancies within the source video. For example, the coding system <b>230</b> may perform motion compensated predictive coding in which video frame or field frames are parsed into sub-units (called &#x201c;pixel blocks,&#x201d; for convenience), and individual pixel blocks are coded differentially with respect to predicted pixel blocks, which are derived from previously-coded video data. A given pixel block may be coded according to any one of a variety of predictive coding modes, such as:<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0000">    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0023">intra-coding, in which an input pixel block is coded differentially with respect to previously coded/decoded data of a common frame;</li>        <li id="ul0002-0002" num="0024">single prediction inter-coding, in which an input pixel block is coded differentially with respect to data of a previously coded/decoded frame; and</li>        <li id="ul0002-0003" num="0025">multi-hypothesis motion compensation predictive coding, in which an input pixel block is coded predictively using decoded data from two or more sources, via temporal or spatial prediction.<br/>The predictive coding modes may be used cooperatively with other coding techniques, such as Transform Skip coding, RRU coding, scaling of prediction sources, palette coding, and the like.</li>    </ul>    </li></ul></p><p id="p-0026" num="0026">The coding system <b>230</b> may include a forward coder <b>232</b>, a decoder <b>233</b>, an in-loop filter <b>234</b>, a frame buffer <b>235</b>, and a predictor <b>236</b>. The coder <b>232</b> may apply the differential coding techniques to the input pixel block using predicted pixel block data supplied by the predictor <b>236</b>. The decoder <b>233</b> may invert the differential coding techniques applied by the coder <b>232</b> to a subset of coded frames designated as reference frames. The in-loop filter <b>234</b> may apply filtering techniques to the reconstructed reference frames generated by the decoder <b>233</b>. The frame buffer <b>235</b> may store the reconstructed reference frames for use in prediction operations. The predictor <b>236</b> may predict data for input pixel blocks from within the reference frames stored in the frame buffer.</p><p id="p-0027" num="0027">The transmitter <b>240</b> may transmit coded video data to a decoding terminal via a channel CH.</p><p id="p-0028" num="0028"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a functional block diagram illustrating components of a decoding terminal according to an aspect of the present disclosure. The decoding terminal may include a receiver <b>310</b> to receive coded video data from the channel, a video decoding system <b>320</b> that decodes coded data, a post-processor <b>330</b>, and a video sink <b>340</b> that consumes the video data.</p><p id="p-0029" num="0029">The receiver <b>310</b> may receive a data stream from the network and may route components of the data stream to appropriate units within the terminal <b>300</b>. Although <figref idref="DRAWINGS">FIGS. <b>2</b> and <b>3</b></figref> illustrate functional units for video coding and decoding, terminals <b>110</b>, <b>120</b> (<figref idref="DRAWINGS">FIG. <b>1</b></figref>) often will include coding/decoding systems for audio data associated with the video and perhaps other processing units (not shown). Thus, the receiver <b>310</b> may parse the coded video data from other elements of the data stream and route it to the video decoder <b>320</b>.</p><p id="p-0030" num="0030">The video decoder <b>320</b> may perform decoding operations that invert coding operations performed by the coding system <b>140</b>. The video decoder may include a decoder <b>322</b>, an in-loop filter <b>324</b>, a frame buffer <b>326</b>, and a predictor <b>328</b>. The decoder <b>322</b> may invert the differential coding techniques applied by the coder <b>142</b> to the coded frames. The in-loop filter <b>324</b> may apply filtering techniques to reconstructed frame data generated by the decoder <b>322</b>. For example, the in-loop filter <b>324</b> may perform various filtering operations (e.g., de-blocking, de-ringing filtering, sample adaptive offset processing, and the like). The filtered frame data may be output from the decoding system. The frame buffer <b>326</b> may store reconstructed reference frames for use in prediction operations. The predictor <b>328</b> may predict data for input pixel blocks from within the reference frames stored by the frame buffer according to prediction reference data provided in the coded video data.</p><p id="p-0031" num="0031">The post-processor <b>330</b> may perform operations to condition the reconstructed video data for display. For example, the post-processor <b>330</b> may perform various filtering operations (e.g., de-blocking, de-ringing filtering, and the like), which may obscure visual artifacts in output video that are generated by the coding/decoding process. The post-processor <b>330</b> also may alter resolution, frame rate, color space, etc. of the reconstructed video to conform it to requirements of the video sink <b>340</b>.</p><p id="p-0032" num="0032">The video sink <b>340</b> represents various hardware and/or software components in a decoding terminal that may consume the reconstructed video. The video sink <b>340</b> typically may include one or more display devices on which reconstructed video may be rendered. Alternatively, the video sink <b>340</b> may be represented by a memory system that stores the reconstructed video for later use. The video sink <b>340</b> also may include one or more application programs that process the reconstructed video data according to controls provided in the application program. In some aspects, the video sink may represent a transmission system that transmits the reconstructed video to a display on another device, separate from the decoding terminal; for example, reconstructed video generated by a notebook computer may be transmitted to a large flat panel display for viewing.</p><p id="p-0033" num="0033">The foregoing discussion of the encoding terminal and the decoding terminal (<figref idref="DRAWINGS">FIGS. <b>2</b> and <b>3</b></figref>) illustrates operations that are performed to code and decode video data in a single direction between terminals, such as from terminal <b>110</b> to terminal <b>120</b> (<figref idref="DRAWINGS">FIG. <b>1</b></figref>). In applications where bidirectional exchange of video is to be performed between the terminals <b>110</b>, <b>120</b>, each terminal <b>110</b>, <b>120</b> will possess the functional units associated with an encoding terminal (<figref idref="DRAWINGS">FIG. <b>2</b></figref>) and each terminal <b>110</b>, <b>120</b> will possess the functional units associated with a decoding terminal (<figref idref="DRAWINGS">FIG. <b>3</b></figref>). Indeed, in certain applications, terminals <b>110</b>, <b>120</b> may exchange multiple streams of coded video in a single direction, in which case, a single terminal (say terminal <b>110</b>) will have multiple instances of an encoding terminal (<figref idref="DRAWINGS">FIG. <b>2</b></figref>) provided therein. Such implementations are fully consistent with the present discussion.</p><p id="p-0034" num="0034"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates exemplary application of MHMC coding. In <figref idref="DRAWINGS">FIG. <b>4</b></figref>, a block <b>412</b> in the current frame <b>410</b> may be coded with reference to N reference frames <b>420</b>.<b>1</b>-<b>420</b>.<i>n</i>&#x2212;1. Motion vectors mv<b>1</b>-mvn may be derived that reference prediction sources in the reference frames <b>420</b>.<b>1</b>-<b>420</b>.<i>n</i>1&#x2212;1. The prediction sources may be combined as per Eq. 1 to yield a prediction block &#x15d; that serves as a predictor for block <b>412</b>.</p><p id="p-0035" num="0035">In an aspect of the present disclosure, coders may develop coding hypotheses for an input block using different block sizes. One set of exemplary coding hypotheses is shown in <figref idref="DRAWINGS">FIGS. <b>5</b>(<i>a</i>) and <b>5</b>(<i>b</i>)</figref>. In <figref idref="DRAWINGS">FIG. <b>5</b>(<i>a</i>)</figref>, an input block <b>512</b> from an input frame <b>510</b> may be coded using coding hypotheses that use different block sizes. A first hypothesis is represented by motion vector mv<b>1</b>, which codes the input block <b>512</b> with reference to a block <b>522</b> in a first reference frame <b>520</b>; the reference block <b>522</b> has a size equal to the size of block <b>512</b>. A second set of hypotheses is represented by motion vectors mv<b>2</b>.<b>1</b>-mv<b>2</b>.<b>4</b>. For the second set of hypotheses, the input block <b>512</b> is partitioned into a plurality of sub-blocks (here, four sub-blocks <b>514</b>.<b>1</b>-<b>514</b>.<b>4</b>), each of which are predictively coded. For ease of illustration, <figref idref="DRAWINGS">FIG. <b>5</b>(<i>a</i>)</figref> illustrates each of the sub-blocks <b>514</b>.<b>1</b>-<b>514</b>.<b>4</b> coded with reference to content from a common reference frame <b>530</b>, shown as reference blocks <b>532</b>.<b>1</b>-<b>532</b>.<b>4</b>. In practice, each of the sub-blocks may be predicted independently of the other sub-blocks, which may cause prediction references to be selected from the same or different reference frames (not shown).</p><p id="p-0036" num="0036"><figref idref="DRAWINGS">FIG. <b>5</b>(<i>b</i>)</figref> illustrates the relationship between the coding hypotheses and a prediction block s <b>540</b> that will be used for coding and decoding. Content of the various hypotheses may be merged together to generate the prediction block. In the example of <figref idref="DRAWINGS">FIG. <b>5</b>(<i>b</i>)</figref>, the prediction block <b>540</b> may be generated from prediction data contributed by block <b>522</b> of the first hypothesis and prediction data contributed by the sub-blocks <b>532</b>.<b>1</b>-<b>532</b>.<b>4</b>, which may be weighted according to respective weight factors as shown in Eq. 1.</p><p id="p-0037" num="0037">Although <figref idref="DRAWINGS">FIGS. <b>5</b>(<i>a</i>) and <b>5</b>(<i>b</i>)</figref> illustrate coding by two hypotheses (the second hypothesis being formed from predictions of four sub-blocks), in principle, MHMC coding may be extended to a greater number of hypotheses. Moreover, although <figref idref="DRAWINGS">FIG. <b>5</b>(<i>b</i>)</figref> illustrates a linear weighting model, non-linear weighting models also may be used.</p><p id="p-0038" num="0038"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a functional block diagram of a coding system <b>600</b> according to an aspect of the present disclosure. The system <b>600</b> may include a pixel block coder <b>610</b>, a pixel block decoder <b>620</b>, a frame buffer <b>630</b>, an in-loop filter system <b>640</b>, a reference frame store <b>650</b>, a predictor <b>660</b>, a controller <b>670</b>, and a syntax unit <b>680</b>. The predictor <b>660</b> may develop the different hypotheses for use during coding of a newly-presented input pixel block s and it may supply a prediction block s to the pixel block coder <b>610</b>. The pixel block coder <b>610</b> may code the new pixel block by predictive coding techniques and present coded pixel block data to the syntax unit <b>680</b>. The pixel block decoder <b>620</b> may decode the coded pixel block data, generating decoded pixel block data therefrom. The frame buffer <b>630</b> may generate reconstructed frame data from the decoded pixel block data. The in-loop filter <b>640</b> may perform one or more filtering operations on the reconstructed frame. For example, the in-loop filter <b>640</b> may perform deblocking filtering, sample adaptive offset (SAO) filtering, adaptive loop filtering (ALF), maximum likelihood (ML) based filtering schemes, deringing, debanding, sharpening, resolution scaling, and the like. The reference frame store <b>650</b> may store the filtered frame, where it may be used as a source of prediction of later-received pixel blocks. The syntax unit <b>680</b> may assemble a data stream from the coded pixel block data, which conforms to a governing coding protocol.</p><p id="p-0039" num="0039">In MHMC coding, the predictor <b>660</b> may select the different hypotheses from among the different candidate prediction modes that are available under a governing coding syntax. The predictor <b>660</b> may decide, for example, the number of hypotheses that may be used, the prediction sources for those hypotheses and, in certain aspects, partitioning sizes at which the predictions will be performed. For example, the predictor <b>660</b> may decide whether a given input pixel block will be coded using a prediction block that matches the sizes of the input pixel block or whether it will be coded using prediction blocks at smaller sizes. The predictor <b>660</b> also may decide, for some smaller-size partitions of the input block, that SKIP coding will be applied to one or more of the partitions (called &#x201c;null&#x201d; coding herein).</p><p id="p-0040" num="0040">The pixel block coder <b>610</b> may include a subtractor <b>612</b>, a transform unit <b>614</b>, a quantizer <b>616</b>, and an entropy coder <b>618</b>. The pixel block coder <b>610</b> may accept pixel blocks of input data at the subtractor <b>612</b>. The subtractor <b>612</b> may receive predicted pixel blocks from the predictor <b>660</b> and generate an array of pixel residuals therefrom representing a difference between the input pixel block and the predicted pixel block. The transform unit <b>614</b> may apply a transform to the sample data output from the subtractor <b>612</b>, to convert data from the pixel domain to a domain of transform coefficients. The quantizer <b>616</b> may perform quantization of transform coefficients output by the transform unit <b>614</b>. The quantizer <b>616</b> may be a uniform or a non-uniform quantizer. The entropy coder <b>618</b> may reduce bandwidth of the output of the coefficient quantizer by coding the output, for example, by variable length code words or using a context adaptive binary arithmetic coder.</p><p id="p-0041" num="0041">The transform unit <b>614</b> may operate in a variety of transform modes as determined by the controller <b>670</b>. For example, the transform unit <b>614</b> may apply a discrete cosine transform (DCT), a discrete sine transform (DST), a Walsh-Hadamard transform, a Haar transform, a Daubechies wavelet transform, or the like. In an aspect, the controller <b>670</b> may select a coding mode M to be applied by the transform unit <b>615</b>, may configure the transform unit <b>615</b> accordingly and may signal the coding mode M in the coded video data, either expressly or impliedly.</p><p id="p-0042" num="0042">The quantizer <b>616</b> may operate according to a quantization parameter Q<sub>P </sub>that is supplied by the controller <b>670</b>. In an aspect, the quantization parameter Q<sub>P </sub>may be applied to the transform coefficients as a multi-value quantization parameter, which may vary, for example, across different coefficient locations within a transform-domain pixel block. Thus, the quantization parameter Q<sub>P </sub>may be provided as a quantization parameters array.</p><p id="p-0043" num="0043">The entropy coder <b>618</b>, as its name implies, may perform entropy coding of data output from the quantizer <b>616</b>. For example, the entropy coder <b>618</b> may perform run length coding, Huffman coding, Golomb coding, Context Adaptive Binary Arithmetic Coding, and the like.</p><p id="p-0044" num="0044">The pixel block decoder <b>620</b> may invert coding operations of the pixel block coder <b>610</b>. For example, the pixel block decoder <b>620</b> may include a dequantizer <b>622</b>, an inverse transform unit <b>624</b>, and an adder <b>626</b>. The pixel block decoder <b>620</b> may take its input data from an output of the quantizer <b>616</b>. Although permissible, the pixel block decoder <b>620</b> need not perform entropy decoding of entropy-coded data since entropy coding is a lossless event. The dequantizer <b>622</b> may invert operations of the quantizer <b>616</b> of the pixel block coder <b>610</b>. The dequantizer <b>622</b> may perform uniform or non-uniform de-quantization as specified by the decoded signal Q<sub>P</sub>. Similarly, the inverse transform unit <b>624</b> may invert operations of the transform unit <b>614</b>. The dequantizer <b>622</b> and the inverse transform unit <b>624</b> may use the same quantization parameters Q<sub>P </sub>and transform mode M as their counterparts in the pixel block coder <b>610</b>. Quantization operations likely will truncate data in various respects and, therefore, data recovered by the dequantizer <b>622</b> likely will possess coding errors when compared to the data presented to the quantizer <b>616</b> in the pixel block coder <b>610</b>.</p><p id="p-0045" num="0045">The adder <b>626</b> may invert operations performed by the subtractor <b>612</b>. It may receive the same prediction pixel block from the predictor <b>660</b> that the subtractor <b>612</b> used in generating residual signals. The adder <b>626</b> may add the prediction pixel block to reconstructed residual values output by the inverse transform unit <b>624</b> and may output reconstructed pixel block data.</p><p id="p-0046" num="0046">As described, the frame buffer <b>630</b> may assemble a reconstructed frame from the output of the pixel block decoders <b>620</b>. The in-loop filter <b>640</b> may perform various filtering operations on recovered pixel block data. For example, the in-loop filter <b>640</b> may include a deblocking filter, a sample adaptive offset (&#x201c;SAO&#x201d;) filter, and/or other types of in loop filters (not shown).</p><p id="p-0047" num="0047">The reference frame store <b>650</b> may store filtered frame data for use in later prediction of other pixel blocks. Different types of prediction data are made available to the predictor <b>660</b> for different prediction modes. For example, for an input pixel block, intra prediction takes a prediction reference from decoded data of the same frame in which the input pixel block is located. Thus, the reference frame store <b>650</b> may store decoded pixel block data of each frame as it is coded. For the same input pixel block, inter prediction may take a prediction reference from previously coded and decoded frame(s) that are designated as reference frames. Thus, the reference frame store <b>650</b> may store these decoded reference frames.</p><p id="p-0048" num="0048">As discussed, the predictor <b>660</b> may supply prediction blocks &#x15d; to the pixel block coder <b>610</b> for use in generating residuals. The predictor <b>660</b> may include, for each of a plurality of hypotheses <b>661</b>.<b>1</b>-<b>661</b>.<i>n</i>, an inter predictor <b>662</b>, an intra predictor <b>663</b>, and a mode decision unit <b>662</b>. The different hypotheses <b>661</b>.<b>1</b>-<b>661</b>.<i>n </i>may operate at different partition sizes as described above. For each hypothesis, the inter predictor <b>662</b> may receive pixel block data representing a new pixel block to be coded and may search reference frame data from store <b>650</b> for pixel block data from reference frame(s) for use in coding the input pixel block. The inter-predictor <b>662</b> may perform its searches at the partition sizes of the respective hypothesis. Thus, when searching at smaller partition sizes, the inter-predictor <b>662</b> may perform multiple searches, one using each of the sub-partitions at work for its respective hypothesis. The inter predictor <b>662</b> may select prediction reference data that provides a closest match to the input pixel block being coded. The inter predictor <b>662</b> may generate prediction reference metadata, such as prediction block size and motion vectors, to identify which portion(s) of which reference frames were selected as source(s) of prediction for the input pixel block.</p><p id="p-0049" num="0049">The intra predictor <b>663</b> may support Intra (I) mode coding. The intra predictor <b>663</b> may search from among pixel block data from the same frame as the pixel block being coded that provides a closest match to the input pixel block. The intra predictor <b>663</b> also may run searches at the partition size for its respective hypothesis and, when sub-partitions are employed, separate searches may be run for each sub-partition. The intra predictor <b>663</b> also may generate prediction mode indicators to identify which portion of the frame was selected as a source of prediction for the input pixel block.</p><p id="p-0050" num="0050">The mode decision unit <b>664</b> may select a final coding mode for the hypothesis from the output of the inter-predictor <b>662</b> and the inter-predictor <b>663</b>. The mode decision unit <b>664</b> may output prediction data and the coding parameters (e.g., selection of reference frames, motion vectors and the like) for the mode selected for the respective hypothesis. Typically, as described above, the mode decision unit <b>664</b> will select a mode that achieves the lowest distortion when video is decoded given a target bitrate. Exceptions may arise when coding modes are selected to satisfy other policies to which the coding system <b>600</b> adheres, such as satisfying a particular channel behavior, or supporting random access or data refresh policies.</p><p id="p-0051" num="0051">Prediction data output from the mode decision units <b>664</b> of the different hypotheses <b>661</b>.<b>1</b>-<b>661</b>.N may be input to a prediction block synthesis unit <b>665</b>, which merges the prediction data into an aggregate prediction block &#x15d;. As described, the prediction block &#x15d; may be formed from a linear combination of the predictions from the individual hypotheses, for example, as set forth in Eq. 1, or non-linear combinations may be performed. The prediction block synthesis unit <b>665</b> may supply the prediction block &#x15d; to the pixel block coder <b>610</b>. The predictor <b>660</b> may output to the controller <b>670</b> parameters representing coding decisions for each hypothesis.</p><p id="p-0052" num="0052">The controller <b>670</b> may control overall operation of the coding system <b>600</b>. The controller <b>670</b> may select operational parameters for the pixel block coder <b>610</b> and the predictor <b>660</b> based on analyses of input pixel blocks and also external constraints, such as coding bitrate targets and other operational parameters. As is relevant to the present discussion, when it selects quantization parameters Q<sub>P</sub>, the use of uniform or non-uniform quantizers, and/or the transform mode M, it may provide those parameters to the syntax unit <b>680</b>, which may include data representing those parameters in the data stream of coded video data output by the system <b>600</b>. The controller <b>670</b> also may select between different modes of operation by which the system may generate reference images and may include metadata identifying the modes selected for each portion of coded data.</p><p id="p-0053" num="0053">During operation, the controller <b>670</b> may revise operational parameters of the quantizer <b>616</b> and the transform unit <b>615</b> at different granularities of image data, either on a per pixel block basis or on a larger granularity (for example, per frame, per slice, per largest coding unit (&#x201c;LCU&#x201d;) or Coding Tree Unit (CTU), or another region). In an aspect, the quantization parameters may be revised on a per-pixel basis within a coded frame.</p><p id="p-0054" num="0054">Additionally, as discussed, the controller <b>670</b> may control operation of the in-loop filter <b>640</b> and the prediction unit <b>660</b>. Such control may include, for the prediction unit <b>660</b>, mode selection (lambda, modes to be tested, search windows, distortion strategies, etc.), and, for the in-loop filter <b>640</b>, selection of filter parameters, reordering parameters, weighted prediction, etc.</p><p id="p-0055" num="0055"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a functional block diagram of a decoding system <b>700</b> according to an aspect of the present disclosure. The decoding system <b>700</b> may include a syntax unit <b>710</b>, a pixel block decoder <b>720</b>, an in-loop filter <b>740</b>, a reference frame store <b>750</b>, a predictor <b>760</b>, and a controller <b>770</b>. As with the encoder (<figref idref="DRAWINGS">FIG. <b>6</b></figref>), the pixel block decoder <b>720</b> and predictor <b>760</b> may be instantiated for each of the hypotheses identified by the coded video data.</p><p id="p-0056" num="0056">The syntax unit <b>710</b> may receive a coded video data stream and may parse the coded data into its constituent parts. Data representing coding parameters may be furnished to the controller <b>770</b>, while data representing coded residuals (the data output by the pixel block coder <b>610</b> of <figref idref="DRAWINGS">FIG. <b>6</b></figref>) may be furnished to its respective pixel block decoder <b>720</b>. The predictor <b>760</b> may generate a prediction block &#x15d; from reference data available in the reference frame store <b>750</b> according to coding parameter data provided in the coded video data. It may supply the prediction block &#x15d; to the pixel block decoder. The pixel block decoder <b>720</b> may invert coding operations applied by the pixel block coder <b>610</b> (<figref idref="DRAWINGS">FIG. <b>6</b></figref>). The frame buffer <b>730</b> may create a reconstructed frame from decoded pixel blocks s&#x2032; output by the pixel block decoder <b>720</b>. The in-loop filter <b>740</b> may filter the reconstructed frame data. The filtered frames may be output from the decoding system <b>700</b>. Filtered frames that are designated to serve as reference frames also may be stored in the reference frame store <b>750</b>.</p><p id="p-0057" num="0057">The pixel block decoder <b>720</b> may include an entropy decoder <b>722</b>, a dequantizer <b>724</b>, an inverse transform unit <b>726</b>, and an adder <b>728</b>. The entropy decoder <b>722</b> may perform entropy decoding to invert processes performed by the entropy coder <b>618</b> (<figref idref="DRAWINGS">FIG. <b>6</b></figref>). The dequantizer <b>724</b> may invert operations of the quantizer <b>716</b> of the pixel block coder <b>610</b> (<figref idref="DRAWINGS">FIG. <b>6</b></figref>). Similarly, the inverse transform unit <b>726</b> may invert operations of the transform unit <b>614</b> (<figref idref="DRAWINGS">FIG. <b>6</b></figref>). They may use the quantization parameters Q<sub>P </sub>and transform modes M that are provided in the coded video data stream. Because quantization is likely to truncate data, the pixel blocks s&#x2032; recovered by the dequantizer <b>724</b>, likely will possess coding errors when compared to the input pixel blocks s presented to the pixel block coder <b>610</b> of the encoder (<figref idref="DRAWINGS">FIG. <b>6</b></figref>).</p><p id="p-0058" num="0058">The adder <b>728</b> may invert operations performed by the subtractor <b>610</b> (<figref idref="DRAWINGS">FIG. <b>6</b></figref>). It may receive a prediction pixel block from the predictor <b>760</b> as determined by prediction references in the coded video data stream. The adder <b>728</b> may add the prediction pixel block to reconstructed residual values output by the inverse transform unit <b>726</b> and may output reconstructed pixel block data.</p><p id="p-0059" num="0059">As described, the frame buffer <b>730</b> may assemble a reconstructed frame from the output of the pixel block decoder <b>720</b>. The in-loop filter <b>740</b> may perform various filtering operations on recovered pixel block data as identified by the coded video data. For example, the in-loop filter <b>740</b> may include a deblocking filter, a sample adaptive offset (&#x201c;SAO&#x201d;) filter, and/or other types of in loop filters. In this manner, operation of the frame buffer <b>730</b> and the in loop filter <b>740</b> mimics operation of the counterpart frame buffer <b>630</b> and in loop filter <b>640</b> of the encoder <b>600</b> (<figref idref="DRAWINGS">FIG. <b>6</b></figref>).</p><p id="p-0060" num="0060">The reference frame store <b>750</b> may store filtered frame data for use in later prediction of other pixel blocks. The reference frame store <b>750</b> may store decoded frames as it is coded for use in intra prediction. The reference frame store <b>750</b> also may store decoded reference frames.</p><p id="p-0061" num="0061">As discussed, the predictor <b>760</b> may supply the prediction blocks &#x15d; to the pixel block decoder <b>720</b>. The predictor <b>760</b> may retrieve prediction data from the reference frame store <b>750</b> for each of the hypotheses represented in the coded video data (represented by hypothesis predictors <b>762</b>.<b>1</b>-<b>762</b>.<i>n</i>). A prediction block synthesis unit <b>764</b> may generate an aggregate prediction block &#x15d; from the prediction data of the different hypothesis. In this manner, the prediction block synthesis unit <b>764</b> may replicate operations of the synthesis unit <b>665</b> from the encoder (<figref idref="DRAWINGS">FIG. <b>6</b></figref>). The predictor <b>760</b> may supply predicted pixel block data as determined by the prediction reference indicators supplied in the coded video data stream.</p><p id="p-0062" num="0062">The controller <b>770</b> may control overall operation of the coding system <b>700</b>. The controller <b>770</b> may set operational parameters for the pixel block decoder <b>720</b> and the predictor <b>760</b> based on parameters received in the coded video data stream. As is relevant to the present discussion, these operational parameters may include quantization parameters Q<sub>P </sub>for the dequantizer <b>724</b> and transform modes M for the inverse transform unit <b>710</b>. As discussed, the received parameters may be set at various granularities of image data, for example, on a per pixel block basis, a per frame basis, a per slice basis, a per LCU/CTU basis, or based on other types of regions defined for the input image.</p><heading id="h-0005" level="2">Techniques of Multi-Hypothesis Motion Compensation</heading><p id="p-0063" num="0063">In an aspect, encoders may provide signalling in a coded bit stream to identify the number of hypotheses that are used to code frame data for each such hypothesis. For example, when frame data is to be coded by MHMC, coded video data may identify a number of hypotheses using a count value. When a given hypothesis can be coded as prediction blocks of different sizes, coded video data may contain data identifying such prediction block sizes.</p><p id="p-0064" num="0064">An exemplary syntax for such identifications is provided in Table 1 below:</p><p id="p-0065" num="0000"><tables id="TABLE-US-00001" num="00001"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="196pt" align="left"/><colspec colname="2" colwidth="21pt" align="left"/><thead><row><entry namest="1" nameend="2" rowsep="1">TABLE 1</entry></row><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry>if(type == MH ) {</entry><entry/></row><row><entry>&#x2003;number_of_hypotheses_minus2</entry><entry>ue(v)</entry></row><row><entry>&#x2003;for( i = 0; i &#x3c; number_of_lists_minus2 + 2; i++ ) {</entry></row><row><entry>&#x2003;&#x2003;list_prediction_implicit_present_flag[ i ]</entry><entry>u(1)</entry></row><row><entry>&#x2003;&#x2003;log2_min_luma_hypothesis_block_size_minus2 [ i ]</entry><entry>ue(v)</entry></row><row><entry>&#x2003;&#x2003;log2_diff_max_min_luma_hypothesis_block_size [ i ]</entry><entry>ue(v)</entry></row><row><entry>&#x2003;}</entry></row><row><entry>}</entry></row><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup></table></tables><br/>Here, type corresponds to the slice type. If MH, then that slice is indicated as a multihypothesis slice that permits 2 or more hypotheses to be used for prediction. The field number_of_hypotheses_minus2 may identify the number of hypotheses available for coding. The field list_prediction_implicit_present_flag[i] enforces that a hypothesis from list i is always present for all CTUs or at least for all inter CTUs in the current slice. The fields log 2_min_luma_hypothesis_block_size_minus2[i] and log 2_diff_max_min_luma_hypothesis_block_size[i] respectively may identify minimum and maximum sizes of prediction blocks that are available for each such hypothesis. Providing minimum and maximum sizes for the prediction blocks may constrain coding complexity by avoiding MHMC combinations outside the indicated size ranges.</p><p id="p-0066" num="0065">The foregoing example permits use of multiple hypotheses for block sizes starting from M<sub>L</sub>&#xd7;N<sub>L </sub>to M<sub>H</sub>&#xd7;N<sub>H</sub>, where M<sub>L</sub>, N<sub>L</sub>, M<sub>H</sub>, N<sub>H </sub>are defined, respectively, by the log 2_min_luma_hypothesis_block_size_minus2[i] and log 2_diff_max_min_luma_hypothesis_block_size[i] fields. For example, if the minimum size for a hypothesis from list i was defined as 16 and the maximum size was defined as 64, then this syntax would permit prediction block sizes from list i with block sizes of, for example, 16&#xd7;16, 32&#xd7;32, and 64&#xd7;64 pixels. If rectangular partitions were also permitted for prediction, these would also be limited within the specified resolutions. Of course, other variants would be supported by the syntax of Table 1. For example, prediction block sizes may be defined starting from a size 32&#xd7;32 and above.</p><p id="p-0067" num="0066">The syntax identified in Table 1 may be provided at different levels of coded video data, such as at the sequence-, picture-, slice-level, a segment, or for a group of CTUs. Table 12 below illustrates application of the syntax to an exemplary HEVC-style slice syntactic element. Moreover, the syntax elements may be assigned to these levels dynamically based on coding decision performed by an encoder. In this manner, the syntax increases flexibility in the encoder and permits the encoder to conserve signalling overhead when the syntax elements may be provided in higher-level elements of the coding protocol.</p><p id="p-0068" num="0067">Table 2, for example, provides an example in which slice segment header syntax is extended to include parameters that indicate for which block sizes a particular hypothesis will be valid or not. A single parameter is identified for the minimum width and height, i.e. log 2_min_luma_hypothesis_block_size_minus2[i] as well as for the maximum, i.e. log 2_diff_max_min_luma_hypothesis_block_size_minus2[i]. The list_prediction_implicit_present_flag[i] discussed above is provided as well. Independent parameters for the width and height could also be used. Additional elements that are impacted by the use of Multihypothesis prediction instead of biprediction, such as the number of references for each list (num_ref_idx_active_minus1_list[i]), the use of zero motion prediction (mvd_list_zero_flag[i]), and from which list the collocated temporal motion vector will be derived.</p><p id="p-0069" num="0000"><tables id="TABLE-US-00002" num="00002"><table frame="none" colsep="0" rowsep="0" pgwide="1"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="287pt" align="center"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE 2</entry></row></thead><tbody valign="top"><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>Exemplary Slice Segment Header to support Multihypothesis prediction</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="offset" colwidth="252pt" align="left"/><colspec colname="1" colwidth="35pt" align="left"/><tbody valign="top"><row><entry/><entry>Descriptor</entry></row><row><entry/><entry namest="offset" nameend="1" align="center" rowsep="1"/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="252pt" align="left"/><colspec colname="2" colwidth="35pt" align="left"/><tbody valign="top"><row><entry>slice_segment_header( ) {</entry><entry/></row><row><entry>&#x2003;first_slice_segment_in_pic_flag</entry><entry>u(1)</entry></row><row><entry>&#x2003;if( nal_unit_type &#x3e;= BLA_W_LP &#x26;&#x26; nal_unit_type &#x3c;= RSV_IRAP_VCL23)</entry></row><row><entry>&#x2003;&#x2003;no_output_of_prior_pics_flag</entry><entry>u(1)</entry></row><row><entry>&#x2003;slice_pic_parameter_set_id</entry><entry>ue(v)</entry></row><row><entry>&#x2003;if( !first_slice_segment_in_pic_flag ) {</entry></row><row><entry>&#x2003;&#x2003;if( dependent_slice_segments_enabled_flag )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;dependent_slice_segment_flag</entry><entry>u(1)</entry></row><row><entry>&#x2003;&#x2003;slice_segment_address</entry><entry>u(v)</entry></row><row><entry>&#x2003;}</entry></row><row><entry>&#x2003;if( !dependent_slice_segment_flag ) {</entry></row><row><entry>&#x2003;&#x2003;for( i = 0; i &#x3c; num_extra_slice_header_bits; i++ )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;slice_reserved_flag[ i ]</entry><entry>u(1)</entry></row><row><entry>&#x2003;&#x2003;slice_type</entry><entry>ue(v)</entry></row><row><entry>&#x2003;&#x2003;if( slice_type == MH ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;number_of_lists_minus2</entry><entry>ue(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;for( i = 0; i &#x3c; number_of_lists_minus2 + 2; i++ ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;list_prediction_implicit_present_flag[ i ]</entry><entry>u(1)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;log2_min_luma_hypothesis_block_size_minus2 [ i ]</entry><entry>ue(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;log2_diff_max_min_luma_hypothesis_block_size [ i ]</entry><entry>ue(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;else { /* slice_type == P */</entry></row><row><entry>&#x2003;&#x2003;&#x2003;number_of_lists_minus2 = &#x2212;1</entry></row><row><entry>&#x2003;&#x2003;&#x2003;list_prediction_implicit_present_flag[ 0 ] = 1</entry></row><row><entry>&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;if( output_flag_present_flag )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;pic_output_flag</entry><entry>u(1)</entry></row><row><entry>&#x2003;&#x2003;if( separate_colour_plane_flag = = 1 )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;colour_plane_id</entry><entry>u(2)</entry></row><row><entry>&#x2003;&#x2003;if( nal_unit_type != IDR_W_RADL &#x26;&#x26; nal_unit_type != IDR_N_LP ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;slice_pic_order_cnt_lsb</entry><entry>u(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;short_term_ref_pic_set_sps_flag</entry><entry>u(1)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;if( !short_term_ref_pic_set_sps_flag )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;st_ref_pic_set( num_short_term_ref_pic_sets )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;else if( num_short_term_ref_pic_sets &#x3e; 1 )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;short_term_ref_pic_set_idx</entry><entry>u(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;if( long_term_ref_pics_present_flag ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;if( num_long_term_ref_pics_sps &#x3e; 0 )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;num_long_term_sps</entry><entry>ue(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;num_long_term_pics</entry><entry>ue(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;for( i = 0; i &#x3c; num_long_term_sps + num_long_term_pics; i++ ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;if( i &#x3c; num_long_term_sps ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;if( num_long_term_ref_pics_sps &#x3e; 1 )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;lt_idx_sps[ i ]</entry><entry>u(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;} else {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;poc_lsb_lt[ i ]</entry><entry>u(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;used_by_curr_pic_lt_flag[ i ]</entry><entry>u(1)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;delta_poc_msb_present_flag[ i ]</entry><entry>u(1)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;if( delta_poc_msb_present_flag[ i ] )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;delta_poc_msb_cycle_lt[ i ]</entry><entry>ue(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;&#x2003;if( sps_temporal_mvp_enabled_flag )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;slice_temporal_mvp_enabled_flag</entry><entry>u(1)</entry></row><row><entry>&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;if( sample_adaptive_offset_enabled_flag ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;slice_sao_luma_flag</entry><entry>u(1)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;if( ChromaArrayType != 0 )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;slice_sao_chroma_flag</entry><entry>u(1)</entry></row><row><entry>&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;if( slice_type = = P | | slice_type = = MH ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;num_ref_idx_active_override_flag</entry><entry>u(1)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;if( num_ref_idx_active_override_flag ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;for( i = 0; i &#x3c; number_of_lists_minus2 + 2; i++ )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;num_ref_idx_ active_minus1_list[ i ]</entry><entry>ue(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;&#x2003;if( lists_modification_present_flag &#x26;&#x26; NumPicTotalCurr &#x3e; 1 )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;ref_pic_lists_modification( )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;mvd_list_zero_flag[ 0 ] = 0</entry></row><row><entry>&#x2003;&#x2003;&#x2003;for( i = 1; i &#x3c; number_of_lists_minus2 + 2; i++ )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;mvd_list_zero_flag[ i ]</entry><entry>u(1)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;if( cabac_init_present_flag )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;cabac_init_flag</entry><entry>u(1)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;if( slice_temporal_mvp_enabled_flag ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;if( slice_type = = MH )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;collocated_from_list</entry><entry>ue(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;else</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;collocated_from_list = 0</entry><entry>ue(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;if( (num_ref_idx_active_minus1_list[ collocated_from_list ] &#x3e; 0 )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;collocated_ref_idx</entry><entry>ue(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;&#x2003;if( ( weighted_pred_flag &#x26;&#x26; slice_type = = P ) | |</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;( weighted_mh_pred_flag &#x26;&#x26; slice_type = = MH ) )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;pred_weight_table( )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;five_minus_max_num_merge_cand</entry><entry>ue(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;if( motion_vector_resolution_control_idc = = 2 )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;use_integer_mv_flag</entry><entry>u(1)</entry></row><row><entry>&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;slice_qp_delta</entry><entry>se(v)</entry></row><row><entry>&#x2003;&#x2003;if( pps_slice_chroma_qp_offsets_present_flag ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;slice_cb_qp_offset</entry><entry>se(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;slice_cr_qp_offset</entry><entry>se(v)</entry></row><row><entry>&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;if( pps_slice_act_qp_offsets_present_flag ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;slice_act_y_qp_offset</entry><entry>se(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;slice_act_cb_qp_offset</entry><entry>se(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;slice_act_cr_qp_offset</entry><entry>se(v)</entry></row><row><entry>&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;if( chroma_qp_offset_list_enabled_flag )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;cu_chroma_qp_offset_enabled_flag</entry><entry>u(1)</entry></row><row><entry>&#x2003;&#x2003;if( deblocking_filter_override_enabled_flag )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;deblocking_filter_override_flag</entry><entry>u(1)</entry></row><row><entry>&#x2003;&#x2003;if( deblocking_filter_override_flag ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;slice_deblocking_filter_disabled_flag</entry><entry>u(1)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;if( !slice_deblocking_filter_disabled_flag ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;slice_beta_offset_div2</entry><entry>se(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;slice_tc_offset_div2</entry><entry>se(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;if( pps_loop_filter_across_slices_enabled_flag &#x26;&#x26;</entry></row><row><entry>&#x2003;&#x2003;&#x2003;( slice_sao_luma_flag | | slice_sao_chroma_flag | |</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;!slice_deblocking_filter_disabled_flag ) )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;slice_loop_filter_across_slices_enabled_flag</entry><entry>u(1)</entry></row><row><entry>&#x2003;}</entry></row><row><entry>&#x2003;if( tiles_enabled_flag | | entropy_coding_sync_enabled_flag ) {</entry></row><row><entry>&#x2003;&#x2003;num_entry_point_offsets</entry><entry>ue(v)</entry></row><row><entry>&#x2003;&#x2003;if( num_entry_point_offsets &#x3e; 0 ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;offset_len_minus1</entry><entry>ue(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;for( i = 0; i &#x3c; num_entry_point_offsets; i++ )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;entry_point_offset_minus1[ i ]</entry><entry>u(v)</entry></row><row><entry>&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;}</entry></row><row><entry>&#x2003;if( slice_segment_header_extension_present_flag ) {</entry></row><row><entry>&#x2003;&#x2003;slice_segment_header_extension_length</entry><entry>ue(v)</entry></row><row><entry>&#x2003;&#x2003;for( i = 0; i &#x3c; slice_segment_header_extension_length; i++)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;slice_segment_header_extension_data_byte[ i ]</entry><entry>u(8)</entry></row><row><entry>&#x2003;}</entry></row><row><entry>&#x2003;byte_alignment( )</entry></row><row><entry>}</entry></row><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0070" num="0068">The foregoing syntax table also provides examples of other coding parameters that may be included. The num_ref_idx_active_minus1_list[i] parameter may specify a maximum reference index for a respective reference picture list that may be used to decode a slice. The collocated_from_list parameter, for example, may identify a list from which temporal motion vector prediction is derived. The collocated_ref_idx parameter may identify a reference index of the collocated picture used for temporal motion vector prediction.</p><p id="p-0071" num="0069">MHMC coding techniques may be represented in coding of a pixel block using a syntax as shown, for example, in Table 3. In this example, prediction units may be defined for each prediction block in use, and their sizes may be identified using the nPbW, nPbH values. Unlike HEVC, which limits prediction to up to two lists, and where the prediction type is indicated with a mode parameter, here the use of a particular hypothesis is indicated through a list specific flag, i.e. list_pred_idc_flag[i][x0][y0]. This flag is either derived based on syntax components signalled earlier in higher levels or based on the utilization of other lists, or explicitly signalled in the bit stream. If this flag is one then that hypothesis will be used in combination with other enabled hypotheses, but if it is set to zero then a hypothesis from that list will not be considered. Depending on its value other parameters, such as reference indices motion vectors, but also other information that may be signalled at this level, such as illumination compensation parameters, can then be signalled.</p><p id="p-0072" num="0000"><tables id="TABLE-US-00003" num="00003"><table frame="none" colsep="0" rowsep="0" pgwide="1"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="308pt" align="center"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE 3</entry></row></thead><tbody valign="top"><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>HEVC-style Prediction Unit Syntax of HEVC To Accommodate MHMC</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="offset" colwidth="273pt" align="left"/><colspec colname="1" colwidth="35pt" align="left"/><tbody valign="top"><row><entry/><entry>Descriptor</entry></row><row><entry/><entry namest="offset" nameend="1" align="center" rowsep="1"/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="273pt" align="left"/><colspec colname="2" colwidth="35pt" align="left"/><tbody valign="top"><row><entry>prediction_unit( x0, y0, nPbW, nPbH ) {</entry><entry/></row><row><entry>&#x2003;if( cu_skip_flag[ x0 ][ y0 ] ) {</entry></row><row><entry>&#x2003;&#x2003;if( MaxNumMergeCand &#x3e; 1 )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;merge_idx[ x0 ][ y0 ]</entry><entry>ae(v)</entry></row><row><entry>&#x2003;} else {/* MODE_INTER*/</entry></row><row><entry>&#x2003;&#x2003;merge_flag[ x0 ][ y0 ]</entry><entry>ae(v)</entry></row><row><entry>&#x2003;&#x2003;if( merge_flag[ x0 ][ y0 ] ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;if( MaxNumMergeCand &#x3e; 1 )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;merge_idx[ x0 ][ y0 ]</entry><entry>ae(v)</entry></row><row><entry>&#x2003;&#x2003;} else {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;set_lists = 0</entry></row><row><entry>&#x2003;&#x2003;&#x2003;for( i = 0; i &#x3c; number_of_lists_minus2 + 2; i++ ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;min_luma_block = (log2_min_luma_hypothesis_block_size_minus2 [ i ] + 2)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;max_luma_block = min_luma_block +</entry></row><row><entry>log2_diff_max_min_luma_hypothesis_block_size [ i ]</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;if( (nPbW&#x3c; (1 &#x3c;&#x3c; min_luma_block)) || (nPbH &#x3c; (1 &#x3c;&#x3c; min_luma_block)) ||</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2002;(nPbW &#x3e; (1 &#x3c;&#x3c; max_luma_block)) || (nPbH &#x3e; (1 &#x3c;&#x3c; max_luma_block)))</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;list_pred_idc_flag[ i ][ x0 ][ y0 ] = 0</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;else if( !list_prediction_implicit_present_flag [ i ] &#x26;&#x26; ( ( i &#x3c;</entry></row><row><entry>number_of_lists_minus2 + 1 ) | | set_lists &#x3e; 0)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;list_pred_idc_flag[ i ][ x0 ][ y0 ]</entry><entry>ae(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;else</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;list_pred_idc_flag[ i ] [ x0 ][ y0 ] = 1</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;set_lists += list_pred_idc_flag[ i ] [ x0 ][ y0 ]</entry></row><row><entry>&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;&#x2003;for( i = 0; i &#x3c; number_of_lists_minus2 + 2; i++ ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;if( list_pred_idc_flag[ i ] [ x0 ][ y0 ] ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;if( num_ref_idx_ active_minus1_list[ i ] &#x3e; 0 )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;ref_idx_list[ i ][ x0 ][ y0 ]</entry><entry>ae(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;if( mvd_list_zero_flag[ i ] &#x26;&#x26; set_lists &#x3e; 1 ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;MvdList[ i ][ x0 ][ y0 ][ 0 ] = 0</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;MvdList[ i ] [ x0 ][ y0 ][ 1 ] = 0</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;} else</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;mvd_coding( x0, y0, i )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;mvp_list_flag[ i ][ x0 ][ y0 ]</entry><entry>ae(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;}</entry></row><row><entry>}</entry></row><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup></table></tables><br/>In this example, the field list_pred_idc_flag[i][x0][y0] indicates whether a list corresponding to hypothesis i is to be used for coding, ref_idx_list[i][x0][y0] indicates an index into the respective list i identifying a reference frame that was used for prediction, and mvp_list_flag[i][x0][y0] indicates a motion vector predictor index of list i.</p><p id="p-0073" num="0070">In another aspect, a coding syntax may impose a restriction on the number of &#x201c;minimum&#x201d; references that can be combined together. For example, the syntax may restrict any hypotheses from list index i, with i&#x3e;1, to be used only when a list with a lower index also is used or, alternatively, if all lower index lists are present. All of these restrictions could also be supported or defined by either implicitly imposing them for all bit streams or by signalling the restrictions for multihypothesis prediction in a higher syntax level. As discussed below, aspects of the present disclosure accommodate such restrictions as multihypothesis modes.</p><p id="p-0074" num="0071">For example, an encoder may signal a parameter mh_mode in the sequence, picture, or slice header, which will indicate the mode of/restrictions imposed on multihypothesis prediction. For example, when or if mh_mode==0, no further restrictions are performed. If mh_mode==1, then an encoder may signal lists with index i&#x3e;1, only and only if all other lists with index j&#x3c;i are also used. Otherwise, those are implicitly set to not present. Other modes could be also defined in addition or in place of the above modes.</p><p id="p-0075" num="0072">In such a case, coding syntax for the prediction unit may be represented as follows:</p><p id="p-0076" num="0000"><tables id="TABLE-US-00004" num="00004"><table frame="none" colsep="0" rowsep="0" pgwide="1"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="308pt" align="center"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE 4</entry></row></thead><tbody valign="top"><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>HEVC-style Alternative Prediction Unit Syntax Modified To Accommodate MHMC</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="offset" colwidth="273pt" align="left"/><colspec colname="1" colwidth="35pt" align="left"/><tbody valign="top"><row><entry/><entry>Descriptor</entry></row><row><entry/><entry namest="offset" nameend="1" align="center" rowsep="1"/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="273pt" align="left"/><colspec colname="2" colwidth="35pt" align="left"/><tbody valign="top"><row><entry>prediction_unit( x0, y0, nPbW, nPbH ) {</entry><entry/></row><row><entry>&#x2003;if( cu_skip_flag[ x0 ][ y0 ] ) {</entry></row><row><entry>&#x2003;&#x2003;if( MaxNumMergeCand &#x3e; 1 )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;merge_idx[ x0 ][ y0 ]</entry><entry>ae(v)</entry></row><row><entry>&#x2003;}else{ /* MODE_INTER */</entry></row><row><entry>&#x2003;&#x2003;merge_flag[ x0 ][ y0 ]</entry><entry>ae(v)</entry></row><row><entry>&#x2003;&#x2003;if( merge_flag[ x0 ][ y0 ] ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;if( MaxNumMergeCand &#x3e; 1 )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;merge_idx[ x0 ][ y0 ]</entry><entry>ae(v)</entry></row><row><entry>&#x2003;&#x2003;} else {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;set_lists = 0</entry></row><row><entry>&#x2003;&#x2003;&#x2003;for( i = 0; i &#x3c; number_of_lists_minus2 + 2; i++ ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;min_luma_block = (log2_min_luma_hypothesis_block_size_minus2 [ i ] + 2)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;max_luma_block = min_luma_block +</entry></row><row><entry>&#x2003;&#x2003;&#x2003;log2_diff_max_min_luma_hypothesis_block_size [ i ]</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;if( (nPbW &#x3c; (1 &#x3c;&#x3c; min_luma_block)) || (nPbH &#x3c; (1 &#x3c;&#x3c; min_luma_block)) ||</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2002;(nPbW &#x3e; (1 &#x3c;&#x3c; max_luma_block)) || (nPbH &#x3e; (1 &#x3c;&#x3c; max_luma_block)) ||</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2002;(mh_mode == 1 &#x26;&#x26; i &#x3e; 1 &#x26;&#x26; set_lists &#x3c; i))</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;list_pred_idc_flag[ i ][ x0 ][ y0 ] = 0</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;else if (mh_mode == 1 &#x26;&#x26; i == 1 &#x26;&#x26; set_lists == 0)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;list_pred_idc_flag[ i ] [ x0 ][ y0 ] = 1</entry></row><row><entry>&#x2003;&#x2003;&#x2003;else if( !list_prediction_implicit_present_flag [ i ] &#x26;&#x26; ( ( i &#x3c;</entry></row><row><entry>&#x2003;&#x2003;&#x2003;number_of_lists_minus2 + 1) | | set_lists &#x3e; 0)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;list_pred_idc_flag[ i ][ x0 ][ y0 ]</entry><entry>ae(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;else</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;list_pred_idc_flag[ i ] [ x0 ][ y0 ] = 1</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;set_lists += list_pred_idc_flag[ i ] [ x0 ][ y0 ]</entry></row><row><entry>&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;&#x2003;for( i = 0; i &#x3c; number_of_lists_minus2 + 2; i++ ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;if( list_pred_idc_flag[ i ] [ x0 ][ y0 ] ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;if( num_ref_idx_ active_minus1_list[ i ] &#x3e; 0 )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;ref_idx_list[ i ][ x0 ][ y0 ]</entry><entry>ae(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;if( mvd_list_zero_flag[ i ] &#x26;&#x26; set_lists &#x3e; 1 ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;MvdList[ i ][ x0 ][ y0 ][ 0 ] = 0</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;MvdList[ i ] [ x0 ][ y0 ][ 1 ] = 0</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;} else</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;mvd_coding( x0, y0, i )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;mvp_list_flag[ i ][ x0 ][ y0 ]</entry><entry>ae(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;}</entry></row><row><entry>}</entry></row><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup></table></tables><br/>In this example, the field list_pred_idc_flag[i][x0][y0] indicates whether a list corresponding to hypothesis i is to be used for coding, ref_idx_list[i][x0][y0] indicates an index into the respective list i identifying a reference frame that was used for prediction, and mvp_list_flag[i][x0][y0] indicates a motion vector predictor index of list i. Unlike the syntax presented in Table 3 it can be seen that mh_mode is now also considered to determine how and if multihypothesis prediction will be performed.</p><p id="p-0077" num="0073">As described, the foregoing techniques reduce the overhead for signalling coding mode for multihypothesis prediction, i.e. when to use and when not to use a certain hypothesis. To further reduce signalling overhead, coding syntax may employ additional techniques, as described below.</p><p id="p-0078" num="0074">In another aspect, motion parameters of a given hypothesis may be derived through relationship(s) with other hypotheses. For example, motion parameters or their differential values may be set to have a value of zero using the mvd_list_zero_flag parameter. Moreover, the syntax may be designed so it can specify any of the following cases for implicit derivation of motion parameters for a list:</p><p id="p-0079" num="0075">a. Enforce all motion parameters for a particular list index to be zero or a fixed vector (sent at a higher syntax layer, such as the sequence or picture parameter sets, or the slice header).</p><p id="p-0080" num="0076">b. Enforce either the vertical or horizontal motion vector component value of a particular list index to a fixed (zero or otherwise) value. Again such information could be signalled at a higher syntax layer.</p><p id="p-0081" num="0077">c. Set illumination compensation (weighted prediction) parameters to fixed values for all or for some of the lists (e.g. for lists with i&#x3e;1). If explicit weighting parameters are present at the block level, the syntax may have explicit parameters only for the earlier lists (i&#x3c;2) and, for all others, illumination parameters could be derived from the parameters of the earlier lists or fixed to default weighting parameters. Parameters could also be derived from previously-decoded neighbouring samples with corresponding increases to the complexity of the codec.</p><p id="p-0082" num="0078">d. Establish a mathematical relationship of the motion vectors of one list i with the motion vectors of another, earlier-in-presence list j (j&#x3c;i). The relationship may also be related to the ref_idx and its associated picture order count or other parameter that may be available in the codec, and which parameter indicates a form of temporal distance relationships between references. Alternatively, the relationship may be based on signalled parameters in a higher level syntax, e.g. a linear model using parameters alpha (&#x3b1;) and beta (&#x3b2;). These parameters could also be vector values and have different elements for the horizontal and vertical motion vector elements. These parameters could then be used to weigh and offset the decoded motion parameters of list j. For example, a motion vector mv<sub>i </sub>may be set as {right arrow over (mv<sub>l</sub>)}=&#x3b1;&#xd7;{right arrow over (mV<sub>J</sub>)}+&#x3b2;. Other non-linear mathematical models may also be employed.</p><p id="p-0083" num="0079">e. Mathematical relationships may be also defined by providing the relationships of a list with more than one other lists. In that case, the motion vector may be computed as</p><p id="p-0084" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mover accent="true">       <mrow>        <mi>m</mi>        <mo>&#x2062;</mo>        <msub>         <mi>v</mi>         <mi>&#x3b9;</mi>        </msub>       </mrow>       <mo>&#x2192;</mo>      </mover>      <mo>=</mo>      <mrow>       <mrow>        <mfrac>         <mn>1</mn>         <mi>i</mi>        </mfrac>        <mo>&#x2062;</mo>        <mrow>         <msubsup>          <mo>&#x2211;</mo>          <mrow>           <mi>j</mi>           <mo>=</mo>           <mn>0</mn>          </mrow>          <mrow>           <mi>j</mi>           <mo>&#x3c;</mo>           <mi>i</mi>          </mrow>         </msubsup>         <mrow>          <msub>           <mi>&#x3b1;</mi>           <mi>j</mi>          </msub>          <mo>&#xd7;</mo>          <mover accent="true">           <msub>            <mi>mv</mi>            <mi>J</mi>           </msub>           <mo>&#x2192;</mo>          </mover>         </mrow>        </mrow>       </mrow>       <mo>+</mo>       <msub>        <mi>&#x3b2;</mi>        <mi>j</mi>       </msub>      </mrow>     </mrow>     <mo>,</mo>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mrow>      <mi>Eq</mi>      <mo>.</mo>      <mtext>   </mtext>      <mn>2</mn>     </mrow>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0085" num="0000">assuming a linear model. Again, non linear models could also be used.</p><p id="p-0086" num="0080">f. Motion vectors for a list could be derived also through motion vector chaining. In particular, the syntax may represent motion parameters for a block at location (x,y) in frame i, using a reference j, if for another list a motion vector is provided pointing to another reference k, in which the co-located block, also at location (x,y) uses a motion vector that points at reference j. In such case, MV<sub>i,j </sub>(x,y)=MV<sub>i,k </sub>(x,y)+MV<sub>k,j </sub>(x,y). Motion vector chaining could be considered using multiple vectors, if these are available through other lists.</p><p id="p-0087" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>mv</i><sub>l,j</sub>(<i>x,y</i>)=&#x3a3;<sub>m=0</sub><sup>m&#x3c;n</sup>(&#x3b1;<sub>j</sub>&#xd7;(<i>mv</i><sub>l,k</sub><sub><sub2>m</sub2></sub>(<i>x,y</i>)+<i>mv</i><sub>k</sub><sub><sub2>m,J</sub2></sub>(<i>x,y</i>)))&#x2003;&#x2003;(Eq. 3)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0088" num="0000">Chaining could also include multiple references if the path to the target reference is not immediate, e.g. MV<sub>i,j </sub>(x,y)=MV<sub>i,k </sub>(x,y)+MV<sub>m,j </sub>(x,y)+MV<sub>m,j </sub>(x,y). In effect, a motion vector from a frame at time t may reference data in a frame at time t&#x2212;2 by building an aggregate motion vector from a first motion vector that references an intermediate frame at time t&#x2212;1 (e.g., a motion vector from time t to t&#x2212;1) and a second motion vector the references the destination frame at time t&#x2212;2 from the intermediate frame (e.g., a motion vector from time t&#x2212;1 to t&#x2212;2).</p><p id="p-0089" num="0081">g. Motion acceleration may also be considered to provide a basis to predict current motion vectors. For example, the predictor of a motion vector may be computed for frame i of a block at location (x,y) from frame i&#x2212;1, and assuming that all previous partitions used references frames that were only of a distance of 1, as follows:</p><p id="p-0090" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>MV</i><sub>(i,i-1)</sub>(<i>x,y</i>)=2*<i>MV</i><sub>(i-1,i-2)</sub>(<i>x,y</i>)&#x2212;<i>MV</i><sub>(i-2,i-3)</sub>(<i>x,y</i>)&#x2003;&#x2003;(Eq. 4)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0091" num="0000">Of course, if the frames have different distance relationships, the acceleration computation would account for such relationships.</p><p id="p-0092" num="0082">h. In another aspect, motion vector derivation may be based on partitions in other frames and their actual motion path. Such process involves higher complexity since now the projected area may not correspond to a block used for coding. Such strategy is used, for example, in the AVS video coding standards. Furthermore, the projected area may contain multiple subpartitions each having its own motion. In such cases, encoders and decoders either may combine all the vectors together or may use all the vectors as independent hypotheses and subpartition the block being coded based on these hypotheses. Combining the vectors could be done using a variety of methods, e.g. using simple averaging, using weighted averaging based on the number of pixels that have a certain motion, by making use of median filtering, selection of the mode candidate, etc. Moreover, chaining could persist across multiple pictures, with the number of pictures to be chained constrained by the codec at a higher level if desired. In such a case also, if there is no usable vector, such as the block being intra encoded, chaining could terminate.</p><p id="p-0093" num="0083">i. Non-linear combinations of prediction samples may be performed instead of or in addition to using straight averaging or weighted averaging. For example different predictors h<sub>i</sub>(x,y) may be combined non-linearly using an equation of the form:</p><p id="p-0094" num="0000"><maths id="MATH-US-00003" num="00003"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mi>S</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mrow>       <mi>x</mi>       <mo>,</mo>       <mi>y</mi>      </mrow>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mrow>      <mfrac>       <mn>1</mn>       <mi>N</mi>      </mfrac>      <mo>&#x2062;</mo>      <mrow>       <msubsup>        <mo>&#x2211;</mo>        <mrow>         <mi>i</mi>         <mo>=</mo>         <mn>0</mn>        </mrow>        <mrow>         <mi>i</mi>         <mo>&#x3c;</mo>         <mi>N</mi>        </mrow>       </msubsup>       <mrow>        <mo>(</mo>        <mrow>         <mrow>          <msub>           <mi>h</mi>           <mi>i</mi>          </msub>          <mo>(</mo>          <mrow>           <mi>x</mi>           <mo>,</mo>           <mi>y</mi>          </mrow>          <mo>)</mo>         </mrow>         <mo>&#xd7;</mo>         <mrow>          <mi>f</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <mrow>           <mrow>            <msub>             <mi>h</mi>             <mi>i</mi>            </msub>            <mo>(</mo>            <mrow>             <mi>x</mi>             <mo>,</mo>             <mi>y</mi>            </mrow>            <mo>)</mo>           </mrow>           <mo>-</mo>           <mrow>            <mi>ref</mi>            <mo>&#x2061;</mo>            <mo>(</mo>            <mrow>             <mi>x</mi>             <mo>,</mo>             <mi>y</mi>            </mrow>            <mo>)</mo>           </mrow>          </mrow>          <mo>)</mo>         </mrow>        </mrow>        <mo>)</mo>       </mrow>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mrow>      <mi>Eq</mi>      <mo>.</mo>      <mtext>   </mtext>      <mn>5</mn>     </mrow>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0095" num="0084">Here, ref(x,y) may represent an anchor predictor that is most correlated with the current signal and f is the non-linear function. The function could be for example a gaussian function or some other similar function. For example, the anchor predictor can be chosen from the most adjacent frame temporally, or the predictor associated with the smallest Qp. The anchor predictor could also be indicated at a higher syntax structure, e.g. the slice header.</p><p id="p-0096" num="0085">j. In another aspect, the different predictors for multihypothesis prediction can be weighted based also on location of the current sample and its distance to the blocks used for predicting the hypotheses motion vectors. In particular, for a block that is predicted using two hypotheses, where the first hypothesis used the motion information from the left block as its predictor and the second hypothesis used the top block as its predictor, the first hypothesis may be weighted to have higher impact on samples that are closer to the left edge of the block and the second hypothesis may be weighted to have higher impact on samples that are closer to the top edge of the block. In another example, if a block has predictors from above, from the left, from the above left, and from above right, as well as a temporal predictor, weights may be applied to each of these predictors. The temporal hypothesis may be assigned a single weight everywhere, whereas the hypothesis from the left may have relatively large weights on the left boundaries of the block with weight reductions at locations towards the right of the reconstructed block. Analogous weight distributions may be applied for the hypothesis at other locations (for example, the above block), where weights may be modulated based on directions of prediction.</p><p id="p-0097" num="0086">k. Lists with i&#x3e;1 could be restricted to use only one reference index. In this case, no signalling of the reference index for these lists would be necessary.</p><p id="p-0098" num="0087">l. Motion vectors for a particular list may be limited to values within a defined window around the (0,0) motion vector, that is known to the decoder. Such limitations may contribute to better data prefetch operations at a decoder for motion compensation using more than 2 hypotheses.</p><p id="p-0099" num="0088">m. Subpixel motion compensation may be limited for multihypothesis prediction. For example, multihypothesis prediction may be limited to only &#x215b;<sup>th</sup>, quarter, half pel, or integer precision. Integer prediction could also be further restricted to certain integer multiples, e.g. multiples of 2, 3, 4, etc, or in general powers of 2. Such restrictions could be imposed on all lists if more hypotheses than 2 are used, or only on certain lists, e.g. list i&#x3e;1, or list combinations, e.g. whenever list 2 is used all lists are restricted to half pel precision. If such precision is used, and if these methods are combined with implicit motion vector derivation from another list, appropriate quantization and clipping of the derived motion parameters may be performed.</p><p id="p-0100" num="0089">n. Limitations could also be imposed on the filters used for subpixel prediction, i.e. in the number of filter taps that will be used to generate those samples. For example, for uni- or bi-prediction, a coding syntax may permit 8 tap filters to be used, but for multihypothesis prediction with more than 2 references only 4 tap or bilinear filtering may be permitted for all lists. The filters could also be selected based on the list index. The limitations could be based on block size as well, i.e. for larger partitions longer filters may be used, but the size of the filters may be limited for smaller partitions.</p><p id="p-0101" num="0090">o. Limitations on the filters could also be imposed differently on luma vs chroma components.</p><p id="p-0102" num="0091">All of the above conditions could be implicitly designed in the codec, or could be signalled in higher syntax levels of the codec, such as sequence, picture parameter sets, or slice headers.</p><p id="p-0103" num="0092">Different ones of foregoing techniques may be applied in a coding syntax for prediction blocks of different size. For example, for larger partitions, the cost of signalling additional motion parameters is small. However, for smaller partitions that cost can be significant. Therefore, for larger partitions, e.g. 16&#xd7;16 and above, explicit motion signalling of lists i&#x3e;1 may be permitted, but, for partition sizes smaller than 16&#xd7;16, derivation may be implicit.</p><p id="p-0104" num="0093">The principles of the foregoing aspects may be used cooperatively with other types of coding techniques. For example, merge and skip modes can also be supported in such codecs, and the derivation of the appropriate motion parameters discussed above may be extended to these modes as well.</p><p id="p-0105" num="0000">Motion Compensated Prediction with Hierarchical Multihypothesis</p><p id="p-0106" num="0094">The foregoing techniques find application in coding systems where video data is coded in recursively-partitioned pixel blocks. For example, HEVC and some other coding systems partition a frame of input data first into a Coding Tree Unit (CTU), also called as the Largest Coding Unit (commonly, an &#x201c;LCU&#x201d;), of a predetermined size (say, 64&#xd7;64 pixels). Each CTU may be partitioned into increasingly smaller pixel blocks back on content contained therein. When the partitioning is completed, relationships of the CTUs and the coding units (&#x201c;CUs&#x201d;) contained within them may represent a hierarchical tree data structure. Moreover, prediction units (&#x201c;PUs&#x201d;) may be provided for the CUs at leaf nodes of the tree structure; the PUs may carry prediction information such as coding mode information, motion vectors, weighted prediction parameters, and the like.</p><p id="p-0107" num="0095">As discussed, aspects of the present disclosure extend multi-hypothesis prediction to provide for different coding hypotheses at different block sizes. For example, a block of size (2M)&#xd7;(2M) can be designated as a bi-predicted block but, for each M&#xd7;M subblock, different motion parameters can be signalled for each hypothesis/list. Furthermore, it is possible that one of the hypotheses/lists would be fixed within the entire block, whereas, for the second hypothesis, further subpartitioning may be performed. In such a scenario, an encoder can keep one of the hypotheses fixed for a larger area, while searching for the second hypothesis using smaller regions. Similarly, a decoder may use hypothesis information to determine whether it should &#x201c;prefetch&#x201d; the entire larger area in memory for generating the first hypothesis, whereas for the second hypothesis smaller areas could be brought into memory and then combined with the first hypothesis to generate the final prediction.</p><p id="p-0108" num="0096">Aspects of the present disclosure introduce a new prediction mode, called the &#x201c;hierarchical multihypothesis&#x201d; mode, which can be added in addition to the existing single list/unipredictive and bipredictive prediction modes available in existing codecs. For example, in HEVC, the parameter inter_pred_idc[x0][y0] may be expanded to indicate that additional subpartitioning is permitted for a block for any of the hypotheses and lists, i.e. List0, List1, or both. If such subpartitioning is permitted for only one of the hypotheses/lists, e.g. List0, then for the other list, e.g. List1, its associated parameters can be signalled immediately, e.g. within a prediction unit, whereas for the first hypothesis/list the signalling of said parameters would have to be done in combination with its partitioning.</p><p id="p-0109" num="0097">In a further aspect, subpartitions may include syntax elements to identify a null prediction which indicates that the additional hypothesis from the current list will not be considered for this subpartition. It may also be possible to signal syntax elements that will indicate the use of intra prediction instead of inter prediction in combination with or instead of other inter hypotheses for that subpartition.</p><p id="p-0110" num="0098">In HEVC, the hierarchical nature of the codec may induce overhead for signalling the prediction mode information, which ordinarily is provided in prediction units that are identified from leaf nodes of coding unit tress. Aspects of the present disclosure provide several techniques to reduce signalling overhead.</p><p id="p-0111" num="0099">In one aspect, the signalling of inter_pred_idc may be provided at higher levels of the coding_quadtree syntax than the prediction unit. The following syntax tables, demonstrate an example of how this could be done. In this particular example, a parameter named &#x201c;quadtree_mode_enable_flag&#x201d; may be provided at a sequence, picture, or slice level in a coding syntax that indicates whether the inter_pred_idc designations are provided at higher levels within the coding_quadtree syntax. If the quadtree_mode_enable_flag value is set to TRUE (1), for example, it may indicate that inter_pred_idc designations are so provided. The encoder may signal the inter prediction mode and the split flags in such higher level constructs, for example, in sequence, picture, or slice level itself, thus reducing the signalled overhead. In this particular example, intra prediction modes are not used within subpartitions, thus reducing overhead further.</p><p id="p-0112" num="0100">Table 5 provides an exemplary coding quadtree syntax provided in an HEVC-style coding application. The values split_cu_l0_flag[x0][y0], split_cu_l1_flag[x0][y0] and split_cu_flag[x0][y0] respectively indicate whether and how a CU may be further partitioned. The inter_pred_idc[x0][y0] may identify a prediction mode of all sub-partitions contained within the CU where it is identified.</p><p id="p-0113" num="0000"><tables id="TABLE-US-00005" num="00005"><table frame="none" colsep="0" rowsep="0" pgwide="1"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="301pt" align="center"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE 5</entry></row></thead><tbody valign="top"><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>Exemplary Coding Quadtree Syntax</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="offset" colwidth="266pt" align="left"/><colspec colname="1" colwidth="35pt" align="left"/><tbody valign="top"><row><entry/><entry>Descriptor</entry></row><row><entry/><entry namest="offset" nameend="1" align="center" rowsep="1"/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="266pt" align="left"/><colspec colname="2" colwidth="35pt" align="left"/><tbody valign="top"><row><entry>coding_quadtree( x0, y0, log2CbSize, cqtDepth ) {</entry><entry/></row><row><entry>&#x2003;if( x0 + ( 1 &#x3c;&#x3c; log2CbSize ) &#x3c;= pic_width_in_luma_samples &#x26;&#x26;</entry></row><row><entry>&#x2003;&#x2003;y0 + ( 1 &#x3c;&#x3c; log2CbSize ) &#x3c;= pic_height_in_luma_samples &#x26;&#x26;</entry></row><row><entry>&#x2003;&#x2003;log2CbSize &#x3e; MinCbLog2SizeY )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;if (quadtree_mode_enable_flag = = 1 &#x26;&#x26; slice_type = = B )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;enable_ctu_mode_flag</entry><entry>ae(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;else</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;enable_ctu_mode_flag = 0</entry></row><row><entry>&#x2003;&#x2003;&#x2003;if (enable_ctu_mode_flag == 1) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;inter_pred_idc[ x0 ][ y0 ]</entry><entry>ae(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;if ( inter_pred_idc[ x0 ][ y0 ] == PRED_HIER_BI) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;split_cu_l0_flag[ x0 ][ y0 ]</entry><entry>ae(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;split_cu_l1_flag[ x0 ][ y0 ]</entry><entry>ae(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;IsHierSplitMode = 1</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;else</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;split_cu_flag[ x0 ][ y0 ]</entry><entry>ae(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;&#x2003;else {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;split_cu_flag[ x0 ][ y0 ]</entry><entry>ae(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;if( cu_qp_delta_enabled_flag &#x26;&#x26; log2CbSize &#x3e;= Log2MinCuQpDeltaSize ) {</entry></row><row><entry>&#x2003;&#x2003;IsCuQpDeltaCoded = 0</entry></row><row><entry>&#x2003;&#x2003;CuQpDeltaVal = 0</entry></row><row><entry>&#x2003;}</entry></row><row><entry>&#x2003;if( cu_chroma_qp_offset_enabled_flag &#x26;&#x26;</entry></row><row><entry>&#x2003;log2CbSize &#x3e;= Log2MinCuChromaQpOffsetSize )</entry></row><row><entry>&#x2003;&#x2003;IsCuChromaQpOffsetCoded = 0</entry></row><row><entry>&#x2003;if (quadtree_mode_enable = = 1 &#x26;&#x26; slice_type = = B) {</entry></row><row><entry>&#x2003;&#x2003;if ( IsHierSplitMode == 1) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;quadtree_splitting( split_cu_l0_flag[ x0 ][ y0 ], PRED_L0,</entry></row><row><entry>&#x2003;&#x2003;&#x2003;x0, y0, log2CbSize, cqtDepth )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;quadtree_splitting( split_cu_l1_flag[ x0 ][ y0 ], PRED_L1,</entry></row><row><entry>&#x2003;&#x2003;&#x2003;x0, y0, log2CbSize, cqtDepth )</entry></row><row><entry>&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;else {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;quadtree_splitting( split_cu_flag[ x0 ][ y0 ], inter_pred_idc[ x0 ][ y0 ],</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;x0, y0, log2CbSize, cqtDepth )</entry></row><row><entry>&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;else {</entry></row><row><entry>&#x2003;&#x2003;if( split_cu_flag[ x0 ][ y0 ] ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;x1 = x0 + ( 1 &#x3c;&#x3c; (log2CbSize &#x2212; 1 ) )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;y1 = y0 + ( 1 &#x3c;&#x3c; (log2CbSize &#x2212; 1 ) )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;coding_quadtree( x0, y0, log2CbSize &#x2212; 1, cqtDepth + 1 )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;if( x1 &#x3c; pic_width_in_luma_samples )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;coding_quadtree( x1, y0, log2CbSize &#x2212; 1, cqtDepth + 1 )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;if( y1 &#x3c; pic_height_in_luma_samples )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;coding_quadtree( x0, y1, log2CbSize &#x2212; 1, cqtDepth + 1 )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;if( x1 &#x3c; pic_width_in_luma_samples &#x26;&#x26; y1 &#x3c; pic_height_in_luma_samples )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;coding_quadtree( x1, y1, log2CbSize &#x2212; 1, cqtDepth + 1 )</entry></row><row><entry>&#x2003;&#x2003;} else</entry></row><row><entry>&#x2003;&#x2003;&#x2003;coding_unit( x0, y0, log2CbSize )</entry></row><row><entry>&#x2003;}</entry></row><row><entry>}</entry></row><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0114" num="0101">Table 6 illustrates an exemplary quadtree splitting syntax, again, working from an HEVC-style coding syntax. This new syntax is utilized when the independent splitting mode for each list is enabled. Otherwise the original quadtree method, as is used in HEVC, is used. As indicated, coding_quadtree_list values may be derived from the inter_pred_idc value, which is signalled in the coding_quadtree syntax. The corresponding list is also passed into this syntax since this will be later used to determine which information, i.e. motion vectors from which corresponding list, will need to be signalled.</p><p id="p-0115" num="0000"><tables id="TABLE-US-00006" num="00006"><table frame="none" colsep="0" rowsep="0" pgwide="1"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="294pt" align="center"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE 6</entry></row></thead><tbody valign="top"><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>Quadtree Splitting Syntax</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="offset" colwidth="259pt" align="left"/><colspec colname="1" colwidth="35pt" align="left"/><tbody valign="top"><row><entry/><entry>Descriptor</entry></row><row><entry/><entry namest="offset" nameend="1" align="center" rowsep="1"/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="294pt" align="left"/><tbody valign="top"><row><entry>quadtree_splitting( split_cu_flag , current_list, x0, y0, log2CbSize, cqtDepth ) {</entry></row><row><entry>&#x2003;if( split_cu_flag) {</entry></row><row><entry>&#x2003;&#x2003;x1 = x0 + ( 1 &#x3c;&#x3c; ( log2CbSize &#x2212; 1 ) )</entry></row><row><entry>&#x2003;&#x2003;y1 = y0 + ( 1 &#x3c;&#x3c; ( log2CbSize &#x2212; 1 ) )</entry></row><row><entry>&#x2003;&#x2003;coding_quadtree_list( current_list, x0, y0, log2CbSize &#x2212; 1, cqtDepth + 1 )</entry></row><row><entry>&#x2003;&#x2003;if( x1 &#x3c; pic_width_in_luma_samples )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;coding_quadtree_list( current_list, x1, y0, log2CbSize &#x2212; 1, cqtDepth + 1 )</entry></row><row><entry>&#x2003;&#x2003;if( y1 &#x3c; pic_height_in_luma_samples )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;coding_quadtree_list( current_list, x0, y1, log2CbSize &#x2212; 1, cqtDepth + 1 )</entry></row><row><entry>&#x2003;&#x2003;if( x1 &#x3c; pic_width_in_luma_samples &#x26;&#x26; y1 &#x3c; pic_height_in_luma_samples )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;coding_quadtree_list( current_list, x1, y1, log2CbSize &#x2212; 1, cqtDepth + 1 )</entry></row><row><entry>&#x2003;} else</entry></row><row><entry>&#x2003;&#x2003;coding_unit_list( current_list, x0, y0, log2CbSize )</entry></row><row><entry>}</entry></row><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0116" num="0102">Table 7 illustrates coding quadtree list syntax, which shows a further use of the split_cu_flag to indicate whether the respective coding unit for the list current_list may be split further for multihypothesis coding. Here, the current_list value is passed from higher level syntactic elements.</p><p id="p-0117" num="0000"><tables id="TABLE-US-00007" num="00007"><table frame="none" colsep="0" rowsep="0" pgwide="1"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="280pt" align="center"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE 7</entry></row></thead><tbody valign="top"><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>Coding Quadtree List Syntax</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="offset" colwidth="245pt" align="left"/><colspec colname="1" colwidth="35pt" align="left"/><tbody valign="top"><row><entry/><entry>Descriptor</entry></row><row><entry/><entry namest="offset" nameend="1" align="center" rowsep="1"/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="245pt" align="left"/><colspec colname="2" colwidth="35pt" align="left"/><tbody valign="top"><row><entry>coding_quadtree_list( current_list, x0, y0, log2CbSize, cqtDepth ) {</entry><entry/></row><row><entry>&#x2003;if( x0 + ( 1 &#x3c;&#x3c; log2CbSize ) &#x3c;= pic_width_in_luma_samples &#x26;&#x26;</entry></row><row><entry>&#x2003;&#x2003;y0 + ( 1 &#x3c;&#x3c; log2CbSize ) &#x3c;= pic_height_in_luma_samples &#x26;&#x26;</entry></row><row><entry>&#x2003;&#x2003;log2CbSize &#x3e; MinCbLog2SizeY )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;split_cu_flag[ x0 ][ y0 ]</entry><entry>ae(v)</entry></row><row><entry>&#x2003;if( cu_qp_delta_enabled_flag &#x26;&#x26; log2CbSize &#x3e;= Log2MinCuQpDeltaSize ) {</entry></row><row><entry>&#x2003;&#x2003;IsCuQpDeltaCoded = 0</entry></row><row><entry>&#x2003;&#x2003;CuQpDeltaVal = 0</entry></row><row><entry>&#x2003;}</entry></row><row><entry>&#x2003;if( cu_chroma_qp_offset_enabled_flag &#x26;&#x26;</entry></row><row><entry>&#x2003;&#x2003;log2CbSize &#x3e;= Log2MinCuChromaQpOffsetSize )</entry></row><row><entry>&#x2003;&#x2003;IsCuChromaQpOffsetCoded = 0</entry></row><row><entry>&#x2003;quadtree_splitting( split_cu_flag[ x0 ][ y0 ], current_list,</entry></row><row><entry>x0, y0, log2CbSize, cqtDepth )</entry></row><row><entry>}</entry></row><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0118" num="0103">Table 8 illustrates an exemplary coding unit list syntax, which makes use of the current_list value as passed from higher level syntactic elements. Here, derivation of prediction_unit_list parameters may use the current_list value from higher level syntactic elements.</p><p id="p-0119" num="0000"><tables id="TABLE-US-00008" num="00008"><table frame="none" colsep="0" rowsep="0" pgwide="1"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="287pt" align="center"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE 8</entry></row></thead><tbody valign="top"><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>Coding Unit List Syntax</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="offset" colwidth="252pt" align="left"/><colspec colname="1" colwidth="35pt" align="left"/><tbody valign="top"><row><entry/><entry>Descriptor</entry></row><row><entry/><entry namest="offset" nameend="1" align="center" rowsep="1"/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="252pt" align="left"/><colspec colname="2" colwidth="35pt" align="left"/><tbody valign="top"><row><entry>coding_unit_list( current_list, x0, y0, log2CbSize ) {</entry><entry/></row><row><entry>&#x2003;if( transquant_bypass_enabled_flag )</entry></row><row><entry>&#x2003;&#x2003;cu_transquant_bypass_flag</entry><entry>ae(v)</entry></row><row><entry>&#x2003;cu_skip_flag[ x0 ][ y0 ]</entry><entry>ae(v)</entry></row><row><entry>&#x2003;nCbS = ( 1 &#x3c;&#x3c; log2CbSize )</entry></row><row><entry>&#x2003;if( cu_skip_flag[ x0 ][ y0 ] )</entry></row><row><entry>&#x2003;&#x2003;prediction_unit_list( current_list, x0, y0, nCbS, nCbS )</entry></row><row><entry>&#x2003;else {</entry></row><row><entry>&#x2003;&#x2003;part_mode</entry><entry>ae(v)</entry></row><row><entry>&#x2003;&#x2003;if( PartMode = = PART_2Nx2N )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;prediction_unit_list ( current_list,x0, y0, nCbS, nCbS )</entry></row><row><entry>&#x2003;&#x2003;else if( PartMode = = PART_2NxN ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;prediction_unit_list ( current_list,x0, y0, nCbS, nCbS / 2 )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;prediction_unit_list ( current_list,x0, y0 + ( nCbS / 2 ), nCbS, nCbS / 2 )</entry></row><row><entry>&#x2003;&#x2003;} else if( PartMode = = PART_Nx2N ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;prediction_unit_list ( current_list,x0, y0, nCbS / 2, nCbS )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;prediction_unit_list ( current_list,x0 + ( nCbS / 2 ), y0, nCbS / 2, nCbS )</entry></row><row><entry>&#x2003;&#x2003;} else if( PartMode = = PART_2NxnU ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;prediction_unit_list ( current_list,x0, y0, nCbS, nCbS / 4 )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;prediction_unit_list ( current_list,x0, y0 + ( nCbS / 4 ), nCbS, nCbS * 3 / 4 )</entry></row><row><entry>&#x2003;&#x2003;} else if( PartMode = = PART_2NxnD ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;prediction_unit_list ( current_list,x0, y0, nCbS, nCbS * 3 / 4 )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;prediction_unit_list ( current_list,x0, y0 + ( nCbS * 3 / 4 ), nCbS, nCbS / 4 )</entry></row><row><entry>&#x2003;&#x2003;} else if( PartMode = = PART nLx2N ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;prediction_unit_list ( current_list,x0, y0, nCbS / 4, nCbS )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;prediction_unit_list ( current_list,x0 + ( nCbS / 4 ), y0, nCbS *3/4, nCbS )</entry></row><row><entry>&#x2003;&#x2003;} else if( PartMode = = PART_nRx2N ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;prediction_unit_list ( current_list,x0, y0, nCbS * 3/4, nCbS )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;prediction_unit_list ( current_list,x0 + ( nCbS * 3 / 4 ), y0, nCbS / 4, nCbS )</entry></row><row><entry>&#x2003;&#x2003;} else {/* PART_NxN */</entry></row><row><entry>&#x2003;&#x2003;&#x2003;prediction_unit_list ( current_list,x0, y0, nCbS / 2, nCbS / 2 )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;prediction_unit_list ( current_list,x0 + ( nCbS / 2), y0, nCbS / 2, nCbS / 2 )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;prediction_unit_list ( current_list,x0, y0 + ( nCbS / 2 ), nCbS / 2, nCbS / 2 )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;prediction_unit_list</entry></row><row><entry>( current_list,x0 + ( nCbS / 2 ), y0 + ( nCbS / 2 ), nCbS / 2, nCbS / 2 )</entry></row><row><entry>&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;}</entry></row><row><entry>}</entry></row><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0120" num="0104">Table 9 illustrates an exemplary syntax for coding prediction units in an aspect of the present disclosure. In this aspect, values of ref_idx_l0, mvp_l0_flag, ref_idx_l1, mvp_l1_flag are signalled at the encoder and known to be decoded at the decoder for only the list current_list, which is passed to the prediction_unit_list process from higher level syntactic elements. This is different from legacy prediction units where the mode was indicated within the prediction unit, and which would indicate whether list0 only, list1 only, or biprediction shall be used and the related parameters will be signalled together within the same prediction unit.</p><p id="p-0121" num="0000"><tables id="TABLE-US-00009" num="00009"><table frame="none" colsep="0" rowsep="0" pgwide="1"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="259pt" align="center"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE 9</entry></row></thead><tbody valign="top"><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>Prediction Unit List Syntax</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="offset" colwidth="224pt" align="left"/><colspec colname="1" colwidth="35pt" align="left"/><tbody valign="top"><row><entry/><entry>Descriptor</entry></row><row><entry/><entry namest="offset" nameend="1" align="center" rowsep="1"/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="224pt" align="left"/><colspec colname="2" colwidth="35pt" align="left"/><tbody valign="top"><row><entry>prediction_unit_list( current_list, x0, y0, nPbW, nPbH ) {</entry><entry/></row><row><entry>&#x2003;if( cu_skip_flag[ x0 ][ y0 ] ) {</entry></row><row><entry>&#x2003;&#x2003;if( MaxNumMergeCand &#x3e; 1 )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;merge_idx[ x0 ][ y0 ]</entry><entry>ae(v)</entry></row><row><entry>&#x2003;} else { /* MODE_INTER */</entry></row><row><entry>&#x2003;&#x2003;merge_flag[ x0 ][ y0 ]</entry><entry>ae(v)</entry></row><row><entry>&#x2003;&#x2003;if( merge_flag[ x0 ][ y0 ] ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;if( MaxNumMergeCand &#x3e; 1 )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;merge_idx[ x0 ][ y0 ]</entry><entry>ae(v)</entry></row><row><entry>&#x2003;&#x2003;} else {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;if(current_list != PRED_L1 ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;if( num_ref_idx_l0_active_minus1 &#x3e; 0 )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;ref_idx_l0[ x0 ][ y0 ]</entry><entry>ae(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;mvd_coding( x0, y0, 0 )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;mvp_l0_flag[ x0 ][ y0 ]</entry><entry>ae(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;&#x2003;if(current_list != PRED_L0 ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;if( num_ref_idx_l1_active_minus1 &#x3e; 0 )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;ref_idx_l1[ x0 ][ y0 ]</entry><entry>ae(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;if( mvd_l1_zero_flag &#x26;&#x26; inter_pred_idc = = PRED_BI ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;MvdL1[ x0 ][ y0 ][ 0 ] = 0</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;MvdL1[ x0 ][ y0 ][ 1 ] = 0</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;} else</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;mvd_coding( x0, y0, 1 )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;mvp_l1_flag[ x0 ][ y0 ]</entry><entry>ae(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;}</entry></row><row><entry>}</entry></row><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0122" num="0105">Although not shown above, the foregoing principles may be extended to signalling of local illumination parameters, also. Moreover, the principles may be extended to other coding blocks such as transform coding, overlapped block motion compensation, and deblocking. Transform coding for example in HEVC can be performed independently from the specifications of the prediction units.</p><p id="p-0123" num="0106">In another aspect, overhead signalling may be limited even further by constraining subpartitioning in hierarchical multihypothesis prediction to one and only one of the lists. When a multiple hypothesis coding mode is selected, then, for one of the lists, the partitioning may be fixed. In such a case, no special indication flag would be needed, and a single set of motion parameters would be sent for one list, whereas, for the other list(s), the syntax may accommodate further subpartitioning and signalling of additional parameters. Such aspects may be built into coding rules for the syntax, in which case the constraint would be enforced always. Alternatively, the constraint may be activated dynamically by an encoder, in which case it would also be signalled at higher-level constructs within a coding syntax (e.g., the sequence, picture, or slice headers). Moreover, the depth subpartitioning of each list may be signalled at a similar level. Such depth could also be associated with the depth of the coding unit, in which case, further subdivision of a coding unit would not occur beyond a certain block size. Such constraints may have an impact on when an encoder sends split flag parameters in the stream. If for example, an encoder reaches the maximum splitting depth, no additional split flags need to be transmitted.</p><heading id="h-0006" level="2">Prediction List Signalling for Bipredictively and MHMC-Coded Pixel Blocks</heading><p id="p-0124" num="0107">In another aspect, coding syntax may include elements that identify prediction lists that are to be used for pixel blocks. Some coding protocols organize prediction references for bipredictively-coded pixel blocks into lists, commonly represented as &#x201c;List0&#x201d; and &#x201c;List1.&#x201d; and identify reference frames for prediction using indexes into such lists. Aspects of the present disclosure, however, introduce signalling at higher levels of a syntax (e.g., at the sequence, picture, slice header, coding tree unit level (or coding unit groups of a certain size in a codec such as HEVC, or macroblocks in a codec like MPEG-4 AVC)), of whether a given list prediction is present in a lower coding unit within the higher level. The signalling may be provided as an &#x201c;implicit prediction&#x201d; flag that, when activated for a given list, indicates that the list is to be used for coding of the pixel blocks referenced by the sequence, slice, etc.</p><p id="p-0125" num="0108">Table 10 provides an example of syntax that may indicate whether prediction lists are present for bipredictively-coded pixel blocks. In this example, the syntax is provided as part of signalling of a slice.</p><p id="p-0126" num="0000"><tables id="TABLE-US-00010" num="00010"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="center"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE 10</entry></row></thead><tbody valign="top"><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>Exemplary Slice Segment Syntax</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="offset" colwidth="168pt" align="left"/><colspec colname="1" colwidth="49pt" align="left"/><tbody valign="top"><row><entry/><entry>Descriptor</entry></row><row><entry/><entry namest="offset" nameend="1" align="center" rowsep="1"/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="offset" colwidth="14pt" align="left"/><colspec colname="1" colwidth="154pt" align="left"/><colspec colname="2" colwidth="49pt" align="left"/><tbody valign="top"><row><entry/><entry>slice_segment_header( ) {</entry><entry/></row><row><entry/><entry>&#x2003;&#x2003;&#x2003;&#x2022; &#x2022; &#x2022;</entry></row><row><entry/><entry>&#x2003;if (slice_type == B) {</entry></row><row><entry/><entry>&#x2003;&#x2003;list0_prediction_implicit_present_flag</entry><entry>u(1)</entry></row><row><entry/><entry>&#x2003;&#x2003;list1_prediction_implicit_present_flag</entry><entry>u(1)</entry></row><row><entry/><entry>&#x2003;}</entry></row><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0127" num="0109">In this example, when either of list0_prediction_implicit_present_flag or list1_prediction_implicit_present_flag are set, it indicates that the respective list (e.g., List0, List 1) data is always present for coding of the pixel blocks contained within the slice. Again, the syntax may be provided for other syntactic elements in coded video data, such as a sequence, a picture, a coding tree unit level or other coding unit groups. Table 12 below also illustrates application of the implicit prediction flag syntax to an exemplary HEVC-style slice syntactic element.</p><p id="p-0128" num="0110">In this manner, for those prediction lists for which the implicit prediction flag is set to TRUE, it is no longer required to provide prediction indications in the data of the coded pixel blocks (for example, a prediction unit) themselves. These flags, when indicated, imply that always that prediction list would be used. For example, if list0_prediction_implicit_present_flag is set, then an inter-predicted block in that slice (excluding Intra, SKIP, and MERGE predicted blocks), would be either a unipredicted block using List0 or a bipredicted block using List0 and another prediction reference. An inter-predicted block could not be a unipredicted block using List1 only. Similarly, if list1_prediction_implicit_present_flag is set, then a block in that slice would be either a unipredicted block using List1 or a bipredicted block using List1 and another prediction reference. If both flags are set TRUE, then the corresponding blocks can only use biprediction using both List0 and List 1.</p><p id="p-0129" num="0111">The principles of the implicit prediction flags may be extended to MHMC coding using a higher number N of hypotheses (N&#x3e;2). In such a case, implicit prediction flags may be provided for the list corresponding to each hypothesis. A list_prediction_implicit_present_flag[i] is shown, for example, in Table 1. When a list_prediction_implicit_present_flag[i] is set for a hypothesis i, it signifies that the associated prediction list (e.g., a List<sub>i</sub>) will be used for coding of pixel blocks contained within the respective syntactic elements (e.g., the sequence, picture, slice, etc.).</p><p id="p-0130" num="0112">It is expected that, by placing the implicit prediction flags in syntactic elements of coded video data that are at higher levels than the coding data of individual pixel blocks (for example, individual prediction units), the bitrate cost of signalling prediction types will be lowered.</p><p id="p-0131" num="0113">Table 11, for example, indicates how syntax of an HEVC-style prediction unit might be simplified using the implicit prediction flags:</p><p id="p-0132" num="0000"><tables id="TABLE-US-00011" num="00011"><table frame="none" colsep="0" rowsep="0" pgwide="1"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="301pt" align="center"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE 11</entry></row></thead><tbody valign="top"><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>Exemplary HEVC-style Prediction Unit Syntax Modified</entry></row><row><entry>To Accommodate Implicit Prediction Flags</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="offset" colwidth="266pt" align="left"/><colspec colname="1" colwidth="35pt" align="left"/><tbody valign="top"><row><entry/><entry>Descriptor</entry></row><row><entry/><entry namest="offset" nameend="1" align="center" rowsep="1"/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="266pt" align="left"/><colspec colname="2" colwidth="35pt" align="left"/><tbody valign="top"><row><entry>prediction_unit( x0, y0, nPbW, nPbH ) {</entry><entry/></row><row><entry>&#x2003;if( cu_skip_flag[ x0 ][ y0 ] ) {</entry></row><row><entry>&#x2003;&#x2003;if( MaxNumMergeCand &#x3e; 1 )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;merge_idx[ x0 ][ y0 ]</entry><entry>ae(v)</entry></row><row><entry>&#x2003;} else { /* MODE_INTER */</entry></row><row><entry>&#x2003;&#x2003;merge_flag[ x0 ][ y0 ]</entry><entry>ae(v)</entry></row><row><entry>&#x2003;&#x2003;if( merge_flag[ x0 ][ y0 ] ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;if( MaxNumMergeCand &#x3e; 1 )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;merge_idx[ x0 ][ y0 ]</entry><entry>ae(v)</entry></row><row><entry>&#x2003;&#x2003;} else {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;if( slice_type = = B ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;if( list0_prediction_implicit_present_flag == 0)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;list0_pred_idc_flag[ x0 ][ y0 ]</entry><entry>ae(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;else</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;list0_pred_idc_flag[ x0 ][ y0 ] = 1</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;if( Iist1_prediction_implicit_present_flag == 0 &#x26;&#x26; list0_pred_idc_flag == 1)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;/* if list0 is not to be used, it implicitly signals that list1 has to be used and thus</entry></row><row><entry>avoid signalling */</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;list1_pred_idc_flag[ x0 ][ y0 ]</entry><entry>ae(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;else</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;list1_pred_idc_flag[ x0 ][ y0 ] = 1</entry></row><row><entry>&#x2003;&#x2003;&#x2003;} else { /* slice_type == P */</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;list0_pred_idc_flag[ x0 ][ y0 ] = 1</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;list1_pred_idc_flag[ x0 ][ y0 ] = 0</entry></row><row><entry>&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;&#x2003;if( list0_pred_idc_flag[ x0 ][ y0 ] == 1 ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;if( num_ref_idx_l0_active_minus1 &#x3e; 0 )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;ref_idx_l0[ x0 ][ y0 ]</entry><entry>ae(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;mvd_coding(x0, y0, 0 )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;mvp_l0_flag[ x0 ][ y0 ]</entry><entry>ae(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;&#x2003;if( list1_pred_idc_flag[ x0 ][ y0 ] == 1 ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;if( num_ref_idx_l1_active_minus1 &#x3e; 0 )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;ref_idx_l1[ x0 ][ y0 ]</entry><entry>ae(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;if( mvd_l1_zero_flag &#x26;&#x26; list0_pred_idc_flag[ x0 ][ y0 ] = = 1 ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;MvdL1[ x0 ][ y0 ][ 0 ] = 0</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;MvdL1[ x0 ][ y0 ][ 1 ] = 0</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;} else</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;mvd_coding( x0, y0, 1 )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;mvp_l1_flag[ x0 ][ y0 ]</entry><entry>ae(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;}</entry></row><row><entry>}</entry></row><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup></table></tables><br/>As shown above, when the list0_prediction_implicit_present_flag is set to TRUE, then prediction unit syntax will not include the flag list0_pred_idc_flag[x0][y0]. Ordinarily, the list0_pred_idc_flag[x0] [y0] specifies whether list0 used for the current prediction unit. In this case this is already known from a higher layer. Similarly, when the list1_prediction_implicit_present_flag is set to TRUE, then prediction unit syntax will not include the flag list1_pred_idc_flag[x0][y0]. As noted in Table 11, when list0 is not to be used (list0_pred_idc_flag&#x2260;1), it signals implicitly that list1 has to be used and thus signalling of list1_pred_idc_flag[x0][y0] can be avoided in such a case.</p><p id="p-0133" num="0114">Table 12 illustrates the syntax of Table 1 applied to an exemplary slice segment header, which is derived from the HEVC protocol:</p><p id="p-0134" num="0000"><tables id="TABLE-US-00012" num="00012"><table frame="none" colsep="0" rowsep="0" pgwide="1"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="287pt" align="center"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE 12</entry></row></thead><tbody valign="top"><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>Slice Segment Header To Support Multihypothesis Prediction</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="offset" colwidth="252pt" align="left"/><colspec colname="1" colwidth="35pt" align="left"/><tbody valign="top"><row><entry/><entry>Descriptor</entry></row><row><entry/><entry namest="offset" nameend="1" align="center" rowsep="1"/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="252pt" align="left"/><colspec colname="2" colwidth="35pt" align="left"/><tbody valign="top"><row><entry>slice_segment_header( ) {</entry><entry/></row><row><entry>&#x2003;first_slice_segment_in_pic_flag</entry><entry>u(1)</entry></row><row><entry>&#x2003;if( nal_unit_type &#x3e;= BLA_W_LP &#x26;&#x26; nal_unit_type &#x3c;= RSV_IRAP_VCL23 )</entry></row><row><entry>&#x2003;&#x2003;no_output_of_prior_pics_flag</entry><entry>u(1)</entry></row><row><entry>&#x2003;slice_pic_parameter_set_id</entry><entry>ue(v)</entry></row><row><entry>&#x2003;if( !first_slice_segment_in_pic_flag ) {</entry></row><row><entry>&#x2003;&#x2003;if( dependent_slice_segments_enabled_flag )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;dependent_slice_segment_flag</entry><entry>u(1)</entry></row><row><entry>&#x2003;&#x2003;slice_segment_address</entry><entry>u(v)</entry></row><row><entry>&#x2003;}</entry></row><row><entry>&#x2003;if( !dependent_slice_segment_flag ) {</entry></row><row><entry>&#x2003;&#x2003;for( i = 0; i &#x3c; num_extra_slice_header_bits; i++ )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;slice_reserved_flag[ i ]</entry><entry>u(1)</entry></row><row><entry>&#x2003;&#x2003;slice_type</entry><entry>ue(v)</entry></row><row><entry>&#x2003;&#x2003;if( slice_type == MH ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;number_of_lists_minus2</entry><entry>ue(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;for( i = 0; i &#x3c; number_of_lists_minus2 + 2; i++ ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;list_prediction_implicit_present_flag[ i ]</entry><entry>u(1)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;log2_min_luma_hypothesis_block_size_minus2 [ i ]</entry><entry>ue(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;log2_diff_max_min_luma_hypothesis_block_size [ i ]</entry><entry>ue(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;else { /* slice_type == P */</entry></row><row><entry>&#x2003;&#x2003;&#x2003;number_of_lists_minus2 = &#x2212;1</entry></row><row><entry>&#x2003;&#x2003;&#x2003;list_prediction_implicit_present_flag[ 0 ] = 1</entry></row><row><entry>&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;if( output_flag_present_flag )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;pic_output_flag</entry><entry>u(1)</entry></row><row><entry>&#x2003;&#x2003;if( separate_colour_plane_flag = = 1 )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;colour_plane_id</entry><entry>u(2)</entry></row><row><entry>&#x2003;&#x2003;if( nal_unit_type != IDR_W_RADL &#x26;&#x26; nal_unit_type != IDR_N_LP ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;slice_pic_order_cnt_lsb</entry><entry>u(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;short_term_ref_pic_set_sps_flag</entry><entry>u(1)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;if( !short_term_ref_pic_set_sps_flag )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;st_ref_pic_set( num_short_term_ref_pic_sets )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;else if( num_short_term_ref_pic_sets &#x3e; 1 )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;short_term_ref_pic_set_idx</entry><entry>u(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;if( long_term_ref_pics_present_flag ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;if( num_long_term_ref_pics_sps &#x3e; 0 )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;num_long_term_sps</entry><entry>ue(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;num_long_term_pics</entry><entry>ue(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;for( i = 0; i &#x3c; num_long_term_sps + num_long_term_pics; i++ ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;if( i &#x3c; num_long_term_sps ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;if( num_long_term_ref_pics_sps &#x3e; 1 )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;lt_idx_sps[ i ]</entry><entry>u(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;} else {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;poc_lsb_lt[ i ]</entry><entry>u(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;used_by_curr_pic_lt_flag[ i ]</entry><entry>u(1)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;delta_poc_msb_present_flag[ i ]</entry><entry>u(1)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;if( delta_poc_msb_present_flag[ i ] )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;delta_poc_msb_cycle_lt[ i ]</entry><entry>ue(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;&#x2003;if( sps_temporal_mvp_enabled_flag )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;slice_temporal_mvp_enabled_flag</entry><entry>u(1)</entry></row><row><entry>&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;if( sample_adaptive_offset_enabled_flag ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;slice_sao_luma_flag</entry><entry>u(1)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;if( ChromaArrayType != 0 )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;slice_sao_chroma_flag</entry><entry>u(1)</entry></row><row><entry>&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;if( slice_type = = P | | slice_type = = MH ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;num_ref_idx_active_override_flag</entry><entry>u(1)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;if( num_ref_idx_active_override_flag ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;for( i = 0; i &#x3c; number_of_lists_minus2 + 2; i++ )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;num_ref_idx_ active_minus1_list[ i ]</entry><entry>ue(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;&#x2003;if( lists_modification_present_flag &#x26;&#x26; NumPicTotalCurr &#x3e; 1 )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;ref_pic_lists_modification( )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;mvd_list_zero_flag[ 0 ] = 0</entry></row><row><entry>&#x2003;&#x2003;&#x2003;for( i = 1; i &#x3c; number_of_lists_minus2 + 2; i++ )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;mvd_list_zero_flag[ i ]</entry><entry>u(1)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;if( cabac_init_present_flag )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;cabac_init_flag</entry><entry>u(1)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;if( slice_temporal_mvp_enabled_flag ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;if( slice_type = = MH )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;collocated_from_list</entry><entry>ue(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;else</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;collocated_from_list = 0</entry><entry>ue(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;if( (num_ref_idx_active_minus1_list[ collocated_from_list ] &#x3e; 0 )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;collocated_ref_idx</entry><entry>ue(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;&#x2003;if( ( weighted_pred_flag &#x26;&#x26; slice_type = = P) | |</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2002;( weighted_mh_pred_flag &#x26;&#x26; slice_type = = MH ) )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;pred_weight_table( )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;five_minus_max_num_merge_cand</entry><entry>ue(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;if( motion_vector_resolution_control_idc = = 2 )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;use_integer_mv_flag</entry><entry>u(1)</entry></row><row><entry>&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;slice_qp_delta</entry><entry>se(v)</entry></row><row><entry>&#x2003;&#x2003;if( pps_slice_chroma_qp_offsets_present_flag ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;slice_cb_qp_offset</entry><entry>se(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;slice_cr_qp_offset</entry><entry>se(v)</entry></row><row><entry>&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;if( pps_slice_act_qp_offsets_present_flag ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;slice_act_y_qp_offset</entry><entry>se(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;slice_act_cb_qp_offset</entry><entry>se(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;slice_act_cr_qp_offset</entry><entry>se(v)</entry></row><row><entry>&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;if( chroma_qp_offset_list_enabled_flag )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;cu_chroma_qp_offset_enabled_flag</entry><entry>u(1)</entry></row><row><entry>&#x2003;&#x2003;if( deblocking_filter_override_enabled_flag )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;deblocking_filter_override_flag</entry><entry>u(1)</entry></row><row><entry>&#x2003;&#x2003;if( deblocking_filter_override_flag ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;slice_deblocking_filter_disabled_flag</entry><entry>u(1)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;if( !slice_deblocking_filter_disabled_flag ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;slice_beta_offset_div2</entry><entry>se(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;slice_tc_offset_div2</entry><entry>se(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;if( pps_loop_filter_across_slices_enabled_flag &#x26;&#x26;</entry></row><row><entry>&#x2003;&#x2003;&#x2003;( slice_sao_luma_flag | | slice_sao_chroma_flag | |</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;!slice_deblocking_filter_disabled_flag ) )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;slice_loop_filter_across_slices_enabled_flag</entry><entry>u(1)</entry></row><row><entry>&#x2003;}</entry></row><row><entry>&#x2003;if( tiles_enabled_flag | | entropy_coding_sync_enabled_flag ) {</entry></row><row><entry>&#x2003;&#x2003;num_entry_point_offsets</entry><entry>ue(v)</entry></row><row><entry>&#x2003;&#x2003;if( num_entry_point_offsets &#x3e; 0 ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;offset_len_minus1</entry><entry>ue(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;for( i = 0; i &#x3c; num_entry_point_offsets; i++ )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;entry_point_offset_minus1 [ i ]</entry><entry>u(v)</entry></row><row><entry>&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;}</entry></row><row><entry>&#x2003;if( slice_segment_header_extension_present_flag ) {</entry></row><row><entry>&#x2003;&#x2003;slice_segment_header_extension_length</entry><entry>ue(v)</entry></row><row><entry>&#x2003;&#x2003;for( i = 0; i &#x3c; slice_segment_header_extension_length; i++)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;slice_segment_header_extension_data_byte[ i ]</entry><entry>u(8)</entry></row><row><entry>&#x2003;}</entry></row><row><entry>&#x2003;byte_alignment( )</entry></row><row><entry>}</entry></row><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0135" num="0115">In other aspects, syntax may be developed to identify non-square prediction block sizes, for example, with parameters to define width and height of the minimum and maximum prediction block sizes.</p><p id="p-0136" num="0116">The slice header syntax of Table 12 may lead to a syntax for a prediction unit that can handle multihypothesis slices, as shown in Table 13.</p><p id="p-0137" num="0000"><tables id="TABLE-US-00013" num="00013"><table frame="none" colsep="0" rowsep="0" pgwide="1"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="287pt" align="center"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE 13</entry></row></thead><tbody valign="top"><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>Prediction Unit Syntax Of HEVC Modified Using The Proposed Method</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="offset" colwidth="252pt" align="left"/><colspec colname="1" colwidth="35pt" align="left"/><tbody valign="top"><row><entry/><entry>Descriptor</entry></row><row><entry/><entry namest="offset" nameend="1" align="center" rowsep="1"/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="252pt" align="left"/><colspec colname="2" colwidth="35pt" align="left"/><tbody valign="top"><row><entry>prediction_unit( x0, y0, nPbW, nPbH ) {</entry><entry/></row><row><entry>&#x2003;if( cu_skip_flag[ x0 ][ y0 ] ) {</entry></row><row><entry>&#x2003;&#x2003;if( MaxNumMergeCand &#x3e; 1 )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;merge_idx[ x0 ][ y0 ]</entry><entry>ae(v)</entry></row><row><entry>&#x2003;} else { /* MODE_INTER */</entry></row><row><entry>&#x2003;&#x2003;merge_flag[ x0 ][ y0 ]</entry><entry>ae(v)</entry></row><row><entry>&#x2003;&#x2003;if( merge_flag[ x0 ][ y0 ] ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;if( MaxNumMergeCand &#x3e; 1 )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;merge_idx[ x0 ][ y0 ]</entry><entry>ae(v)</entry></row><row><entry>&#x2003;&#x2003;} else {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;set_lists = 0</entry></row><row><entry>&#x2003;&#x2003;&#x2003;for( i = 0; i &#x3c; number_of_lists_minus2 + 2; i++ ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;if( !list_prediction_implicit_present_flag [ i ] &#x26;&#x26; ( ( i &#x3c;</entry></row><row><entry>number_of_lists_minus2 + 1) | | set_lists &#x3e; 0)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;list_pred_idc_flag[ i ][ x0 ][ y0 ]</entry><entry>ae(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;else</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;list_pred_idc_flag[ i ] [ x0 ][ y0 ] = 1</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;set_lists += list_pred_idc_flag[ i ] [ x0 ][ y0 ]</entry></row><row><entry>&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;&#x2003;for( i = 0; i &#x3c; number_of_lists_minus2 + 2; i++ ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;if( list_pred_idc_flag[ i ] [ x0 ][ y0 ] ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;if( num_ref_idx_ active_minus1_list[ i ] &#x3e; 0 )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;ref_idx_list[ i ][ x0 ][ y0 ]</entry><entry>ae(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;if( mvd_list_zero_flag[ i ] &#x26;&#x26; list_pred_idc_flag[ 0 ][ x0 ][ y0 ] ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;MvdList[ i ][ x0 ][ y0 ][ 0 ] = 0</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;MvdList[ i ] [ x0 ][ y0 ][ 1 ] = 0</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;} else</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;mvd_coding( x0, y0, i )</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;mvp_list_flag[ i ][ x0 ][ y0 ]</entry><entry>ae(v)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;}</entry></row><row><entry>}</entry></row><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup></table></tables><br/>Here, again, when the list_prediction_implicit_present_flag [i] of a list i is set to TRUE, then prediction unit syntax will not include the flag list_pred_idc_flag[i][x0][y0].</p><p id="p-0138" num="0117">Other aspects can achieve further conservation of signalling overhead. For example, rather than using the control:</p><p id="p-0139" num="0000"><tables id="TABLE-US-00014" num="00014"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="left"/><thead><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry>if( mvd_list_zero_flag[ i ] &#x26;&#x26; list_pred_idc_flag[ 0 ][ x0 ][ y0 ] ) {</entry></row><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row></tbody></tgroup></table></tables><br/>a coding syntax may employ an alternate control, as follows:</p><p id="p-0140" num="0000"><tables id="TABLE-US-00015" num="00015"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="offset" colwidth="56pt" align="left"/><colspec colname="1" colwidth="161pt" align="left"/><thead><row><entry/><entry namest="offset" nameend="1" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/><entry>if( mvd_list_zero_flag[ i ] ) {</entry></row><row><entry/><entry namest="offset" nameend="1" align="center" rowsep="1"/></row></tbody></tgroup></table></tables><br/>In this aspect, all motion vector differences in list i would be forced to zero, which conserves bitrate that otherwise would be allocated to coding of such elements. However, of course, the motion vector field for this list is considerably constrained.</p><p id="p-0141" num="0118">In another aspect, coding syntax rules may be to set MvdList to zero only when a block is predicted by more than one lists, regardless of which lists are used. For example, the following condition may be applied:</p><p id="p-0142" num="0000"><tables id="TABLE-US-00016" num="00016"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="offset" colwidth="35pt" align="left"/><colspec colname="1" colwidth="182pt" align="left"/><thead><row><entry/><entry namest="offset" nameend="1" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/><entry>if( mvd_list_zero_flag[ i ] &#x26;&#x26; set_lists &#x3e; 1 ) {</entry></row><row><entry/><entry namest="offset" nameend="1" align="center" rowsep="1"/></row></tbody></tgroup></table></tables><br/>This technique increases the flexibility of signalling list prediction for bipredicted and multihypothesis predicted partitions between an encoder and a decoder.</p><p id="p-0143" num="0119">In a further aspect, signalling may be provided at a lower level than a slice unit, that identifies which lists are to be used by also restricting the reference indices of any list[i] to one. Doing so may save bits for uni-prediction or multihypothesis prediction. Such signalling could be applied, for example at fixed CTU intervals. For example, signalling may be provided for N CTUs with N potentially specified at a higher level (such as the sequence parameter sets, picture parameter sets, or a slice, at a CTU level or even lower), but could also be signalled in a new coding unit, e.g. a CTU Grouping unit, that can have an arbitrary number of CTUs, e.g. M, associated with it. Such number could be signalled within the CTU Grouping unit, but could also be omitted and the CTU Grouping unit could contain appropriate termination syntax (Start and end codes) that allows one to determine how many CTUs are contained within.</p><p id="p-0144" num="0120">In such an aspect, coding syntax may provide additional information within a CTU grouping, such as follows:</p><p id="p-0145" num="0121">a. Indications that a particular list index is enabled or disabled for all CTUs in the CTU grouping unit. An encoder also may enable or disable list indices for uni-prediction or multihypothesis prediction if desired.</p><p id="p-0146" num="0122">b. For each enabled list index, indications of whether all reference indices or only one will be used. An encoder may provide such indications independently or jointly for uni-prediction or multihypothesis prediction (i.e. for uni-prediction all references of a particular list could be used and only one for multi-hypothesis prediction if both modes are enabled).</p><p id="p-0147" num="0123">c. If only one reference index is to be used, explicitly selections of which reference index that would be. In this case, if an encoder provides such a selection in a CTU grouping unit, the encoder need not signal the reference index for the blocks contained within the CTU grouping unit.</p><heading id="h-0007" level="2">Motion Vector Prediction Techniques for Multi-Hypothesis Motion Compensation</heading><p id="p-0148" num="0124">Other aspects of the present disclosure provide techniques for motion vector prediction both for bipredictive multihypothesis motion compensation (e.g., up to 2 hypotheses) and for multi-hypotheses prediction (N&#x3e;2).</p><p id="p-0149" num="0125">According to such techniques, a coder may select to use, as predictors, one or more motion vectors of the current block from a list or lists that have already been signalled in the bit stream. For example, assume that, by the time list1 motion vectors are to be coded, list0 motion vector parameters have already been signalled for the current block. In this example, the list1 motion vectors for the same block may be predicted directly from information of the list0 coding, instead of the surrounding partitions. That is, an encoder may compute a predictor vector as</p><p id="p-0150" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><img id="CUSTOM-CHARACTER-00001" he="2.79mm" wi="10.58mm" file="US20230007272A1-20230105-P00001.TIF" alt="custom-character" img-content="character" img-format="tif"/>=&#x3b1;*<img id="CUSTOM-CHARACTER-00002" he="3.22mm" wi="11.68mm" file="US20230007272A1-20230105-P00002.TIF" alt="custom-character" img-content="character" img-format="tif"/>+<img id="CUSTOM-CHARACTER-00003" he="3.22mm" wi="1.78mm" file="US20230007272A1-20230105-P00003.TIF" alt="custom-character" img-content="character" img-format="tif"/>where&#x2003;&#x2003;(Eq. 6)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0151" num="0000">&#x3b1; is a scaling factor that relates to the distances of the references in L0 and L1 compared to the current picture, and {right arrow over (&#x3b2;)} is a constant that could be sent at a higher level syntax structure (e.g. the sequence parameter sets, picture parameter sets, or the slice header). The value of &#x3b1; for example, can be computed using the picture order count (poc) information associated with each picture,</p><p id="p-0152" num="0000"><maths id="MATH-US-00004" num="00004"><math overflow="scroll"> <mrow>  <mrow>   <mo>(</mo>   <mrow>    <mrow>     <mi>e</mi>     <mo>.</mo>     <mi>g</mi>     <mo>.</mo>    </mrow>    <mo>,</mo>    <mtext>   </mtext>    <mrow>     <mrow>      <mi>as</mi>      <mo>&#x2062;</mo>      <mtext>   </mtext>      <mi>a</mi>     </mrow>     <mo>=</mo>     <mfrac>      <mrow>       <mrow>        <mi>p</mi>        <mo>&#x2062;</mo>        <mi>o</mi>        <mo>&#x2062;</mo>        <mrow>         <mrow>          <msub>           <mi>c</mi>           <mrow>            <mi>r</mi>            <mo>&#x2062;</mo>            <mi>e</mi>            <mo>&#x2062;</mo>            <mi>f</mi>           </mrow>          </msub>          <mo>[</mo>          <mrow>           <mi>l</mi>           <mo>&#x2062;</mo>           <mi>ist</mi>           <mo>&#x2062;</mo>           <mn>1</mn>          </mrow>          <mo>]</mo>         </mrow>         <mo>[</mo>         <mrow>          <mi>refidxl</mi>          <mo>&#x2062;</mo>          <mn>1</mn>         </mrow>         <mo>]</mo>        </mrow>       </mrow>       <mo>-</mo>       <mrow>        <mi>p</mi>        <mo>&#x2062;</mo>        <mi>o</mi>        <mo>&#x2062;</mo>        <msub>         <mi>c</mi>         <mrow>          <mi>c</mi>          <mo>&#x2062;</mo>          <mi>u</mi>          <mo>&#x2062;</mo>          <mi>r</mi>         </mrow>        </msub>       </mrow>      </mrow>      <mrow>       <mrow>        <mi>p</mi>        <mo>&#x2062;</mo>        <mi>o</mi>        <mo>&#x2062;</mo>        <mrow>         <mrow>          <msub>           <mi>c</mi>           <mrow>            <mi>r</mi>            <mo>&#x2062;</mo>            <mi>e</mi>            <mo>&#x2062;</mo>            <mi>f</mi>           </mrow>          </msub>          <mo>[</mo>          <mrow>           <mi>l</mi>           <mo>&#x2062;</mo>           <mi>ist</mi>           <mo>&#x2062;</mo>           <mn>0</mn>          </mrow>          <mo>]</mo>         </mrow>         <mo>[</mo>         <mrow>          <mi>refidxl</mi>          <mo>&#x2062;</mo>          <mn>0</mn>         </mrow>         <mo>]</mo>        </mrow>       </mrow>       <mo>-</mo>       <mrow>        <mi>p</mi>        <mo>&#x2062;</mo>        <mi>o</mi>        <mo>&#x2062;</mo>        <msub>         <mi>c</mi>         <mrow>          <mi>c</mi>          <mo>&#x2062;</mo>          <mi>u</mi>          <mo>&#x2062;</mo>          <mi>r</mi>         </mrow>        </msub>       </mrow>      </mrow>     </mfrac>    </mrow>   </mrow>   <mo>)</mo>  </mrow>  <mo>.</mo> </mrow></math></maths></p><p id="p-0153" num="0000">The use of this predictor also may be signalled implicitly, i.e. enabled always for a partition. Further, an encoder may signal which predictor should be used from a possible list of motion vector predictor candidates using a parameter such as a flag. If, for example, this parameter or flag is set to 0, then the collocated vector in the same block from the other list would be used, otherwise another predictor, e.g. a predictor based on a similar derivation as in HEVC, or a median, or averaging candidate of the spatial or temporal neighbourhood motion vectors could be used.</p><p id="p-0154" num="0126">These principles may be extended to hypotheses coming from lists with index higher than 1. In one aspect, prediction could come from list0, if the prediction is always available, or from the lowest index available list used for prediction. In another aspect, signalling may be provided to identify the list that is to be used for predicting the parameters of the current list. Such signalling could be done globally (e.g. at a higher syntax element such as the sequence or picture parameter sets, or the slice header), where an encoder may signal how vectors of a particular list, when used in multihypothesis prediction, would be predicted, or done at the block level using a parameter similar to conventional mvp_l&#xd7;_flag parameters.</p><p id="p-0155" num="0127">In another aspect, an encoder may combine the parameters mvp_l&#xd7;_flags into a single parameter that signals predictors jointly for all lists. This aspect may reduce signalling overhead as compared to cases in which signalling is provided independently for each hypothesis. That is, instead of signalling that L0 and L1 would use the first predictor using the value of 0 for mvp_l0_flag and mvp_l1_flag independently, in this new parameter mvp_selector, if its value is 0 both lists are selected from the same first predictor. If the value is 1, then both are selected from the second predictor, if that is available, whereas if its value is equal to 2, then for L0 it indicates that an encoder used its first predictor and, for L1, its second predictor and so on. Such correspondence between the value of mvp_selector and which predictor to use for each list could be pre-specified as a syntax coding rule, or it could also be signalled inside the bit stream at a higher syntax element (e.g. in the sequence or picture parameter sets, or the slice header). The mvp_selector could be used with more than 2 lists.</p><p id="p-0156" num="0128">In another aspect, an encoder may entropy code the element mvp_selector using an entropy coding scheme such as UVLC (exp-golomb coding) or Context Adaptive Binary Arithmetic Coding (CABAC). In the case of CABAC, occurrence statistics may be used to update probability statistics and thus how the value is encoded using an adaptive arithmetic encoding engine. With respect to use of CABAC, the statistics collected would be impacted by how many lists are enabled each time. If a single list is to be used, there are only two possible state values for example, whereas if 2 lists are used, then more states are used. To maintain efficiency, an encoder may maintain a table of mvp_selector parameters depending on the number and/or characteristics of combined lists. Adaptation for entropy coding of each entry in the table may be done independently. In another aspect, a single entry may be maintained, but the statistics may be normalized as coding is performed based on the number of lists used for the current block. If, for example, only uniprediction is to be used, values for mvp_selector above 1 would not make sense. In that case, the normalization would be based only on the number of occurrences of values 0 and 1.</p><p id="p-0157" num="0129">Prediction of motion vectors could also be performed jointly using both neighbouring (spatial and/or temporal) and collocated (motion vectors for the current block but from another list) candidates. In particular, an encoder may use the information of both the mvd (motion vector error) and mvp (motion vector predictor) from the collocated candidate, to derive an mvp for the current list, (e.g., list[i], i=1) as follows:</p><p id="p-0158" num="0000"><maths id="MATH-US-00005" num="00005"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mover>      <mrow>       <mi>mv</mi>       <mo>&#x2062;</mo>       <msub>        <mi>p</mi>        <mrow>         <mi>L</mi>         <mo>&#x2062;</mo>         <mi>&#x3b9;</mi>         <mo>&#x2062;</mo>         <mi>stL</mi>         <mo>&#x2062;</mo>         <mtext>  </mtext>         <mn>1</mn>        </mrow>       </msub>      </mrow>      <mo>&#x21c0;</mo>     </mover>     <mo>=</mo>     <mrow>      <mover>       <mrow>        <mi>mv</mi>        <mo>&#x2062;</mo>        <msub>         <mi>p</mi>         <mrow>          <mi>ne</mi>          <mo>&#x2062;</mo>          <mi>&#x3b9;</mi>          <mo>&#x2062;</mo>          <mi>ghbor</mi>         </mrow>        </msub>       </mrow>       <mo>&#x21c0;</mo>      </mover>      <mo>+</mo>      <mrow>       <mfrac>        <mover>         <mrow>          <mi>m</mi>          <mo>&#x2062;</mo>          <mi>v</mi>          <mo>&#x2062;</mo>          <msub>           <mi>p</mi>           <mrow>            <mi>n</mi>            <mo>&#x2062;</mo>            <mi>e</mi>            <mo>&#x2062;</mo>            <mi>&#x3b9;</mi>            <mo>&#x2062;</mo>            <mi>g</mi>            <mo>&#x2062;</mo>            <mi>h</mi>            <mo>&#x2062;</mo>            <mi>b</mi>            <mo>&#x2062;</mo>            <mi>o</mi>            <mo>&#x2062;</mo>            <mi>r</mi>           </mrow>          </msub>         </mrow>         <mo>&#x21c0;</mo>        </mover>        <mover>         <mrow>          <mi>m</mi>          <mo>&#x2062;</mo>          <mi>v</mi>          <mo>&#x2062;</mo>          <msub>           <mi>p</mi>           <mrow>            <mi>L</mi>            <mo>&#x2062;</mo>            <mi>&#x3b9;</mi>            <mo>&#x2062;</mo>            <mi>stL</mi>            <mo>&#x2062;</mo>            <mn>0</mn>           </mrow>          </msub>         </mrow>         <mo>&#x21c0;</mo>        </mover>       </mfrac>       <mo>*</mo>       <mover>        <msub>         <mi>mvd</mi>         <mrow>          <mi>L</mi>          <mo>&#x2062;</mo>          <mi>&#x3b9;</mi>          <mo>&#x2062;</mo>          <mi>s</mi>          <mo>&#x2062;</mo>          <mi>t</mi>          <mo>&#x2062;</mo>          <mi>L</mi>          <mo>&#x2062;</mo>          <mn>0</mn>         </mrow>        </msub>        <mo>&#x21c0;</mo>       </mover>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mrow>      <mi>Eq</mi>      <mo>.</mo>      <mtext>   </mtext>      <mn>7</mn>     </mrow>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0159" num="0130">In this derivation, prediction uses both the neighbouring predictor, the predictor of the list0 vector, and its motion vector residual error. In another aspect, the scaling factor for mvd</p><p id="p-0160" num="0000"><maths id="MATH-US-00006" num="00006"><math overflow="scroll"> <mrow>  <mo>(</mo>  <mfrac>   <mover>    <mrow>     <mi>m</mi>     <mo>&#x2062;</mo>     <mi>v</mi>     <mo>&#x2062;</mo>     <msub>      <mi>p</mi>      <mrow>       <mi>n</mi>       <mo>&#x2062;</mo>       <mi>e</mi>       <mo>&#x2062;</mo>       <mi>&#x3b9;</mi>       <mo>&#x2062;</mo>       <mi>g</mi>       <mo>&#x2062;</mo>       <mi>h</mi>       <mo>&#x2062;</mo>       <mi>b</mi>       <mo>&#x2062;</mo>       <mi>o</mi>       <mo>&#x2062;</mo>       <mi>r</mi>      </mrow>     </msub>    </mrow>    <mo>&#x21c0;</mo>   </mover>   <mover>    <msub>     <mi>mvp</mi>     <mrow>      <mi>L</mi>      <mo>&#x2062;</mo>      <mi>&#x3b9;</mi>      <mo>&#x2062;</mo>      <mi>stL</mi>      <mo>&#x2062;</mo>      <mn>0</mn>     </mrow>    </msub>    <mo>&#x21c0;</mo>   </mover>  </mfrac>  <mo>)</mo> </mrow></math></maths></p><p id="p-0161" num="0000">could be replaced using a single value that is related to the temporal distances between references. This alternative approach may reduce complexity at the expense of lowered accuracy.</p><p id="p-0162" num="0131">In another aspect, syntax rules may impose the use of a single predictor per list and identify, for the current block, which earlier-encoded list could be used to predict a subsequent list. In such case, the syntax still may permit the use of more than one lists to predict the current list. This could be done, for example, using an averaging, either linear or nonlinear. For example predictors may be derived as:</p><p id="p-0163" num="0000"><maths id="MATH-US-00007" num="00007"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mover>      <mrow>       <mi>mv</mi>       <mo>&#x2062;</mo>       <msub>        <mi>p</mi>        <mrow>         <mi>L</mi>         <mo>&#x2062;</mo>         <mi>&#x3b9;</mi>         <mo>&#x2062;</mo>         <mi>s</mi>         <mo>&#x2062;</mo>         <mi>t</mi>         <mo>&#x2062;</mo>         <mi>L</mi>         <mo>&#x2062;</mo>         <mi>X</mi>        </mrow>       </msub>      </mrow>      <mo>&#x21c0;</mo>     </mover>     <mo>=</mo>     <mrow>      <mover>       <mrow>        <mi>mv</mi>        <mo>&#x2062;</mo>        <msub>         <mi>p</mi>         <mrow>          <mi>ne</mi>          <mo>&#x2062;</mo>          <mi>&#x3b9;</mi>          <mo>&#x2062;</mo>          <mi>ghbor</mi>         </mrow>        </msub>       </mrow>       <mo>&#x21c0;</mo>      </mover>      <mo>+</mo>      <mrow>       <mfrac>        <mn>1</mn>        <mi>X</mi>       </mfrac>       <mo>&#x2062;</mo>       <mrow>        <msubsup>         <mo>&#x2211;</mo>         <mrow>          <mi>i</mi>          <mo>=</mo>          <mn>0</mn>         </mrow>         <mrow>          <mi>i</mi>          <mo>&#x3c;</mo>          <mi>X</mi>         </mrow>        </msubsup>        <mrow>         <msub>          <mi>w</mi>          <mi>i</mi>         </msub>         <mo>&#x2062;</mo>         <mfrac>          <mover>           <mrow>            <mi>m</mi>            <mo>&#x2062;</mo>            <mi>v</mi>            <mo>&#x2062;</mo>            <msub>             <mi>p</mi>             <mrow>              <mi>n</mi>              <mo>&#x2062;</mo>              <mi>e</mi>              <mo>&#x2062;</mo>              <mi>&#x3b9;</mi>              <mo>&#x2062;</mo>              <mi>g</mi>              <mo>&#x2062;</mo>              <mi>h</mi>              <mo>&#x2062;</mo>              <mi>b</mi>              <mo>&#x2062;</mo>              <mi>o</mi>              <mo>&#x2062;</mo>              <mi>r</mi>             </mrow>            </msub>           </mrow>           <mo>&#x21c0;</mo>          </mover>          <mover>           <mrow>            <mi>mv</mi>            <mo>&#x2062;</mo>            <msub>             <mi>p</mi>             <mrow>              <mi>L</mi>              <mo>&#x2062;</mo>              <mrow>               <mi>&#x3b9;</mi>               <mo>&#x2062;</mo>               <mi>stL</mi>               <mo>&#x2062;</mo>               <mo>_</mo>               <mo>&#x2062;</mo>               <mi>&#x3b9;</mi>              </mrow>             </mrow>            </msub>           </mrow>           <mo>&#x21c0;</mo>          </mover>         </mfrac>         <mo>*</mo>         <mover>          <msub>           <mi>mvd</mi>           <mrow>            <mi>L</mi>            <mo>&#x2062;</mo>            <mi>&#x3b9;</mi>            <mo>&#x2062;</mo>            <mrow>             <mi>stL</mi>             <mo>&#x2062;</mo>             <mo>_</mo>             <mo>&#x2062;</mo>             <mi>&#x3b9;</mi>            </mrow>           </mrow>          </msub>          <mo>&#x21c0;</mo>         </mover>        </mrow>       </mrow>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mrow>      <mi>Eq</mi>      <mo>.</mo>      <mtext>   </mtext>      <mn>8</mn>     </mrow>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0164" num="0132">In the above, w<sub>i </sub>are weights that may be either pre-specified by syntax rules or signalled at a higher coding level. The weights w<sub>i </sub>need not sum to X (e.g., &#x3a3;<sub>i=0</sub><sup>i&#x3c;X</sup>w<sub>i</sub>&#x2260;1).</p><p id="p-0165" num="0133">In another aspect, signalling of which mvds to use from earlier lists may be done using an mvp_selector parameter signalled in the prediction unit.</p><p id="p-0166" num="0134">In a further aspect, an encoder may use a single predictor for multiple hypotheses or tie the computation of the predictors for multiple hypotheses together. Such coding decisions may be signalled in the coding bit stream or may be defined by coding rules of its syntax.</p><p id="p-0167" num="0135">In one example, mvp_l0[ref_idx_l0] and mvp_l1[ref_idx_l1] can be tied together. This could be done by signalling which lists and which references will be accounted at the same time at a higher level syntax structure (e.g. the sequence level, the picture level, or the slice level). Such consideration could also be made at a group of CTUs level as well. The relationship between the two mvps could be derived by using a simple scaling factor (scale_factor) that could be signalled explicitly or, alternatively, could be determined by POC differences of the two references. In such as case, instead of forming mvp_l0[ref_idx_l0] from one candidate set and forming mvp_l1[ref_idx_l1] from another candidate set, a decoder can form both at the same time from a combined candidate set. This can be achieved by a variety of ways, for instance if mvp_l0[ref_idx_l0] is computed as median vector of three candidates, the decoder can re-formulate that as minimizing a sum over three terms.</p><p id="p-0168" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>argmin<sub>x</sub>{&#x3a3;<sub>i</sub><i>|x&#x2212;mv</i><sub>candidates[i]</sub>|}&#x2003;&#x2003;(Eq. 9)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0169" num="0000">Now with combined candidates, the decoder minimizes a similar sum with six terms.</p><p id="p-0170" num="0136">In the above mvp_l1[ref_idx_l1] can then be computed as scale_factor*mvp_l0[ref_idx_l0]. Other motion vectors, such as mvp_l0[0:L], can be determined together in a similar fashion. This concept can also be extended to multiple lists (&#x3e;2).</p><p id="p-0171" num="0137">In another aspect, motion vector prediction of a given pixel block may be predicted from other pixel blocks that are neighbours of the pixel block being coded. A bi-predictive or multihypothesis candidate may be based on pixel blocks in a spatial neighbourhood of the block being coded, which may be represented an index j into a merge_idx list. This candidate j is associated with motion vectors and references for all its lists, e.g. L0 and L1. A coder may introduce an additional candidate, (called j+1), which uses the same predictor for L0, but for L1 only the reference index is reused. For the L1 motion vector candidate, an encoder may scale its motion vectors based on relative temporal displacement between the L1 and the L0 candidates. In another case, an encoder may select a first, unipredicted candidate (e.g. L0), for skip or merge, which has a consequence that no neighbours would provide candidates for another list. In such a case, an encoder may indicate, using merge_idx, a derived candidate for another list (say, L1) given its L0 candidate, which enables biprediction or multihypothesis prediction using another merge_idx.</p><p id="p-0172" num="0138">The foregoing techniques may be extended to coding of Overlapped Block Motion Compensation with multihypothesis prediction as well as affine block motion compensation, for coding systems that support those features.</p><p id="p-0173" num="0139">The foregoing discussion has described operation of the aspects of the present disclosure in the context of video coders and decoders. Commonly, these components are provided as electronic devices. Video decoders and/or controllers can be embodied in integrated circuits, such as application specific integrated circuits, field programmable gate arrays, and/or digital signal processors. Alternatively, they can be embodied in computer programs that execute on camera devices, personal computers, notebook computers, tablet computers, smartphones, or computer servers. Such computer programs typically are stored in physical storage media such as electronic-, magnetic-, and/or optically-based storage devices, where they are read to a processor and executed. Decoders commonly are packaged in consumer electronics devices, such as smartphones, tablet computers, gaming systems, DVD players, portable media players and the like; and they also can be packaged in consumer software applications such as video games, media players, media editors, and the like. And, of course, these components may be provided as hybrid systems that distribute functionality across dedicated hardware components and programmed general-purpose processors, as desired.</p><p id="p-0174" num="0140">Video coders and decoders may exchange video through channels in a variety of ways. They may communicate with each other via communication and/or computer networks as illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. In still other applications, video coders may output video data to storage devices, such as electrical, magnetic and/or optical storage media, which may be provided to decoders sometime later. In such applications, the decoders may retrieve the coded video data from the storage devices and decode it.</p><p id="p-0175" num="0141">Several embodiments of the invention are specifically illustrated and/or described herein. However, it will be appreciated that modifications and variations of the invention are covered by the above teachings and within the purview of the appended claims without departing from the spirit and intended scope of the invention.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230007272A1-20230105-M00001.NB"><img id="EMI-M00001" he="5.67mm" wi="76.20mm" file="US20230007272A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002" nb-file="US20230007272A1-20230105-M00002.NB"><img id="EMI-M00002" he="5.67mm" wi="76.20mm" file="US20230007272A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00003" nb-file="US20230007272A1-20230105-M00003.NB"><img id="EMI-M00003" he="5.67mm" wi="76.20mm" file="US20230007272A1-20230105-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00004" nb-file="US20230007272A1-20230105-M00004.NB"><img id="EMI-M00004" he="6.35mm" wi="76.20mm" file="US20230007272A1-20230105-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00005" nb-file="US20230007272A1-20230105-M00005.NB"><img id="EMI-M00005" he="6.35mm" wi="76.20mm" file="US20230007272A1-20230105-M00005.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00006" nb-file="US20230007272A1-20230105-M00006.NB"><img id="EMI-M00006" he="6.69mm" wi="76.20mm" file="US20230007272A1-20230105-M00006.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00007" nb-file="US20230007272A1-20230105-M00007.NB"><img id="EMI-M00007" he="6.69mm" wi="76.20mm" file="US20230007272A1-20230105-M00007.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><claims id="claims"><claim id="CLM-01-33" num="01-33"><claim-text><b>1</b>-<b>33</b>. (canceled)</claim-text></claim><claim id="CLM-00034" num="00034"><claim-text><b>34</b>. A system for video decoding, comprising:<claim-text>a syntax unit for parsing a coded data stream;</claim-text><claim-text>a predictor for developing hypotheses of pixel blocks;</claim-text><claim-text>a prediction synthesizer for aggregating hypotheses of pixel blocks;</claim-text><claim-text>a pixel block decoder for decoding pixel blocks with respect to corresponding prediction blocks;</claim-text><claim-text>a controller for causing the system to:<claim-text>parse, by the syntax unit, a coded data stream to identify a first count of hypotheses from a syntax layer selected from a group of sequence-level, picture-level, slice-level, segment-level, and group-of-CTUs-level syntax layers, a second count of prediction lists of reference frames from a slice-level syntax layer, selected prediction lists and selected reference frames in the prediction lists for each hypothesis, and associated prediction data applied to a first pixel block, of an input frame:</claim-text><claim-text>generate, by the predictor, the first count of prediction blocks as hypotheses for the first pixel block from the prediction data, each prediction block is generated from pixel data of a corresponding selected reference frame in a corresponding selected prediction list associated with a coding hypothesis coded, wherein the first count and the second count are not the same;</claim-text><claim-text>aggregate, by the prediction synthesizer, the prediction blocks into an aggregate prediction block for the first pixel block; and</claim-text><claim-text>decode, by the pixel block decoder, the first pixel block with reference to the aggregate prediction block.</claim-text></claim-text></claim-text></claim><claim id="CLM-00035" num="00035"><claim-text><b>35</b>. The system of <claim-ref idref="CLM-00034">claim 34</claim-ref>, wherein the aggregating the prediction blocks comprises weighting the prediction data, associated with the respective hypotheses, according to their respective motion vectors.</claim-text></claim><claim id="CLM-00036" num="00036"><claim-text><b>36</b>. The system of <claim-ref idref="CLM-00034">claim 34</claim-ref>, wherein the controller further causes the system to, responsive to channel data identifying, for at least one hypothesis, an affine transform, scaling prediction data, associated with the at least one hypothesis, according to the affine transform.</claim-text></claim><claim id="CLM-00037" num="00037"><claim-text><b>37</b>. The system of <claim-ref idref="CLM-00034">claim 34</claim-ref>, wherein, for at least one hypothesis, the generating the prediction block comprises:<claim-text>predicting of a motion vector, of prediction data associated with the at least one hypothesis, based on one or more motion vectors of prediction data associated with one or more other hypotheses.</claim-text></claim-text></claim><claim id="CLM-00038" num="00038"><claim-text><b>38</b>. The system of <claim-ref idref="CLM-00037">claim 37</claim-ref>, wherein the predicting of the motion vector is further based on a temporal distance between the input frame and one or more reference frames associated with the one or more other hypotheses.</claim-text></claim><claim id="CLM-00039" num="00039"><claim-text><b>39</b>. The system of <claim-ref idref="CLM-00037">claim 37</claim-ref>, wherein the one or more other hypotheses are hypotheses developed for pixel blocks from a spatiotemporal neighborhood of the pixel block, hypotheses coded based on the identified prediction lists, or a combination thereof.</claim-text></claim><claim id="CLM-00040" num="00040"><claim-text><b>40</b>. The system of <claim-ref idref="CLM-00037">claim 37</claim-ref>, wherein the predicting of the motion vector is further based on a trend over time in values of the one or more motion vectors of prediction data associated with the one or more other hypotheses.</claim-text></claim><claim id="CLM-00041" num="00041"><claim-text><b>41</b>. The system of <claim-ref idref="CLM-00037">claim 37</claim-ref>, wherein the predicting of the motion vector is further based on chaining, wherein the motion vector references data in a destination frame by aggregating a first motion vector of the input frame, of prediction data associated with one of the one or more other hypotheses, that references an intermediate frame and a second motion vector of an intermediate frame, of prediction data associated with another of the one or more other hypotheses, that references the destination frame.</claim-text></claim><claim id="CLM-00042" num="00042"><claim-text><b>42</b>. The system of <claim-ref idref="CLM-00034">claim 34</claim-ref>, wherein the controller further causes the system to, responsive to channel data identifying, for at least one hypothesis, a minimum size of prediction blocks and a maximum size of prediction blocks, the generating prediction block for the at least one hypothesis comprises generating one or more prediction blocks at sizes between the identified minimum and maximum sizes.</claim-text></claim><claim id="CLM-00043" num="00043"><claim-text><b>43</b>. The system of <claim-ref idref="CLM-00034">claim 34</claim-ref>, wherein the portion of the coded video stream identifying the prediction lists and the prediction data, are provided within the coded video stream at the level of a sequence, a picture, a slice, a segment, or a group of CTUs.</claim-text></claim><claim id="CLM-00044" num="00044"><claim-text><b>44</b>. The system of <claim-ref idref="CLM-00034">claim 34</claim-ref>, wherein the controller further causes the system to, responsive to channel data identifying, for at least one hypothesis, a zero value for motion vector, generating the prediction block for the at least one hypothesis from collocated pixel block of a reference frame provided by the respective prediction list.</claim-text></claim><claim id="CLM-00045" num="00045"><claim-text><b>45</b>. The system of <claim-ref idref="CLM-00034">claim 34</claim-ref>, wherein the controller further causes the system to, responsive to channel data identifying, for at least one hypothesis, a skip mode, using collocated pixel block of a reference frame provided by the respective prediction list to restore the pixel block.</claim-text></claim><claim id="CLM-00046" num="00046"><claim-text><b>46</b>. The system of <claim-ref idref="CLM-00034">claim 34</claim-ref>, wherein the controller further causes the system to, responsive to channel data identifying, for at least one hypothesis, a merge mode, using prediction data of a pixel block that resides in a reference frame provided by the respective prediction list.</claim-text></claim><claim id="CLM-00047" num="00047"><claim-text><b>47</b>. The system of <claim-ref idref="CLM-00034">claim 34</claim-ref>, wherein the controller further causes the system to, responsive to channel data identifying prediction data and an associated collocated list:<claim-text>generating a further prediction block from the prediction data and the associated collocated list, and aggregating the further prediction block into the aggregate prediction block.</claim-text></claim-text></claim><claim id="CLM-00048" num="00048"><claim-text><b>48</b>. The system of <claim-ref idref="CLM-00034">claim 34</claim-ref>, wherein the controller further causes the system to, responsive to channel data identifying, for at least one hypothesis, a constant value for a component of motion vectors of prediction data associated with the at least one hypothesis, using the constant value for the component of the motion vectors.</claim-text></claim><claim id="CLM-00049" num="00049"><claim-text><b>49</b>. The system of <claim-ref idref="CLM-00034">claim 34</claim-ref>, wherein the controller further causes the system to, responsive to channel data identifying, for at least one hypothesis, a constant value for an illumination compensation parameter of prediction data associated with the at least one hypothesis, using the constant value for the illumination compensation parameter.</claim-text></claim><claim id="CLM-00050" num="00050"><claim-text><b>50</b>. The system of <claim-ref idref="CLM-00034">claim 34</claim-ref>, wherein at least one of the identified selected prediction list has one reference index.</claim-text></claim><claim id="CLM-00051" num="00051"><claim-text><b>51</b>. The system of <claim-ref idref="CLM-00034">claim 34</claim-ref>, wherein the controller further causes the system to, responsive to channel data indicating, for at least one hypothesis that a motion vector is within a predetermined range, generating the prediction block for the at least one hypothesis according to a motion vector within the predetermined range.</claim-text></claim><claim id="CLM-00052" num="00052"><claim-text><b>52</b>. The system of <claim-ref idref="CLM-00034">claim 34</claim-ref>, wherein the controller further causes the system to, responsive to channel data identifying, for at least one hypothesis, a pixel increment value for motion vectors, generating the prediction block for the at least one hypothesis using a motion vector set according to the pixel increment value.</claim-text></claim><claim id="CLM-00053" num="00053"><claim-text><b>53</b>. The system of <claim-ref idref="CLM-00034">claim 34</claim-ref>, wherein the controller further causes the system to, responsive to channel data identifying, for at least one hypothesis, a sub-pixel increment value for motion vectors, generating the prediction block for the at least one hypothesis using a motion vector set according to the sub-pixel increment value.</claim-text></claim><claim id="CLM-00054" num="00054"><claim-text><b>54</b>. The system of <claim-ref idref="CLM-00034">claim 34</claim-ref>, wherein the controller further causes the system to, responsive to channel data identifying a parameter for a sub-pixel prediction filter, filtering the decoded pixel blocks with the sub-pixel prediction filter using the parameter.</claim-text></claim><claim id="CLM-00055" num="00055"><claim-text><b>55</b>. The system of <claim-ref idref="CLM-00034">claim 34</claim-ref>, wherein, responsive to channel data indicating, for at least one hypothesis, that the pixel block has been partitioned according to a partition depth, the generating a prediction block comprises generating a prediction block for each partition.</claim-text></claim><claim id="CLM-00056" num="00056"><claim-text><b>56</b>. The system of <claim-ref idref="CLM-00034">claim 34</claim-ref>, wherein, responsive to channel data indicating, for at least one hypothesis, that the pixel block has been partitioned according to a predetermined partition depth, the generating a prediction block comprises generating a prediction block for each partition.</claim-text></claim><claim id="CLM-00057" num="00057"><claim-text><b>57</b>. The system of <claim-ref idref="CLM-00034">claim 34</claim-ref>, wherein the controller further causes the system to, responsive to channel data identifying a parameter of an in-loop filter, filtering the decoded pixel blocks with the in-loop filter using the parameter.</claim-text></claim><claim id="CLM-00058" num="00058"><claim-text><b>58</b>. A non-transitory computer readable medium storing a computer program that, when executed by a processor, cause the processor to:<claim-text>parse a coded data stream to identify a first count of hypotheses from a syntax layer selected from a group of sequence-level, picture-level, slice-level, segment-level, and group-of-CTUs-level syntax layers, a second count of prediction lists of reference frames from a slice-level syntax layer, selected prediction lists and selected reference frames in the prediction lists for each hypothesis, and associated prediction data applied to a first pixel block, of an input frame:</claim-text><claim-text>generate the first count of prediction blocks as hypotheses for the first pixel block from the prediction data, each prediction block is generated from pixel data of a corresponding selected reference frame in a corresponding selected prediction list associated with a coding hypothesis coded, wherein the first count and the second count are not the same;</claim-text><claim-text>aggregate the prediction blocks into an aggregate prediction block for the first pixel block; and</claim-text><claim-text>decode the first pixel block with reference to the aggregate prediction block.</claim-text></claim-text></claim><claim id="CLM-00059" num="00059"><claim-text><b>59</b>. A system for video coding, comprising:<claim-text>a pixel block coder for differentially coding pixel blocks with respect to corresponding prediction blocks;</claim-text><claim-text>a predictor for developing hypotheses of pixel blocks;</claim-text><claim-text>a prediction synthesizer for aggregating hypotheses of pixel blocks;</claim-text><claim-text>a syntax unit for generating a coded data stream; and</claim-text><claim-text>a controller for causing the system to:<claim-text>develop, by the predictor, a first count of coding hypotheses for an input pixel block, of an input frame, to be coded, each hypothesis developed by:<claim-text>selecting a prediction list of reference frames, out of a second count of prediction lists, to be used in coding of the hypothesis, wherein the first and second counts are not the same, and</claim-text><claim-text>generating a prediction block as a developed hypothesis for the input pixel block from pixel data of a selected reference frame in the selected prediction list, wherein the pixel data and the selected reference frame are identified by prediction data associated with the hypothesis;</claim-text></claim-text><claim-text>aggregate, by the prediction synthesizer, the prediction blocks generated from the developed hypotheses into an aggregate prediction block for the input pixel block;</claim-text><claim-text>code, by the pixel block coder, the input pixel block differentially with reference to the aggregate prediction block; and</claim-text><claim-text>generate, by the syntax unit, a coded data stream with identifiers of the selected prediction lists and associated prediction data for the respective hypotheses and wherein an indication of the first count of coding hypotheses are coded in a syntax layer selected from a group of sequence-level, picture-level, slice-level, segment-level, and group-of-CTUs-level syntax layers, and the second count of prediction lists is coded in a slice-level syntax layer.</claim-text></claim-text></claim-text></claim></claims></us-patent-application>