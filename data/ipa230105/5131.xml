<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005132A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005132</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17894275</doc-number><date>20220824</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>33</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>5</main-group><subgroup>50</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>24</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>0008</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>337</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>5</main-group><subgroup>50</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>242</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20132</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20224</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30108</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">IMAGE INSPECTION DEVICE AND IMAGE INSPECTION METHOD</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>PCT/JP2020/017946</doc-number><date>20200427</date></document-id><parent-status>PENDING</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>17894275</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Mitsubishi Electric Corporation</orgname><address><city>Tokyo</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>OKAHARA</last-name><first-name>Kohei</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>MINEZAWA</last-name><first-name>Akira</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>Mitsubishi Electric Corporation</orgname><role>03</role><address><city>Tokyo</city><country>JP</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">An image inspection device includes: an image acquisition unit to acquire an inspection target image; a geometric transformation processing unit to estimate a geometric transformation parameter for aligning a position of an inspection target in the inspection target image with a first reference image in which a position of the inspection target is known, and geometrically transform the inspection target image using the estimated geometric transformation parameter, thereby generating an aligned image in which the position of the inspection target is aligned with the first reference image; an image restoration processing unit to restore the aligned image, using an image generation network to receive an input image generated using the inspection target image and infer the aligned image as a correct image; and an abnormality determination unit to determine an abnormality of the inspection target using a difference image between the aligned image and the restored aligned image.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="111.59mm" wi="71.29mm" file="US20230005132A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="197.87mm" wi="100.75mm" file="US20230005132A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="183.81mm" wi="122.94mm" orientation="landscape" file="US20230005132A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="141.56mm" wi="73.32mm" file="US20230005132A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="183.64mm" wi="93.98mm" file="US20230005132A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="235.71mm" wi="153.75mm" file="US20230005132A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application is a Continuation of PCT International Application No. PCT/JP2020/017946 filed on Apr. 27, 2020, which is hereby expressly incorporated by reference into the present application.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">The present disclosure relates to an image inspection device and an image inspection method.</p><heading id="h-0003" level="1">BACKGROUND ART</heading><p id="p-0004" num="0003">A technique for determining an abnormality of an inspection target on the basis of a result of inspecting an image in which the inspection target is photographed has been proposed. For example, the image inspection method described in Non-Patent Literature 1 causes an auto encoder or a Generative Adversarial Network (GAN) to learn an image generation method for restoring a normal image on the basis of a feature extracted from the normal image in which a normal inspection target is photographed. This image generation method has a property that a normal image cannot be accurately restored by a feature extracted from an abnormal image in which an abnormal inspection target is photographed. The image inspection method described in Non-Patent Literature 1 calculates a difference image between an image in which an inspection target is photographed and a restored image, and determines an abnormality of the inspection target on the basis of the difference image.</p><heading id="h-0004" level="1">CITATION LIST</heading><heading id="h-0005" level="1">Non-Patent Literature</heading><p id="p-0005" num="0000"><ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0004">Non-Patent Literature 1: Schlegl, Thomas, et al., &#x201c;Unsupervised anomaly detection with generative adversarial networks to guide marker discovery&#x201d;, ICIP 2017.</li></ul></p><heading id="h-0006" level="1">SUMMARY OF INVENTION</heading><heading id="h-0007" level="1">Technical Problem</heading><p id="p-0006" num="0005">When a part of the appearance of a product being the subject is an inspection target, a certain region in an image in which the product is photographed is an inspection target image region. In this case, between an image photographed in a state where the product directly faces the camera and an image photographed in a state where the product does not directly face the camera, a shift occurs in the position and posture of the inspection target in the image. The conventional technique described in Non-Patent Literature 1 has a problem that it can be seen that there is an abnormality in the inspection target due to occurrence of a shift in the position and posture, but it is not possible to accurately determine in which part of the inspection target the abnormality has occurred.</p><p id="p-0007" num="0006">The present disclosure solves the above problems, and an object of the present disclosure is to obtain an image inspection device and an image inspection method capable of performing image inspection robust to changes in positions and postures of an inspection target and a photographing device.</p><heading id="h-0008" level="1">Solution to Problem</heading><p id="p-0008" num="0007">An image inspection device according to the present disclosure includes: image acquisition circuitry to acquire a first image in which an inspection target is photographed; geometric transformation processing circuitry to estimate a geometric transformation parameter used for aligning a position of the inspection target in the first image with a first reference image in which a position of the inspection target is known, and geometrically transform the first image by using the estimated geometric transformation parameter, thereby generating a second image in which the position of the inspection target in the first image is aligned with the first reference image; image restoration processing circuitry to restore the second image, by using an image generation network to receive an input of a third image generated by using the first image and infer the second image as a correct image; and abnormality determination circuitry to determine an abnormality of the inspection target, by using a difference image between the second image obtained by the geometric transformation on the first image and the restored second image.</p><heading id="h-0009" level="1">Advantageous Effects of Invention</heading><p id="p-0009" num="0008">According to the present disclosure, even when changes occur in positions and postures of an inspection target and a photographing device, the inspection target on a first image is aligned by geometric transformation using a first reference image in which the position of the inspection target is known. A second image is restored, by using an image generation network that infers, as a correct image, the second image in which the inspection target is aligned. The abnormality of the inspection target is determined, by using a difference image between the second image obtained by the geometric transformation on the first image and the restored second image. As a result, the image inspection device according to the present disclosure can perform image inspection robust to the changes in the positions and postures of the inspection target and the photographing device.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0010" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>1</b>A</figref> is a schematic diagram illustrating an image photographed in a state where a subject directly faces a camera, and <figref idref="DRAWINGS">FIG. <b>1</b>B</figref> is a schematic diagram illustrating an image photographed in a state where the subject does not directly face the camera.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram illustrating a configuration of an image inspection device according to a first embodiment.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flowchart illustrating an image inspection method according to the first embodiment.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>4</b>A</figref> is a block diagram illustrating a hardware configuration for implementing the functions of the image inspection device according to the first embodiment, and <figref idref="DRAWINGS">FIG. <b>4</b>B</figref> is a block diagram illustrating a hardware configuration for executing software for implementing the functions of the image inspection device according to the first embodiment.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a block diagram illustrating a configuration of an image inspection device according to a second embodiment.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flowchart illustrating an image inspection method according to the second embodiment.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0011" level="1">DESCRIPTION OF EMBODIMENTS</heading><heading id="h-0012" level="1">First Embodiment</heading><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>1</b>A</figref> is a schematic diagram illustrating an image A photographed in a state where a subject B directly faces a camera. <figref idref="DRAWINGS">FIG. <b>1</b>B</figref> is a schematic diagram illustrating an image A<b>1</b> photographed in a state where the subject B does not directly face the camera. When the subject B to be inspected is photographed in a state of directly facing the camera, for example, the image A in which the subject B is photographed is obtained as illustrated in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>. In the image A, one component Ba of the subject B is photographed at a predetermined position.</p><p id="p-0017" num="0016">In a case where a position and a posture of the subject B are shifted or a position and a posture of the camera are shifted, the subject B is photographed in a state of not directly facing the camera. For example, as illustrated in <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>, the subject B is obliquely photographed in the image A<b>1</b>, and the positional shift of a component Ba in the image A<b>1</b> may be erroneously recognized as being photographed like a component Bb due to occurrence of abnormality in the component Ba. That is, this positional shift is a factor that the abnormality of the component Ba cannot be accurately determined.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram illustrating a configuration of an image inspection device <b>1</b> according to a first embodiment. In <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the image inspection device <b>1</b> is connected to a photographing device <b>2</b> and a storage device <b>3</b>, receives an input of an image in which an inspection target is photographed by the photographing device <b>2</b>, and determines an abnormality of the inspection target using the input image and data stored in the storage device <b>3</b>.</p><p id="p-0019" num="0018">The photographing device <b>2</b> is a camera that photographs an inspection target, and is, for example, a network camera, an analog camera, a USB camera, or an HD-SDI camera. The storage device <b>3</b> is a storage device that stores data used or generated in image inspection processing performed by the image inspection device <b>1</b>, and includes a main memory <b>3</b><i>a </i>and an auxiliary memory <b>3</b><i>b. </i></p><p id="p-0020" num="0019">The auxiliary memory <b>3</b><i>b </i>stores a learned model that is an image generation network, parameter information such as model information defining a configuration of the learned model, a first reference image used for alignment of an inspection target, a second reference image used for creation of an image input to the image generation network, threshold information used for abnormality determination of the inspection target, and annotation information such as a position of the inspection target and a region of the inspection target in the image. The information stored in the auxiliary memory <b>3</b><i>b </i>is read into the main memory <b>3</b><i>a </i>and used by the image inspection device <b>1</b>.</p><p id="p-0021" num="0020">As illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the image inspection device <b>1</b> includes an image acquisition unit <b>11</b>, a geometric transformation processing unit <b>12</b>, an image restoration processing unit <b>13</b>, and an abnormality determination unit <b>14</b>. The image acquisition unit <b>11</b> acquires an image in which the inspection target is photographed by the photographing device <b>2</b> via an input interface (I/F). The image in which the inspection target is photographed by the photographing device <b>2</b> is a first image including not only an image in a state in which the subject as the inspection target directly faces a photographing field of view of the photographing device <b>2</b> but also an image in a state in which the subject does not directly face the photographing field of view of the photographing device <b>2</b>.</p><p id="p-0022" num="0021">The geometric transformation processing unit <b>12</b> estimates a geometric transformation parameter used for aligning the position of the inspection target in the image acquired by the image acquisition unit <b>11</b> with the first reference image in which the position of the inspection target is known. Then, the geometric transformation processing unit <b>12</b> uses the estimated geometric transformation parameter to geometrically transform the image acquired by the image acquisition unit <b>11</b>, thereby generating an image in which the position of the inspection target is aligned with the first reference image.</p><p id="p-0023" num="0022">The first reference image is an image in which the position of the inspection target is known, and is photographed in a state where the inspection target directly faces the photographing field of view of the photographing device <b>2</b>. For example, when the component Ba illustrated in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref> is an inspection target, the image A in which the position of the component Ba is known can be used as the first reference image. The image generated by the geometric transformation processing unit <b>12</b> is a second image in which the position of the inspection target is aligned with the first reference image.</p><p id="p-0024" num="0023">The image restoration processing unit <b>13</b> inputs an input image generated using the image acquired by the image acquisition unit <b>11</b> to the image generation network, thereby restoring an image in which the position of the inspection target is aligned with the first reference image from the input image. The input image to the image generation network is a third image generated using the inspection target image acquired by the image acquisition unit <b>11</b>, and is, for example, a difference image between the inspection target image acquired by the image acquisition unit <b>11</b> and the second reference image in which the position of the inspection target is known.</p><p id="p-0025" num="0024">The image generation network is a learned model that receives, as an input, the input image generated by the image restoration processing unit <b>13</b> and infers, as a correct image, an image in which the position of the inspection target is aligned with the first reference image. For example, the image generation network has learned image conversion between an input image and an output image by using, as learning data, a plurality of pairs of a correct image (output image) that is an image in which a normal inspection target generated by the geometric transformation processing is photographed and an input image that is an image related to the normal inspection target generated by the image restoration processing unit <b>13</b>.</p><p id="p-0026" num="0025">The abnormality determination unit <b>14</b> calculates a difference image between the inspection target image geometrically transformed by the geometric transformation processing unit <b>12</b> and the inspection target image restored by the image restoration processing unit <b>13</b>, and determines an abnormality of the inspection target using the difference image. For example, the abnormality determination unit <b>14</b> specifies the inspection target in the difference image on the basis of the annotation information indicating the position of the inspection target and the region of the inspection target in the image, and determines the abnormality of the inspection target on the basis of a result of comparing a difference image region of the specified inspection target with the threshold information. The difference image is, for example, an amplitude image, a phase image, or an intensity image. The threshold information is a threshold of an amplitude, a phase, or an intensity.</p><p id="p-0027" num="0026">An image inspection method according to the first embodiment is as follows.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flowchart illustrating the image inspection method according to the first embodiment, and illustrates a series of processes of the image inspection executed by the image inspection device <b>1</b>.</p><p id="p-0029" num="0028">The product to be inspected is disposed in the photographing field of view of the photographing device <b>2</b>, and is photographed by the photographing device <b>2</b>. An image of the inspection target photographed by the photographing device <b>2</b> is an &#x201c;inspection target image&#x201d;. The image acquisition unit <b>11</b> acquires inspection target images sequentially photographed by the photographing device <b>2</b> (step ST<b>1</b>). The inspection target image acquired by the image acquisition unit <b>11</b> is output to the geometric transformation processing unit <b>12</b>.</p><p id="p-0030" num="0029">The geometric transformation processing unit <b>12</b> estimates a geometric transformation parameter used for aligning the position of the inspection target in the inspection target image with the first reference image in which the position of the inspection target is known, and geometrically transforms the inspection target image using the geometric transformation parameter, thereby generating an image in which the position of the inspection target is aligned with the first reference image (step ST<b>2</b>). For example, the geometric transformation processing unit <b>12</b> estimates the geometric transformation parameter through image registration processing.</p><p id="p-0031" num="0030">The image registration is processing of estimating a geometric transformation parameter between an attention image and a reference image, on the basis of the similarity between the feature points extracted from the attention image and the reference image or the similarity between the image regions image-converted between the attention image and the reference image. Examples of the geometric transformation processing include Euclidean transformation, affine transformation, and homography transformation that are linear transformations. Furthermore, the geometric transformation processing may be at least one of image rotation, image inversion, or cropping.</p><p id="p-0032" num="0031">In the auxiliary memory <b>3</b><i>b </i>included in the storage device <b>3</b>, an inspection target image photographed in a state where the inspection target directly faces the photographing field of view of the photographing device <b>2</b> is stored as a first reference image. Information indicating the position of the inspection target in the inspection target image and the image region of the inspection target in the inspection target image is annotated in the first reference image. For example, the image A illustrated in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref> is stored in the storage device <b>3</b> as a first reference image, and annotation information indicating the position of the component Ba and the image region of the component Ba is added to each of the first reference images.</p><p id="p-0033" num="0032">The geometric transformation processing unit <b>12</b> executes image registration processing of aligning the position of the inspection target in the inspection target image photographed by the photographing device <b>2</b> with the position specified on the basis of the annotation information added to the first reference image, and estimates the geometric transformation parameter necessary for the alignment. Then, the geometric transformation processing unit <b>12</b> performs the geometric transformation processing using the geometric transformation parameter on the inspection target image photographed by the photographing device <b>2</b>, thereby generating the image of the inspection target photographed in the same position and posture as the first reference image. Hereinafter, the image generated by the geometric transformation processing unit <b>12</b> is an &#x201c;aligned image&#x201d;.</p><p id="p-0034" num="0033">The image restoration processing unit <b>13</b> generates an input image to the image generation network (step ST<b>3</b>). For example, when the image generation network is a neural network having a skip connection across a plurality of layers as in the U-net, learning is performed in such a way that the weight of the route to be skip-connected increases. Therefore, the image generation network learns to output the input image as it is, and it is difficult to extract the difference between the aligned image and the output image.</p><p id="p-0035" num="0034">Therefore, the image restoration processing unit <b>13</b> inputs, as an input image, an image obtained by processing the inspection target image, to the image generation network. The image obtained by processing the inspection target image may be, for example, a difference image between the inspection target image and the second reference image. As for the second reference image, for example, an average image of a plurality of inspection target images each of in which a normal inspection target is photographed is used and stored in the auxiliary memory <b>3</b><i>b</i>. Note that, when the image generation network has no skip connection, the input image may be the aligned image.</p><p id="p-0036" num="0035">The image restoration processing unit <b>13</b> restores the aligned image, by inputting the input image generated as described above to the image generation network (step ST<b>4</b>). For example, the image generation network receives an input of the difference image between the inspection target image and the second reference image, and infers (restores) the aligned image.</p><p id="p-0037" num="0036">The abnormality determination unit <b>14</b> determines an abnormality of the inspection target, by using a difference image between the inspection target image geometrically transformed by the geometric transformation processing unit <b>12</b> and the aligned image restored by the image restoration processing unit <b>13</b> (step ST<b>5</b>). For example, when extracting the difference image between the geometrically transformed inspection target image and the restored aligned image, the abnormality determination unit <b>14</b> can specify of which inspection target a position and an image region the extracted difference image is on the basis of the annotation information added to the first reference image. The abnormality determination unit <b>14</b> determines that there is an abnormality in the inspection target of which the position and the image region have been specified.</p><p id="p-0038" num="0037">As for a method of extracting the difference image, there is a method of using a sum or an average value of absolute differences of pixel values for each certain region (for example, for each component region in an image or for each pixel block of a certain size). In addition, as for a method of extracting a difference image, there is a method of using a structural similarity (SSIM or PSNR) of an image for each certain region. In a case where a pixel value of interest in the difference image is larger than the threshold, the abnormality determination unit <b>14</b> determines that there is an abnormality in the inspection target corresponding to the difference image region.</p><p id="p-0039" num="0038">A hardware configuration for implementing the functions of the image inspection device <b>1</b> is as follows.</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>4</b>A</figref> is a block diagram illustrating a hardware configuration for implementing the functions of the image inspection device <b>1</b>. <figref idref="DRAWINGS">FIG. <b>4</b>B</figref> is a block diagram illustrating a hardware configuration for executing software for implementing the functions of the image inspection device <b>1</b>. In <figref idref="DRAWINGS">FIGS. <b>4</b>A and <b>4</b>B</figref>, an input I/F <b>100</b> is an interface that receives an input of a video image photographed by the photographing device <b>2</b>. A file I/F <b>101</b> is an interface that relays data exchanged with the storage device <b>3</b>.</p><p id="p-0041" num="0040">The functions of the image acquisition unit <b>11</b>, the geometric transformation processing unit <b>12</b>, the image restoration processing unit <b>13</b>, and the abnormality determination unit <b>14</b> included in the image inspection device <b>1</b> are implemented by a processing circuit. That is, the image inspection device <b>1</b> includes a processing circuit for executing the processing of steps ST<b>1</b> to ST<b>5</b> illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>. The processing circuit may be dedicated hardware or a central processing unit (CPU) that executes a program stored in a memory.</p><p id="p-0042" num="0041">In a case where the processing circuit is a processing circuit <b>102</b> of dedicated hardware illustrated in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, the processing circuit <b>102</b> corresponds to, for example, a single circuit, a composite circuit, a programmed processor, a parallel programmed processor, an application specific integrated circuit (ASIC), a field-programmable gate array (FPGA), or a combination thereof. The functions of the image acquisition unit <b>11</b>, the geometric transformation processing unit <b>12</b>, the image restoration processing unit <b>13</b>, and the abnormality determination unit <b>14</b> included in the image inspection device <b>1</b> may be implemented by separate processing circuits, or these functions may be collectively implemented by one processing circuit.</p><p id="p-0043" num="0042">In a case where the processing circuit is a processor <b>103</b> illustrated in <figref idref="DRAWINGS">FIG. <b>4</b>B</figref>, the functions of the image acquisition unit <b>11</b>, the geometric transformation processing unit <b>12</b>, the image restoration processing unit <b>13</b>, and the abnormality determination unit <b>14</b> included in the image inspection device <b>1</b> are implemented by software, firmware, or a combination of software and firmware. Note that, software or firmware is written as a program and stored in a memory <b>104</b>.</p><p id="p-0044" num="0043">The processor <b>103</b> reads and executes the program stored in the memory <b>104</b>, thereby implementing the functions of the image acquisition unit <b>11</b>, the geometric transformation processing unit <b>12</b>, the image restoration processing unit <b>13</b>, and the abnormality determination unit <b>14</b> included in the image inspection device <b>1</b>. For example, the image inspection device <b>1</b> includes the memory <b>104</b> that stores programs that result in execution of the processing from step ST<b>1</b> to step ST<b>5</b> illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref> when executed by the processor <b>103</b>. These programs cause a computer to execute procedures or methods performed by the image acquisition unit <b>11</b>, the geometric transformation processing unit <b>12</b>, the image restoration processing unit <b>13</b>, and the abnormality determination unit <b>14</b>. The memory <b>104</b> may be a computer-readable storage medium that stores a program for causing the computer to function as the image acquisition unit <b>11</b>, the geometric transformation processing unit <b>12</b>, the image restoration processing unit <b>13</b>, and the abnormality determination unit <b>14</b>.</p><p id="p-0045" num="0044">Examples of the memory <b>104</b> correspond to a nonvolatile or volatile semiconductor memory, such as a random access memory (RAM), a read only memory (ROM), a flash memory, an erasable programmable read only memory (EPROM), or an electrically-EPROM (EEPROM), a magnetic disk, a flexible disk, an optical disk, a compact disk, a mini disk, and a DVD.</p><p id="p-0046" num="0045">Some of the functions of the image acquisition unit <b>11</b>, the geometric transformation processing unit <b>12</b>, the image restoration processing unit <b>13</b>, and the abnormality determination unit <b>14</b> included in the image inspection device <b>1</b> may be implemented by dedicated hardware, and the remaining may be implemented by software or firmware. For example, the function of the image acquisition unit <b>11</b> is implemented by the processing circuit <b>102</b> which is dedicated hardware, and the functions of the geometric transformation processing unit <b>12</b>, the image restoration processing unit <b>13</b>, and the abnormality determination unit <b>14</b> are implemented by the processor <b>103</b> reading and executing a program stored in the memory <b>104</b>. Thus, the processing circuit can implement the above functions by hardware, software, firmware, or a combination thereof.</p><p id="p-0047" num="0046">As described above, in the image inspection device <b>1</b> according to the first embodiment, even when changes occur in the positions and postures of the inspection target and the photographing device <b>2</b>, the inspection target on the inspection target image is aligned by the geometric transformation using the first reference image in which the position of the inspection target is known. The aligned image is restored using an image generation network that infers, as a correct image, the aligned image in which the inspection target is aligned. The abnormality of the inspection target is determined using the difference image between the inspection target image aligned by the geometric transformation and the restored aligned image. As a result, the image inspection device <b>1</b> can perform image inspection robust to changes in the positions and postures of the inspection target and the photographing device.</p><heading id="h-0013" level="1">Second Embodiment</heading><p id="p-0048" num="0047"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a block diagram illustrating a configuration of an image inspection device <b>1</b>A according to a second embodiment. In <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the image inspection device <b>1</b>A is connected to the photographing device <b>2</b> and the storage device <b>3</b>, receives an input of an image in which an inspection target is photographed by the photographing device <b>2</b>, and determines an abnormality of the inspection target using the input image and data stored in the storage device <b>3</b>. The image inspection device <b>1</b>A includes an image acquisition unit <b>11</b>A, a geometric transformation processing unit <b>12</b>A, an image restoration processing unit <b>13</b>A, and an abnormality determination unit <b>14</b>A.</p><p id="p-0049" num="0048">The image acquisition unit <b>11</b>A acquires an inspection target image in which the inspection target is photographed by the photographing device <b>2</b> via the input I/F, and outputs the acquired image to the geometric transformation processing unit <b>12</b>A and the image restoration processing unit <b>13</b>A. The inspection target image acquired by the image acquisition unit <b>11</b>A is a first image including not only an image in a state in which the subject as the inspection target directly faces a photographing field of view of the photographing device <b>2</b> but also an image in a state in which the subject does not directly face the photographing field of view of the photographing device <b>2</b>.</p><p id="p-0050" num="0049">The geometric transformation processing unit <b>12</b>A estimates a geometric transformation parameter for aligning the position of the inspection target in the inspection target image acquired by the image acquisition unit <b>11</b>A with the first reference image in which the position of the inspection target is known, and geometrically transforms the inspection target image by using the geometric transformation parameter, thereby generating an aligned image in which the position of the inspection target is aligned with the first reference image.</p><p id="p-0051" num="0050">The image restoration processing unit <b>13</b>A inputs the inspection target image (first image) acquired by the image acquisition unit <b>11</b>A to the image generation network, thereby restoring the aligned image from the input image. The abnormality determination unit <b>14</b>A calculates a difference image between the inspection target image geometrically transformed by the geometric transformation processing unit <b>12</b>A and the aligned image restored by the image restoration processing unit <b>13</b>A, and determines an abnormality of the inspection target using the difference image.</p><p id="p-0052" num="0051">An image inspection method according to the second embodiment is as follows.</p><p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flowchart illustrating the image inspection method according to the second embodiment, and illustrates a series of processes of image inspection executed by the image inspection device <b>1</b>A. The image acquisition unit <b>11</b>A acquires inspection target images sequentially photographed by the photographing device <b>2</b> (step ST<b>1</b><i>a</i>). The inspection target image acquired by the image acquisition unit <b>11</b>A is output to the geometric transformation processing unit <b>12</b>A and the image restoration processing unit <b>13</b>A.</p><p id="p-0054" num="0053">The geometric transformation processing unit <b>12</b>A estimates a geometric transformation parameter used for aligning the position of the inspection target in the inspection target image with the first reference image in which the position of the inspection target is known, and geometrically transforms the inspection target image by using the geometric transformation parameter, thereby generating an aligned image in which the position of the inspection target is aligned with the first reference image (step ST<b>2</b><i>ab</i>). Note that, similarly to the geometric transformation processing unit <b>12</b> according to the first embodiment, the geometric transformation processing unit <b>12</b>A estimates the geometric transformation parameter by, for example, image registration processing, and performs the geometric transformation processing using the geometric transformation parameter on the inspection target image acquired by the image acquisition unit <b>11</b>A, thereby generating an aligned image.</p><p id="p-0055" num="0054">In addition, the image restoration processing unit <b>13</b>A restores the aligned image by directly inputting the inspection target image acquired by the image acquisition unit <b>11</b>A to the image generation network (step ST<b>2</b><i>aa</i>). For example, the image generation network has learned image conversion between an input image and an output image by using, as learning data, a plurality of pairs of a correct image (output image) that is an aligned image generated by the geometric transformation processing unit <b>12</b>A and an input image that is an unaligned inspection target image acquired by the image acquisition unit <b>11</b>A. Note that the image conversion of the learning target by the image generation network also includes geometric transformation of aligning the position of the inspection target in the unaligned inspection target image with the first reference image in which the position of the inspection target is known.</p><p id="p-0056" num="0055">The abnormality determination unit <b>14</b>A determines an abnormality of the inspection target, by using a difference image between the inspection target image geometrically transformed by the geometric transformation processing unit <b>12</b>A and the aligned image restored by the image restoration processing unit <b>13</b>A (step ST<b>3</b> <i>a</i>). For example, when extracting the difference image between the geometrically transformed inspection target image and the restored aligned image, the abnormality determination unit <b>14</b>A can specify of which inspection target a position and an image region the extracted difference image is on the basis of the annotation information added to the first reference image. The abnormality determination unit <b>14</b>A determines that there is an abnormality in the inspection target of which the position and the image region have been specified.</p><p id="p-0057" num="0056">Note that the functions of the image acquisition unit <b>11</b>A, the geometric transformation processing unit <b>12</b>A, the image restoration processing unit <b>13</b>A, and the abnormality determination unit <b>14</b>A included in the image inspection device <b>1</b>A are implemented by a processing circuit. That is, the image inspection device <b>1</b>A includes a processing circuit for executing the processing from step ST<b>1</b><i>a </i>to step ST<b>3</b><i>a </i>illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. The processing circuit may be the processing circuit <b>102</b> of dedicated hardware illustrated in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, or may be the processor <b>103</b> that executes the program stored in the memory <b>104</b> illustrated in <figref idref="DRAWINGS">FIG. <b>4</b>B</figref>.</p><p id="p-0058" num="0057">As described above, in the image inspection device <b>1</b>A according to the second embodiment, the input image to the image generation network is the inspection target image photographed by the photographing device <b>2</b>. The image generation network receives an input of the inspection target image and infers the aligned image. The image restoration processing unit <b>13</b>A restores the aligned image using the image generation network. As a result, the image inspection device <b>1</b>A can perform image inspection robust to changes in the positions and postures of the inspection target and the photographing device. In addition, since the processing of generating the input image to the image generation network is omitted, the arithmetic processing amount is reduced as compared with the image inspection method according to the first embodiment. Furthermore, since the geometric transformation processing and the image restoration processing can be performed in parallel, the takt time of the image inspection can be shortened.</p><p id="p-0059" num="0058">Note that combinations of the embodiments, modifications of any components of each of the embodiments, or omissions of any components in each of the embodiments are possible.</p><heading id="h-0014" level="1">INDUSTRIAL APPLICABILITY</heading><p id="p-0060" num="0059">The image inspection device according to the present disclosure can be used, for example, for abnormality inspection of a product.</p><heading id="h-0015" level="1">REFERENCE SIGNS LIST</heading><p id="p-0061" num="0060"><b>1</b>,<b>1</b>A: image inspection device, <b>2</b>: photographing device, <b>3</b>: storage device, <b>3</b><i>a</i>: main memory, <b>3</b><i>b</i>: auxiliary memory, <b>11</b>,<b>11</b>A: image acquisition unit, <b>12</b>,<b>12</b>A: geometric transformation processing unit, <b>13</b>,<b>13</b>A: Image restoration processing unit, <b>14</b>,<b>14</b>A: abnormality determination unit, <b>100</b>: input I/F, <b>101</b>: file I/F, <b>102</b>: processing circuit, <b>103</b>: processor, <b>104</b>: memory</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An image inspection device, comprising:<claim-text>image acquisition circuitry to acquire a first image in which an inspection target is photographed;</claim-text><claim-text>geometric transformation processing circuitry to estimate a geometric transformation parameter used for aligning a position of the inspection target in the first image with a first reference image in which a position of the inspection target is known, and geometrically transform the first image by using the estimated geometric transformation parameter, thereby generating a second image in which the position of the inspection target in the first image is aligned with the first reference image;</claim-text><claim-text>image restoration processing circuitry to restore the second image, by using an image generation network to receive an input of a third image generated by using the first image and infer the second image as a correct image; and</claim-text><claim-text>abnormality determination circuitry to determine an abnormality of the inspection target, by using a difference image between the second image obtained by the geometric transformation on the first image and the restored second image.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The image inspection device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the third image is a difference image between the first image and a second reference image in which a position of the inspection target is known.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The image inspection device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the third image is the first image,</claim-text><claim-text>the image generation network receives an input of the first image and infers the second image, and</claim-text><claim-text>the image restoration processing circuitry restores the second image using the image generation network.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The image inspection device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the geometric transformation processing circuitry generates the second image, by geometrically transforming the first image through image registration on the first reference image.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The image inspection device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the geometric transformation processing circuitry generates the second image, by performing at least one of image rotation, image inversion, or cropping on the first image.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. An image inspection method, comprising:<claim-text>acquiring a first image in which an inspection target is photographed;</claim-text><claim-text>estimating a geometric transformation parameter used for aligning a position of the inspection target in the first image with a first reference image in which a position of the inspection target is known, and geometrically transforming the first image by using the estimated geometric transformation parameter, thereby generating a second image in which the position of the inspection target in the first image is aligned with the first reference image;</claim-text><claim-text>restoring the second image, by using an image generation network to receive an input of a third image generated by using the first image and infer the second image as a correct image; and</claim-text><claim-text>determining an abnormality of the inspection target, by using a difference image between the second image obtained by the geometric transformation on the first image and the restored second image.</claim-text></claim-text></claim></claims></us-patent-application>