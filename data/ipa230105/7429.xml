<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230007430A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230007430</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17778621</doc-number><date>20201113</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>JP</country><doc-number>2019-216096</doc-number><date>20191129</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>S</subclass><main-group>7</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>S</subclass><main-group>7</main-group><subgroup>304</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>S</subclass><main-group>7</main-group><subgroup>307</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">SIGNAL PROCESSING DEVICE, SIGNAL PROCESSING METHOD, AND PROGRAM</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>SONY GROUP CORPORATION</orgname><address><city>TOKYO</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>TSUCHIDA</last-name><first-name>YUJI</first-name><address><city>TOKYO</city><country>JP</country></address></addressbook></inventor></inventors></us-parties><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/JP2020/042377</doc-number><date>20201113</date></document-id><us-371c12-date><date>20220520</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">The present technology relates to a signal processing device, a signal processing method, and a program that allow for prevention of distortion of a sound space. The signal processing device includes: a relative azimuth prediction unit configured to predict, on the basis of a delay time in accordance with a distance from a virtual sound source to a listener, a relative azimuth of the virtual sound source when a sound of the virtual sound source reaches the listener; and a BRIR generation unit configured to acquire a head-related transfer function of the relative azimuth for each one of a plurality of the virtual sound sources and generate a BRIR on the basis of a plurality of the acquired head-related transfer functions. The present technology can be applied to the signal processing device.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="123.36mm" wi="158.75mm" file="US20230007430A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="225.89mm" wi="156.46mm" orientation="landscape" file="US20230007430A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="225.98mm" wi="156.38mm" orientation="landscape" file="US20230007430A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="225.89mm" wi="156.46mm" orientation="landscape" file="US20230007430A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="225.98mm" wi="156.46mm" orientation="landscape" file="US20230007430A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="227.58mm" wi="157.99mm" file="US20230007430A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="234.70mm" wi="159.68mm" orientation="landscape" file="US20230007430A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="234.70mm" wi="159.68mm" orientation="landscape" file="US20230007430A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="234.70mm" wi="159.68mm" orientation="landscape" file="US20230007430A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="229.53mm" wi="153.92mm" file="US20230007430A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="234.61mm" wi="157.56mm" orientation="landscape" file="US20230007430A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="234.10mm" wi="157.56mm" orientation="landscape" file="US20230007430A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="234.53mm" wi="157.56mm" orientation="landscape" file="US20230007430A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="158.33mm" wi="130.13mm" orientation="landscape" file="US20230007430A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0002" num="0001">The present technology relates to a signal processing device, a signal processing method, and a program, and more particularly, to a signal processing device, a signal processing method, and a program that allow for prevention of distortion of a sound space.</p><heading id="h-0002" level="1">BACKGROUND ART</heading><p id="p-0003" num="0002">For example, in virtual reality (VR) or augmented reality (AR) using a head-mounted display, not only video but also sound is binaurally reproduced from headphones for enhanced immersion in some cases. Such sound reproduction is called sound VR or sound AR.</p><p id="p-0004" num="0003">Furthermore, regarding display of a video in a head-mounted display, a method for correcting a drawing direction on the basis of prediction of a head motion has been proposed for the purpose of improving VR sickness caused by a delay in a video processing system (see, for example, Patent Document 1).</p><heading id="h-0003" level="1">CITATION LIST</heading><heading id="h-0004" level="1">Patent Document</heading><p id="p-0005" num="0000"><ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0004">Patent Document 1: Japanese Patent Application Laid-Open No. 2019-28368</li></ul></p><heading id="h-0005" level="1">SUMMARY OF THE INVENTION</heading><heading id="h-0006" level="1">Problems to be Solved by the Invention</heading><p id="p-0006" num="0005">On the other hand, regarding binaural reproduction of sound in the head-mounted display, in a similar manner to the case of video, a processing delay causes a reproduced output to deviate from an intended direction.</p><p id="p-0007" num="0006">Moreover, while a light wave propagates instantaneously in the range of distance covered in VR, a sound wave propagates with a significant delay. Thus, in sound VR or sound AR, a deviation also occurs in the direction of the reproduced output, depending on a head motion of a listener and a propagation delay time.</p><p id="p-0008" num="0007">When such a deviation of the reproduced output associated with a processing delay or a motion of the listener's head occurs, a sound space that is supposed to be reproduced is distorted, and accurate sound reproduction cannot be achieved.</p><p id="p-0009" num="0008">The present technology has been made in view of such a situation, and allows for prevention of distortion of a sound space.</p><heading id="h-0007" level="1">Solutions to Problems</heading><p id="p-0010" num="0009">One aspect of the present technology provides a signal processing device including: a relative azimuth prediction unit configured to predict, on the basis of a delay time in accordance with a distance from a virtual sound source to a listener, a relative azimuth of the virtual sound source when a sound of the virtual sound source reaches the listener; and a BRIR generation unit configured to acquire a head-related transfer function of the relative azimuth for each one of a plurality of the virtual sound sources and generate a BRIR on the basis of a plurality of the acquired head-related transfer functions.</p><p id="p-0011" num="0010">One aspect of the present technology provides a signal processing method or a program including steps of: predicting, on the basis of a delay time in accordance with a distance from a virtual sound source to a listener, a relative azimuth of the virtual sound source when a sound of the virtual sound source reaches the listener; and acquiring a head-related transfer function of the relative azimuth for each one of a plurality of the virtual sound sources and generating a BRIR on the basis of a plurality of the acquired head-related transfer functions.</p><p id="p-0012" num="0011">In one aspect of the present technology, on the basis of a delay time in accordance with a distance from a virtual sound source to a listener, a relative azimuth of the virtual sound source when a sound of the virtual sound source reaches the listener is predicted; and a head-related transfer function of the relative azimuth is acquired for each one of a plurality of the virtual sound sources and a BRIR is generated on the basis of a plurality of the acquired head-related transfer functions.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0008" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram illustrating a display example of a three-dimensional bubble chart of an RIR.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a diagram illustrating the position of a virtual sound source perceived by a listener in a case where a head remains stationary.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram illustrating the position of the virtual sound source perceived by the listener when the head is rotating at a constant angular velocity.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram illustrating a BRIR correction in accordance with the rotation of the head.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram illustrating a configuration example of a signal processing device.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a diagram schematically illustrating an outline of prediction of a predicted relative azimuth.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a diagram illustrating an example of a timing chart at the time of generating a BRIR and an output signal.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a diagram illustrating an example of a timing chart at the time of generating a BRIR and an output signal.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a flowchart illustrating BRIR generation processing.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a diagram illustrating an effect of reducing the deviation of the relative azimuth of the virtual sound source.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a diagram illustrating an effect of reducing the deviation of the relative azimuth of the virtual sound source.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a diagram illustrating an effect of reducing the deviation of the relative azimuth of the virtual sound source.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a diagram illustrating a configuration example of a computer.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0009" level="1">MODE FOR CARRYING OUT THE INVENTION</heading><p id="p-0026" num="0025">An embodiment to which the present technology is applied will be described below with reference to the drawings.</p><heading id="h-0010" level="1">First Embodiment</heading><p id="p-0027" num="0026">&#x3c;Present Technology&#x3e;</p><p id="p-0028" num="0027">In the present technology, distortion (skew) of a sound space is corrected with the use of head angular velocity information and head angular acceleration information for more accurate sound reproduction.</p><p id="p-0029" num="0028">For example, in sound VR or sound AR, processing of convolving, with an input sound source, a binaural-room impulse response (BRIR) obtained by convolving a head-related impulse response (HRIR) with a room impulse response (RIR) is performed.</p><p id="p-0030" num="0029">Here, the RIR is information constituted by a transmission characteristic of sound in a predetermined space and the like. Furthermore, the HRIR is a head-related transfer function. In particular, a head related transfer function (HRTF), which is information regarding a frequency domain for adding a transmission characteristic from an object (sound source) to each of the left and right ears of a listener, is expressed in time domain.</p><p id="p-0031" num="0030">The BRIR is an impulse response for reproducing sound (binaural sound) that would be heard by a listener in a case where a sound is emitted from an object in a predetermined space.</p><p id="p-0032" num="0031">The RIR is constituted by information regarding each one of a plurality of virtual sound sources such as a direct sound and an indirect sound, and each virtual sound source has different attributes such as spatial coordinates and intensity.</p><p id="p-0033" num="0032">For example, when one object (audio object) emits a sound in a space, a listener hears a direct sound and an indirect sound (reflected sound) from the object.</p><p id="p-0034" num="0033">When each of such a direct sound and an indirect sound is regarded as one virtual sound source, it can be said that the object is constituted by a plurality of virtual sound sources, and information constituted by a transmission characteristic of the sound of each one of the plurality of virtual sound sources and the like is the RIR of the object.</p><p id="p-0035" num="0034">In general, in a technology for reproducing a BRIR in accordance with a head azimuth of a listener by a head tracking, a BRIR measured or calculated for each head azimuth in a state where the listener's head remains stationary is held in a coefficient memory or the like. Then, at the time of sound reproduction, a BRIR held in the coefficient memory or the like is selected and used in accordance with head azimuth information from a sensor.</p><p id="p-0036" num="0035">However, such a method is based on the premise that the listener's head remains stationary, and is not capable of accurately reproducing the sound space during a head motion.</p><p id="p-0037" num="0036">Specifically, for example, in a case where sounds are simultaneously emitted from two virtual sound sources, there is a delay of about one second between reproduction of the sound from the one of the virtual sound sources that is located at a shorter distance, such as at a distance of 1 m from the listener, and reproduction of the sound from the virtual sound source located at a longer distance, such as at a distance of 340 m from the listener.</p><p id="p-0038" num="0037">However, in a general head tracking, sound signals of these two virtual sound sources are convolved with a BRIR of one azimuth selected on the basis of the same head azimuth information.</p><p id="p-0039" num="0038">Thus, in a state where the listener's head remains stationary, the azimuths of these two virtual sound sources with respect to the listener are correct. However, in a state where the head azimuth changes in accordance with a head motion during one second, the azimuths of the two virtual sound sources with respect to the listener are not correct, and a deviation occurs also in a relative azimuth relationship therebetween. This is perceived by the listener as distortion of the sound space, and has caused a problem in grasping the sound space by hearing.</p><p id="p-0040" num="0039">Therefore, in the present technology, BRIR combining processing (rendering) corresponding to a head tracking is performed with the use of head angular velocity information and head angular acceleration information in addition to head angle information, which is sensor information used in a general head tracking.</p><p id="p-0041" num="0040">With this arrangement, distortion (skew) of a sound space perceived when a listener (user) rotates the listener's head is corrected, which has not been possible with a general head tracking.</p><p id="p-0042" num="0041">Specifically, on the basis of information regarding the time required for propagation between the listener and each virtual sound source used for BRIR rendering and information regarding a delay in processing of convolution operation, a delay time from when head rotational motion information for the BRIR rendering is acquired until the sound from the virtual sound source reaches the listener is calculated.</p><p id="p-0043" num="0042">Then, at the time of the BRIR rendering, the relative azimuth is corrected in advance so that each virtual sound source may exist in a predicted relative azimuth at a time in the future delayed by that delay time. Thus, an azimuth deviation of each virtual sound source is corrected, in which the generation amount is determined depending on the distance to the virtual sound source and a pattern of the head rotational motion.</p><p id="p-0044" num="0043">For example, in a general head tracking, a BRIR measured or calculated for each head azimuth is held in a coefficient memory or the like, and the BRIR is selected and used in accordance with head azimuth information from a sensor.</p><p id="p-0045" num="0044">On the other hand, BRIRs are successively combined by rendering in the present technology.</p><p id="p-0046" num="0045">That is, information of all virtual sound sources is held in a memory as RIRs independently from each other, and the BRIRs are reconstructed with the use of an HRIR entire circumference database and head rotational motion information.</p><p id="p-0047" num="0046">Since a relative azimuth of a virtual sound source from a listener during a head rotational motion depends also on the distance from the listener to the virtual sound source, it is necessary to correct the relative azimuth independently for each virtual sound source.</p><p id="p-0048" num="0047">In a general technique, only BRIRs in a state where the head remains stationary have been able to be accurately reproduced in principle. In the present technology, the relative azimuth is corrected independently for each virtual sound source by BRIR rendering, so that the sound space during the head rotational motion can be reproduced more accurately.</p><p id="p-0049" num="0048">Furthermore, a relative azimuth prediction unit is incorporated in a BRIR generation processing unit that performs the above-described BRIR rendering. The relative azimuth prediction unit accepts three inputs: information regarding the time required for propagation to the listener, which is an attribute of each virtual sound source; head angle information, head angular velocity information, and head angular acceleration information from a sensor; and processing latency information of a convolution signal processing unit.</p><p id="p-0050" num="0049">By incorporating the relative azimuth prediction unit, it is possible to individually predict the relative azimuth of each virtual sound source when the sound of the virtual sound source reaches the listener, so that the optimum azimuth is corrected for each virtual sound source at the time of BRIR rendering. With this arrangement, a perception that a sound space is distorted during a head rotational motion is prevented.</p><p id="p-0051" num="0050">Now, the present technology will be described below in more detail.</p><p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a display example of a three-dimensional bubble chart of an RIR.</p><p id="p-0053" num="0052">In <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the origin of orthogonal coordinates is located at the position of a listener, and one circle drawn in the drawing represents one virtual sound source.</p><p id="p-0054" num="0053">In particular, here, the position and size of each circle respectively represent the spatial position of the virtual sound source and the relative intensity of the virtual sound source from the listener's perspective, that is, the loudness of the sound of the virtual sound source heard by the listener.</p><p id="p-0055" num="0054">Furthermore, the distance from the origin of each virtual sound source corresponds to the propagation time it takes the sound of the virtual sound source to reach the listener.</p><p id="p-0056" num="0055">An RIR is constituted by such information regarding a plurality of virtual sound sources corresponding to one object that exists in a space.</p><p id="p-0057" num="0056">Here, an influence of a head motion of a listener on a plurality of virtual sound sources of an RIR will be described with reference to <figref idref="DRAWINGS">FIGS. <b>2</b> to <b>4</b></figref>. Note that, in <figref idref="DRAWINGS">FIGS. <b>2</b> to <b>4</b></figref>, the same reference numerals are given to portions that correspond to each other, and the description thereof will be omitted as appropriate.</p><p id="p-0058" num="0057">Hereinafter, a virtual sound source in which 0 is set as the value of an ID for identifying the virtual sound source and a virtual sound source in which n is set as the value of the ID will be described as an example, the virtual sound sources being included in the plurality of virtual sound sources illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0059" num="0058">For example, the virtual sound source with ID=0 in <figref idref="DRAWINGS">FIG. <b>1</b></figref> is relatively close to the listener, that is, has a relatively short distance from the origin.</p><p id="p-0060" num="0059">On the other hand, the virtual sound source with ID=n in <figref idref="DRAWINGS">FIG. <b>1</b></figref> is relatively far from the listener, that is, has a relatively long distance from the origin.</p><p id="p-0061" num="0060">Note that, hereinafter, the virtual sound source with ID=0 will also be referred to as a virtual sound source AD<b>0</b>, and the virtual sound source with ID=n will also be referred to as a virtual sound source ADn.</p><p id="p-0062" num="0061"><figref idref="DRAWINGS">FIG. <b>2</b></figref> schematically illustrates the position of the virtual sound source perceived by the listener in a case where the listener's head remains stationary. In particular, <figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates a listener U<b>11</b> as viewed from above.</p><p id="p-0063" num="0062">In the example illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the virtual sound source AD<b>0</b> is at a position P<b>11</b>, and the virtual sound source ADn is at a position P<b>12</b>. Therefore, the virtual sound source AD<b>0</b> and the virtual sound source ADn are located in front of the listener U<b>11</b>, and the listener U<b>11</b> perceives that the sound of the virtual sound source AD<b>0</b> and the sound of the virtual sound source ADn are heard from the front of the listener.</p><p id="p-0064" num="0063">Next, <figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates the positions of the virtual sound sources perceived by the listener U<b>11</b> when the head of the listener U<b>11</b> rotates counterclockwise at a constant angular velocity.</p><p id="p-0065" num="0064">In this example, the listener U<b>11</b> rotates the listener's head at a constant angular velocity in the direction indicated by an arrow W<b>11</b>, that is, in the counterclockwise direction in the drawing.</p><p id="p-0066" num="0065">Since BRIR rendering generally requires a large amount of processing, the BRIR is updated at an interval of several thousands to tens of thousands of samples. This corresponds to an interval of 0.1 seconds or more in terms of time.</p><p id="p-0067" num="0066">Thus, a delay occurs during a period from when a BRIR is updated and then the BRIR is subjected to convolution signal processing with an input sound source until a processed sound in which the BRIR has been reflected starts to be output. Then, the change in the azimuth of the virtual sound source due to the head motion during that period fails to be reflected in the BRIR.</p><p id="p-0068" num="0067">As a result, for example, in the example illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, a deviation of the azimuth (hereinafter also referred to as an azimuth deviation A<b>1</b>) by an amount represented by an area A<b>1</b> occurs. The azimuth deviation A<b>1</b> is distortion depending on a delay time T_proc of rendering to be described later and a delay time T_delay of convolution signal processing.</p><p id="p-0069" num="0068">Furthermore, also after the processed sound of the virtual sound source in which the BRIR has been reflected has started to be output, there is a time delay corresponding to the propagation delay of the sound of each virtual sound source during a period until the processed sound of each virtual sound source reaches the listener U<b>11</b>, that is, until the processed sound of each virtual sound source is reproduced by headphones or the like.</p><p id="p-0070" num="0069">Therefore, in a case where the head azimuth of the listener U<b>11</b> changes due to a head motion also during that period, this change in the head azimuth is not reflected in the BRIR, and a deviation of the azimuth (hereinafter also referred to as an azimuth deviation A<b>2</b>) represented by an area A<b>2</b> further occurs.</p><p id="p-0071" num="0070">The azimuth deviation A<b>2</b> is a distortion depending on the distance between the listener U<b>11</b> and the virtual sound source, and increases in proportion to the distance.</p><p id="p-0072" num="0071">The listener U<b>11</b> perceives the azimuth deviation A<b>1</b> and the azimuth deviation A<b>2</b> as distortion of a concentric sound space.</p><p id="p-0073" num="0072">Thus, in the example illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the sound of the virtual sound source AD<b>0</b> is reproduced in such a way that a sound image, which is supposed to be localized at the position P<b>11</b> as viewed from the listener U<b>11</b>, is actually localized at a position P<b>21</b>.</p><p id="p-0074" num="0073">Similarly, as for the virtual sound source ADn, a sound image, which is supposed to be localized at the position P<b>12</b> as viewed from the listener U<b>11</b>, is actually localized at a position P<b>22</b>.</p><p id="p-0075" num="0074">Thus, in the present technology, as illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the relative azimuth of each virtual sound source viewed from the listener U<b>11</b> is corrected in advance to be a predicted azimuth (hereinafter also referred to as a predicted relative azimuth) at the time when the sound of each virtual sound source reaches the listener U<b>11</b>, and then BRIR rendering is performed.</p><p id="p-0076" num="0075">With this arrangement, the deviation of the azimuth of each virtual sound source and the distortion of the sound space caused by the rotation of the head of the listener U<b>11</b> are corrected. In other words, distortion of the sound space is prevented. As a result, more accurate sound reproduction can be achieved.</p><p id="p-0077" num="0076">Here, the relative azimuth of a virtual sound source is an azimuth indicating the relative position (direction) of the virtual sound source with respect to the front direction of the listener U<b>11</b>. That is, the relative azimuth of the virtual sound source is angle information indicating the apparent position (direction) of the virtual sound source viewed from the listener U<b>11</b>.</p><p id="p-0078" num="0077">For example, the relative azimuth of the virtual sound source is represented by an azimuth angle indicating the position of the virtual sound source defined with the front direction of the listener U<b>11</b> as the origin of polar coordinates. Here, in particular, the relative azimuth of the virtual sound source obtained by prediction, that is, a predicted value (estimated value) of the relative azimuth is referred to as a predicted relative azimuth.</p><p id="p-0079" num="0078">In the example in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the relative azimuth of the virtual sound source AD<b>0</b> is corrected by an amount indicated by an arrow W<b>21</b> to be a predicted relative azimuth Ac(<b>0</b>), and the relative azimuth of the virtual sound source ADn is corrected by an amount indicated by an arrow W<b>22</b> to be a predicted relative azimuth Ac(n).</p><p id="p-0080" num="0079">Therefore, at the time of sound reproduction, the sound images of the virtual sound source AD<b>0</b> and the virtual sound source ADn are localized in the correct directions (azimuths) as viewed from the listener U<b>11</b>.</p><heading id="h-0011" level="1">Configuration Example of Signal Processing Device</heading><p id="p-0081" num="0080"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram illustrating a configuration example of one embodiment of a signal processing device to which the present technology is applied.</p><p id="p-0082" num="0081">In <figref idref="DRAWINGS">FIG. <b>5</b></figref>, a signal processing device <b>11</b> is constituted by, for example, headphones, a head-mounted display, and the like, and includes a BRIR generation processing unit <b>21</b> and a convolution signal processing unit <b>22</b>.</p><p id="p-0083" num="0082">In the signal processing device <b>11</b>, the BRIR generation processing unit <b>21</b> performs BRIR rendering.</p><p id="p-0084" num="0083">Furthermore, the convolution signal processing unit <b>22</b> performs convolution signal processing of an input signal, which is a sound signal of an object that has been input, and a BRIR generated by the BRIR generation processing unit <b>21</b>, and generates an output signal for reproducing a direct sound, an indirect sound, and the like of the object.</p><p id="p-0085" num="0084">Note that, in the following description, it is assumed that N virtual sound sources exist as virtual sound sources corresponding to an object, and an i-th (where 0&#x2264;i&#x2264;N-1) virtual sound source is also referred to as a virtual sound source i. The virtual sound source i is a virtual sound source with ID=i.</p><p id="p-0086" num="0085">Furthermore, here, input signals of M channels are input to the convolution signal processing unit <b>22</b>, and an input signal of an m-th (where 1&#x2264;m&#x2264;M) channel (channel m) is also referred to as an input signal m. These input signals m are sound signals for reproducing the sound of the object.</p><p id="p-0087" num="0086">The BRIR generation processing unit <b>21</b> includes a sensor unit <b>31</b>, a virtual sound source counter <b>32</b>, an RIR database memory <b>33</b>, a relative azimuth prediction unit <b>34</b>, an HRIR database memory <b>35</b>, an attribute application unit <b>36</b>, a left ear cumulative addition unit <b>37</b>, and a right ear cumulative addition unit <b>38</b>.</p><p id="p-0088" num="0087">Furthermore, the convolution signal processing unit <b>22</b> includes a left ear convolution signal processing unit <b>41</b>-<b>1</b> to a left ear convolution signal processing unit <b>41</b>-M, a right ear convolution signal processing unit <b>42</b>-<b>1</b> to a right ear convolution signal processing unit <b>42</b>-M, an addition unit <b>43</b>, and an addition unit <b>44</b>.</p><p id="p-0089" num="0088">Note that, hereinafter, the left ear convolution signal processing unit <b>41</b>-<b>1</b> to the left ear convolution signal processing unit <b>41</b>-M will also be simply referred to as left ear convolution signal processing units <b>41</b> in a case where it is not particularly necessary to distinguish between them.</p><p id="p-0090" num="0089">Similarly, hereinafter, the right ear convolution signal processing unit <b>42</b>-<b>1</b> to the right ear convolution signal processing unit <b>42</b>-M will also be simply referred to as right ear convolution signal processing units <b>42</b> in a case where it is not particularly necessary to distinguish between them.</p><p id="p-0091" num="0090">The sensor unit <b>31</b> is constituted by, for example, an angular velocity sensor, an angular acceleration sensor, or the like attached to the head of a user who is a listener. The sensor unit <b>31</b> acquires, by measurement, head rotational motion information, which is information regarding a movement of the listener's head, that is, a rotational motion of the head, and supplies the information to the relative azimuth prediction unit <b>34</b>.</p><p id="p-0092" num="0091">Here, the head rotational motion information includes, for example, at least one of head angle information As, head angular velocity information Bs, or head angular acceleration information Cs.</p><p id="p-0093" num="0092">The head angle information As is angle information indicating a head azimuth, which is an absolute head orientation (direction) of a listener in a space.</p><p id="p-0094" num="0093">For example, the head angle information As is represented by an azimuth angle indicating the orientation of the head (head azimuth) of the listener defined using, as the origin of polar coordinates, a predetermined direction in a space such as a room where the listener is.</p><p id="p-0095" num="0094">The head angular velocity information Bs is information indicating the angular velocity of a movement of the listener's head, and the head angular acceleration information Cs is information indicating the angular acceleration of the movement of the listener's head.</p><p id="p-0096" num="0095">Note that an example in which the head rotational motion information includes the head angle information As, the head angular velocity information Bs, and the head angular acceleration information Cs will be described below. However, the head rotational motion information may not include the head angular velocity information Bs or the head angular acceleration information Cs, or may include another piece of information indicating the movement (rotational motion) of the listener's head.</p><p id="p-0097" num="0096">For example, the head angular acceleration information Cs is only required to be used in a case where the head angular acceleration information Cs can be acquired. In a case where the head angular acceleration information Cs can be used, the relative azimuth can be predicted with higher accuracy, but, in essence, the head angular acceleration information Cs is not necessarily required.</p><p id="p-0098" num="0097">Furthermore, the angular velocity sensor for obtaining the head angular velocity information Bs is not limited to a general vibration gyro sensor, but may be of any detection principle such as one using an image, ultrasonic waves, a laser, or the like.</p><p id="p-0099" num="0098">The virtual sound source counter <b>32</b> generates count values in order from <b>1</b> up to a maximum number N of virtual sound sources included in an RIR database, and supplies the count values to the RIR database memory <b>33</b>.</p><p id="p-0100" num="0099">The RIR database memory <b>33</b> holds the RIR database. In the RIR database, a generation time T(i), a generation azimuth A(i), attribute information, and the like for each virtual sound source i are recorded in association with each other as an RIR, that is, transmission characteristics of a predetermined space.</p><p id="p-0101" num="0100">Here, the generation time T(i) indicates the time at which a sound of the virtual sound source i is generated, for example, the reproduction start time of the sound of the virtual sound source i in an output signal frame.</p><p id="p-0102" num="0101">The generation azimuth A(i) indicates an absolute azimuth (direction) of the virtual sound source i in the space, that is, angle information such as an azimuth angle indicating an absolute generation position of the sound of the virtual sound source i.</p><p id="p-0103" num="0102">Furthermore, the attribute information is information indicating characteristics of the virtual sound source i such as intensity (loudness) and a frequency characteristic of the sound of the virtual sound source i.</p><p id="p-0104" num="0103">The RIR database memory <b>33</b> uses a count value supplied from the virtual sound source counter <b>32</b> as a retrieval key to retrieve and read, from the RIR database that is held, the generation time T(i), the generation azimuth A(i), and the attribute information of the virtual sound source i indicated by the count value.</p><p id="p-0105" num="0104">The RIR database memory <b>33</b> supplies the generation time T(i) and the generation azimuth A(i) that have been read to the relative azimuth prediction unit <b>34</b>, supplies the generation time T(i) to the left ear cumulative addition unit <b>37</b> and the right ear cumulative addition unit <b>38</b>, and supplies the attribute information to the attribute application unit <b>36</b>.</p><p id="p-0106" num="0105">The relative azimuth prediction unit <b>34</b> predicts a predicted relative azimuth Ac(i) of the virtual sound source i on the basis of the head rotational motion information supplied from the sensor unit <b>31</b> and the generation time T(i) and the generation azimuth A(i) supplied from the RIR database memory <b>33</b>.</p><p id="p-0107" num="0106">Here, the predicted relative azimuth Ac(i) is a predicted value of a relative direction (azimuth) of the virtual sound source i with respect to the listener at the time when the sound of the virtual sound source i reaches the user who is the listener, that is, a predicted value of the relative azimuth of the virtual sound source i viewed from the listener.</p><p id="p-0108" num="0107">In other words, the predicted relative azimuth Ac(i) is a predicted value of the relative azimuth of the virtual sound source i at the time when the sound of the virtual sound source i is reproduced by an output signal, that is, at the time when the sound of the virtual sound source i is actually presented to the listener.</p><p id="p-0109" num="0108"><figref idref="DRAWINGS">FIG. <b>6</b></figref> schematically illustrates an outline of prediction of the predicted relative azimuth Ac(i).</p><p id="p-0110" num="0109">Note that, in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, a vertical axis represents the absolute azimuth in the front direction of the listener's head, that is, the head azimuth, and a horizontal axis represents the time.</p><p id="p-0111" num="0110">In this example, a curve L<b>11</b> indicates the actual movement of the listener's head, that is, the change in the actual head azimuth.</p><p id="p-0112" num="0111">For example, at time t<b>0</b> at which the head angle information As or the like is acquired by the sensor unit <b>31</b>, the head azimuth of the listener is the azimuth indicated by the head angle information As.</p><p id="p-0113" num="0112">Furthermore, although the actual head azimuth of the listener after time t<b>0</b> is unknown at the point of time t<b>0</b>, the head azimuth after time t<b>0</b> is predicted on the basis of the head angle information As, the head angular velocity information Bs, and the head angular acceleration information Cs at time t<b>0</b>.</p><p id="p-0114" num="0113">Here, an arrow B<b>11</b> represents the angular velocity indicated by the head angular velocity information Bs acquired at time t<b>0</b>, and an arrow B<b>12</b> represents the angular acceleration indicated by the head angular acceleration information Cs acquired at time to. Furthermore, a curve L<b>12</b> represents a result of prediction of the head azimuth of the listener after time t<b>0</b> estimated at the point of time t<b>0</b>.</p><p id="p-0115" num="0114">For example, Tc(<b>0</b>) is set as a delay time from when the sensor unit <b>31</b> acquires head rotational motion information, which is obtained for the virtual sound source AD<b>0</b> with ID=0, that is, i=0th, until the sound of the virtual sound source AD<b>0</b> reaches the listener.</p><p id="p-0116" num="0115">In this case, the value of the curve L<b>12</b> at time t<b>0</b>+Tc(<b>0</b>) is the predicted value of the head azimuth when the listener actually listens to the sound of the virtual sound source AD<b>0</b>.</p><p id="p-0117" num="0116">Therefore, the difference between the head azimuth and the head azimuth indicated by the head angle information As is expressed by Ac(<b>0</b>)&#x2212;{A(<b>0</b>)&#x2212;As}.</p><p id="p-0118" num="0117">Similarly, for example, when the delay time of the virtual sound source ADn with ID=n is expressed by Tc(n), the difference between the value of the curve L<b>12</b> at time t<b>0</b>+Tc(n) and the head azimuth indicated by the head angle information As is expressed by Ac(n)&#x2212;{A(n)&#x2212;As}.</p><p id="p-0119" num="0118">Returning to the description of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, more specifically, when obtaining the predicted relative azimuth Ac(i), the relative azimuth prediction unit <b>34</b> first calculates the following Equation (1) on the basis of the generation time T(i) to calculate a delay time Tc(i) of the virtual sound source i.</p><p id="p-0120" num="0119">The delay time Tc(i) is the time from when the sensor unit <b>31</b> acquires the head rotational motion information of the listener's head until the sound of the virtual sound source i reaches the listener.</p><p id="p-0121" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>Tc</i>(<i>i</i>)=<i>T</i>_proc+<i>T</i>_delay+<i>T</i>(<i>i</i>)&#x2003;&#x2003;(1)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0122" num="0120">Note that, in Equation (1), T_proc indicates a delay time due to processing of generating (updating) a BRIR.</p><p id="p-0123" num="0121">More specifically, T_proc indicates the delay time from when the sensor unit <b>31</b> acquires head rotational motion information until a BRIR is updated and application of the BRIR is started in the left ear convolution signal processing unit <b>41</b> and the right ear convolution signal processing unit <b>42</b>.</p><p id="p-0124" num="0122">Furthermore, in Equation (1), T_delay indicates a delay time due to convolution signal processing of the BRIR.</p><p id="p-0125" num="0123">More specifically, T_delay indicates a delay time from when application of the BRIR is started in the left ear convolution signal processing unit <b>41</b> and the right ear convolution signal processing unit <b>42</b>, that is, from when convolution signal processing is started, until start of reproduction of the beginning of the output signal (the beginning of the frame) corresponding to a result of the processing. In particular, the delay time T_delay is determined by an algorithm of the convolution signal processing of the BRIR and a sampling frequency and a frame size of the output signal.</p><p id="p-0126" num="0124">A sum of the delay time T_proc and the delay time T_delay corresponds to the above-described azimuth deviation A<b>1</b> in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, and the generation time T(i) corresponds to the above-described azimuth deviation A<b>2</b> in <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0127" num="0125">When the delay time Tc(i) is obtained in this way, the relative azimuth prediction unit <b>34</b> calculates the predicted relative azimuth Ac(i) by calculating the following Equation (2) on the basis of the delay time Tc(i), the generation azimuth A(i), the head angle information As, the head angular velocity information Bs, and the head angular acceleration information Cs. Note that Equation (1) and Equation (2) may be calculated simultaneously.</p><p id="p-0128" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>Ac</i>(<i>i</i>)=<i>A</i>(<i>i</i>)&#x2212;{<i>As+Bs&#xd7;Tc</i>(<i>i</i>)+<i>Cs&#xd7;Tc</i>(<i>i</i>)<sup>2</sup>}&#x2003;&#x2003;[Math. 2]<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0129" num="0126">Furthermore, the method of predicting the predicted relative azimuth Ac(i) is not limited to the method described above, but may be any method. For example, the method may be combined with a technique such as multiple regression analysis using previous records of the head movement.</p><p id="p-0130" num="0127">The relative azimuth prediction unit <b>34</b> supplies the predicted relative azimuth Ac(i) obtained for the virtual sound source i to the HRIR database memory <b>35</b>.</p><p id="p-0131" num="0128">The HRIR database memory <b>35</b> holds an HRIR database including an HRIR (head-related transfer function) for each direction with the listener's head as the origin of polar coordinates. In particular, HRIRs in the HRIR database are impulse responses of two systems, an HRIR for the left ear and an HRIR for the right ear.</p><p id="p-0132" num="0129">The HRIR database memory <b>35</b> retrieves and reads, from the HRIR database, HRIRs in the direction indicated by the predicted relative azimuth Ac(i) supplied from the relative azimuth prediction unit <b>34</b>, and supplies the read HRIRs, that is, the HRIR for the left ear and the HRIR for the right ear, to the attribute application unit <b>36</b>.</p><p id="p-0133" num="0130">The attribute application unit <b>36</b> acquires the HRIRs output from the HRIR database memory <b>35</b>, and adds a transmission characteristic for the virtual sound source i to the acquired HRIRs on the basis of the attribute information.</p><p id="p-0134" num="0131">Specifically, on the basis of the attribute information from the RIR database memory <b>33</b>, the attribute application unit <b>36</b> performs signal processing such as gain calculation or digital filter processing by a finite impulse response (FIR) filter or the like on the HRIRs from the HRIR database memory <b>35</b>.</p><p id="p-0135" num="0132">The attribute application unit <b>36</b> supplies the HRIRs for the left ear obtained as a result of the signal processing to the left ear cumulative addition unit <b>37</b>, and supplies the HRIRs for the right ear to the right ear cumulative addition unit <b>38</b>.</p><p id="p-0136" num="0133">On the basis of the generation time T(i) of the virtual sound source i supplied from the RIR database memory <b>33</b>, the left ear cumulative addition unit <b>37</b> cumulatively adds the HRIRs for the left ear supplied from the attribute application unit <b>36</b> in a data buffer having the same length as data of the BRIR for the left ear to be finally output.</p><p id="p-0137" num="0134">At this time, the address (position) of the data buffer at which the cumulative addition of the HRIRs for the left ear is started is an address corresponding to the generation time T(i) of the virtual sound source i, more specifically, an address corresponding to a value obtained by multiplying the generation time T(i) by the sampling frequency of the output signal.</p><p id="p-0138" num="0135">While the count values of 1 to N are output by the virtual sound source counter <b>32</b>, the above-described cumulative addition is performed. With this arrangement, the HRIRs for the left ear of the N virtual sound sources are added (combined), and a final BRIR for the left ear is obtained.</p><p id="p-0139" num="0136">The left ear cumulative addition unit <b>37</b> supplies the BRIR for the left ear to the left ear convolution signal processing unit <b>41</b>.</p><p id="p-0140" num="0137">Similarly, on the basis of the generation time T(i) of the virtual sound source i supplied from the RIR database memory <b>33</b>, the right ear cumulative addition unit <b>38</b> cumulatively adds the HRIRs for the right ear supplied from the attribute application unit <b>36</b> in a data buffer having the same length as data of the BRIR for the right ear to be finally output.</p><p id="p-0141" num="0138">Also in this case, the address (position) of the data buffer at which the cumulative addition of the HRIRs for the right ear is started is an address corresponding to the generation time T(i) of the virtual sound source i.</p><p id="p-0142" num="0139">The right ear cumulative addition unit <b>38</b> supplies the right ear convolution signal processing unit <b>42</b> with the BRIR for the right ear obtained by cumulative addition of the HRIRs for the right ear.</p><p id="p-0143" num="0140">The attribute application unit <b>36</b> to the right ear cumulative addition unit <b>38</b> perform processing of generating a BRIR for an object by adding, to an HRIR, a transmission characteristic indicated by attribute information of a virtual sound source and combining the HRIRs to which the transmission characteristics obtained one for each virtual sound source have been added. This processing corresponds to processing of convolving an HRIR and an RIR.</p><p id="p-0144" num="0141">Therefore, it can be said that the block constituted by the attribute application unit <b>36</b> to the right ear cumulative addition unit <b>38</b> functions as a BRIR generation unit that generates a BRIR by adding a transmission characteristic of a virtual sound source to an HRIR and combining the HRIRs to which the transmission characteristics have been added.</p><p id="p-0145" num="0142">Note that, since the RIR database is different from channel to channel of the input signal, the BRIR is generated for each channel of the input signal.</p><p id="p-0146" num="0143">Therefore, more specifically, the BRIR generation processing unit <b>21</b> is provided with the RIR database memory <b>33</b> for each channel m (where 1&#x2264;m&#x2264;M) of the input signal, for example.</p><p id="p-0147" num="0144">Then, the RIR database memory <b>33</b> is switched for each channel m and the above-described processing is performed, and thus a BRIR of each channel m is generated.</p><p id="p-0148" num="0145">The convolution signal processing unit <b>22</b> performs convolution signal processing of the BRIR and the input signal to generate an output signal.</p><p id="p-0149" num="0146">That is, a left ear convolution signal processing unit <b>41</b>-<i>m </i>(where 1 m M) convolves a supplied input signal m and a BRIR for the left ear supplied from the left ear cumulative addition unit <b>37</b>, and supplies an output signal for the left ear obtained as a result to the addition unit <b>43</b>.</p><p id="p-0150" num="0147">Similarly, a right ear convolution signal processing unit <b>42</b>-<i>m </i>(where 1 m M) convolves a supplied input signal m and a BRIR for the right ear supplied from the right ear cumulative addition unit <b>38</b>, and supplies an output signal for the right ear obtained as a result to the addition unit <b>44</b>.</p><p id="p-0151" num="0148">The addition unit <b>43</b> adds the output signals supplied from the left ear convolution signal processing units <b>41</b>, and outputs a final output signal for the left ear obtained as a result.</p><p id="p-0152" num="0149">The addition unit <b>44</b> adds the output signals supplied from the right ear convolution signal processing units <b>42</b>, and outputs a final output signal for the right ear obtained as a result.</p><p id="p-0153" num="0150">The output signals obtained by the addition unit <b>43</b> and the addition unit <b>44</b> in this way are sound signals for reproducing a sound of each one of a plurality of virtual sound sources corresponding to the object.</p><p id="p-0154" num="0151">&#x3c;Generation of BRIR&#x3e;</p><p id="p-0155" num="0152">Here, generation of a BRIR and generation of an output signal with the use of the BRIR will be described.</p><p id="p-0156" num="0153"><figref idref="DRAWINGS">FIGS. <b>7</b> and <b>8</b></figref> illustrate examples of a timing chart at the time of generation of a BRIR and an output signal. In particular, here, an example in which Overlap-Add method is used for convolution signal processing of an input signal and a BRIR is illustrated.</p><p id="p-0157" num="0154">Note that, in <figref idref="DRAWINGS">FIGS. <b>7</b> and <b>8</b></figref>, the same reference numerals are given to the corresponding portions, and the description thereof will be omitted as appropriate. Furthermore, in <figref idref="DRAWINGS">FIGS. <b>7</b> and <b>8</b></figref>, the horizontal direction indicates the time.</p><p id="p-0158" num="0155"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates a timing chart in a case where the BRIR is updated at a time interval equivalent to the time frame size of the convolution signal processing of the BRIR, that is, the length of an input signal frame.</p><p id="p-0159" num="0156">For example, a portion indicated by an arrow Q<b>11</b> indicates a timing at which a BRIR is generated. In the drawing, each of downward arrows in the portion indicated by the arrow Q<b>11</b> indicates a timing at which the sensor unit <b>31</b> acquires the head angle information As, that is, the head rotational motion information.</p><p id="p-0160" num="0157">Furthermore, each square in the portion indicated by the arrow Q<b>11</b> represents a period during which a k-th BRIR (hereinafter also referred to as a BRIRk) is generated, and here, the generation of the BRIR is started at the timing when the head angle information As is acquired.</p><p id="p-0161" num="0158">Specifically, for example, generation (update) of a BRIR <b>2</b> is started at time t<b>0</b>, and the processing of generating the BRIR <b>2</b> ends by time t<b>1</b>. That is, the BRIR <b>2</b> is obtained at the timing of time t<b>1</b>.</p><p id="p-0162" num="0159">Furthermore, a portion indicated by an arrow Q<b>12</b> indicates a timing of convolution signal processing of an input signal frame and a BRIR.</p><p id="p-0163" num="0160">For example, a period from time t<b>1</b> to time t<b>2</b> is a period of an input signal frame <b>2</b>, and this period is when the input signal frame <b>2</b> and the BRIR <b>2</b> are convolved.</p><p id="p-0164" num="0161">Therefore, focusing on the input signal frame <b>2</b> and the BRIR <b>2</b>, the time from time t<b>0</b> at which generation of the BRIR <b>2</b> is started to time t<b>1</b> from which convolution of the BRIR <b>2</b> can be started is the above-described delay time T_proc.</p><p id="p-0165" num="0162">Furthermore, convolution and overlap-add of the input signal frame <b>2</b> and the BRIR <b>2</b> are performed during the period from time t<b>1</b> to time t<b>2</b>, and an output signal frame <b>2</b> starts to be output at time t<b>2</b>. Such a time from time t<b>1</b> to time t<b>2</b> is the delay time T_delay.</p><p id="p-0166" num="0163">A portion indicated by an arrow Q<b>13</b> illustrates an output signal block (frame) before the overlap-add, and a portion indicated by an arrow Q<b>14</b> illustrates a final output signal frame obtained by the overlap-add.</p><p id="p-0167" num="0164">That is, each square in the portion indicated by the arrow Q<b>13</b> represents one block of the output signal before the overlap-add obtained by the convolution between the input signal and the BRIR.</p><p id="p-0168" num="0165">On the other hand, each square in the portion indicated by the arrow Q<b>14</b> represents one frame of the final output signal obtained by the overlap-add.</p><p id="p-0169" num="0166">At the time of overlap-add, two neighboring output signal blocks are added, and one final frame of the output signal is obtained.</p><p id="p-0170" num="0167">For example, an output signal block <b>2</b> is constituted by a signal obtained by convolution between the input signal frame <b>2</b> and the BRIR <b>2</b>. Then, overlap-add of the second half of an output signal block <b>1</b> and the first half of the block <b>2</b> following the output signal block <b>1</b> is performed, and a final output signal frame <b>2</b> is obtained.</p><p id="p-0171" num="0168">Here, focusing on a predetermined virtual sound source i reproduced by the output signal frame <b>2</b>, the sum of the delay time T_proc, the delay time T_delay, and the generation time T(i) for the virtual sound source i is the above-described delay time Tc(i).</p><p id="p-0172" num="0169">Therefore, it can be seen that the delay time Tc(i) for the input signal frame <b>2</b> corresponding to the output signal frame <b>2</b> is the time from time t<b>0</b> to time t<b>3</b>, for example.</p><p id="p-0173" num="0170">Furthermore, <figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates a timing chart in a case where the BRIR is updated at a time interval equivalent to twice the time frame size of the convolution signal processing of the BRIR, that is, the length of the input signal frame.</p><p id="p-0174" num="0171">For example, a portion indicated by an arrow Q<b>21</b> indicates a timing at which a BRIR is generated, and a portion indicated by an arrow Q<b>22</b> indicates a timing of convolution signal processing of an input signal frame and the BRIR.</p><p id="p-0175" num="0172">Furthermore, a portion indicated by an arrow Q<b>23</b> illustrates an output signal block (frame) before overlap-add, and a portion indicated by an arrow Q<b>24</b> illustrates a final output signal frame obtained by the overlap-add.</p><p id="p-0176" num="0173">In particular, in this example, one BRIR is generated at a time interval of two frames of the input signal. Therefore, focusing on the BRIR <b>2</b> as an example, the BRIR <b>2</b> is used not only for convolution with the input signal frame <b>2</b> but also for convolution with an input signal frame <b>3</b>.</p><p id="p-0177" num="0174">Furthermore, the output signal block <b>2</b> is obtained by convolution between the BRIR <b>2</b> and the input signal frame <b>2</b>, and overlap-add of the first half of the output signal block <b>2</b> and the second half of the block <b>1</b> immediately before the block <b>2</b> is performed, and thus a final output signal frame <b>2</b> is obtained.</p><p id="p-0178" num="0175">Also in such an output signal frame <b>2</b>, in a similar manner to the case in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the time from time t<b>0</b> at which generation of the BRIR <b>2</b> is started to time t<b>3</b> indicated by the generation time T(i) for the virtual sound source i is the delay time Tc(i) for the virtual sound source i.</p><p id="p-0179" num="0176">Note that <figref idref="DRAWINGS">FIGS. <b>7</b> and <b>8</b></figref> illustrate examples in which Overlap-Add method is used as the convolution signal processing, but the present invention is not limited thereto, and Overlap-Save method, time domain convolution processing, or the like may be used. Even in such a case, only the delay time T_delay is different, and an appropriate BRIR can be generated and an output signal can be obtained in a similar manner to the case of Overlap-Add method.</p><p id="p-0180" num="0177">&#x3c;Description of BRIR Generation Processing&#x3e;</p><p id="p-0181" num="0178">Next, an operation of the signal processing device <b>11</b> will be described.</p><p id="p-0182" num="0179">When an input signal starts to be supplied, the signal processing device <b>11</b> performs BRIR generation processing, generates a BRIR, performs convolution signal processing, and outputs an output signal. The BRIR generation processing by the signal processing device <b>11</b> will be described below with reference to a flowchart in <figref idref="DRAWINGS">FIG. <b>9</b></figref>.</p><p id="p-0183" num="0180">In step S<b>11</b>, the BRIR generation processing unit <b>21</b> acquires the maximum number N of virtual sound sources in the RIR database from the RIR database memory <b>33</b>, and supplies the maximum number N of virtual sound sources to the virtual sound source counter <b>32</b> to cause the virtual sound source counter <b>32</b> to start outputting a count value.</p><p id="p-0184" num="0181">When the count value is supplied from the virtual sound source counter <b>32</b>, the RIR database memory <b>33</b> reads, from the RIR database, and outputs the generation time T(i), the generation azimuth A(i), and the attribute information of the virtual sound source i indicated by the count value for each channel of the input signal.</p><p id="p-0185" num="0182">In step S<b>12</b>, the relative azimuth prediction unit <b>34</b> acquires the delay time T_delay determined in advance.</p><p id="p-0186" num="0183">In step S<b>13</b>, the left ear cumulative addition unit <b>37</b> and the right ear cumulative addition unit <b>38</b> initialize, to <b>0</b>, values held in BRIR data buffers of the M channels that are held.</p><p id="p-0187" num="0184">In step S<b>14</b>, the sensor unit <b>31</b> acquires head rotational motion information, and supplies the head rotational motion information to the relative azimuth prediction unit <b>34</b>.</p><p id="p-0188" num="0185">For example, in step S<b>14</b>, information indicating a movement of the listener's head including the head angle information As, the head angular velocity information Bs, and the head angular acceleration information Cs is acquired as the head rotational motion information.</p><p id="p-0189" num="0186">In step S<b>15</b>, the relative azimuth prediction unit <b>34</b> acquires the head angle information As in the sensor unit <b>31</b>, that is, acquisition time t<b>0</b> of the head rotational motion information.</p><p id="p-0190" num="0187">In step S<b>16</b>, the relative azimuth prediction unit <b>34</b> sets a scheduled application start time of the next BRIR, that is, scheduled start time t<b>1</b> of convolution between the BRIR and the input signal.</p><p id="p-0191" num="0188">In step S<b>17</b>, the relative azimuth prediction unit <b>34</b> calculates the delay time T_proc=t<b>1</b>-t<b>0</b> on the basis of acquisition time t<b>0</b> and time t<b>1</b>.</p><p id="p-0192" num="0189">In step S<b>18</b>, the relative azimuth prediction unit <b>34</b> acquires the generation time T(i) of the virtual sound source i output from the RIR database memory <b>33</b>.</p><p id="p-0193" num="0190">Furthermore, in step S<b>19</b>, the relative azimuth prediction unit <b>34</b> acquires the generation azimuth A(i) of the virtual sound source i output from the RIR database memory <b>33</b>.</p><p id="p-0194" num="0191">In step S<b>20</b>, the relative azimuth prediction unit <b>34</b> calculates Equation (1) described above on the basis of the delay time T_delay acquired in step S<b>12</b>, the delay time T_proc obtained in step S<b>17</b>, and the generation time T(i) acquired in step S<b>18</b> to calculate the delay time Tc(i) of the virtual sound source i.</p><p id="p-0195" num="0192">In step S<b>21</b>, the relative azimuth prediction unit <b>34</b> calculates the predicted relative azimuth Ac(i) of the virtual sound source i and supplies the predicted relative azimuth Ac(i) to the HRIR database memory <b>35</b>.</p><p id="p-0196" num="0193">For example, in step S<b>21</b>, Equation (2) described above is calculated on the basis of the delay time Tc(i) calculated in step S<b>20</b>, the head rotational motion information acquired in step S<b>14</b>, and the generation azimuth A(i) acquired in step S<b>19</b>, and thus the predicted relative azimuth Ac(i) is calculated.</p><p id="p-0197" num="0194">Furthermore, the HRIR database memory <b>35</b> reads, from the HRIR database, and outputs the HRIR in the direction indicated by the predicted relative azimuth Ac(i) supplied from the relative azimuth prediction unit <b>34</b>. With this arrangement, the HRIR of each of the left and right ears in accordance with the predicted relative azimuth Ac(i) indicating the positional relationship between the listener and the virtual sound source i in consideration of the rotation of the head is output.</p><p id="p-0198" num="0195">In step S<b>22</b>, the attribute application unit <b>36</b> acquires the HRIR for the left ear and the HRIR for the right ear in accordance with the predicted relative azimuth Ac(i) output from the HRIR database memory <b>35</b>.</p><p id="p-0199" num="0196">In step S<b>23</b>, the attribute application unit <b>36</b> acquires the attribute information of the virtual sound source i output from the RIR database memory <b>33</b>.</p><p id="p-0200" num="0197">In step S<b>24</b>, the attribute application unit <b>36</b> performs signal processing based on the attribute information acquired in step S<b>23</b> on the HRIR for the left ear and the HRIR for the right ear acquired in step S<b>22</b>.</p><p id="p-0201" num="0198">For example, in step S<b>24</b>, as the signal processing based on the attribute information, gain calculation (calculation for gain correction) is performed for the HRIRs on the basis of gain information determined by the intensity of the sound of the virtual sound source i as the attribute information.</p><p id="p-0202" num="0199">Furthermore, for example, as the signal processing based on the attribute information, digital filter processing or the like is performed for the HRIRs on the basis of a filter determined by a frequency characteristic as the attribute information.</p><p id="p-0203" num="0200">The attribute application unit <b>36</b> supplies the HRIR for the left ear obtained by the signal processing to the left ear cumulative addition unit <b>37</b>, and supplies the HRIR for the right ear to the right ear cumulative addition unit <b>38</b>.</p><p id="p-0204" num="0201">In step S<b>25</b>, the left ear cumulative addition unit <b>37</b> and the right ear cumulative addition unit <b>38</b> perform cumulative addition of the HRIRs on the basis of the generation time T(i) of the virtual sound source i supplied from the RIR database memory <b>33</b>.</p><p id="p-0205" num="0202">Specifically, the left ear cumulative addition unit <b>37</b> cumulatively adds the HRIR for the left ear obtained in step S<b>24</b> to a value stored in the data buffer provided in the left ear cumulative addition unit <b>37</b>, that is, to the HRIR for the left ear that has been obtained by the cumulative addition so far.</p><p id="p-0206" num="0203">At this time, the HRIR for the left ear obtained in step S<b>24</b> and the value already stored in the data buffer are added so that the position of an address corresponding to the generation time T(i) in the data buffer is located at the beginning of the HRIR for the left ear to be cumulatively added, and the value obtained as a result is written back to the data buffer.</p><p id="p-0207" num="0204">Similarly to the case of the left ear cumulative addition unit <b>37</b>, the right ear cumulative addition unit <b>38</b> also cumulatively adds the HRIR for the right ear obtained in step S<b>24</b> to a value stored in the data buffer provided in the right ear cumulative addition unit <b>38</b>.</p><p id="p-0208" num="0205">The processing in steps S<b>18</b> to S<b>25</b> described above is performed for each channel of the input signal supplied to the convolution signal processing unit <b>22</b>.</p><p id="p-0209" num="0206">In step S<b>26</b>, the BRIR generation processing unit <b>21</b> determines whether or not the processing has been performed on all the N virtual sound sources.</p><p id="p-0210" num="0207">For example, in step S<b>26</b>, in a case where the above-described processing in steps S<b>18</b> to S<b>25</b> has been performed on virtual sound sources 0 to N-1 corresponding to the count values 1 to N output from the virtual sound source counter <b>32</b>, it is determined that the processing has been performed on all the virtual sound sources.</p><p id="p-0211" num="0208">In a case where it is determined in step S<b>26</b> that the processing has not been performed on all the virtual sound sources, the processing returns to step S<b>18</b>, and the above-described processing is repeated.</p><p id="p-0212" num="0209">In this case, when a count value is output from the virtual sound source counter <b>32</b> and the above-described processing in steps S<b>18</b> to S<b>25</b> is performed for the virtual sound source i indicated by the count value, the next count value is output from the virtual sound source counter <b>32</b>.</p><p id="p-0213" num="0210">Then, in steps S<b>18</b> to S<b>25</b> to be performed next, the processing for the virtual sound source i indicated by the count value is performed.</p><p id="p-0214" num="0211">Furthermore, in a case where it is determined in step S<b>26</b> that the processing has been performed on all the virtual sound sources, the HRIRs of all the virtual sound sources have been added (combined) and a BRIR has been obtained. Thereafter, the processing proceeds to step S<b>27</b>.</p><p id="p-0215" num="0212">In step S<b>27</b>, the left ear cumulative addition unit <b>37</b> and the right ear cumulative addition unit <b>38</b> transfer (supply) the BRIRs held in the data buffers to the left ear convolution signal processing unit <b>41</b> and the right ear convolution signal processing unit <b>42</b>.</p><p id="p-0216" num="0213">Then, the left ear convolution signal processing unit <b>41</b> convolves the supplied input signal and the BRIR for the left ear supplied from the left ear cumulative addition unit <b>37</b> at a predetermined timing, and supplies an output signal for the left ear obtained as a result to the addition unit <b>43</b>. At this time, overlap-add of output signal blocks is performed as appropriate, and an output signal frame is generated.</p><p id="p-0217" num="0214">Furthermore, the addition unit <b>43</b> adds the output signals supplied from the left ear convolution signal processing units <b>41</b>, and outputs a final output signal for the left ear obtained as a result.</p><p id="p-0218" num="0215">Similarly, the right ear convolution signal processing unit <b>42</b> convolves the supplied input signal and the BRIR for the right ear supplied from the right ear cumulative addition unit <b>38</b> at a predetermined timing, and supplies an output signal for the right ear obtained as a result to the addition unit <b>44</b>.</p><p id="p-0219" num="0216">The addition unit <b>44</b> adds the output signals supplied from the right ear convolution signal processing units <b>42</b>, and outputs a final output signal for the right ear obtained as a result.</p><p id="p-0220" num="0217">In step S<b>28</b>, the BRIR generation processing unit <b>21</b> determines whether or not the convolution signal processing is to be continuously performed.</p><p id="p-0221" num="0218">For example, in step S<b>28</b>, in a case such as a case where the listener or the like has given an instruction to end the processing or a case where the convolution signal processing has been performed on all the frames of the input signal, it is determined that the convolution signal processing is to be ended, that is, the convolution signal processing is not to be continuously performed.</p><p id="p-0222" num="0219">In a case where it is determined in step S<b>28</b> that the convolution signal processing is to be continuously performed, thereafter, the processing returns to step S<b>13</b>, and the above-described processing is repeated.</p><p id="p-0223" num="0220">That is, for example, in a case where the convolution signal processing is to be continuously performed, the virtual sound source counter <b>32</b> newly outputs count values in order from <b>1</b> to N, and a BRIR is generated (updated) in accordance with the count values.</p><p id="p-0224" num="0221">On the other hand, in a case where it is determined in step S<b>28</b> that the convolution signal processing is not to be continuously performed, the BRIR generation processing ends.</p><p id="p-0225" num="0222">As described above, the signal processing device <b>11</b> calculates the predicted relative azimuth Ac(i) using not only the head angle information As but also the head angular velocity information Bs and the head angular acceleration information Cs, and generates a BRIR in accordance with the predicted relative azimuth Ac(i). In this way, it is possible to prevent generation of distortion of the sound space and achieve more accurate sound reproduction.</p><p id="p-0226" num="0223">Here, the effect of reducing a deviation of a relative azimuth of a virtual sound source with respect to a listener in the present technology will be described with reference to <figref idref="DRAWINGS">FIGS. <b>10</b> to <b>12</b></figref>.</p><p id="p-0227" num="0224">Note that, in <figref idref="DRAWINGS">FIGS. <b>10</b> to <b>12</b></figref>, the same reference numerals are given to portions that correspond to each other, and the description thereof will be omitted as appropriate. Furthermore, in <figref idref="DRAWINGS">FIGS. <b>10</b> to <b>12</b></figref>, the vertical axis indicates the relative azimuth of the virtual sound source with respect to the listener, and the horizontal axis indicates the time.</p><p id="p-0228" num="0225">Furthermore, here, a case where the present technology is applied to the example illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref> will be described. That is, the deviations of the relative azimuths of the virtual sound source AD<b>0</b> (ID=0) and the virtual sound source ADn (ID=n) with respect to the listener U<b>11</b> reproduced in sound VR or sound AR when the listener U<b>11</b> moves the head at a constant angular velocity in the direction indicated by the arrow W<b>11</b>, that is, a temporal transition of a relative azimuth error, will be described.</p><p id="p-0229" num="0226">First, <figref idref="DRAWINGS">FIG. <b>10</b></figref> illustrates the deviations of the relative azimuths when the sounds of the virtual sound source AD<b>0</b> and the virtual sound source ADn are reproduced by a general head tracking method.</p><p id="p-0230" num="0227">Here, the head angle information indicating the head azimuth of the listener U<b>11</b>, that is, the head rotational motion information is acquired, and the BRIR is updated (generated) on the basis of the head angle information.</p><p id="p-0231" num="0228">In particular, an arrow B<b>51</b> indicates the time at which the head angle information is acquired, and an arrow B<b>52</b> indicates the time at which the BRIR is updated and starts to be applied.</p><p id="p-0232" num="0229">Furthermore, in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, a straight line L<b>51</b> indicates an actual correct relative azimuth at each time of the virtual sound source AD<b>0</b> with respect to the listener U<b>11</b>. Furthermore, a straight line L<b>52</b> indicates an actual correct relative azimuth at each time of the virtual sound source ADn with respect to the listener U<b>11</b>.</p><p id="p-0233" num="0230">On the other hand, a polygonal line L<b>53</b> indicates the relative azimuth of the virtual sound source AD<b>0</b> and the virtual sound source ADn with respect to the listener U<b>11</b> at each time, which are reproduced by sound reproduction.</p><p id="p-0234" num="0231">In <figref idref="DRAWINGS">FIG. <b>10</b></figref>, it can be seen that a deviation indicated by a hatched area is generated at each time between the actual correct relative azimuth and the relative azimuths of the virtual sound source AD<b>0</b> and the virtual sound source ADn reproduced by sound reproduction.</p><p id="p-0235" num="0232">Thus, for example, in the signal processing device <b>11</b>, when only the azimuth deviation A<b>1</b> illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, that is, only the distortion depending on the delay time T_proc and the delay time T_delay is corrected, the deviations of the relative azimuths of the virtual sound source AD<b>0</b> and the virtual sound source ADn are as illustrated in <figref idref="DRAWINGS">FIG. <b>11</b></figref>.</p><p id="p-0236" num="0233">In the example in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, the head angle information As and the like, that is, the head rotational motion information is acquired at each time indicated by an arrow B<b>61</b>, and the BRIR is updated and starts to be applied at each time indicated by an arrow B<b>62</b>.</p><p id="p-0237" num="0234">In this example, a polygonal line L<b>61</b> indicates the relative azimuth of the virtual sound source AD<b>0</b> and the virtual sound source ADn with respect to the listener U<b>11</b> at each time reproduced by sound reproduction based on the output signal in a case where the distortion depending on the delay time T_proc and the delay time T_delay is corrected by the signal processing device <b>11</b>.</p><p id="p-0238" num="0235">Furthermore, a hatched area at each time indicates a deviation between the relative azimuths of the virtual sound source AD<b>0</b> and the virtual sound source ADn reproduced by sound reproduction and the actual correct relative azimuth.</p><p id="p-0239" num="0236">The polygonal line L<b>61</b> is at a position closer to the straight line L<b>51</b> and the straight line L<b>52</b> at each time as compared with the case of the polygonal line L<b>53</b> in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, and it can be seen that the deviations of the relative azimuths of the virtual sound source AD<b>0</b> and the virtual sound source ADn are smaller.</p><p id="p-0240" num="0237">In this way, by correcting the distortion depending on the delay time T_proc and the delay time T_delay, it is possible to reduce the deviation of the relative azimuth and achieve more correct sound reproduction.</p><p id="p-0241" num="0238">However, in the example in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, the distance from the virtual sound source to the listener U<b>11</b>, that is, the azimuth deviation A<b>2</b> in <figref idref="DRAWINGS">FIG. <b>3</b></figref> depending on the propagation delay of the sound of the virtual sound source is not corrected.</p><p id="p-0242" num="0239">In <figref idref="DRAWINGS">FIG. <b>11</b></figref>, as can be seen from the fact that the deviation of the relative azimuth of the virtual sound source ADn is larger than that of the virtual sound source AD<b>0</b>, the deviation of the relative azimuth becomes larger for a virtual sound source located farther from the listener U<b>11</b>.</p><p id="p-0243" num="0240">On the other hand, in the signal processing device <b>11</b>, not only the azimuth deviation A<b>1</b> illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref> but also the azimuth deviation A<b>2</b> is corrected, and this allows for a reduction in the deviation of the relative azimuth regardless of the position of the virtual sound source as illustrated in <figref idref="DRAWINGS">FIG. <b>12</b></figref>.</p><p id="p-0244" num="0241">In this example, a polygonal line L<b>71</b> indicates the relative azimuth of the virtual sound source AD<b>0</b> with respect to the listener U<b>11</b> at each time reproduced by sound reproduction based on the output signal in a case where the distortion depending on the delay time T_proc and the delay time T_delay and the distortion depending on the distance to the virtual sound source are corrected by the signal processing device <b>11</b>.</p><p id="p-0245" num="0242">Furthermore, a hatched area between the straight line L<b>51</b> and the polygonal line L<b>71</b> indicates a deviation between the relative azimuth of the virtual sound source AD<b>0</b> reproduced by sound reproduction and the actual correct relative azimuth.</p><p id="p-0246" num="0243">Similarly, a polygonal line L<b>72</b> indicates the relative azimuth of the virtual sound source ADn with respect to the listener U<b>11</b> at each time reproduced by sound reproduction based on the output signal in a case where the distortion depending on the delay time T_proc and the delay time T_delay and the distortion depending on the distance to the virtual sound source are corrected by the signal processing device <b>11</b>.</p><p id="p-0247" num="0244">Furthermore, a hatched area between the straight line L<b>52</b> and the polygonal line L<b>72</b> indicates a deviation between the relative azimuth of the virtual sound source ADn reproduced by sound reproduction and the actual correct relative azimuth.</p><p id="p-0248" num="0245">In this example, the effect of improving (effect of reducing) the deviation of the relative azimuth at each time is equivalent regardless of the distance from the listener U<b>11</b> to the virtual sound source, that is, for both the virtual sound source AD<b>0</b> and the virtual sound source ADn. Furthermore, it can be seen that the deviations of the relative azimuths are further smaller than those in the example in <figref idref="DRAWINGS">FIG. <b>11</b></figref>.</p><p id="p-0249" num="0246">Note that, as the deviations of the relative azimuths of the virtual sound source AD<b>0</b> and the virtual sound source ADn, a deviation associated with a frequency of BRIR update being intermittent remains, but this cannot be improved in principle other than by increasing the frequency of BRIR update. Therefore, in the present technology, the deviation of the relative azimuth of a virtual sound source is minimized.</p><p id="p-0250" num="0247">As described above, instead of holding a BRIR determined in advance as in a general head tracking, the present technology uses a BRIR rendering method to independently hold the generation azimuth and the generation time of each virtual sound source, and BRIRs are successively combined with the use of head rotational motion information and prediction of the relative azimuth.</p><p id="p-0251" num="0248">Therefore, only BRIRs in a state determined in advance such as the entire circumference in the horizontal direction on the premise that the head remains stationary have been able to be used in a general head tracking, but the present technology makes it possible to obtain an appropriate BRIR for a variety of motions of the listener's head such as the azimuth and the angular velocity of the head. With this arrangement, the distortion of the sound space can be corrected, and more accurate sound reproduction can be achieved.</p><p id="p-0252" num="0249">In particular, in the present technology, a predicted relative azimuth is calculated with the use of not only the head angle information but also the head angular velocity information and the head angular acceleration information, and a BRIR is generated in accordance with the predicted relative azimuth. This makes it possible to appropriately correct the deviation of the relative azimuth associated with a head motion that changes in accordance with the distance from the listener to the virtual sound source. With this arrangement, the distortion of the sound space during a head motion can be corrected, and more accurate sound reproduction can be achieved.</p><heading id="h-0012" level="1">Configuration Example of Computer</heading><p id="p-0253" num="0250">Meanwhile, the series of pieces of processing described above can be executed not only by hardware but also by software. In a case where the series of pieces of processing is executed by software, a program constituting the software is installed on a computer. Here, the computer includes a computer incorporated in dedicated hardware, or a general-purpose personal computer capable of executing various functions with various programs installed therein, for example.</p><p id="p-0254" num="0251"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a block diagram illustrating a configuration example of hardware of a computer that executes the series of pieces of processing described above in accordance with a program.</p><p id="p-0255" num="0252">In the computer, a central processing unit (CPU) <b>501</b>, a read only memory (ROM) <b>502</b>, and a random access memory (RAM) <b>503</b> are connected to each other by a bus <b>504</b>.</p><p id="p-0256" num="0253">The bus <b>504</b> is further connected with an input/output interface <b>505</b>. The input/output interface <b>505</b> is connected with an input unit <b>506</b>, an output unit <b>507</b>, a recording unit <b>508</b>, a communication unit <b>509</b>, and a drive <b>510</b>.</p><p id="p-0257" num="0254">The input unit <b>506</b> includes a keyboard, a mouse, a microphone, an imaging element, or the like. The output unit <b>507</b> includes a display, a speaker, or the like. The recording unit <b>508</b> includes a hard disk, a non-volatile memory, or the like. The communication unit <b>509</b> includes a network interface or the like. The drive <b>510</b> drives a removable recording medium <b>511</b> such as a magnetic disk, an optical disk, a magneto-optical disk, or a semiconductor memory.</p><p id="p-0258" num="0255">To perform the series of pieces of processing described above, the computer having a configuration as described above causes the CPU <b>501</b> to, for example, load a program recorded in the recording unit <b>508</b> into the RAM <b>503</b> via the input/output interface <b>505</b> and the bus <b>504</b> and then execute the program.</p><p id="p-0259" num="0256">The program to be executed by the computer (CPU <b>501</b>) can be provided by, for example, being recorded on the removable recording medium <b>511</b> as a package medium or the like. Furthermore, the program can be provided via a wired or wireless transmission medium such as a local area network, the Internet, or digital satellite broadcasting.</p><p id="p-0260" num="0257">Inserting the removable recording medium <b>511</b> into the drive <b>510</b> allows the computer to install the program into the recording unit <b>508</b> via the input/output interface <b>505</b>. Furthermore, the program can be received by the communication unit <b>509</b> via a wired or wireless transmission medium and installed into the recording unit <b>508</b>. In addition, the program can be installed in advance in the ROM <b>502</b> or the recording unit <b>508</b>.</p><p id="p-0261" num="0258">Note that the program to be executed by the computer may be a program that performs the pieces of processing in chronological order as described in the present specification, or may be a program that performs the pieces of processing in parallel or when needed, for example, when the processing is called.</p><p id="p-0262" num="0259">Furthermore, embodiments of the present technology are not limited to the embodiment described above but can be modified in various ways within a scope of the present technology.</p><p id="p-0263" num="0260">For example, the present technology can have a cloud computing configuration in which a plurality of devices shares one function and collaborates in processing via a network.</p><p id="p-0264" num="0261">Furthermore, each step described in the flowcharts described above can be executed by one device or can be shared by a plurality of devices.</p><p id="p-0265" num="0262">Moreover, in a case where a plurality of pieces of processing is included in one step, the plurality of pieces of processing included in that one step can be executed by one device or can be shared by a plurality of devices.</p><p id="p-0266" num="0263">Moreover, the present technology can also have the following configurations.</p><p id="p-0267" num="0264">(1)</p><p id="p-0268" num="0265">A signal processing device including:</p><p id="p-0269" num="0266">a relative azimuth prediction unit configured to predict, on the basis of a delay time in accordance with a distance from a virtual sound source to a listener, a relative azimuth of the virtual sound source when a sound of the virtual sound source reaches the listener; and</p><p id="p-0270" num="0267">a BRIR generation unit configured to acquire a head-related transfer function of the relative azimuth for each one of a plurality of the virtual sound sources and generate a BRIR on the basis of a plurality of the acquired head-related transfer functions.</p><p id="p-0271" num="0268">(2)</p><p id="p-0272" num="0269">The signal processing device according to (1), further including:</p><p id="p-0273" num="0270">a convolution signal processing unit configured to generate an output signal for reproducing the sounds of the plurality of the virtual sound sources by performing convolution signal processing of an input signal and the BRIR.</p><p id="p-0274" num="0271">(3)</p><p id="p-0275" num="0272">The signal processing device according to (2), in which</p><p id="p-0276" num="0273">the relative azimuth prediction unit predicts the relative azimuth on the basis of a delay time due to the generation of the BRIR and the convolution signal processing.</p><p id="p-0277" num="0274">(4)</p><p id="p-0278" num="0275">The signal processing device according to any one of (1) to (3), in which</p><p id="p-0279" num="0276">the relative azimuth prediction unit predicts the relative azimuth on the basis of information indicating a movement of the listener's head.</p><p id="p-0280" num="0277">(5)</p><p id="p-0281" num="0278">The signal processing device according to (4), in which</p><p id="p-0282" num="0279">the information indicating the movement of the listener's head is at least one of angle information, angular velocity information, or angular acceleration information of the listener's head.</p><p id="p-0283" num="0280">(6)</p><p id="p-0284" num="0281">The signal processing device according to any one of (1) to (5), in which</p><p id="p-0285" num="0282">the relative azimuth prediction unit predicts the relative azimuth on the basis of a generation azimuth of the virtual sound source.</p><p id="p-0286" num="0283">(7)</p><p id="p-0287" num="0284">The signal processing device according to any one of (1) to (6), in which</p><p id="p-0288" num="0285">the BRIR generation unit generates the BRIR by adding a transmission characteristic for the virtual sound source to the head-related transfer function for each one of the plurality of the virtual sound sources, and combining the head-related transfer functions to which the transmission characteristics have been added, the head-related transfer functions being obtained one for each one of the plurality of the virtual sound sources.</p><p id="p-0289" num="0286">(8)</p><p id="p-0290" num="0287">The signal processing device according to (7), in which</p><p id="p-0291" num="0288">the BRIR generation unit adds the transmission characteristic to the head-related transfer function by performing gain correction in accordance with intensity of the sound of the virtual sound source or filter processing in accordance with a frequency characteristic of the virtual sound source.</p><p id="p-0292" num="0289">(9)</p><p id="p-0293" num="0290">A signal processing method including:</p><p id="p-0294" num="0291">by a signal processing device,</p><p id="p-0295" num="0292">predicting, on the basis of a delay time in accordance with a distance from a virtual sound source to a listener, a relative azimuth of the virtual sound source when a sound of the virtual sound source reaches the listener; and</p><p id="p-0296" num="0293">acquiring a head-related transfer function of the relative azimuth for each one of a plurality of the virtual sound sources and generating a BRIR on the basis of a plurality of the acquired head-related transfer functions.</p><p id="p-0297" num="0294">(10)</p><p id="p-0298" num="0295">A program for causing a computer to execute processing including steps of:</p><p id="p-0299" num="0296">predicting, on the basis of a delay time in accordance with a distance from a virtual sound source to a listener, a relative azimuth of the virtual sound source when a sound of the virtual sound source reaches the listener; and</p><p id="p-0300" num="0297">acquiring a head-related transfer function of the relative azimuth for each one of a plurality of the virtual sound sources and generating a BRIR on the basis of a plurality of the acquired head-related transfer functions.</p><heading id="h-0013" level="1">REFERENCE SIGNS LIST</heading><p id="p-0301" num="0000"><ul id="ul0002" list-style="none">    <li id="ul0002-0001" num="0298"><b>11</b> Signal processing device</li>    <li id="ul0002-0002" num="0299"><b>21</b> BRIR generation processing unit</li>    <li id="ul0002-0003" num="0300"><b>22</b> Convolution signal processing unit</li>    <li id="ul0002-0004" num="0301"><b>31</b> Sensor unit</li>    <li id="ul0002-0005" num="0302"><b>33</b> RIR database memory</li>    <li id="ul0002-0006" num="0303"><b>34</b> Relative azimuth prediction unit</li>    <li id="ul0002-0007" num="0304"><b>35</b> HRIR database memory</li>    <li id="ul0002-0008" num="0305"><b>36</b> Attribute application unit</li>    <li id="ul0002-0009" num="0306"><b>37</b> Left ear cumulative addition unit</li>    <li id="ul0002-0010" num="0307"><b>38</b> Right ear cumulative addition unit</li>    <li id="ul0002-0011" num="0308"><b>41</b>-<b>1</b> to <b>41</b>-M, <b>41</b> Left ear convolution signal processing unit</li>    <li id="ul0002-0012" num="0309"><b>42</b>-<b>1</b> to <b>42</b>-M, <b>42</b> Right ear convolution signal processing unit</li></ul></p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A signal processing device comprising:<claim-text>a relative azimuth prediction unit configured to predict, on a basis of a delay time in accordance with a distance from a virtual sound source to a listener, a relative azimuth of the virtual sound source when a sound of the virtual sound source reaches the listener; and</claim-text><claim-text>a BRIR generation unit configured to acquire a head-related transfer function of the relative azimuth for each one of a plurality of the virtual sound sources and generate a BRIR on a basis of a plurality of the acquired head-related transfer functions.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The signal processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>a convolution signal processing unit configured to generate an output signal for reproducing the sounds of the plurality of the virtual sound sources by performing convolution signal processing of an input signal and the BRIR.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The signal processing device according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein<claim-text>the relative azimuth prediction unit predicts the relative azimuth on a basis of a delay time due to the generation of the BRIR and the convolution signal processing.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The signal processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the relative azimuth prediction unit predicts the relative azimuth on a basis of information indicating a movement of the listener's head.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The signal processing device according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein<claim-text>the information indicating the movement of the listener's head is at least one of angle information, angular velocity information, or angular acceleration information of the listener's head.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The signal processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the relative azimuth prediction unit predicts the relative azimuth on a basis of a generation azimuth of the virtual sound source.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The signal processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the BRIR generation unit generates the BRIR by adding a transmission characteristic for the virtual sound source to the head-related transfer function for each one of the plurality of the virtual sound sources, and combining the head-related transfer functions to which the transmission characteristics have been added, the head-related transfer functions being obtained one for each one of the plurality of the virtual sound sources.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The signal processing device according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein<claim-text>the BRIR generation unit adds the transmission characteristic to the head-related transfer function by performing gain correction in accordance with intensity of the sound of the virtual sound source or filter processing in accordance with a frequency characteristic of the virtual sound source.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. A signal processing method comprising:<claim-text>by a signal processing device,</claim-text><claim-text>predicting, on a basis of a delay time in accordance with a distance from a virtual sound source to a listener, a relative azimuth of the virtual sound source when a sound of the virtual sound source reaches the listener; and</claim-text><claim-text>acquiring a head-related transfer function of the relative azimuth for each one of a plurality of the virtual sound sources and generating a BRIR on a basis of a plurality of the acquired head-related transfer functions.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. A program for causing a computer to execute processing including steps of:<claim-text>predicting, on a basis of a delay time in accordance with a distance from a virtual sound source to a listener, a relative azimuth of the virtual sound source when a sound of the virtual sound source reaches the listener; and</claim-text><claim-text>acquiring a head-related transfer function of the relative azimuth for each one of a plurality of the virtual sound sources and generating a BRIR on a basis of a plurality of the acquired head-related transfer functions.</claim-text></claim-text></claim></claims></us-patent-application>