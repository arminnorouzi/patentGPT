<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230007815A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230007815</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17364468</doc-number><date>20210630</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>05</class><subclass>K</subclass><main-group>7</main-group><subgroup>20</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>05</class><subclass>K</subclass><main-group>7</main-group><subgroup>20827</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>05</class><subclass>K</subclass><main-group>7</main-group><subgroup>20836</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>05</class><subclass>K</subclass><main-group>7</main-group><subgroup>20309</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>05</class><subclass>K</subclass><main-group>7</main-group><subgroup>20318</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">IN-RACK REFRIGERANT DISTRIBUTION UNIT WITH PRESSURE CONTROL SYSTEM</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Nvidia Corporation</orgname><address><city>Santa Clara</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Heydari</last-name><first-name>Ali</first-name><address><city>Albany</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Systems and methods for cooling a datacenter are disclosed. In at least one embodiment, an in-rack refrigerant distribution unit (RDU) distributes refrigerant to one or more colds plates in association with a pressure control system to enable a pressure-drop before an expansion valve for a liquid-phase of a refrigerant based in part on a pressure of a liquid-phase of a refrigerant exceeding a first threshold and based in part on a temperature associated with one or more cold plates being below a second threshold.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="108.29mm" wi="158.75mm" file="US20230007815A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="223.52mm" wi="162.81mm" orientation="landscape" file="US20230007815A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="235.20mm" wi="172.89mm" orientation="landscape" file="US20230007815A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="238.68mm" wi="175.26mm" orientation="landscape" file="US20230007815A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="238.59mm" wi="173.74mm" orientation="landscape" file="US20230007815A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="227.50mm" wi="155.36mm" file="US20230007815A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="232.16mm" wi="147.91mm" file="US20230007815A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="241.30mm" wi="135.38mm" file="US20230007815A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="238.51mm" wi="166.71mm" orientation="landscape" file="US20230007815A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="232.24mm" wi="166.71mm" orientation="landscape" file="US20230007815A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="234.87mm" wi="165.02mm" orientation="landscape" file="US20230007815A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="211.07mm" wi="173.23mm" orientation="landscape" file="US20230007815A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="246.38mm" wi="149.78mm" file="US20230007815A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="235.97mm" wi="171.28mm" orientation="landscape" file="US20230007815A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="225.89mm" wi="154.35mm" orientation="landscape" file="US20230007815A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="234.36mm" wi="151.89mm" orientation="landscape" file="US20230007815A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="155.53mm" wi="160.53mm" orientation="landscape" file="US20230007815A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="191.01mm" wi="158.33mm" orientation="landscape" file="US20230007815A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="188.81mm" wi="173.99mm" orientation="landscape" file="US20230007815A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00019" num="00019"><img id="EMI-D00019" he="233.43mm" wi="149.61mm" orientation="landscape" file="US20230007815A1-20230105-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00020" num="00020"><img id="EMI-D00020" he="224.37mm" wi="159.68mm" file="US20230007815A1-20230105-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00021" num="00021"><img id="EMI-D00021" he="222.59mm" wi="154.09mm" orientation="landscape" file="US20230007815A1-20230105-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00022" num="00022"><img id="EMI-D00022" he="249.34mm" wi="169.67mm" file="US20230007815A1-20230105-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00023" num="00023"><img id="EMI-D00023" he="172.21mm" wi="170.69mm" orientation="landscape" file="US20230007815A1-20230105-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00024" num="00024"><img id="EMI-D00024" he="229.36mm" wi="167.47mm" orientation="landscape" file="US20230007815A1-20230105-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00025" num="00025"><img id="EMI-D00025" he="229.36mm" wi="166.71mm" orientation="landscape" file="US20230007815A1-20230105-D00025.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00026" num="00026"><img id="EMI-D00026" he="172.21mm" wi="171.45mm" orientation="landscape" file="US20230007815A1-20230105-D00026.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00027" num="00027"><img id="EMI-D00027" he="223.69mm" wi="172.30mm" orientation="landscape" file="US20230007815A1-20230105-D00027.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00028" num="00028"><img id="EMI-D00028" he="240.62mm" wi="170.94mm" file="US20230007815A1-20230105-D00028.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00029" num="00029"><img id="EMI-D00029" he="240.11mm" wi="165.95mm" file="US20230007815A1-20230105-D00029.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00030" num="00030"><img id="EMI-D00030" he="239.01mm" wi="165.02mm" orientation="landscape" file="US20230007815A1-20230105-D00030.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00031" num="00031"><img id="EMI-D00031" he="212.01mm" wi="149.52mm" file="US20230007815A1-20230105-D00031.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00032" num="00032"><img id="EMI-D00032" he="244.94mm" wi="168.49mm" file="US20230007815A1-20230105-D00032.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00033" num="00033"><img id="EMI-D00033" he="235.46mm" wi="172.55mm" orientation="landscape" file="US20230007815A1-20230105-D00033.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00034" num="00034"><img id="EMI-D00034" he="238.68mm" wi="174.07mm" orientation="landscape" file="US20230007815A1-20230105-D00034.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00035" num="00035"><img id="EMI-D00035" he="242.99mm" wi="166.88mm" file="US20230007815A1-20230105-D00035.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00036" num="00036"><img id="EMI-D00036" he="234.95mm" wi="149.61mm" file="US20230007815A1-20230105-D00036.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00037" num="00037"><img id="EMI-D00037" he="227.67mm" wi="149.61mm" file="US20230007815A1-20230105-D00037.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00038" num="00038"><img id="EMI-D00038" he="240.37mm" wi="172.64mm" file="US20230007815A1-20230105-D00038.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00039" num="00039"><img id="EMI-D00039" he="239.35mm" wi="166.88mm" orientation="landscape" file="US20230007815A1-20230105-D00039.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00040" num="00040"><img id="EMI-D00040" he="244.60mm" wi="170.94mm" file="US20230007815A1-20230105-D00040.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00041" num="00041"><img id="EMI-D00041" he="217.76mm" wi="159.26mm" file="US20230007815A1-20230105-D00041.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00042" num="00042"><img id="EMI-D00042" he="248.67mm" wi="153.50mm" file="US20230007815A1-20230105-D00042.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00043" num="00043"><img id="EMI-D00043" he="136.82mm" wi="137.33mm" file="US20230007815A1-20230105-D00043.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00044" num="00044"><img id="EMI-D00044" he="166.96mm" wi="136.91mm" file="US20230007815A1-20230105-D00044.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00045" num="00045"><img id="EMI-D00045" he="189.74mm" wi="137.08mm" file="US20230007815A1-20230105-D00045.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00046" num="00046"><img id="EMI-D00046" he="151.13mm" wi="160.70mm" file="US20230007815A1-20230105-D00046.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00047" num="00047"><img id="EMI-D00047" he="191.35mm" wi="168.74mm" file="US20230007815A1-20230105-D00047.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00048" num="00048"><img id="EMI-D00048" he="132.08mm" wi="110.74mm" file="US20230007815A1-20230105-D00048.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">FIELD</heading><p id="p-0002" num="0001">At least one embodiment pertains to cooling systems, including systems and methods for operating those cooling systems. In at least one embodiment, such a cooling system can be utilized in a datacenter containing one or more racks or computing servers.</p><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">Datacenter cooling systems use fans to circulate air through server components. Certain supercomputers or other high capacity computers may use water or other cooling systems instead of air-cooling systems to draw heat away from the server components or racks of the datacenter to an area external to the datacenter. The cooling systems may include a chiller within the datacenter area, which may include area external to the datacenter itself. Further, the area external to the datacenter may include a cooling tower or other external heat exchanger that receives heated coolant from the datacenter and that disperses the heat by forced air or other means to the environment (or an external cooling medium). The cooled coolant is recirculated back into the datacenter. The chiller and the cooling tower together form a chilling facility.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0004" num="0003"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates an exemplary datacenter cooling system subject to improvements described in at least one embodiment;</p><p id="p-0005" num="0004"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates server-level features associated with an in-rack refrigerant distribution unit having a pressure control system, according to at least one embodiment;</p><p id="p-0006" num="0005"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates rack-level features associated with an in-rack refrigerant distribution unit having a pressure control system, according to at least one embodiment;</p><p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates datacenter-level features associated with an in-rack refrigerant distribution unit having a pressure control system, according to at least one embodiment;</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates a method associated with a datacenter cooling system of <figref idref="DRAWINGS">FIGS. <b>2</b>-<b>4</b></figref>, according to at least one embodiment;</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates a distributed system, in accordance with at least one embodiment;</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates an exemplary datacenter, in accordance with at least one embodiment;</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates a client-server network, in accordance with at least one embodiment;</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>9</b></figref> illustrates a computer network, in accordance with at least one embodiment;</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>10</b>A</figref> illustrates a networked computer system, in accordance with at least one embodiment;</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>10</b>B</figref> illustrates a networked computer system, in accordance with at least one embodiment;</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>10</b>C</figref> illustrates a networked computer system, in accordance with at least one embodiment;</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>11</b></figref> illustrates one or more components of a system environment in which services may be offered as third-party network services, in accordance with at least one embodiment;</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>12</b></figref> illustrates a cloud computing environment, in accordance with at least one embodiment;</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>13</b></figref> illustrates a set of functional abstraction layers provided by a cloud computing environment, in accordance with at least one embodiment;</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>14</b></figref> illustrates a supercomputer at a chip level, in accordance with at least one embodiment;</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>15</b></figref> illustrates a supercomputer at a rack module level, in accordance with at least one embodiment;</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>16</b></figref> illustrates a supercomputer at a rack level, in accordance with at least one embodiment;</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>17</b></figref> illustrates a supercomputer at a whole system level, in accordance with at least one embodiment;</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>18</b>A</figref> illustrates inference and/or training logic, in accordance with at least one embodiment;</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>18</b>B</figref> illustrates inference and/or training logic, in accordance with at least one embodiment;</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>19</b></figref> illustrates training and deployment of a neural network, in accordance with at least one embodiment;</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>20</b></figref> illustrates an architecture of a system of a network, in accordance with at least one embodiment;</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>21</b></figref> illustrates an architecture of a system of a network, in accordance with at least one embodiment;</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>22</b></figref> illustrates a control plane protocol stack, in accordance with at least one embodiment;</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>23</b></figref> illustrates a user plane protocol stack, in accordance with at least one embodiment;</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>24</b></figref> illustrates components of a core network, in accordance with at least one embodiment;</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>25</b></figref> illustrates components of a system to support network function virtualization (NFV), in accordance with at least one embodiment;</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>26</b></figref> illustrates a processing system, in accordance with at least one embodiment;</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>27</b></figref> illustrates a computer system, in accordance with at least one embodiment;</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>28</b></figref> illustrates a system, in accordance with at least one embodiment;</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>29</b></figref> illustrates an exemplary integrated circuit, in accordance with at least one embodiment;</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>30</b></figref> illustrates a computing system, according to at least one embodiment;</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>31</b></figref> illustrates an APU, in accordance with at least one embodiment;</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>32</b></figref> illustrates a CPU, in accordance with at least one embodiment;</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>33</b></figref> illustrates an exemplary accelerator integration slice, in accordance with at least one embodiment;</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIGS. <b>34</b>A-<b>34</b>B</figref> illustrate exemplary graphics processors, in accordance with at least one embodiment;</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>35</b>A</figref> illustrates a graphics core, in accordance with at least one embodiment;</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>35</b>B</figref> illustrates a GPGPU, in accordance with at least one embodiment;</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. <b>36</b>A</figref> illustrates a parallel processor, in accordance with at least one embodiment;</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. <b>36</b>B</figref> illustrates a processing cluster, in accordance with at least one embodiment;</p><p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. <b>36</b>C</figref> illustrates a graphics multiprocessor, in accordance with at least one embodiment;</p><p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. <b>37</b></figref> illustrates a software stack of a programming platform, in accordance with at least one embodiment;</p><p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. <b>38</b></figref> illustrates a CUDA implementation of a software stack of <figref idref="DRAWINGS">FIG. <b>37</b></figref>, in accordance with at least one embodiment;</p><p id="p-0048" num="0047"><figref idref="DRAWINGS">FIG. <b>39</b></figref> illustrates a ROCm implementation of a software stack of <figref idref="DRAWINGS">FIG. <b>37</b></figref>, in accordance with at least one embodiment;</p><p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. <b>40</b></figref> illustrates an OpenCL implementation of a software stack of <figref idref="DRAWINGS">FIG. <b>37</b></figref>, in accordance with at least one embodiment;</p><p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. <b>41</b></figref> illustrates software that is supported by a programming platform, in accordance with at least one embodiment; and</p><p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. <b>42</b></figref> illustrates compiling code to execute on programming platforms of <figref idref="DRAWINGS">FIGS. <b>37</b>-<b>40</b></figref>, in accordance with at least one embodiment.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0004" level="1">DETAILED DESCRIPTION</heading><p id="p-0052" num="0051">In at least one embodiment, an exemplary datacenter <b>100</b> can be utilized as illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, which has a cooling system subject to improvements described herein. In at least one embodiment, numerous specific details are set forth to provide a thorough understanding, but concepts herein may be practiced without one or more of these specific details. In at least one embodiment, datacenter cooling systems can respond to sudden high heat requirements caused by changing computing-loads in present day computing components. In at least one embodiment, as these requirements are subject to change or tend to range from a minimum to a maximum of different cooling requirements, these requirements must be met in an economical manner, using an appropriate cooling system. In at least one embodiment, for moderate to high cooling requirements, liquid cooling system may be used. In at least one embodiment, high cooling requirements are economically satisfied by localized immersion cooling. In at least one embodiment, these different cooling requirements also reflect different heat features of a datacenter. In at least one embodiment, heat generated from these components, servers, and racks are cumulatively referred to as a heat feature or a cooling requirement as cooling requirement must address a heat feature entirely.</p><p id="p-0053" num="0052">In at least one embodiment, a datacenter liquid cooling system is disclosed. In at least one embodiment, this datacenter cooling system addresses heat features in associated computing or datacenter devices, such as in graphics processing units (GPUs), in switches, in dual inline memory module (DIMMs), or central processing units (CPUs). In at least one embodiment, these components may be referred to herein as high heat density computing components. Furthermore, in at least one embodiment, an associated computing or datacenter device may be a processing card having one or more GPUs, switches, or CPUs thereon. In at least one embodiment, each of GPUs, switches, and CPUs may be a heat generating feature of a computing device. In at least one embodiment, a GPU, a CPU, or a switch may have one or more cores, and each core may be a heat generating feature.</p><p id="p-0054" num="0053">In at least one embodiment, an in-rack refrigerant distribution unit (RDU) herein has a pressure control system. In at least one embodiment, a pressure control system enables a pressure-drop within an in-rack RDU or an associated cooling manifold so that excess pressure of a liquid-phase of a refrigerant may be bled, prior to an expansion valve and prior to entry to a cold plate. In at least one embodiment, an applied pressure-drop may be in response to a lesser cooling requirement for an associated cold plate. In at least one embodiment, a lesser cooling requirement may be when heat dissipated into a cold plate from an associated computing device is low. In at least one embodiment, heat dissipated may be measured as a temperature associated with a cold plate. In at least one embodiment, for low temperature heat dissipation, high pressure of a refrigerant may not be needed. In at least one embodiment, when there may be excess pressure associated with a liquid-phase of a refrigerant, such excess pressure may be bled prior to an expansion valve so that low pressure refrigerant is provided to a cold plate to prevent damage or wear of refrigerant lines.</p><p id="p-0055" num="0054">In at least one embodiment, an in-rack RDU associated with a pressure control system is in a form-factor of a 1 unit (U)&#x2014;or 3 units (3 U) of servers of a rack. In at least one embodiment, an in-rack RDU distributes refrigerant from a secondary refrigerant cooling loop that interfaces with a primary refrigerant cooling loop. In at least one embodiment, a secondary refrigerant cooling loop circulates through a cold plate to extract heat from at least one computing device. In at least one embodiment, a pressure control system enables an applied pressure-drop to prevent damage to refrigerant lines that may be under high pressure depending on cooling requirements of a cold plate. In at least one embodiment, an applied pressure-drop may occur within an in-rack RDU or an associated cooling manifold so that, when it is determined that a pressure of liquid-phase of a refrigerant exceeds a first threshold and a temperature associated with a cold plate is below a second threshold, then an in-rack RDU can provide less cooling (where refrigerant may be at a lower pressure).</p><p id="p-0056" num="0055">In at least one embodiment, an in-rack RDU associated with a pressure control system is able to address issues of a datacenter cooling system using only coolants of a primary and of a secondary coolant loops. In at least one embodiment, refrigerant may be limited in a datacenter cooling system due to a complexity in maintaining high pressure lines. In at least one embodiment, an in-rack RDU associated with a pressure control system provides a pressure control system to cause an applied pressure-drop before an expansion valve based in part on a lower cooling requirement of a cold plate. In at least one embodiment, at a lower cooling requirement, a lower flow rate is enabled to a cold plate via a pressure bleed enabled by a pressure control system associated with one or more flow controllers. In at least one embodiment, such a feature is able to address pressure-related issues that may exist in higher pressure refrigerant lines.</p><p id="p-0057" num="0056">In at least one embodiment, an issue addressed by such features is to enable cold plates that were previously limited to flowing coolant therein, as part of primary and secondary cooling loops, to remove more high-density heat in an efficient manner using refrigerant. In at least one embodiment, higher rates of coolant may have been required to remove high heat from high heat density computing devices. In at least one embodiment, such higher rates of coolant may erode material of a cold plate or cause corrosion within a cold plate. In at least one embodiment, such features enable a pressure control system having at least one processor to manage flow pressures, and therefore, flow rate of refrigerant to address issues where temperatures of refrigerant that are either too hot or too cold can cause damage to plumbing features and to computing devices. In at least one embodiment, high flow rates from high pressure refrigerant may also damage plumbing or cause wear and tear. In at least one embodiment, a pressure control system enables an artificial pressure-drop in a server tray or box forming an in-rack RDU, which in turn manages flow and variation of thermal load absorbed by a refrigerant. In at least one embodiment, an in-rack RDU may be designed for 2000 watts, but if only 200 watts of heat needs to be removed, at least one flow controller acting as a bleeder value (such as an in-line solenoid valve) may bleed pressure off prior to an expansion valve and a cold plate.</p><p id="p-0058" num="0057">In at least one embodiment, features herein enable a solution that uses an in-rack RDU to interface between refrigerant cooling loops instead of a coolant distribution unit (CDU) interfacing between coolant loops. In at least one embodiment, one or more refrigerant to air heat exchangers (R2AHXs) or refrigerant to refrigerant heat exchangers (R2RHXs) may be associated with a pressure control system, within an in-rack RDU. In at least one embodiment, such an R2RHX may be to interface between a first refrigerant cooling loop and a second refrigerant cooling loop so that refrigerant can be used in all plumbing lines for removal of high amounts of heat while being less corrosive or cause lesser erosion than higher flow rates of coolant otherwise required. In at least one embodiment, such an R2AHX may be to interface between a first refrigerant cooling loop and air cooling for similar reasons as using an R2RHX.</p><p id="p-0059" num="0058">In at least one embodiment, a first refrigerant cooling loop or a second refrigerant cooling loop may include a pumped refrigerant cooling system that is utilized for removing heat in at least one cold plate to an R2RHX or an R2AHX, instead of a secondary cooling loop that uses coolant. In at least one embodiment, an in-rack RDU and associated pressure control system may include one or more refrigerant pumps, one or more condenser units, one or more refrigerant reservoirs and other refrigeration systems such as filters, strainers, and charging ports. In at least one embodiment, an R2RHX includes plate or gasket-type heat exchangers functioning as condenser units within some of its plates and evaporator units within some of its plates. In at least one embodiment, such features allow for efficient utilization of vapor compression refrigeration in 2-phase evaporators in datacenters without requiring primary side chiller-based cooling system. In at least one embodiment, such features also allow for more efficient and low-power heat removal from a datacenter, allows for refrigeration at an edge location without datacenter infrastructure, and enables removal of heat at much higher densities than either air or liquid cooled systems that may be based on coolant flows.</p><p id="p-0060" num="0059">In at least one embodiment, a second refrigerant used to cool a first refrigerant of a first refrigerant cooling loop may be of a same or similar chemistry as a second refrigerant of a second refrigerant cooling loop or a first refrigerant of a first refrigerant cooling loop. In at least one embodiment, first refrigerant may be carried within a line or area of an in-rack RDU, but that may be at a higher pressure than required for a low temperature associated with lower heat generated and dissipated into a cold plate, which in turn has a lower cooling requirement.</p><p id="p-0061" num="0060">In at least one embodiment, such issues of a lower cooling requirement, when a first refrigerant is at a higher pressure, may be addressed by a pressure control system to cause reduction in pressure prior to an expansion valve associated with a cold plate. In at least one embodiment, a second condenser unit of a second refrigerant loop may be placed over a server rack or tray and uses one or more flow controllers, such as refrigerant pumps, to enables movement of a second refrigerant to a second condenser unit for dissipation of heat from a cold plate.</p><p id="p-0062" num="0061">In at least one embodiment, such features lower working fluid temperatures through an evaporation of a refrigerant, such as R134a&#xae; used in an in-rack RDU. In at least one embodiment, such features allow removal of higher amounts of heat from liquid cooled cold plates without increasing a fluid rates above limits designated for such cold plates, as such increase of flow rates may erode or corrode a cold plate. In at least one embodiment, an OCU (overhead condenser unit) allows for removal of heat of a datacenter without a need for central chiller system having a primary cooling loop, a secondary cooling loop, and one or more coolant distribution units (CDUs). In at least one embodiment, such features allow for utilization of refrigerant cooling loops within edge datacenters, where there may be no coolant-based datacenter cooling systems available.</p><p id="p-0063" num="0062">In at least one embodiment, an in-rack RDU with a pressure control system uses a refrigerant or engineered fluid, such as Novec&#xae; 7000, R1234Ze&#xae;, R134a&#xae;, or some of 3M&#xae;'s engineering fluid with a cold plate and a condenser unit for heat removal from at least one computing device. In at least one embodiment, such a cold plate is adapted to support an evaporator section therein. In at least one embodiment, pressure stabilization may be enabled by a pump or other flow controllers for controlling refrigerant and may be within a pressure control system, prior to an expansion valve. In at least one embodiment, a pressure control system is enabled to flow refrigerant at a determined flow rate using a pressure reduction associated with a lower temperature and with a lower cooling requirement. In at least one embodiment, such refrigerant flows between a condenser unit external to a cold plate and an evaporator section of a cold plate.</p><p id="p-0064" num="0063">In at least one embodiment, an in-rack RDU with a pressure control system allows a refrigerant to flow directly to a cold plate adapted for refrigerant and that has at least one evaporator therein. In at least one embodiment, expansion valves associated with at least one cold plate, an R2RHX, or an R2AHX enables a refrigerant through a cold plate to cause phase transformation of a refrigerant and to cause heat to be absorbed from at least one computing device. In at least one embodiment, a pressure control system enables bleeding of pressure by a refrigerant loop from a high-pressure side to a low-pressure side of a condenser unit within an in-rack RDU. In at least one embodiment, a pressure control system may bleed pressure to one or more refrigerant reservoirs within an in-rack RDU, and such refrigerant may be returned to a low-pressure side via a gravity flow to a refrigerant pump that then provides it to a condenser unit for recirculation.</p><p id="p-0065" num="0064">In at least one embodiment, heat from at least one computing device is absorbed into a first refrigerant of a first refrigerant cooling loop, which is then transferred to a first condenser unit using refrigerant pumps, and which interfaces with a second evaporator section different from a cold plate's evaporator section. In at least one embodiment, such a second evaporator section enables transfer of heat to a second refrigerant of a second refrigerant cooling loop. In at least one embodiment, a second condenser unit of such a second refrigerant cooling loop causes release of at least part of absorbed heat into an ambient environment that may be external to a datacenter. In at least one embodiment, sensors such as temperature, flow, humidity, leak, pressure, and fluid or coolant chemistry allow for intelligent operation of an in-rack RDU with a pressure control system in case of any issues in a secondary or a primary cooling loop. In at least one embodiment, such sensors may provide input to a pressure control system to cause such a system to respond to a cooling requirement by reducing pressure of a refrigerant prior to an expansion valve.</p><p id="p-0066" num="0065">In at least one embodiment, an in-rack RDU with a pressure control system is able to address issues in cold plates that were primarily able to support coolant for cooling purposes. In at least one embodiment, a refrigerant is in a liquid to vapor phase transformation within an evaporator section of a cold plate. In at least one embodiment, a first or a second refrigerant is in a vapor phase when entering a first or a second condenser unit, but in a fluid state when pumped to a respective evaporator section of a cold plate or an R2RHX. In at least one embodiment, varying pressures may be created within such a closed cooling loop of refrigerant cooling loops having refrigerant-adapted cold plates, an R2AHX, and an R2RHX. In at least one embodiment, refrigerant pumps may be available fluid pumping or displacement pumping of vapor.</p><p id="p-0067" num="0066">In at least one embodiment, an exemplary datacenter <b>100</b> can be utilized as illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, which has a cooling system subject to improvements described herein. In at least one embodiment, a datacenter <b>100</b> may be one or more rooms <b>102</b> having racks <b>110</b> and auxiliary equipment to house one or more servers on one or more server trays. In at least one embodiment, a datacenter <b>100</b> is supported by a cooling tower <b>104</b> located external to a datacenter <b>100</b>. In at least one embodiment, a cooling tower <b>104</b> dissipates heat from within a datacenter <b>100</b> by acting on a primary cooling loop <b>106</b>. In at least one embodiment, a cooling distribution unit (CDU) <b>112</b> is used between a primary cooling loop <b>106</b> and a second or secondary cooling loop <b>108</b> to enable absorption of heat from a second or secondary cooling loop <b>108</b> to a primary cooling loop <b>106</b>. In at least one embodiment, a secondary cooling loop <b>108</b> can access various plumbing into a server tray as required, in an aspect. In at least one embodiment, loops <b>106</b>, <b>108</b> are illustrated as line drawings, but a person of ordinary skill would recognize that one or more plumbing features may be used. In at least one embodiment, flexible polyvinyl chloride (PVC) pipes may be used along with associated plumbing to move fluid along in each provided loop <b>106</b>; <b>108</b>. In at least one embodiment, one or more coolant pumps may be used to maintain pressure differences within coolant loops <b>106</b>, <b>108</b> to enable movement of coolant according to temperature sensors in various locations, including in a room, in one or more racks <b>110</b>, and/or in server boxes or server trays within one or more racks <b>110</b>.</p><p id="p-0068" num="0067">In at least one embodiment, coolant in a primary cooling loop <b>106</b> and in a secondary cooling loop <b>108</b> may be at least water and an additive. In at least one embodiment, an additive may be glycol or propylene glycol. In operation, in at least one embodiment, each of a primary and a secondary cooling loops may have their own coolant. In at least one embodiment, coolant in secondary cooling loops may be proprietary to requirements of components in a server tray or in associated racks <b>110</b>. In at least one embodiment, a CDU <b>112</b> is capable of sophisticated control of coolants, independently or concurrently, within provided coolant loops <b>106</b>, <b>108</b>. In at least one embodiment, a CDU may be adapted to control flow rate of coolant so that coolant is appropriately distributed to absorbed heat generated within associated racks <b>110</b>. In at least one embodiment, more flexible tubing <b>114</b> is provided from a secondary cooling loop <b>108</b> to enter each server tray to provide coolant to electrical and/or computing components therein.</p><p id="p-0069" num="0068">In at least one embodiment, tubing <b>118</b> that forms part of a secondary cooling loop <b>108</b> may be referred to as room manifolds. Separately, in at least one embodiment, further tubing <b>116</b> may extend from row manifold tubing <b>118</b> and may also be part of a secondary cooling loop <b>108</b> but may be referred to as row manifolds. In at least one embodiment, coolant tubing <b>114</b> enters racks as part of a secondary cooling loop <b>108</b> but may be referred to as rack cooling manifold within one or more racks. In at least one embodiment, row manifolds <b>116</b> extend to all racks along a row in a datacenter <b>100</b>. In at least one embodiment, plumbing of a secondary cooling loop <b>108</b>, including coolant manifolds <b>118</b>, <b>116</b>, and <b>114</b> may be improved by at least one embodiment herein. In at least one embodiment, a chiller <b>120</b> may be provided in a primary cooling loop within datacenter <b>102</b> to support cooling before a cooling tower. In at least one embodiment, additional cooling loops that may exist in a primary control loop and that provide cooling external to a rack and external to a secondary cooling loop, may be taken together with a primary cooling loop and is distinct from a secondary cooling loop, for this disclosure.</p><p id="p-0070" num="0069">In at least one embodiment, in operation, heat generated within server trays of provided racks <b>110</b> may be transferred to a coolant exiting one or more racks <b>110</b> via flexible tubing of a row manifold <b>114</b> of a second cooling loop <b>108</b>. In at least one embodiment, second coolant (in a secondary cooling loop <b>108</b>) from a CDU <b>112</b>, for cooling provided racks <b>110</b>, moves towards one or more racks <b>110</b> via provided tubing. In at least one embodiment, second coolant from a CDU <b>112</b> passes from on one side of a room manifold having tubing <b>118</b>, to one side of a rack <b>110</b> via a row manifold <b>116</b>, and through one side of a server tray via different tubing <b>114</b>. In at least one embodiment, spent or returned second coolant (or exiting second coolant carrying heat from computing components) exits out of another side of a server tray (such as enter left side of a rack and exits right side of a rack for a server tray after looping through a server tray or through components on a server tray). In at least one embodiment, spent second coolant that exits a server tray or a rack <b>110</b> comes out of different side (such as exiting side) of tubing <b>114</b> and moves to a parallel, but also exiting side of a row manifold <b>116</b>. In at least one embodiment, from a row manifold <b>116</b>, spent second coolant moves in a parallel portion of a room manifold <b>118</b> and is going in an opposite direction than incoming second coolant (which may also be renewed second coolant), and towards a CDU <b>112</b>.</p><p id="p-0071" num="0070">In at least one embodiment, spent second coolant exchanges its heat with a primary coolant in a primary cooling loop <b>106</b> via a CDU <b>112</b>. In at least one embodiment, spent second coolant may be renewed (such as relatively cooled when compared to a temperature at a spent second coolant stage) and ready to be cycled back to through a second cooling loop <b>108</b> to one or more computing components. In at least one embodiment, various flow and temperature control features in a CDU <b>112</b> enable control of heat exchanged from spent second coolant or flow of second coolant in and out of a CDU <b>112</b>. In at least one embodiment, a CDU <b>112</b> may be also able to control a flow of primary coolant in primary cooling loop <b>106</b>.</p><p id="p-0072" num="0071">In at least one embodiment, server-level features <b>200</b> as illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref> can be associated with an in-rack RDU having a pressure control system for a datacenter cooling system. In at least one embodiment, server-level features <b>200</b> include a server tray or box <b>202</b>. In at least one embodiment, a server tray or box <b>202</b> includes a server manifold <b>204</b> to be intermediately coupled between provided cold plates <b>210</b>A-D of a server tray or box <b>202</b> and rack manifolds of a rack hosting a server tray or box <b>202</b>. In at least one embodiment, a server tray or box <b>202</b> includes one or more cold plates <b>210</b>A-D associated with one or more computing or datacenter components or devices <b>220</b>A-D.</p><p id="p-0073" num="0072">In at least one embodiment, one or more server-level cooling loops <b>214</b>A, B may be provided between a server manifold <b>204</b> and one or more colds plates <b>210</b>A-D. In at least one embodiment, each server-level cooling loop <b>214</b>A; B includes an inlet line <b>210</b> and an outlet line <b>212</b>. In at least one embodiment, when there are series configured cold plates <b>210</b>A, B, an intermediate line <b>216</b> may be provided. In at least one embodiment, one or more cold plates <b>210</b>A-D may support distinct ports and channels for a secondary coolant of a secondary cooling loop or a different fluid, such as a refrigerant, circulated from a pre-loaded R2RHX. In at least one embodiment, secondary coolant for cooling associated computing devices may be provided to a server manifold <b>204</b> via provided inlet and outlets <b>206</b>A, <b>206</b>B. In at least one embodiment, refrigerant for cooling may be provided to a server manifold <b>204</b> via provided inlets and outlets <b>208</b>A, <b>208</b>B.</p><p id="p-0074" num="0073">In at least one embodiment, a server tray <b>202</b> is an immersive-cooled server tray that may be flooded by fluid. In at least one embodiment, a fluid for an immersive-cooled server tray may be a dielectric engineered fluid capable of being used in an immersive-cooled server. In at least one embodiment, a secondary coolant or refrigerant may be used to cool engineered fluid. In at least one embodiment, a refrigerant may be used to cool engineered fluid when a primary cooling loop associated with a secondary cooling loop circulating a secondary coolant has failed or is failing. In at least one embodiment, at least one cold plate therefore has ports for a secondary cooling loop and for a refrigerant cooling loop and can support a refrigerant cooling loop that is activated in an event of a failure in a primary cooling loop. In at least one embodiment, an in-rack RDU with a pressure control system may be used without a secondary cooling loop.</p><p id="p-0075" num="0074">In at least one embodiment, at least one dual-cooling cold plate <b>210</b>B; <b>250</b> may be configured to work alongside regular cold plates <b>210</b>A, C, D. In at least one embodiment, a three-dimensional (3D) blow-up illustration (cold plate <b>250</b>) provides internal detail of at least some features that may be included in a dual-cooling cold plate <b>210</b>B. In at least one embodiment, a tear-through of a first section <b>250</b>B of a cold plate <b>250</b> having microchannels <b>270</b> (also <b>270</b>A) therein illustrates a distinct second section <b>250</b>A having different microchannels <b>264</b>. In at least one embodiment, a regular cold plate may have one set of microchannels <b>264</b>; <b>270</b> instead of two sets illustrated. In at least one embodiment, a dual-cooling cold plate <b>250</b> has distinct paths <b>264</b>, <b>270</b> (each path also referred to as microchannels) for secondary coolant of a secondary cooling loop and for refrigerant of a refrigerant cooling loop. In at least one embodiment, secondary coolant or refrigerant may not be dielectric in property. In at least one embodiment, in a use case of an immersive-cooled server, refrigerant that may be a dielectric engineered fluid may be adapted for both, a cold plate application and an immersive-cooled server tray application.</p><p id="p-0076" num="0075">In at least one embodiment, some microchannels <b>270</b> are paths provided by fins <b>270</b>A or other such aspects that are raised internally and perpendicularly to a base of a cold plate section <b>250</b>B so that there are gaps therebetween for fluid or coolant flow. In at least one embodiment, some microchannels <b>264</b> are fluid pathways in a different cold plate section <b>250</b>A of a cold plate <b>250</b>. In at least one embodiment, some microchannels <b>264</b> for refrigerant represent an evaporation (or evaporator) section for a cold plate <b>250</b>. In at least one embodiment, a flow controller <b>280</b> on an inlet side of a cold plate <b>250</b> may act as an expansion valve; and this enables refrigerant to enter a cold plate <b>250</b> and to expand under lower pressure, as well as to transform phase during absorption of heat from at least one computing device, before exiting a cold plate <b>250</b>. In at least one embodiment, reference to a cold plate, along with its dual-cooling features, may imply a reference to a cold plate that can support at least two types of cooling loops, unless otherwise stated. In at least one embodiment, both types of colds plates receive refrigerant for cooling, but one type can support both, a secondary cooling loop and a refrigerant cooling loop (also referred to as a refrigerant loop). In at least one embodiment, a standard coolant, such as facility water may be used in a secondary cooling loop.</p><p id="p-0077" num="0076">In at least one embodiment, a refrigerant may only support cold plate usage and may not be available for immersive cooling. In at least one embodiment, each type of cold plate receives different refrigerant and secondary coolant from respective refrigerant cooling loops, or a secondary or other cooling loops interfacing with a primary cooling loop. In at least one embodiment, in situations where different fluids (such as coolants) are used with different coolant distribution units (CDUs) of different secondary loops, then a different cooling loops may be suited for a dual-cooling cold plate, along with a refrigerant cooling loop, so that different channels may be used for each of a refrigerant and for different secondary coolants. In at least one embodiment, any cold plate referenced herein is able to work above a dew point to prevent moisture formation, such as at above 20 degrees Fahrenheit, or above a determined ambient dew point.</p><p id="p-0078" num="0077">In at least one embodiment, a dual-cooling cold plate <b>250</b> is adapted to receive a two types of fluids (such as a secondary coolant and a refrigerant) and to keep two types of fluids distinct from each other via their distinct ports <b>252</b>, <b>272</b>; <b>268</b>, <b>262</b> and their distinct paths <b>264</b>, <b>270</b>, such as by distinct sections separated by gaskets and plates (such as in a gasket type cold plate). In at least one embodiment, each distinct path is a fluid path. In at least one embodiment, fluid (such as a refrigerant) from a refrigerant source and a secondary coolant may be provided simultaneously to address additional cooling requirements.</p><p id="p-0079" num="0078">In at least one embodiment, a dual-cooling cold plate <b>250</b> includes ports <b>252</b>, <b>272</b> to receive refrigerant into a cold plate <b>250</b> and to pass refrigerant out of a cold plate <b>250</b>. In at least one embodiment, a dual-cooling cold plate <b>250</b> includes ports <b>268</b>, <b>262</b> to receive a secondary coolant into a cold plate <b>250</b> and to pass a secondary coolant out of a cold plate <b>250</b>. In at least one embodiment, ports <b>252</b>, <b>272</b> may have valve covers <b>254</b>, <b>260</b> (or features of an expansion valve) that may be directional, and pressure controlled to enable expansion of refrigerant through a cold plate <b>250</b>. In at least one embodiment, valve covers may be associated with all provided ports, but expansion valves may be specific to refrigerant inlets. In at least one embodiment, provided valve covers <b>254</b>, <b>260</b> are mechanical features of associated flow controllers that also have corresponding electronic features (such as at least one processor to execute instructions stored in associated memory and to control mechanical features for associated flow controllers).</p><p id="p-0080" num="0079">In at least one embodiment, each valve may be actuated by an electronic feature of an associated flow controller. In at least one embodiment, electronic and mechanical features of provided flow controllers are integrated. In at least one embodiment, electronic and mechanical features of provided flow controllers are physically distinct. In at least one embodiment, reference to flow controllers may be to one or more of provided electronic and mechanical features or to their union but is at least in reference to features enabling control of flow of coolant or refrigerant through each cold plate or an immersion-cooled server tray or box.</p><p id="p-0081" num="0080">In at least one embodiment, electronic features of provided flow controllers receive control signals and assert control over mechanical features. In at least one embodiment, electronic features of provided flow controllers may be actuators or other electronic parts of other similar electromechanical features. In at least one embodiment, flow pumps may be used as flow controllers. In at least one embodiment, impellers, pistons, or bellows may be mechanical features, and an electronic motor and circuitry form electronic features of provided flow controllers.</p><p id="p-0082" num="0081">In at least one embodiment, circuitry of provided flow controllers may include processors, memories, switches, sensors, and other components, altogether forming electronic features of provided flow controllers. In at least one embodiment, provided ports <b>252</b>, <b>262</b>, <b>272</b>, <b>268</b> of provided flow controllers are adapted to either allow entry or to allow egress of an immersive fluid. In at least one embodiment, flow controllers <b>280</b> (capable of acting as an expansion valve) may be associated with fluid lines <b>276</b> (also <b>256</b>, <b>274</b>) that enable entry and egress of a refrigerant to a cold plate <b>210</b>B. In at least one embodiment, other flow controllers may be similarly associated with coolant lines <b>210</b>, <b>216</b>, <b>212</b> (also <b>266</b>, <b>258</b>) to enable entry and egress of a secondary coolant to a cold plate <b>210</b>B.</p><p id="p-0083" num="0082">In at least one embodiment, higher cooling requirements may require higher flow rate or flow volume of a refrigerant. In at least one embodiment, such cooling requirements may be addressed by a pump or other flow controllers to prevent excessive heat build-up in an in-rack RDU with a pressure control system but may be addressed from other series pumps between a cold plate <b>210</b>B and a condenser unit external to a cold plate <b>210</b>B. In at least one embodiment, a size of a pump or other flow controllers may be dictated by a type of a refrigerant used and its thermal properties (such as, minimum temperature in its fluid phase versus maximum temperature in its vapor phase, as well as maximum temperatures of all computing devices associated with such a system). In at least one embodiment, this information may be used to determine a pump or other flow controllers' capacity for a refrigerant in a specific area prior to environmental heat causing it to be ineffective towards removing heat from an associated computing device <b>220</b>B.</p><p id="p-0084" num="0083">In at least one embodiment, refrigerant enters provided fluid lines <b>276</b> via dedicated fluid inlet and outlet lines <b>208</b>A, B. In at least one embodiment, a server manifold <b>204</b> is adapted with channels therein (illustrated by dotted lines) to support distinct paths to distinct fluid lines <b>276</b> (also <b>256</b>, <b>274</b>) and to any remaining loops <b>214</b>A, B that are associated with secondary coolant inlet and outlet lines <b>206</b>A, B. In at least one embodiment, there may be multiple manifolds to support refrigerant and secondary coolant distinctly. In at least one embodiment, there may be multiple manifolds to support entry and egress, distinctly, for each of a refrigerant and a secondary coolant. In at least one embodiment, if a refrigerant is singularly used without a secondary cooling loop, then a fluid flow via one of provided fluid paths (at least within a cold plate or a server tray) may be enabled to a refrigerant source or to a fluid line (such as lines <b>370</b>A, B, distinct from secondary coolant row manifold <b>350</b> in <figref idref="DRAWINGS">FIG. <b>3</b></figref>).</p><p id="p-0085" num="0084">In at least one embodiment, a first flow may be to enable secondary coolant to flow through one or more provided ports <b>252</b>, <b>272</b> and an associated path <b>270</b>. In at least one embodiment, a dual-cooling cold plate <b>250</b> may have isolated plate sections <b>250</b>A, <b>250</b>B that are flooded with a refrigerant and/or a secondary coolant, while being kept distinct from each other by gaskets or seals. In at least one embodiment, a second flow may be to enable refrigerant to flow through provided ports <b>268</b>, <b>262</b>, and an associated path <b>264</b> through fins or microchannels <b>270</b>A that are throughout a base of cold plate section <b>250</b>B.</p><p id="p-0086" num="0085">In at least one embodiment, flow controllers <b>278</b> may be associated with a fluid inlet <b>276</b> and outlet portions at a server manifold <b>204</b> instead of provided flow controllers <b>280</b> at respective cold plates. In at least one embodiment, a first flow uses only refrigerant and may be enabled when a failure is determined in a secondary cooling loop or a primary cooling loop, so that a secondary coolant is unable to effectively absorb heat from at least one computing device. In at least one embodiment, a failure may be that a secondary coolant is not sufficiently cooled via a CDU and so it may be unable to absorb sufficient heat of at least one computing device via its associated cold plate.</p><p id="p-0087" num="0086">In at least one embodiment, rack-level features <b>300</b> as illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref> can be associated with an in-rack refrigerant distribution unit having a pressure control system. In at least one embodiment, rack-level features <b>300</b> include a rack <b>302</b> having brackets <b>304</b>, <b>306</b> to hang cooling manifolds <b>314</b>A, B. In at least one embodiment, while a rack <b>330</b> is separately illustrated from a rack <b>302</b>, this rack <b>330</b> may be illustrative of a rear perspective view of a rack <b>302</b>. In at least one embodiment, as such, brackets <b>334</b>, <b>336</b> provided on rack <b>330</b> are perspective views of brackets <b>304</b>, <b>306</b> provided on rack <b>302</b>. In at least one embodiment, brackets <b>304</b>, <b>306</b> provided for a rack are flat structures against an inner wall of a rack. In at least one embodiment, brackets <b>304</b>, <b>306</b> provided for a rack extend from an inner wall of a rack. In at least one embodiment, brackets <b>304</b>, <b>306</b> provided for a rack are affixed to an inner wall of a rack and have multiple mounting points facing one or more directions, including inside or towards a rear of a rack.</p><p id="p-0088" num="0087">In at least one embodiment, cooling manifolds <b>314</b>A, B may be provided to pass secondary coolant between server-level features <b>200</b> (and illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref> as server trays or boxes <b>308</b>, <b>308</b>A) and a CDU (such as CDU <b>406</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>) of a secondary cooling loop of a datacenter cooling system. In at least one embodiment, different CDUs may serve different racks. In at least one embodiment, different rack cooling manifolds may be distinctly part of a secondary cooling loop and a refrigerant cooling loop.</p><p id="p-0089" num="0088">In at least one embodiment, row manifold <b>350</b> may be part of a secondary cooling loop to feed an inlet rack manifold <b>314</b>A via provided lines <b>310</b>A, <b>310</b>. In at least one embodiment, secondary coolant proceeds via a provided line <b>316</b> to cold plate <b>326</b> to extract or absorb heat from associated computing device <b>324</b> within a server <b>308</b>; and proceeds via a provided line <b>318</b> to outlet rack manifold <b>314</b>B and through provided lines <b>312</b>, <b>312</b>A, and back into a same or a different row manifold <b>350</b>.</p><p id="p-0090" num="0089">In at least one embodiment, an in-rack RDU with a pressure control system can work independent of a secondary cooling loop and can cool at least one computing device associated with a refrigerant enabled (dual cooling or single cooling) cold plate <b>326</b>, via provided lines <b>312</b>B, <b>310</b>B for a refrigerant cooling loop along with refrigerant cooling lines <b>370</b>A, B associated with an R2RHX <b>360</b> that is at least partly within an in-rack RDU (such as in-rack RDU <b>424</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>). In at least one embodiment, an refrigerant-to-air heat exchanger (R2AHX) using condenser unit <b>362</b> and one or more fans <b>364</b> may provide sufficient cooling for a refrigerant to be distributed from an in-rack RDU.</p><p id="p-0091" num="0090">In at least one embodiment, even though illustrated as separate units above a rack, this is illustrative only, and an R2AHX or an R2RHX <b>360</b> is substantially within an in-rack RDU, such as in a 1 U to 3 U server tray or box <b>308</b>A. In at least one embodiment, such an in-rack RDU is a top most server tray or box <b>308</b>A of a rack, but may be located elsewhere in a rack. In at least one embodiment, if an R2RHX is used, at least a second condenser unit <b>374</b> may be associated with such an R2RHX <b>360</b>, and such a second condenser unit <b>374</b> may be external to an in-rack RDU to dissipate heat to an ambient environment. In at least one embodiment, if an R2AHX <b>360</b> is used, there is no further second condenser unit and air from one or more fans <b>364</b> is sufficient to dissipate heat to from an in-rack RDU to an ambient environment. In at least one embodiment, provided lines <b>312</b>B, <b>310</b>B for a refrigerant are coupled to independent cold plates via direct lines <b>320</b>, <b>354</b>, <b>322</b>, and via one or more flow controllers <b>368</b>. In at least one embodiment, a pump or other flow controllers <b>368</b> for controlling flow of refrigerant, by acting as an expansion valve, may be provided on an entry side to a cold plate <b>326</b>.</p><p id="p-0092" num="0091">In at least one embodiment, other such pumps or other flow controllers may include a further function as a bleeder valve to enable pressure stabilization as part of a pressure control system. In at least one embodiment, such a pump or other flow controller <b>368</b>A may be located before an expansion valve associated with a cold plate, such as illustrated and described in reference to in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. In at least one embodiment, such an expansion valve may be within an in-rack RDU <b>308</b>A, such as enabled via a pump or flow controller <b>368</b>A functioning as an expansion valve. In at least one embodiment, pressure stabilization may be also represented by stable pressure of a refrigerant that is enabled to flow between a first condenser unit <b>362</b> (of an R2AHX or an R2RHX <b>360</b>), which is external to a cold plate <b>326</b> but within an in-rack RDU <b>308</b>A, and its evaporator or evaporator section (such as, enabled by channels <b>264</b> in <figref idref="DRAWINGS">FIG. <b>2</b></figref>). In at least one embodiment, a reservoir or other pressure dissipation feature <b>368</b>B may be coupled to a bleeder valve <b>368</b>A to enable pressure bleed from a high-pressure side to a low-pressure side of a condenser unit <b>362</b>. In at least one embodiment, a pump or other flow controller <b>368</b>C may then enable released refrigerant to flow from such a reservoir or other pressure dissipation feature <b>368</b>B into a condenser unit <b>362</b> having heat exchange pipes <b>366</b> therein.</p><p id="p-0093" num="0092">In at least one embodiment, one or more diverter flow controllers <b>310</b>C, <b>312</b>C isolate each of a secondary cooling loop and a refrigerant cooling loop. In at least one embodiment, one or more diverter flow controllers have a downstream expansion valve to provide refrigerant with expansion features for efficient heat absorption at appropriate pressures. In at least one embodiment, a first refrigerant may be used with at least one cold plate <b>326</b> and with an R2AHX or an R2RHX <b>360</b> of an in-rack RDU <b>308</b>A, with such an R2RHX <b>360</b> having a first condenser unit <b>364</b> and a second evaporator section <b>372</b>.</p><p id="p-0094" num="0093">In at least one embodiment, one or more fans <b>364</b> may be associated with lines or gasket plates <b>364</b> of a first condenser unit <b>362</b> to enable efficient heat transfer from a first refrigerant to a second refrigerant. In at least one embodiment, there are no fans <b>364</b>, but a first condenser unit <b>364</b> and a second evaporator section <b>372</b> may interface via a gasket heat exchanger where heat from a first refrigerant is transferred to a second refrigerant via conduction between gasket plates separating such refrigerants. In at least one embodiment, an in-rack RDU <b>308</b> may provide refrigerant for cooling to racks <b>308</b> of a rack <b>302</b> (or of other racks) via direct lines, via a manifold <b>314</b>A, B which may have channels for refrigerant flow distinct from coolant flow, or via other refrigerant-specific manifolds <b>370</b>A, B.</p><p id="p-0095" num="0094">In at least one embodiment, therefore, a datacenter cooling system has an in-rack refrigerant distribution unit (RDU) <b>308</b>A to distribute refrigerant to one or more colds plates. In at least one embodiment, such an in-rack RDU <b>308</b>A may have an associated pressure control system (such as reference number <b>438</b> in <figref idref="DRAWINGS">FIG. <b>4</b></figref>). In at least one embodiment, such a pressure control system may have associated flow controllers or pumps <b>368</b>A, to function as a bleeder valve, and a reservoir or other pressure dissipation feature <b>368</b>B to enable a pressure-drop before an expansion valve <b>368</b> for a liquid-phase of a refrigerant based in part on a first pressure of a liquid-phase of a refrigerant exceeding a first threshold and based in part on a temperature associated with one or more cold plates being below a second threshold.</p><p id="p-0096" num="0095">In at least one embodiment, therefore, an R2AHX or an R2RHX <b>360</b> of an in-rack RDU <b>308</b>A enables radiant, conduction, and convection for heat transfer, of heat extracted using a cold plate having a first evaporator section, from a first condenser unit <b>362</b> of a first refrigerant cooling loop to an ambient environment or to a second evaporator section <b>372</b> of a second refrigerant cooling loop. In at least one embodiment, such heat imparted to a refrigerant of a second evaporator section <b>372</b> may be passed via one of provided lines <b>376</b> to a second condenser unit <b>374</b> for dissipation to an ambient environment. In at least one embodiment, provided lines <b>320</b>, <b>322</b>, <b>354</b> of a rack <b>302</b> may be associated with a refrigerant of a first refrigerant cooling loop that may be different than a refrigerant of a second refrigerant cooling loop. In at least one embodiment, such provided lines may be associated with an inlet <b>310</b>B and an outlet <b>312</b>B to engage with an R2AHX or an R2RHX <b>360</b> and with associated lines <b>370</b>A, B as well as associated components <b>360</b>-<b>380</b>.</p><p id="p-0097" num="0096">In at least one embodiment, a datacenter cooling system includes an R2RHX <b>360</b> that is partly within an in-rack RDU <b>308</b>A and that is associated with a fan <b>364</b> and a condenser unit <b>362</b>, which may have one or more refrigerant pumps or compressors for a refrigeration cycle. In at least one embodiment, an in-rack RDU with a pressure control system includes heat exchange pipes or gasket plates. In at least one embodiment, sections, parts, or components <b>360</b>-<b>380</b> of an in-rack RDU with a pressure control system may be integrated together into a singular unit. In at least one embodiment, sections, parts, or components <b>360</b>-<b>380</b> of such a system may be integrated between racks and may be associated with a rack at bracket areas provided on a rack <b>330</b> or via its brackets <b>334</b>, <b>336</b>. In at least one embodiment, such integration uses a pump or other flow controllers to offer flow stabilization of a refrigerant at an outlet of such an integrated system between provided racks. In at least one embodiment, an in-rack RDU <b>308</b>A is able to distribute first refrigerant to one or more servers of one or more racks which controlling pressure of such first refrigerant using its pressure control system and using cooling requirements associated with temperature from one or more cold plates, computing devices, or areas within such one or more servers or one or more racks.</p><p id="p-0098" num="0097">In at least one embodiment, heat exchange pipes in a second evaporator section <b>372</b> circulates distinct refrigerant (such as, a second refrigerant) than in a first condenser section <b>362</b>. In at least one embodiment, such a feature enables circulation of a first refrigerant entering through one of provided flow controllers <b>368</b> and exiting through another one of such provided flow controllers <b>368</b>. In at least one embodiment, on such an exit side from a first condenser unit <b>362</b>, a pump or other flow controllers may be provided for flow stabilization and to prevent excessive vaporization of refrigerant. In at least one embodiment, a pump or other flow controllers may be used when sudden demands are placed on a first refrigerant cooling loop that may be a closed loop. In at least one embodiment, such a pump pumps first refrigerant that collects to a refrigerant reservoir or other pressure dissipation feature <b>368</b>B. In at least one embodiment, refrigerant of an in-rack RDU may be required to immediately address a high cooling requirement. In at least one embodiment, such refrigerant may be provided by a pump or other flow controllers for controlling refrigerant flow without straining a closed loop of a first refrigerant cooling loop.</p><p id="p-0099" num="0098">In at least one embodiment, such straining is with respect to insufficient refrigerant in fluid form and excess vaporization of refrigerant that may be required to handle lower or higher fluid flow therethrough. In at least one embodiment, heat exchange pipes in a second evaporator section <b>372</b> enables air to circulate for cooling of an R2RHX <b>360</b> within an integrated or separated feature of an in-rack RDU with a pressure control system (RDU, such as reference numeral <b>424</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>). In at least one embodiment, one or more refrigerant or displacement pumps <b>378</b>, <b>380</b> enable circulation of second refrigerant of a second refrigerant cooling loop that interfaces with a first refrigerant cooling loop.</p><p id="p-0100" num="0099">In at least one embodiment, an R2AHX or an R2RHX <b>360</b> of an in-rack RDU with a pressure control system is part of or incorporated within a rack <b>302</b> (or <b>330</b>). In at least one embodiment, a separate facility or primary lines provide refrigerant between one or more R2RHX <b>360</b> and one or more racks <b>302</b>. In at least one embodiment, an R2AHX or an R2RHX <b>360</b> includes channels instead of pipes to pass refrigerant for cooling of refrigerant or dissipation of retained heat therein. In at least one embodiment, a datacenter cooling system is able to address a first cooling requirement of a rack <b>330</b> (or <b>302</b>), in a first mode, by an R2AHX or an R2RHX <b>360</b> of a rack <b>330</b> (and its supporting infrastructure&#x2014;such as, two-phase capable cold plate and compressor or pump <b>378</b>, <b>380</b>).</p><p id="p-0101" num="0100">In at least one embodiment, in a first mode, an R2RHX <b>360</b> may be used to dissipate heat from a second refrigerant of a second refrigerant cooling loop via circulating air from a fan through a second condenser unit <b>374</b>. In at least one embodiment, a second refrigerant cooling loop (such as refrigerant cooling loop <b>434</b> in <figref idref="DRAWINGS">FIG. <b>4</b></figref>) may be used to remove heat from an R2AHX or an R2RHX <b>360</b> and to dissipate such heat external to a datacenter using its own second condenser unit (<b>434</b>B in <figref idref="DRAWINGS">FIG. <b>4</b></figref>) or using air alone. In at least one embodiment, a datacenter cooling system is able to address a second cooling requirement of a rack <b>330</b> (or <b>302</b>) in a second mode by a secondary cooling loop interfacing with a CDU, a primary coolant, and a chilling facility. In at least one embodiment, for high density computing components both modes are in operation for any cooling requirement determined for a rack. In at least one embodiment, flow of refrigerant occurs by default in one or more refrigerant cooling loops, but may be enabled further via provided refrigerant pumps <b>378</b>, <b>380</b> or even compressors. In at least one embodiment, a compressor may be associated with a first condenser unit <b>362</b> and may be located within an in-rack RDU <b>308</b>A.</p><p id="p-0102" num="0101">In at least one embodiment, a first cooling requirement and a second cooling requirement may pertain to different heat features of a datacenter. In at least one embodiment, a first cooling requirement may be associated with heat generated from one or more computing devices that may be addressed by a refrigerant for addressing such a first cooling requirement. In at least one embodiment, a second cooling requirement may be also associated with heat generated from one or more computing devices by being retained within a refrigerant and/or a secondary coolant, via a cold plate, for instance, and that may need dissipation by one or more of a R2RHX and/or by a primary coolant via a CDU. In at least one embodiment, an amount of heat generated, absorbed, extracted, or retained may be a temperature value that needs to be below an operating value or an operating range; or that needs to be maintained at an operating value or range (such as, of at least one computing device).</p><p id="p-0103" num="0102">In at least one embodiment, at least one processor may be provided to determine a temperature associated with a computing device <b>324</b> in a rack <b>330</b> (or <b>302</b>). In at least one embodiment, at least one processor is able to cause a datacenter cooling system to operate in a first mode or a second mode based at least in part on a temperature associated with or determined from a computing device <b>324</b>. In at least one embodiment, at least one processor can enable a first mode of a datacenter cooling system to provide cooling using an R2RHX. In at least one embodiment, such at least one processor can enable a second mode of a datacenter cooling system to provide cooling using a coolant.</p><p id="p-0104" num="0103">In at least one embodiment, an immersive-cooled server <b>352</b> within a rack <b>302</b> (or <b>330</b>) may have its cooling requirements addressed concurrently with an air-cooled, coolant-cooled, or refrigerant-cooled server <b>308</b> within a rack <b>302</b> (or <b>330</b>). In at least one embodiment, an immersive-cooled server <b>352</b> may include a dielectric engineered fluid surrounding a computing device. In at least one embodiment, an immersive-cooled server <b>352</b> may include a second heat exchanger to exchange heat between a dielectric engineered fluid and refrigerant to be circulated in a R2RHX <b>360</b>.</p><p id="p-0105" num="0104">In at least one embodiment, at least one server tray or box <b>308</b>A may be designated for including a control system within an in-rack RDU with a pressure control system so that, if a refrigerant is used, such a system may be isolated from a secondary cooling loop. In at least one embodiment, a control system in a server tray or box <b>308</b>A may include safety features (such as sensors to provide sensor data or proper function), communication features (to communicate with at least one flow controller for an active mode and to communicate to an external monitor), power features to power one or more flow controllers and at least one processor (and its related features), and control features offered by at least one processor that may be associated with at least one flow controller.</p><p id="p-0106" num="0105">In at least one embodiment, a cold plate <b>326</b> may be associated with a computing device <b>324</b>. In at least one embodiment, a cold plate may have first ports for a first portion of microchannels to support a secondary coolant distinctly from a second portion of microchannels that support a refrigerant of a condenser unit <b>362</b>. In at least one embodiment, at least one processor may be adapted to receive sensor inputs from sensors associated with a computing device <b>324</b>. In at least one embodiment, sensors may also be associated with one or more of a rack, a secondary coolant, or a refrigerant. In at least one embodiment, at least one processor may be adapted to determine a first cooling requirement and a second cooling requirement based in part on sensor inputs. In at least one embodiment, sensor inputs may be temperature sensed at one or more time intervals from sensors as described. In at least one embodiment, at least one processor can enable a refrigerant cooling loop to provide cooling for at least one computing device, which is supported by a condenser unit and by pressure stabilization using a pump or other flow controllers functioning as a bleeder valve.</p><p id="p-0107" num="0106">In at least one embodiment, one or more neural networks are adapted to receive sensor inputs from provided sensors and are adapted to infer a first cooling requirement and a second cooling requirement for a datacenter cooling system, such as for a refrigerant cooling loop of a datacenter cooling system. In at least one embodiment, flow controllers of such a refrigerant cooling loop enables flow of a refrigerant therein. In at least one embodiment, at least one processor may cause at least one flow controller to enable flow of a refrigerant through an R2RHX and may be adapted to prevent flow of secondary coolant to a secondary cooling loop. In at least one embodiment, one or more diverter flow controllers <b>310</b>C, <b>312</b>C may be enabled to cause such flow and prevention of flow of a refrigerant and of a secondary coolant. In at least one embodiment, provided lines <b>310</b>B, <b>312</b>B may be provided to fluidly couple with inlet line and outlet line <b>370</b>A, B of a R2RHX <b>360</b>. In at least one embodiment, further flow controllers <b>368</b> may be enabled to prevent or cause flow of a refrigerant through an R2AHX or an R2RHX <b>360</b>.</p><p id="p-0108" num="0107">In at least one embodiment, at least one processor can determine a temperature associated with one or more cold plate. In at least one embodiment, at least one processor can enable one or more flow controllers to cause refrigerant to flow through a pressure control system to reach a second pressure that is below a first threshold. In at least one embodiment, if cooling requirement associated with a temperature of a refrigerant is low, then lesser cooling is required and a pressure of such a refrigerant may be bled so that it flows at a lower flow rates, under lesser pressure, through a cold plate.</p><p id="p-0109" num="0108">In at least one embodiment, at least one processor may be associated with an in-rack RDU to receive sensor inputs from sensors associated with at least one computing device. In at least one embodiment, at least one processor can enable a first refrigerant cooling loop or a second refrigerant cooling loop to provide cooling using one or more cold plates for at least one computing device. In at least one embodiment, one or more neural networks of at least one processor can receive sensor inputs and can infer a cooling requirement for a first refrigerant cooling loop or a second refrigerant cooling loop. In at least one embodiment, a cooling requirement may be in response to a temperature associated with one or more cold plates, as sensed by a sensor. In at least one embodiment, a cooling requirement may be to provide cooling with lesser pressure from a first refrigerant cooling loop and to enable a second refrigerant cooling loop to interface with a first refrigerant cooling loop so that heat may be transferred out from an in-rack RDU.</p><p id="p-0110" num="0109">In at least one embodiment, a refrigerant is cooler on a side exiting a condenser unit <b>364</b>. In at least one embodiment, such a cooler refrigerant has a higher density and can be pumped by a refrigerant pump <b>368</b> that functions as a flow controller to drive coolant at a flow rate or volume to at least one cold plate <b>326</b> either directly using inlet lines <b>320</b>A, <b>320</b> and outlet lines <b>354</b>, <b>322</b>, <b>322</b>A, or indirectly via a manifold <b>314</b>A, <b>314</b>B, which may be used solely for refrigerant or may have separated channels therein for refrigerant and coolant. In at least one embodiment, reduced density refrigerant is caused by heat absorbed from a computing device <b>324</b> and transmitted through an associated cold plate <b>326</b>. In at least one embodiment, such heat makes a refrigerant more buoyant relative to its fluid phase. In at least one embodiment, such reduced density may result in vaporization of a refrigerant and a displacement pump may provide movement for such refrigerant in its vapor phase.</p><p id="p-0111" num="0110">In at least one embodiment, at least one processor may cause flow of a refrigerant when cooling within an R2AHX or an R2RHX <b>360</b> in a first mode, differently than secondary cooling loop-based cooling in an assisted-cooling mode that uses both a secondary coolant and a refrigerant. In at least one embodiment, at least one processor may cause one or more flow controllers <b>368</b> to control a refrigerant when cooling within an R2AHX or an R2RHX <b>360</b> in a second mode that may be more demanding (such as a higher cooling requirement) than a first mode having lesser cooling requirements. In at least one embodiment, one or more flow controllers <b>368</b> may enable more or less of refrigerant to flow therethrough without any back-pressure in a closed system. In at least one embodiment, electrical coupling may be provided to power at least one component of a flow controller. In at least one embodiment, at least one processor may be adapted to receive sensor inputs from sensors associated with at least one computing device, such as computing device <b>324</b>. In at least one embodiment, at least one processor may determine a change in a coolant state based in part on sensor inputs. In at least one embodiment, a coolant state may relate to a temperature of coolant (or refrigerant), a flow rate, a flow volume, or status (such as flowing or not).</p><p id="p-0112" num="0111">In at least one embodiment, a coolant state may be sensed from an egress or an entry to one or more of a cold plate, a rack, or a cooling manifold. In at least one embodiment, at least one processor can cause a datacenter cooling system to operate in a first mode, in an assisted-cooling mode, or a second mode based in part on a change determined for a coolant state. In at least one embodiment, when it is determined that coolant temperatures at an egress from a cold plate are not beyond a threshold (implying that not much heat is being generated by an associated computing device), a first mode may be enabled for a refrigerant to flow through an R2AHX or an R2RHX <b>360</b> and an appropriate cold plate. In at least one embodiment, this enables economical use of a datacenter cooling system to provide cooling from one or more of a secondary cooling loop or an in-rack RDU <b>308</b>A.</p><p id="p-0113" num="0112">In at least one embodiment, when temperature at a hot aisle of a rack, at a vicinity of a computing device, or of a fluid (secondary coolant or local coolant) is determined to be beyond a threshold (implying that more heat is being generated by an associated computing device than can be handled by forced air alone), a second or even an assisted-cooling mode for a datacenter cooling system may be enabled to use more refrigerant, a higher flow rate of a refrigerant, a higher pressure of a refrigerant, or an assist from a secondary cooling loop to provide cooling for a computing device. In at least one embodiment, an assisted-cooling mode engages or enables a secondary cooling loop in addition to a first mode or second mode already provided to cool at least one computing device via an in-rack RDU <b>308</b>A having refrigerant circulating from a cold plate associated with at least one computing device to provide further cooling than provided by a secondary cooling loop.</p><p id="p-0114" num="0113">In at least one embodiment, an in-rack RDU <b>308</b>A is interfaced or associated with at least one cold plate to absorb heat from at least one computing device using a first refrigerant as part of a first refrigerant cooling loop and to dissipate such heat within an R2AHX or an R2RHX <b>360</b> to an ambient environment or to a second refrigerant of a second refrigerant cooling loop. In at least one embodiment, an R2RHX <b>360</b> is further interfaced or associated with a second condenser unit <b>374</b> to dissipate heat from a second refrigerant to an ambient environment. In at least one embodiment, refrigerant pumps and/or displacement pump <b>378</b>, <b>380</b> may be used to assist in second refrigerant flow. In at least one embodiment, a second condenser unit <b>374</b> can cause dissipation of at least part of a heat from a refrigerant to an area external to or within a datacenter, such as a hot aisle of a datacenter. In at least one embodiment, a second condenser unit may have associated flow controllers (such as valves and micro pumps) and lines <b>376</b> prior to association or interfacing with heat exchange pipes of an R2AHX or an R2RHX <b>360</b>.</p><p id="p-0115" num="0114">In at least one embodiment, datacenter-level features <b>400</b> as illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref> can be associated with an in-rack refrigerant distribution unit having a pressure control system. In at least one embodiment, datacenter-level features <b>400</b>, within a datacenter <b>402</b>, may include racks <b>404</b> for hosting one or more server trays or boxes; one or more CDUs <b>406</b> for exchanging heat between a secondary cooling loop <b>412</b> and a primary cooling loop <b>422</b>; one or more row manifolds <b>410</b> for distributing coolant from a CDU <b>406</b>; and associated various flow controllers <b>420</b>, and inlet and outlet lines <b>412</b>, <b>414</b>, <b>416</b>, <b>418</b>.</p><p id="p-0116" num="0115">In at least one embodiment, an in-rack RDU <b>424</b> with pressure control system <b>438</b> is provided in association with each of provided racks <b>404</b> in a datacenter <b>402</b>. In at least one embodiment, an area above racks <b>404</b> is an area for hosting a second condenser unit to discharge heat associated with an R2RHX (together illustrated as unit <b>432</b>), from at least one computing device in at least one serer tray or box <b>404</b>A of at least one rack <b>404</b> during a first mode of operation of a datacenter cooling system. In at least one embodiment, a first and a second condenser units are associated with an R2AHX or R2RHX <b>432</b> that is located within an in-rack RDU <b>424</b>. In at least one embodiment, an R2RHX <b>432</b> enables transfer of heat from a first refrigerant cooling loop to a second refrigerant cooling loop, and subsequently removal of heat from at least one cold plate to a second condenser unit external to a cold plate.</p><p id="p-0117" num="0116">In at least one embodiment, a first refrigerant cooling loop is between one or more cold plates and one or more first condenser units <b>432</b>A, B that may terminate within one or more R2AHX or R2RHX <b>432</b> of one or more in-rack RDUs <b>424</b>. In at least one embodiment, a second refrigerant cooling loop <b>434</b> may be associated with a first refrigerant cooling loop within such an in-rack RDU <b>424</b>. In at least one embodiment, such a second refrigerant cooling loop is supported by one or more second evaporator sections <b>434</b>A that are located within one or more R2RHX <b>432</b> of an in-rack RDU <b>424</b>. In at least one embodiment, second refrigerant within such one or more second evaporator sections <b>424</b>A is enabled to absorb heat from first condenser units <b>432</b>A-N and to dissipate such heat via one or more second condenser units <b>434</b>B.</p><p id="p-0118" num="0117">In at least one embodiment, flow controllers <b>432</b>C such as refrigerant pumps or flow valves may be provided in association with a first condenser unit(s) <b>432</b>A-N (of associated heat exchangers <b>432</b>) and associated first evaporator sections of cold plates within racks <b>404</b>. In at least one embodiment, a second evaporator section <b>434</b>A may be within such an R2RHX <b>432</b> <b>434</b> to extract heat dissipated from a first condenser unit <b>432</b>A; B. In at least one embodiment, a second condenser unit <b>434</b>B is able to dissipate heat extracted from such a first condenser unit(s) <b>432</b>A-N by a same or different refrigerant than in a first refrigerant cooling loop.</p><p id="p-0119" num="0118">In at least one embodiment, one or more pumps or other flow controllers <b>436</b> for controlling a pressure of a refrigerant may be provided in association with (such as, within) an in-rack RDU <b>424</b>. In at least one embodiment, flow rate or flow volume of refrigerant may be provided, also, for flow stabilization of refrigerant (along with such pressure stabilization) for each associated refrigerant cooling loop. In at least one embodiment, different condenser units <b>432</b>A-N may be provided for each R2RHX <b>432</b> within one or more in-rack RDUs <b>424</b>, but there may be multiple such R2RHXs associated with a single external condensing unit <b>432</b>B or a single second refrigerant cooling loop. In at least one embodiment, multiple first refrigerant cooling loops therefore terminate in an area allowing different first condensing units to impart or dissipate heat to a second evaporator section within a combined R2RHX <b>432</b> of one or more RDUs <b>424</b> for such multiple first refrigerant cooling loops and a single second refrigerant cooling loop. In at least one embodiment, multiple first refrigerant cooling loops may interface via multiple R2RHXs and multiple second refrigerant cooling loops.</p><p id="p-0120" num="0119">In at least one embodiment, different racks <b>404</b> of a datacenter cooling system may cooperatively have an in-rack RDU with a pressure control system for liquid cooling. In at least one embodiment, such a configuration may require multiple first refrigerant cooling loops to interface with a single second refrigerant cooling loop via a single R2RHX. In at least one embodiment, an in-rack RDU <b>424</b> may have may include one or more refrigerant pumps <b>436</b>, one or more condenser units <b>432</b>A-N, one or more refrigerant reservoirs, and other refrigeration systems such as filters, strainers, and charging ports.</p><p id="p-0121" num="0120">In at least one embodiment, different row manifolds <b>410</b>A, B may be associated with different racks for coolant or refrigerant purposes. In at least one embodiment, different coolant may be a chemical match or mismatch with respect to a secondary coolant. In at least one embodiment, different fluid sources are provided as redundant features to different CDUs depending on chemistries of different secondary coolant used with each of different provided CDUs. In at least one embodiment, there need not be a secondary cooling loop and CDU for one or more racks <b>404</b>, and instead, an in-rack RDU with a pressure control system within an in-rack RDU may be sufficient to provide cooling for a rack <b>404</b>. In at least one embodiment, these racks not associated with a secondary cooling loop may be sufficiently addressed by one or more in-rack RDUs with pressure control systems.</p><p id="p-0122" num="0121">In at least one embodiment, <figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates that there may be a bypass line and a direct line from a flow controller <b>436</b> of an in-rack RDU <b>424</b>. In at least one embodiment, a bypass line from an R2AHX or R2RHX <b>432</b> of an in-rack RDU <b>424</b> to one or more cold plates or to a manifold is so that refrigerant may be provided at a default pressure by circumventing a pressure control system <b>438</b>. In at least one embodiment, a direct line through a pressure control system <b>438</b> may be provided from R2AHX or R2RHX <b>432</b> to one or more cold plates or to a manifold. In at least one embodiment, at least one processor may be used with distinct or a bypass flow controller to engage a bypass or a direct line depending on a temperature of at least one cold plate or a computing device (and an associated cooling requirement). In at least one embodiment, a default pressure of refrigerant flows via a bypass line to an expansion valve. In at least one embodiment, a pressure reduced flow of refrigerant may be enabled by a direct line to an expansion valve.</p><p id="p-0123" num="0122">In at least one embodiment, a rack <b>404</b> may be associated with at least one processor for operating an in-rack RDU with a pressure control system. In at least one embodiment, a processor may include one or more circuits. In at least one embodiment, one or more circuits of a processor may be adapted to determine cooling requirements for a datacenter cooling system. In at least one embodiment, a processor may cause a first mode or a second mode of operation for a datacenter cooling system to address a first cooling requirement and a second cooling requirement by an in-rack RDU that causes R2AHX or R2RHX to exchange heat between a first refrigerant from a cold plate and a second refrigerant of a second condenser unit or between such a first refrigerant and air of an ambient environment.</p><p id="p-0124" num="0123">In at least one embodiment, such an operation may be independent of a secondary coolant and of a primary coolant from a chilling facility <b>408</b>. In at least one embodiment, a processor may cause an assisted-cooling mode of operation for a datacenter cooling system to address a further cooling requirement (than a first or a second cooling requirement addressed by a refrigerant) by a secondary cooling loop having a row manifold <b>410</b>A, flow controllers <b>416</b>, <b>418</b>, and a CDU <b>406</b> that is, in turn, coupled to a primary cooling loop <b>422</b> having a chilling facility <b>408</b>. In at least one embodiment, distinct from a row manifold <b>410</b>A, a refrigerant manifold <b>410</b>B distributes first refrigerant of a first refrigerant cooling loop to one or more cold plates. In at least one embodiment, further refrigerant manifolds, such as within a rack or a server may interface with a row refrigerant manifold <b>410</b>B. In at least one embodiment, such a row refrigerant manifold <b>410</b>B may directly provide first refrigerant to a cold plate of a server within a rack. In at least one embodiment, each line providing refrigerant from an in-rack RDU may be coupled to a pressure control system that may include at least a bleeder valve to cause reduction in pressure depending on a cooling requirement (and therefore a temperature) associated with a cold plate or a computing device.</p><p id="p-0125" num="0124">In at least one embodiment, a refrigerant cooling loop may be more economical than a secondary cooling loop, but a secondary cooling loop may address higher cooling requirements than a refrigerant cooling loop. In at least one embodiment, a refrigerant operating at lower pressure to address low temperatures of heat generated also enables lesser pressure in a refrigerant system, thereby leading to less wear on such a system. In at least one embodiment, all modes of cooling are caused to occur concurrently for provided racks <b>404</b>. In at least one embodiment, a gasket or pipe heat exchanger may be used as a cold plate to support a secondary coolant and a refrigerant distinctly.</p><p id="p-0126" num="0125">In at least one embodiment, a processor used with an in-rack RDU having a pressure control system includes an output to provide signals for one or more flow controllers <b>436</b>, <b>412</b>, <b>420</b>. In at least one embodiment, one or more flow controllers <b>436</b>, <b>412</b>, <b>420</b> may enable flow of refrigerant through an R2AHX or R2RHX <b>432</b> of an in-rack RDU and may prevent flow of secondary coolant to a secondary cooling loop in a mode of a datacenter cooling system so that an in-rack RDU with a pressure control system provides a singular source of cooling in a rack. In at least one embodiment, this feature enables use of an in-rack RDU with a pressure control system in isolation without a secondary cooling loop, a primary cooling loop, a CDU, and associated chilling towers. In at least one embodiment, such cooling may be provided for a period of time till any issue in a primary cooling loop has be addressed. In at least one embodiment, such cooling may be of a capacity defined by a downtime in a service level agreement (SLA).</p><p id="p-0127" num="0126">In at least one embodiment, a processor used with an in-rack RDU having a pressure control system includes an input to receive sensor inputs from sensors associated with at least one computing device of a rack <b>404</b>. In at least one embodiment, sensors may be concurrently or separately associated with a rack, a secondary coolant, or a refrigerant from an associated cold plate of a rack. In at least one embodiment, a processor may determine a first cooling requirement and a second cooling requirement based in part on sensor inputs from these associated sensors. In at least one embodiment, based in part on sensor inputs from these associated sensors, flow rate may be adjusted for one or more of a primary coolant, a secondary coolant, or a refrigerant through a cold plate (or for a secondary coolant), through a CDU (for a primary coolant), or through an R2AHX or R2RHX <b>432</b> and an adapted cold plate (for a refrigerant).</p><p id="p-0128" num="0127">In at least one embodiment, one or more neural networks may be provided within at least one processor to receive sensor inputs and to infer a first cooling requirement and a second cooling requirement from computing devices or aspects of a datacenter cooling system. In at least one embodiment, one or more neural networks may infer a failure of a secondary cooling loop or a primary cooling loop. In at least one embodiment, based in part on sensor inputs associated with pressures, flow rates, flow volumes, temperature, humidity, and leaks, one or more circuits of a processor may cause one or more flow controllers to support first, second, or assisted-cooling modes of cooling. In at least one embodiment, at least one processor associated with an in-rack RDU can cause one or more flow controllers to enable two or more of: a first refrigerant cooling loop to provide refrigerant, a second refrigerant cooling loop to interface with a first refrigerant cooling loop within an in-rack RDU, or prevent flow of a secondary coolant to a secondary cooling loop.</p><p id="p-0129" num="0128">In at least one embodiment, a processor used with a rack <b>404</b> and an in-rack RDU with a pressure control system includes one or more circuits. In at least one embodiment, one or more circuits of a processor may cause a first mode, a second mode, or an assisted-cooling of different modes of operation for a datacenter cooling system. In at least one embodiment, causing a first mode, a second mode, or an assisted-cooling mode is in reference to causing a datacenter cooling system to operate in a first mode, a second mode, or an assisted-cooling mode.</p><p id="p-0130" num="0129">In at least one embodiment, a datacenter cooling system includes an R2AHX or R2RHX <b>432</b> within an in-rack RDU having a pressure control system for a refrigerant cooling loop. In at least one embodiment, one or more circuits of a processor may be provided to train one or more neural networks to infer cooling requirements from sensor inputs of sensors associated with a rack, a computing device, a secondary coolant, or with a refrigerant from at least one cold plate of a rack. In at least one embodiment, these may be cooling requirements for one or more refrigerant cooling loops. In at least one embodiment, a processor may cause a cold plate to provide cooling for a computing device using an evaporator section within a cold plate. In at least one embodiment, a first refrigerant cooling loop may provide cooling at a default flow rate, but a processor may cause cooling by enabling additional flow therethrough of a refrigerant using such flow controllers.</p><p id="p-0131" num="0130">In at least one embodiment, such cooling may be to remove heat from a computing device by a refrigerant flowing through an evaporator section within a cold plate. In at least one embodiment, as a refrigerant absorbs or removes heats from a computing device, it changes to a vapor phase along with pressure changes there. In at least one embodiment, an evaporator section may be associated with a pumps to perform pressure stabilization prior to a refrigerant entering a cold plate. In at least one embodiment, such refrigerant is a first refrigerant that is distinct from a second refrigerant to extract heat from or absorb heat from a first refrigerant. In at least one embodiment, pressure stabilization may be so that refrigerant is always enabled to flow between a condenser unit that is located external to a cold plate, in an R2RHX, and an evaporator even if heat generation decreases and cooling requirement decreases consequently. In at least one embodiment, a first or a second refrigerant cooling loop is always in operation under a default flow rate or flow volume. In at least one embodiment, enabling flow or enabling a first or a second refrigerant cooling loop is in reference to changing a default flow rate or flow volume from a default flow rate or flow volume of a flow or within a first or a second refrigerant cooling loop.</p><p id="p-0132" num="0131">In at least one embodiment, an output of a processor used with an in-rack RDU with a pressure control system may be adapted to provide signals for one or more flow controllers. In at least one embodiment, this enables flow of refrigerant from an in-rack RDU at a lower pressure than a default pressure. In at least one embodiment, such an output may also enable prevention of flow of secondary coolant to a secondary cooling loop in a first or a second mode of a datacenter cooling system.</p><p id="p-0133" num="0132">In at least one embodiment, such an output may be to control flow rate of refrigerant from refrigerant reservoir of an in-rack RDU. In at least one embodiment, a secondary cooling loop is not used with such an in-rack RDU with a pressure control system; however, when used, then it is possible to use at least one diversion flow controller to cause refrigerant to flow between a cold plate and an R2AHX or R2RHX <b>432</b>, or to cause secondary coolant to flow between a cold plate and a CDU, for concurrent or separate use with an in-rack RDU with a pressure control system. In at least one embodiment, one or more flow controller may be controlled by such an output to support different flow rates of a refrigerant therethrough.</p><p id="p-0134" num="0133">In at least one embodiment, one or more neural networks of a processor may be adapted to receive sensor inputs. In at least one embodiment, one or more neural networks may be trained to infer a first cooling requirement and a second cooling requirement as part of an analysis of prior sensor inputs and prior cooling requirements. In at least one embodiment, one or more neural networks may be trained with correlated data of prior sensor inputs and prior cooling requirements so that new sensor inputs within thresholds of prior sensor inputs may be correlated to prior cooling requirements or variations thereof.</p><p id="p-0135" num="0134">In at least one embodiment, an output of a processor used with an in-rack RDU with a pressure control system may be adapted to provide signals to cause one or more flow controllers to be adjusted in a first mode or a second mode so that refrigerant flow occurs in a first mode or in a second mode, different than secondary coolant flow that occurs in an assisted-cooling mode. In at least one embodiment, refrigerant flow or volume may be increased or decreased to an R2RHX depending on which mode is active.</p><p id="p-0136" num="0135">In at least one embodiment, an input of a processor used with an in-rack RDU with a pressure control system is adapted to receive sensor inputs associated with a temperature from at least one computing device, from secondary coolant, or from refrigerant exiting a cold plate. In at least one embodiment, one or more neural networks of a processor may be trained to infer that a change in coolant state has occurred based in part on a temperature and on prior temperatures of at least one computing device, secondary coolant, or refrigerant. In at least one embodiment, one or more circuits of a processor may be adapted to cause a first mode, a second mode, or an assisted-cooling mode of operation for a datacenter cooling system. In at least one embodiment, one or more circuits can enable or disable a refrigerant cooling loop.</p><p id="p-0137" num="0136">In at least one embodiment, a processor to be used with an in-rack RDU with a pressure control system includes one or more circuits to cause a first mode, a second mode, or an assisted-cooling mode of operation for a datacenter cooling system. In at least one embodiment, one or more circuits or a processor is to include one or more neural networks to infer cooling requirements from sensor inputs of sensors associated with a rack <b>404</b> or with secondary coolant or refrigerant from at least one cold plate. In at least one embodiment, a processor may be adapted to cause a first mode or second mode to address a first or a second cooling requirement by enabling refrigerant flow through a refrigerant-to-refrigerant heat exchanger in one or more flow rates. In at least one embodiment, a processor may be adapted to also cause an assisted-cooling mode to address a further cooling requirement by a secondary cooling loop and a CDU to cool fluid circulating from a cold plate.</p><p id="p-0138" num="0137">In at least one embodiment, a processor has one or more circuits, which are adapted to determine that a pressure of a liquid-phase of a refrigerant exceeds a first threshold, and which are adapted to determine that a temperature associated with one or more cold plates is below a second threshold. In at least one embodiment, a processor can enable a pressure control system of an in-rack refrigerant distribution unit (RDU) to cause a response to a temperature and to cause a pressure-drop, before an expansion valve, for a liquid-phase of a refrigerant.</p><p id="p-0139" num="0138">In at least one embodiment, a processor as such includes an output to provide signals for one or more flow controllers to enable a first refrigerant cooling loop to provide such a refrigerant that may be pressure stabilized or controlled, but can also enable a second refrigerant cooling loop to interface with a first refrigerant cooling loop.</p><p id="p-0140" num="0139">In at least one embodiment, a processor as such can include an input to receive sensor inputs from sensors associated with at least one computing device or with one or more cold plates. In at least one embodiment, a processor can determine a first cooling requirement associated with a refrigerant of a first refrigerant cooling loop or with a refrigerant or a different refrigerant of a second refrigerant cooling loop. In at least one embodiment, a processor can determine a second cooling requirement associated with a secondary cooling loop, where a secondary cooling loop is associated with a primary cooling loop. In at least one embodiment, this at least enables adjustment to a pressure of a first refrigerant through a cold plate using input of a temperature of a second refrigerant that interfaces at an in-rack RDU with a first refrigerant.</p><p id="p-0141" num="0140">In at least one embodiment, a processor as such includes one or more neural networks to receive sensor inputs and to infer a first cooling requirement and a second cooling requirement. In at least one embodiment, a processor as such has one or more neural networks to infer a failure of a secondary cooling loop. In at least one embodiment, one or more circuits can cause one or more flow controllers to activate a first refrigerant cooling loop to provide a refrigerant for a cold plate.</p><p id="p-0142" num="0141">In at least one embodiment, each of at least one processor described throughout <figref idref="DRAWINGS">FIGS. <b>1</b>-<b>4</b></figref> has inference and/or training logic <b>1815</b> that may include, without limitation, code and/or data storage <b>1801</b> to store forward and/or output weight and/or input/output data, and/or other parameters to configure neurons or layers of a neural network trained and/or used for inferencing in aspects of one or more embodiments. In at least one embodiment, training logic <b>1815</b> may include, or be coupled to code and/or data storage <b>1801</b> to store graph code or other software to control timing and/or order, in which weight and/or other parameter information may be to be loaded to configure, logic, including integer and/or floating point units (collectively, arithmetic logic units (ALUs). In at least one embodiment, code, such as graph code, loads weight or other parameter information into processor ALUs based on an architecture of a neural network to which such code corresponds. In at least one embodiment, code and/or data storage <b>1801</b> stores weight parameters and/or input/output data of each layer of a neural network trained or used in conjunction with one or more embodiments during forward propagation of input/output data and/or weight parameters during training and/or inferencing using aspects of one or more embodiments. In at least one embodiment, any portion of code and/or data storage <b>1801</b> may be included with other on-chip or off-chip data storage, including a processor's L1, L2, or L3 cache or system memory.</p><p id="p-0143" num="0142">In at least one embodiment, an inference and/or training logic <b>1815</b> of at least one processor may be part of a building management system (BMS) for controlling flow controllers at one or more of a server-level, a rack-level, and a row-level. In at least one embodiment, a determination to engage a flow controller associated with a secondary cooling loop, an in-rack RDU with a pressure control system, an in-rack RDU, a CDU, cold plates, or other cooling manifolds may be provided to one or more neural networks of an inference and/or training logic <b>1815</b> to cause one or more neural networks to infer which flow controllers to gracefully engage or disengage for coolant requirements for one or more cold plates, servers, racks from either an R2AHX or R2RHX <b>432</b>, or a secondary cooling loop of a datacenter cooling system. In at least one embodiment, increase or decrease of pressure from an in-rack RDU may be enabled by flow controllers that are controlled by an inference and/or training logic <b>1815</b> of at least one processor associated with control logic that may be, in turn, associated with a local cooling loop.</p><p id="p-0144" num="0143">In at least one embodiment, at least one processor may be associated with a local cooling loop and with a secondary cooling loop. In at least one embodiment, at least one processor may be associated with an in-rack RDU with a pressure control system. In at least one embodiment, at least one processor includes control logic, such as inference and/or training logic <b>1815</b> and is associated with at least one flow controller. In at least one embodiment, at least one flow controller may have their own respective processor or micro controller. In at least one embodiment, a processor or a micro controller performs instructions sent to it from a control logic. In at least one embodiment, a control logic may be to determine a change in a coolant state, such as a failure in a secondary cooling loop (such as a CDU and cooling manifolds) or a primary cooling loop (such as a chilling facility, cooling manifolds, and also an associated CDU). In at least one embodiment, a failure may also occur with a cooling manifold requiring replacement. In at least one embodiment, a control logic may cause at least one flow controller to provide a response, such as by engaging an in-rack RDU having multiple refrigerant cooling loops with their respective condenser units, refrigerants, evaporator sections, R2RHXes and a supportive cold plate, to provide cooling for at least one computing device. In at least one embodiment, such engagement may be only to adjust pressure of refrigerant, which refrigerant is always flowing through a cold plate.</p><p id="p-0145" num="0144">In at least one embodiment, a control logic may cause a first signal to at least one flow controller to enable a stopping of a secondary coolant from a secondary cooling loop as part of a coolant response. In at least one embodiment, a control logic may cause a second signal to at least one flow controller to enable a starting of a refrigerant from a refrigerant cooling loop as part of a response. In at least one embodiment, a control logic may receive sensor inputs from sensors associated with secondary coolant of a CDU, a refrigerant, and/or at least one computing device. In at least one embodiment, at least one processor can determine a change in a coolant state based in part on sensor inputs. In at least one embodiment, one or more neural networks of an inference and/or training logic <b>1815</b> may be adapted to receive sensor inputs and to infer a change in a coolant state.</p><p id="p-0146" num="0145">In at least one embodiment, at least one processor may include one or more circuits for one or more neural networks, such as an inference and/or training logic <b>1815</b>. In at least one embodiment, an inference and/or training logic <b>1815</b> may be adapted to infer, from sensor inputs associated with at least one server or at least one rack, a change in a coolant state, such as coolant from a CDU being ineffective or retaining too much heat upon entry into a rack. In at least one embodiment, one or more circuits may be adapted to cause at least one flow controller to provide a response from a refrigerant cooling loop.</p><p id="p-0147" num="0146">In at least one embodiment, control logic associated with one or more circuits may cause a first signal (along with any associated signals) to at least one flow controller to enable a response&#x2014;either from a secondary cooling loop or a refrigerant cooling loop having an in-rack RDU with a pressure control system. In at least one embodiment, a second signal may be provided to at least flow controller and may also enable only an R2AHX or R2RHX <b>432</b> in different modes without a secondary cooling loop but may engage or activate a secondary cooling loop if further cooling is required. In at least one embodiment, a distributed or an integrated architecture is enabled by one or more circuits of at least one processor. In at least one embodiment, a distributed architecture may be supported by distinctly located circuits of one or more circuits.</p><p id="p-0148" num="0147">In at least one embodiment, one or more neural networks of an inference and/or training logic <b>1815</b> may be adapted to infer that an increase or a decrease in cooling requirements of at least one computing component of at least one server. In at least one embodiment, one or more circuits may be adapted to cause a cooling loop to economically address decreased cooling requirements or to supplement increased cooling requirements for at least one computing component. In at least one embodiment, enabling a cooling loop represents a response from a refrigerant cooling loop to preempt a respective increase or a respective decrease in cooling requirements of at least one computing component of at least one server based in part on workload sent to at least one computing component.</p><p id="p-0149" num="0148">In at least one embodiment, at least one processor includes one or more circuits, such as an inference and/or training logic <b>1815</b>, to train one or more neural networks to make inferences from provided data. In at least one embodiment, inference and/or training logic <b>1815</b> may infer, from sensor inputs associated with at least one server or at least one rack, a change in a coolant state. In at least one embodiment, an inference may be used to enable one or more circuits to cause at least one flow controller of a refrigerant cooling loop to provide a response. In at least one embodiment, a response may be to cause a refrigerant response from a refrigerant cooling loop to absorb heat into a refrigerant and to exchange absorbed heat to an environment via condenser unit having fans (or to a second refrigerant cooling loop), instead of a secondary cooling loop having a CDU.</p><p id="p-0150" num="0149">In at least one embodiment, one or more circuits may be adapted to train one or more neural networks to infer that an increase or a decrease in cooling requirements of at least one computing component of at least one server. In at least one embodiment, one or more circuits may be adapted to train one or more neural networks to infer that an increase or a decrease in flow output from a secondary cooling loop is associated with an improper flow of secondary coolant because of a failed CDU or a respective increase or a respective decrease in power requirements of at least one computing component of at least one server.</p><p id="p-0151" num="0150">In at least one embodiment, one or more neural networks may be trained to make inferences by prior associated heat features or cooling requirements from computing devices, servers, or racks, and cooling capacity or capabilities indicated by a fluid source of a local cooling loop, such as by an in-rack RDU with a pressure control system having a specific cooling capability that is above a forced air cooling capability, but that may be below a cooling capability of a secondary cooling loop. In at least one embodiment, prior cooling requirements satisfied by a refrigerant cooling loop may be used to cause one or more neural networks to make similar inferences for future similar cooling requirements (in consideration of small variations there from) to be satisfied by adjusting one or more flow controllers to engage a refrigerant cooling loop at different pressures, and also at different flow volumes or different flow rates.</p><p id="p-0152" num="0151"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates a method <b>500</b> associated with a datacenter cooling system of FIGS. <b>2</b>-<b>4</b>, according to at least one embodiment. In at least one embodiment, a method <b>500</b> includes a step <b>502</b> for providing an in-rack refrigerant distribution unit (RDU) to distribute refrigerant to one or more colds plates. In at least one embodiment, a step <b>504</b> in such a method <b>500</b> is for enabling a determination of a temperature associated with one or more cold plates. In at least one embodiment, a step <b>506</b> in such a method <b>500</b> is for verifying that a temperature associated with one or more cold plates has been determined. In at least one embodiment, step <b>504</b> may be repeated based in part on an outcome from step <b>506</b>. In at least one embodiment, a step <b>508</b> in such a method <b>500</b> is for enabling an in-rack RDU to be associated with a pressure control system.</p><p id="p-0153" num="0152">In at least one embodiment, a bypass line may be provided from an R2RHX of an in-rack RDU to one or more cold plates or to a manifold so that refrigerant may be provided at a default pressure by circumventing a pressure control system. In at least one embodiment, a direct line through a pressure control system may be provided from an R2AHX or R2RHX of an in-rack RDU to one or more cold plates or to a manifold. In at least one embodiment, step <b>508</b> may engage a bypass or a direct line depending on a temperature of at least one cold plate or a computing device (and an associated cooling requirement). In at least one embodiment, a step <b>510</b> in such a method <b>500</b> is for enabling a pressure control system to cause a pressure-drop before an expansion valve for a liquid-phase of a refrigerant based in part on a pressure of a liquid-phase of a refrigerant exceeding a first threshold and based in part on heat dissipated being below a second threshold. In at least one embodiment, therefore, refrigerant may be provided at a reduced pressure caused by a pressure control system bleeding part of an initial pressure in a refrigerant and may be provided from a direct line.</p><p id="p-0154" num="0153">In at least one embodiment, method <b>500</b> may include a further step or a sub-step for determining, using at least one processor, a temperature associated one or more cold plates. In at least one embodiment, such a temperature may be a temperature from heat dissipated into a refrigerant of one or more cold plates or a temperature of a computing device associated with one or more cold plates. In at least one embodiment, method <b>500</b> may include a further step or a sub-step for determining a first cooling requirement or a second cooling requirement using such a temperature. In at least one embodiment, method <b>500</b> may include a further step or a sub-step for causing, based in part on a first cooling requirement or a second cooling requirement, a first refrigerant cooling loop to distribute a first refrigerant, a second refrigerant cooling loop to interface with a first refrigerant cooling loop within an in-rack RDU, or a secondary cooling loop, where a secondary cooling loop may be associated with a primary cooling loop.</p><p id="p-0155" num="0154">In at least one embodiment, method <b>500</b> may include a further step or a sub-step for receiving, in at least one processor, sensor inputs from sensors associated with at least one computing device, a rack, a secondary coolant, one or more cold plates, or a first refrigerant of a first refrigerant cooling loop. In at least one embodiment, method <b>500</b> may include a further step or a sub-step for determining, using at least one processor, a first cooling requirement and a second cooling requirement based in part on such sensor inputs. In at least one embodiment, method <b>500</b> may include a further step or a sub-step for enabling a refrigerant-to-refrigerant heat exchanger (R2RHX) of an in-rack RDU to couple multiple condenser units from multiple racks with a second evaporator section associated with a second refrigerant cooling loop, so that different heat from different racks can be dissipated to an ambient environment from a second condenser unit that is located external to an in-rack RDU.</p><p id="p-0156" num="0155">In at least one embodiment, method <b>500</b> may include a further step or a sub-step for receiving, by at least one processor, sensor inputs from sensors associated with at least one computing device. In at least one embodiment, method <b>500</b> may include a further step or a sub-step for determining, by at least one processor, a change in a coolant state based in part on such sensor inputs. In at least one embodiment, method <b>500</b> may include a further step or a sub-step for causing, based in part on a change in a coolant state, a first refrigerant cooling loop, a second refrigerant cooling loop, or a secondary cooling loop, where such a secondary cooling loop may be associated with a primary cooling loop.</p><heading id="h-0005" level="1">Servers and Data Centers</heading><p id="p-0157" num="0156">The following figures set forth, without limitation, exemplary network server and datacenter based systems that can be used to implement at least one embodiment.</p><p id="p-0158" num="0157"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates a distributed system <b>600</b>, in accordance with at least one embodiment. In at least one embodiment, distributed system <b>600</b> includes one or more client computing devices <b>602</b>, <b>604</b>, <b>606</b>, and <b>608</b>, which are configured to execute and operate a client application such as a web browser, proprietary client, and/or variations thereof over one or more network(s) <b>610</b>. In at least one embodiment, server <b>612</b> may be communicatively coupled with remote client computing devices <b>602</b>, <b>604</b>, <b>606</b>, and <b>608</b> via network <b>610</b>.</p><p id="p-0159" num="0158">In at least one embodiment, server <b>612</b> may be adapted to run one or more services or software applications such as services and applications that may manage session activity of single sign-on (SSO) access across multiple datacenters. In at least one embodiment, server <b>612</b> may also provide other services or software applications can include non-virtual and virtual environments. In at least one embodiment, these services may be offered as web-based or cloud services or under a Software as a Service (SaaS) model to users of client computing devices <b>602</b>, <b>604</b>, <b>606</b>, and/or <b>608</b>. In at least one embodiment, users operating client computing devices <b>602</b>, <b>604</b>, <b>606</b>, and/or <b>608</b> may in turn utilize one or more client applications to interact with server <b>612</b> to utilize services provided by these components.</p><p id="p-0160" num="0159">In at least one embodiment, software components <b>618</b>, <b>620</b> and <b>622</b> of system <b>600</b> are implemented on server <b>612</b>. In at least one embodiment, one or more components of system <b>600</b> and/or services provided by these components may also be implemented by one or more of client computing devices <b>602</b>, <b>604</b>, <b>606</b>, and/or <b>608</b>. In at least one embodiment, users operating client computing devices may then utilize one or more client applications to use services provided by these components. In at least one embodiment, these components may be implemented in hardware, firmware, software, or combinations thereof. It should be appreciated that various different system configurations are possible, which may be different from distributed system <b>600</b>. The embodiment shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref> is thus at least one embodiment of a distributed system for implementing an embodiment system and is not intended to be limiting.</p><p id="p-0161" num="0160">In at least one embodiment, client computing devices <b>602</b>, <b>604</b>, <b>606</b>, and/or <b>608</b> may include various types of computing systems. In at least one embodiment, a client computing device may include portable handheld devices (e.g., an iPhone&#xae;, cellular telephone, an iPad&#xae;, computing tablet, a personal digital assistant (PDA)) or wearable devices (e.g., a Google Glass&#xae; head mounted display), running software such as Microsoft Windows Mobile&#xae;, and/or a variety of mobile operating systems such as iOS, Windows Phone, Android, BlackBerry 10, Palm OS, and/or variations thereof. In at least one embodiment, devices may support various applications such as various Internet-related apps, e-mail, short message service (SMS) applications, and may use various other communication protocols. In at least one embodiment, client computing devices may also include general purpose personal computers including, by way of at least one embodiment, personal computers and/or laptop computers running various versions of Microsoft Windows&#xae;, Apple Macintosh&#xae;, and/or Linux operating systems.</p><p id="p-0162" num="0161">In at least one embodiment, client computing devices can be workstation computers running any of a variety of commercially-available UNIX&#xae; or UNIX-like operating systems, including without limitation a variety of GNU/Linux operating systems, such as Google Chrome OS. In at least one embodiment, client computing devices may also include electronic devices such as a thin-client computer, an Internet-enabled gaming system (e.g., a Microsoft Xbox gaming console with or without a Kinect&#xae; gesture input device), and/or a personal messaging device, capable of communicating over network(s) <b>610</b>. Although distributed system <b>600</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref> is shown with four client computing devices, any number of client computing devices may be supported. Other devices, such as devices with sensors, etc., may interact with server <b>612</b>.</p><p id="p-0163" num="0162">In at least one embodiment, network(s) <b>610</b> in distributed system <b>600</b> may be any type of network that can support data communications using any of a variety of available protocols, including without limitation TCP/IP (transmission control protocol/Internet protocol), SNA (systems network architecture), IPX (Internet packet exchange), AppleTalk, and/or variations thereof. In at least one embodiment, network(s) <b>610</b> can be a local area network (LAN), networks based on Ethernet, Token-Ring, a wide-area network, Internet, a virtual network, a virtual private network (VPN), an intranet, an extranet, a public switched telephone network (PSTN), an infra-red network, a wireless network (e.g., a network operating under any of the Institute of Electrical and Electronics (IEEE) 802.11 suite of protocols, Bluetooth&#xae;, and/or any other wireless protocol), and/or any combination of these and/or other networks.</p><p id="p-0164" num="0163">In at least one embodiment, server <b>612</b> may be composed of one or more general purpose computers, specialized server computers (including, by way of at least one embodiment, PC (personal computer) servers, UNIX&#xae; servers, mid-range servers, mainframe computers, rack-mounted servers, etc.), server farms, server clusters, or any other appropriate arrangement and/or combination. In at least one embodiment, server <b>612</b> can include one or more virtual machines running virtual operating systems, or other computing architectures involving virtualization. In at least one embodiment, one or more flexible pools of logical storage devices can be virtualized to maintain virtual storage devices for a server. In at least one embodiment, virtual networks can be controlled by server <b>612</b> using software defined networking. In at least one embodiment, server <b>612</b> may be adapted to run one or more services or software applications.</p><p id="p-0165" num="0164">In at least one embodiment, server <b>612</b> may run any operating system, as well as any commercially available server operating system. In at least one embodiment, server <b>612</b> may also run any of a variety of additional server applications and/or mid-tier applications, including HTTP (hypertext transport protocol) servers, FTP (file transfer protocol) servers, CGI (common gateway interface) servers, JAVA&#xae; servers, database servers, and/or variations thereof. In at least one embodiment, exemplary database servers include without limitation those commercially available from Oracle, Microsoft, Sybase, IBM (International Business Machines), and/or variations thereof.</p><p id="p-0166" num="0165">In at least one embodiment, server <b>612</b> may include one or more applications to analyze and consolidate data feeds and/or event updates received from users of client computing devices <b>602</b>, <b>604</b>, <b>606</b>, and <b>608</b>. In at least one embodiment, data feeds and/or event updates may include, but are not limited to, Twitter&#xae; feeds, Facebook&#xae; updates or real-time updates received from one or more third party information sources and continuous data streams, which may include real-time events related to sensor data applications, financial tickers, network performance measuring tools (e.g., network monitoring and traffic management applications), clickstream analysis tools, automobile traffic monitoring, and/or variations thereof. In at least one embodiment, server <b>612</b> may also include one or more applications to display data feeds and/or real-time events via one or more display devices of client computing devices <b>602</b>, <b>604</b>, <b>606</b>, and <b>608</b>.</p><p id="p-0167" num="0166">In at least one embodiment, distributed system <b>600</b> may also include one or more databases <b>614</b> and <b>616</b>. In at least one embodiment, databases may provide a mechanism for storing information such as user interactions information, usage patterns information, adaptation rules information, and other information. In at least one embodiment, databases <b>614</b> and <b>616</b> may reside in a variety of locations. In at least one embodiment, one or more of databases <b>614</b> and <b>616</b> may reside on a non-transitory storage medium local to (and/or resident in) server <b>612</b>. In at least one embodiment, databases <b>614</b> and <b>616</b> may be remote from server <b>612</b> and in communication with server <b>612</b> via a network-based or dedicated connection. In at least one embodiment, databases <b>614</b> and <b>616</b> may reside in a storage-area network (SAN). In at least one embodiment, any necessary files for performing functions attributed to server <b>612</b> may be stored locally on server <b>612</b> and/or remotely, as appropriate. In at least one embodiment, databases <b>614</b> and <b>616</b> may include relational databases, such as databases that are adapted to store, update, and retrieve data in response to SQL-formatted commands.</p><p id="p-0168" num="0167"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates an exemplary datacenter <b>700</b>, in accordance with at least one embodiment. In at least one embodiment, datacenter <b>700</b> includes, without limitation, a datacenter infrastructure layer <b>710</b>, a framework layer <b>720</b>, a software layer <b>730</b> and an application layer <b>740</b>.</p><p id="p-0169" num="0168">In at least one embodiment, as shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, datacenter infrastructure layer <b>710</b> may include a resource orchestrator <b>712</b>, grouped computing resources <b>714</b>, and node computing resources (&#x201c;node C.R.s&#x201d;) <b>716</b>(<b>1</b>)-<b>716</b>(N), where &#x201c;N&#x201d; represents any whole, positive integer. In at least one embodiment, node C.R.s <b>716</b>(<b>1</b>)-<b>716</b>(N) may include, but are not limited to, any number of central processing units (&#x201c;CPUs&#x201d;) or other processors (including accelerators, field programmable gate arrays (&#x201c;FPGAs&#x201d;), graphics processors, etc.), memory devices (e.g., dynamic read-only memory), storage devices (e.g., solid state or disk drives), network input/output (&#x201c;NW I/O&#x201d;) devices, network switches, virtual machines (&#x201c;VMs&#x201d;), power modules, and cooling modules, etc. In at least one embodiment, one or more node C.R.s from among node C.R.s <b>716</b>(<b>1</b>)-<b>716</b>(N) may be a server having one or more of above-mentioned computing resources.</p><p id="p-0170" num="0169">In at least one embodiment, grouped computing resources <b>714</b> may include separate groupings of node C.R.s housed within one or more racks (not shown), or many racks housed in datacenters at various geographical locations (also not shown). Separate groupings of node C.R.s within grouped computing resources <b>714</b> may include grouped compute, network, memory or storage resources that may be configured or allocated to support one or more workloads. In at least one embodiment, several node C.R.s including CPUs or processors may grouped within one or more racks to provide compute resources to support one or more workloads. In at least one embodiment, one or more racks may also include any number of power modules, cooling modules, and network switches, in any combination.</p><p id="p-0171" num="0170">In at least one embodiment, resource orchestrator <b>712</b> may configure or otherwise control one or more node C.R.s <b>716</b>(<b>1</b>)-<b>716</b>(N) and/or grouped computing resources <b>714</b>. In at least one embodiment, resource orchestrator <b>712</b> may include a software design infrastructure (&#x201c;SDI&#x201d;) management entity for datacenter <b>700</b>. In at least one embodiment, resource orchestrator <b>712</b> may include hardware, software or some combination thereof.</p><p id="p-0172" num="0171">In at least one embodiment, as shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, framework layer <b>720</b> includes, without limitation, a job scheduler <b>732</b>, a configuration manager <b>734</b>, a resource manager <b>736</b> and a distributed file system <b>738</b>. In at least one embodiment, framework layer <b>720</b> may include a framework to support software <b>752</b> of software layer <b>730</b> and/or one or more application(s) <b>742</b> of application layer <b>740</b>. In at least one embodiment, software <b>752</b> or application(s) <b>742</b> may respectively include web-based service software or applications, such as those provided by Amazon Web Services, Google Cloud and Microsoft Azure. In at least one embodiment, framework layer <b>720</b> may be, but is not limited to, a type of free and open-source software web application framework such as Apache Spark&#x2122; (hereinafter &#x201c;Spark&#x201d;) that may utilize distributed file system <b>738</b> for large-scale data processing (e.g., &#x201c;big data&#x201d;). In at least one embodiment, job scheduler <b>732</b> may include a Spark driver to facilitate scheduling of workloads supported by various layers of datacenter <b>700</b>. In at least one embodiment, configuration manager <b>734</b> may be capable of configuring different layers such as software layer <b>730</b> and framework layer <b>720</b>, including Spark and distributed file system <b>738</b> for supporting large-scale data processing. In at least one embodiment, resource manager <b>736</b> may be capable of managing clustered or grouped computing resources mapped to or allocated for support of distributed file system <b>738</b> and job scheduler <b>732</b>. In at least one embodiment, clustered or grouped computing resources may include grouped computing resource <b>714</b> at datacenter infrastructure layer <b>710</b>. In at least one embodiment, resource manager <b>736</b> may coordinate with resource orchestrator <b>712</b> to manage these mapped or allocated computing resources.</p><p id="p-0173" num="0172">In at least one embodiment, software <b>752</b> included in software layer <b>730</b> may include software used by at least portions of node C.R.s <b>716</b>(<b>1</b>)-<b>716</b>(N), grouped computing resources <b>714</b>, and/or distributed file system <b>738</b> of framework layer <b>720</b>. One or more types of software may include, but are not limited to, Internet web page search software, e-mail virus scan software, database software, and streaming video content software.</p><p id="p-0174" num="0173">In at least one embodiment, application(s) <b>742</b> included in application layer <b>740</b> may include one or more types of applications used by at least portions of node C.R.s <b>716</b>(<b>1</b>)-<b>716</b>(N), grouped computing resources <b>714</b>, and/or distributed file system <b>738</b> of framework layer <b>720</b>. In at least one or more types of applications may include, without limitation, CUDA applications, 5G network applications, artificial intelligence application, datacenter applications, and/or variations thereof.</p><p id="p-0175" num="0174">In at least one embodiment, any of configuration manager <b>734</b>, resource manager <b>736</b>, and resource orchestrator <b>712</b> may implement any number and type of self-modifying actions based on any amount and type of data acquired in any technically feasible fashion. In at least one embodiment, self-modifying actions may relieve a datacenter operator of datacenter <b>700</b> from making possibly bad configuration decisions and possibly avoiding underutilized and/or poor performing portions of a datacenter.</p><p id="p-0176" num="0175"><figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates a client-server network <b>804</b> formed by a plurality of network server computers <b>802</b> which are interlinked, in accordance with at least one embodiment. In at least one embodiment, each network server computer <b>802</b> stores data accessible to other network server computers <b>802</b> and to client computers <b>806</b> and networks <b>808</b> which link into a wide area network <b>804</b>. In at least one embodiment, configuration of a client-server network <b>804</b> may change over time as client computers <b>806</b> and one or more networks <b>808</b> connect and disconnect from a network <b>804</b>, and as one or more trunk line server computers <b>802</b> are added or removed from a network <b>804</b>. In at least one embodiment, when a client computer <b>806</b> and a network <b>808</b> are connected with network server computers <b>802</b>, client-server network includes such client computer <b>806</b> and network <b>808</b>. In at least one embodiment, the term computer includes any device or machine capable of accepting data, applying prescribed processes to data, and supplying results of processes.</p><p id="p-0177" num="0176">In at least one embodiment, client-server network <b>804</b> stores information which is accessible to network server computers <b>802</b>, remote networks <b>808</b> and client computers <b>806</b>. In at least one embodiment, network server computers <b>802</b> are formed by main frame computers minicomputers, and/or microcomputers having one or more processors each. In at least one embodiment, server computers <b>802</b> are linked together by wired and/or wireless transfer media, such as conductive wire, fiber optic cable, and/or microwave transmission media, satellite transmission media or other conductive, optic or electromagnetic wave transmission media. In at least one embodiment, client computers <b>806</b> access a network server computer <b>802</b> by a similar wired or a wireless transfer medium. In at least one embodiment, a client computer <b>806</b> may link into a client-server network <b>804</b> using a modem and a standard telephone communication network. In at least one embodiment, alternative carrier systems such as cable and satellite communication systems also may be used to link into client-server network <b>804</b>. In at least one embodiment, other private or time-shared carrier systems may be used. In at least one embodiment, network <b>804</b> is a global information network, such as the Internet. In at least one embodiment, network is a private intranet using similar protocols as the Internet, but with added security measures and restricted access controls. In at least one embodiment, network <b>804</b> is a private, or semi-private network using proprietary communication protocols.</p><p id="p-0178" num="0177">In at least one embodiment, client computer <b>806</b> is any end user computer, and may also be a mainframe computer, mini-computer or microcomputer having one or more microprocessors. In at least one embodiment, server computer <b>802</b> may at times function as a client computer accessing another server computer <b>802</b>. In at least one embodiment, remote network <b>808</b> may be a local area network, a network added into a wide area network through an independent service provider (ISP) for the Internet, or another group of computers interconnected by wired or wireless transfer media having a configuration which is either fixed or changing over time. In at least one embodiment, client computers <b>806</b> may link into and access a network <b>804</b> independently or through a remote network <b>808</b>.</p><p id="p-0179" num="0178"><figref idref="DRAWINGS">FIG. <b>9</b></figref> illustrates a computer network <b>908</b> connecting one or more computing machines, in accordance with at least one embodiment. In at least one embodiment, network <b>908</b> may be any type of electronically connected group of computers including, for instance, the following networks: Internet, Intranet, Local Area Networks (LAN), Wide Area Networks (WAN) or an interconnected combination of these network types. In at least one embodiment, connectivity within a network <b>908</b> may be a remote modem, Ethernet (IEEE 802.3), Token Ring (IEEE 802.5), Fiber Distributed Datalink Interface (FDDI), Asynchronous Transfer Mode (ATM), or any other communication protocol. In at least one embodiment, computing devices linked to a network may be desktop, server, portable, handheld, set-top box, personal digital assistant (PDA), a terminal, or any other desired type or configuration. In at least one embodiment, depending on their functionality, network connected devices may vary widely in processing power, internal memory, and other performance aspects.</p><p id="p-0180" num="0179">In at least one embodiment, communications within a network and to or from computing devices connected to a network may be either wired or wireless. In at least one embodiment, network <b>908</b> may include, at least in part, the world-wide public Internet which generally connects a plurality of users in accordance with a client-server model in accordance with a transmission control protocol/internet protocol (TCP/IP) specification. In at least one embodiment, client-server network is a dominant model for communicating between two computers. In at least one embodiment, a client computer (&#x201c;client&#x201d;) issues one or more commands to a server computer (&#x201c;server&#x201d;). In at least one embodiment, server fulfills client commands by accessing available network resources and returning information to a client pursuant to client commands. In at least one embodiment, client computer systems and network resources resident on network servers are assigned a network address for identification during communications between elements of a network. In at least one embodiment, communications from other network connected systems to servers will include a network address of a relevant server/network resource as part of communication so that an appropriate destination of a data/request is identified as a recipient. In at least one embodiment, when a network <b>908</b> comprises the global Internet, a network address is an IP address in a TCP/IP format which may, at least in part, route data to an e-mail account, a website, or other Internet tool resident on a server. In at least one embodiment, information and services which are resident on network servers may be available to a web browser of a client computer through a domain name (e.g. www.site.com) which maps to an IP address of a network server.</p><p id="p-0181" num="0180">In at least one embodiment, a plurality of clients <b>902</b>, <b>904</b>, and <b>906</b> are connected to a network <b>908</b> via respective communication links. In at least one embodiment, each of these clients may access a network <b>908</b> via any desired form of communication, such as via a dial-up modem connection, cable link, a digital subscriber line (DSL), wireless or satellite link, or any other form of communication. In at least one embodiment, each client may communicate using any machine that is compatible with a network <b>908</b>, such as a personal computer (PC), work station, dedicated terminal, personal data assistant (PDA), or other similar equipment. In at least one embodiment, clients <b>902</b>, <b>904</b>, and <b>906</b> may or may not be located in a same geographical area.</p><p id="p-0182" num="0181">In at least one embodiment, a plurality of servers <b>910</b>, <b>912</b>, and <b>914</b> are connected to a network <b>918</b> to serve clients that are in communication with a network <b>918</b>. In at least one embodiment, each server is typically a powerful computer or device that manages network resources and responds to client commands. In at least one embodiment, servers include computer readable data storage media such as hard disk drives and RAM memory that store program instructions and data. In at least one embodiment, servers <b>910</b>, <b>912</b>, <b>914</b> run application programs that respond to client commands. In at least one embodiment, server <b>910</b> may run a web server application for responding to client requests for HTML pages and may also run a mail server application for receiving and routing electronic mail. In at least one embodiment, other application programs, such as an FTP server or a media server for streaming audio/video data to clients may also be running on a server <b>910</b>. In at least one embodiment, different servers may be dedicated to performing different tasks. In at least one embodiment, server <b>910</b> may be a dedicated web server that manages resources relating to web sites for various users, whereas a server <b>912</b> may be dedicated to provide electronic mail (email) management. In at least one embodiment, other servers may be dedicated for media (audio, video, etc.), file transfer protocol (FTP), or a combination of any two or more services that are typically available or provided over a network. In at least one embodiment, each server may be in a location that is the same as or different from that of other servers. In at least one embodiment, there may be multiple servers that perform mirrored tasks for users, thereby relieving congestion or minimizing traffic directed to and from a single server. In at least one embodiment, servers <b>910</b>, <b>912</b>, <b>914</b> are under control of a web hosting provider in a business of maintaining and delivering third party content over a network <b>918</b>.</p><p id="p-0183" num="0182">In at least one embodiment, web hosting providers deliver services to two different types of clients. In at least one embodiment, one type, which may be referred to as a browser, requests content from servers <b>910</b>, <b>912</b>, <b>914</b> such as web pages, email messages, video clips, etc. In at least one embodiment, a second type, which may be referred to as a user, hires a web hosting provider to maintain a network resource such as a web site, and to make it available to browsers. In at least one embodiment, users contract with a web hosting provider to make memory space, processor capacity, and communication bandwidth available for their desired network resource in accordance with an amount of server resources a user desires to utilize.</p><p id="p-0184" num="0183">In at least one embodiment, in order for a web hosting provider to provide services for both of these clients, application programs which manage a network resources hosted by servers must be properly configured. In at least one embodiment, program configuration process involves defining a set of parameters which control, at least in part, an application program's response to browser requests and which also define, at least in part, a server resources available to a particular user.</p><p id="p-0185" num="0184">In one embodiment, an intranet server <b>916</b> is in communication with a network <b>908</b> via a communication link. In at least one embodiment, intranet server <b>916</b> is in communication with a server manager <b>918</b>. In at least one embodiment, server manager <b>918</b> comprises a database of an application program configuration parameters which are being utilized in servers <b>910</b>, <b>912</b>, <b>914</b>. In at least one embodiment, users modify a database <b>920</b> via an intranet <b>916</b>, and a server manager <b>918</b> interacts with servers <b>910</b>, <b>912</b>, <b>914</b> to modify application program parameters so that they match a content of a database. In at least one embodiment, a user logs onto an intranet server <b>916</b> by connecting to an intranet <b>916</b> via computer <b>902</b> and entering authentication information, such as a username and password.</p><p id="p-0186" num="0185">In at least one embodiment, when a user wishes to sign up for new service or modify an existing service, an intranet server <b>916</b> authenticates a user and provides a user with an interactive screen display/control panel that allows a user to access configuration parameters for a particular application program. In at least one embodiment, a user is presented with a number of modifiable text boxes that describe aspects of a configuration of a user's web site or other network resource. In at least one embodiment, if a user desires to increase memory space reserved on a server for its web site, a user is provided with a field in which a user specifies a desired memory space. In at least one embodiment, in response to receiving this information, an intranet server <b>916</b> updates a database <b>920</b>. In at least one embodiment, server manager <b>918</b> forwards this information to an appropriate server, and a new parameter is used during application program operation. In at least one embodiment, an intranet server <b>916</b> is configured to provide users with access to configuration parameters of hosted network resources (e.g., web pages, email, FTP sites, media sites, etc.), for which a user has contracted with a web hosting service provider.</p><p id="p-0187" num="0186"><figref idref="DRAWINGS">FIG. <b>10</b>A</figref> illustrates a networked computer system <b>1000</b>A, in accordance with at least one embodiment. In at least one embodiment, networked computer system <b>1000</b>A comprises a plurality of nodes or personal computers (&#x201c;PCs&#x201d;) <b>1002</b>, <b>1018</b>, <b>1020</b>. In at least one embodiment, personal computer or node <b>1002</b> comprises a processor <b>1014</b>, memory <b>1016</b>, video camera <b>1004</b>, microphone <b>1006</b>, mouse <b>1008</b>, speakers <b>1010</b>, and monitor <b>1012</b>. In at least one embodiment, PCs <b>1002</b>, <b>1018</b>, <b>1020</b> may each run one or more desktop servers of an internal network within a given company, for instance, or may be servers of a general network not limited to a specific environment. In at least one embodiment, there is one server per PC node of a network, so that each PC node of a network represents a particular network server, having a particular network URL address. In at least one embodiment, each server defaults to a default web page for that server's user, which may itself contain embedded URLs pointing to further subpages of that user on that server, or to other servers or pages on other servers on a network.</p><p id="p-0188" num="0187">In at least one embodiment, nodes <b>1002</b>, <b>1018</b>, <b>1020</b> and other nodes of a network are interconnected via medium <b>1022</b>. In at least one embodiment, medium <b>1022</b> may be, a communication channel such as an Integrated Services Digital Network (&#x201c;ISDN&#x201d;). In at least one embodiment, various nodes of a networked computer system may be connected through a variety of communication media, including local area networks (&#x201c;LANs&#x201d;), plain-old telephone lines (&#x201c;POTS&#x201d;), sometimes referred to as public switched telephone networks (&#x201c;PSTN&#x201d;), and/or variations thereof. In at least one embodiment, various nodes of a network may also constitute computer system users inter-connected via a network such as the Internet. In at least one embodiment, each server on a network (running from a particular node of a network at a given instance) has a unique address or identification within a network, which may be specifiable in terms of an URL.</p><p id="p-0189" num="0188">In at least one embodiment, a plurality of multi-point conferencing units (&#x201c;MCUs&#x201d;) may thus be utilized to transmit data to and from various nodes or &#x201c;endpoints&#x201d; of a conferencing system. In at least one embodiment, nodes and/or MCUs may be interconnected via an ISDN link or through a local area network (&#x201c;LAN&#x201d;), in addition to various other communications media such as nodes connected through the Internet. In at least one embodiment, nodes of a conferencing system may, in general, be connected directly to a communications medium such as a LAN or through an MCU, and that a conferencing system may comprise other nodes or elements such as routers, servers, and/or variations thereof.</p><p id="p-0190" num="0189">In at least one embodiment, processor <b>1014</b> is a general-purpose programmable processor. In at least one embodiment, processors of nodes of networked computer system <b>1000</b>A may also be special-purpose video processors. In at least one embodiment, various peripherals and components of a node such as those of node <b>1002</b> may vary from those of other nodes. In at least one embodiment, node <b>1018</b> and node <b>1020</b> may be configured identically to or differently than node <b>1002</b>. In at least one embodiment, a node may be implemented on any suitable computer system in addition to PC systems.</p><p id="p-0191" num="0190"><figref idref="DRAWINGS">FIG. <b>10</b>B</figref> illustrates a networked computer system <b>1000</b>B, in accordance with at least one embodiment. In at least one embodiment, system <b>1000</b>B illustrates a network such as LAN <b>1024</b>, which may be used to interconnect a variety of nodes that may communicate with each other. In at least one embodiment, attached to LAN <b>1024</b> are a plurality of nodes such as PC nodes <b>1026</b>, <b>1028</b>, <b>1030</b>. In at least one embodiment, a node may also be connected to the LAN via a network server or other means. In at least one embodiment, system <b>1000</b>B comprises other types of nodes or elements, for at least one embodiment including routers, servers, and nodes.</p><p id="p-0192" num="0191"><figref idref="DRAWINGS">FIG. <b>10</b>C</figref> illustrates a networked computer system <b>1000</b>C, in accordance with at least one embodiment. In at least one embodiment, system <b>1000</b>C illustrates a WWW system having communications across a backbone communications network such as Internet <b>1032</b>, which may be used to interconnect a variety of nodes of a network. In at least one embodiment, WWW is a set of protocols operating on top of the Internet, and allows a graphical interface system to operate thereon for accessing information through the Internet. In at least one embodiment, attached to Internet <b>1032</b> in WWW are a plurality of nodes such as PCs <b>1040</b>, <b>1042</b>, <b>1044</b>. In at least one embodiment, a node is interfaced to other nodes of WWW through a WWW HTTP server such as servers <b>1034</b>, <b>1036</b>. In at least one embodiment, PC <b>1044</b> may be a PC forming a node of network <b>1032</b> and itself running its server <b>1036</b>, although PC <b>1044</b> and server <b>1036</b> are illustrated separately in <figref idref="DRAWINGS">FIG. <b>10</b>C</figref> for illustrative purposes.</p><p id="p-0193" num="0192">In at least one embodiment, WWW is a distributed type of application, characterized by WWW HTTP, WWW's protocol, which runs on top of the Internet's transmission control protocol/Internet protocol (&#x201c;TCP/IP&#x201d;). In at least one embodiment, WWW may thus be characterized by a set of protocols (i.e., HTTP) running on the Internet as its &#x201c;backbone.&#x201d;</p><p id="p-0194" num="0193">In at least one embodiment, a web browser is an application running on a node of a network that, in WWW-compatible type network systems, allows users of a particular server or node to view such information and thus allows a user to search graphical and text-based files that are linked together using hypertext links that are embedded in documents or files available from servers on a network that understand HTTP. In at least one embodiment, when a given web page of a first server associated with a first node is retrieved by a user using another server on a network such as the Internet, a document retrieved may have various hypertext links embedded therein and a local copy of a page is created local to a retrieving user. In at least one embodiment, when a user clicks on a hypertext link, locally-stored information related to a selected hypertext link is typically sufficient to allow a user's machine to open a connection across the Internet to a server indicated by a hypertext link.</p><p id="p-0195" num="0194">In at least one embodiment, more than one user may be coupled to each HTTP server, through a LAN such as LAN <b>1038</b> as illustrated with respect to WWW HTTP server <b>1034</b>. In at least one embodiment, system <b>1000</b>C may also comprise other types of nodes or elements. In at least one embodiment, a WWW HTTP server is an application running on a machine, such as a PC. In at least one embodiment, each user may be considered to have a unique &#x201c;server,&#x201d; as illustrated with respect to PC <b>1044</b>. In at least one embodiment, a server may be considered to be a server such as WWW HTTP server <b>1034</b>, which provides access to a network for a LAN or plurality of nodes or plurality of LANs. In at least one embodiment, there are a plurality of users, each having a desktop PC or node of a network, each desktop PC potentially establishing a server for a user thereof. In at least one embodiment, each server is associated with a particular network address or URL, which, when accessed, provides a default web page for that user. In at least one embodiment, a web page may contain further links (embedded URLs) pointing to further subpages of that user on that server, or to other servers on a network or to pages on other servers on a network.</p><heading id="h-0006" level="1">Cloud Computing and Services</heading><p id="p-0196" num="0195">The following figures set forth, without limitation, exemplary cloud-based systems that can be used to implement at least one embodiment.</p><p id="p-0197" num="0196">In at least one embodiment, cloud computing is a style of computing in which dynamically scalable and often virtualized resources are provided as a service over the Internet. In at least one embodiment, users need not have knowledge of, expertise in, or control over technology infrastructure, which can be referred to as &#x201c;in the cloud,&#x201d; that supports them. In at least one embodiment, cloud computing incorporates infrastructure as a service, platform as a service, software as a service, and other variations that have a common theme of reliance on the Internet for satisfying computing needs of users. In at least one embodiment, a typical cloud deployment, such as in a private cloud (e.g., enterprise network), or a datacenter (DC) in a public cloud (e.g., Internet) can consist of thousands of servers (or alternatively, VMs), hundreds of Ethernet, Fiber Channel or Fiber Channel over Ethernet (FCoE) ports, switching and storage infrastructure, etc. In at least one embodiment, cloud can also consist of network services infrastructure like IPsec VPN hubs, firewalls, load balancers, wide area network (WAN) optimizers etc. In at least one embodiment, remote subscribers can access cloud applications and services securely by connecting via a VPN tunnel, such as an IPsec VPN tunnel.</p><p id="p-0198" num="0197">In at least one embodiment, cloud computing is a model for enabling convenient, on-demand network access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal management effort or service provider interaction.</p><p id="p-0199" num="0198">In at least one embodiment, cloud computing is characterized by on-demand self-service, in which a consumer can unilaterally provision computing capabilities, such as server time and network storage, as needed automatically without requiring human inter-action with each service's provider. In at least one embodiment, cloud computing is characterized by broad network access, in which capabilities are available over a network and accessed through standard mechanisms that promote use by heterogeneous thin or thick client platforms (e.g., mobile phones, laptops, and PDAs). In at least one embodiment, cloud computing is characterized by resource pooling, in which a provider's computing resources are pooled to serve multiple consumers using a multi-tenant model, with different physical and virtual resources dynamically as-signed and reassigned according to consumer demand. In at least one embodiment, there is a sense of location independence in that a customer generally has no control or knowledge over an exact location of provided resources, but may be able to specify location at a higher level of abstraction (e.g., country, state, or datacenter).</p><p id="p-0200" num="0199">In at least one embodiment, resources include storage, processing, memory, network bandwidth, and virtual machines. In at least one embodiment, cloud computing is characterized by rapid elasticity, in which capabilities can be rapidly and elastically provisioned, in some cases automatically, to quickly scale out and rapidly released to quickly scale in. In at least one embodiment, to a consumer, capabilities available for provisioning often appear to be unlimited and can be purchased in any quantity at any time. In at least one embodiment, cloud computing is characterized by measured service, in which cloud systems automatically control and optimize resource use by leveraging a metering capability at some level of abstraction appropriate to a type of service (e.g., storage, processing, bandwidth, and active user accounts). In at least one embodiment, resource usage can be monitored, controlled, and reported providing transparency for both a provider and consumer of a utilized service.</p><p id="p-0201" num="0200">In at least one embodiment, cloud computing may be associated with various services. In at least one embodiment, cloud Software as a Service (SaaS) may refer to as service in which a capability provided to a consumer is to use a provider's applications running on a cloud infrastructure. In at least one embodiment, applications are accessible from various client devices through a thin client interface such as a web browser (e.g., web-based email). In at least one embodiment, consumer does not manage or control underlying cloud infrastructure including network, servers, operating systems, storage, or even individual application capabilities, with a possible exception of limited user-specific application configuration settings.</p><p id="p-0202" num="0201">In at least one embodiment, cloud Platform as a Service (PaaS) may refer to a service in which a capability provided to a consumer is to deploy onto cloud infrastructure consumer-created or acquired applications created using programming languages and tools supported by a provider. In at least one embodiment, consumer does not manage or control underlying cloud infrastructure including networks, servers, operating systems, or storage, but has control over deployed applications and possibly application hosting environment configurations.</p><p id="p-0203" num="0202">In at least one embodiment, cloud Infrastructure as a Service (IaaS) may refer to a service in which a capability provided to a consumer is to provision processing, storage, networks, and other fundamental computing resources where a consumer is able to deploy and run arbitrary software, which can include operating systems and applications. In at least one embodiment, consumer does not manage or control underlying cloud infrastructure, but has control over operating systems, storage, deployed applications, and possibly limited control of select networking components (e.g., host firewalls).</p><p id="p-0204" num="0203">In at least one embodiment, cloud computing may be deployed in various ways. In at least one embodiment, a private cloud may refer to a cloud infrastructure that is operated solely for an organization. In at least one embodiment, a private cloud may be managed by an organization or a third party and may exist on-premises or off-premises. In at least one embodiment, a community cloud may refer to a cloud infrastructure that is shared by several organizations and supports a specific community that has shared concerns (e.g., mission, security requirements, policy, and compliance considerations). In at least one embodiment, a community cloud may be managed by organizations or a third party and may exist on-premises or off-premises. In at least one embodiment, a public cloud may refer to a cloud infrastructure that is made available to a general public or a large industry group and is owned by an organization providing cloud services. In at least one embodiment, a hybrid cloud may refer to a cloud infrastructure is a composition of two or more clouds (private, community, or public) that remain unique entities, but are bound together by standardized or proprietary technology that enables data and application portability (e.g., cloud bursting for load-balancing between clouds). In at least one embodiment, a cloud computing environment is service oriented with a focus on statelessness, low coupling, modularity, and semantic interoperability.</p><p id="p-0205" num="0204"><figref idref="DRAWINGS">FIG. <b>11</b></figref> illustrates one or more components of a system environment <b>1100</b> in which services may be offered as third party network services, in accordance with at least one embodiment. In at least one embodiment, a third party network may be referred to as a cloud, cloud network, cloud computing network, and/or variations thereof. In at least one embodiment, system environment <b>1100</b> includes one or more client computing devices <b>1104</b>, <b>1106</b>, and <b>1108</b> that may be used by users to interact with a third party network infrastructure system <b>1102</b> that provides third party network services, which may be referred to as cloud computing services. In at least one embodiment, third party network infrastructure system <b>1102</b> may comprise one or more computers and/or servers.</p><p id="p-0206" num="0205">It should be appreciated that third party network infrastructure system <b>1102</b> depicted in <figref idref="DRAWINGS">FIG. <b>11</b></figref> may have other components than those depicted. Further, <figref idref="DRAWINGS">FIG. <b>11</b></figref> depicts an embodiment of a third party network infrastructure system. In at least one embodiment, third party network infrastructure system <b>1102</b> may have more or fewer components than depicted in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, may combine two or more components, or may have a different configuration or arrangement of components.</p><p id="p-0207" num="0206">In at least one embodiment, client computing devices <b>1104</b>, <b>1106</b>, and <b>1108</b> may be configured to operate a client application such as a web browser, a proprietary client application, or some other application, which may be used by a user of a client computing device to interact with third party network infrastructure system <b>1102</b> to use services provided by third party network infrastructure system <b>1102</b>. Although exemplary system environment <b>1100</b> is shown with three client computing devices, any number of client computing devices may be supported. In at least one embodiment, other devices such as devices with sensors, etc. may interact with third party network infrastructure system <b>1102</b>. In at least one embodiment, network(s) <b>1110</b> may facilitate communications and exchange of data between client computing devices <b>1104</b>, <b>1106</b>, and <b>1108</b> and third party network infrastructure system <b>1102</b>.</p><p id="p-0208" num="0207">In at least one embodiment, services provided by third party network infrastructure system <b>1102</b> may include a host of services that are made available to users of a third party network infrastructure system on demand. In at least one embodiment, various services may also be offered including without limitation online data storage and backup solutions, Web-based e-mail services, hosted office suites and document collaboration services, database management and processing, managed technical support services, and/or variations thereof. In at least one embodiment, services provided by a third party network infrastructure system can dynamically scale to meet needs of its users.</p><p id="p-0209" num="0208">In at least one embodiment, a specific instantiation of a service provided by third party network infrastructure system <b>1102</b> may be referred to as a &#x201c;service instance.&#x201d; In at least one embodiment, in general, any service made available to a user via a communication network, such as the Internet, from a third party network service provider's system is referred to as a &#x201c;third party network service.&#x201d; In at least one embodiment, in a public third party network environment, servers and systems that make up a third party network service provider's system are different from a customer's own on-premises servers and systems. In at least one embodiment, a third party network service provider's system may host an application, and a user may, via a communication network such as the Internet, on demand, order and use an application.</p><p id="p-0210" num="0209">In at least one embodiment, a service in a computer network third party network infrastructure may include protected computer network access to storage, a hosted database, a hosted web server, a software application, or other service provided by a third party network vendor to a user. In at least one embodiment, a service can include password-protected access to remote storage on a third party network through the Internet. In at least one embodiment, a service can include a web service-based hosted relational database and a script-language middleware engine for private use by a networked developer. In at least one embodiment, a service can include access to an email software application hosted on a third party network vendor's web site.</p><p id="p-0211" num="0210">In at least one embodiment, third party network infrastructure system <b>1102</b> may include a suite of applications, middleware, and database service offerings that are delivered to a customer in a self-service, subscription-based, elastically scalable, reliable, highly available, and secure manner. In at least one embodiment, third party network infrastructure system <b>1102</b> may also provide &#x201c;big data&#x201d; related computation and analysis services. In at least one embodiment, term &#x201c;big data&#x201d; is generally used to refer to extremely large data sets that can be stored and manipulated by analysts and researchers to visualize large amounts of data, detect trends, and/or otherwise interact with data. In at least one embodiment, big data and related applications can be hosted and/or manipulated by an infrastructure system on many levels and at different scales. In at least one embodiment, tens, hundreds, or thousands of processors linked in parallel can act upon such data in order to present it or simulate external forces on data or what it represents. In at least one embodiment, these data sets can involve structured data, such as that organized in a database or otherwise according to a structured model, and/or unstructured data (e.g., emails, images, data blobs (binary large objects), web pages, complex event processing). In at least one embodiment, by leveraging an ability of an embodiment to relatively quickly focus more (or fewer) computing resources upon an objective, a third party network infrastructure system may be better available to carry out tasks on large data sets based on demand from a business, government agency, research organization, private individual, group of like-minded individuals or organizations, or other entity.</p><p id="p-0212" num="0211">In at least one embodiment, third party network infrastructure system <b>1102</b> may be adapted to automatically provision, manage and track a customer's subscription to services offered by third party network infrastructure system <b>1102</b>. In at least one embodiment, third party network infrastructure system <b>1102</b> may provide third party network services via different deployment models. In at least one embodiment, services may be provided under a public third party network model in which third party network infrastructure system <b>1102</b> is owned by an organization selling third party network services and services are made available to a general public or different industry enterprises. In at least one embodiment, services may be provided under a private third party network model in which third party network infrastructure system <b>1102</b> is operated solely for a single organization and may provide services for one or more entities within an organization. In at least one embodiment, third party network services may also be provided under a community third party network model in which third party network infrastructure system <b>1102</b> and services provided by third party network infrastructure system <b>1102</b> are shared by several organizations in a related community. In at least one embodiment, third party network services may also be provided under a hybrid third party network model, which is a combination of two or more different models.</p><p id="p-0213" num="0212">In at least one embodiment, services provided by third party network infrastructure system <b>1102</b> may include one or more services provided under Software as a Service (SaaS) category, Platform as a Service (PaaS) category, Infrastructure as a Service (IaaS) category, or other categories of services including hybrid services. In at least one embodiment, a customer, via a subscription order, may order one or more services provided by third party network infrastructure system <b>1102</b>. In at least one embodiment, third party network infrastructure system <b>1102</b> then performs processing to provide services in a customer's subscription order.</p><p id="p-0214" num="0213">In at least one embodiment, services provided by third party network infrastructure system <b>1102</b> may include, without limitation, application services, platform services and infrastructure services. In at least one embodiment, application services may be provided by a third party network infrastructure system via a SaaS platform. In at least one embodiment, SaaS platform may be configured to provide third party network services that fall under a SaaS category. In at least one embodiment, SaaS platform may provide capabilities to build and deliver a suite of on-demand applications on an integrated development and deployment platform. In at least one embodiment, SaaS platform may manage and control underlying software and infrastructure for providing SaaS services. In at least one embodiment, by utilizing services provided by a SaaS platform, customers can utilize applications executing on a third party network infrastructure system. In at least one embodiment, customers can acquire an application services without a need for customers to purchase separate licenses and support. In at least one embodiment, various different SaaS services may be provided. In at least one embodiment, this may include, without limitation, services that provide solutions for sales performance management, enterprise integration, and business flexibility for large organizations.</p><p id="p-0215" num="0214">In at least one embodiment, platform services may be provided by third party network infrastructure system <b>1102</b> via a PaaS platform. In at least one embodiment, PaaS platform may be configured to provide third party network services that fall under a PaaS category. In at least one embodiment, platform services may include without limitation services that enable organizations to consolidate existing applications on a shared, common architecture, as well as an ability to build new applications that leverage shared services provided by a platform. In at least one embodiment, PaaS platform may manage and control underlying software and infrastructure for providing PaaS services. In at least one embodiment, customers can acquire PaaS services provided by third party network infrastructure system <b>1102</b> without a need for customers to purchase separate licenses and support.</p><p id="p-0216" num="0215">In at least one embodiment, by utilizing services provided by a PaaS platform, customers can employ programming languages and tools supported by a third party network infrastructure system and also control deployed services. In at least one embodiment, platform services provided by a third party network infrastructure system may include database third party network services, middleware third party network services and third party network services. In at least one embodiment, database third party network services may support shared service deployment models that enable organizations to pool database resources and offer customers a Database as a Service in a form of a database third party network. In at least one embodiment, middleware third party network services may provide a platform for customers to develop and deploy various business applications, and third party network services may provide a platform for customers to deploy applications, in a third party network infrastructure system.</p><p id="p-0217" num="0216">In at least one embodiment, various different infrastructure services may be provided by an IaaS platform in a third party network infrastructure system. In at least one embodiment, infrastructure services facilitate management and control of underlying computing resources, such as storage, networks, and other fundamental computing resources for customers utilizing services provided by a SaaS platform and a PaaS platform.</p><p id="p-0218" num="0217">In at least one embodiment, third party network infrastructure system <b>1102</b> may also include infrastructure resources <b>1130</b> for providing resources used to provide various services to customers of a third party network infrastructure system. In at least one embodiment, infrastructure resources <b>1130</b> may include pre-integrated and optimized combinations of hardware, such as servers, storage, and networking resources to execute services provided by a Paas platform and a Saas platform, and other resources.</p><p id="p-0219" num="0218">In at least one embodiment, resources in third party network infrastructure system <b>1102</b> may be shared by multiple users and dynamically re-allocated per demand. In at least one embodiment, resources may be allocated to users in different time zones. In at least one embodiment, third party network infrastructure system <b>1102</b> may enable a first set of users in a first time zone to utilize resources of a third party network infrastructure system for a specified number of hours and then enable a re-allocation of same resources to another set of users located in a different time zone, thereby maximizing utilization of resources.</p><p id="p-0220" num="0219">In at least one embodiment, a number of internal shared services <b>1132</b> may be provided that are shared by different components or modules of third party network infrastructure system <b>1102</b> to enable provision of services by third party network infrastructure system <b>1102</b>. In at least one embodiment, these internal shared services may include, without limitation, a security and identity service, an integration service, an enterprise repository service, an enterprise manager service, a virus scanning and white list service, a high availability, backup and recovery service, service for enabling third party network support, an email service, a notification service, a file transfer service, and/or variations thereof.</p><p id="p-0221" num="0220">In at least one embodiment, third party network infrastructure system <b>1102</b> may provide comprehensive management of third party network services (e.g., SaaS, PaaS, and IaaS services) in a third party network infrastructure system. In at least one embodiment, third party network management functionality may include capabilities for provisioning, managing and tracking a customer's subscription received by third party network infrastructure system <b>1102</b>, and/or variations thereof.</p><p id="p-0222" num="0221">In at least one embodiment, as depicted in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, third party network management functionality may be provided by one or more modules, such as an order management module <b>1120</b>, an order orchestration module <b>1122</b>, an order provisioning module <b>1124</b>, an order management and monitoring module <b>1126</b>, and an identity management module <b>1128</b>. In at least one embodiment, these modules may include or be provided using one or more computers and/or servers, which may be general purpose computers, specialized server computers, server farms, server clusters, or any other appropriate arrangement and/or combination.</p><p id="p-0223" num="0222">In at least one embodiment, at step <b>1134</b>, a customer using a client device, such as client computing devices <b>1104</b>, <b>1106</b> or <b>1108</b>, may interact with third party network infrastructure system <b>1102</b> by requesting one or more services provided by third party network infrastructure system <b>1102</b> and placing an order for a subscription for one or more services offered by third party network infrastructure system <b>1102</b>. In at least one embodiment, a customer may access a third party network User Interface (UI) such as third party network UI <b>1112</b>, third party network UI <b>1114</b> and/or third party network UI <b>1116</b> and place a subscription order via these UIs. In at least one embodiment, order information received by third party network infrastructure system <b>1102</b> in response to a customer placing an order may include information identifying a customer and one or more services offered by a third party network infrastructure system <b>1102</b> that a customer intends to subscribe to.</p><p id="p-0224" num="0223">In at least one embodiment, at step <b>1136</b>, an order information received from a customer may be stored in an order database <b>1118</b>. In at least one embodiment, if this is a new order, a new record may be created for an order. In at least one embodiment, order database <b>1118</b> can be one of several databases operated by third party network infrastructure system <b>1118</b> and operated in conjunction with other system elements.</p><p id="p-0225" num="0224">In at least one embodiment, at step <b>1138</b>, an order information may be forwarded to an order management module <b>1120</b> that may be configured to perform billing and accounting functions related to an order, such as verifying an order, and upon verification, booking an order.</p><p id="p-0226" num="0225">In at least one embodiment, at step <b>1140</b>, information regarding an order may be communicated to an order orchestration module <b>1122</b> that is configured to orchestrate provisioning of services and resources for an order placed by a customer. In at least one embodiment, order orchestration module <b>1122</b> may use services of order provisioning module <b>1124</b> for provisioning. In at least one embodiment, order orchestration module <b>1122</b> enables management of business processes associated with each order and applies business logic to determine whether an order should proceed to provisioning.</p><p id="p-0227" num="0226">In at least one embodiment, at step <b>1142</b>, upon receiving an order for a new subscription, order orchestration module <b>1122</b> sends a request to order provisioning module <b>1124</b> to allocate resources and configure resources needed to fulfill a subscription order. In at least one embodiment, order provisioning module <b>1124</b> enables an allocation of resources for services ordered by a customer. In at least one embodiment, order provisioning module <b>1124</b> provides a level of abstraction between third party network services provided by third party network infrastructure system <b>1100</b> and a physical implementation layer that is used to provision resources for providing requested services. In at least one embodiment, this enables order orchestration module <b>1122</b> to be isolated from implementation details, such as whether or not services and resources are actually provisioned in real-time or pre-provisioned and only allocated/assigned upon request.</p><p id="p-0228" num="0227">In at least one embodiment, at step <b>1144</b>, once services and resources are provisioned, a notification may be sent to subscribing customers indicating that a requested service is now ready for use. In at least one embodiment, information (e.g. a link) may be sent to a customer that enables a customer to start using requested services.</p><p id="p-0229" num="0228">In at least one embodiment, at step <b>1146</b>, a customer's subscription order may be managed and tracked by an order management and monitoring module <b>1126</b>. In at least one embodiment, order management and monitoring module <b>1126</b> may be configured to collect usage statistics regarding a customer use of subscribed services. In at least one embodiment, statistics may be collected for an amount of storage used, an amount data transferred, a number of users, and an amount of system up time and system down time, and/or variations thereof.</p><p id="p-0230" num="0229">In at least one embodiment, third party network infrastructure system <b>1100</b> may include an identity management module <b>1128</b> that is configured to provide identity services, such as access management and authorization services in third party network infrastructure system <b>1100</b>. In at least one embodiment, identity management module <b>1128</b> may control information about customers who wish to utilize services provided by third party network infrastructure system <b>1102</b>. In at least one embodiment, such information can include information that authenticates identities of such customers and information that describes which actions those customers are authorized to perform relative to various system resources (e.g., files, directories, applications, communication ports, memory segments, etc.). In at least one embodiment, identity management module <b>1128</b> may also include management of descriptive information about each customer and about how and by whom that descriptive information can be accessed and modified.</p><p id="p-0231" num="0230"><figref idref="DRAWINGS">FIG. <b>12</b></figref> illustrates a cloud computing environment <b>1202</b>, in accordance with at least one embodiment. In at least one embodiment, cloud computing environment <b>1202</b> comprises one or more computer system/servers <b>1204</b> with which computing devices such as, personal digital assistant (PDA) or cellular telephone <b>1206</b>A, desktop computer <b>1206</b>B, laptop computer <b>1206</b>C, and/or automobile computer system <b>1206</b>N communicate. In at least one embodiment, this allows for infrastructure, platforms and/or software to be offered as services from cloud computing environment <b>1202</b>, so as to not require each client to separately maintain such resources. It is understood that types of computing devices <b>1206</b>A-N shown in <figref idref="DRAWINGS">FIG. <b>12</b></figref> are intended to be illustrative only and that cloud computing environment <b>1202</b> can communicate with any type of computerized device over any type of network and/or network/addressable connection (e.g., using a web browser).</p><p id="p-0232" num="0231">In at least one embodiment, a computer system/server <b>1204</b>, which can be denoted as a cloud computing node, is operational with numerous other general purpose or special purpose computing system environments or configurations. In at least one embodiment, computing systems, environments, and/or configurations that may be suitable for use with computer system/server <b>1204</b> include, but are not limited to, personal computer systems, server computer systems, thin clients, thick clients, hand-held or laptop devices, multiprocessor systems, microprocessor-based systems, set top boxes, programmable consumer electronics, network PCs, minicomputer systems, mainframe computer systems, and distributed cloud computing environments that include any of the above systems or devices, and/or variations thereof.</p><p id="p-0233" num="0232">In at least one embodiment, computer system/server <b>1204</b> may be described in a general context of computer system-executable instructions, such as program modules, being executed by a computer system. In at least one embodiment, program modules include routines, programs, objects, components, logic, data structures, and so on, that perform particular tasks or implement particular abstract data types. In at least one embodiment, exemplary computer system/server <b>1204</b> may be practiced in distributed loud computing environments where tasks are performed by remote processing devices that are linked through a communications network. In at least one embodiment, in a distributed cloud computing environment, program modules may be located in both local and remote computer system storage media including memory storage devices.</p><p id="p-0234" num="0233"><figref idref="DRAWINGS">FIG. <b>13</b></figref> illustrates a set of functional abstraction layers provided by cloud computing environment <b>1202</b> (<figref idref="DRAWINGS">FIG. <b>12</b></figref>), in accordance with at least one embodiment. It should be understood in advance that components, layers, and functions shown in <figref idref="DRAWINGS">FIG. <b>13</b></figref> are intended to be illustrative only, and components, layers, and functions may vary.</p><p id="p-0235" num="0234">In at least one embodiment, hardware and software layer <b>1302</b> includes hardware and software components. In at least one embodiment, hardware components include mainframes, various RISC (Reduced Instruction Set Computer) architecture based servers, various computing systems, supercomputing systems, storage devices, networks, networking components, and/or variations thereof. In at least one embodiment, software components include network application server software, various application server software, various database software, and/or variations thereof.</p><p id="p-0236" num="0235">In at least one embodiment, virtualization layer <b>1304</b> provides an abstraction layer from which following exemplary virtual entities may be provided: virtual servers, virtual storage, virtual networks, including virtual private networks, virtual applications, virtual clients, and/or variations thereof.</p><p id="p-0237" num="0236">In at least one embodiment, management layer <b>1306</b> provides various functions. In at least one embodiment, resource provisioning provides dynamic procurement of computing resources and other resources that are utilized to perform tasks within a cloud computing environment. In at least one embodiment, metering provides usage tracking as resources are utilized within a cloud computing environment, and billing or invoicing for consumption of these resources. In at least one embodiment, resources may comprise application software licenses. In at least one embodiment, security provides identity verification for users and tasks, as well as protection for data and other resources. In at least one embodiment, user interface provides access to a cloud computing environment for both users and system administrators. In at least one embodiment, service level management provides cloud computing resource allocation and management such that required service levels are met. In at least one embodiment, Service Level Agreement (SLA) management provides pre-arrangement for, and procurement of, cloud computing resources for which a future requirement is anticipated in accordance with an SLA.</p><p id="p-0238" num="0237">In at least one embodiment, workloads layer <b>1308</b> provides functionality for which a cloud computing environment is utilized. In at least one embodiment, workloads and functions which may be provided from this layer include: mapping and navigation, software development and management, educational services, data analytics and processing, transaction processing, and service delivery.</p><heading id="h-0007" level="1">Supercomputing</heading><p id="p-0239" num="0238">The following figures set forth, without limitation, exemplary supercomputer-based systems that can be used to implement at least one embodiment.</p><p id="p-0240" num="0239">In at least one embodiment, a supercomputer may refer to a hardware system exhibiting substantial parallelism and comprising at least one chip, where chips in a system are interconnected by a network and are placed in hierarchically organized enclosures. In at least one embodiment, a large hardware system filling a machine room, with several racks, each containing several boards/rack modules, each containing several chips, all interconnected by a scalable network, is at least one embodiment of a supercomputer. In at least one embodiment, a single rack of such a large hardware system is at least one other embodiment of a supercomputer. In at least one embodiment, a single chip exhibiting substantial parallelism and containing several hardware components can equally be considered to be a supercomputer, since as feature sizes may decrease, an amount of hardware that can be incorporated in a single chip may also increase.</p><p id="p-0241" num="0240"><figref idref="DRAWINGS">FIG. <b>14</b></figref> illustrates a supercomputer at a chip level, in accordance with at least one embodiment. In at least one embodiment, inside an FPGA or ASIC chip, main computation is performed within finite state machines (<b>1404</b>) called thread units. In at least one embodiment, task and synchronization networks (<b>1402</b>) connect finite state machines and are used to dispatch threads and execute operations in correct order. In at least one embodiment, a multi-level partitioned on-chip cache hierarchy (<b>1408</b>, <b>1412</b>) is accessed using memory networks (<b>1406</b>, <b>1410</b>). In at least one embodiment, off-chip memory is accessed using memory controllers (<b>1416</b>) and an off-chip memory network (<b>1414</b>). In at least one embodiment, I/O controller (<b>1418</b>) is used for cross-chip communication when a design does not fit in a single logic chip.</p><p id="p-0242" num="0241"><figref idref="DRAWINGS">FIG. <b>15</b></figref> illustrates a supercomputer at a rock module level, in accordance with at least one embodiment. In at least one embodiment, within a rack module, there are multiple FPGA or ASIC chips (<b>1502</b>) that are connected to one or more DRAM units (<b>1504</b>) which constitute main accelerator memory. In at least one embodiment, each FPGA/ASIC chip is connected to its neighbor FPGA/ASIC chip using wide busses on a board, with differential high speed signaling (<b>1506</b>). In at least one embodiment, each FPGA/ASIC chip is also connected to at least one high-speed serial communication cable.</p><p id="p-0243" num="0242"><figref idref="DRAWINGS">FIG. <b>16</b></figref> illustrates a supercomputer at a rack level, in accordance with at least one embodiment. <figref idref="DRAWINGS">FIG. <b>17</b></figref> illustrates a supercomputer at a whole system level, in accordance with at least one embodiment. In at least one embodiment, referring to <figref idref="DRAWINGS">FIG. <b>16</b></figref> and <figref idref="DRAWINGS">FIG. <b>17</b></figref>, between rack modules in a rack and across racks throughout an entire system, high-speed serial optical or copper cables (<b>1602</b>, <b>1702</b>) are used to realize a scalable, possibly incomplete hypercube network. In at least one embodiment, one of FPGA/ASIC chips of an accelerator is connected to a host system through a PCI-Express connection (<b>1704</b>). In at least one embodiment, host system comprises a host microprocessor (<b>1708</b>) that a software part of an application runs on and a memory consisting of one or more host memory DRAM units (<b>1706</b>) that is kept coherent with memory on an accelerator. In at least one embodiment, host system can be a separate module on one of racks, or can be integrated with one of a supercomputer's modules. In at least one embodiment, cube-connected cycles topology provide communication links to create a hypercube network for a large supercomputer. In at least one embodiment, a small group of FPGA/ASIC chips on a rack module can act as a single hypercube node, such that a total number of external links of each group is increased, compared to a single chip. In at least one embodiment, a group contains chips A, B, C and D on a rack module with internal wide differential busses connecting A, B, C and D in a torus organization. In at least one embodiment, there are 12 serial communication cables connecting a rack module to an outside world. In at least one embodiment, chip A on a rack module connects to serial communication cables <b>0</b>, <b>1</b>, <b>2</b>. In at least one embodiment, chip B connects to cables <b>3</b>, <b>4</b>, <b>5</b>. In at least one embodiment, chip C connects to <b>6</b>, <b>7</b>, <b>8</b>. In at least one embodiment, chip D connects to <b>9</b>, <b>10</b>, <b>11</b>. In at least one embodiment, an entire group {A, B, C, D} constituting a rack module can form a hypercube node within a supercomputer system, with up to 212=4096 rack modules (16384 FPGA/ASIC chips). In at least one embodiment, for chip A to send a message out on link <b>4</b> of group {A, B, C, D}, a message has to be routed first to chip B with an on-board differential wide bus connection. In at least one embodiment, a message arriving into a group {A, B, C, D} on link <b>4</b> (i.e., arriving at B) destined to chip A, also has to be routed first to a correct destination chip (A) internally within a group {A, B, C, D}. In at least one embodiment, parallel supercomputer systems of other sizes may also be implemented.</p><heading id="h-0008" level="1">Artificial Intelligence</heading><p id="p-0244" num="0243">The following figures set forth, without limitation, exemplary artificial intelligence-based systems that can be used to implement at least one embodiment.</p><p id="p-0245" num="0244"><figref idref="DRAWINGS">FIG. <b>18</b>A</figref> illustrates inference and/or training logic <b>1815</b> used to perform inferencing and/or training operations associated with one or more embodiments. Details regarding inference and/or training logic <b>1815</b> are provided below in conjunction with <figref idref="DRAWINGS">FIGS. <b>18</b>A and/or <b>18</b>B</figref>.</p><p id="p-0246" num="0245">In at least one embodiment, inference and/or training logic <b>1815</b> may include, without limitation, code and/or data storage <b>1801</b> to store forward and/or output weight and/or input/output data, and/or other parameters to configure neurons or layers of a neural network trained and/or used for inferencing in aspects of one or more embodiments. In at least one embodiment, training logic <b>1815</b> may include, or be coupled to code and/or data storage <b>1801</b> to store graph code or other software to control timing and/or order, in which weight and/or other parameter information is to be loaded to configure, logic, including integer and/or floating point units (collectively, arithmetic logic units (ALUs). In at least one embodiment, code, such as graph code, loads weight or other parameter information into processor ALUs based on an architecture of a neural network to which such code corresponds. In at least one embodiment code and/or data storage <b>1801</b> stores weight parameters and/or input/output data of each layer of a neural network trained or used in conjunction with one or more embodiments during forward propagation of input/output data and/or weight parameters during training and/or inferencing using aspects of one or more embodiments. In at least one embodiment, any portion of code and/or data storage <b>1801</b> may be included with other on-chip or off-chip data storage, including a processor's L1, L2, or L3 cache or system memory.</p><p id="p-0247" num="0246">In at least one embodiment, any portion of code and/or data storage <b>1801</b> may be internal or external to one or more processors or other hardware logic devices or circuits. In at least one embodiment, code and/or code and/or data storage <b>1801</b> may be cache memory, dynamic randomly addressable memory (&#x201c;DRAM&#x201d;), static randomly addressable memory (&#x201c;SRAM&#x201d;), non-volatile memory (e.g., flash memory), or other storage. In at least one embodiment, a choice of whether code and/or code and/or data storage <b>1801</b> is internal or external to a processor, in at least one embodiment, or comprising DRAM, SRAM, flash or some other storage type may depend on available storage on-chip versus off-chip, latency requirements of training and/or inferencing functions being performed, batch size of data used in inferencing and/or training of a neural network, or some combination of these factors.</p><p id="p-0248" num="0247">In at least one embodiment, inference and/or training logic <b>1815</b> may include, without limitation, a code and/or data storage <b>1805</b> to store backward and/or output weight and/or input/output data corresponding to neurons or layers of a neural network trained and/or used for inferencing in aspects of one or more embodiments. In at least one embodiment, code and/or data storage <b>1805</b> stores weight parameters and/or input/output data of each layer of a neural network trained or used in conjunction with one or more embodiments during backward propagation of input/output data and/or weight parameters during training and/or inferencing using aspects of one or more embodiments. In at least one embodiment, training logic <b>1815</b> may include, or be coupled to code and/or data storage <b>1805</b> to store graph code or other software to control timing and/or order, in which weight and/or other parameter information is to be loaded to configure, logic, including integer and/or floating point units (collectively, arithmetic logic units (ALUs).</p><p id="p-0249" num="0248">In at least one embodiment, code, such as graph code, causes loading of weight or other parameter information into processor ALUs based on an architecture of a neural network to which such code corresponds. In at least one embodiment, any portion of code and/or data storage <b>1805</b> may be included with other on-chip or off-chip data storage, including a processor's L1, L2, or L3 cache or system memory. In at least one embodiment, any portion of code and/or data storage <b>1805</b> may be internal or external to one or more processors or other hardware logic devices or circuits. In at least one embodiment, code and/or data storage <b>1805</b> may be cache memory, DRAM, SRAM, non-volatile memory (e.g., flash memory), or other storage. In at least one embodiment, a choice of whether code and/or data storage <b>1805</b> is internal or external to a processor, in at least one embodiment, or comprising DRAM, SRAM, flash memory or some other storage type may depend on available storage on-chip versus off-chip, latency requirements of training and/or inferencing functions being performed, batch size of data used in inferencing and/or training of a neural network, or some combination of these factors.</p><p id="p-0250" num="0249">In at least one embodiment, code and/or data storage <b>1801</b> and code and/or data storage <b>1805</b> may be separate storage structures. In at least one embodiment, code and/or data storage <b>1801</b> and code and/or data storage <b>1805</b> may be a combined storage structure. In at least one embodiment, code and/or data storage <b>1801</b> and code and/or data storage <b>1805</b> may be partially combined and partially separate. In at least one embodiment, any portion of code and/or data storage <b>1801</b> and code and/or data storage <b>1805</b> may be included with other on-chip or off-chip data storage, including a processor's L1, L2, or L3 cache or system memory.</p><p id="p-0251" num="0250">In at least one embodiment, inference and/or training logic <b>1815</b> may include, without limitation, one or more arithmetic logic unit(s) (&#x201c;ALU(s)&#x201d;) <b>1810</b>, including integer and/or floating point units, to perform logical and/or mathematical operations based, at least in part on, or indicated by, training and/or inference code (e.g., graph code), a result of which may produce activations (e.g., output values from layers or neurons within a neural network) stored in an activation storage <b>1820</b> that are functions of input/output and/or weight parameter data stored in code and/or data storage <b>1801</b> and/or code and/or data storage <b>1805</b>. In at least one embodiment, activations stored in activation storage <b>1820</b> are generated according to linear algebraic and or matrix-based mathematics performed by ALU(s) <b>1810</b> in response to performing instructions or other code, wherein weight values stored in code and/or data storage <b>1805</b> and/or data storage <b>1801</b> are used as operands along with other values, such as bias values, gradient information, momentum values, or other parameters or hyperparameters, any or all of which may be stored in code and/or data storage <b>1805</b> or code and/or data storage <b>1801</b> or another storage on or off-chip.</p><p id="p-0252" num="0251">In at least one embodiment, ALU(s) <b>1810</b> are included within one or more processors or other hardware logic devices or circuits, whereas in another embodiment, ALU(s) <b>1810</b> may be external to a processor or other hardware logic device or circuit that uses them (e.g., a co-processor). In at least one embodiment, ALUs <b>1810</b> may be included within a processor's execution units or otherwise within a bank of ALUs accessible by a processor's execution units either within same processor or distributed between different processors of different types (e.g., central processing units, graphics processing units, fixed function units, etc.). In at least one embodiment, code and/or data storage <b>1801</b>, code and/or data storage <b>1805</b>, and activation storage <b>1820</b> may share a processor or other hardware logic device or circuit, whereas in another embodiment, they may be in different processors or other hardware logic devices or circuits, or some combination of same and different processors or other hardware logic devices or circuits. In at least one embodiment, any portion of activation storage <b>1820</b> may be included with other on-chip or off-chip data storage, including a processor's L1, L2, or L3 cache or system memory. Furthermore, inferencing and/or training code may be stored with other code accessible to a processor or other hardware logic or circuit and fetched and/or processed using a processor's fetch, decode, scheduling, execution, retirement and/or other logical circuits.</p><p id="p-0253" num="0252">In at least one embodiment, activation storage <b>1820</b> may be cache memory, DRAM, SRAM, non-volatile memory (e.g., flash memory), or other storage. In at least one embodiment, activation storage <b>1820</b> may be completely or partially within or external to one or more processors or other logical circuits. In at least one embodiment, a choice of whether activation storage <b>1820</b> is internal or external to a processor, in at least one embodiment, or comprising DRAM, SRAM, flash memory or some other storage type may depend on available storage on-chip versus off-chip, latency requirements of training and/or inferencing functions being performed, batch size of data used in inferencing and/or training of a neural network, or some combination of these factors.</p><p id="p-0254" num="0253">In at least one embodiment, inference and/or training logic <b>1815</b> illustrated in <figref idref="DRAWINGS">FIG. <b>18</b>A</figref> may be used in conjunction with an application-specific integrated circuit (&#x201c;ASIC&#x201d;), such as a TensorFlow&#xae; Processing Unit from Google, an inference processing unit (IPU) from Graphcore&#x2122;, or a Nervana&#xae; (e.g., &#x201c;Lake Crest&#x201d;) processor from Intel Corp. In at least one embodiment, inference and/or training logic <b>1815</b> illustrated in <figref idref="DRAWINGS">FIG. <b>18</b>A</figref> may be used in conjunction with central processing unit (&#x201c;CPU&#x201d;) hardware, graphics processing unit (&#x201c;GPU&#x201d;) hardware or other hardware, such as field programmable gate arrays (&#x201c;FPGAs&#x201d;).</p><p id="p-0255" num="0254"><figref idref="DRAWINGS">FIG. <b>18</b>B</figref> illustrates inference and/or training logic <b>1815</b>, according to at least one embodiment. In at least one embodiment, inference and/or training logic <b>1815</b> may include, without limitation, hardware logic in which computational resources are dedicated or otherwise exclusively used in conjunction with weight values or other information corresponding to one or more layers of neurons within a neural network. In at least one embodiment, inference and/or training logic <b>1815</b> illustrated in <figref idref="DRAWINGS">FIG. <b>18</b>B</figref> may be used in conjunction with an application-specific integrated circuit (ASIC), such as TensorFlow&#xae; Processing Unit from Google, an inference processing unit (IPU) from Graphcore&#x2122;, or a Nervana&#xae; (e.g., &#x201c;Lake Crest&#x201d;) processor from Intel Corp. In at least one embodiment, inference and/or training logic <b>1815</b> illustrated in <figref idref="DRAWINGS">FIG. <b>18</b>B</figref> may be used in conjunction with central processing unit (CPU) hardware, graphics processing unit (GPU) hardware or other hardware, such as field programmable gate arrays (FPGAs). In at least one embodiment, inference and/or training logic <b>1815</b> includes, without limitation, code and/or data storage <b>1801</b> and code and/or data storage <b>1805</b>, which may be used to store code (e.g., graph code), weight values and/or other information, including bias values, gradient information, momentum values, and/or other parameter or hyperparameter information. In at least one embodiment illustrated in <figref idref="DRAWINGS">FIG. <b>18</b>B</figref>, each of code and/or data storage <b>1801</b> and code and/or data storage <b>1805</b> is associated with a dedicated computational resource, such as computational hardware <b>1802</b> and computational hardware <b>1806</b>, respectively. In at least one embodiment, each of computational hardware <b>1802</b> and computational hardware <b>1806</b> comprises one or more ALUs that perform mathematical functions, such as linear algebraic functions, only on information stored in code and/or data storage <b>1801</b> and code and/or data storage <b>1805</b>, respectively, result of which is stored in activation storage <b>1820</b>.</p><p id="p-0256" num="0255">In at least one embodiment, each of code and/or data storage <b>1801</b> and <b>1805</b> and corresponding computational hardware <b>1802</b> and <b>1806</b>, respectively, correspond to different layers of a neural network, such that resulting activation from one storage/computational pair <b>1801</b>/<b>1802</b> of code and/or data storage <b>1801</b> and computational hardware <b>1802</b> is provided as an input to a next storage/computational pair <b>1805</b>/<b>1806</b> of code and/or data storage <b>1805</b> and computational hardware <b>1806</b>, in order to mirror a conceptual organization of a neural network. In at least one embodiment, each of storage/computational pairs <b>1801</b>/<b>1802</b> and <b>1805</b>/<b>1806</b> may correspond to more than one neural network layer. In at least one embodiment, additional storage/computation pairs (not shown) subsequent to or in parallel with storage/computation pairs <b>1801</b>/<b>1802</b> and <b>1805</b>/<b>1806</b> may be included in inference and/or training logic <b>1815</b>.</p><p id="p-0257" num="0256"><figref idref="DRAWINGS">FIG. <b>19</b></figref> illustrates training and deployment of a deep neural network, according to at least one embodiment. In at least one embodiment, untrained neural network <b>1906</b> is trained using a training dataset <b>1902</b>. In at least one embodiment, training framework <b>1904</b> is a PyTorch framework, whereas in other embodiments, training framework <b>1904</b> is a TensorFlow, Boost, Caffe, Microsoft Cognitive Toolkit/CNTK, MXNet, Chainer, Keras, Deeplearning4j, or other training framework. In at least one embodiment, training framework <b>1904</b> trains an untrained neural network <b>1906</b> and enables it to be trained using processing resources described herein to generate a trained neural network <b>1908</b>. In at least one embodiment, weights may be chosen randomly or by pre-training using a deep belief network. In at least one embodiment, training may be performed in either a supervised, partially supervised, or unsupervised manner.</p><p id="p-0258" num="0257">In at least one embodiment, untrained neural network <b>1906</b> is trained using supervised learning, wherein training dataset <b>1902</b> includes an input paired with a desired output for an input, or where training dataset <b>1902</b> includes input having a known output and an output of neural network <b>1906</b> is manually graded. In at least one embodiment, untrained neural network <b>1906</b> is trained in a supervised manner and processes inputs from training dataset <b>1902</b> and compares resulting outputs against a set of expected or desired outputs. In at least one embodiment, errors are then propagated back through untrained neural network <b>1906</b>. In at least one embodiment, training framework <b>1904</b> adjusts weights that control untrained neural network <b>1906</b>. In at least one embodiment, training framework <b>1904</b> includes tools to monitor how well untrained neural network <b>1906</b> is converging towards a model, such as trained neural network <b>1908</b>, suitable to generating correct answers, such as in result <b>1914</b>, based on input data such as a new dataset <b>1912</b>. In at least one embodiment, training framework <b>1904</b> trains untrained neural network <b>1906</b> repeatedly while adjust weights to refine an output of untrained neural network <b>1906</b> using a loss function and adjustment algorithm, such as stochastic gradient descent. In at least one embodiment, training framework <b>1904</b> trains untrained neural network <b>1906</b> until untrained neural network <b>1906</b> achieves a desired accuracy. In at least one embodiment, trained neural network <b>1908</b> can then be deployed to implement any number of machine learning operations.</p><p id="p-0259" num="0258">In at least one embodiment, untrained neural network <b>1906</b> is trained using unsupervised learning, wherein untrained neural network <b>1906</b> attempts to train itself using unlabeled data. In at least one embodiment, unsupervised learning training dataset <b>1902</b> will include input data without any associated output data or &#x201c;ground truth&#x201d; data. In at least one embodiment, untrained neural network <b>1906</b> can learn groupings within training dataset <b>1902</b> and can determine how individual inputs are related to untrained dataset <b>1902</b>. In at least one embodiment, unsupervised training can be used to generate a self-organizing map in trained neural network <b>1908</b> capable of performing operations useful in reducing dimensionality of new dataset <b>1912</b>. In at least one embodiment, unsupervised training can also be used to perform anomaly detection, which allows identification of data points in new dataset <b>1912</b> that deviate from normal patterns of new dataset <b>1912</b>.</p><p id="p-0260" num="0259">In at least one embodiment, semi-supervised learning may be used, which is a technique in which in training dataset <b>1902</b> includes a mix of labeled and unlabeled data. In at least one embodiment, training framework <b>1904</b> may be used to perform incremental learning, such as through transferred learning techniques. In at least one embodiment, incremental learning enables trained neural network <b>1908</b> to adapt to new dataset <b>1912</b> without forgetting knowledge instilled within trained neural network <b>1408</b> during initial training.</p><p id="p-0261" num="0260">The following figures set forth, without limitation, exemplary 5G network-based systems that can be used to implement at least one embodiment.</p><p id="p-0262" num="0261"><figref idref="DRAWINGS">FIG. <b>20</b></figref> illustrates an architecture of a system <b>2000</b> of a network, in accordance with at least one embodiment. In at least one embodiment, system <b>2000</b> is shown to include a user equipment (UE) <b>2002</b> and a UE <b>2004</b>. In at least one embodiment, UEs <b>2002</b> and <b>2004</b> are illustrated as smartphones (e.g., handheld touchscreen mobile computing devices connectable to one or more cellular networks) but may also comprise any mobile or non-mobile computing device, such as Personal Data Assistants (PDAs), pagers, laptop computers, desktop computers, wireless handsets, or any computing device including a wireless communications interface.</p><p id="p-0263" num="0262">In at least one embodiment, any of UEs <b>2002</b> and <b>2004</b> can comprise an Internet of Things (IoT) UE, which can comprise a network access layer designed for low-power IoT applications utilizing short-lived UE connections. In at least one embodiment, an IoT UE can utilize technologies such as machine-to-machine (M2M) or machine-type communications (MTC) for exchanging data with an MTC server or device via a public land mobile network (PLMN), Proximity-Based Service (ProSe) or device-to-device (D2D) communication, sensor networks, or IoT networks. In at least one embodiment, a M2M or MTC exchange of data may be a machine-initiated exchange of data. In at least one embodiment, an IoT network describes interconnecting IoT UEs, which may include uniquely identifiable embedded computing devices (within Internet infrastructure), with short-lived connections. In at least one embodiment, an IoT UEs may execute background applications (e.g., keep alive messages, status updates, etc.) to facilitate connections of an IoT network.</p><p id="p-0264" num="0263">In at least one embodiment, UEs <b>2002</b> and <b>2004</b> may be configured to connect, e.g., communicatively couple, with a radio access network (RAN) <b>2016</b>. In at least one embodiment, RAN <b>2016</b> may be, in at least one embodiment, an Evolved Universal Mobile Telecommunications System (UMTS) Terrestrial Radio Access Network (E-UTRAN), a NextGen RAN (NG RAN), or some other type of RAN. In at least one embodiment, UEs <b>2002</b> and <b>2004</b> utilize connections <b>2012</b> and <b>2014</b>, respectively, each of which comprises a physical communications interface or layer. In at least one embodiment, connections <b>2012</b> and <b>2014</b> are illustrated as an air interface to enable communicative coupling, and can be consistent with cellular communications protocols, such as a Global System for Mobile Communications (GSM) protocol, a code-division multiple access (CDMA) network protocol, a Push-to-Talk (PTT) protocol, a PTT over Cellular (POC) protocol, a Universal Mobile Telecommunications System (UMTS) protocol, a 3GPP Long Term Evolution (LTE) protocol, a fifth generation (5G) protocol, a New Radio (NR) protocol, and variations thereof.</p><p id="p-0265" num="0264">In at least one embodiment, UEs <b>2002</b> and <b>2004</b> may further directly exchange communication data via a ProSe interface <b>2006</b>. In at least one embodiment, ProSe interface <b>2006</b> may alternatively be referred to as a sidelink interface comprising one or more logical channels, including but not limited to a Physical Sidelink Control Channel (PSCCH), a Physical Sidelink Shared Channel (PSSCH), a Physical Sidelink Discovery Channel (PSDCH), and a Physical Sidelink Broadcast Channel (PSBCH).</p><p id="p-0266" num="0265">In at least one embodiment, UE <b>2004</b> is shown to be configured to access an access point (AP) <b>2010</b> via connection <b>2008</b>. In at least one embodiment, connection <b>2008</b> can comprise a local wireless connection, such as a connection consistent with any IEEE 802.11 protocol, wherein AP <b>2010</b> would comprise a wireless fidelity (WiFi&#xae;) router. In at least one embodiment, AP <b>2010</b> is shown to be connected to an Internet without connecting to a core network of a wireless system.</p><p id="p-0267" num="0266">In at least one embodiment, RAN <b>2016</b> can include one or more access nodes that enable connections <b>2012</b> and <b>2014</b>. In at least one embodiment, these access nodes (ANs) can be referred to as base stations (BSs), NodeBs, evolved NodeBs (eNBs), next Generation NodeBs (gNB), RAN nodes, and so forth, and can comprise ground stations (e.g., terrestrial access points) or satellite stations providing coverage within a geographic area (e.g., a cell). In at least one embodiment, RAN <b>2016</b> may include one or more RAN nodes for providing macrocells, e.g., macro RAN node <b>2018</b>, and one or more RAN nodes for providing femtocells or picocells (e.g., cells having smaller coverage areas, smaller user capacity, or higher bandwidth compared to macrocells), e.g., low power (LP) RAN node <b>2020</b>.</p><p id="p-0268" num="0267">In at least one embodiment, any of RAN nodes <b>2018</b> and <b>2020</b> can terminate an air interface protocol and can be a first point of contact for UEs <b>2002</b> and <b>2004</b>. In at least one embodiment, any of RAN nodes <b>2018</b> and <b>2020</b> can fulfill various logical functions for RAN <b>2016</b> including, but not limited to, radio network controller (RNC) functions such as radio bearer management, uplink and downlink dynamic radio resource management and data packet scheduling, and mobility management.</p><p id="p-0269" num="0268">In at least one embodiment, UEs <b>2002</b> and <b>2004</b> can be configured to communicate using Orthogonal Frequency-Division Multiplexing (OFDM) communication signals with each other or with any of RAN nodes <b>2018</b> and <b>2020</b> over a multi-carrier communication channel in accordance various communication techniques, such as, but not limited to, an Orthogonal Frequency Division Multiple Access (OFDMA) communication technique (e.g., for downlink communications) or a Single Carrier Frequency Division Multiple Access (SC-FDMA) communication technique (e.g., for uplink and ProSe or sidelink communications), and/or variations thereof. In at least one embodiment, OFDM signals can comprise a plurality of orthogonal sub-carriers.</p><p id="p-0270" num="0269">In at least one embodiment, a downlink resource grid can be used for downlink transmissions from any of RAN nodes <b>2018</b> and <b>2020</b> to UEs <b>2002</b> and <b>2004</b>, while uplink transmissions can utilize similar techniques. In at least one embodiment, a grid can be a time frequency grid, called a resource grid or time-frequency resource grid, which is a physical resource in a downlink in each slot. In at least one embodiment, such a time frequency plane representation is a common practice for OFDM systems, which makes it intuitive for radio resource allocation. In at least one embodiment, each column and each row of a resource grid corresponds to one OFDM symbol and one OFDM subcarrier, respectively. In at least one embodiment, a duration of a resource grid in a time domain corresponds to one slot in a radio frame. In at least one embodiment, a smallest time-frequency unit in a resource grid is denoted as a resource element. In at least one embodiment, each resource grid comprises a number of resource blocks, which describe a mapping of certain physical channels to resource elements. In at least one embodiment, each resource block comprises a collection of resource elements. In at least one embodiment, in a frequency domain, this may represent a smallest quantity of resources that currently can be allocated. In at least one embodiment, there are several different physical downlink channels that are conveyed using such resource blocks.</p><p id="p-0271" num="0270">In at least one embodiment, a physical downlink shared channel (PDSCH) may carry user data and higher-layer signaling to UEs <b>2002</b> and <b>2004</b>. In at least one embodiment, a physical downlink control channel (PDCCH) may carry information about a transport format and resource allocations related to PDSCH channel, among other things. In at least one embodiment, it may also inform UEs <b>2002</b> and <b>2004</b> about a transport format, resource allocation, and HARQ (Hybrid Automatic Repeat Request) information related to an uplink shared channel. In at least one embodiment, typically, downlink scheduling (assigning control and shared channel resource blocks to UE <b>2002</b> within a cell) may be performed at any of RAN nodes <b>2018</b> and <b>2020</b> based on channel quality information fed back from any of UEs <b>2002</b> and <b>2004</b>. In at least one embodiment, downlink resource assignment information may be sent on a PDCCH used for (e.g., assigned to) each of UEs <b>2002</b> and <b>2004</b>.</p><p id="p-0272" num="0271">In at least one embodiment, a PDCCH may use control channel elements (CCEs) to convey control information. In at least one embodiment, before being mapped to resource elements, PDCCH complex valued symbols may first be organized into quadruplets, which may then be permuted using a sub-block interleaver for rate matching. In at least one embodiment, each PDCCH may be transmitted using one or more of these CCEs, where each CCE may correspond to nine sets of four physical resource elements known as resource element groups (REGs). In at least one embodiment, four Quadrature Phase Shift Keying (QPSK) symbols may be mapped to each REG. In at least one embodiment, PDCCH can be transmitted using one or more CCEs, depending on a size of a downlink control information (DCI) and a channel condition. In at least one embodiment, there can be four or more different PDCCH formats defined in LTE with different numbers of CCEs (e.g., aggregation level, L=1, 2, 4, or 8).</p><p id="p-0273" num="0272">In at least one embodiment, an enhanced physical downlink control channel (EPDCCH) that uses PDSCH resources may be utilized for control information transmission. In at least one embodiment, EPDCCH may be transmitted using one or more enhanced control channel elements (ECCEs). In at least one embodiment, each ECCE may correspond to nine sets of four physical resource elements known as an enhanced resource element groups (EREGs). In at least one embodiment, an ECCE may have other numbers of EREGs in some situations.</p><p id="p-0274" num="0273">In at least one embodiment, RAN <b>2016</b> is shown to be communicatively coupled to a core network (CN) <b>2038</b> via an S1 interface <b>2022</b>. In at least one embodiment, CN <b>2038</b> may be an evolved packet core (EPC) network, a NextGen Packet Core (NPC) network, or some other type of CN. In at least one embodiment, S1 interface <b>2022</b> is split into two parts: S1-U interface <b>2026</b>, which carries traffic data between RAN nodes <b>2018</b> and <b>2020</b> and serving gateway (S-GW) <b>2030</b>, and a S1-mobility management entity (MME) interface <b>2024</b>, which is a signaling interface between RAN nodes <b>2018</b> and <b>2020</b> and MMEs <b>2028</b>.</p><p id="p-0275" num="0274">In at least one embodiment, CN <b>2038</b> comprises MMES <b>2028</b>, S-GW <b>2030</b>, Packet Data Network (PDN) Gateway (P-GW) <b>2034</b>, and a home subscriber server (HSS) <b>2032</b>. In at least one embodiment, MMES <b>2028</b> may be similar in function to a control plane of legacy Serving General Packet Radio Service (GPRS) Support Nodes (SGSN). In at least one embodiment, MMES <b>2028</b> may manage mobility aspects in access such as gateway selection and tracking area list management. In at least one embodiment, HSS <b>2032</b> may comprise a database for network users, including subscription related information to support a network entities' handling of communication sessions. In at least one embodiment, CN <b>2038</b> may comprise one or several HSSs <b>2032</b>, depending on a number of mobile subscribers, on a capacity of an equipment, on an organization of a network, etc. In at least one embodiment, HSS <b>2032</b> can provide support for routing/roaming, authentication, authorization, naming/addressing resolution, location dependencies, etc.</p><p id="p-0276" num="0275">In at least one embodiment, S-GW <b>2030</b> may terminate a S1 interface <b>2022</b> towards RAN <b>2016</b>, and routes data packets between RAN <b>2016</b> and CN <b>2038</b>. In at least one embodiment, S-GW <b>2030</b> may be a local mobility anchor point for inter-RAN node handovers and also may provide an anchor for inter-3GPP mobility. In at least one embodiment, other responsibilities may include lawful intercept, charging, and some policy enforcement.</p><p id="p-0277" num="0276">In at least one embodiment, P-GW <b>2034</b> may terminate an SGi interface toward a PDN. In at least one embodiment, P-GW <b>2034</b> may route data packets between an EPC network <b>2038</b> and external networks such as a network including application server <b>2040</b> (alternatively referred to as application function (AF)) via an Internet Protocol (IP) interface <b>2042</b>. In at least one embodiment, application server <b>2040</b> may be an element offering applications that use IP bearer resources with a core network (e.g., UMTS Packet Services (PS) domain, LTE PS data services, etc.). In at least one embodiment, P-GW <b>2034</b> is shown to be communicatively coupled to an application server <b>2040</b> via an IP communications interface <b>2042</b>. In at least one embodiment, application server <b>2040</b> can also be configured to support one or more communication services (e.g., Voice-over-Internet Protocol (VoIP) sessions, PTT sessions, group communication sessions, social networking services, etc.) for UEs <b>2002</b> and <b>2004</b> via CN <b>2038</b>.</p><p id="p-0278" num="0277">In at least one embodiment, P-GW <b>2034</b> may further be a node for policy enforcement and charging data collection. In at least one embodiment, policy and Charging Enforcement Function (PCRF) <b>2036</b> is a policy and charging control element of CN <b>2038</b>. In at least one embodiment, in a non-roaming scenario, there may be a single PCRF in a Home Public Land Mobile Network (HPLMN) associated with a UE's Internet Protocol Connectivity Access Network (IP-CAN) session. In at least one embodiment, in a roaming scenario with local breakout of traffic, there may be two PCRFs associated with a UE's IP-CAN session: a Home PCRF (H-PCRF) within a HPLMN and a Visited PCRF (V-PCRF) within a Visited Public Land Mobile Network (VPLMN). In at least one embodiment, PCRF <b>2036</b> may be communicatively coupled to application server <b>2040</b> via P-GW <b>2034</b>. In at least one embodiment, application server <b>2040</b> may signal PCRF <b>2036</b> to indicate a new service flow and select an appropriate Quality of Service (QoS) and charging parameters. In at least one embodiment, PCRF <b>2036</b> may provision this rule into a Policy and Charging Enforcement Function (PCEF) (not shown) with an appropriate traffic flow template (TFT) and QoS class of identifier (QCI), which commences a QoS and charging as specified by application server <b>2040</b>.</p><p id="p-0279" num="0278"><figref idref="DRAWINGS">FIG. <b>21</b></figref> illustrates an architecture of a system <b>2100</b> of a network in accordance with some embodiments. In at least one embodiment, system <b>2100</b> is shown to include a UE <b>2102</b>, a 5G access node or RAN node (shown as (R)AN node <b>2108</b>), a User Plane Function (shown as UPF <b>2104</b>), a Data Network (DN <b>2106</b>), which may be, in at least one embodiment, operator services, Internet access or 3rd party services, and a 5G Core Network (5GC) (shown as CN <b>2110</b>).</p><p id="p-0280" num="0279">In at least one embodiment, CN <b>2110</b> includes an Authentication Server Function (AUSF <b>2114</b>); a Core Access and Mobility Management Function (AMF <b>2112</b>); a Session Management Function (SMF <b>2118</b>); a Network Exposure Function (NEF <b>2116</b>); a Policy Control Function (PCF <b>2122</b>); a Network Function (NF) Repository Function (NRF <b>2120</b>); a Unified Data Management (UDM <b>2124</b>); and an Application Function (AF <b>2126</b>). In at least one embodiment, CN <b>2110</b> may also include other elements that are not shown, such as a Structured Data Storage network function (SDSF), an Unstructured Data Storage network function (UDSF), and variations thereof.</p><p id="p-0281" num="0280">In at least one embodiment, UPF <b>2104</b> may act as an anchor point for intra-RAT and inter-RAT mobility, an external PDU session point of interconnect to DN <b>2106</b>, and a branching point to support multi-homed PDU session. In at least one embodiment, UPF <b>2104</b> may also perform packet routing and forwarding, packet inspection, enforce user plane part of policy rules, lawfully intercept packets (UP collection); traffic usage reporting, perform QoS handling for user plane (e.g. packet filtering, gating, UL/DL rate enforcement), perform Uplink Traffic verification (e.g., SDF to QoS flow mapping), transport level packet marking in uplink and downlink, and downlink packet buffering and downlink data notification triggering. In at least one embodiment, UPF <b>2104</b> may include an uplink classifier to support routing traffic flows to a data network. In at least one embodiment, DN <b>2106</b> may represent various network operator services, Internet access, or third party services.</p><p id="p-0282" num="0281">In at least one embodiment, AUSF <b>2114</b> may store data for authentication of UE <b>2102</b> and handle authentication related functionality. In at least one embodiment, AUSF <b>2114</b> may facilitate a common authentication framework for various access types.</p><p id="p-0283" num="0282">In at least one embodiment, AMF <b>2112</b> may be responsible for registration management (e.g., for registering UE <b>2102</b>, etc.), connection management, reachability management, mobility management, and lawful interception of AMF-related events, and access authentication and authorization. In at least one embodiment, AMF <b>2112</b> may provide transport for SM messages for SMF <b>2118</b>, and act as a transparent proxy for routing SM messages. In at least one embodiment, AMF <b>2112</b> may also provide transport for short message service (SMS) messages between UE <b>2102</b> and an SMS function (SMSF) (not shown by <figref idref="DRAWINGS">FIG. <b>21</b></figref>). In at least one embodiment, AMF <b>2112</b> may act as Security Anchor Function (SEA), which may include interaction with AUSF <b>2114</b> and UE <b>2102</b> and receipt of an intermediate key that was established as a result of UE <b>2102</b> authentication process. In at least one embodiment, where USIM based authentication is used, AMF <b>2112</b> may retrieve security material from AUSF <b>2114</b>. In at least one embodiment, AMF <b>2112</b> may also include a Security Context Management (SCM) function, which receives a key from SEA that it uses to derive access-network specific keys. In at least one embodiment, furthermore, AMF <b>2112</b> may be a termination point of RAN CP interface (N2 reference point), a termination point of NAS (NI) signaling, and perform NAS ciphering and integrity protection.</p><p id="p-0284" num="0283">In at least one embodiment, AMF <b>2112</b> may also support NAS signaling with a UE <b>2102</b> over an N3 interworking-function (IWF) interface. In at least one embodiment, N3IWF may be used to provide access to untrusted entities. In at least one embodiment, N3IWF may be a termination point for N2 and N3 interfaces for control plane and user plane, respectively, and as such, may handle N2 signaling from SMF and AMF for PDU sessions and QoS, encapsulate/de-encapsulate packets for IPSec and N3 tunneling, mark N3 user-plane packets in uplink, and enforce QoS corresponding to N3 packet marking taking into account QoS requirements associated to such marking received over N2. In at least one embodiment, N3IWF may also relay uplink and downlink control-plane NAS (NI) signaling between UE <b>2102</b> and AMF <b>2112</b>, and relay uplink and downlink user-plane packets between UE <b>2102</b> and UPF <b>2104</b>. In at least one embodiment, N3IWF also provides mechanisms for IPsec tunnel establishment with UE <b>2102</b>.</p><p id="p-0285" num="0284">In at least one embodiment, SMF <b>2118</b> may be responsible for session management (e.g., session establishment, modify and release, including tunnel maintain between UPF and AN node); UE IP address allocation &#x26; management (including optional Authorization); Selection and control of UP function; Configures traffic steering at UPF to route traffic to proper destination; termination of interfaces towards Policy control functions; control part of policy enforcement and QoS; lawful intercept (for SM events and interface to LI System); termination of SM parts of NAS messages; downlink Data Notification; initiator of AN specific SM information, sent via AMF over N2 to AN; determine SSC mode of a session. In at least one embodiment, SMF <b>2118</b> may include following roaming functionality: handle local enforcement to apply QoS SLAB (VPLMN); charging data collection and charging interface (VPLMN); lawful intercept (in VPLMN for SM events and interface to LI System); support for interaction with external DN for transport of signaling for PDU session authorization/ authentication by external DN.</p><p id="p-0286" num="0285">In at least one embodiment, NEF <b>2116</b> may provide means for securely exposing services and capabilities provided by 3GPP network functions for third party, internal exposure/re-exposure, Application Functions (e.g., AF <b>2126</b>), edge computing or fog computing systems, etc. In at least one embodiment, NEF <b>2116</b> may authenticate, authorize, and/or throttle AFs. In at least one embodiment, NEF <b>2116</b> may also translate information exchanged with AF <b>2126</b> and information exchanged with internal network functions. In at least one embodiment, NEF <b>2116</b> may translate between an AF-Service-Identifier and an internal 5GC information. In at least one embodiment, NEF <b>2116</b> may also receive information from other network functions (NFs) based on exposed capabilities of other network functions. In at least one embodiment, this information may be stored at NEF <b>2116</b> as structured data, or at a data storage NF using a standardized interfaces. In at least one embodiment, stored information can then be re-exposed by NEF <b>2116</b> to other NFs and AFs, and/or used for other purposes such as analytics.</p><p id="p-0287" num="0286">In at least one embodiment, NRF <b>2120</b> may support service discovery functions, receive NF Discovery Requests from NF instances, and provide information of discovered NF instances to NF instances. In at least one embodiment, NRF <b>2120</b> also maintains information of available NF instances and their supported services.</p><p id="p-0288" num="0287">In at least one embodiment, PCF <b>2122</b> may provide policy rules to control plane function(s) to enforce them, and may also support unified policy framework to govern network behavior. In at least one embodiment, PCF <b>2122</b> may also implement a front end (FE) to access subscription information relevant for policy decisions in a UDR of UDM <b>2124</b>.</p><p id="p-0289" num="0288">In at least one embodiment, UDM <b>2124</b> may handle subscription-related information to support a network entities' handling of communication sessions, and may store subscription data of UE <b>2102</b>. In at least one embodiment, UDM <b>2124</b> may include two parts, an application FE and a User Data Repository (UDR). In at least one embodiment, UDM may include a UDM FE, which is in charge of processing of credentials, location management, subscription management and so on. In at least one embodiment, several different front ends may serve a same user in different transactions. In at least one embodiment, UDM-FE accesses subscription information stored in an UDR and performs authentication credential processing; user identification handling; access authorization; registration/mobility management; and subscription management. In at least one embodiment, UDR may interact with PCF <b>2122</b>. In at least one embodiment, UDM <b>2124</b> may also support SMS management, wherein an SMS-FE implements a similar application logic as discussed previously.</p><p id="p-0290" num="0289">In at least one embodiment, AF <b>2126</b> may provide application influence on traffic routing, access to a Network Capability Exposure (NCE), and interact with a policy framework for policy control. In at least one embodiment, NCE may be a mechanism that allows a 5GC and AF <b>2126</b> to provide information to each other via NEF <b>2116</b>, which may be used for edge computing implementations. In at least one embodiment, network operator and third party services may be hosted close to UE <b>2102</b> access point of attachment to achieve an efficient service delivery through a reduced end-to-end latency and load on a transport network. In at least one embodiment, for edge computing implementations, 5GC may select a UPF <b>2104</b> close to UE <b>2102</b> and execute traffic steering from UPF <b>2104</b> to DN <b>2106</b> via N6 interface. In at least one embodiment, this may be based on UE subscription data, UE location, and information provided by AF <b>2126</b>. In at least one embodiment, AF <b>2126</b> may influence UPF (re)selection and traffic routing. In at least one embodiment, based on operator deployment, when AF <b>2126</b> is considered to be a trusted entity, a network operator may permit AF <b>2126</b> to interact directly with relevant NFs.</p><p id="p-0291" num="0290">In at least one embodiment, CN <b>2110</b> may include an SMSF, which may be responsible for SMS subscription checking and verification, and relaying SM messages to/from UE <b>2102</b> to/from other entities, such as an SMS-GMSC/IWMSC/SMS-router. In at least one embodiment, SMS may also interact with AMF <b>2112</b> and UDM <b>2124</b> for notification procedure that UE <b>2102</b> is available for SMS transfer (e.g., set a UE not reachable flag, and notifying UDM <b>2124</b> when UE <b>2102</b> is available for SMS).</p><p id="p-0292" num="0291">In at least one embodiment, system <b>2100</b> may include following service-based interfaces: Namf: Service-based interface exhibited by AMF; Nsmf: Service-based interface exhibited by S1VIF; Nnef: Service-based interface exhibited by NEF; Npcf: Service-based interface exhibited by PCF; Nudm: Service-based interface exhibited by UDM; Naf: Service-based interface exhibited by AF; Nnrf: Service-based interface exhibited by NRF; and Nausf: Service-based interface exhibited by AUSF.</p><p id="p-0293" num="0292">In at least one embodiment, system <b>2100</b> may include following reference points: N1: Reference point between UE and AMF; N2: Reference point between (R)AN and AMF; N3: Reference point between (R)AN and UPF; N4: Reference point between SMF and UPF; and N6: Reference point between UPF and a Data Network. In at least one embodiment, there may be many more reference points and/or service-based interfaces between a NF services in NFs, however, these interfaces and reference points have been omitted for clarity. In at least one embodiment, an NS reference point may be between a PCF and AF; an N7 reference point may be between PCF and S1VIF; an N11 reference point between AMF and SMF; etc. In at least one embodiment, CN <b>2110</b> may include an Nx interface, which is an inter-CN interface between MME and AMF <b>2112</b> in order to enable interworking between CN <b>2110</b> and CN <b>7221</b>.</p><p id="p-0294" num="0293">In at least one embodiment, system <b>2100</b> may include multiple RAN nodes (such as (R)AN node <b>2108</b>) wherein an Xn interface is defined between two or more (R)AN node <b>2108</b> (e.g., gNBs) that connecting to 5GC <b>410</b>, between a (R)AN node <b>2108</b> (e.g., gNB) connecting to CN <b>2110</b> and an eNB (e.g., a macro RAN node), and/or between two eNBs connecting to CN <b>2110</b>.</p><p id="p-0295" num="0294">In at least one embodiment, Xn interface may include an Xn user plane (Xn-U) interface and an Xn control plane (Xn-C) interface. In at least one embodiment, Xn-U may provide non-guaranteed delivery of user plane PDUs and support/provide data forwarding and flow control functionality. In at least one embodiment, Xn-C may provide management and error handling functionality, functionality to manage a Xn-C interface; mobility support for UE <b>2102</b> in a connected mode (e.g., CM-CONNECTED) including functionality to manage UE mobility for connected mode between one or more (R)AN node <b>2108</b>. In at least one embodiment, mobility support may include context transfer from an old (source) serving (R)AN node <b>2108</b> to new (target) serving (R)AN node <b>2108</b>; and control of user plane tunnels between old (source) serving (R)AN node <b>2108</b> to new (target) serving (R)AN node <b>2108</b>.</p><p id="p-0296" num="0295">In at least one embodiment, a protocol stack of a Xn-U may include a transport network layer built on Internet Protocol (IP) transport layer, and a GTP-U layer on top of a UDP and/or IP layer(s) to carry user plane PDUs. In at least one embodiment, Xn-C protocol stack may include an application layer signaling protocol (referred to as Xn Application Protocol (Xn-AP)) and a transport network layer that is built on an SCTP layer. In at least one embodiment, SCTP layer may be on top of an IP layer. In at least one embodiment, SCTP layer provides a guaranteed delivery of application layer messages. In at least one embodiment, in a transport IP layer point-to-point transmission is used to deliver signaling PDUs. In at least one embodiment, Xn-U protocol stack and/or a Xn-C protocol stack may be same or similar to an user plane and/or control plane protocol stack(s) shown and described herein.</p><p id="p-0297" num="0296"><figref idref="DRAWINGS">FIG. <b>22</b></figref> is an illustration of a control plane protocol stack in accordance with some embodiments. In at least one embodiment, a control plane <b>2200</b> is shown as a communications protocol stack between UE <b>2002</b> (or alternatively, UE <b>2004</b>), RAN <b>2016</b>, and MME(s) <b>2028</b>.</p><p id="p-0298" num="0297">In at least one embodiment, PHY layer <b>2202</b> may transmit or receive information used by MAC layer <b>2204</b> over one or more air interfaces. In at least one embodiment, PHY layer <b>2202</b> may further perform link adaptation or adaptive modulation and coding (AMC), power control, cell search (e.g., for initial synchronization and handover purposes), and other measurements used by higher layers, such as an RRC layer <b>2210</b>. In at least one embodiment, PHY layer <b>2202</b> may still further perform error detection on transport channels, forward error correction (FEC) coding/de-coding of transport channels, modulation/demodulation of physical channels, interleaving, rate matching, mapping onto physical channels, and Multiple Input Multiple Output (MIMO) antenna processing.</p><p id="p-0299" num="0298">In at least one embodiment, MAC layer <b>2204</b> may perform mapping between logical channels and transport channels, multiplexing of MAC service data units (SDUs) from one or more logical channels onto transport blocks (TB) to be delivered to PHY via transport channels, de-multiplexing MAC SDUs to one or more logical channels from transport blocks (TB) delivered from PHY via transport channels, multiplexing MAC SDUs onto TBs, scheduling information reporting, error correction through hybrid automatic repeat request (HARD), and logical channel prioritization.</p><p id="p-0300" num="0299">In at least one embodiment, RLC layer <b>2206</b> may operate in a plurality of modes of operation, including: Transparent Mode (TM), Unacknowledged Mode (UM), and Acknowledged Mode (AM). In at least one embodiment, RLC layer <b>2206</b> may execute transfer of upper layer protocol data units (PDUs), error correction through automatic repeat request (ARQ) for AM data transfers, and concatenation, segmentation and reassembly of RLC SDUs for UM and AM data transfers. In at least one embodiment, RLC layer <b>2206</b> may also execute re-segmentation of RLC data PDUs for AM data transfers, reorder RLC data PDUs for UM and AM data transfers, detect duplicate data for UM and AM data transfers, discard RLC SDUs for UM and AM data transfers, detect protocol errors for AM data transfers, and perform RLC re-establishment.</p><p id="p-0301" num="0300">In at least one embodiment, PDCP layer <b>2208</b> may execute header compression and decompression of IP data, maintain PDCP Sequence Numbers (SNs), perform in-sequence delivery of upper layer PDUs at re-establishment of lower layers, eliminate duplicates of lower layer SDUs at re-establishment of lower layers for radio bearers mapped on RLC AM, cipher and decipher control plane data, perform integrity protection and integrity verification of control plane data, control timer-based discard of data, and perform security operations (e.g., ciphering, deciphering, integrity protection, integrity verification, etc.).</p><p id="p-0302" num="0301">In at least one embodiment, main services and functions of a RRC layer <b>2210</b> may include broadcast of system information (e.g., included in Master Information Blocks (MIBs) or System Information Blocks (SIBs) related to a non-access stratum (NAS)), broadcast of system information related to an access stratum (AS), paging, establishment, maintenance and release of an RRC connection between an UE and E-UTRAN (e.g., RRC connection paging, RRC connection establishment, RRC connection modification, and RRC connection release), establishment, configuration, maintenance and release of point-to-point radio bearers, security functions including key management, inter radio access technology (RAT) mobility, and measurement configuration for UE measurement reporting. In at least one embodiment, said MIBs and SIBs may comprise one or more information elements (IEs), which may each comprise individual data fields or data structures.</p><p id="p-0303" num="0302">In at least one embodiment, UE <b>2002</b> and RAN <b>2016</b> may utilize a Uu interface (e.g., an LTE-Uu interface) to exchange control plane data via a protocol stack comprising PHY layer <b>2202</b>, MAC layer <b>2204</b>, RLC layer <b>2206</b>, PDCP layer <b>2208</b>, and RRC layer <b>2210</b>.</p><p id="p-0304" num="0303">In at least one embodiment, non-access stratum (NAS) protocols (NAS protocols <b>2212</b>) form a highest stratum of a control plane between UE <b>2002</b> and MME(s) <b>2028</b>. In at least one embodiment, NAS protocols <b>2212</b> support mobility of UE <b>2002</b> and session management procedures to establish and maintain IP connectivity between UE <b>2002</b> and P-GW <b>2034</b>.</p><p id="p-0305" num="0304">In at least one embodiment, Si Application Protocol (S1-AP) layer (Si-AP layer <b>2222</b>) may support functions of a Si interface and comprise Elementary Procedures (EPs). In at least one embodiment, an EP is a unit of interaction between RAN <b>2016</b> and CN <b>2028</b>. In at least one embodiment, S1-AP layer services may comprise two groups: UE-associated services and non UE-associated services. In at least one embodiment, these services perform functions including, but not limited to: E-UTRAN Radio Access Bearer (E-RAB) management, UE capability indication, mobility, NAS signaling transport, RAN Information Management (RIM), and configuration transfer.</p><p id="p-0306" num="0305">In at least one embodiment, Stream Control Transmission Protocol (SCTP) layer (alternatively referred to as a stream control transmission protocol/internet protocol (SCTP/IP) layer) (SCTP layer <b>2220</b>) may ensure reliable delivery of signaling messages between RAN <b>2016</b> and MME(s) <b>2028</b> based, in part, on an IP protocol, supported by an IP layer <b>2218</b>. In at least one embodiment, L2 layer <b>2216</b> and an L1 layer <b>2214</b> may refer to communication links (e.g., wired or wireless) used by a RAN node and MME to exchange information.</p><p id="p-0307" num="0306">In at least one embodiment, RAN <b>2016</b> and MME(s) <b>2028</b> may utilize an Si -MME interface to exchange control plane data via a protocol stack comprising a L1 layer <b>2214</b>, L2 layer <b>2216</b>, IP layer <b>2218</b>, SCTP layer <b>2220</b>, and Si -AP layer <b>2222</b>.</p><p id="p-0308" num="0307"><figref idref="DRAWINGS">FIG. <b>23</b></figref> is an illustration of a user plane protocol stack in accordance with at least one embodiment. In at least one embodiment, a user plane <b>2300</b> is shown as a communications protocol stack between a UE <b>2002</b>, RAN <b>2016</b>, S-GW <b>2030</b>, and P-GW <b>2034</b>. In at least one embodiment, user plane <b>2300</b> may utilize a same protocol layers as control plane <b>2200</b>. In at least one embodiment, UE <b>2002</b> and RAN <b>2016</b> may utilize a Uu interface (e.g., an LTE-Uu interface) to exchange user plane data via a protocol stack comprising PHY layer <b>2202</b>, MAC layer <b>2204</b>, RLC layer <b>2206</b>, PDCP layer <b>2208</b>.</p><p id="p-0309" num="0308">In at least one embodiment, General Packet Radio Service (GPRS) Tunneling Protocol for a user plane (GTP-U) layer (GTP-U layer <b>2304</b>) may be used for carrying user data within a GPRS core network and between a radio access network and a core network. In at least one embodiment, user data transported can be packets in any of IPv4, IPv6, or PPP formats. In at least one embodiment, UDP and IP security (UDP/IP) layer (UDP/IP layer <b>2302</b>) may provide checksums for data integrity, port numbers for addressing different functions at a source and destination, and encryption and authentication on selected data flows. In at least one embodiment, RAN <b>2016</b> and S-GW <b>2030</b> may utilize an Si -U interface to exchange user plane data via a protocol stack comprising L1 layer <b>2214</b>, L2 layer <b>2216</b>, UDP/IP layer <b>2302</b>, and GTP-U layer <b>2304</b>. In at least one embodiment, S-GW <b>2030</b> and P-GW <b>2034</b> may utilize an S5/S8a interface to exchange user plane data via a protocol stack comprising L1 layer <b>2214</b>, L2 layer <b>2216</b>, UDP/IP layer <b>2302</b>, and GTP-U layer <b>2304</b>. In at least one embodiment, as discussed above with respect to <figref idref="DRAWINGS">FIG. <b>22</b></figref>, NAS protocols support a mobility of UE <b>2002</b> and session management procedures to establish and maintain IP connectivity between UE <b>2002</b> and P-GW <b>2034</b>.</p><p id="p-0310" num="0309"><figref idref="DRAWINGS">FIG. <b>24</b></figref> illustrates components <b>2400</b> of a core network in accordance with at least one embodiment. In at least one embodiment, components of CN <b>2038</b> may be implemented in one physical node or separate physical nodes including components to read and execute instructions from a machine-readable or computer-readable medium (e.g., a non-transitory machine-readable storage medium). In at least one embodiment, Network Functions Virtualization (NFV) is utilized to virtualize any or all of above described network node functions via executable instructions stored in one or more computer readable storage mediums (described in further detail below). In at least one embodiment, a logical instantiation of CN <b>2038</b> may be referred to as a network slice <b>2402</b> (e.g., network slice <b>2402</b> is shown to include HSS <b>2032</b>, MME(s) <b>2028</b>, and S-GW <b>2030</b>). In at least one embodiment, a logical instantiation of a portion of CN <b>2038</b> may be referred to as a network sub-slice <b>2404</b> (e.g., network sub-slice <b>2404</b> is shown to include P-GW <b>2034</b> and PCRF <b>2036</b>).</p><p id="p-0311" num="0310">In at least one embodiment, NFV architectures and infrastructures may be used to virtualize one or more network functions, alternatively performed by proprietary hardware, onto physical resources comprising a combination of industry-standard server hardware, storage hardware, or switches. In at least one embodiment, NFV systems can be used to execute virtual or reconfigurable implementations of one or more EPC components/functions.</p><p id="p-0312" num="0311"><figref idref="DRAWINGS">FIG. <b>25</b></figref> is a block diagram illustrating components, according to at least one embodiment, of a system <b>2500</b> to support network function virtualization (NFV). In at least one embodiment, system <b>2500</b> is illustrated as including a virtualized infrastructure manager (shown as VIM <b>2502</b>), a network function virtualization infrastructure (shown as NFVI <b>2504</b>), a VNF manager (shown as VNFM <b>2506</b>), virtualized network functions (shown as VNF <b>2508</b>), an element manager (shown as EM <b>2510</b>), an NFV Orchestrator (shown as NFVO <b>2512</b>), and a network manager (shown as NM <b>2514</b>).</p><p id="p-0313" num="0312">In at least one embodiment, VIM <b>2502</b> manages resources of NFVI <b>2504</b>. In at least one embodiment, NFVI <b>2504</b> can include physical or virtual resources and applications (including hypervisors) used to execute system <b>2500</b>. In at least one embodiment, VIM <b>2502</b> may manage a life cycle of virtual resources with NFVI <b>2504</b> (e.g., creation, maintenance, and tear down of virtual machines (VMs) associated with one or more physical resources), track VM instances, track performance, fault and security of VM instances and associated physical resources, and expose VM instances and associated physical resources to other management systems.</p><p id="p-0314" num="0313">In at least one embodiment, VNFM <b>2506</b> may manage VNF <b>2508</b>. In at least one embodiment, VNF <b>2508</b> may be used to execute EPC components/functions. In at least one embodiment, VNFM <b>2506</b> may manage a life cycle of VNF <b>2508</b> and track performance, fault and security of virtual aspects of VNF <b>2508</b>. In at least one embodiment, EM <b>2510</b> may track performance, fault and security of functional aspects of VNF <b>2508</b>. In at least one embodiment, tracking data from VNFM <b>2506</b> and EM <b>2510</b> may comprise, in at least one embodiment, performance measurement (PM) data used by VIM <b>2502</b> or NFVI <b>2504</b>. In at least one embodiment, both VNFM <b>2506</b> and EM <b>2510</b> can scale up/down a quantity of VNFs of system <b>2500</b>.</p><p id="p-0315" num="0314">In at least one embodiment, NFVO <b>2512</b> may coordinate, authorize, release and engage resources of NFVI <b>2504</b> in order to provide a requested service (e.g., to execute an EPC function, component, or slice). In at least one embodiment, NM <b>2514</b> may provide a package of end-user functions with responsibility for a management of a network, which may include network elements with VNFs, non-virtualized network functions, or both (management of VNFs may occur via an EM <b>2510</b>).</p><heading id="h-0009" level="1">Computer-Based Systems</heading><p id="p-0316" num="0315">The following figures set forth, without limitation, exemplary computer-based systems that can be used to implement at least one embodiment.</p><p id="p-0317" num="0316"><figref idref="DRAWINGS">FIG. <b>26</b></figref> illustrates a processing system <b>2600</b>, in accordance with at least one embodiment. In at least one embodiment, processing system <b>2600</b> includes one or more processors <b>2602</b> and one or more graphics processors <b>2608</b>, and may be a single processor desktop system, a multiprocessor workstation system, or a server system having a large number of processors <b>2602</b> or processor cores <b>2607</b>. In at least one embodiment, processing system <b>2600</b> is a processing platform incorporated within a system-on-a-chip (&#x201c;Sort&#x201d;) integrated circuit for use in mobile, handheld, or embedded devices.</p><p id="p-0318" num="0317">In at least one embodiment, processing system <b>2600</b> can include, or be incorporated within a server-based gaming platform, a game console, a media console, a mobile gaming console, a handheld game console, or an online game console. In at least one embodiment, processing system <b>2600</b> is a mobile phone, smart phone, tablet computing device or mobile Internet device. In at least one embodiment, processing system <b>2600</b> can also include, couple with, or be integrated within a wearable device, such as a smart watch wearable device, smart eyewear device, augmented reality device, or virtual reality device. In at least one embodiment, processing system <b>2600</b> is a television or set top box device having one or more processors <b>2602</b> and a graphical interface generated by one or more graphics processors <b>2608</b>.</p><p id="p-0319" num="0318">In at least one embodiment, one or more processors <b>2602</b> each include one or more processor cores <b>2607</b> to process instructions which, when executed, perform operations for system and user software. In at least one embodiment, each of one or more processor cores <b>2607</b> is configured to process a specific instruction set <b>2609</b>. In at least one embodiment, instruction set <b>2609</b> may facilitate Complex Instruction Set Computing (&#x201c;CISC&#x201d;), Reduced Instruction Set Computing (&#x201c;RISC&#x201d;), or computing via a Very Long Instruction Word (&#x201c;VLIW&#x201d;). In at least one embodiment, processor cores <b>2607</b> may each process a different instruction set <b>2609</b>, which may include instructions to facilitate emulation of other instruction sets. In at least one embodiment, processor core <b>2607</b> may also include other processing devices, such as a digital signal processor (&#x201c;DSP&#x201d;).</p><p id="p-0320" num="0319">In at least one embodiment, processor <b>2602</b> includes cache memory (cache&#x201c;) <b>2604</b>. In at least one embodiment, processor <b>2602</b> can have a single internal cache or multiple levels of internal cache. In at least one embodiment, cache memory is shared among various components of processor <b>2602</b>. In at least one embodiment, processor <b>2602</b> also uses an external cache (e.g., a Level 3 (&#x201d;L3&#x2033;) cache or Last Level Cache (&#x201c;LLC&#x201d;)) (not shown), which may be shared among processor cores <b>2607</b> using known cache coherency techniques. In at least one embodiment, register file <b>2606</b> is additionally included in processor <b>2602</b> which may include different types of registers for storing different types of data (e.g., integer registers, floating point registers, status registers, and an instruction pointer register). In at least one embodiment, register file <b>2606</b> may include general-purpose registers or other registers.</p><p id="p-0321" num="0320">In at least one embodiment, one or more processor(s) <b>2602</b> are coupled with one or more interface bus(es) <b>2610</b> to transmit communication signals such as address, data, or control signals between processor <b>2602</b> and other components in processing system <b>2600</b>. In at least one embodiment interface bus <b>2610</b>, in one embodiment, can be a processor bus, such as a version of a Direct Media Interface (&#x201c;DMI&#x201d;) bus. In at least one embodiment, interface bus <b>2610</b> is not limited to a DMI bus, and may include one or more Peripheral Component Interconnect buses (e.g., &#x201c;PCI,&#x201d; PCI Express (&#x201c;PCIe&#x201d;)), memory buses, or other types of interface buses. In at least one embodiment processor(s) <b>2602</b> include an integrated memory controller <b>2616</b> and a platform controller hub <b>2630</b>. In at least one embodiment, memory controller <b>2616</b> facilitates communication between a memory device and other components of processing system <b>2600</b>, while platform controller hub (&#x201c;PCH&#x201d;) <b>2630</b> provides connections to Input/Output (&#x201c;I/O&#x201d;) devices via a local I/O bus.</p><p id="p-0322" num="0321">In at least one embodiment, memory device <b>2620</b> can be a dynamic random access memory (&#x201c;DRAM&#x201d;) device, a static random access memory (&#x201c;SRAM&#x201d;) device, flash memory device, phase-change memory device, or some other memory device having suitable performance to serve as processor memory. In at least one embodiment memory device <b>2620</b> can operate as system memory for processing system <b>2600</b>, to store data <b>2622</b> and instructions <b>2621</b> for use when one or more processors <b>2602</b> executes an application or process. In at least one embodiment, memory controller <b>2616</b> also couples with an optional external graphics processor <b>2612</b>, which may communicate with one or more graphics processors <b>2608</b> in processors <b>2602</b> to perform graphics and media operations. In at least one embodiment, a display device <b>2611</b> can connect to processor(s) <b>2602</b>. In at least one embodiment display device <b>2611</b> can include one or more of an internal display device, as in a mobile electronic device or a laptop device or an external display device attached via a display interface (e.g., DisplayPort, etc.). In at least one embodiment, display device <b>2611</b> can include a head mounted display (&#x201c;HIVID&#x201d;) such as a stereoscopic display device for use in virtual reality (&#x201c;VR&#x201d;) applications or augmented reality (&#x201c;AR&#x201d;) applications.</p><p id="p-0323" num="0322">In at least one embodiment, platform controller hub <b>2630</b> enables peripherals to connect to memory device <b>2620</b> and processor <b>2602</b> via a high-speed I/O bus. In at least one embodiment, I/O peripherals include, but are not limited to, an audio controller <b>2646</b>, a network controller <b>2634</b>, a firmware interface <b>2628</b>, a wireless transceiver <b>2626</b>, touch sensors <b>2625</b>, a data storage device <b>2624</b> (e.g., hard disk drive, flash memory, etc.). In at least one embodiment, data storage device <b>2624</b> can connect via a storage interface (e.g., SATA) or via a peripheral bus, such as PCI, or PCIe. In at least one embodiment, touch sensors <b>2625</b> can include touch screen sensors, pressure sensors, or fingerprint sensors. In at least one embodiment, wireless transceiver <b>2626</b> can be a Wi-Fi transceiver, a Bluetooth transceiver, or a mobile network transceiver such as a 3G, 4G, or Long Term Evolution (&#x201c;LTE&#x201d;) transceiver. In at least one embodiment, firmware interface <b>2628</b> enables communication with system firmware, and can be, in at least one embodiment, a unified extensible firmware interface (&#x201c;UEFI&#x201d;). In at least one embodiment, network controller <b>2634</b> can enable a network connection to a wired network. In at least one embodiment, a high-performance network controller (not shown) couples with interface bus <b>2610</b>. In at least one embodiment, audio controller <b>2646</b> is a multi-channel high definition audio controller. In at least one embodiment, processing system <b>2600</b> includes an optional legacy I/O controller <b>2640</b> for coupling legacy (e.g., Personal System 2 (&#x201c;PS/2&#x201d;)) devices to processing system <b>2600</b>. In at least one embodiment, platform controller hub <b>2630</b> can also connect to one or more Universal Serial Bus (&#x201c;USB&#x201d;) controllers <b>2642</b> connect input devices, such as keyboard and mouse <b>2643</b> combinations, a camera <b>2644</b>, or other USB input devices.</p><p id="p-0324" num="0323">In at least one embodiment, an instance of memory controller <b>2616</b> and platform controller hub <b>2630</b> may be integrated into a discreet external graphics processor, such as external graphics processor <b>2612</b>. In at least one embodiment, platform controller hub <b>2630</b> and/or memory controller <b>2616</b> may be external to one or more processor(s) <b>2602</b>. In at least one embodiment, processing system <b>2600</b> can include an external memory controller <b>2616</b> and platform controller hub <b>2630</b>, which may be configured as a memory controller hub and peripheral controller hub within a system chipset that is in communication with processor(s) <b>2602</b>.</p><p id="p-0325" num="0324"><figref idref="DRAWINGS">FIG. <b>27</b></figref> illustrates a computer system <b>2700</b>, in accordance with at least one embodiment. In at least one embodiment, computer system <b>2700</b> may be a system with interconnected devices and components, an SOC, or some combination. In at least on embodiment, computer system <b>2700</b> is formed with a processor <b>2702</b> that may include execution units to execute an instruction. In at least one embodiment, computer system <b>2700</b> may include, without limitation, a component, such as processor <b>2702</b> to employ execution units including logic to perform algorithms for processing data. In at least one embodiment, computer system <b>2700</b> may include processors, such as PENTIUM&#xae; Processor family, Xeon&#x2122;, Itanium&#xae;, XScale&#x2122; and/or StrongARM&#x2122;, Intel&#xae; Core&#x2122;, or Intel&#xae; Nervana&#x2122; microprocessors available from Intel Corporation of Santa Clara, Calif., although other systems (including PCs having other microprocessors, engineering workstations, set-top boxes and like) may also be used. In at least one embodiment, computer system <b>2700</b> may execute a version of WINDOWS' operating system available from Microsoft Corporation of Redmond, Wash., although other operating systems (UNIX and Linux in at least one embodiment), embedded software, and/or graphical user interfaces, may also be used.</p><p id="p-0326" num="0325">In at least one embodiment, computer system <b>2700</b> may be used in other devices such as handheld devices and embedded applications. Some ones of the at least one embodiments of handheld devices include cellular phones, Internet Protocol devices, digital cameras, personal digital assistants (&#x201c;PDAs&#x201d;), and handheld PCs. In at least one embodiment, embedded applications may include a microcontroller, a digital signal processor (DSP), an SoC, network computers (&#x201c;NetPCs&#x201d;), set-top boxes, network hubs, wide area network (&#x201c;WAN&#x201d;) switches, or any other system that may perform one or more instructions.</p><p id="p-0327" num="0326">In at least one embodiment, computer system <b>2700</b> may include, without limitation, processor <b>2702</b> that may include, without limitation, one or more execution units <b>2708</b> that may be configured to execute a Compute Unified Device Architecture (&#x201c;CUDA&#x201d;) (CUDA&#xae; is developed by NVIDIA Corporation of Santa Clara, Calif.) program. In at least one embodiment, a CUDA program is at least a portion of a software application written in a CUDA programming language. In at least one embodiment, computer system <b>2700</b> is a single processor desktop or server system. In at least one embodiment, computer system <b>2700</b> may be a multiprocessor system. In at least one embodiment, processor <b>2702</b> may include, without limitation, a CISC microprocessor, a RISC microprocessor, a VLIW microprocessor, a processor implementing a combination of instruction sets, or any other processor device, such as a digital signal processor, in at least one embodiment. In at least one embodiment, processor <b>2702</b> may be coupled to a processor bus <b>2710</b> that may transmit data signals between processor <b>2702</b> and other components in computer system <b>2700</b>.</p><p id="p-0328" num="0327">In at least one embodiment, processor <b>2702</b> may include, without limitation, a Level 1 (&#x201c;L1&#x201d;) internal cache memory (&#x201c;cache&#x201d;) <b>2704</b>. In at least one embodiment, processor <b>2702</b> may have a single internal cache or multiple levels of internal cache. In at least one embodiment, cache memory may reside external to processor <b>2702</b>. In at least one embodiment, processor <b>2702</b> may also include a combination of both internal and external caches. In at least one embodiment, a register file <b>2706</b> may store different types of data in various registers including, without limitation, integer registers, floating point registers, status registers, and instruction pointer register.</p><p id="p-0329" num="0328">In at least one embodiment, execution unit <b>2708</b>, including, without limitation, logic to perform integer and floating point operations, also resides in processor <b>2702</b>. Processor <b>2702</b> may also include a microcode (&#x201c;ucode&#x201d;) read only memory (&#x201c;ROM&#x201d;) that stores microcode for certain macro instructions. In at least one embodiment, execution unit <b>2708</b> may include logic to handle a packed instruction set <b>2709</b>. In at least one embodiment, by including packed instruction set <b>2709</b> in an instruction set of a general-purpose processor <b>2702</b>, along with associated circuitry to execute instructions, operations used by many multimedia applications may be performed using packed data in a general-purpose processor <b>2702</b>. In at least one embodiment, many multimedia applications may be accelerated and executed more efficiently by using full width of a processor's data bus for performing operations on packed data, which may eliminate a need to transfer smaller units of data across a processor's data bus to perform one or more operations one data element at a time.</p><p id="p-0330" num="0329">In at least one embodiment, execution unit <b>2708</b> may also be used in microcontrollers, embedded processors, graphics devices, DSPs, and other types of logic circuits. In at least one embodiment, computer system <b>2700</b> may include, without limitation, a memory <b>2720</b>. In at least one embodiment, memory <b>2720</b> may be implemented as a DRAM device, an SRAM device, flash memory device, or other memory device. Memory <b>2720</b> may store instruction(s) <b>2719</b> and/or data <b>2721</b> represented by data signals that may be executed by processor <b>2702</b>.</p><p id="p-0331" num="0330">In at least one embodiment, a system logic chip may be coupled to processor bus <b>2710</b> and memory <b>2720</b>. In at least one embodiment, a system logic chip may include, without limitation, a memory controller hub (&#x201c;MCH&#x201d;) <b>2716</b>, and processor <b>2702</b> may communicate with MCH <b>2716</b> via processor bus <b>2710</b>. In at least one embodiment, MCH <b>2716</b> may provide a high bandwidth memory path <b>2718</b> to memory <b>2720</b> for instruction and data storage and for storage of graphics commands, data and textures. In at least one embodiment, MCH <b>2716</b> may direct data signals between processor <b>2702</b>, memory <b>2720</b>, and other components in computer system <b>2700</b> and to bridge data signals between processor bus <b>2710</b>, memory <b>2720</b>, and a system I/O <b>2722</b>. In at least one embodiment, system logic chip may provide a graphics port for coupling to a graphics controller. In at least one embodiment, MCH <b>2716</b> may be coupled to memory <b>2720</b> through high bandwidth memory path <b>2718</b> and graphics/video card <b>2712</b> may be coupled to MCH <b>2716</b> through an Accelerated Graphics Port (&#x201c;AGP&#x201d;) interconnect <b>2714</b>.</p><p id="p-0332" num="0331">In at least one embodiment, computer system <b>2700</b> may use system I/O <b>2722</b> that is a proprietary hub interface bus to couple MCH <b>2716</b> to I/O controller hub (&#x201c;ICH&#x201d;) <b>2730</b>. In at least one embodiment, ICH <b>2730</b> may provide direct connections to some I/O devices via a local I/O bus. In at least one embodiment, local I/O bus may include, without limitation, a high-speed I/O bus for connecting peripherals to memory <b>2720</b>, a chipset, and processor <b>2702</b>. Examples may include, without limitation, an audio controller <b>2729</b>, a firmware hub (&#x201c;flash BIOS&#x201d;) <b>2728</b>, a wireless transceiver <b>2726</b>, a data storage <b>2724</b>, a legacy I/O controller <b>2723</b> containing a user input interface <b>2725</b> and a keyboard interface, a serial expansion port <b>2777</b>, such as a USB, and a network controller <b>2734</b>. Data storage <b>2724</b> may comprise a hard disk drive, a floppy disk drive, a CD-ROM device, a flash memory device, or other mass storage device.</p><p id="p-0333" num="0332">In at least one embodiment, <figref idref="DRAWINGS">FIG. <b>27</b></figref> illustrates a system, which includes interconnected hardware devices or &#x201c;chips.&#x201d; In at least one embodiment, <figref idref="DRAWINGS">FIG. <b>27</b></figref> may illustrate an exemplary SoC. In at least one embodiment, devices illustrated in <figref idref="DRAWINGS">FIG. <b>27</b></figref> may be interconnected with proprietary interconnects, standardized interconnects (e.g., PCIe), or some combination thereof. In at least one embodiment, one or more components of system <b>2700</b> are interconnected using compute express link (&#x201c;CXL&#x201d;) interconnects.</p><p id="p-0334" num="0333"><figref idref="DRAWINGS">FIG. <b>28</b></figref> illustrates a system <b>2800</b>, in accordance with at least one embodiment. In at least one embodiment, system <b>2800</b> is an electronic device that utilizes a processor <b>2810</b>. In at least one embodiment, system <b>2800</b> may be, in at least one embodiment and without limitation, a notebook, a tower server, a rack server, a blade server, a laptop, a desktop, a tablet, a mobile device, a phone, an embedded computer, or any other suitable electronic device.</p><p id="p-0335" num="0334">In at least one embodiment, system <b>2800</b> may include, without limitation, processor <b>2810</b> communicatively coupled to any suitable number or kind of components, peripherals, modules, or devices. In at least one embodiment, processor <b>2810</b> is coupled using a bus or interface, such as an I2C bus, a System Management Bus (&#x201c;SMBus&#x201d;), a Low Pin Count (&#x201c;LPC&#x201d;) bus, a Serial Peripheral Interface (&#x201c;SPI&#x201d;), a High Definition Audio (&#x201c;HDA&#x201d;) bus, a Serial Advance Technology Attachment (&#x201c;SATA&#x201d;) bus, a USB (versions 1, 2, 3), or a Universal Asynchronous Receiver/Transmitter (&#x201c;UART&#x201d;) bus. In at least one embodiment, <figref idref="DRAWINGS">FIG. <b>28</b></figref> illustrates a system which includes interconnected hardware devices or &#x201c;chips.&#x201d; In at least one embodiment, <figref idref="DRAWINGS">FIG. <b>28</b></figref> may illustrate an exemplary SoC. In at least one embodiment, devices illustrated in <figref idref="DRAWINGS">FIG. <b>28</b></figref> may be interconnected with proprietary interconnects, standardized interconnects (e.g., PCIe) or some combination thereof. In at least one embodiment, one or more components of <figref idref="DRAWINGS">FIG. <b>28</b></figref> are interconnected using CXL interconnects.</p><p id="p-0336" num="0335">In at least one embodiment, <figref idref="DRAWINGS">FIG. <b>28</b></figref> may include a display <b>2824</b>, a touch screen <b>2825</b>, a touch pad <b>2830</b>, a Near Field Communications unit (&#x201c;NFC&#x201d;) <b>2845</b>, a sensor hub <b>2840</b>, a thermal sensor <b>2846</b>, an Express Chipset (&#x201c;EC&#x201d;) <b>2835</b>, a Trusted Platform Module (&#x201c;TPM&#x201d;) <b>2838</b>, BIOS/firmware/flash memory (&#x201c;BIOS, FW Flash&#x201d;) <b>2822</b>, a DSP <b>2860</b>, a Solid State Disk (&#x201c;SSD&#x201d;) or Hard Disk Drive (&#x201c;HDD&#x201d;) <b>2820</b>, a wireless local area network unit (&#x201c;WLAN&#x201d;) <b>2850</b>, a Bluetooth unit <b>2852</b>, a Wireless Wide Area Network unit (&#x201c;WWAN&#x201d;) <b>2856</b>, a Global Positioning System (&#x201c;GPS&#x201d;) <b>2855</b>, a camera (&#x201c;USB 3.0 camera&#x201d;) <b>2854</b> such as a USB 3.0 camera, or a Low Power Double Data Rate (&#x201c;LPDDR&#x201d;) memory unit (&#x201c;LPDDR3&#x201d;) <b>2815</b> implemented, in at least one embodiment, LPDDR3 standard. These components may each be implemented in any suitable manner.</p><p id="p-0337" num="0336">In at least one embodiment, other components may be communicatively coupled to processor <b>2810</b> through components discussed above. In at least one embodiment, an accelerometer <b>2841</b>, an Ambient Light Sensor (&#x201c;ALS&#x201d;) <b>2842</b>, a compass <b>2843</b>, and a gyroscope <b>2844</b> may be communicatively coupled to sensor hub <b>2840</b>. In at least one embodiment, a thermal sensor <b>2839</b>, a fan <b>2837</b>, a keyboard <b>2846</b>, and a touch pad <b>2830</b> may be communicatively coupled to EC <b>2835</b>. In at least one embodiment, a speaker <b>2863</b>, a headphones <b>2864</b>, and a microphone (&#x201c;mic&#x201d;) <b>2865</b> may be communicatively coupled to an audio unit (&#x201c;audio codec and class d amp&#x201d;) <b>2864</b>, which may in turn be communicatively coupled to DSP <b>2860</b>. In at least one embodiment, audio unit <b>2864</b> may include, without limitation, an audio coder/decoder (&#x201c;codec&#x201d;) and a class D amplifier. In at least one embodiment, a SIM card (&#x201c;SIM&#x201d;) <b>2857</b> may be communicatively coupled to WWAN unit <b>2856</b>. In at least one embodiment, components such as WLAN unit <b>2850</b> and Bluetooth unit <b>2852</b>, as well as WWAN unit <b>2856</b> may be implemented in a Next Generation Form Factor (&#x201c;NGFF&#x201d;).</p><p id="p-0338" num="0337"><figref idref="DRAWINGS">FIG. <b>29</b></figref> illustrates an exemplary integrated circuit <b>2900</b>, in accordance with at least one embodiment. In at least one embodiment, exemplary integrated circuit <b>2900</b> is an SoC that may be fabricated using one or more IP cores. In at least one embodiment, integrated circuit <b>2900</b> includes one or more application processor(s) <b>2905</b> (e.g., CPUs), at least one graphics processor <b>2910</b>, and may additionally include an image processor <b>2915</b> and/or a video processor <b>2920</b>, any of which may be a modular IP core. In at least one embodiment, integrated circuit <b>2900</b> includes peripheral or bus logic including a USB controller <b>2925</b>, a UART controller <b>2930</b>, an SPI/SDIO controller <b>2935</b>, and an I2S/I2C controller <b>2940</b>. In at least one embodiment, integrated circuit <b>2900</b> can include a display device <b>2945</b> coupled to one or more of a high-definition multimedia interface (&#x201c;HDMI&#x201d;) controller <b>2950</b> and a mobile industry processor interface (&#x201c;MIPI&#x201d;) display interface <b>2955</b>. In at least one embodiment, storage may be provided by a flash memory subsystem <b>2960</b> including flash memory and a flash memory controller. In at least one embodiment, a memory interface may be provided via a memory controller <b>2965</b> for access to SDRAM or SRAM memory devices. In at least one embodiment, some integrated circuits additionally include an embedded security engine <b>2970</b>.</p><p id="p-0339" num="0338"><figref idref="DRAWINGS">FIG. <b>30</b></figref> illustrates a computing system <b>3000</b>, according to at least one embodiment; In at least one embodiment, computing system <b>3000</b> includes a processing subsystem <b>3001</b> having one or more processor(s) <b>3002</b> and a system memory <b>3004</b> communicating via an interconnection path that may include a memory hub <b>3005</b>. In at least one embodiment, memory hub <b>3005</b> may be a separate component within a chipset component or may be integrated within one or more processor(s) <b>3002</b>. In at least one embodiment, memory hub <b>3005</b> couples with an I/O subsystem <b>3011</b> via a communication link <b>3006</b>. In at least one embodiment, I/O subsystem <b>3011</b> includes an I/O hub <b>3007</b> that can enable computing system <b>3000</b> to receive input from one or more input device(s) <b>3008</b>. In at least one embodiment, I/O hub <b>3007</b> can enable a display controller, which may be included in one or more processor(s) <b>3002</b>, to provide outputs to one or more display device(s) <b>3010</b>A. In at least one embodiment, one or more display device(s) <b>3010</b>A coupled with I/O hub <b>3007</b> can include a local, internal, or embedded display device.</p><p id="p-0340" num="0339">In at least one embodiment, processing subsystem <b>3001</b> includes one or more parallel processor(s) <b>3012</b> coupled to memory hub <b>3005</b> via a bus or other communication link <b>3013</b>. In at least one embodiment, communication link <b>3013</b> may be one of any number of standards based communication link technologies or protocols, such as, but not limited to PCIe, or may be a vendor specific communications interface or communications fabric. In at least one embodiment, one or more parallel processor(s) <b>3012</b> form a computationally focused parallel or vector processing system that can include a large number of processing cores and/or processing clusters, such as a many integrated core processor. In at least one embodiment, one or more parallel processor(s) <b>3012</b> form a graphics processing subsystem that can output pixels to one of one or more display device(s) <b>3010</b>A coupled via I/O Hub <b>3007</b>. In at least one embodiment, one or more parallel processor(s) <b>3012</b> can also include a display controller and display interface (not shown) to enable a direct connection to one or more display device(s) <b>3010</b>B.</p><p id="p-0341" num="0340">In at least one embodiment, a system storage unit <b>3014</b> can connect to I/O hub <b>3007</b> to provide a storage mechanism for computing system <b>3000</b>. In at least one embodiment, an I/O switch <b>3016</b> can be used to provide an interface mechanism to enable connections between I/O hub <b>3007</b> and other components, such as a network adapter <b>3018</b> and/or wireless network adapter <b>3019</b> that may be integrated into a platform, and various other devices that can be added via one or more add-in device(s) <b>3020</b>. In at least one embodiment, network adapter <b>3018</b> can be an Ethernet adapter or another wired network adapter. In at least one embodiment, wireless network adapter <b>3019</b> can include one or more of a Wi-Fi, Bluetooth, NFC, or other network device that includes one or more wireless radios.</p><p id="p-0342" num="0341">In at least one embodiment, computing system <b>3000</b> can include other components not explicitly shown, including USB or other port connections, optical storage drives, video capture devices, and/or variations thereof, that may also be connected to I/O hub <b>3007</b>. In at least one embodiment, communication paths interconnecting various components in <figref idref="DRAWINGS">FIG. <b>30</b></figref> may be implemented using any suitable protocols, such as PCI based protocols (e.g., PCIe), or other bus or point-to-point communication interfaces and/or protocol(s), such as NVLink high-speed interconnect, or interconnect protocols.</p><p id="p-0343" num="0342">In at least one embodiment, one or more parallel processor(s) <b>3012</b> incorporate circuitry optimized for graphics and video processing, including, in at least one embodiment, video output circuitry, and constitutes a graphics processing unit (&#x201c;GPU&#x201d;). In at least one embodiment, one or more parallel processor(s) <b>3012</b> incorporate circuitry optimized for general purpose processing. In at least embodiment, components of computing system <b>3000</b> may be integrated with one or more other system elements on a single integrated circuit. In at least one embodiment, one or more parallel processor(s) <b>3012</b>, memory hub <b>3005</b>, processor(s) <b>3002</b>, and I/O hub <b>3007</b> can be integrated into a SoC integrated circuit. In at least one embodiment, components of computing system <b>3000</b> can be integrated into a single package to form a system in package (&#x201c;SIP&#x201d;) configuration. In at least one embodiment, at least a portion of components of computing system <b>3000</b> can be integrated into a multi-chip module (&#x201c;MCM&#x201d;), which can be interconnected with other multi-chip modules into a modular computing system. In at least one embodiment, I/O subsystem <b>3011</b> and display devices <b>3010</b>B are omitted from computing system <b>3000</b>.</p><heading id="h-0010" level="1">Processing Systems</heading><p id="p-0344" num="0343">The following figures set forth, without limitation, exemplary processing systems that can be used to implement at least one embodiment.</p><p id="p-0345" num="0344"><figref idref="DRAWINGS">FIG. <b>31</b></figref> illustrates an accelerated processing unit (&#x201c;APU&#x201d;) <b>3100</b>, in accordance with at least one embodiment. In at least one embodiment, APU <b>3100</b> is developed by AMD Corporation of Santa Clara, Calif. In at least one embodiment, APU <b>3100</b> can be configured to execute an application program, such as a CUDA program. In at least one embodiment, APU <b>3100</b> includes, without limitation, a core complex <b>3110</b>, a graphics complex <b>3140</b>, fabric <b>3160</b>, I/O interfaces <b>3170</b>, memory controllers <b>3180</b>, a display controller <b>3192</b>, and a multimedia engine <b>3194</b>. In at least one embodiment, APU <b>3100</b> may include, without limitation, any number of core complexes <b>3110</b>, any number of graphics complexes <b>3150</b>, any number of display controllers <b>3192</b>, and any number of multimedia engines <b>3194</b> in any combination. For explanatory purposes, multiple instances of like objects are denoted herein with reference numbers identifying an object and parenthetical numbers identifying an instance where needed.</p><p id="p-0346" num="0345">In at least one embodiment, core complex <b>3110</b> is a CPU, graphics complex <b>3140</b> is a GPU, and APU <b>3100</b> is a processing unit that integrates, without limitation, <b>3110</b> and <b>3140</b> onto a single chip. In at least one embodiment, some tasks may be assigned to core complex <b>3110</b> and other tasks may be assigned to graphics complex <b>3140</b>. In at least one embodiment, core complex <b>3110</b> is configured to execute main control software associated with APU <b>3100</b>, such as an operating system. In at least one embodiment, core complex <b>3110</b> is a master processor of APU <b>3100</b>, controlling and coordinating operations of other processors. In at least one embodiment, core complex <b>3110</b> issues commands that control an operation of graphics complex <b>3140</b>. In at least one embodiment, core complex <b>3110</b> can be configured to execute host executable code derived from CUDA source code, and graphics complex <b>3140</b> can be configured to execute device executable code derived from CUDA source code.</p><p id="p-0347" num="0346">In at least one embodiment, core complex <b>3110</b> includes, without limitation, cores <b>3120</b>(<b>1</b>)-<b>3120</b>(<b>4</b>) and an L3 cache <b>3130</b>. In at least one embodiment, core complex <b>3110</b> may include, without limitation, any number of cores <b>3120</b> and any number and type of caches in any combination. In at least one embodiment, cores <b>3120</b> are configured to execute instructions of a particular instruction set architecture (&#x201c;ISA&#x201d;). In at least one embodiment, each core <b>3120</b> is a CPU core.</p><p id="p-0348" num="0347">In at least one embodiment, each core <b>3120</b> includes, without limitation, a fetch/decode unit <b>3122</b>, an integer execution engine <b>3124</b>, a floating point execution engine <b>3126</b>, and an L2 cache <b>3128</b>. In at least one embodiment, fetch/decode unit <b>3122</b> fetches instructions, decodes such instructions, generates micro-operations, and dispatches separate micro-instructions to integer execution engine <b>3124</b> and floating point execution engine <b>3126</b>. In at least one embodiment, fetch/decode unit <b>3122</b> can concurrently dispatch one micro-instruction to integer execution engine <b>3124</b> and another micro-instruction to floating point execution engine <b>3126</b>. In at least one embodiment, integer execution engine <b>3124</b> executes, without limitation, integer and memory operations. In at least one embodiment, floating point engine <b>3126</b> executes, without limitation, floating point and vector operations. In at least one embodiment, fetch-decode unit <b>3122</b> dispatches micro-instructions to a single execution engine that replaces both integer execution engine <b>3124</b> and floating point execution engine <b>3126</b>.</p><p id="p-0349" num="0348">In at least one embodiment, each core <b>3120</b>(<i>i</i>), where i is an integer representing a particular instance of core <b>3120</b>, may access L2 cache <b>3128</b>(<i>i</i>) included in core <b>3120</b>(<i>i</i>). In at least one embodiment, each core <b>3120</b> included in core complex <b>3110</b>(<i>j</i>), where j is an integer representing a particular instance of core complex <b>3110</b>, is connected to other cores <b>3120</b> included in core complex <b>3110</b>(<i>j</i>) via L3 cache <b>3130</b>(<i>j</i>) included in core complex <b>3110</b>(<i>j</i>). In at least one embodiment, cores <b>3120</b> included in core complex <b>3110</b>(<i>j</i>), where j is an integer representing a particular instance of core complex <b>3110</b>, can access all of L3 cache <b>3130</b>(<i>j</i>) included in core complex <b>3110</b>(<i>j</i>). In at least one embodiment, L3 cache <b>3130</b> may include, without limitation, any number of slices.</p><p id="p-0350" num="0349">In at least one embodiment, graphics complex <b>3140</b> can be configured to perform compute operations in a highly-parallel fashion. In at least one embodiment, graphics complex <b>3140</b> is configured to execute graphics pipeline operations such as draw commands, pixel operations, geometric computations, and other operations associated with rendering an image to a display. In at least one embodiment, graphics complex <b>3140</b> is configured to execute operations unrelated to graphics. In at least one embodiment, graphics complex <b>3140</b> is configured to execute both operations related to graphics and operations unrelated to graphics.</p><p id="p-0351" num="0350">In at least one embodiment, graphics complex <b>3140</b> includes, without limitation, any number of compute units <b>3150</b> and an L2 cache <b>3142</b>. In at least one embodiment, compute units <b>3150</b> share L2 cache <b>3142</b>. In at least one embodiment, L2 cache <b>3142</b> is partitioned. In at least one embodiment, graphics complex <b>3140</b> includes, without limitation, any number of compute units <b>3150</b> and any number (including zero) and type of caches. In at least one embodiment, graphics complex <b>3140</b> includes, without limitation, any amount of dedicated graphics hardware.</p><p id="p-0352" num="0351">In at least one embodiment, each compute unit <b>3150</b> includes, without limitation, any number of SIMD units <b>3152</b> and a shared memory <b>3154</b>. In at least one embodiment, each SIMD unit <b>3152</b> implements a SIMD architecture and is configured to perform operations in parallel. In at least one embodiment, each compute unit <b>3150</b> may execute any number of thread blocks, but each thread block executes on a single compute unit <b>3150</b>. In at least one embodiment, a thread block includes, without limitation, any number of threads of execution. In at least one embodiment, a workgroup is a thread block. In at least one embodiment, each SIMD unit <b>3152</b> executes a different warp. In at least one embodiment, a warp is a group of threads (e.g., 16 threads), where each thread in a warp belongs to a single thread block and is configured to process a different set of data based on a single set of instructions. In at least one embodiment, predication can be used to disable one or more threads in a warp. In at least one embodiment, a lane is a thread. In at least one embodiment, a work item is a thread. In at least one embodiment, a wavefront is a warp. In at least one embodiment, different wavefronts in a thread block may synchronize together and communicate via shared memory <b>3154</b>.</p><p id="p-0353" num="0352">In at least one embodiment, fabric <b>3160</b> is a system interconnect that facilitates data and control transmissions across core complex <b>3110</b>, graphics complex <b>3140</b>, I/O interfaces <b>3170</b>, memory controllers <b>3180</b>, display controller <b>3192</b>, and multimedia engine <b>3194</b>. In at least one embodiment, APU <b>3100</b> may include, without limitation, any amount and type of system interconnect in addition to or instead of fabric <b>3160</b> that facilitates data and control transmissions across any number and type of directly or indirectly linked components that may be internal or external to APU <b>3100</b>. In at least one embodiment, I/O interfaces <b>3170</b> are representative of any number and type of I/O interfaces (e.g., PCI , PCI-Extended (&#x201c;PCI-X&#x201d;), PCIe, gigabit Ethernet (&#x201c;GBE&#x201d;), USB, etc.). In at least one embodiment, various types of peripheral devices are coupled to I/O interfaces <b>3170</b> In at least one embodiment, peripheral devices that are coupled to I/O interfaces <b>3170</b> may include, without limitation, keyboards, mice, printers, scanners, joysticks or other types of game controllers, media recording devices, external storage devices, network interface cards, and so forth.</p><p id="p-0354" num="0353">In at least one embodiment, display controller AMD92 displays images on one or more display device(s), such as a liquid crystal display (&#x201c;LCD&#x201d;) device. In at least one embodiment, multimedia engine <b>240</b> includes, without limitation, any amount and type of circuitry that is related to multimedia, such as a video decoder, a video encoder, an image signal processor, etc. In at least one embodiment, memory controllers <b>3180</b> facilitate data transfers between APU <b>3100</b> and a unified system memory <b>3190</b>. In at least one embodiment, core complex <b>3110</b> and graphics complex <b>3140</b> share unified system memory <b>3190</b>.</p><p id="p-0355" num="0354">In at least one embodiment, APU <b>3100</b> implements a memory subsystem that includes, without limitation, any amount and type of memory controllers <b>3180</b> and memory devices (e.g., shared memory <b>3154</b>) that may be dedicated to one component or shared among multiple components. In at least one embodiment, APU <b>3100</b> implements a cache subsystem that includes, without limitation, one or more cache memories (e.g., L2 caches <b>2728</b>, L3 cache <b>3130</b>, and L2 cache <b>3142</b>) that may each be private to or shared between any number of components (e.g., cores <b>3120</b>, core complex <b>3110</b>, SIMD units <b>3152</b>, compute units <b>3150</b>, and graphics complex <b>3140</b>).</p><p id="p-0356" num="0355"><figref idref="DRAWINGS">FIG. <b>32</b></figref> illustrates a CPU <b>3200</b>, in accordance with at least one embodiment. In at least one embodiment, CPU <b>3200</b> is developed by AMD Corporation of Santa Clara, Calif. In at least one embodiment, CPU <b>3200</b> can be configured to execute an application program. In at least one embodiment, CPU <b>3200</b> is configured to execute main control software, such as an operating system. In at least one embodiment, CPU <b>3200</b> issues commands that control an operation of an external GPU (not shown). In at least one embodiment, CPU <b>3200</b> can be configured to execute host executable code derived from CUDA source code, and an external GPU can be configured to execute device executable code derived from such CUDA source code. In at least one embodiment, CPU <b>3200</b> includes, without limitation, any number of core complexes <b>3210</b>, fabric <b>3260</b>, I/O interfaces <b>3270</b>, and memory controllers <b>3280</b>.</p><p id="p-0357" num="0356">In at least one embodiment, core complex <b>3210</b> includes, without limitation, cores <b>3220</b>(<b>1</b>)-<b>3220</b>(<b>4</b>) and an L3 cache <b>3230</b>. In at least one embodiment, core complex <b>3210</b> may include, without limitation, any number of cores <b>3220</b> and any number and type of caches in any combination. In at least one embodiment, cores <b>3220</b> are configured to execute instructions of a particular ISA. In at least one embodiment, each core <b>3220</b> is a CPU core.</p><p id="p-0358" num="0357">In at least one embodiment, each core <b>3220</b> includes, without limitation, a fetch/decode unit <b>3222</b>, an integer execution engine <b>3224</b>, a floating point execution engine <b>3226</b>, and an L2 cache <b>3228</b>. In at least one embodiment, fetch/decode unit <b>3222</b> fetches instructions, decodes such instructions, generates micro-operations, and dispatches separate micro-instructions to integer execution engine <b>3224</b> and floating point execution engine <b>3226</b>. In at least one embodiment, fetch/decode unit <b>3222</b> can concurrently dispatch one micro-instruction to integer execution engine <b>3224</b> and another micro-instruction to floating point execution engine <b>3226</b>. In at least one embodiment, integer execution engine <b>3224</b> executes, without limitation, integer and memory operations. In at least one embodiment, floating point engine <b>3226</b> executes, without limitation, floating point and vector operations. In at least one embodiment, fetch-decode unit <b>3222</b> dispatches micro-instructions to a single execution engine that replaces both integer execution engine <b>3224</b> and floating point execution engine <b>3226</b>.</p><p id="p-0359" num="0358">In at least one embodiment, each core <b>3220</b>(<i>i</i>), where i is an integer representing a particular instance of core <b>3220</b>, may access L2 cache <b>3228</b>(<i>i</i>) included in core <b>3220</b>(<i>i</i>). In at least one embodiment, each core <b>3220</b> included in core complex <b>3210</b>(<i>j</i>), where j is an integer representing a particular instance of core complex <b>3210</b>, is connected to other cores <b>3220</b> in core complex <b>3210</b>(<i>j</i>) via L3 cache <b>3230</b>(<i>j</i>) included in core complex <b>3210</b>(<i>j</i>). In at least one embodiment, cores <b>3220</b> included in core complex <b>3210</b>(<i>j</i>), where j is an integer representing a particular instance of core complex <b>3210</b>, can access all of L3 cache <b>3230</b>(<i>j</i>) included in core complex <b>3210</b>(<i>j</i>). In at least one embodiment, L3 cache <b>3230</b> may include, without limitation, any number of slices.</p><p id="p-0360" num="0359">In at least one embodiment, fabric <b>3260</b> is a system interconnect that facilitates data and control transmissions across core complexes <b>3210</b>(<b>1</b>)-<b>3210</b>(N) (where N is an integer greater than zero), I/O interfaces <b>3270</b>, and memory controllers <b>3280</b>. In at least one embodiment, CPU <b>3200</b> may include, without limitation, any amount and type of system interconnect in addition to or instead of fabric <b>3260</b> that facilitates data and control transmissions across any number and type of directly or indirectly linked components that may be internal or external to CPU <b>3200</b>. In at least one embodiment, I/O interfaces <b>3270</b> are representative of any number and type of I/O interfaces (e.g., PCI , PCI-X, PCIe, GBE, USB, etc.). In at least one embodiment, various types of peripheral devices are coupled to I/O interfaces <b>3270</b> In at least one embodiment, peripheral devices that are coupled to I/O interfaces <b>3270</b> may include, without limitation, displays, keyboards, mice, printers, scanners, joysticks or other types of game controllers, media recording devices, external storage devices, network interface cards, and so forth.</p><p id="p-0361" num="0360">In at least one embodiment, memory controllers <b>3280</b> facilitate data transfers between CPU <b>3200</b> and a system memory <b>3290</b>. In at least one embodiment, core complex <b>3210</b> and graphics complex <b>3240</b> share system memory <b>3290</b>. In at least one embodiment, CPU <b>3200</b> implements a memory subsystem that includes, without limitation, any amount and type of memory controllers <b>3280</b> and memory devices that may be dedicated to one component or shared among multiple components. In at least one embodiment, CPU <b>3200</b> implements a cache subsystem that includes, without limitation, one or more cache memories (e.g., L2 caches <b>3228</b> and L3 caches <b>3230</b>) that may each be private to or shared between any number of components (e.g., cores <b>3220</b> and core complexes <b>3210</b>).</p><p id="p-0362" num="0361"><figref idref="DRAWINGS">FIG. <b>33</b></figref> illustrates an exemplary accelerator integration slice <b>3390</b>, in accordance with at least one embodiment. As used herein, a &#x201c;slice&#x201d; comprises a specified portion of processing resources of an accelerator integration circuit. In at least one embodiment, an accelerator integration circuit provides cache management, memory access, context management, and interrupt management services on behalf of multiple graphics processing engines included in a graphics acceleration module. Graphics processing engines may each comprise a separate GPU. Alternatively, graphics processing engines may comprise different types of graphics processing engines within a GPU such as graphics execution units, media processing engines (e.g., video encoders/decoders), samplers, and blit engines. In at least one embodiment, a graphics acceleration module may be a GPU with multiple graphics processing engines. In at least one embodiment, graphics processing engines may be individual GPUs integrated on a common package, line card, or chip.</p><p id="p-0363" num="0362">An application effective address space <b>3382</b> within system memory <b>3314</b> stores process elements <b>3383</b>. In one embodiment, process elements <b>3383</b> are stored in response to GPU invocations <b>3381</b> from applications <b>3380</b> executed on processor <b>3307</b>. A process element <b>3383</b> contains process state for corresponding application <b>3380</b>. A work descriptor (&#x201c;WD&#x201d;) <b>3384</b> contained in process element <b>3383</b> can be a single job requested by an application or may contain a pointer to a queue of jobs. In at least one embodiment, WD <b>3384</b> is a pointer to a job request queue in application effective address space <b>3382</b>.</p><p id="p-0364" num="0363">Graphics acceleration module <b>3346</b> and/or individual graphics processing engines can be shared by all or a subset of processes in a system. In at least one embodiment, an infrastructure for setting up process state and sending WD <b>3384</b> to graphics acceleration module <b>3346</b> to start a job in a virtualized environment may be included.</p><p id="p-0365" num="0364">In at least one embodiment, a dedicated-process programming model is implementation-specific. In this model, a single process owns graphics acceleration module <b>3346</b> or an individual graphics processing engine. Because graphics acceleration module <b>3346</b> is owned by a single process, a hypervisor initializes an accelerator integration circuit for an owning partition and an operating system initializes accelerator integration circuit for an owning process when graphics acceleration module <b>3346</b> is assigned.</p><p id="p-0366" num="0365">In operation, a WD fetch unit <b>3391</b> in accelerator integration slice <b>3390</b> fetches next WD <b>3384</b> which includes an indication of work to be done by one or more graphics processing engines of graphics acceleration module <b>3346</b>. Data from WD <b>3384</b> may be stored in registers <b>3345</b> and used by a memory management unit (&#x201c;MMU&#x201d;) <b>3339</b>, interrupt management circuit <b>3347</b> and/or context management circuit <b>3348</b> as illustrated. In at least one embodiment of MMU <b>3339</b> includes segment/page walk circuitry for accessing segment/page tables <b>3386</b> within OS virtual address space <b>3385</b>. Interrupt management circuit <b>3347</b> may process interrupt events (&#x201c;INT&#x201d;) <b>3392</b> received from graphics acceleration module <b>3346</b>. When performing graphics operations, an effective address <b>3393</b> generated by a graphics processing engine is translated to a real address by MMU <b>3339</b>.</p><p id="p-0367" num="0366">In one embodiment, a same set of registers <b>3345</b> are duplicated for each graphics processing engine and/or graphics acceleration module <b>3346</b> and may be initialized by a hypervisor or operating system. Each of these duplicated registers may be included in accelerator integration slice <b>3390</b>. Exemplary registers that may be initialized by a hypervisor are shown in Table 1.</p><p id="p-0368" num="0000"><tables id="TABLE-US-00001" num="00001"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="center"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE 1</entry></row><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>Hypervisor Initialized Registers</entry></row><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="14pt" align="center"/><colspec colname="2" colwidth="203pt" align="left"/><tbody valign="top"><row><entry>1</entry><entry>Slice Control Register</entry></row><row><entry>2</entry><entry>Real Address (RA) Scheduled Processes Area Pointer</entry></row><row><entry>3</entry><entry>Authority Mask Override Register</entry></row><row><entry>4</entry><entry>Interrupt Vector Table Entry Offset</entry></row><row><entry>5</entry><entry>Interrupt Vector Table Entry Limit</entry></row><row><entry>6</entry><entry>State Register</entry></row><row><entry>7</entry><entry>Logical Partition ID</entry></row><row><entry>8</entry><entry>Real address (RA) Hypervisor Accelerator Utilization Record Pointer</entry></row><row><entry>9</entry><entry>Storage Description Register</entry></row><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0369" num="0367">Exemplary registers that may be initialized by an operating system are shown in Table 2.</p><p id="p-0370" num="0000"><tables id="TABLE-US-00002" num="00002"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="center"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE 2</entry></row><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>Operating System Initialized Registers</entry></row><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="28pt" align="center"/><colspec colname="2" colwidth="189pt" align="left"/><tbody valign="top"><row><entry>1</entry><entry>Process and Thread Identification</entry></row><row><entry>2</entry><entry>Effective Address (EA) Context Save/Restore Pointer</entry></row><row><entry>3</entry><entry>Virtual Address (VA) Accelerator Utilization Record Pointer</entry></row><row><entry>4</entry><entry>Virtual Address (VA) Storage Segment Table Pointer</entry></row><row><entry>5</entry><entry>Authority Mask</entry></row><row><entry>6</entry><entry>Work descriptor</entry></row><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0371" num="0368">In one embodiment, each WD <b>3384</b> is specific to a particular graphics acceleration module <b>3346</b> and/or a particular graphics processing engine. It contains all information required by a graphics processing engine to do work or it can be a pointer to a memory location where an application has set up a command queue of work to be completed.</p><p id="p-0372" num="0369"><figref idref="DRAWINGS">FIGS. <b>34</b>A-<b>34</b>B</figref> illustrate exemplary graphics processors, in accordance with at least one embodiment. In at least one embodiment, any of the exemplary graphics processors may be fabricated using one or more IP cores. In addition to what is illustrated, other logic and circuits may be included in at least one embodiment, including additional graphics processors/cores, peripheral interface controllers, or general-purpose processor cores. In at least one embodiment, the exemplary graphics processors are for use within an SoC.</p><p id="p-0373" num="0370"><figref idref="DRAWINGS">FIG. <b>34</b>A</figref> illustrates an exemplary graphics processor <b>3410</b> of an SoC integrated circuit that may be fabricated using one or more IP cores, in accordance with at least one embodiment. <figref idref="DRAWINGS">FIG. <b>34</b>B</figref> illustrates an additional exemplary graphics processor <b>3440</b> of an SoC integrated circuit that may be fabricated using one or more IP cores, in accordance with at least one embodiment. In at least one embodiment, graphics processor <b>3410</b> of <figref idref="DRAWINGS">FIG. <b>34</b>A</figref> is a low power graphics processor core. In at least one embodiment, graphics processor <b>3440</b> of <figref idref="DRAWINGS">FIG. <b>34</b>B</figref> is a higher performance graphics processor core. In at least one embodiment, each of graphics processors <b>3410</b>, <b>3440</b> can be variants of graphics processor <b>510</b> of <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0374" num="0371">In at least one embodiment, graphics processor <b>3410</b> includes a vertex processor <b>3405</b> and one or more fragment processor(s) <b>3415</b>A-<b>3415</b>N (e.g., <b>3415</b>A, <b>3415</b>B, <b>3415</b>C, <b>3415</b>D, through <b>3415</b>N-<b>1</b>, and <b>3415</b>N). In at least one embodiment, graphics processor <b>3410</b> can execute different shader programs via separate logic, such that vertex processor <b>3405</b> is optimized to execute operations for vertex shader programs, while one or more fragment processor(s) <b>3415</b>A-<b>3415</b>N execute fragment (e.g., pixel) shading operations for fragment or pixel shader programs. In at least one embodiment, vertex processor <b>3405</b> performs a vertex processing stage of a 3D graphics pipeline and generates primitives and vertex data. In at least one embodiment, fragment processor(s) <b>3415</b>A-<b>3415</b>N use primitive and vertex data generated by vertex processor <b>3405</b> to produce a framebuffer that is displayed on a display device. In at least one embodiment, fragment processor(s) <b>3415</b>A-<b>3415</b>N are optimized to execute fragment shader programs as provided for in an OpenGL API, which may be used to perform similar operations as a pixel shader program as provided for in a Direct 3D API.</p><p id="p-0375" num="0372">In at least one embodiment, graphics processor <b>3410</b> additionally includes one or more MMU(s) <b>3420</b>A-<b>3420</b>B, cache(s) <b>3425</b>A-<b>3425</b>B, and circuit interconnect(s) <b>3430</b>A-<b>3430</b>B. In at least one embodiment, one or more MMU(s) <b>3420</b>A-<b>3420</b>B provide for virtual to physical address mapping for graphics processor <b>3410</b>, including for vertex processor <b>3405</b> and/or fragment processor(s) <b>3415</b>A-<b>3415</b>N, which may reference vertex or image/texture data stored in memory, in addition to vertex or image/texture data stored in one or more cache(s) <b>3425</b>A-<b>3425</b>B. In at least one embodiment, one or more MMU(s) <b>3420</b>A-<b>3420</b>B may be synchronized with other MMUs within a system, including one or more MMUs associated with one or more application processor(s) <b>505</b>, image processors <b>515</b>, and/or video processors <b>520</b> of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, such that each processor <b>505</b>-<b>520</b> can participate in a shared or unified virtual memory system. In at least one embodiment, one or more circuit interconnect(s) <b>3430</b>A-<b>3430</b>B enable graphics processor <b>3410</b> to interface with other IP cores within an SoC, either via an internal bus of an SoC or via a direct connection.</p><p id="p-0376" num="0373">In at least one embodiment, graphics processor <b>3440</b> includes one or more MMU(s) <b>3420</b>A-<b>3420</b>B, caches <b>3425</b>A-<b>3425</b>B, and circuit interconnects <b>3430</b>A-<b>3430</b>B of graphics processor <b>3410</b> of <figref idref="DRAWINGS">FIG. <b>34</b>A</figref>. In at least one embodiment, graphics processor <b>3440</b> includes one or more shader core(s) <b>3455</b>A-<b>3455</b>N (e.g., <b>3455</b>A, <b>3455</b>B, <b>3455</b>C, <b>3455</b>D, <b>3455</b>E, <b>3455</b>F, through <b>3455</b>N-<b>1</b>, and <b>3455</b>N), which provides for a unified shader core architecture in which a single core or type or core can execute all types of programmable shader code, including shader program code to implement vertex shaders, fragment shaders, and/or compute shaders. In at least one embodiment, a number of shader cores can vary. In at least one embodiment, graphics processor <b>3440</b> includes an inter-core task manager <b>3445</b>, which acts as a thread dispatcher to dispatch execution threads to one or more shader cores <b>3455</b>A-<b>3455</b>N and a tiling unit <b>3458</b> to accelerate tiling operations for tile-based rendering, in which rendering operations for a scene are subdivided in image space, in at least one embodiment to exploit local spatial coherence within a scene or to optimize use of internal caches.</p><p id="p-0377" num="0374"><figref idref="DRAWINGS">FIG. <b>35</b>A</figref> illustrates a graphics core <b>3500</b>, in accordance with at least one embodiment. In at least one embodiment, graphics core <b>3500</b> may be included within graphics processor <b>2410</b> of <figref idref="DRAWINGS">FIG. <b>24</b></figref>. In at least one embodiment, graphics core <b>3500</b> may be a unified shader core <b>3455</b>A-<b>3455</b>N as in <figref idref="DRAWINGS">FIG. <b>34</b>B</figref>. In at least one embodiment, graphics core <b>3500</b> includes a shared instruction cache <b>3502</b>, a texture unit <b>3518</b>, and a cache/shared memory <b>3520</b> that are common to execution resources within graphics core <b>3500</b>. In at least one embodiment, graphics core <b>3500</b> can include multiple slices <b>3501</b>A-<b>3501</b>N or partition for each core, and a graphics processor can include multiple instances of graphics core <b>3500</b>. Slices <b>3501</b>A-<b>3501</b>N can include support logic including a local instruction cache <b>3504</b>A-<b>3504</b>N, a thread scheduler <b>3506</b>A-<b>3506</b>N, a thread dispatcher <b>3508</b>A-<b>3508</b>N, and a set of registers <b>3510</b>A-<b>3510</b>N. In at least one embodiment, slices <b>3501</b>A-<b>3501</b>N can include a set of additional function units (&#x201c;AFUs&#x201d;) <b>3512</b>A-<b>3512</b>N, floating-point units (&#x201c;FPUs&#x201d;) <b>3514</b>A-<b>3514</b>N, integer arithmetic logic units (&#x201c;ALUs&#x201d;) <b>3516</b>-<b>3516</b>N, address computational units (&#x201c;ACUs&#x201d;) <b>3513</b>A-<b>3513</b>N, double-precision floating-point units (&#x201c;DPFPUs&#x201d;) <b>3515</b>A-<b>3515</b>N, and matrix processing units (&#x201c;MPUs&#x201d;) <b>3517</b>A-<b>3517</b>N.</p><p id="p-0378" num="0375">In at least one embodiment, FPUs <b>3514</b>A-<b>3514</b>N can perform single-precision (32-bit) and half-precision (16-bit) floating point operations, while DPFPUs <b>3515</b>A-<b>3515</b>N perform double precision (64-bit) floating point operations. In at least one embodiment, ALUs <b>3516</b>A-<b>3516</b>N can perform variable precision integer operations at 8-bit, 16-bit, and 32-bit precision, and can be configured for mixed precision operations. In at least one embodiment, MPUs <b>3517</b>A-<b>3517</b>N can also be configured for mixed precision matrix operations, including half-precision floating point and 8-bit integer operations. In at least one embodiment, MPUs <b>3517</b>-<b>3517</b>N can perform a variety of matrix operations to accelerate CUDA programs, including enabling support for accelerated general matrix to matrix multiplication (&#x201c;GEMM&#x201d;). In at least one embodiment, AFUs <b>3512</b>A-<b>3512</b>N can perform additional logic operations not supported by floating-point or integer units, including trigonometric operations (e.g., Sine, Cosine, etc.).</p><p id="p-0379" num="0376"><figref idref="DRAWINGS">FIG. <b>35</b>B</figref> illustrates a general-purpose graphics processing unit (&#x201c;GPGPU&#x201d;) <b>3530</b>, in accordance with at least one embodiment. In at least one embodiment, GPGPU <b>3530</b> is highly-parallel and suitable for deployment on a multi-chip module. In at least one embodiment, GPGPU <b>3530</b> can be configured to enable highly-parallel compute operations to be performed by an array of GPUs. In at least one embodiment, GPGPU <b>3530</b> can be linked directly to other instances of GPGPU <b>3530</b> to create a multi-GPU cluster to improve execution time for CUDA programs. In at least one embodiment, GPGPU <b>3530</b> includes a host interface <b>3532</b> to enable a connection with a host processor. In at least one embodiment, host interface <b>3532</b> is a PCIe interface. In at least one embodiment, host interface <b>3532</b> can be a vendor specific communications interface or communications fabric. In at least one embodiment, GPGPU <b>3530</b> receives commands from a host processor and uses a global scheduler <b>3534</b> to distribute execution threads associated with those commands to a set of compute clusters <b>3536</b>A-<b>3536</b>H. In at least one embodiment, compute clusters <b>3536</b>A-<b>3536</b>H share a cache memory <b>3538</b>. In at least one embodiment, cache memory <b>3538</b> can serve as a higher-level cache for cache memories within compute clusters <b>3536</b>A-<b>3536</b>H.</p><p id="p-0380" num="0377">In at least one embodiment, GPGPU <b>3530</b> includes memory <b>3544</b>A-<b>3544</b>B coupled with compute clusters <b>3536</b>A-<b>3536</b>H via a set of memory controllers <b>3542</b>A-<b>3542</b>B. In at least one embodiment, memory <b>3544</b>A-<b>3544</b>B can include various types of memory devices including DRAM or graphics random access memory, such as synchronous graphics random access memory (&#x201c;SGRAM&#x201d;), including graphics double data rate (&#x201c;GDDR&#x201d;) memory.</p><p id="p-0381" num="0378">In at least one embodiment, compute clusters <b>3536</b>A-<b>3536</b>H each include a set of graphics cores, such as graphics core <b>3500</b> of <figref idref="DRAWINGS">FIG. <b>35</b>A</figref>, which can include multiple types of integer and floating point logic units that can perform computational operations at a range of precisions including suited for computations associated with CUDA programs. In at least one embodiment, at least a subset of floating point units in each of compute clusters <b>3536</b>A-<b>3536</b>H can be configured to perform 16-bit or 32-bit floating point operations, while a different subset of floating point units can be configured to perform 64-bit floating point operations.</p><p id="p-0382" num="0379">In at least one embodiment, multiple instances of GPGPU <b>3530</b> can be configured to operate as a compute cluster. In at least one embodiment, compute clusters <b>3536</b>A-<b>3536</b>H may implement any technically feasible communication techniques for synchronization and data exchange. In at least one embodiment, multiple instances of GPGPU <b>3530</b> communicate over host interface <b>3532</b>. In at least one embodiment, GPGPU <b>3530</b> includes an I/O hub <b>3539</b> that couples GPGPU <b>3530</b> with a GPU link <b>3540</b> that enables a direct connection to other instances of GPGPU <b>3530</b>. In at least one embodiment, GPU link <b>3540</b> is coupled to a dedicated GPU-to-GPU bridge that enables communication and synchronization between multiple instances of GPGPU <b>3530</b>. In at least one embodiment GPU link <b>3540</b> couples with a high speed interconnect to transmit and receive data to other GPGPUs <b>3530</b> or parallel processors. In at least one embodiment, multiple instances of GPGPU <b>3530</b> are located in separate data processing systems and communicate via a network device that is accessible via host interface <b>3532</b>. In at least one embodiment GPU link <b>3540</b> can be configured to enable a connection to a host processor in addition to or as an alternative to host interface <b>3532</b>. In at least one embodiment, GPGPU <b>3530</b> can be configured to execute a CUDA program.</p><p id="p-0383" num="0380"><figref idref="DRAWINGS">FIG. <b>36</b>A</figref> illustrates a parallel processor <b>3600</b>, in accordance with at least one embodiment. In at least one embodiment, various components of parallel processor <b>3600</b> may be implemented using one or more integrated circuit devices, such as programmable processors, application specific integrated circuits (&#x201c;ASICs&#x201d;), or FPGAs.</p><p id="p-0384" num="0381">In at least one embodiment, parallel processor <b>3600</b> includes a parallel processing unit <b>3602</b>. In at least one embodiment, parallel processing unit <b>3602</b> includes an I/O unit <b>3604</b> that enables communication with other devices, including other instances of parallel processing unit <b>3602</b>. In at least one embodiment, I/O unit <b>3604</b> may be directly connected to other devices. In at least one embodiment, I/O unit <b>3604</b> connects with other devices via use of a hub or switch interface, such as memory hub <b>605</b>. In at least one embodiment, connections between memory hub <b>605</b> and I/O unit <b>3604</b> form a communication link. In at least one embodiment, I/O unit <b>3604</b> connects with a host interface <b>3606</b> and a memory crossbar <b>3616</b>, where host interface <b>3606</b> receives commands directed to performing processing operations and memory crossbar <b>3616</b> receives commands directed to performing memory operations.</p><p id="p-0385" num="0382">In at least one embodiment, when host interface <b>3606</b> receives a command buffer via I/O unit <b>3604</b>, host interface <b>3606</b> can direct work operations to perform those commands to a front end <b>3608</b>. In at least one embodiment, front end <b>3608</b> couples with a scheduler <b>3610</b>, which is configured to distribute commands or other work items to a processing array <b>3612</b>. In at least one embodiment, scheduler <b>3610</b> ensures that processing array <b>3612</b> is properly configured and in a valid state before tasks are distributed to processing array <b>3612</b>. In at least one embodiment, scheduler <b>3610</b> is implemented via firmware logic executing on a microcontroller. In at least one embodiment, microcontroller implemented scheduler <b>3610</b> is configurable to perform complex scheduling and work distribution operations at coarse and fine granularity, enabling rapid preemption and context switching of threads executing on processing array <b>3612</b>. In at least one embodiment, host software can prove workloads for scheduling on processing array <b>3612</b> via one of multiple graphics processing doorbells. In at least one embodiment, workloads can then be automatically distributed across processing array <b>3612</b> by scheduler <b>3610</b> logic within a microcontroller including scheduler <b>3610</b>.</p><p id="p-0386" num="0383">In at least one embodiment, processing array <b>3612</b> can include up to &#x201c;N&#x201d; clusters (e.g., cluster <b>3614</b>A, cluster <b>3614</b>B, through cluster <b>3614</b>N). In at least one embodiment, each cluster <b>3614</b>A-<b>3614</b>N of processing array <b>3612</b> can execute a large number of concurrent threads. In at least one embodiment, scheduler <b>3610</b> can allocate work to clusters <b>3614</b>A-<b>3614</b>N of processing array <b>3612</b> using various scheduling and/or work distribution algorithms, which may vary depending on a workload arising for each type of program or computation. In at least one embodiment, scheduling can be handled dynamically by scheduler <b>3610</b>, or can be assisted in part by compiler logic during compilation of program logic configured for execution by processing array <b>3612</b>. In at least one embodiment, different clusters <b>3614</b>A-<b>3614</b>N of processing array <b>3612</b> can be allocated for processing different types of programs or for performing different types of computations.</p><p id="p-0387" num="0384">In at least one embodiment, processing array <b>3612</b> can be configured to perform various types of parallel processing operations. In at least one embodiment, processing array <b>3612</b> is configured to perform general-purpose parallel compute operations. In at least one embodiment, processing array <b>3612</b> can include logic to execute processing tasks including filtering of video and/or audio data, performing modeling operations, including physics operations, and performing data transformations.</p><p id="p-0388" num="0385">In at least one embodiment, processing array <b>3612</b> is configured to perform parallel graphics processing operations. In at least one embodiment, processing array <b>3612</b> can include additional logic to support execution of such graphics processing operations, including, but not limited to texture sampling logic to perform texture operations, as well as tessellation logic and other vertex processing logic. In at least one embodiment, processing array <b>3612</b> can be configured to execute graphics processing related shader programs such as, but not limited to vertex shaders, tessellation shaders, geometry shaders, and pixel shaders. In at least one embodiment, parallel processing unit <b>3602</b> can transfer data from system memory via I/O unit <b>3604</b> for processing. In at least one embodiment, during processing, transferred data can be stored to on-chip memory (e.g., a parallel processor memory <b>3622</b>) during processing, then written back to system memory.</p><p id="p-0389" num="0386">In at least one embodiment, when parallel processing unit <b>3602</b> is used to perform graphics processing, scheduler <b>3610</b> can be configured to divide a processing workload into approximately equal sized tasks, to better enable distribution of graphics processing operations to multiple clusters <b>3614</b>A-<b>3614</b>N of processing array <b>3612</b>. In at least one embodiment, portions of processing array <b>3612</b> can be configured to perform different types of processing. In at least one embodiment, a first portion may be configured to perform vertex shading and topology generation, a second portion may be configured to perform tessellation and geometry shading, and a third portion may be configured to perform pixel shading or other screen space operations, to produce a rendered image for display. In at least one embodiment, intermediate data produced by one or more of clusters <b>3614</b>A-<b>3614</b>N may be stored in buffers to allow intermediate data to be transmitted between clusters <b>3614</b>A-<b>3614</b>N for further processing.</p><p id="p-0390" num="0387">In at least one embodiment, processing array <b>3612</b> can receive processing tasks to be executed via scheduler <b>3610</b>, which receives commands defining processing tasks from front end <b>3608</b>. In at least one embodiment, processing tasks can include indices of data to be processed, e.g., surface (patch) data, primitive data, vertex data, and/or pixel data, as well as state parameters and commands defining how data is to be processed (e.g., what program is to be executed). In at least one embodiment, scheduler <b>3610</b> may be configured to fetch indices corresponding to tasks or may receive indices from front end <b>3608</b>. In at least one embodiment, front end <b>3608</b> can be configured to ensure processing array <b>3612</b> is configured to a valid state before a workload specified by incoming command buffers (e.g., batch-buffers, push buffers, etc.) is initiated.</p><p id="p-0391" num="0388">In at least one embodiment, each of one or more instances of parallel processing unit <b>3602</b> can couple with parallel processor memory <b>3622</b>. In at least one embodiment, parallel processor memory <b>3622</b> can be accessed via memory crossbar <b>3616</b>, which can receive memory requests from processing array <b>3612</b> as well as I/O unit <b>3604</b>. In at least one embodiment, memory crossbar <b>3616</b> can access parallel processor memory <b>3622</b> via a memory interface <b>3618</b>. In at least one embodiment, memory interface <b>3618</b> can include multiple partition units (e.g., a partition unit <b>3620</b>A, partition unit <b>3620</b>B, through partition unit <b>3620</b>N) that can each couple to a portion (e.g., memory unit) of parallel processor memory <b>3622</b>. In at least one embodiment, a number of partition units <b>3620</b>A-<b>3620</b>N is configured to be equal to a number of memory units, such that a first partition unit <b>3620</b>A has a corresponding first memory unit <b>3624</b>A, a second partition unit <b>3620</b>B has a corresponding memory unit <b>3624</b>B, and an Nth partition unit <b>3620</b>N has a corresponding Nth memory unit <b>3624</b>N. In at least one embodiment, a number of partition units <b>3620</b>A-<b>3620</b>N may not be equal to a number of memory devices.</p><p id="p-0392" num="0389">In at least one embodiment, memory units <b>3624</b>A-<b>3624</b>N can include various types of memory devices, including DRAM or graphics random access memory, such as SGRAM, including GDDR memory. In at least one embodiment, memory units <b>3624</b>A-<b>3624</b>N may also include 3D stacked memory, including but not limited to high bandwidth memory (&#x201c;HBM&#x201d;). In at least one embodiment, render targets, such as frame buffers or texture maps may be stored across memory units <b>3624</b>A-<b>3624</b>N, allowing partition units <b>3620</b>A-<b>3620</b>N to write portions of each render target in parallel to efficiently use available bandwidth of parallel processor memory <b>3622</b>. In at least one embodiment, a local instance of parallel processor memory <b>3622</b> may be excluded in favor of a unified memory design that utilizes system memory in conjunction with local cache memory.</p><p id="p-0393" num="0390">In at least one embodiment, any one of clusters <b>3614</b>A-<b>3614</b>N of processing array <b>3612</b> can process data that will be written to any of memory units <b>3624</b>A-<b>3624</b>N within parallel processor memory <b>3622</b>. In at least one embodiment, memory crossbar <b>3616</b> can be configured to transfer an output of each cluster <b>3614</b>A-<b>3614</b>N to any partition unit <b>3620</b>A-<b>3620</b>N or to another cluster <b>3614</b>A-<b>3614</b>N, which can perform additional processing operations on an output. In at least one embodiment, each cluster <b>3614</b>A-<b>3614</b>N can communicate with memory interface <b>3618</b> through memory crossbar <b>3616</b> to read from or write to various external memory devices. In at least one embodiment, memory crossbar <b>3616</b> has a connection to memory interface <b>3618</b> to communicate with I/O unit <b>3604</b>, as well as a connection to a local instance of parallel processor memory <b>3622</b>, enabling processing units within different clusters <b>3614</b>A-<b>3614</b>N to communicate with system memory or other memory that is not local to parallel processing unit <b>3602</b>. In at least one embodiment, memory crossbar <b>3616</b> can use virtual channels to separate traffic streams between clusters <b>3614</b>A-<b>3614</b>N and partition units <b>3620</b>A-<b>3620</b>N.</p><p id="p-0394" num="0391">In at least one embodiment, multiple instances of parallel processing unit <b>3602</b> can be provided on a single add-in card, or multiple add-in cards can be interconnected. In at least one embodiment, different instances of parallel processing unit <b>3602</b> can be configured to inter-operate even if different instances have different numbers of processing cores, different amounts of local parallel processor memory, and/or other configuration differences. In at least one embodiment, some instances of parallel processing unit <b>3602</b> can include higher precision floating point units relative to other instances. In at least one embodiment, systems incorporating one or more instances of parallel processing unit <b>3602</b> or parallel processor <b>3600</b> can be implemented in a variety of configurations and form factors, including but not limited to desktop, laptop, or handheld personal computers, servers, workstations, game consoles, and/or embedded systems.</p><p id="p-0395" num="0392"><figref idref="DRAWINGS">FIG. <b>36</b>B</figref> illustrates a processing cluster <b>3694</b>, in accordance with at least one embodiment. In at least one embodiment, processing cluster <b>3694</b> is included within a parallel processing unit. In at least one embodiment, processing cluster <b>3694</b> is one of processing clusters <b>3614</b>A-<b>3614</b>N of <figref idref="DRAWINGS">FIG. <b>36</b></figref>. In at least one embodiment, processing cluster <b>3694</b> can be configured to execute many threads in parallel, where the term &#x201c;thread&#x201d; refers to an instance of a particular program executing on a particular set of input data. In at least one embodiment, single instruction, multiple data (&#x201c;SIMD&#x201d;) instruction issue techniques are used to support parallel execution of a large number of threads without providing multiple independent instruction units. In at least one embodiment, single instruction, multiple thread (&#x201c;SIMT&#x201d;) techniques are used to support parallel execution of a large number of generally synchronized threads, using a common instruction unit configured to issue instructions to a set of processing engines within each processing cluster <b>3694</b>.</p><p id="p-0396" num="0393">In at least one embodiment, operation of processing cluster <b>3694</b> can be controlled via a pipeline manager <b>3632</b> that distributes processing tasks to SIMT parallel processors. In at least one embodiment, pipeline manager <b>3632</b> receives instructions from scheduler <b>3610</b> of <figref idref="DRAWINGS">FIG. <b>36</b></figref> and manages execution of those instructions via a graphics multiprocessor <b>3634</b> and/or a texture unit <b>3636</b>. In at least one embodiment, graphics multiprocessor <b>3634</b> is an exemplary instance of a SIMT parallel processor. However, in at least one embodiment, various types of SIMT parallel processors of differing architectures may be included within processing cluster <b>3694</b>. In at least one embodiment, one or more instances of graphics multiprocessor <b>3634</b> can be included within processing cluster <b>3694</b>. In at least one embodiment, graphics multiprocessor <b>3634</b> can process data and a data crossbar <b>3640</b> can be used to distribute processed data to one of multiple possible destinations, including other shader units. In at least one embodiment, pipeline manager <b>3632</b> can facilitate distribution of processed data by specifying destinations for processed data to be distributed via data crossbar <b>3640</b>.</p><p id="p-0397" num="0394">In at least one embodiment, each graphics multiprocessor <b>3634</b> within processing cluster <b>3694</b> can include an identical set of functional execution logic (e.g., arithmetic logic units, load/store units (&#x201c;LSUs&#x201d;), etc.). In at least one embodiment, functional execution logic can be configured in a pipelined manner in which new instructions can be issued before previous instructions are complete. In at least one embodiment, functional execution logic supports a variety of operations including integer and floating point arithmetic, comparison operations, Boolean operations, bit-shifting, and computation of various algebraic functions. In at least one embodiment, same functional-unit hardware can be leveraged to perform different operations and any combination of functional units may be present.</p><p id="p-0398" num="0395">In at least one embodiment, instructions transmitted to processing cluster <b>3694</b> constitute a thread. In at least one embodiment, a set of threads executing across a set of parallel processing engines is a thread group. In at least one embodiment, a thread group executes a program on different input data. In at least one embodiment, each thread within a thread group can be assigned to a different processing engine within graphics multiprocessor <b>3634</b>. In at least one embodiment, a thread group may include fewer threads than a number of processing engines within graphics multiprocessor <b>3634</b>. In at least one embodiment, when a thread group includes fewer threads than a number of processing engines, one or more of processing engines may be idle during cycles in which that thread group is being processed. In at least one embodiment, a thread group may also include more threads than a number of processing engines within graphics multiprocessor <b>3634</b>. In at least one embodiment, when a thread group includes more threads than a number of processing engines within graphics multiprocessor <b>3634</b>, processing can be performed over consecutive clock cycles. In at least one embodiment, multiple thread groups can be executed concurrently on graphics multiprocessor <b>3634</b>.</p><p id="p-0399" num="0396">In at least one embodiment, graphics multiprocessor <b>3634</b> includes an internal cache memory to perform load and store operations. In at least one embodiment, graphics multiprocessor <b>3634</b> can forego an internal cache and use a cache memory (e.g., L1 cache <b>3648</b>) within processing cluster <b>3694</b>. In at least one embodiment, each graphics multiprocessor <b>3634</b> also has access to Level 2 (&#x201c;L2&#x201d;) caches within partition units (e.g., partition units <b>3620</b>A-<b>3620</b>N of <figref idref="DRAWINGS">FIG. <b>36</b>A</figref>) that are shared among all processing clusters <b>3694</b> and may be used to transfer data between threads. In at least one embodiment, graphics multiprocessor <b>3634</b> may also access off-chip global memory, which can include one or more of local parallel processor memory and/or system memory. In at least one embodiment, any memory external to parallel processing unit <b>3602</b> may be used as global memory. In at least one embodiment, processing cluster <b>3694</b> includes multiple instances of graphics multiprocessor <b>3634</b> that can share common instructions and data, which may be stored in L1 cache <b>3648</b>.</p><p id="p-0400" num="0397">In at least one embodiment, each processing cluster <b>3694</b> may include an MMU <b>3645</b> that is configured to map virtual addresses into physical addresses. In at least one embodiment, one or more instances of MMU <b>3645</b> may reside within memory interface <b>3618</b> of <figref idref="DRAWINGS">FIG. <b>36</b></figref>. In at least one embodiment, MMU <b>3645</b> includes a set of page table entries (&#x201c;PTEs&#x201d;) used to map a virtual address to a physical address of a tile and optionally a cache line index. In at least one embodiment, MMU <b>3645</b> may include address translation lookaside buffers (&#x201c;TLBs&#x201d;) or caches that may reside within graphics multiprocessor <b>3634</b> or L1 cache <b>3648</b> or processing cluster <b>3694</b>. In at least one embodiment, a physical address is processed to distribute surface data access locality to allow efficient request interleaving among partition units. In at least one embodiment, a cache line index may be used to determine whether a request for a cache line is a hit or miss.</p><p id="p-0401" num="0398">In at least one embodiment, processing cluster <b>3694</b> may be configured such that each graphics multiprocessor <b>3634</b> is coupled to a texture unit <b>3636</b> for performing texture mapping operations, e.g., determining texture sample positions, reading texture data, and filtering texture data. In at least one embodiment, texture data is read from an internal texture L1 cache (not shown) or from an L1 cache within graphics multiprocessor <b>3634</b> and is fetched from an L2 cache, local parallel processor memory, or system memory, as needed. In at least one embodiment, each graphics multiprocessor <b>3634</b> outputs a processed task to data crossbar <b>3640</b> to provide a processed task to another processing cluster <b>3694</b> for further processing or to store a processed task in an L2 cache, a local parallel processor memory, or a system memory via memory crossbar <b>3616</b>. In at least one embodiment, a pre-raster operations unit (&#x201c;preROP&#x201d;) <b>3642</b> is configured to receive data from graphics multiprocessor <b>3634</b>, direct data to ROP units, which may be located with partition units as described herein (e.g., partition units <b>3620</b>A-<b>3620</b>N of <figref idref="DRAWINGS">FIG. <b>36</b></figref>). In at least one embodiment, PreROP <b>3642</b> can perform optimizations for color blending, organize pixel color data, and perform address translations.</p><p id="p-0402" num="0399"><figref idref="DRAWINGS">FIG. <b>36</b>C</figref> illustrates a graphics multiprocessor <b>3696</b>, in accordance with at least one embodiment. In at least one embodiment, graphics multiprocessor <b>3696</b> is graphics multiprocessor <b>3634</b> of <figref idref="DRAWINGS">FIG. <b>36</b>B</figref>. In at least one embodiment, graphics multiprocessor <b>3696</b> couples with pipeline manager <b>3632</b> of processing cluster <b>3694</b>. In at least one embodiment, graphics multiprocessor <b>3696</b> has an execution pipeline including but not limited to an instruction cache <b>3652</b>, an instruction unit <b>3654</b>, an address mapping unit <b>3656</b>, a register file <b>3658</b>, one or more GPGPU cores <b>3662</b>, and one or more LSUs <b>3666</b>. GPGPU cores <b>3662</b> and LSUs <b>3666</b> are coupled with cache memory <b>3672</b> and shared memory <b>3670</b> via a memory and cache interconnect <b>3668</b>.</p><p id="p-0403" num="0400">In at least one embodiment, instruction cache <b>3652</b> receives a stream of instructions to execute from pipeline manager <b>3632</b>. In at least one embodiment, instructions are cached in instruction cache <b>3652</b> and dispatched for execution by instruction unit <b>3654</b>. In at least one embodiment, instruction unit <b>3654</b> can dispatch instructions as thread groups (e.g., warps), with each thread of a thread group assigned to a different execution unit within GPGPU core <b>3662</b>. In at least one embodiment, an instruction can access any of a local, shared, or global address space by specifying an address within a unified address space. In at least one embodiment, address mapping unit <b>3656</b> can be used to translate addresses in a unified address space into a distinct memory address that can be accessed by LSUs <b>3666</b>.</p><p id="p-0404" num="0401">In at least one embodiment, register file <b>3658</b> provides a set of registers for functional units of graphics multiprocessor <b>3696</b>. In at least one embodiment, register file <b>3658</b> provides temporary storage for operands connected to data paths of functional units (e.g., GPGPU cores <b>3662</b>, LSUs <b>3666</b>) of graphics multiprocessor <b>3696</b>. In at least one embodiment, register file <b>3658</b> is divided between each of functional units such that each functional unit is allocated a dedicated portion of register file <b>3658</b>. In at least one embodiment, register file <b>3658</b> is divided between different thread groups being executed by graphics multiprocessor <b>3696</b>.</p><p id="p-0405" num="0402">In at least one embodiment, GPGPU cores <b>3662</b> can each include FPUs and/or integer ALUs that are used to execute instructions of graphics multiprocessor <b>3696</b>. GPGPU cores <b>3662</b> can be similar in architecture or can differ in architecture. In at least one embodiment, a first portion of GPGPU cores <b>3662</b> include a single precision FPU and an integer ALU while a second portion of GPGPU cores <b>3662</b> include a double precision FPU. In at least one embodiment, FPUs can implement IEEE 754-2008 standard for floating point arithmetic or enable variable precision floating point arithmetic. In at least one embodiment, graphics multiprocessor <b>3696</b> can additionally include one or more fixed function or special function units to perform specific functions such as copy rectangle or pixel blending operations. In at least one embodiment one or more of GPGPU cores <b>3662</b> can also include fixed or special function logic.</p><p id="p-0406" num="0403">In at least one embodiment, GPGPU cores <b>3662</b> include SIMD logic capable of performing a single instruction on multiple sets of data. In at least one embodiment GPGPU cores <b>3662</b> can physically execute SIMD4, SIMD8, and SIMD16 instructions and logically execute SIMD1, SIMD2, and SIMD32 instructions. In at least one embodiment, SIMD instructions for GPGPU cores <b>3662</b> can be generated at compile time by a shader compiler or automatically generated when executing programs written and compiled for single program multiple data (&#x201c;SPMD&#x201d;) or SIMT architectures. In at least one embodiment, multiple threads of a program configured for an SIMT execution model can executed via a single SIMD instruction. In at least one embodiment, eight SIMT threads that perform the same or similar operations can be executed in parallel via a single SIMD8 logic unit.</p><p id="p-0407" num="0404">In at least one embodiment, memory and cache interconnect <b>3668</b> is an interconnect network that connects each functional unit of graphics multiprocessor <b>3696</b> to register file <b>3658</b> and to shared memory <b>3670</b>. In at least one embodiment, memory and cache interconnect <b>3668</b> is a crossbar interconnect that allows LSU <b>3666</b> to implement load and store operations between shared memory <b>3670</b> and register file <b>3658</b>. In at least one embodiment, register file <b>3658</b> can operate at a same frequency as GPGPU cores <b>3662</b>, thus data transfer between GPGPU cores <b>3662</b> and register file <b>3658</b> is very low latency. In at least one embodiment, shared memory <b>3670</b> can be used to enable communication between threads that execute on functional units within graphics multiprocessor <b>3696</b>. In at least one embodiment, cache memory <b>3672</b> can be used as a data cache in at least one embodiment, to cache texture data communicated between functional units and texture unit <b>3636</b>. In at least one embodiment, shared memory <b>3670</b> can also be used as a program managed cached. In at least one embodiment, threads executing on GPGPU cores <b>3662</b> can programmatically store data within shared memory in addition to automatically cached data that is stored within cache memory <b>3672</b>.</p><p id="p-0408" num="0405">In at least one embodiment, a parallel processor or GPGPU as described herein is communicatively coupled to host/processor cores to accelerate graphics operations, machine-learning operations, pattern analysis operations, and various general purpose GPU (GPGPU) functions. In at least one embodiment, a GPU may be communicatively coupled to host processor/cores over a bus or other interconnect (e.g., a high speed interconnect such as PCIe or NVLink). In at least one embodiment, a GPU may be integrated on a same package or chip as cores and communicatively coupled to cores over a processor bus/interconnect that is internal to a package or a chip. In at least one embodiment, regardless of a manner in which a GPU is connected, processor cores may allocate work to a GPU in a form of sequences of commands/instructions contained in a WD. In at least one embodiment, a GPU then uses dedicated circuitry/logic for efficiently processing these commands/instructions.</p><heading id="h-0011" level="1">General Computing</heading><p id="p-0409" num="0406">The following figures set forth, without limitation, exemplary software constructs within general computing that can be used to implement at least one embodiment.</p><p id="p-0410" num="0407"><figref idref="DRAWINGS">FIG. <b>37</b></figref> illustrates a software stack of a programming platform, in accordance with at least one embodiment. In at least one embodiment, a programming platform is a platform for leveraging hardware on a computing system to accelerate computational tasks. A programming platform may be accessible to software developers through libraries, compiler directives, and/or extensions to programming languages, in at least one embodiment. In at least one embodiment, a programming platform may be, but is not limited to, CUDA, Radeon Open Compute Platform (&#x201c;ROCm&#x201d;), OpenCL (OpenCL&#x2122; is developed by Khronos group), SYCL, or Intel One API.</p><p id="p-0411" num="0408">In at least one embodiment, a software stack <b>3700</b> of a programming platform provides an execution environment for an application <b>3701</b>. In at least one embodiment, application <b>3701</b> may include any computer software capable of being launched on software stack <b>3700</b>. In at least one embodiment, application <b>3701</b> may include, but is not limited to, an artificial intelligence (&#x201c;AI&#x201d;)/machine learning (&#x201c;ML&#x201d;) application, a high performance computing (&#x201c;HPC&#x201d;) application, a virtual desktop infrastructure (&#x201c;VDI&#x201d;), or a datacenter workload.</p><p id="p-0412" num="0409">In at least one embodiment, application <b>3701</b> and software stack <b>3700</b> run on hardware <b>3707</b>. Hardware <b>3707</b> may include one or more GPUs, CPUs, FPGAs, AI engines, and/or other types of compute devices that support a programming platform, in at least one embodiment. In at least one embodiment, such as with CUDA, software stack <b>3700</b> may be vendor specific and compatible with only devices from particular vendor(s). In at least one embodiment, such as in with OpenCL, software stack <b>3700</b> may be used with devices from different vendors. In at least one embodiment, hardware <b>3707</b> includes a host connected to one more devices that can be accessed to perform computational tasks via application programming interface (&#x201c;API&#x201d;) calls. A device within hardware <b>3707</b> may include, but is not limited to, a GPU, FPGA, AI engine, or other compute device (but may also include a CPU) and its memory, as opposed to a host within hardware <b>3707</b> that may include, but is not limited to, a CPU (but may also include a compute device) and its memory, in at least one embodiment.</p><p id="p-0413" num="0410">In at least one embodiment, software stack <b>3700</b> of a programming platform includes, without limitation, a number of libraries <b>3703</b>, a runtime <b>3705</b>, and a device kernel driver <b>3706</b> Each of libraries <b>3703</b> may include data and programming code that can be used by computer programs and leveraged during software development, in at least one embodiment. In at least one embodiment, libraries <b>3703</b> may include, but are not limited to, pre-written code and subroutines, classes, values, type specifications, configuration data, documentation, help data, and/or message templates. In at least one embodiment, libraries <b>3703</b> include functions that are optimized for execution on one or more types of devices. In at least one embodiment, libraries <b>3703</b> may include, but are not limited to, functions for performing mathematical, deep learning, and/or other types of operations on devices. In at least one embodiment, libraries <b>3803</b> are associated with corresponding APIs <b>3802</b>, which may include one or more APIs, that expose functions implemented in libraries <b>3803</b>.</p><p id="p-0414" num="0411">In at least one embodiment, application <b>3701</b> is written as source code that is compiled into executable code, as discussed in greater detail below in conjunction with <figref idref="DRAWINGS">FIG. <b>42</b></figref>. Executable code of application <b>3701</b> may run, at least in part, on an execution environment provided by software stack <b>3700</b>, in at least one embodiment. In at least one embodiment, during execution of application <b>3701</b>, code may be reached that needs to run on a device, as opposed to a host. In such a case, runtime <b>3705</b> may be called to load and launch requisite code on a device, in at least one embodiment. In at least one embodiment, runtime <b>3705</b> may include any technically feasible runtime system that is able to support execution of application S01.</p><p id="p-0415" num="0412">In at least one embodiment, runtime <b>3705</b> is implemented as one or more runtime libraries associated with corresponding APIs, which are shown as API(s) <b>3704</b>. One or more of such runtime libraries may include, without limitation, functions for memory management, execution control, device management, error handling, and/or synchronization, among other things, in at least one embodiment. In at least one embodiment, memory management functions may include, but are not limited to, functions to allocate, deallocate, and copy device memory, as well as transfer data between host memory and device memory. In at least one embodiment, execution control functions may include, but are not limited to, functions to launch a function (sometimes referred to as a &#x201c;kernel&#x201d; when a function is a global function callable from a host) on a device and set attribute values in a buffer maintained by a runtime library for a given function to be executed on a device.</p><p id="p-0416" num="0413">Runtime libraries and corresponding API(s) <b>3704</b> may be implemented in any technically feasible manner, in at least one embodiment. In at least one embodiment, one (or any number of) API may expose a low-level set of functions for fine-grained control of a device, while another (or any number of) API may expose a higher-level set of such functions. In at least one embodiment, a high-level runtime API may be built on top of a low-level API. In at least one embodiment, one or more of runtime APIs may be language-specific APIs that are layered on top of a language-independent runtime API.</p><p id="p-0417" num="0414">In at least one embodiment, device kernel driver <b>3706</b> is configured to facilitate communication with an underlying device. In at least one embodiment, device kernel driver <b>3706</b> may provide low-level functionalities upon which APIs, such as API(s) <b>3704</b>, and/or other software relies. In at least one embodiment, device kernel driver <b>3706</b> may be configured to compile intermediate representation (&#x201c;IR&#x201d;) code into binary code at runtime. For CUDA, device kernel driver <b>3706</b> may compile Parallel Thread Execution (&#x201c;PTX&#x201d;) IR code that is not hardware specific into binary code for a specific target device at runtime (with caching of compiled binary code), which is also sometimes referred to as &#x201c;finalizing&#x201d; code, in at least one embodiment. Doing so may permit finalized code to run on a target device, which may not have existed when source code was originally compiled into PTX code, in at least one embodiment. Alternatively, in at least one embodiment, device source code may be compiled into binary code offline, without requiring device kernel driver <b>3706</b> to compile IR code at runtime.</p><p id="p-0418" num="0415"><figref idref="DRAWINGS">FIG. <b>38</b></figref> illustrates a CUDA implementation of software stack <b>3700</b> of <figref idref="DRAWINGS">FIG. <b>37</b></figref>, in accordance with at least one embodiment. In at least one embodiment, a CUDA software stack <b>3800</b>, on which an application <b>3801</b> may be launched, includes CUDA libraries <b>3803</b>, a CUDA runtime <b>3805</b>, a CUDA driver <b>3807</b>, and a device kernel driver <b>3808</b>. In at least one embodiment, CUDA software stack <b>3800</b> executes on hardware <b>3809</b>, which may include a GPU that supports CUDA and is developed by NVIDIA Corporation of Santa Clara, Calif.</p><p id="p-0419" num="0416">In at least one embodiment, application <b>3801</b>, CUDA runtime <b>3805</b>, and device kernel driver <b>3808</b> may perform similar functionalities as application <b>3701</b>, runtime <b>3705</b>, and device kernel driver <b>3706</b>, respectively, which are described above in conjunction with <figref idref="DRAWINGS">FIG. <b>37</b></figref>. In at least one embodiment, CUDA driver <b>3807</b> includes a library (libcuda.so) that implements a CUDA driver API <b>3806</b>. Similar to a CUDA runtime API <b>3804</b> implemented by a CUDA runtime library (cudart), CUDA driver API <b>3806</b> may, without limitation, expose functions for memory management, execution control, device management, error handling, synchronization, and/or graphics interoperability, among other things, in at least one embodiment. In at least one embodiment, CUDA driver API <b>3806</b> differs from CUDA runtime API <b>3804</b> in that CUDA runtime API <b>3804</b> simplifies device code management by providing implicit initialization, context (analogous to a process) management, and module (analogous to dynamically loaded libraries) management. In contrast to high-level CUDA runtime API <b>3804</b>, CUDA driver API <b>3806</b> is a low-level API providing more fine-grained control of a device, particularly with respect to contexts and module loading, in at least one embodiment. In at least one embodiment, CUDA driver API <b>3806</b> may expose functions for context management that are not exposed by CUDA runtime API <b>3804</b>. In at least one embodiment, CUDA driver API <b>3806</b> is also language-independent and supports, e.g., OpenCL in addition to CUDA runtime API <b>3804</b>. Further, in at least one embodiment, development libraries, including CUDA runtime <b>3805</b>, may be considered as separate from driver components, including user-mode CUDA driver <b>3807</b> and kernel-mode device driver <b>3808</b> (also sometimes referred to as a &#x201c;display&#x201d; driver).</p><p id="p-0420" num="0417">In at least one embodiment, CUDA libraries <b>3803</b> may include, but are not limited to, mathematical libraries, deep learning libraries, parallel algorithm libraries, and/or signal/image/video processing libraries, which parallel computing applications such as application <b>3801</b> may utilize. In at least one embodiment, CUDA libraries <b>3803</b> may include mathematical libraries such as a cuBLAS library that is an implementation of Basic Linear Algebra Subprograms (&#x201c;BLAS&#x201d;) for performing linear algebra operations, a cuFFT library for computing fast Fourier transforms (&#x201c;FFTs&#x201d;), and a cuRAND library for generating random numbers, among others. In at least one embodiment, CUDA libraries <b>3803</b> may include deep learning libraries such as a cuDNN library of primitives for deep neural networks and a TensorRT platform for high-performance deep learning inference, among others.</p><p id="p-0421" num="0418"><figref idref="DRAWINGS">FIG. <b>39</b></figref> illustrates a ROCm implementation of software stack <b>3700</b> of <figref idref="DRAWINGS">FIG. <b>37</b></figref>, in accordance with at least one embodiment. In at least one embodiment, a ROCm software stack <b>3900</b>, on which an application <b>3901</b> may be launched, includes a language runtime <b>3903</b>, a system runtime <b>3905</b>, a thunk <b>3907</b>, a ROCm kernel driver <b>3908</b>, and a device kernel driver <b>3909</b>. In at least one embodiment, ROCm software stack <b>3900</b> executes on hardware <b>3910</b>, which may include a GPU that supports ROCm and is developed by AMD Corporation of Santa Clara, Calif.</p><p id="p-0422" num="0419">In at least one embodiment, application <b>3901</b> may perform similar functionalities as application <b>3701</b> discussed above in conjunction with <figref idref="DRAWINGS">FIG. <b>37</b></figref>. In addition, language runtime <b>3903</b> and system runtime <b>3905</b> may perform similar functionalities as runtime <b>3705</b> discussed above in conjunction with <figref idref="DRAWINGS">FIG. <b>37</b></figref>, in at least one embodiment. In at least one embodiment, language runtime <b>3903</b> and system runtime <b>3905</b> differ in that system runtime <b>3905</b> is a language-independent runtime that implements a ROCr system runtime API <b>3904</b> and makes use of a Heterogeneous System Architecture (&#x201c;HAS&#x201d;) Runtime API. HAS runtime API is a thin, user-mode API that exposes interfaces to access and interact with an AMD GPU, including functions for memory management, execution control via architected dispatch of kernels, error handling, system and agent information, and runtime initialization and shutdown, among other things, in at least one embodiment. In contrast to system runtime <b>3905</b>, language runtime <b>3903</b> is an implementation of a language-specific runtime API <b>3902</b> layered on top of ROCr system runtime API <b>3904</b>, in at least one embodiment. In at least one embodiment, language runtime API may include, but is not limited to, a Heterogeneous compute Interface for Portability (&#x201c;HIP&#x201d;) language runtime API, a Heterogeneous Compute Compiler (&#x201c;HCC&#x201d;) language runtime API, or an OpenCL API, among others. HIP language in particular is an extension of C++ programming language with functionally similar versions of CUDA mechanisms, and, in at least one embodiment, a HIP language runtime API includes functions that are similar to those of CUDA runtime API <b>3804</b> discussed above in conjunction with <figref idref="DRAWINGS">FIG. <b>38</b></figref>, such as functions for memory management, execution control, device management, error handling, and synchronization, among other things.</p><p id="p-0423" num="0420">In at least one embodiment, thunk (ROCt) <b>3907</b> is an interface that can be used to interact with underlying ROCm driver <b>3908</b>. In at least one embodiment, ROCm driver <b>3908</b> is a ROCk driver, which is a combination of an AMDGPU driver and a HAS kernel driver (amdkfd). In at least one embodiment, AMDGPU driver is a device kernel driver for GPUs developed by AMD that performs similar functionalities as device kernel driver <b>3706</b> discussed above in conjunction with <figref idref="DRAWINGS">FIG. <b>37</b></figref>. In at least one embodiment, HAS kernel driver is a driver permitting different types of processors to share system resources more effectively via hardware features.</p><p id="p-0424" num="0421">In at least one embodiment, various libraries (not shown) may be included in ROCm software stack <b>3900</b> above language runtime <b>3903</b> and provide functionality similarity to CUDA libraries <b>3803</b>, discussed above in conjunction with <figref idref="DRAWINGS">FIG. <b>38</b></figref>. In at least one embodiment, various libraries may include, but are not limited to, mathematical, deep learning, and/or other libraries such as a hipBLAS library that implements functions similar to those of CUDA cuBLAS, a rocFFT library for computing FFTs that is similar to CUDA cuFFT, among others.</p><p id="p-0425" num="0422"><figref idref="DRAWINGS">FIG. <b>40</b></figref> illustrates an OpenCL implementation of software stack <b>3700</b> of <figref idref="DRAWINGS">FIG. <b>37</b></figref>, in accordance with at least one embodiment. In at least one embodiment, an OpenCL software stack <b>4000</b>, on which an application <b>4001</b> may be launched, includes an OpenCL framework <b>4005</b>, an OpenCL runtime <b>4006</b>, and a driver <b>4007</b>. In at least one embodiment, OpenCL software stack <b>4000</b> executes on hardware <b>3809</b> that is not vendor-specific. As OpenCL is supported by devices developed by different vendors, specific OpenCL drivers may be required to interoperate with hardware from such vendors, in at least one embodiment.</p><p id="p-0426" num="0423">In at least one embodiment, application <b>4001</b>, OpenCL runtime <b>4006</b>, device kernel driver <b>4007</b>, and hardware <b>4008</b> may perform similar functionalities as application <b>3701</b>, runtime <b>3705</b>, device kernel driver <b>3706</b>, and hardware <b>3707</b>, respectively, that are discussed above in conjunction with <figref idref="DRAWINGS">FIG. <b>37</b></figref>. In at least one embodiment, application <b>4001</b> further includes an OpenCL kernel <b>4002</b> with code that is to be executed on a device.</p><p id="p-0427" num="0424">In at least one embodiment, OpenCL defines a &#x201c;platform&#x201d; that allows a host to control devices connected to a host. In at least one embodiment, an OpenCL framework provides a platform layer API and a runtime API, shown as platform API <b>400</b> and runtime API <b>4005</b>. In at least one embodiment, runtime API <b>4005</b> uses contexts to manage execution of kernels on devices. In at least one embodiment, each identified device may be associated with a respective context, which runtime API <b>4005</b> may use to manage command queues, program objects, and kernel objects, share memory objects, among other things, for that device. In at least one embodiment, platform API <b>4003</b> exposes functions that permit device contexts to be used to select and initialize devices, submit work to devices via command queues, and enable data transfer to and from devices, among other things. In addition, OpenCL framework provides various built-in functions (not shown), including math functions, relational functions, and image processing functions, among others, in at least one embodiment.</p><p id="p-0428" num="0425">In at least one embodiment, a compiler <b>4004</b> is also included in OpenCL frame-work <b>4005</b>. Source code may be compiled offline prior to executing an application or online during execution of an application, in at least one embodiment. In contrast to CUDA and ROCm, OpenCL applications in at least one embodiment may be compiled online by compiler <b>4004</b>, which is included to be representative of any number of compilers that may be used to compile source code and/or IR code, such as Standard Portable Intermediate Representation (&#x201c;SPIR-V&#x201d;) code, into binary code. Alternatively, in at least one embodiment, OpenCL applications may be compiled offline, prior to execution of such applications.</p><p id="p-0429" num="0426"><figref idref="DRAWINGS">FIG. <b>41</b></figref> illustrates software that is supported by a programming platform, in accordance with at least one embodiment. In at least one embodiment, a programming platform <b>4104</b> is configured to support various programming models <b>4103</b>, middlewares and/or libraries <b>4102</b>, and frameworks <b>4101</b> that an application <b>4100</b> may rely upon. In at least one embodiment, application <b>4100</b> may be an AI/ML application implemented using, in at least one embodiment, a deep learning framework such as MXNet, PyTorch, or TensorFlow, which may rely on libraries such as cuDNN, NVIDIA Collective Communications Library (&#x201c;NCCL&#x201d;), and/or NVIDA Developer Data Loading Library (&#x201c;DALI&#x201d;) CUDA libraries to provide accelerated computing on underlying hardware.</p><p id="p-0430" num="0427">In at least one embodiment, programming platform <b>4104</b> may be one of a CUDA, ROCm, or OpenCL platform described above in conjunction with <figref idref="DRAWINGS">FIG. <b>33</b></figref>, <figref idref="DRAWINGS">FIG. <b>34</b></figref>, and <figref idref="DRAWINGS">FIG. <b>40</b></figref>, respectively. In at least one embodiment, programming platform <b>4104</b> supports multiple programming models <b>4103</b>, which are abstractions of an underlying computing system permitting expressions of algorithms and data structures. Programming models <b>4103</b> may expose features of underlying hardware in order to improve performance, in at least one embodiment. In at least one embodiment, programming models <b>4103</b> may include, but are not limited to, CUDA, HIP, OpenCL, C++Accelerated Massive Parallelism (&#x201c;C++AMP&#x201d;), Open Multi-Processing (&#x201c;OpenMP&#x201d;), Open Accelerators (&#x201c;OpenACC&#x201d;), and/or Vulcan Compute.</p><p id="p-0431" num="0428">In at least one embodiment, libraries and/or middlewares <b>4102</b> provide implementations of abstractions of programming models <b>4104</b>. In at least one embodiment, such libraries include data and programming code that may be used by computer programs and leveraged during software development. In at least one embodiment, such middlewares include software that provides services to applications beyond those available from programming platform <b>4104</b>. In at least one embodiment, libraries and/or middlewares <b>4102</b> may include, but are not limited to, cuBLAS, cuFFT, cuRAND, and other CUDA libraries, or rocBLAS, rocFFT, rocRAND, and other ROCm libraries. In addition, in at least one embodiment, libraries and/or middlewares <b>4102</b> may include NCCL and ROCm Communication Collectives Library (&#x201c;RCCL&#x201d;) libraries providing communication routines for GPUs, a MlOpen library for deep learning acceleration, and/or an Eigen library for linear algebra, matrix and vector operations, geometrical transformations, numerical solvers, and related algorithms.</p><p id="p-0432" num="0429">In at least one embodiment, application frameworks <b>4101</b> depend on libraries and/or middlewares <b>4102</b>. In at least one embodiment, each of application frameworks <b>4101</b> is a software framework used to implement a standard structure of application software. An AI/ML application may be implemented using a framework such as Caffe, Caffe2, TensorFlow, Keras, PyTorch, or MxNet deep learning frameworks, in at least one embodiment.</p><p id="p-0433" num="0430"><figref idref="DRAWINGS">FIG. <b>42</b></figref> illustrates compiling code to execute on one of programming platforms of <figref idref="DRAWINGS">FIGS. <b>37</b>-<b>40</b></figref>, in accordance with at least one embodiment. In at least one embodiment, a compiler <b>4201</b> receives source code <b>4200</b> that includes both host code as well as device code. In at least one embodiment, complier <b>4201</b> is configured to convert source code <b>4200</b> into host executable code <b>4202</b> for execution on a host and device executable code <b>4203</b> for execution on a device. In at least one embodiment, source code <b>4200</b> may either be compiled offline prior to execution of an application, or online during execution of an application.</p><p id="p-0434" num="0431">In at least one embodiment, source code <b>4200</b> may include code in any programming language supported by compiler <b>4201</b>, such as C++, C, Fortran, etc. In at least one embodiment, source code <b>4200</b> may be included in a single-source file having a mixture of host code and device code, with locations of device code being indicated therein. In at least one embodiment, a single-source file may be a .cu file that includes CUDA code or a .hip.cpp file that includes HIP code. Alternatively, in at least one embodiment, source code <b>4200</b> may include multiple source code files, rather than a single-source file, into which host code and device code are separated.</p><p id="p-0435" num="0432">In at least one embodiment, compiler <b>4201</b> is configured to compile source code <b>4200</b> into host executable code <b>4202</b> for execution on a host and device executable code <b>4203</b> for execution on a device. In at least one embodiment, compiler <b>4201</b> performs operations including parsing source code <b>4200</b> into an abstract system tree (AST), performing optimizations, and generating executable code. In at least one embodiment in which source code <b>4200</b> includes a single-source file, compiler <b>4201</b> may separate device code from host code in such a single-source file, compile device code and host code into device executable code <b>4203</b> and host executable code <b>4202</b>, respectively, and link device executable code <b>4203</b> and host executable code <b>4202</b> together in a single file, as discussed in greater detail below with respect to <figref idref="DRAWINGS">FIG. <b>26</b></figref>.</p><p id="p-0436" num="0433">In at least one embodiment, host executable code <b>4202</b> and device executable code <b>4203</b> may be in any suitable format, such as binary code and/or IR code. In a case of CUDA, host executable code <b>4202</b> may include native object code and device executable code <b>4203</b> may include code in PTX intermediate representation, in at least one embodiment. In a case of ROCm, both host executable code <b>4202</b> and device executable code <b>4203</b> may include target binary code, in at least one embodiment.</p><p id="p-0437" num="0434">Other variations are within spirit of present disclosure. Thus, while disclosed techniques are susceptible to various modifications and alternative constructions, certain illustrated embodiments thereof are shown in drawings and have been described above in detail. It should be understood, however, that there is no intention to limit disclosure to specific form or forms disclosed, but on contrary, intention is to cover all modifications, alternative constructions, and equivalents falling within spirit and scope of disclosure, as defined in appended claims.</p><p id="p-0438" num="0435">Use of terms &#x201c;a&#x201d; and &#x201c;an&#x201d; and &#x201c;the&#x201d; and similar referents in context of describing disclosed embodiments (especially in context of following claims) are to be construed to cover both singular and plural, unless otherwise indicated herein or clearly contradicted by context, and not as a definition of a term. Terms &#x201c;comprising,&#x201d; &#x201c;having,&#x201d; &#x201c;including,&#x201d; and &#x201c;containing&#x201d; are to be construed as open-ended terms (meaning &#x201c;including, but not limited to,&#x201d;) unless otherwise noted. term &#x201c;connected,&#x201d; when unmodified and referring to physical connections, is to be construed as partly or wholly contained within, attached to, or joined together, even if there is something intervening. Recitation of ranges of values herein are merely intended to serve as a shorthand method of referring individually to each separate value falling within range, unless otherwise indicated herein and each separate value is incorporated into specification as if it were individually recited herein. In at least one embodiment, use of term &#x201c;set&#x201d; (e.g., &#x201c;a set of items&#x201d;) or &#x201c;subset&#x201d; unless otherwise noted or contradicted by context, is to be construed as a nonempty collection comprising one or more members. Further, unless otherwise noted or contradicted by context, term &#x201c;subset&#x201d; of a corresponding set does not necessarily denote a proper subset of corresponding set, but subset and corresponding set may be equal.</p><p id="p-0439" num="0436">Conjunctive language, such as phrases of form &#x201c;at least one of A, B, and C,&#x201d; or &#x201c;at least one of A, B and C,&#x201d; unless specifically stated otherwise or otherwise clearly contradicted by context, is otherwise understood with context as used in general to present that an item, term, etc., may be either A or B or C, or any nonempty subset of set of A and B and C. In at least one embodiment of a set having three members, conjunctive phrases &#x201c;at least one of A, B, and C&#x201d; and &#x201c;at least one of A, B and C&#x201d; refer to any of following sets: {A}, {B}, {C}, {A, B}, {A, C}, {B, C}, {A, B, C}. Thus, such conjunctive language is not generally intended to imply that certain embodiments require at least one of A, at least one of B and at least one of C each to be present. In addition, unless otherwise noted or contradicted by context, term &#x201c;plurality&#x201d; indicates a state of being plural (e.g., &#x201c;a plurality of items&#x201d; indicates multiple items). In at least one embodiment, a number of items in a plurality is at least two, but can be more when so indicated either explicitly or by context. Further, unless stated otherwise or otherwise clear from context, phrase &#x201c;based on&#x201d; means &#x201c;based at least in part on&#x201d; and not &#x201c;based solely on.&#x201d;</p><p id="p-0440" num="0437">Operations of processes described herein can be performed in any suitable order unless otherwise indicated herein or otherwise clearly contradicted by context. In at least one embodiment, a process such as those processes described herein (or variations and/or combinations thereof) is performed under control of one or more computer systems configured with executable instructions and is implemented as code (e.g., executable instructions, one or more computer programs or one or more applications) executing collectively on one or more processors, by hardware or combinations thereof. In at least one embodiment, code is stored on a computer-readable storage medium. In at least one embodiment, in form of a computer program comprising a plurality of instructions executable by one or more processors. In at least one embodiment, a computer-readable storage medium is a non-transitory computer-readable storage medium that excludes transitory signals (e.g., a propagating transient electric or electromagnetic transmission) but includes non-transitory data storage circuitry (e.g., buffers, cache, and queues) within transceivers of transitory signals. In at least one embodiment, code (e.g., executable code or source code) is stored on a set of one or more non-transitory computer-readable storage media having stored thereon executable instructions (or other memory to store executable instructions) that, when executed (i.e., as a result of being executed) by one or more processors of a computer system, cause computer system to perform operations described herein. A set of non-transitory computer-readable storage media, in at least one embodiment, comprises multiple non-transitory computer-readable storage media and one or more of individual non-transitory storage media of multiple non-transitory computer-readable storage media lack all of code while multiple non-transitory computer-readable storage media collectively store all of code. In at least one embodiment, executable instructions are executed such that different instructions are executed by different processors&#x2014;in at least one embodiment, a non-transitory computer-readable storage medium store instructions and a main central processing unit (&#x201c;CPU&#x201d;) executes some of instructions while a graphics processing unit (&#x201c;GPU&#x201d;) executes other instructions. In at least one embodiment, different components of a computer system have separate processors and different processors execute different subsets of instructions.</p><p id="p-0441" num="0438">Accordingly, in at least one embodiment, computer systems are configured to implement one or more services that singly or collectively perform operations of processes described herein and such computer systems are configured with applicable hardware and/or software that enable performance of operations. Further, a computer system that implements at least one embodiment of present disclosure is a single device and, in another embodiment, is a distributed computer system comprising multiple devices that operate differently such that distributed computer system performs operations described herein and such that a single device does not perform all operations.</p><p id="p-0442" num="0439">Use of any and all of the at least one embodiments, or exemplary language (e.g., &#x201c;such as&#x201d;) provided herein, is intended merely to better illuminate embodiments of disclosure and does not pose a limitation on scope of disclosure unless otherwise claimed. No language in specification should be construed as indicating any non-claimed element as essential to practice of disclosure.</p><p id="p-0443" num="0440">All references, including publications, patent applications, and patents, cited herein are hereby incorporated by reference to same extent as if each reference were individually and specifically indicated to be incorporated by reference and were set forth in its entirety herein.</p><p id="p-0444" num="0441">In description and claims, terms &#x201c;coupled&#x201d; and &#x201c;connected,&#x201d; along with their derivatives, may be used. It should be understood that these terms may be not intended as synonyms for each other. Rather, in ones of at least one embodiments, &#x201c;connected&#x201d; or &#x201c;coupled&#x201d; may be used to indicate that two or more elements are in direct or indirect physical or electrical contact with each other. &#x201c;Coupled&#x201d; may also mean that two or more elements are not in direct contact with each other, but yet still co-operate or interact with each other.</p><p id="p-0445" num="0442">Unless specifically stated otherwise, it may be appreciated that throughout specification terms such as &#x201c;processing,&#x201d; &#x201c;computing,&#x201d; &#x201c;calculating,&#x201d; &#x201c;determining,&#x201d; or like, refer to action and/or processes of a computer or computing system, or similar electronic computing device, that manipulate and/or transform data represented as physical, such as electronic, quantities within computing system's registers and/or memories into other data similarly represented as physical quantities within computing system's memories, registers or other such information storage, transmission or display devices.</p><p id="p-0446" num="0443">In a similar manner, term &#x201c;processor&#x201d; may refer to any device or portion of a device that processes electronic data from registers and/or memory and transform that electronic data into other electronic data that may be stored in registers and/or memory. As non-limiting ones of the at least one embodiments, &#x201c;processor&#x201d; may be a CPU or a GPU. A &#x201c;computing platform&#x201d; may comprise one or more processors. As used herein, &#x201c;software&#x201d; processes may include, in at least one embodiment, software and/or hardware entities that perform work over time, such as tasks, threads, and intelligent agents. Also, each process may refer to multiple processes, for carrying out instructions in sequence or in parallel, continuously or intermittently. Terms &#x201c;system&#x201d; and &#x201c;method&#x201d; are used herein interchangeably insofar as system may embody one or more methods and methods may be considered a system.</p><p id="p-0447" num="0444">In present document, references may be made to obtaining, acquiring, receiving, or inputting analog or digital data into a subsystem, computer system, or computer-implemented machine. In at least one embodiment, process of obtaining, acquiring, receiving, or inputting analog and digital data can be accomplished in a variety of ways such as by receiving data as a parameter of a function call or a call to an application programming interface. In some implementations, process of obtaining, acquiring, receiving, or inputting analog or digital data can be accomplished by transferring data via a serial or parallel interface. In another implementation, process of obtaining, acquiring, receiving, or inputting analog or digital data can be accomplished by transferring data via a computer network from providing entity to acquiring entity. References may also be made to providing, outputting, transmitting, sending, or presenting analog or digital data. In various ones of the at least one embodiments, process of providing, outputting, transmitting, sending, or presenting analog or digital data can be accomplished by transferring data as an input or output parameter of a function call, a parameter of an application programming interface or interprocess communication mechanism.</p><p id="p-0448" num="0445">Although discussion above sets forth ones of the at least one embodiments having implementations of described techniques, other architectures may be used to implement described functionality, and are intended to be within scope of this disclosure. Furthermore, although specific distributions of responsibilities are defined above for purposes of discussion, various functions and responsibilities might be distributed and divided in different ways, depending on circumstances.</p><p id="p-0449" num="0446">Furthermore, although subject matter has been described in language specific to structural features and/or methodological acts, it is to be understood that subject matter claimed in appended claims is not necessarily limited to specific features or acts described. Rather, specific features and acts are disclosed as exemplary forms of implementing the claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A datacenter cooling system, comprising:<claim-text>an in-rack refrigerant distribution unit (RDU) to distribute refrigerant to one or more colds plates, the in-rack RDU associated with a pressure control system to enable a pressure-drop before an expansion valve for a liquid-phase of the refrigerant based in part on a first pressure of the liquid-phase of the refrigerant exceeding a first threshold and based in part on a temperature associated with the one or more cold plates being below a second threshold.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The datacenter cooling system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>at least one processor to determine the temperature associated with the one or more cold plates and to enable one or more flow controllers to cause the refrigerant to flow through the pressure control system to reach a second pressure that is below the first threshold.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The datacenter cooling system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>a first evaporator section associated with the one or more cold plates and a first refrigerant cooling loop;</claim-text><claim-text>a first condenser unit associated with the first refrigerant cooling loop and within the in-rack RDU;</claim-text><claim-text>a second evaporator section within the in-rack RDU and associated with a second refrigerant cooling loop and with the first condenser unit, the second evaporator section to form part of a refrigerant-to-refrigerant heat exchanger (R2RHX) along with the first condenser unit;</claim-text><claim-text>a second condenser unit of the second refrigerant cooling loop located external to a datacenter to enable dissipation of heat from the one or more cold plates to an ambient environment that is external to the datacenter.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The datacenter cooling system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>an R2RHX to couple, within the in-rack RDU, multiple condenser units from multiple racks with a second evaporator section associated with the second refrigerant cooling loop so that different heat from different racks is dissipated to the ambient environment from a second condenser unit that is external to the in-rack RDU.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The datacenter cooling system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>at least one processor associated with the in-rack RDU to receive sensor inputs from sensors associated with at least one computing device, the at least one processor to enable a first refrigerant cooling loop or a second refrigerant cooling loop to provide cooling using the one or more cold plates.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The datacenter cooling system of <claim-ref idref="CLM-00005">claim 5</claim-ref>, further comprising:<claim-text>one or more neural networks of the at least one processor to receive the sensor inputs and to infer a cooling requirement for the first refrigerant cooling loop or the second refrigerant cooling loop, the cooling requirement in response to the temperature associated with the one or more cold plates.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The datacenter cooling system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>at least one processor associated with the in-rack RDU to cause one or more flow controllers to enable two or more of: a first refrigerant cooling loop to provide the refrigerant, a second refrigerant cooling loop to interface with the first refrigerant cooling loop within the in-rack RDU, or prevent flow of a secondary coolant to a secondary cooling loop.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The datacenter cooling system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein a first refrigerant cooling loop or a second refrigerant cooling loop enables flow of the refrigerant or an engineered fluid therethrough.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The datacenter cooling system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>one or more flow controllers to support flow through the refrigerant through a first refrigerant cooling loop, along with flow the refrigerant or a different refrigerant in a second refrigerant cooling loop.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The datacenter cooling system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>at least one processor to enable a first mode of a datacenter cooling system to provide cooling using a first refrigerant cooling loop to provide the refrigerant and to enable a second mode of the datacenter cooling system to provide cooling using a secondary cooling loop.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. A processor comprising one or more circuits, the one or more circuits to determine a pressure of a liquid-phase of a refrigerant exceeding a first threshold and to determine a temperature associated with one or more cold plates being below a second threshold, the processor to enable a pressure control system of an in-rack refrigerant distribution unit (RDU) to cause a response to the temperature and to cause a pressure-drop, before an expansion valve, for the liquid-phase of the refrigerant.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The processor of <claim-ref idref="CLM-00011">claim 11</claim-ref>, further comprising:<claim-text>an output to provide signals for one or more flow controllers to enable a first refrigerant cooling loop to provide the refrigerant or to enable a second refrigerant cooling loop to interface with the first refrigerant cooling loop.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The processor of <claim-ref idref="CLM-00011">claim 11</claim-ref>, further comprising:<claim-text>an input to receive sensor inputs from sensors associated with at least one computing device or the one or more cold plates, the processor to determine a first cooling requirement associated with the refrigerant of a first refrigerant cooling loop or with the refrigerant or a different refrigerant of the second refrigerant cooling loop and to determine a second cooling requirement associated with a secondary cooling loop, the secondary cooling loop associated with a primary cooling loop.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The processor of <claim-ref idref="CLM-00013">claim 13</claim-ref>, further comprising:<claim-text>one or more neural networks to receive the sensor inputs and to infer the first cooling requirement and the second cooling requirement.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The processor of <claim-ref idref="CLM-00011">claim 11</claim-ref>, further comprising:<claim-text>one or more neural networks to infer a failure of a secondary cooling loop, the one or more circuits to cause one or more flow controllers to activate a first refrigerant cooling loop to provide the refrigerant.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. A method for datacenter cooling system, comprising:<claim-text>providing an in-rack refrigerant distribution unit (RDU) to distribute refrigerant to one or more colds plates;</claim-text><claim-text>determining a temperature associated with the one or more cold plates;</claim-text><claim-text>enabling the in-rack RDU to be associated with a pressure control system; and</claim-text><claim-text>enabling the pressure control system to cause a pressure-drop before an expansion valve for a liquid-phase of the refrigerant based in part on a pressure of the liquid-phase of the refrigerant exceeding a first threshold and based in part on the temperature being below a second threshold.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref>, further comprising:<claim-text>determining, using at least one processor, the temperature associated with the one or more cold plates;</claim-text><claim-text>determining a first cooling requirement or a second cooling requirement using the temperature; and</claim-text><claim-text>causing, based in part on the first cooling requirement or the second cooling requirement, a first refrigerant cooling loop to distribute the refrigerant, a second refrigerant cooling loop to interface with the first refrigerant cooling loop within the in-rack RDU, or a secondary cooling loop, the secondary cooling loop associated with a primary cooling loop.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The method of <claim-ref idref="CLM-00017">claim 17</claim-ref>, further comprising:<claim-text>receiving, in the at least one processor, sensor inputs from sensors associated with at least one computing device, a rack, a secondary coolant, the one or more cold plates, or the refrigerant; and</claim-text><claim-text>determining, using the at least one processor, the first cooling requirement and the second cooling requirement based in part on the sensor inputs.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref>, further comprising:<claim-text>enabling a refrigerant-to-refrigerant heat exchanger (R2RHX) of the in-rack RDU to couple multiple condenser units from multiple racks with a second evaporator section associated with a second refrigerant cooling loop so that different heat from different racks is dissipated to an ambient environment from a second condenser unit that is located external to the in-rack RDU.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref>, further comprising:<claim-text>receiving, by at least one processor, sensor inputs from sensors associated with at least one computing device;</claim-text><claim-text>determining, by the at least one processor, a change in a coolant state based in part on the sensor inputs; and</claim-text><claim-text>causing, based in part on the change in the coolant state, the first refrigerant cooling loop, the second refrigerant cooling loop, or a secondary cooling loop, the secondary cooling loop associated with a primary cooling loop.</claim-text></claim-text></claim></claims></us-patent-application>