<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230000423A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221220" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230000423</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17856528</doc-number><date>20220701</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>11</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>0205</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>0531</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>16</class><subclass>H</subclass><main-group>40</main-group><subgroup>63</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>16</class><subclass>H</subclass><main-group>20</main-group><subgroup>70</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>4076</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>11</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>7267</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>6804</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>681</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>02055</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>0531</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>4803</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>74</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>7221</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20180101</date></cpc-version-indicator><section>G</section><class>16</class><subclass>H</subclass><main-group>40</main-group><subgroup>63</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20180101</date></cpc-version-indicator><section>G</section><class>16</class><subclass>H</subclass><main-group>20</main-group><subgroup>70</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>02438</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">SYSTEMS AND METHODS FOR EVALUATING AND MITIGATING PROBLEM BEHAVIOR BY DETECTING PRECURSORS</invention-title><us-related-documents><us-provisional-application><document-id><country>US</country><doc-number>63217585</doc-number><date>20210701</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="obligated-assignee"><addressbook><orgname>Vanderbilt University</orgname><address><city>Nashville</city><state>TN</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant><us-applicant sequence="01" app-type="applicant" designation="us-only" applicant-authority-category="obligated-assignee"><addressbook><orgname>North Carolina State University</orgname><address><city>Raleigh</city><state>NC</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Sarkar</last-name><first-name>Nilanjan</first-name><address><city>Brentwood</city><state>TN</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Zheng</last-name><first-name>Zhaobo</first-name><address><city>Nashville</city><state>TN</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Weitlauf</last-name><first-name>Amy S.</first-name><address><city>Nashville</city><state>TN</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Staubitz</last-name><first-name>John</first-name><address><city>Nashville</city><state>TN</state><country>US</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>Warren</last-name><first-name>Zachary E.</first-name><address><city>Nashville</city><state>TN</state><country>US</country></address></addressbook></inventor><inventor sequence="05" designation="us-only"><addressbook><last-name>Dieffenderfer</last-name><first-name>James P.</first-name><address><city>Raleigh</city><state>NC</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Systems and methods for predicting problem behavior in individuals with developmental and behavior disabilities. A plurality of sensors are configured to collect multimodal data signals of a subject individual including a wearable upper body motion sensing device with a plurality of inertial measurement units (IMUs). An electronic controller is configured to receive output signals from each of IMUs and to model an upper body position of the subject individual based on the output signals from the IMUs. A trained machine-learning model is then applied by providing an input data set that includes multimodal signal data (e.g., including signal data from at least one IMU) and/or features extracted from the multimodal signal data. The machine-learning model is trained to produce as output an indication of whether a precursor to the problem behavior is detected and, in response to detecting the precursor, a notification (or alarm) is generated.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="66.55mm" wi="42.67mm" file="US20230000423A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="220.22mm" wi="140.46mm" file="US20230000423A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="217.68mm" wi="143.76mm" file="US20230000423A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="211.67mm" wi="160.70mm" file="US20230000423A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="150.37mm" wi="126.49mm" file="US20230000423A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="228.09mm" wi="128.44mm" orientation="landscape" file="US20230000423A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="176.53mm" wi="108.12mm" orientation="landscape" file="US20230000423A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="213.95mm" wi="121.24mm" orientation="landscape" file="US20230000423A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application claims the priority benefit of U.S. Provisional Patent Application No. 63/217,585, filed Jul. 1, 2021, entitled &#x201c;PORTABLE APPLICATION TO RECORD GROUNDTRUTH OF AFFECTIVE STATES AND MULTIMODAL DATA COLLECTION SYSTEM TO PREDICT IMMINENT PRECURSOR OF PROBLEMATIC BEHAVIORS,&#x201d; the entire contents of which are hereby incorporated herein by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">The present invention relates to systems and methods for evaluating and treating individuals (e.g., children and adolescents) that may exhibit problem behaviors including, for example, individuals with developmental disabilities and/or autism spectrum disorder (ASD).</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0004" num="0003">Autism Spectrum Disorder (ASD) is a neurodevelopmental disorder that affects 1 in every 59 children in the United States. Over two thirds of children with ASD display frequent problem behaviors (PB) that may entail physical harm to self or others, non-compliance, property destruction, and elopement. Although common in ASD, these behaviors are not limited to autism diagnoses. They also occur as part of other developmental or genetic disorders, such as intellectual disability, as well as psychiatric disorders. Precursors (PC) are signs that a PB may be imminent. If a parent or caregiver can observe the precursors of PB in time to intervene, some PB can be mitigated or prevented.</p><p id="p-0005" num="0004">In some implementations, the systems and methods described herein provide a multimodal data capture system that gathers multi-sensory data from an individual and utilizes a trained machine-learning model to detect precursors of problem behavior based on data signals collected from a plurality of sensors and/or features extracted from the data signals. Data channels may include, for example, motion (e.g., accelerations, joint angles, gesture patterns, pose, etc.), audio (e.g., pitch, volume, speech, words, etc.), physiological signals (e.g., heart rate, body temperature, skin conductance, bio-impedance, etc.), facial expressions (e.g., happy, mouth open, eyes open, etc.), and head rotations/position.</p><p id="p-0006" num="0005">In some implementations, the machine-learning model is trained specifically for an individual based on observed occurrences of problem behavior &#x26; precursors as well as contemporaneously captured data signals. In other implementations, a group or collective model is trained for use by a plurality of different users based on observed data and captured data signals. In some implementations, the system may include a self-monitoring application (e.g., implemented on a portable device such as a smart phone) configured to collect the data signals and apply the trained machine-learning model. In some implementations, the application may be configured to generate an output notification (e.g., text, audio, visual, etc.) configured to warn the user that he/she is showing precursor signs of problem behavior that warrant attention. In some implementations, the system may be configured to provide recommendations for mitigation (e.g., take deep breaths, temporarily step away from situation, etc.). In some implementations, other individuals (e.g., parents, caregivers, therapists, educators, etc.) may also receive a notification of the detected precursor.</p><p id="p-0007" num="0006">In some implementations, the systems and methods described herein provide a multi-modal data collection system on precursors of problem behaviors that include multiple channels of data signals. In some implementations, the system includes a wearable motion sensor system that is feasible to such populations (e.g., a &#x201c;wearable intelligent non-invasive gesture sensor&#x201d; (WINGS) system for capturing motion signal data).</p><p id="p-0008" num="0007">In some implementations, the system is configured to further train (e.g., retrain) the machine-learning model based on user indications of false positives and false negatives. For example, in some implementations, in response to the machine-learning model indicating that a precursor has been detected based on the captured signals, the system may generate a prompt to a user (e.g., the individual being monitored, a caregiver, a parent, etc.) asking the user to confirm whether a precursor actually occurred and/or whether a problem behavior resulted. Conversely, in some implementations, the software application may be configured to allow a user (e.g., the individual being monitored, a caregiver, a parent, etc.) to provide a timestamped user input indicating that an undetected precursor and/or problem behavior occurred. Based on the user feedback, the system is configured to identify a set of captured data signals corresponding to the time of the user input and to retrain the machine-learning model to associate the set of captured data signals with a precursor (in the event of a false negative) and/or to retrain the machine-learning model to no longer associate the set of captured data signals with the precursor (in the event of a false positive).</p><p id="p-0009" num="0008">In some implementations, the systems and methods described herein provide a mechanism for training the machine-learning model based, for example, on data signals collected during administration of an evaluation analysis session for the individual. For example, the interview-informed synthesized contingency analysis (IISCA) is one approach used for the assessment of children and adolescents with problem behaviors. The IISCA includes a controlled series of experimental trials in which the relation between a child's problem behavior and the environment is verified through the structured presentation of alternating preferred and then non-preferred situations. In some implementations, the system described herein provide a software-based application for use by a behavior analyst during an IISCA via which the behavior analyst records behaviors and affective states of the individual undergoing the IISCA. For example, in some implementations, the software application is configured to enable the behavior analyst to indicate observed information such as (1) occurrences of problem behavior, (2) occurrences of precursors, and (3) confirmation that the subject is &#x201c;calm.&#x201d;</p><p id="p-0010" num="0009">In some implementations, the subject individual is asked to wear the multi-modal data capture system while undergoing the IISCA (or other behavior analysis protocol). The collected data from the multi-modal data capture system is timestamped and stored on a memory (e.g.,, a local memory or a cloud storage system). Concurrently, information from the software application indicating observed occurrences of problem behavior, precursors, and calm demeanor are also timestamped and stored. The system is then configured to train the machine-learning model by associating sets of data signals with the corresponding behavior state (e.g., which data signals are associated with precursors, which are associated with problem behaviors, and which are associated with a calm state).</p><p id="p-0011" num="0010">In some implementations, the multi-modal data capture system and the trained machine-learning model are configured to detect precursors to problem behavior. In this manner, the system is configured to detect/predict that the conditions for problem behavior are present and that, without intervention/mitigation, the problem behavior is likely to occur imminently. However, the system is configured to detect the precursors before the problem behavior actually occurs so that, in some cases, the problem behavior might be preventable.</p><p id="p-0012" num="0011">In some implementations, the multi-modal data capture system includes a wrist-worn device (configured to sense physiological signals), a microphone (configured to detect words, sounds, patterns, etc.), and a non-invasive set of gesture sensors (e.g., inertial measurement units (IMUs) configured to capture data that can be used to reconstruct/model the upper body pose and movements (e.g., a child that begins to flail their arms around may be a precursor)). In some implementations, the microphone is selectively attachable to a garment worn by the subject individual. In other implementations, the microphone and/or other sensors may be integrated into a piece of clothing worn by the subject individual. For example, in some implementations, an upper body garment (e.g., a shirt, a sweatshirt, a vest) is configured with a series of pockets positioned at different locations. Each pocket is sized and positioned to non-invasively hold an inertial measurement unit. Similarly, in some implementations, the microphone may be integrated into the garment itself (e.g., in the hood of a hooded sweatshirt) so that it does not need to be worn or attached to the subject individual separately.</p><p id="p-0013" num="0012">In one embodiment, the invention provides a system for predicting problem behavior in individuals with developmental and behavior disabilities. A wearable upper body motion sensing device includes a plurality of inertial measurement units (IMUs) positioned at known positions relative to each other. An electronic controller is configured to receive output signals from each of IMUs and to model an upper body position of the subject individual based on the output signals from the IMUs. A trained machine-learning model is then applied by providing an input data set that includes signal data from at least one IMU, the modeled upper body position information, and/or a sequence of upper body position information. The machine-learning model is trained to produce as output an indication of whether a precursor to the problem behavior is detected and, in response to detecting the precursor, a notification (or alarm) is generated.</p><p id="p-0014" num="0013">In some implementations, the system is configured to receiving multimodal data captured by the wearable upper body motion sensing device and other sensor systems including, for example, physiological data, audio data, and/or video data. In some implementations, the input data set further includes data signals from one or more of the other sensors systems and/or features extracted from the data signals from the one or more of the other sensor systems.</p><p id="p-0015" num="0014">In another embodiment, the invention provides a method of training the machine-learning model to be used to detect precursors to the problem behavior. The subject individual is equipped with the wearable upper body motion sensing device (and, in some implementations, one or more additional sensor systems to generate multimodal training data). A behavior assessment (e.g., the IISCA) is administered to the subject individual while recording signal data from the plurality of inertial measurement units. Concurrently, user inputs are received through a computer-based user device while the behavior assessment is administered. The user inputs provide real-time indications of observed occurrences of the problem behavior and observed occurrences of the precursor. A set of training data is then generated by defining a plurality of ground truth labels based on the received real-time indications of observed occurrences of the problem behavior and the observed occurrences of the precursor. Each real-time indication is mapped to recorded signal data form the plurality of inertial measurement units. The machine-learning model is then trained, based on the set of training data, to produce an output corresponding to the ground truth label in response to receiving as input the signal data mapped to the ground truth label, upper body position information determined based on the signal data mapped to the ground truth label, and/or a sequence of upper body position information determined based on the signal data mapped to the ground truth label.</p><p id="p-0016" num="0015">Other aspects of the invention will become apparent by consideration of the detailed description and accompanying drawings.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a flowchart of a method for training a machine-learning model to detect precursors of problem behavior in a subject individual (e.g., an individual with developmental disabilities (IDD)).</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a flowchart of a method for applying the trained machine-learning model of <figref idref="DRAWINGS">FIG. <b>1</b></figref> to detect precursors of problem behavior based on data signals captured by a multi-modal data capture system.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a block diagram of the multi-modal data capture system used in the method of <figref idref="DRAWINGS">FIG. <b>2</b></figref> according to one implementation.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a block diagram of a wearable non-invasive gesture sensor system according to one implementations.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIGS. <b>5</b>A, <b>5</b>B, and <b>5</b>C</figref> are different perspective views of a wearable garment configured to hold the inertial measurement units of the system of <figref idref="DRAWINGS">FIG. <b>4</b></figref> during use.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is an example of a graphical model of pose and movement of a subject individual recreated based on data signals captured by the inertial measurement units of the system of <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>7</b>A</figref> is another example of a graphical model of a pose of a subject individual.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>7</b>B</figref> is a perspective view of the subject individual posed in the position corresponding to the graphical model of <figref idref="DRAWINGS">FIG. <b>7</b>A</figref>.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a screenshot of a graphical user interface of a software-based application for recording behavior observations during an IISCA.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a state-diagram of the operation of the software-based application of <figref idref="DRAWINGS">FIG. <b>8</b></figref>.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0027" num="0026">Before any embodiments of the invention are explained in detail, it is to be understood that the invention is not limited in its application to the details of construction and the arrangement of components set forth in the following description or illustrated in the following drawings. The invention is capable of other embodiments and of being practiced or of being carried out in various ways.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates an example of a framework for developing a machine-learning model trained to associate multi-modal signal data with precursors to problem behavior as a mechanism for anticipating problem behaviors before they occur. In the example of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, training data for the machine-learning model is generated in the context of an Interview Informed Synthesized Contingency Analysis (IISCA). IISCA is a type of practical functional assessment (PFA). IN this assessment, a behavior analyst methodically manipulates the environment to test caregiver hypotheses around what environmental stimuli serve as antecedents or establishing operations (EOs) to a problem behavior, and which stimuli serve as reinforcers to the problem behavior. During the assessment, the subject (e.g., IDD) interacts with a behavior analyst and/or therapist and receives behavioral reinforcement.</p><p id="p-0029" num="0028">During the IISCA session (step <b>101</b>), a multi-modal data capture system operates to collect multi-modal data indicative of the subject's response to stimuli (step <b>103</b>). As described in further detail below, in some implementations, the multi-modal data capture system is configured to capture data such as, for example, physiological signals, motion signals, pose signals, audio signals, and/or facial expression data. The captured multi-modal data is then processed for feature selection (step <b>105</b>). For example, captured video data may be processed to identify/categorize facial expressions by type, the physiological signals may be processed to quantify a heart rate, the motion signals may be processed to determine a pose and/or body gesture of the subject, and the audio signal may be processed to identify changes in pitch and/or specific words used by the subject.</p><p id="p-0030" num="0029">While the multi-modal data capture system operates to collect the multi-modal data, the behavior analyst observes the subject and, using a software-based application (as described in further detail below) (step <b>107</b>), records observations regarding the behavioral state of the subject including, for example, when problem behaviors are observed, when precursors are observed, and when the subject appears calm (step <b>107</b>). These recorded behavioral states are timestamped and then associated with features extracted from the multi-modal data to form the set of training data. The observed behavioral states serve as the &#x201c;ground truth&#x201d; (step <b>109</b>) for the temporally corresponding multi-modal data and/or extracted features while training the machine learning model (step <b>111</b>). The machine-learning model is trained by mapping the features extracted form the multi-modal data against the ground truth so that, in response to receiving a set of input data (including, for example, the multi-modal data and/or features extracted from the multi-modal data), the machine-learning model produces an output indicative of whether a precursor behavior is detected which would indicate that an occurrence of problem behavior is imminent if not mitigated.</p><p id="p-0031" num="0030">The example of <figref idref="DRAWINGS">FIG. <b>1</b></figref> (and other examples discussed herein below) describe training the machine-learning model in the context of an IISCA session. However, in other implementations, other types of behavior assessment protocols may be used to generate ground truth behavioral state observations while multi-modal data is captured. Furthermore, in some implementations, the system is configured to train the machine-learning model specifically to map extracted features to the ground truth behavioral state while, in other implementations, the system may be configured to train the machine-learning model based on the (raw or filtered) multi-modal data signals themselves and/or a combination of extracted features and signals from the set of multi-modal data signals. Similarly, in some implementations, the machine-learning model is trained to map features/signals specifically to observed precursors that are flagged by the behavior analyst as &#x201c;precursors.&#x201d; However, in other implementations, the system is additionally or alternatively configured to respond to observed occurrences of problem behavior by analyzing the features/signals preceding the occurrence of the problem behavior to attempt to discern signals, features, or combinations thereof that can be indicative of a &#x201c;precursor&#x201d; to the problem behavior&#x2014;even if the subject did not exhibit any observable &#x201c;precursor&#x201d; behavior. In this manner, the machine-learning model can be trained to detect precursors to problem behavior that may not be visually observable by a behavior analyst.</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates an example of a method, executed by a computer-based system and the multi-modal data capture system, for detecting precursors and predicting imminent problem behavior in a subject based on the trained machine-learning model. The multi-modal data capture system collects multi-modal data streams (step <b>201</b>) and sets of the multi-modal data and/or features extracted from the multi-modal data are provided as input to the trained machine-learning model (step <b>203</b>). If the output of the machine-learning model indicates that a precursor is detected (step <b>205</b>), the system generates an output notification and/or mitigation to attempt to prevent the problem behavior from occurring (step <b>207</b>).</p><p id="p-0033" num="0032">In some implementations, the system is configured to respond to a detected precursor by generating an audio and/or visual notification, for example, on a device such as a smart phone, tablet computer, or computer workstation. In some implementations, the system can be configured to generate the notification on the devices associated with one or more different users. For example, in some implementations, the system is configured to generate the notification on devices associated with the subject/patient, the behavior analyst/therapist, a parent, a caregiver, and/or an educator. In some implementations, the system may be configurable to allow the user to indicate which users are to receive the notifications.</p><p id="p-0034" num="0033">Additionally or alternatively, in some implementations, the system may be configured to initiate an action to attempt to mitigate the problem behavior (i.e., the prevent the problem behavior from occurring). For example, the system may be configured to automatically launch a software application on the portable device of the subject/patient initiating breathing exercises to attempt to calm the subject/patient. In other implementations, the system may be configured to include a recommended action in the notification displayed to the user. For example, the system may generate a notification indicating that a precursor has been detected and recommending that the subject step away from the situation for a period of time to calm down. In still other implementations, the system may be configured to initiate some form of actuators to perform an automated system task in response to detecting the precursor. Finally, in some implementations, the system may be configured to perform the same notification/mitigation whenever a precursor is detected. However, in other implementations, the machine-learning model may be trained to identify a specific type or category of precursor and the system may be configured to perform a different type of notification/task based on the type/category of the detected precursor.</p><p id="p-0035" num="0034">Returning to the example of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, in some implementations, the system is configured to generate updated training data based on user-provided feedback regarding occurrences of false positives and/or false negatives. For example, if the system generates a notification indicating that a precursor has been detected, but a user observing the subject (e.g., a parent, caregiver, or the subject themselves) disagrees with the assessment or, for example, if no problem behavior occurs after the machine-learning model detects the alleged precursor, the user may provide a user input via a user interface (e.g., of a software application) indicating a false positive. Conversely, if a problem behavior is observed in the subject without the machine-learning model first detecting a precursor, the user may provide a user input via the user interface indicating a false negative. In response to receiving one or more user inputs indicative of false positives or false negatives (step <b>209</b>), the system will retrain the machine-learning model (step <b>211</b>) based on the captured multi-modal data and/or the extracted features using the user input as the ground truth for the new training data. In some implementations, this retraining is performed periodically after expiration of a time interval while, in some implementations, the machine-learning model is retrained in response to capturing a threshold amount of new training data (e.g., a threshold number of false positives and/or false negatives). In some implementations, the retraining is performed when the multi-modal data capture system is not in use.</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates an example of a multi-modal data capture system. The system of <figref idref="DRAWINGS">FIG. <b>3</b></figref> integrates and synchronizes multiple data modalities of different time scales. The system includes one or more video/camera based systems <b>301</b>, a system of wearable motion sensors <b>303</b>, one or more audio sensors <b>305</b>, and one or more physiological sensors <b>307</b>. In some implementations, the data captured by the video/camera-based system <b>301</b> is processed to extract features such as, for example, head pose, facial expression, and head motion. In some implementations, the data captured by the system of wearable motion sensors <b>303</b> is processed to extract features such as, for example, body movements and body pose. In some implementations, the data captured by the audio sensor <b>305</b> is processed to extract features such as, for example, individual words and phrases, changes in verbal pitch, and changes in volume of the subject's voice. In some implementations, data captured by the physiological sensor <b>307</b> is processed to extract features such as, for example, bio-impedance, heart rate, skin conductance, and body temperature.</p><p id="p-0037" num="0036">In the example of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the sensor systems are communicatively coupled to a data collection computer system <b>309</b>. The data collection computer system <b>309</b> includes an electronic processor <b>311</b> and a non-transitory, computer-readable memory <b>313</b>. The memory <b>313</b> stores data and computer-executable instructions that, when executed by the electronic processor <b>311</b>, provide the functionality of the data collection computer system <b>309</b>&#x2014;including, for example, the functionality described herein. In some implementations, the data signals collected by the various sensor systems are transmitted to the data collection computer system <b>309</b> and stored to the memory <b>313</b>. In some implementations, the data collection computer system <b>309</b> receives the collected multi-modal data signals and stores the data in a cloud-computing environment. In some implementations, the data collection computer system <b>309</b> is configured to analyze the collected data signals and to perform feature extraction. Similarly, in some implementations, the data collection computer system <b>309</b> is configured to apply the trained machine-learning model by providing a set of input data (including, for example, some or all of the collected data signals and/or features extracted from the data signals) as input to the trained machine-learning model and then taking appropriate action (e.g., generating notifications to the appropriate devices) in response to the machine-learning model generating an output indicative of a detected precursor.</p><p id="p-0038" num="0037">The multi-modal data collection system of <figref idref="DRAWINGS">FIG. <b>3</b></figref> is only one example. In other implementations, the multi-modal data collection system may include other sensor systems in addition to or instead of those illustrated in the example of <figref idref="DRAWINGS">FIG. <b>3</b></figref>. Conversely, in some implementations, the multi-modal data collection system may include only some, but not all of the sensor systems illustrated in the example of <figref idref="DRAWINGS">FIG. <b>3</b></figref>. For example, in some implementations, the multi-modal data collection system may include a set of wearable motion sensors <b>303</b>, an audio sensor <b>305</b>, and a physiological sensor <b>307</b>, but does not include or utilize video/camera data. Similarly, the example of <figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates a set of examples of types of features that may be extracted from the captured multi-modal data. However, in other implementations, the system may be configured to extract more, fewer, and/or different features from the multi-modal data.</p><p id="p-0039" num="0038">In some implementations, the video/camera-based system <b>301</b> includes a stereoscopic camera system such as, for example, a Microsoft Kinect V2 device. In the example of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the Microsoft Kinect V2 device is configured to detect the facial expressions and head rotations of the subject. The Microsoft Kinect API computes positions of eyes, nose and mouth among different points on the face from its color camera and depth sensor to recognize facial expressions and compute head rotations. In some implementations, the API is integrated with the software executed by the data collection computer system <b>309</b> to read these measurements in C# scripts. In some such implementations, the system is designed to track a first subject that enters the field of view of the Kinect device and detected facial expressions are assigned to one of a plurality of different predetermined categories including, for example, &#x201c;happy,&#x201d; &#x201c;eyes closed,&#x201d; &#x201c;mouth open,&#x201d; &#x201c;looking away,&#x201d; and &#x201c;engaged.&#x201d; All the measures are computed in real-time and vary on a discrete numerical scale that ranges from 0, 0.5 and 1, meaning no, probably, and yes, respectively. The head rotations are measured in terms of roll, pitch and yaw angles of the head. The sampling rates of the head rotations and facial expressions are both 10 Hz and the signals are recorded with time stamps with millisecond precision. The Kinect is placed on the wall by a 3D printed structure which can adjust the pan and tilt angles of the Kinect so that it directly faces the child during observation.</p><p id="p-0040" num="0039">In some implementations, the physiological sensors <b>307</b> are implemented as a wrist-worn device configured with a plurality of sensors/electrodes&#x2014;for example, an E4 wristband. In some implementations, the wristband itself is non-invasive and looks like a smart watch. The sampling rates used by the wrist-worn device to measure blood volume pulse (BVP) and electrodermal activity (EDA) are 64 Hz and 4 Hz, respectively. In some implementations, the wrist-worn device also includes an integrated contact temperatures sensor configured to measure body temperature when the sensor is in contact with the skin surface of the subject. In some implementations, wrist-worn device includes an electronic controller and a wireless transceiver. The electronic controller of the wrist-worn device is configured to record the sensed data with precise time stamps. The real-time physiological data stream is transferred to the data collection computer system <b>309</b> via the wireless transceiver.</p><p id="p-0041" num="0040">In some implementations, the wearable motion sensors <b>303</b> are integrated into a wearable garment and can thereby non-invasively measure body movements and poses when the garment is worn by the subject. <figref idref="DRAWINGS">FIGS. <b>4</b> through <b>5</b>C</figref> illustrates an example of a body movement tracking system (i.e., a &#x201c;wearable intelligent non-invasive gesture sensor&#x201d; or &#x201c;WINGS&#x201d;). The body movement tracking system is a portable, non-invasive tool for measuring upper body motion. The body movement tracking system integrates a plurality of inertial measurement units (IMUs) into a wearable garment such as, for example, a sweater, a shirt, or a vest that resembles clothing that the subject is comfortable wearing. To increase the likelihood that the platform will be tolerated by children with varying levels of activity, sensory sensitivity, and cognitive functioning, the device focuses on comfort, concealment, robustness, and safety.</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates an example of the electronic components of the body movement tracking system. A plurality of inertial measurement units (IMUs) <b>401</b>.<b>1</b>, <b>401</b>.<b>2</b>, <b>401</b>.<b>3</b>, . . . <b>401</b>.<i>n </i>are positioned at locations along the arms and back of the subject. Each IMU <b>401</b> is configured to measure acceleration and magnetic field in three-dimensions. As described in further detail below, the IMUs <b>401</b> are positioned at strategic locations on the arms and torso of the subject such that the data collection computer system <b>309</b> can determine relative position and angles of the body segments in order to generate a model of body movement and position based on the output signals from the plurality of IMUs <b>401</b>. In some implementations, the body movement tracking system includes a total of seven IMUs <b>401</b> to measure joint angles of each forearm, upper arm, and the back. However, in other implementations, the body movement tracking system may include more or fewer IMUs <b>401</b>.</p><p id="p-0043" num="0042">Each IMU is wiredly coupled to a data multiplexer <b>403</b> that is configured to search and loop through the connected IMUs <b>401</b> and to periodically couple each IMU <b>401</b> to the data input channel of a motion sensor system controller <b>405</b>. The motion sensor system controller <b>405</b> includes an electronic processor <b>407</b> and a non-transitory, computer-readable memory <b>409</b>. The motion sensor system controller <b>405</b> is configured to send the motion data via a wireless transmitter <b>411</b> to a wireless received of the data collection computer system <b>309</b>.</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIGS. <b>5</b>A through <b>5</b>B</figref> illustrate an example of a garment configured for use with the body movement tracking system of <figref idref="DRAWINGS">FIG. <b>4</b></figref>. The garment <b>501</b> is a sweatshirt equipped with an upper arm pocket <b>503</b> and a forearm pocket <b>505</b> on each arm and at least one back pocket <b>507</b> on the back. The pockets <b>503</b>, <b>505</b>, <b>507</b> are each sized to hold an individual IMU <b>401</b> and, in some implementations are selectively closable with hook-and-loop fasteners positioned along the opening of each pocket <b>503</b>, <b>505</b>, <b>507</b>. In other implementations, the pockets <b>503</b>, <b>505</b>, <b>507</b> are sewn shut. The garment <b>501</b> is designed such that the pockets <b>503</b>, <b>505</b>, <b>507</b> are discrete and do not stand out. The data multiplexer, the motion sensor system controller <b>405</b>, and the wireless transmitter <b>411</b> are also integrated into the garment <b>501</b> (e.g., in a hood of a hooded sweatshirt or in another pocket). In some implementations, a microphone (e.g., audio sensor <b>305</b>) is also integrated into the garment <b>501</b> (for example, along the neck of the garment <b>501</b>).</p><p id="p-0045" num="0044">From the accelerometer readings (accl<sub>x</sub>, accl<sub>y </sub>and accl<sub>z</sub>) and the magnetometer readings (mag<sub>x</sub>, mag<sub>y </sub>and mag<sub>z</sub>) output from each of the IMUs <b>401</b>, the data collection computer system <b>309</b> is able to compute the roll, pitch and yaw angles (&#x3b8;, &#x3c8;, &#x3d5;) of the torso and limbs according to equations (1), (2) and (3). The roll and pitch angles are computed by the IMU orientations with respect to the gravitational direction. The yaw angle can be computed by the relative IMU orientations with respect to the earth magnetic field.</p><p id="p-0046" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mi>&#x3b8;</mi>     <mo>=</mo>     <mrow>      <msup>       <mi>tan</mi>       <mrow>        <mo>-</mo>        <mn>1</mn>       </mrow>      </msup>      <mo>&#x2062;</mo>      <mtext>  </mtext>      <mrow>       <mo>(</mo>       <mfrac>        <mrow>         <mi>a</mi>         <mo>&#x2062;</mo>         <mi>c</mi>         <mo>&#x2062;</mo>         <mi>c</mi>         <mo>&#x2062;</mo>         <msub>          <mi>l</mi>          <mi>y</mi>         </msub>        </mrow>        <msqrt>         <mrow>          <mrow>           <mi>a</mi>           <mo>&#x2062;</mo>           <mi>c</mi>           <mo>&#x2062;</mo>           <mi>c</mi>           <mo>&#x2062;</mo>           <msubsup>            <mi>l</mi>            <mi>y</mi>            <mn>2</mn>           </msubsup>          </mrow>          <mo>+</mo>          <mrow>           <mi>a</mi>           <mo>&#x2062;</mo>           <mi>c</mi>           <mo>&#x2062;</mo>           <mi>c</mi>           <mo>&#x2062;</mo>           <msubsup>            <mi>l</mi>            <mi>z</mi>            <mn>2</mn>           </msubsup>          </mrow>         </mrow>        </msqrt>       </mfrac>       <mo>)</mo>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>1</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths><maths id="MATH-US-00001-2" num="00001.2"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mi>&#x3c8;</mi>     <mo>=</mo>     <mrow>      <msup>       <mi>tan</mi>       <mrow>        <mo>-</mo>        <mn>1</mn>       </mrow>      </msup>      <mo>&#x2062;</mo>      <mtext>  </mtext>      <mrow>       <mo>(</mo>       <mfrac>        <mrow>         <mi>a</mi>         <mo>&#x2062;</mo>         <mi>c</mi>         <mo>&#x2062;</mo>         <mi>c</mi>         <mo>&#x2062;</mo>         <msub>          <mi>l</mi>          <mi>x</mi>         </msub>        </mrow>        <msqrt>         <mrow>          <msubsup>           <mi>accl</mi>           <mi>y</mi>           <mn>2</mn>          </msubsup>          <mo>+</mo>          <mrow>           <mi>a</mi>           <mo>&#x2062;</mo>           <mi>c</mi>           <mo>&#x2062;</mo>           <mi>c</mi>           <mo>&#x2062;</mo>           <msubsup>            <mi>l</mi>            <mi>z</mi>            <mn>2</mn>           </msubsup>          </mrow>         </mrow>        </msqrt>       </mfrac>       <mo>)</mo>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>2</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths><maths id="MATH-US-00001-3" num="00001.3"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mi>&#x3d5;</mi>     <mo>=</mo>     <mrow>      <msup>       <mi>tan</mi>       <mrow>        <mo>-</mo>        <mn>1</mn>       </mrow>      </msup>      <mo>&#x2062;</mo>      <mtext>  </mtext>      <mrow>       <mo>(</mo>       <mfrac>        <mrow>         <mrow>          <mi>m</mi>          <mo>&#x2062;</mo>          <mi>a</mi>          <mo>&#x2062;</mo>          <msub>           <mi>g</mi>           <mi>z</mi>          </msub>          <mo>&#x2062;</mo>          <mi>s</mi>          <mo>&#x2062;</mo>          <mi>&#x3c8;</mi>         </mrow>         <mo>-</mo>         <mrow>          <mi>m</mi>          <mo>&#x2062;</mo>          <mi>a</mi>          <mo>&#x2062;</mo>          <msub>           <mi>g</mi>           <mi>y</mi>          </msub>          <mo>&#x2062;</mo>          <mi>c</mi>          <mo>&#x2062;</mo>          <mi>&#x3b8;</mi>         </mrow>        </mrow>        <mrow>         <mrow>          <mi>m</mi>          <mo>&#x2062;</mo>          <mi>a</mi>          <mo>&#x2062;</mo>          <msub>           <mi>g</mi>           <mi>x</mi>          </msub>          <mo>&#x2062;</mo>          <mi>c</mi>          <mo>&#x2062;</mo>          <mi>&#x3b8;</mi>         </mrow>         <mo>+</mo>         <mrow>          <mi>m</mi>          <mo>&#x2062;</mo>          <mi>a</mi>          <mo>&#x2062;</mo>          <msub>           <mi>g</mi>           <mi>y</mi>          </msub>          <mo>&#x2062;</mo>          <mi>s</mi>          <mo>&#x2062;</mo>          <mi>&#x3b8;</mi>          <mo>&#x2062;</mo>          <mi>s</mi>          <mo>&#x2062;</mo>          <mi>&#x3c8;</mi>         </mrow>         <mo>+</mo>         <mrow>          <mi>m</mi>          <mo>&#x2062;</mo>          <mi>a</mi>          <mo>&#x2062;</mo>          <msub>           <mi>g</mi>           <mi>z</mi>          </msub>          <mo>&#x2062;</mo>          <mi>c</mi>          <mo>&#x2062;</mo>          <mi>&#x3c8;</mi>          <mo>&#x2062;</mo>          <mi>s</mi>          <mo>&#x2062;</mo>          <mi>&#x3b8;</mi>         </mrow>        </mrow>       </mfrac>       <mo>)</mo>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>3</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0047" num="0045">With the roll, pitch and yaw angles of different joints, data collection computer system <b>309</b> then computes the 3D positions and orientations of each joint using forward kinematics. As shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the base frame is set at the spine base of the subject. The base frame positive directions of the x, y and z axes are front, left directions of the subject and up, respectively. Then coordinate frame s<sub>n </sub>is attached to each body joint. Homogeneous transformation matrices H<sub>Joint n</sub><sup>Joint n-1 </sup>between the nth joint and the last joint consist of two parts, a 3-by3 rotation matrix R<sub>n</sub><sup>n-1 </sup>and a 1-by-3 translation vector d<sub>0</sub><sup>n-1</sup>. The rotation and translation matrices can align and move the previous coordinate frame to the current coordinate frame, respectively. The rotation matrix is computed by roll, pitch and yaw angles while the translation vector is computed by the body link lengths which are manually measured from different sizes of garments <b>501</b> (or measured separately for the specific individual subject). Each homogeneous transformation matrix is computed as equation (4). Then the overall homogeneous transformation matrix H<sub>Joint n</sub><sup>Origin </sup>between the base frame and the nth frame can be computed by multiplying all the homogeneous transformation matrices as in equation (5). From this matrix, d<sub>n</sub><sup>0 </sup>can be read and that is the 3D position of the nth joint position with respect to the base frame.</p><p id="p-0048" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <msubsup>      <mi>H</mi>      <mrow>       <mi>Jo</mi>       <mo>&#x2062;</mo>       <mtext>  </mtext>       <mi>int</mi>       <mo>&#x2062;</mo>       <mtext>  </mtext>       <mi>n</mi>      </mrow>      <mrow>       <mrow>        <mi>Jo</mi>        <mo>&#x2062;</mo>        <mtext>  </mtext>        <mi>int</mi>        <mo>&#x2062;</mo>        <mtext>  </mtext>        <mi>n</mi>       </mrow>       <mo>-</mo>       <mn>1</mn>      </mrow>     </msubsup>     <mo>=</mo>     <mrow>      <mrow>       <mo>[</mo>       <mtable>        <mtr>         <mtd>          <msubsup>           <mi>R</mi>           <mi>n</mi>           <mrow>            <mi>n</mi>            <mo>&#x2062;</mo>            <mn>1</mn>           </mrow>          </msubsup>         </mtd>         <mtd>          <msubsup>           <mi>d</mi>           <mn>0</mn>           <mrow>            <mi>n</mi>            <mo>-</mo>            <mn>1</mn>           </mrow>          </msubsup>         </mtd>        </mtr>        <mtr>         <mtd>          <mover accent="true">           <mn>0</mn>           <mo>&#x2192;</mo>          </mover>         </mtd>         <mtd>          <mn>1</mn>         </mtd>        </mtr>       </mtable>       <mo>]</mo>      </mrow>      <mo>=</mo>      <mrow>       <mrow>        <mo>[</mo>        <mtable>         <mtr>          <mtd>           <mrow>            <msub>             <mi>R</mi>             <mrow>              <mi>x</mi>              <mo>,</mo>              <mi>&#x3c8;</mi>             </mrow>            </msub>            <mo>&#x2062;</mo>            <msub>             <mi>R</mi>             <mrow>              <mi>y</mi>              <mo>,</mo>              <mi>&#x3b8;</mi>             </mrow>            </msub>            <mo>&#x2062;</mo>            <msub>             <mi>R</mi>             <mrow>              <mi>z</mi>              <mo>,</mo>              <mi>&#x3d5;</mi>             </mrow>            </msub>           </mrow>          </mtd>          <mtd>           <msubsup>            <mi>d</mi>            <mn>0</mn>            <mrow>             <mi>n</mi>             <mo>-</mo>             <mn>1</mn>            </mrow>           </msubsup>          </mtd>         </mtr>         <mtr>          <mtd>           <mover accent="true">            <mn>0</mn>            <mo>&#x2192;</mo>           </mover>          </mtd>          <mtd>           <mn>1</mn>          </mtd>         </mtr>        </mtable>        <mo>]</mo>       </mrow>       <mo>=</mo>       <mspace linebreak="newline"/>       <mrow>        <mpadded>         <mo>[</mo>        </mpadded>        <mtable>         <mtr>          <mtd>           <mrow>            <mi>c</mi>            <mo>&#x2062;</mo>            <msub>             <mi>&#x3d5;</mi>             <mi>n</mi>            </msub>            <mo>&#x2062;</mo>            <mi>c</mi>            <mo>&#x2062;</mo>            <msub>             <mi>&#x3b8;</mi>             <mi>n</mi>            </msub>           </mrow>          </mtd>          <mtd>           <mrow>            <mrow>             <mi>c</mi>             <mo>&#x2062;</mo>             <msub>              <mi>&#x3d5;</mi>              <mi>n</mi>             </msub>             <mo>&#x2062;</mo>             <mi>s</mi>             <mo>&#x2062;</mo>             <msub>              <mi>&#x3b8;</mi>              <mi>n</mi>             </msub>             <mo>&#x2062;</mo>             <mi>s</mi>             <mo>&#x2062;</mo>             <msub>              <mi>&#x3c8;</mi>              <mi>n</mi>             </msub>            </mrow>            <mo>-</mo>            <mrow>             <mi>s</mi>             <mo>&#x2062;</mo>             <msub>              <mi>&#x3d5;</mi>              <mi>n</mi>             </msub>             <mo>&#x2062;</mo>             <mi>c</mi>             <mo>&#x2062;</mo>             <msub>              <mi>&#x3c8;</mi>              <mi>n</mi>             </msub>            </mrow>           </mrow>          </mtd>          <mtd>           <mrow>            <mrow>             <mi>s</mi>             <mo>&#x2062;</mo>             <msub>              <mi>&#x3d5;</mi>              <mi>n</mi>             </msub>             <mo>&#x2062;</mo>             <mi>s</mi>             <mo>&#x2062;</mo>             <msub>              <mi>&#x3c8;</mi>              <mi>n</mi>             </msub>            </mrow>            <mo>+</mo>            <mrow>             <mi>c</mi>             <mo>&#x2062;</mo>             <msub>              <mi>&#x3d5;</mi>              <mi>n</mi>             </msub>             <mo>&#x2062;</mo>             <mi>s</mi>             <mo>&#x2062;</mo>             <msub>              <mi>&#x3b8;</mi>              <mi>n</mi>             </msub>             <mo>&#x2062;</mo>             <mi>c</mi>             <mo>&#x2062;</mo>             <msub>              <mi>&#x3c8;</mi>              <mi>n</mi>             </msub>            </mrow>           </mrow>          </mtd>          <mtd>           <msubsup>            <mi>x</mi>            <mi>n</mi>            <mrow>             <mi>n</mi>             <mo>-</mo>             <mn>1</mn>            </mrow>           </msubsup>          </mtd>         </mtr>         <mtr>          <mtd>           <mrow>            <mi>s</mi>            <mo>&#x2062;</mo>            <msub>             <mi>&#x3d5;</mi>             <mi>n</mi>            </msub>            <mo>&#x2062;</mo>            <mi>c</mi>            <mo>&#x2062;</mo>            <msub>             <mi>&#x3b8;</mi>             <mi>n</mi>            </msub>           </mrow>          </mtd>          <mtd>           <mrow>            <mrow>             <mi>s</mi>             <mo>&#x2062;</mo>             <msub>              <mi>&#x3d5;</mi>              <mi>n</mi>             </msub>             <mo>&#x2062;</mo>             <mi>s</mi>             <mo>&#x2062;</mo>             <msub>              <mi>&#x3b8;</mi>              <mi>n</mi>             </msub>             <mo>&#x2062;</mo>             <mi>s</mi>             <mo>&#x2062;</mo>             <msub>              <mi>&#x3c8;</mi>              <mi>n</mi>             </msub>            </mrow>            <mo>+</mo>            <mrow>             <mi>c</mi>             <mo>&#x2062;</mo>             <msub>              <mi>&#x3d5;</mi>              <mi>n</mi>             </msub>             <mo>&#x2062;</mo>             <mi>c</mi>             <mo>&#x2062;</mo>             <msub>              <mi>&#x3c8;</mi>              <mi>n</mi>             </msub>            </mrow>           </mrow>          </mtd>          <mtd>           <mrow>            <mrow>             <mi>s</mi>             <mo>&#x2062;</mo>             <msub>              <mi>&#x3d5;</mi>              <mi>n</mi>             </msub>             <mo>&#x2062;</mo>             <mi>s</mi>             <mo>&#x2062;</mo>             <msub>              <mi>&#x3b8;</mi>              <mi>n</mi>             </msub>             <mo>&#x2062;</mo>             <mi>c</mi>             <mo>&#x2062;</mo>             <msub>              <mi>&#x3c8;</mi>              <mi>n</mi>             </msub>            </mrow>            <mo>-</mo>            <mrow>             <mi>c</mi>             <mo>&#x2062;</mo>             <msub>              <mi>&#x3d5;</mi>              <mi>n</mi>             </msub>             <mo>&#x2062;</mo>             <mi>s</mi>             <mo>&#x2062;</mo>             <msub>              <mi>&#x3c8;</mi>              <mi>n</mi>             </msub>            </mrow>           </mrow>          </mtd>          <mtd>           <msubsup>            <mi>y</mi>            <mi>n</mi>            <mrow>             <mi>n</mi>             <mo>-</mo>             <mn>1</mn>            </mrow>           </msubsup>          </mtd>         </mtr>         <mtr>          <mtd>           <mrow>            <mrow>             <mo>-</mo>             <mi>s</mi>            </mrow>            <mo>&#x2062;</mo>            <msub>             <mi>&#x3b8;</mi>             <mi>n</mi>            </msub>           </mrow>          </mtd>          <mtd>           <mrow>            <mi>c</mi>            <mo>&#x2062;</mo>            <msub>             <mi>&#x3b8;</mi>             <mi>n</mi>            </msub>            <mo>&#x2062;</mo>            <mi>s</mi>            <mo>&#x2062;</mo>            <msub>             <mi>&#x3c8;</mi>             <mi>n</mi>            </msub>           </mrow>          </mtd>          <mtd>           <mrow>            <mi>c</mi>            <mo>&#x2062;</mo>            <msub>             <mi>&#x3b8;</mi>             <mi>n</mi>            </msub>            <mo>&#x2062;</mo>            <mi>c</mi>            <mo>&#x2062;</mo>            <msub>             <mi>&#x3c8;</mi>             <mi>n</mi>            </msub>           </mrow>          </mtd>          <mtd>           <msubsup>            <mi>z</mi>            <mi>n</mi>            <mrow>             <mi>n</mi>             <mo>-</mo>             <mn>1</mn>            </mrow>           </msubsup>          </mtd>         </mtr>         <mtr>          <mtd>           <mn>0</mn>          </mtd>          <mtd>           <mn>0</mn>          </mtd>          <mtd>           <mn>0</mn>          </mtd>          <mtd>           <mn>1</mn>          </mtd>         </mtr>        </mtable>        <mo>]</mo>       </mrow>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>4</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths><maths id="MATH-US-00002-2" num="00002.2"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <msubsup>      <mi>H</mi>      <mrow>       <mi>Jo</mi>       <mo>&#x2062;</mo>       <mtext>  </mtext>       <mi>int</mi>       <mo>&#x2062;</mo>       <mrow>        <mtext> </mtext>        <mtext> </mtext>       </mrow>       <mo>&#x2062;</mo>       <mi>n</mi>      </mrow>      <mi>Origin</mi>     </msubsup>     <mo>=</mo>     <mrow>      <mrow>       <msubsup>        <mi>H</mi>        <mrow>         <mi>Jo</mi>         <mo>&#x2062;</mo>         <mi>int</mi>         <mo>&#x2062;</mo>         <mtext>  </mtext>         <mn>1</mn>        </mrow>        <mi>Origin</mi>       </msubsup>       <mo>&#x2062;</mo>       <mi>&#x25a1;</mi>       <mo>&#x2062;</mo>       <msubsup>        <mi>H</mi>        <mrow>         <mi>Jo</mi>         <mo>&#x2062;</mo>         <mi>int</mi>         <mo>&#x2062;</mo>         <mtext> </mtext>         <mn>2</mn>        </mrow>        <mrow>         <mi>Jo</mi>         <mo>&#x2062;</mo>         <mi>int</mi>         <mo>&#x2062;</mo>         <mtext> </mtext>         <mn>1</mn>        </mrow>       </msubsup>       <mo>&#x2062;</mo>       <mo>&#x2026;</mo>       <mo>&#x2062;</mo>       <mtext>  </mtext>       <msubsup>        <mi>H</mi>        <mrow>         <mi>Jo</mi>         <mo>&#x2062;</mo>         <mtext>  </mtext>         <mi>int</mi>         <mo>&#x2062;</mo>         <mtext> </mtext>         <mi>n</mi>        </mrow>        <mrow>         <mrow>          <mi>Jo</mi>          <mo>&#x2062;</mo>          <mtext>  </mtext>          <mi>int</mi>          <mo>&#x2062;</mo>          <mrow>           <mtext> </mtext>           <mtext> </mtext>          </mrow>          <mo>&#x2062;</mo>          <mi>n</mi>         </mrow>         <mo>-</mo>         <mn>1</mn>        </mrow>       </msubsup>      </mrow>      <mo>=</mo>      <mrow>       <mrow>        <mrow>         <mo>[</mo>         <mtable>          <mtr>           <mtd>            <msubsup>             <mi>R</mi>             <mn>1</mn>             <mn>0</mn>            </msubsup>           </mtd>           <mtd>            <msubsup>             <mi>d</mi>             <mn>1</mn>             <mn>0</mn>            </msubsup>           </mtd>          </mtr>          <mtr>           <mtd>            <mover accent="true">             <mn>0</mn>             <mo>&#x2192;</mo>            </mover>           </mtd>           <mtd>            <mn>1</mn>           </mtd>          </mtr>         </mtable>         <mo>]</mo>        </mrow>        <mo>&#x2062;</mo>        <mrow>         <mo>&#x2026;</mo>         <mo>[</mo>         <mtable>          <mtr>           <mtd>            <msubsup>             <mi>R</mi>             <mi>n</mi>             <mrow>              <mi>n</mi>              <mo>&#x2062;</mo>              <mi>&#x2010;</mi>              <mo>&#x2062;</mo>              <mn>1</mn>             </mrow>            </msubsup>           </mtd>           <mtd>            <msubsup>             <mi>d</mi>             <mi>n</mi>             <mrow>              <mi>n</mi>              <mo>&#x2062;</mo>              <mi>&#x2010;</mi>              <mo>&#x2062;</mo>              <mn>1</mn>             </mrow>            </msubsup>           </mtd>          </mtr>          <mtr>           <mtd>            <mover accent="true">             <mn>0</mn>             <mo>&#x2192;</mo>            </mover>           </mtd>           <mtd>            <mn>1</mn>           </mtd>          </mtr>         </mtable>         <mo>]</mo>        </mrow>       </mrow>       <mo>=</mo>       <mrow>        <mo>[</mo>        <mtable>         <mtr>          <mtd>           <msubsup>            <mi>R</mi>            <mi>n</mi>            <mn>0</mn>           </msubsup>          </mtd>          <mtd>           <msubsup>            <mi>d</mi>            <mi>n</mi>            <mn>0</mn>           </msubsup>          </mtd>         </mtr>         <mtr>          <mtd>           <mover accent="true">            <mn>0</mn>            <mo>&#x2192;</mo>           </mover>          </mtd>          <mtd>           <mn>1</mn>          </mtd>         </mtr>        </mtable>        <mo>]</mo>       </mrow>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>5</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0049" num="0046">Thus, we have the 3D positions of each body joint and we can construct and visualize body gestures. In some implementations, a software program (e.g., a MATLAB program) is configured to visualize the upper body gestures in real time. A comparison of a visualized gesture and a corresponding photograph of the subject is shown in <figref idref="DRAWINGS">FIGS. <b>7</b>A and <b>7</b>B</figref>. The lines in <figref idref="DRAWINGS">FIG. <b>7</b>A</figref> represent body limbs while the dots represent body joints.</p><p id="p-0050" num="0047">As described above in reference to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the machine-learning model that is used to detect precursors is trained during performance of a behavioral assessment protocol such as, for example, the IISCA. The IISCA usually requires observers to record the occurrence of precursors of problem behaviors by paper and pen while timestamping events via a stopwatch in order to record the conditions under which target behaviors were observed. This method requires significant attention on the part of the behavioral analyst and is limited in precision with regard to recording the onset time of precursors.</p><p id="p-0051" num="0048"><figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates an example of a graphical user interface for a software application configured to be used by a behavioral analyst observing the IISCA. In particular, the software application is designed to be run on a portable device such as, for example, a tablet computer. The software application includes three different operating modes: Initialization, Session Information, and Summary. The graphical user interface <b>801</b> illustrated in <figref idref="DRAWINGS">FIG. <b>8</b></figref> is for use during the &#x201c;Session Information&#x201d; phase. In some implementations, other graphical user interface pages are displayed during the Initialization and Summary phases while, in other implementations, relevant information for the Initialization and Summary phases may be displayed in different windows (e.g., pop-up windows).</p><p id="p-0052" num="0049">During the IISCA, there are two therapist-imposed conditions within the assessment protocol: establishing operations (EO) and reinforcing stimulus (SR). Establishing Operations (EO) represent those antecedent conditions reported to evoke behavioral escalation by care givers. Reinforcing Stimulus (SR) (or &#x201c;Synthesized Reinforcement&#x201d;) represent intervals in which antecedent conditions are arranged to prevent, de-escalate, and restore a state of happy and relaxed engagement in the subject.</p><p id="p-0053" num="0050">The graphical user interface <b>801</b> for the Session phase includes several buttons relating to observer actions and observed subject behaviors. A series of buttons are provided to provide an interface for assessing the establishing operations and the response of the subject. For example, the behavioral analysts presses the &#x201c;Easy EO Demand&#x201d; button <b>803</b> when an &#x201c;easy&#x201d; EO demand is initiated and presses the &#x201c;Easy EO Comply&#x201d; button <b>805</b> when the subject complies with the EO demand. Similarly, a &#x201c;Hard EO Demand&#x201d; button <b>807</b> is pressed with a more difficult/challenging EO demand is initiated and the &#x201c;Hard EO Comply&#x201d; button <b>809</b> is pressed when the subject is observed as complying with the EO demand. The graphical user interface <b>801</b> also includes numeric fields <b>811</b>, <b>813</b> displaying the number of EO conditions under which the subject has successfully complied without exhibiting the problem behavior. The graphical user interface <b>801</b> also allows the behavioral analyst to toggle between the two conditions (i.e., move between establishing operations and reinforcing stimulus) by clicking the EO button <b>815</b> or the SR button <b>817</b> (as described in further detail below).</p><p id="p-0054" num="0051">The graphical user interface <b>801</b> also provides a series of buttons via which the behavior analyst can record observations of the behavioral state of the subject. A &#x201c;Problematic Behavior&#x201d; button <b>821</b> is to be pressed when problem behavior is observed and a numeric field <b>823</b> indicates the number of occurrences of the problem behavior during the current IISCA session. Similarly, a &#x201c;Pre-Cursor&#x201d; button <b>825</b> is to be pressed when precursor behavior is observed and a corresponding numeric field <b>827</b> indicates the number of occurrences of precursor behavior during the current IISCA session. A &#x201c;CALM&#x201d; button <b>829</b> is to be pressed to indicate when the subject is observed to be in a &#x201c;calm&#x201d; behavioral state (i.e., no problem or precursor behaviors).</p><p id="p-0055" num="0052">Finally, a &#x201c;NEXT SESSION&#x201d; button <b>831</b> is provided to be pressed by the user at the end of each IISCA session and an &#x201c;END&#x201d; button <b>833</b>&#x201d; is provided to be pressed to when all IISCA sessions are completed for the subject. As described further below, pressing the &#x201c;NEXT SESSION&#x201d; button <b>831</b> causes the software application to generate and display summary data for the individual IISCA session and pressing the &#x201c;END&#x201d; button <b>833</b> causes the software application to generate and display summary data for all of the IISCA sessions and closes the &#x201c;Session&#x201d; graphical user interface <b>801</b>.</p><p id="p-0056" num="0053">In some implementations, the software application operates as a finite state machine (FSM) that captures the IISCA protocol as illustrated in <figref idref="DRAWINGS">FIG. <b>9</b></figref>. The software application starts in the initialization state <b>901</b>. After entering the appropriate session and subject information through the graphical user interface, the user presses a &#x201c;Start&#x201d; button and the software application proceeds to the Synthesized Reinforcement (SR) phase <b>905</b>. The software application remains in the SR phase <b>905</b> until the user presses the CALM button <b>829</b> indicating that the subject is observed to be in a calm behavioral state. In response to the pressing of the CALM button <b>829</b>, the software application moves to a ticking phase <b>911</b> in which a timer monitors to make sure that the child is truly happy, relaxed, and engaged for at least 90 continuous seconds. After expiration of the 90 second timer, the software application moves to an EO Ready phase <b>913</b> in which the graphical user interface <b>801</b> indicates that the establishing operations conditions may be applied. In some implementations, this is indicated by displaying the EO Button <b>815</b> in a different color (e.g., green when in the EO Ready phase <b>913</b> and red when in the Ticking phase <b>911</b>).</p><p id="p-0057" num="0054">If precursor behavior is observed (e.g., as indicated by a user pressing the Pre-Cursor button <b>825</b>) while in the ticking phase <b>911</b> or the EO ready phase <b>913</b>, the software application returns to the SR phase <b>905</b>. Otherwise, if the user presses the EO button <b>815</b> while the software application is in the EO Ready phase <b>913</b>, the software application moves into the Establishing Operations (EO) phase <b>903</b>. Upon pressing the EO button <b>815</b>, the behavior analyst and/or a therapist are to begin the applicable Establishing Operation. While operating in the EO phase <b>903</b>, if the precursor button <b>825</b> is pressed, a precursor event is recorded (with a timestamp) and the software application moves into a second ticking phase <b>907</b> in which a 30 second countdown is provided before the software application moves into an SR Ready phase <b>909</b> and provides an indication via the graphical user interface of readiness for the SR condition.</p><p id="p-0058" num="0055">The software application guides the user (e.g., the behavior analyst) between the EO state and the SR state in this way until the assessment session is finished. When each individual assessment session is completed, the software application moves into a &#x201c;generate summary&#x201d; phase <b>915</b>. If more sessions are to be conducted, the user presses the &#x201c;Next Session&#x201d; button <b>831</b> and the software application returns to the SR phase <b>905</b> for the next session. When all sessions are competed, the user presses the END button <b>833</b> and the software application moves into the end phase <b>917</b>.</p><p id="p-0059" num="0056">In order to collect training data, a pilot study was conducted with 7 children with ADS aged 10 to 15 years old (6 boys, 1 girl; mean age=12.20 years, SD=1.37). The protocol was reviewed and approved by the Institutional Review Board (IRB) at Vanderbilt University. The research team members explained the purpose, protocols, and any potential risks of the experiment to both the parents and the children and answered all their question before seeking informed consent from the parents and informed assents from the children. Because the purpose of the study was to evoke and respond to precursor to problem behaviors and prevent escalation to dangerous problem behaviors, parents and a dedicated behavior analyst data collector observed the assessment sessions to ensure that all precursors and problem behaviors as well as the target affect of happy, relaxed, and engaged were correctly recorded. All participants completed all of their experiments.</p><p id="p-0060" num="0057">The pilot study was conducted in a child-proof room having two compartments: an experimental space and an observation space. The child sits in the experimental space with a BCBA (Board certified behavioral analyst). The seat for children is 2 meters away from the Kinect and a video camera. The child wears an E4 sensor on the non-dominant wrist and the body motion sensor system of <figref idref="DRAWINGS">FIG. <b>4</b></figref> on the upper body. Four observers including an engineer, one of the parents, a behavior data collector and a behavior assessment manager are seated in the observation space, which has a one-way mirror towards the experimental space. The observers and the parent can see the therapist and the child, but not the other way around.</p><p id="p-0061" num="0058">The child is first invited to the experimental space by the therapist. Then we close the door between the two spaces. The therapist puts the E4 sensor on the wrist of the child and helps him or her wear the body motion sensor garment <b>501</b>. Meanwhile, the parent and the other observers enter the observation room. Afterwards, the Kinect calibration is performed when the therapist is out of the view of the Kinect. Therefore, the Kinect recognizes the body ID of the child and only tracks data from the child. The therapist will be recognized as a different body ID and therefore does not interfere with the data collection. Each experiment lasted for approximately one hour.</p><p id="p-0062" num="0059">The experiment followed a modified version of the IISCA protocol. We conducted multiple therapeutic sessions in a single experiment visit to capture data on different behavioral states. These sessions are labeled as control (C) and test (T). The sessions are structured as CTCTT, which is a classical design for single subject research. The control sessions contain only SR conditions and the test sessions alternate between EO and SR presentations. EO is followed by SR and EO is applied once again after at least 30 seconds have elapsed during which the children stay calm. During EO presentations, the therapist simulates the antecedent conditions that were most likely to evoke precursors and problem behaviors. These tasks were reported by parents in the open-ended interview and include asking them to complete homework assignments, withdrawing preferred attention, and removing preferred toys or electronics. During SR condition presentations, the therapist offers free access to their favorite toys and electronics, removes all demands of letting them work and the work-related materials, and provides them with the kinds of attention reported to be preferred. Parents of the children observed from behind the one-sided mirror, watch the behaviors of the child, and give feedback to the data collector and manager who verified the occurrence of precursors or problem behaviors and the presence or absence of a calm state.</p><p id="p-0063" num="0060">All 7 children completed the entire experiments. The average duration of each experimental session was 54.2 minutes (min=36.5 minutes, max=63.1 minutes, SD=11.5 minutes), with time variability across children largely due to differences in how long it took for each child to calm down during SR sessions. The body motion sensor garment is the most invasive component in the platform; 6 out of 7 children tolerated it without problem. Some children even put garment <b>501</b> on themselves. The only child who did not tolerate the garment <b>501</b> the entire time put it on at the beginning and then decided to take it off after 15 minutes because he was very touch sensitive.</p><p id="p-0064" num="0061">The other platform component that one had to wear, the E4 wristband, was less invasive and tolerated well by all children. With regard to staying within the view of the Kinect, 1 child was unable to stay seated at the table throughout the entire experiment and instead spent some time on the floor with toys and thus the Kinect was not able to track the child for the entire duration of the experiment.</p><p id="p-0065" num="0062">Multi-modal data was collected data using the systems and methods described above. Movement data collected at a sampling rate of 15 Hz. Raw signals for accelerations contain sensing noises and so a low-pass filter was applied with a cut-off frequency of 10 Hz. The threshold was chosen according to the usual speed of human motions so that the noises were filtered out while keeping information-rich signals for analysis. Peripheral physiological data including BVP and EDA were collected with the sampling rates of 64 Hz and 4 Hz, respectively. BVP signals were filtered by a 1 Hz to 8 Hz band pass filter. The EDA signals were not filtered. There were some missing data entries in the head rotations and facial expressions modalities. An interpolation algorithm was used to fill the sparse data. For the missing data points, we assign the numerical mean value of the 20 closest available head rotations and the most frequent class among the 20 closest available facial expressions, respectively.</p><p id="p-0066" num="0063">From the processed and filtered data, different features were selected and extracted. Pitch, roll and yaw angles were computed as shown in the equations (1)-(3), above, to construct the upper body gestures. Certain patterns of gestures are significantly related to problem behaviors and gestures are an important indicator for it. Common gestures observed during problem behaviors include fist throwing, upper body swinging, laying back and repetitive arm movements. Besides gestures, average magnitude of accelerations of each joint can also be used as a measure of activity level as shown in equation (6). The activity level indicates the intensity of physical movements of the children. Fast and repetitive movements such as throwing fist and fidgeting are a common category of problem behaviors and these movements have increased activity level.</p><p id="p-0067" num="0000"><maths id="MATH-US-00003" num="00003"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mi>A</mi>      <mo>&#x2062;</mo>      <mi>L</mi>     </mrow>     <mo>=</mo>     <mfrac>      <mrow>       <munderover>        <mo>&#x2211;</mo>        <mrow>         <mi>i</mi>         <mo>=</mo>         <mn>1</mn>        </mrow>        <mi>n</mi>       </munderover>       <msqrt>        <mrow>         <msubsup>          <mi>a</mi>          <msub>           <mi>x</mi>           <mi>i</mi>          </msub>          <mn>2</mn>         </msubsup>         <mo>+</mo>         <msubsup>          <mi>a</mi>          <msub>           <mi>y</mi>           <mi>i</mi>          </msub>          <mn>2</mn>         </msubsup>         <mo>+</mo>         <msubsup>          <mi>a</mi>          <msub>           <mi>z</mi>           <mi>i</mi>          </msub>          <mn>2</mn>         </msubsup>        </mrow>       </msqrt>      </mrow>      <mn>7</mn>     </mfrac>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>6</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0068" num="0064">Peripheral physiological data is a strong indicator of stress and several features including heart rate (HR) and skin conductance have been shown to have correlations with problem behaviors. Thus, we computed HR by inter-beat-intervals (IBI) of the BVP signal. From the EDA data, we separated and characterized the data into two types, which were tonic skin conductance level (SCL) and phasic skin conductance response (SCR). These features carry information about the stress level of a person.</p><p id="p-0069" num="0065">Head banging is a very frequent problem behaviors reported for children with ASD who exhibit problem behaviors. Measures of head rotations can be used to predict head banging. Thus roll, pitch and yaw angles of the head were chosen a features. From facial expressions, we extracted features of closing of eyes and mouths, engagement, looking away and happy. Children often show their frustration and anger on their faces and facial expressions can help locate more insidious precursors.</p><p id="p-0070" num="0066">Once all the features were extracted, we then synchronized the different data modalities with varying updating frequencies. Motion data modality had the highest updating frequency and we interpolated other features into each motion data interval to form multimodal data entries. First, the algorithm catches the next motion data entry. Secondly, the algorithm computes the relative time from the beginning of the experiment by its time stamp. Thirdly, the algorithm catches the most recent available values on other data modalities before that relative time point. Finally, the algorithm interpolates these values into the motion data entry. In this way, each of the multimodal data entry has all the features synchronized.</p><p id="p-0071" num="0067">The software application of <figref idref="DRAWINGS">FIGS. <b>8</b> and <b>9</b></figref> provides the time stamps of precursors of problem behaviors captured by the observers. With these time stamps, we can assign classes to each multimodal data entry between absence and presence of imminent precursor. With the insight from our IISCA practitioners we chose the data from 90 seconds prior to the episode of precursor to the point of precursor generation to be the most representative for precursors of problem behaviors. Thus, for the multimodal data entry, if the data entry was collected within 90 seconds prior to the precursor we assigned it the label 1. Otherwise, the class was assigned label 0. The two classes 0 and 1 have an average ratio of 6:4. For our experiment, each child had an average of 27242 samples.</p><p id="p-0072" num="0068">In this experiment, individualized models were trained (based on data for each individual subject) and group models were also trained (based on aggregated data for all subjects). The individualized models were built with data of each child to adapt personal behavioral characters with better accuracy. The group models were built with data from all the children to explore the general group behavioral pattern. In order to find the most accurate ML algorithm, we explored several standard ML algorithms with our datasets. We used the library scikit-learn on Jupyter Notebook. The samples were randomly divided into training and test sets with a ratio of 80 to 20. Then we ran a 5-fold cross validation to compute the accuracies of each algorithm.</p><p id="p-0073" num="0069">For individualized models, Random Forest (RF), k Nearest Neighbors (kNN), Decision Tree (DT) and Neural Network (NN) were found to have high prediction accuracies while Support Vector Machine (SVM), Discriminant Analysis (DA) and Naive Bayes (NB) have comparatively lower accuracies. For group models, RF, kNN, and DT also show high accuracies while DA has significantly lower accuracy. SVM, NB and NN have higher prediction accuracies in group models as compared to individual models. Group model has the samples from all participants combined and the increased sample size would help the prediction to be more precise, but it may lose track of data pattern of individualized behaviors. Based on our observation, the problem behaviors of each child can differ significantly. This trade-off may be the reason that some models still have high accuracies for group models or even better performs while others have lower accuracies. The RF individualized model has the best average prediction accuracy.</p><p id="p-0074" num="0070">The RF algorithm offers estimates of importance of each feature. The feature importance is computed as the total decrease in node impurity weighted by the probability of reaching that node. We also analyzed the relative importance of motion-based, physiology-based, and facial expression-based features. The features included were motion signals of each joint, physiological signals, head rotations and facial expressions. The results are consistent with the experimental observations. Main types of problem behaviors include the banging of torso and movement of right arm is more informative because it is the dominant side. Head Rotations has an importance of 0.0689 and Physiological data has an importance of 0.0331 including both HR and EDA. The Facial Expressions modality only has an importance of 0.0023.</p><p id="p-0075" num="0071">We also analyzed the performance of the prediction algorithm when only motion-based and when only physiology-based features were used. We used the best performing algorithm RF to learn the data pattern of each child. The average prediction accuracies for the multimodal model, physiological data only model and motion data only model were 98.16%, 86.88% and 91.63%, respectively.</p><p id="p-0076" num="0072">As mentioned earlier, we assigned the class of imminent precursor when the data was collected within the last 90 seconds prior to the observed precursor. To analyze the effect of the time window on the prediction of precursors, we varied the time window for the class of imminent precursors from 30 seconds prior to the observed precursor to 120 seconds in steps of 30 seconds. To avoid effects of different ratio of classes, we resample the minor class so that the two classes have a 1:1 ratio. Additionally, during the experiments, we observed that some of the problem behaviors needed some time to cool down after the observed precursors because it took some time for our therapists to intervene and for the child to calm down. Therefore, we also explored assigning the class of imminent precursor during a 10 seconds delay time after the observed precursor. As shown in <figref idref="DRAWINGS">FIG. <b>16</b></figref>, prediction accuracies for 30, 60, 90 seconds do not have significant differences but it significantly decreases for the 120 second time window. Also the models with the cool down window of 10 seconds had an average prediction accuracy increase of 0.06%. This analysis validates that the 90 seconds window seems to be the optimal window for precursor prediction that has good prediction accuracy with ample time for the caregivers to intervene.</p><p id="p-0077" num="0073">Accordingly, the methods and system described herein provide, among other things, mechanisms for automatically detect precursor conditions for problem behavior based on multimodal data signals prior to the occurrence of the problem behavior using a trained machine-learning model. Other features and advantages of the invention are set forth in the following claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001 MATH-US-00001-2 MATH-US-00001-3" nb-file="US20230000423A1-20230105-M00001.NB"><img id="EMI-M00001" he="23.96mm" wi="76.20mm" file="US20230000423A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002 MATH-US-00002-2" nb-file="US20230000423A1-20230105-M00002.NB"><img id="EMI-M00002" he="38.78mm" wi="76.20mm" file="US20230000423A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00003" nb-file="US20230000423A1-20230105-M00003.NB"><img id="EMI-M00003" he="10.58mm" wi="76.20mm" file="US20230000423A1-20230105-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A system for predicting problem behavior in individuals with developmental and behavior disabilities, the system comprising:<claim-text>a wearable upper body motion sensing device including a plurality of inertial measurement units positions at known positioned relative to each other; and</claim-text><claim-text>an electronic controller configured to<claim-text>receive output signals from each of the plurality of inertial measurement units while the wearable upper body motion sensing device is worn by a subject individual,</claim-text><claim-text>model an upper body position of the subject individual based on the output signals from the plurality of inertial measurement units,</claim-text><claim-text>apply a trained machine-learning model by providing, as input to the trained machine-learning model, an input data set that includes at least one selected from a group consisting of signal data from at least one inertial measurement unit, the upper body position information, and a sequence of upper body position information,<claim-text>wherein the trained machine-learning model is trained to produce as output, in response to receiving the input data set, an indication of whether a precursor to the problem behavior is detected, and</claim-text></claim-text><claim-text>generate a notification indicating that the precursor to the problem behavior is detected in response to the trained machine-learning model producing an output indicating that the precursor to the problem behavior has been detected.</claim-text></claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the wearable upper body motion sensing device further includes a wearable upper body garment, wherein each of the plurality of inertial measurement units is affixed to the wearable upper body garments at positions such that, when the wearable upper body garment is worn by the subject individual the plurality of inertial measurement units includes a first inertial measurement unit positioned on a right forearm of the subject individual, a second inertial measurement unit positioned on a left forearm of the subject individual, a third inertial measurement unit positioned on a right upper arm of the subject individual, a fourth inertial measurement unit positioned on a left upper arm of the subject individual, and a fifth inertial measurement unit positioned on a back of the subject individual.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The system of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the wearable upper body garment includes at least one selected from a group consisting of a shirt, a crewneck sweatshirt, and a hooded sweatshirt.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The system of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the wearable upper body garment includes the hooded sweatshirt and wherein cabling and electronic circuitry for the wearable upper body motion sensing device is positioned within a hood of the hooded sweatshirt.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The system of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the wearable upper body garment includes a plurality of pockets positioned at different locations on the wearable upper body garment, wherein each pocket of the plurality of pockets is sized to receive a different individual inertial measurement unit of the plurality of inertial measurement units.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The system of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein each pocket of the plurality of pocket is selectively closeable by a hook-and-loop closure material.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the system further includes at least one physiological sensor,<claim-text>wherein the electronic controller if further configured to receive output signals from the at least one physiological sensor, and</claim-text><claim-text>wherein the electronic controller is configured to applying the trained machine-learning model by providing, as input to the trained machine-learning model, an input data set that further includes at least one selected from a group consisting of signal data from the at least one physiological sensor and biometric feature metrics extracted from the signal data from the at least one physiological sensor.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The system of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the electronic controller is further configured to extract at least one biometric feature from the signal data from the at least one physiological sensor, the least one biometric feature being selected from a group consisting of a bioimpedance, a heart rate, a skin conductance, and a body temperature.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The system of <claim-ref idref="CLM-00007">claim 7</claim-ref>, further comprising a wrist worn device including the at least one physiological sensor, wherein the wrist worn device has an appearance of a wristwatch.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising a microphone, wherein the electronic controller is configured to receive output signals from the microphone and to extract at least one audio feature from the audio data, the at least one audio feature being selected from a group consisting of individual words, individual phrases, a change in verbal pitch of the subject individual, and a change in verbal volume of the subject individual, and wherein the electronic controller is configured to applying the trained machine-learning model by providing, as the input to the trained machine-learning model, an input data set that further includes at least one selected from a group consisting of the audio data and the at least one audio feature extracted from the audio data.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the electronic controller is configured to generate the notification indicating that the precursor to the problem behavior is detected by causing a portable device associated with at least one individual to output the notification, wherein the at least one individual is selected from a group consisting of the subject individual, a parent of the subject individual, a caregiver of the subject individual, a teacher of the subject individual, and a therapist of the subject individual.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the electronic controller is configured to generate the notification indicating that the precursor to the problem behavior is detected by causing a portable device associated with the subject individual the notification indicating that the precursor has been detected and identifying a recommended mitigation to prevent the problem behavior from occurring.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the electronic controller is further configured to<claim-text>receive a first user input indicative of an occurrence of a false positive and a second user input indicative of an occurrence of a false negative, wherein the first user input indicates that the machine-learning model determined that a precursor had been detected when not precursor or problem behavior was observed by the user, wherein the second user input indicates that the machine learning model did not determine that the precursor had been detected when precursor or problem behavior was observed by the user, and</claim-text><claim-text>retrain the machine-learning model by determining a ground truth label based on the first user input or the second user input and mapping the new ground truth label to signal data from plurality of inertial measurement units captured at a same time as the first user input or the second user input is received.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. A method of training the machine-learning model used in the system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, the method comprising:<claim-text>equipping the subject individual with the wearable upper body motion sensing device;</claim-text><claim-text>administering a behavior assessment while recording signal data from the plurality of inertial measurement units, wherein administering the behavioral assessment includes a controlled sequence of actions in which the subject individual is exposed to stimulus that may provoke the problem behavior and the precursor;</claim-text><claim-text>receiving, from a user through a computer-based user device, real-time indications of observed occurrences of the problem behavior and observed occurrences of the precursor while the behavior assessment is administered;</claim-text><claim-text>generating a set of training data by<claim-text>defining a plurality of ground truth labels based on the received real-time indications of the observed occurrences of the problem behavior and the observed occurrences of the precursor, and</claim-text><claim-text>mapping each real-time indication to recorded signal data from the plurality of inertial measurement units; and</claim-text></claim-text><claim-text>training the machine-learning model to produce an output corresponding to the ground truth label in response to receiving as input at least one selected from a group consisting of the signal data from at least one inertial measurement unit mapped to the ground truth label, upper body position information determined based on the signal data mapped to the ground truth label, and a sequence of upper body position information determined based on the signal data mapped to the ground truth label.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein administering the behavior assessment includes administering an interview-informed synthesized contingency analysis (IISCA).</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein training the machine-learning model includes training an individualized machine-learning model based only on ground truth labels and recorded signal data for the subject individual.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein training the machine-learning model includes training a group machine-learning model based on aggregated ground truth labels and recorded signal data for each of a plurality of subject individuals.</claim-text></claim></claims></us-patent-application>