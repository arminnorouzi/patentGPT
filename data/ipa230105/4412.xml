<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004413A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004413</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17367118</doc-number><date>20210702</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>455</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>45558</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc></classifications-cpc><invention-title id="d2e43">DISTRIBUTED AUTONOMOUS LIFECYCLE MANAGEMENT OF HYPERVISORS IN A VIRTUALIZED COMPUTING SYSTEM</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="obligated-assignee"><addressbook><orgname>VMware, Inc.</orgname><address><city>Palo Alto</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>KAILA</last-name><first-name>Ashish</first-name><address><city>Cupertino</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>SUNDRIYAL</last-name><first-name>Suresh</first-name><address><city>Mountain View</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>GUNTI</last-name><first-name>Mukund</first-name><address><city>Sunnyvale</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">An example method of hypervisor lifecycle management in a virtualized computing system having a cluster of hosts is described. The method includes: obtaining, by remediation software executing in a host of the hosts, a host state document from a distributed key-value store, the host state document defining a desired state of software in the host, the software including a hypervisor; and performing, by the remediation software in coordination with other hosts of the hosts through the distributed key-value store, a lifecycle operation on the software of the host in response to determining that a current state of the software does not match the desired state.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="109.22mm" wi="158.75mm" file="US20230004413A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="241.05mm" wi="170.69mm" orientation="landscape" file="US20230004413A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="154.77mm" wi="125.22mm" file="US20230004413A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="238.08mm" wi="163.49mm" file="US20230004413A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="241.89mm" wi="151.81mm" file="US20230004413A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><p id="p-0002" num="0001">Applications today are deployed onto a combination of virtual machines (VMs), containers, application services, and more within a software-defined datacenter (SDDC). The SDDC includes a server virtualization layer having clusters of physical servers that are virtualized and managed by virtualization management servers. Each host includes a virtualization layer (e.g., a hypervisor) that provides a software abstraction of a physical server (e.g., central processing unit (CPU), random access memory (RAM), storage, network interface card (NIC), etc.) to the VMs. A virtual infrastructure administrator (&#x201c;VI admin&#x201d;) interacts with a virtualization management server to create server clusters (&#x201c;host clusters&#x201d;), add/remove servers (&#x201c;hosts&#x201d;) from host clusters, deploy/move/remove VMs on the hosts, deploy/configure networking and storage virtualized infrastructure, and the like. The virtualization management server sits on top of the server virtualization layer of the SDDC and treats host clusters as pools of compute capacity for use by applications.</p><p id="p-0003" num="0002">A hypervisor lifecycle includes patching and upgrading the base operating system (OS), patching and upgrading the installed software, and managing the configuration of the hypervisor. It is desirable to perform these operations in a manner such that the VMs running on the hypervisor are not affected. A hypervisor can include a maintenance mode where the VMs running thereon are migrated to other host(s) in the cluster with spare capacity, which frees the hypervisor to be patched/upgraded/configured without affecting VM operations. Apart from VMs, there are other considerations, such as availability constraints of distributed storage solutions that add additional constraints on the number of hypervisors in the cluster that can concurrently be in the maintenance mode.</p><p id="p-0004" num="0003">in order to meet all these constraints, a virtualized computing system can include an external coordination engine to select candidate hosts that can be remediated concurrently and determine the order in which the hosts are remediated. Such external coordination engines, however, become a central point of failure. In addition, such external coordination engines fail to scale as the number of hosts in the cluster increases.</p><heading id="h-0001" level="1">SUMMARY</heading><p id="p-0005" num="0004">An example method of hypervisor lifecycle management in a virtualized computing system having a cluster of hosts is described. The method includes: obtaining, by remediation software executing in a host of the hosts, a host state document from a distributed key-value store, the host state document defining a desired state of software in the host, the software including a hypervisor; and performing, by the remediation software in coordination with other hosts of the hosts through the distributed key-value store, a lifecycle operation on the software of the host in response to determining that a current state of the software does not match the desired state.</p><p id="p-0006" num="0005">Further embodiments include a non-transitory computer-readable storage medium comprising instructions that cause a computer system to carry out the above methods, as well as a computer system configured to carry out the above methods.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0002" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram of a virtualized computing system in which embodiments described herein may be implemented.</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram depicting a software platform according to an embodiment.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flow diagram depicting a method of autonomous lifecycle management for a hypervisor according to an embodiment.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flow diagram depicting a method of remediation of a host according to an embodiment.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0003" level="1">DETAILED DESCRIPTION</heading><p id="p-0011" num="0010">Distributed autonomous lifecycle management of hypervisors in a virtualized computing system is described. In embodiments, the techniques described herein remove the dependence on an external coordination engine to perform hypervisor lifecycle management. The techniques allow the hosts in the cluster to autonomously coordinate and perform their own hypervisor lifecycle operations. In embodiments, the techniques leverage a distributed key-value store (DKVS) in the host cluster to coordinate the hypervisor lifecycle operations. The DKVS provides high availability, redundancy, and fault tolerance, which allows these operations to scale with the number of hosts in the cluster. In embodiments, the hypervisor software and its configuration are specified using a declarative human-readable form that can be formulated by a user in a state document. In embodiments, the remediation logic for hyper-visor lifecycle operations is disposed in the hosts themselves. Given a desired state document, a host can remediate itself to conform to the specified state without interaction with an external coordination engine. In embodiments, the information about which hosts have which software images available is stored in the DKVS. This allows hosts to download software images without using a centralized repository. Rather, the hosts can use a peer-to-peer mechanism to maximize parallelism and increase throughput of image downloads. These and further aspects of the techniques are described below with respect to the drawings.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram of a virtualized computing system <b>100</b> in which embodiments described herein may be implemented. System <b>100</b> includes a cluster of hosts <b>120</b> (&#x201c;host cluster <b>118</b>&#x201d;) that may be constructed on server-grade hardware platforms such as an x86 architecture platforms. For purposes of clarity, only one host cluster <b>118</b> is shown. However, virtualized computing system <b>100</b> can include many of such host clusters <b>118</b>. As shown, a hardware platform <b>122</b> of each host <b>120</b> includes conventional components of a computing device, such as one or more central processing units (CPUs) <b>160</b>, system memory (e.g., random access memory (RAM) <b>162</b>), one or more network interface controllers (NICs) <b>164</b>, and optionally local storage <b>163</b>. CPUs <b>160</b> are configured to execute instructions, for example, executable instructions that perform one or more operations described herein, which may be stored in RAM <b>162</b>. NICs <b>164</b> enable host <b>120</b> to communicate with other devices through a physical network <b>180</b>. Physical network <b>180</b> enables communication between hosts <b>120</b> and between other components and hosts <b>120</b> (other components discussed further herein). Physical network <b>180</b> can include a plurality of VLANs to provide external network virtualization as described further herein.</p><p id="p-0013" num="0012">In the embodiment illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, hosts <b>120</b> access shared storage <b>170</b> by using NICs <b>164</b> to connect to network <b>180</b>. In another embodiment, each host <b>120</b> contains a host bus adapter (HBA.) through which input/output operations (IOs) are sent to shared storage <b>170</b> over a separate network (e.g., a fibre channel (FC) network). Shared storage <b>170</b> include one or more storage arrays, such as a storage area network (SAN), network attached storage (NAS), or the like. Shared storage <b>170</b> may comprise magnetic disks, solid-state disks (SSDs), flash memory, and the like as well as combinations thereof. In some embodiments, hosts <b>120</b> include local storage <b>163</b> (e.g., hard disk drives, solid-state drives, etc.). Local storage <b>163</b> in each host <b>120</b> can be aggregated and provisioned as part of a virtual SAN (VSAN), which is another form of shared storage <b>170</b>. Virtualization management server <b>116</b> can select which local storage devices in hosts <b>120</b> are part of a vSAN for host duster <b>118</b>.</p><p id="p-0014" num="0013">A software platform <b>124</b> of each host <b>120</b> provides a virtualization layer, referred to herein as a hypervisor <b>150</b>, which directly executes on hardware platform <b>122</b>. In an embodiment, there is no intervening software, such as a host operating system (OS), between hypervisor <b>150</b> and hardware platform <b>122</b>, Thus, hypervisor <b>150</b> is a Type-1 hypervisor (also known as a &#x201c;bare-metal&#x201d; hypervisor <b>9</b>. As a result, the virtualization layer in host duster <b>118</b> (collectively hypervisors <b>150</b>) is a bare-metal virtualization layer executing directly on host hardware platforms. Hypervisor <b>150</b> abstracts processor, memory, storage, and network resources of hardware platform <b>122</b> to provide a virtual machine execution space within which multiple virtual machines (VM) <b>140</b> may be concurrently instantiated and executed. One example of hypervisor <b>150</b> that may be configured and used in embodiments described herein is a VMware ESXi&#x2122; hypervisor provided as part of the VMware vSphere&#xae; solution made commercially available by VMware, Inc. of Palo Alto, Calif. An embodiment of software platform <b>124</b> is discussed further below with respect to <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0015" num="0014">In embodiments, host duster <b>118</b> is configured with a software-defined (SD) network layer <b>175</b>. SD network layer <b>175</b> includes logical network services executing on virtualized infrastructure in host cluster <b>118</b>. The virtualized infrastructure that supports the logical network services includes hypervisor-based components, such as resource pools, distributed switches, distributed switch port groups and uplinks, etc., as well as VM-based components, such as router control VMs, load balancer VMs, edge service VMs, etc. Logical network services include logical switches, logical routers, logical firewalls, logical virtual private networks (VPNs), logical load balancers, and the like, implemented on top of the virtualized infrastructure. In embodiments, virtualized computing system <b>100</b> includes edge transport nodes <b>178</b> that provide an interface of host cluster <b>118</b> to an external network (e.g., a corporate network, the public Internet, etc.). Edge transport nodes <b>178</b> can include a gateway between the internal logical networking of host cluster <b>118</b> and the external network. Edge transport nodes <b>178</b> can be physical servers or VMs.</p><p id="p-0016" num="0015">Virtualization management server <b>116</b> is a physical or virtual server that manages host cluster <b>118</b> and the virtualization layer therein. Virtualization management server <b>116</b> installs agent(s) <b>152</b> in hypervisor <b>150</b> to add a host <b>120</b> as a managed entity. Virtualization management server <b>116</b> logically groups hosts <b>120</b> into host cluster <b>118</b> to provide cluster-level functions to hosts <b>120</b>, such as VM migration between hosts <b>120</b> (e.g., for load balancing), distributed power management, dynamic VM placement according to affinity and anti-affinity rules, and high-availability, The number of hosts <b>120</b> in host cluster <b>118</b> may be one or many. Virtualization management server <b>116</b> can manage more than one host cluster <b>118</b>.</p><p id="p-0017" num="0016">In an embodiment, virtualized computing system <b>100</b> further includes a network manager <b>112</b>. Network manager <b>112</b> is a physical or virtual server that orchestrates SD network layer <b>175</b>. In an embodiment, network manager <b>112</b> comprises one or more virtual servers deployed as VMs. Network manager <b>112</b> installs additional agents <b>152</b> in hypervisor <b>150</b> to add a host <b>120</b> as a managed entity, referred to as a transport node. In this manner, host cluster <b>118</b> can be a cluster <b>103</b> of transport nodes. One example of an SD networking platform that can be configured and used in embodiments described herein as network manager <b>112</b> and SD network layer <b>175</b> is a VMware NSX&#xae; platform made commercially available by VMware Inc. of Palo Alto, Calif..</p><p id="p-0018" num="0017">Network manager <b>112</b> can deploy one or more transport zones in virtualized computing system <b>100</b>, including VLAN transport zone(s) and an overlay transport zone. A VLAN transport zone spans a set of hosts <b>120</b> (e.g., host cluster <b>118</b>) and is backed by external network virtualization of physical network <b>180</b> (e.g., a VLAN). One example VLAN transport zone uses a management VLAN <b>182</b>, on physical network <b>180</b> that enables a management network connecting hosts <b>120</b> and the VI control plane (e.g., virtualization management server <b>116</b> and network manager <b>112</b>). An overlay transport zone using overlay VLAN <b>184</b> on physical network <b>180</b> enables an overlay network that spans a set of hosts <b>120</b> (e.g., host cluster <b>118</b>) and provides internal network virtualization using software components (e.g., the virtualization layer and services executing in VMs). Host-to-host traffic for the overlay transport zone is carried by physical network <b>180</b> on the overlay VLAN <b>184</b> using layer-2-over-layer-3 tunnels. Network manager <b>112</b> can configure SD network layer <b>175</b> to provide a cluster network <b>186</b> using the overlay network. The overlay transport zone can be extended into at least one of edge transport nodes <b>178</b> to provide ingress/egress between cluster network <b>186</b> and an external network.</p><p id="p-0019" num="0018">Virtualization management server <b>116</b> and network manager <b>112</b> comprise a virtual infrastructure (Vi) control plane <b>113</b> of virtualized computing system <b>100</b>. In embodiments, network manager <b>112</b> is omitted and virtualization management server <b>116</b> handles virtual networking. Virtualization management server <b>116</b> can include VI services <b>108</b>. VI services <b>108</b> include various virtualization management services, such as a distributed resource scheduler (DRS) <b>109</b>, high-availability (HA) service, single sign-on (SSO) service, virtualization management daemon, vSAN service, and the like. DRS <b>109</b> is configured to aggregate the resources of host cluster <b>118</b> to provide resource pools and enforce resource allocation policies. DRS <b>109</b> also provides resource management in the form of load balancing, power management, VM placement, and the like. HA service is configured to pool VMs and hosts into a monitored cluster and, in the event of a failure, restart VMs on alternate hosts in the cluster. A single host is elected as a master, which communicates with the HA service and monitors the state of protected VMs on subordinate hosts. The HA service uses admission control to ensure enough resources are reserved in the cluster for VM recovery when a host fails. SSO service comprises security token service, administration server, directory service, identity management service, and the like configured to implement an SSO platform. for authenticating users. The virtualization management daemon is configured to manage objects, such as data centers, clusters, hosts, VMs, resource pools, datastores, and the like.</p><p id="p-0020" num="0019">A VI admin can interact with virtualization management server <b>116</b> through a VM management client <b>106</b>. Through VM management client <b>106</b>, a VI admin commands virtualization management server <b>116</b> to form host cluster <b>118</b>, configure resource pools, resource allocation policies, and other cluster-level functions, configure storage and networking, and the like.</p><p id="p-0021" num="0020">Hypervisor <b>150</b> further includes remediation software <b>153</b> for performing lifecycle operations on hypervisor <b>150</b>. Remediation software <b>153</b> in hypervisor <b>150</b> removes the dependence on an external coordination engine and allows hosts <b>120</b> to autonomously coordinate and perform their own lifecycle operations. Lifecycle operations include patching and upgrading the base operating system, patching and upgrading installed software, managing the configuration of hypervisor <b>150</b>, and the like.</p><p id="p-0022" num="0021">Virtualized computing system <b>100</b> includes a distributed key-value store (DKVS) <b>171</b>. In embodiments, DKVS <b>171</b> comprises software executing in a plurality of VMs <b>140</b>. For purposes of clarity, DKVS <b>171</b> is shown as a separate logical component in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. In embodiments, remediation software <b>153</b> leverages DKVS <b>171</b> to coordinate lifecycle operations. DKVS <b>171</b> provides high availability, redundancy, and fault tolerance that allows the lifecycle operations to scale with the number of hosts <b>120</b> in host cluster <b>118</b>. Users create host state documents <b>142</b>, which are stored in DKVS <b>171</b>. For example, users can interact with virtualization management server <b>116</b> using VM management client <b>106</b> to define or provide host state documents <b>142</b>. Virtualization management server <b>116</b> can store host state documents <b>142</b> in DKVS <b>171</b>. Each host state document <b>142</b> defines software and configuration for hypervisor <b>150</b> in a host in a declarative, human-readable form.</p><p id="p-0023" num="0022">According to embodiments, software installation bundles (SABs), more generally referred to herein as payloads, are logically grouped into &#x201c;components.&#x201d; In the embodiments, a component is a unit of shipment and installation, and a successful installation of a component typically will appear to the end user as enabling some specific feature of hypervisor <b>150</b>. For example, if a software vendor wants to ship a user-visible feature that requires a plug-in, a driver, and a solution, the software vendor will create separate payloads for each of the plug-in, the driver, and the solution, and then group them together as one component. From the end user's perspective, it is sufficient to install this one component onto a server to enable this feature on the server. A component may be part of another software image, such as a base image or an add-on, as further described below, or it may be a stand-alone component provided by a third-party or the end user (hereinafter referred to as &#x201c;user component&#x201d;).</p><p id="p-0024" num="0023">A &#x201c;base image&#x201d; is a collection of components that are sufficient to boot up a server with the virtualization software. For example, the components for the base image include a core kernel component and components for basic drivers and in-box drivers. The core kernel component is made up of a kernel payload and other payloads that have inter-dependencies with the kernel payload. According to embodiments, the collection of components that make up the base image is packaged and released as one unit.</p><p id="p-0025" num="0024">An &#x201c;add-on&#x201d; car &#x201c; add-on image&#x201d; is a collection of components that the OEM wants to bring together to customize its servers. Using add-ons, the OEM can add, update or remove components that are present in the base image. The add-on is layered on top of the base image and the combination includes all the drivers and solutions that are necessary to customize, boot up and monitor the OEM's servers. Although an &#x201c;add-on&#x201d; is always layered on top of a base image, the add-on content and the base image content are not, tied together. As a result, an OEM is able to independently manage the lifecycle of its releases. In addition, end users can update the add-on content and the base image content independently of each other.</p><p id="p-0026" num="0025">&#x201c;Solutions&#x201d; are features that indirectly impact the desired image when they are enabled by the end user. In other words, the end-user decides to enable the solution in a user interface but does not decide what components to install. The solution's management layer decides the right set of components based on constraints. Examples solutions include HA (high availability), and NSX (network virtualization platform of VMware, Inc.).</p><p id="p-0027" num="0026">One example form for expressing the desired state is a host state document <b>142</b>. A host state document can define (<b>1</b>) base image, (<b>2</b>) add-on, (<b>3</b>) solution, (<b>4</b>) user component(s), and (<b>5</b>) firmware package, and the like for hypervisor <b>150</b> and its host <b>120</b>. As discussed further below, remediation software <b>153</b> can obtain or be notified of a host state document <b>142</b> for its respective host <b>120</b> and perform lifecycle operations in case the current state of host <b>120</b> differs from the desired state specified in host state document <b>142</b>. In embodiments, DRS <b>109</b> stores some of its data as cluster state data <b>144</b> in DKVS <b>171</b>, Remediation software <b>153</b> can access cluster state data <b>144</b> during lifecycle operations to perform health checks on host cluster <b>118</b>, as discussed further below.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram depicting software platform <b>124</b> according to an embodiment. As described above, software platform <b>124</b> of host <b>120</b> includes hypervisor <b>150</b> that supports execution of VMs <b>140</b>. in an embodiment, hypervisor <b>150</b> includes a VM management daemon <b>213</b>, a host daemon <b>214</b>, and remediation software <b>153</b>. VM management daemon <b>213</b> is an agent <b>152</b> installed by virtualization management server <b>116</b>. VM management daemon <b>213</b> provides an interface to host daemon <b>214</b> for virtualization management server <b>116</b>. Host daemon <b>214</b> is configured to create, configure, and remove VMs. Network agents <b>222</b> comprises agents <b>152</b> installed by network manager <b>112</b>. Network agents <b>222</b> are configured to cooperate with network manager <b>112</b> to implement logical network services. Network agents <b>222</b> configure the respective host as a transport node in a cluster <b>103</b> of transport nodes. Each VM <b>140</b> has applications <b>202</b> running therein on top of an OS <b>204</b>. Remediation software <b>153</b> performs lifecycle operations on hypervisor <b>150</b> as discussed further herein. Hypervisor <b>150</b> is installed on host <b>120</b> from a software image <b>224</b>, which can include a collection of SIBs for the base OS and software executing thereon (e.g., addons). Software platform <b>124</b> can include one or more software images <b>224</b> (stored on host <b>120</b>, e.g., in local storage).</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flow diagram depicting a method <b>300</b> of autonomous lifecycle management for a hypervisor according to an embodiment. Method <b>300</b> begins at step <b>300</b>, where a user updates DKVS <b>171</b> with a host state document <b>142</b>. In embodiments, the user interacts with virtualization management server <b>116</b>, which in turn stores a host state document <b>142</b> in DKVS <b>171</b>. In embodiments, host state document <b>142</b> declaratively specifies the target OS/software version hypervisor <b>150</b> needs to be patched/upgraded to; an optional location where patches/upgraded software can be obtained; and any configuration changes required on host <b>120</b> (e.g., configuration changes for hypervisor <b>150</b>).</p><p id="p-0030" num="0029">At step <b>304</b>, remediation software <b>153</b> in a host <b>120</b> detects or is notified of host state document <b>142</b>. For example, remediation software <b>153</b> can periodically monitor DKVS <b>171</b> to determine if a new host state document is available. In another example, remediation software <b>153</b> can receive a notification from DKVS <b>171</b> or virtualization management server <b>116</b> that a new host state document is available.</p><p id="p-0031" num="0030">At step <b>306</b>, remediation software <b>153</b> determines compliance of hypervisor <b>150</b> with host state document <b>142</b>. If hypervisor <b>150</b> is already in compliance with host state document at step <b>308</b>, the method <b>300</b> ends at step <b>316</b>. If at step <b>308</b> hypervisor <b>150</b> is in drift with respect to host state document <b>142</b>, method <b>300</b> proceeds instead to step <b>310</b>.</p><p id="p-0032" num="0031">At step <b>310</b>, remediation software <b>153</b> performs remediation in coordination with other hosts <b>120</b> in host cluster <b>118</b>. Embodiments of the remediation process are described below with respect to <figref idref="DRAWINGS">FIG. <b>4</b></figref>. In an embodiment, at step <b>311</b>, remediation software <b>153</b> obtains software for remediation from another host in host cluster <b>118</b>. For example, remediation software <b>153</b> can determine from information in DKVS <b>171</b> the location of software for updating/patching hypervisor <b>150</b> for the remediation operation (e.g., a software image <b>224</b>). Remediation software <b>153</b> can then request and obtain a software image <b>224</b> from another host for use in the remediation operation. In another embodiment, remediation software <b>153</b> can obtain a software image <b>224</b> from a repository (e.g., identified in the host state document).</p><p id="p-0033" num="0032">At step <b>312</b>, remediation software <b>153</b> determines if the remediation operation is successful. If not, method <b>300</b> proceeds to step <b>314</b>, where remediation software <b>153</b> marks host state document <b>142</b> as invalid and updates DKVS <b>171</b> with information indicating that the remediation operation for host <b>120</b> has failed. Method <b>300</b> then ends at step <b>316</b>. If at step <b>312</b> the remediation operation is successful, method <b>300</b> ends at step <b>316</b>.</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flow diagram depicting a method <b>400</b> of remediation of a host according to an embodiment. Method <b>400</b> begins at step <b>402</b>, where remediation software <b>153</b> requests a lifecycle lock from DKVS <b>171</b>. DKVS <b>171</b> can store information that determines how many hosts <b>120</b> can perform lifecycle operations concurrently. Remediation software <b>153</b> can examine such information in DKVS <b>171</b> to determine if it is free to perform lifecycle operations. If at step <b>404</b> a lock cannot be obtained, method <b>400</b> proceeds to step <b>406</b>, where remediation software <b>153</b> can retry obtaining the lock after a delay. If at step <b>404</b> a lifecycle lock has been obtained, method <b>400</b> proceeds to step <b>408</b>.</p><p id="p-0035" num="0034">At step <b>408</b>, remediation software <b>153</b> determines if host state document <b>142</b> is valid. For example, remediation software <b>153</b> can check the desired state specified in host state document <b>142</b> for any errors. If at step <b>410</b> host state document <b>142</b> is invalid, method <b>400</b> proceeds to step <b>412</b>, where remediation software <b>153</b> exists with invalid status indicating that the remediation operation is not successful. If at step <b>410</b> host state document <b>142</b> is valid, method <b>400</b> proceeds to step <b>414</b>.</p><p id="p-0036" num="0035">At step <b>414</b>, remediation software <b>153</b> performs a health check of host <b>120</b> and host cluster <b>118</b>. Remediation software <b>153</b> can obtain cluster state data <b>144</b> from DKVS <b>171</b> when determining health of host cluster <b>118</b>. Example health checks of host <b>120</b> include host connectivity, whether all required software services are up and running, and the like. Example health checks of host cluster <b>118</b> include vSAN cluster health, any distributed storage availability checks, and the like. At step <b>416</b>, remediation software <b>153</b> requests migration of VMs <b>140</b> from host <b>120</b> to oilier host(s) <b>120</b> in host cluster <b>118</b> and requests hypervisor <b>150</b> enter into maintenance mode. Maintenance mode for hypervisor <b>150</b> allows hypervisor <b>150</b> to perform lifecycle operations.</p><p id="p-0037" num="0036">At step <b>418</b>, remediation software <b>153</b> executes one or more lifecycle operations to make hypervisor <b>150</b> compliant with the desired state as specified in host state document <b>142</b>. Example lifecycle operations include upgrading/patching the base OS, upgrading/patching installed software (e.g., addons), and changing the configuration of hypervisor <b>150</b>. At step <b>420</b>, if a reboot is required to implement the changes, remediation software <b>153</b> requests a reboot. At step <b>422</b>, remediation software <b>153</b> executes health checks on host <b>120</b> and host cluster <b>118</b>. Health checks performed in step <b>422</b> may be the same or similar to those performed in step <b>414</b>. However, in step <b>422</b>, hypervisor <b>150</b> has had its state changed to match the desired state in host state document <b>142</b>.</p><p id="p-0038" num="0037">At step <b>424</b>, remediation software <b>153</b> requests exit of the maintenance mode and requests migration of VMs back to host <b>120</b>. At step <b>426</b>, remediation software <b>153</b> releases the lifecycle lock obtained at step <b>402</b>.</p><p id="p-0039" num="0038">One or more embodiments of the invention also relate to a device or an apparatus for performing these operations. The apparatus may be specially constructed for required purposes, or the apparatus may be a general-purpose computer selectively activated or configured by a computer program stored in the computer. Various general-purpose machines may be used with computer programs written in accordance with the teachings herein, or it may be more convenient to construct a more specialized apparatus to perform the required operations.</p><p id="p-0040" num="0039">The embodiments described herein may be practiced with other computer system configurations including hand-held devices, microprocessor systems, microprocessor-based or programmable consumer electronics, minicomputers, mainframe computers, etc.</p><p id="p-0041" num="0040">One or more embodiments of the present invention may be implemented as one or more computer programs or as one or more computer program modules embodied in computer readable media. The term computer readable medium refers to any data storage device that can store data which can thereafter be input to a computer system. Computer readable media may be based on any existing or subsequently developed technology that embodies computer programs in a manner that enables a computer to read the programs. Examples of computer readable media are hard drives, NAS systems, read-only memory (ROM), RAM, compact disks (CDs), digital versatile disks (DVDs), magnetic tapes, and other optical and non-optical data storage devices. A computer readable medium can also be distributed over a network-coupled computer system so that the computer readable code is stored and executed in a distributed fashion.</p><p id="p-0042" num="0041">Although one or more embodiments of the present invention have been described in some detail for clarity of understanding, certain changes may be made within the scope of the claims. Accordingly, the described embodiments are to be considered as illustrative and not restrictive, and the scope of the claims is not to be limited to details given herein but may be modified within the scope and equivalents of the claims. In the claims, elements and/or steps do not imply any particular order of operation unless explicitly stated in the claims.</p><p id="p-0043" num="0042">Virtualization systems in accordance with the various embodiments may be implemented as hosted embodiments, non-hosted embodiments, or as embodiments that blur distinctions between the two. Furthermore, various virtualization operations may be wholly or partially implemented in hardware. For example, a hardware implementation may employ a look-up table for modification of storage access requests to secure non-disk data.</p><p id="p-0044" num="0043">Many variations, additions, and improvements are possible, regardless of the degree of virtualization. The virtualization software can therefore include components of a host, console, or guest OS that perform virtualization functions.</p><p id="p-0045" num="0044">Plural instances may be provided for components, operations, or structures described herein as a single instance. Boundaries between components, operations, and data stores are somewhat arbitrary, and particular operations are illustrated in the context of specific illustrative configurations. Other allocations of functionality are envisioned and may fall within the scope of the invention. In general, structures and functionalities presented as separate components in exemplary configurations may be implemented as a combined structure or component. Similarly, structures and functionalities presented as a single component may be implemented as separate components. These and other variations, additions, and improvements may fall within the scope of the appended claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method of hypervisor lifecycle management in a virtualized computing system having a cluster of hosts, the method comprising:<claim-text>obtaining, by remediation software executing in a host of the hosts, a host state document from a distributed key-value store, the host state document defining a desired state of software in the host, the software including a hypervisor; and</claim-text><claim-text>performing, by the remediation software in coordination with other hosts of the hosts through the distributed key-value store, a lifecycle operation on the software of the host in response to determining that a current state of the software does not match the desired state.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the step of performing comprises:<claim-text>requesting, by the remediation software, a lifecycle lock from the distributed key-value store;</claim-text><claim-text>entering a maintenance mode of the hypervisor; and</claim-text><claim-text>executing the lifecycle operation during the maintenance mode; and</claim-text><claim-text>exiting the maintenance mode of the hypervisor after the lifecycle operation.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the step of performing further comprises:<claim-text>executing a health check of the host before entering the maintenance mode, after executing the lifecycle operation, or both before entering the maintenance mode and after executing the lifecycle operation.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the step of performing further comprises:<claim-text>obtaining cluster state data from the distributed key-value store at the remediation software; and</claim-text><claim-text>executing a health check on the host cluster using the cluster state data before entering the maintenance mode, after executing the lifecycle operation, or both before entering the maintenance mode and after executing the lifecycle operation.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the desired state of the hypervisor in the host state document includes at least one of a desired version of a base operating system (OS), a desired version of software executing on the base OS, and a configuration of the hypervisor.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the host state document specifies a repository having software for use in the lifecycle operation, and wherein the step of performing comprises obtaining the software from the repository for use with the lifecycle operation.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>determining, by the remediation software from the distributed key-value store, another host of the host cluster that stores software for use in the lifecycle operation; and</claim-text><claim-text>obtaining, by the remediation software, the software from the other host for use with the lifecycle operation.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. A non-transitory computer readable medium comprising instructions to be executed in a computing device to cause the computing device to carry out a method of hypervisor lifecycle management in a virtualized computing system having a cluster of hosts, the method comprising:<claim-text>obtaining, by remediation software executing in a host of the hosts, a host state document from a distributed key-value store, the host state document defining a desired state of software in the host, the software including a hypervisor; and</claim-text><claim-text>performing, by the remediation software in coordination with other hosts of the hosts through the distributed key-value store, a lifecycle operation on the software of the host in response to determining that a current state of the software does not match the desired state.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The non-transitory computer readable medium of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the step of performing comprises:<claim-text>requesting, by the remediation software, a lifecycle lock from the distributed key-value store;</claim-text><claim-text>entering a maintenance mode of the hypervisor; and</claim-text><claim-text>executing the lifecycle operation during the maintenance mode; and</claim-text><claim-text>exiting the maintenance mode of the hypervisor after the lifecycle operation.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The non-transitory computer readable medium of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the step of performing further comprises:<claim-text>executing a health check of the host before entering the maintenance mode, after executing the lifecycle operation, or both before entering the maintenance mode and after executing the lifecycle operation.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The non-transitory computer readable medium of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the step of performing further comprises:<claim-text>obtaining cluster state data from the distributed key-value store at the remediation software; and</claim-text><claim-text>executing a health check on the host cluster using the cluster state data before entering the maintenance mode, after executing the lifecycle operation, or both before entering the maintenance mode and after executing the lifecycle operation.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The non-transitory computer readable medium of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the desired state of the hypervisor in the host state document includes at least one of a desired version of a base operating system (OS), a desired version of software executing on the base OS, and a configuration of the hypervisor.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The non-transitory computer readable medium of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the host state document specifies a repository having software for use in the lifecycle operation, and wherein the step of performing comprises obtaining the software from the repository for use with the lifecycle operation.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The non-transitory computer readable medium of <claim-ref idref="CLM-00008">claim 8</claim-ref>, further comprising:<claim-text>determining, by the remediation software from the distributed key-value store, another host of the host cluster that stores software for use in the lifecycle operation; and</claim-text><claim-text>obtaining, by the remediation software, the software from the other host for use with the lifecycle operation.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. A virtualized computing system having a cluster comprising hosts connected to a network, the virtualized computing system comprising:<claim-text>a distributed key-value store configured to store a host state document; and</claim-text><claim-text>a first host of the hosts configured to execute remediation software, the remediation software configured to:<claim-text>obtain the host state document from the distributed key-value store, the host state document defining a desired state of software in the first host, the software including a hypervisor; and</claim-text><claim-text>perform, in coordination with other hosts of the hosts through the distributed key-value store, a lifecycle operation on the software of the first host in response to determining that a current state of the software does not match the desired state.</claim-text></claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The virtualized computing system of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the remediation software is configured to perform the lifecycle operation by:<claim-text>requesting, by the remediation software, a lifecycle lock from the distributed key-value store;</claim-text><claim-text>entering a maintenance mode of the hypervisor; and</claim-text><claim-text>executing the lifecycle operation during the maintenance mode; and</claim-text><claim-text>exiting the maintenance mode of the hypervisor after the lifecycle operation.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The virtualized computing system of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the remediation software is configured to perform the lifecycle operation by:<claim-text>executing a health check of the host before entering the maintenance mode, after executing the lifecycle operation, or both before entering the maintenance mode and after executing the lifecycle operation.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The virtualized computing system of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the remediation software is configured to perform the lifecycle operation by:<claim-text>obtaining cluster state data from the distributed key-value store at the remediation software; and</claim-text><claim-text>executing a health check on the host cluster using the cluster state data before entering the maintenance mode, after executing the lifecycle operation, or both before entering the maintenance mode and after executing the lifecycle operation.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The virtualized computing system of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the desired state of the hypervisor in the host state document includes at least one of a desired version of a base operating system (OS), a desired version of software executing on the base OS, and a configuration of the hypervisor.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The virtualized computing system of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the host state document specifies a repository having software for use in the lifecycle operation, and wherein the step of performing comprises obtaining the software from the repository for use with the lifecycle operation.</claim-text></claim></claims></us-patent-application>