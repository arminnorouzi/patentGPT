<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230007210A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230007210</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17809189</doc-number><date>20220627</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>45</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>21</main-group><subgroup>431</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>7</main-group><subgroup>01</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>45</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>21</main-group><subgroup>4316</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>7</main-group><subgroup>01</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">Signaling the Purpose of Preselection</invention-title><us-related-documents><us-provisional-application><document-id><country>US</country><doc-number>63216975</doc-number><date>20210630</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Lemon Inc.</orgname><address><city>Grand Cayman</city><country>KY</country></address></addressbook><residence><country>KY</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Wang</last-name><first-name>Ye-kui</first-name><address><city>San Diego</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A method of processing media data. The method includes determining, for a conversion between the media data and a media data file, that a preselection element includes an indicator, wherein the indicator indicates that a purpose of the preselection element is for providing a picture-in-picture experience where a supplementary video appears to be overlaid on a target picture-in-picture region in a main video; and performing a conversion between the media data and the media data file based on the indicator. A corresponding video coding apparatus and non-transitory computer-readable recording medium are also disclosed.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="226.06mm" wi="158.75mm" file="US20230007210A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="167.64mm" wi="143.68mm" file="US20230007210A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="159.60mm" wi="150.79mm" file="US20230007210A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="204.55mm" wi="159.85mm" file="US20230007210A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="161.54mm" wi="127.85mm" file="US20230007210A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="250.53mm" wi="170.52mm" file="US20230007210A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="165.44mm" wi="94.66mm" orientation="landscape" file="US20230007210A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="98.98mm" wi="120.90mm" file="US20230007210A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="165.35mm" wi="123.02mm" orientation="landscape" file="US20230007210A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="245.36mm" wi="155.87mm" orientation="landscape" file="US20230007210A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="237.66mm" wi="157.23mm" orientation="landscape" file="US20230007210A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="121.50mm" wi="140.80mm" file="US20230007210A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This patent application claims the benefit of U.S. Provisional Patent Application No. 63/216,975 filed Jun. 30, 2021, by Lemon, Inc., and titled &#x201c;Support of Picture-in-Picture Services in DASH,&#x201d; which is hereby incorporated by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">The present disclosure is generally related to video streaming and, in particular, to the support of picture-in-picture services in Dynamic Adaptive Streaming over Hypertext Transfer Protocol (DASH).</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">Digital video accounts for the largest bandwidth use on the internet and other digital communication networks. As the number of connected user devices capable of receiving and displaying video increases, it is expected that the bandwidth demand for digital video usage will continue to grow.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0005" num="0004">The disclosed aspects/embodiments provide techniques that incorporate an indicator in a preselection element. In an embodiment, the indicator indicates that the purpose of the preselection comprises picture-in-picture (a.k.a., a picture-in-picture experience or picture-in-picture services). Thus, the video coding process is improved relative to conventional video coding techniques.</p><p id="p-0006" num="0005">A first aspect relates to a method of processing media data. The method includes determining, for a conversion between the media data and a media data file, that a preselection element includes an indicator, wherein the indicator indicates that a purpose of the preselection element is for providing a picture-in-picture experience where a supplementary video appears to be overlaid on a target picture-in-picture region in a main video; and performing the conversion between the media data and the media data file based on the indicator.</p><p id="p-0007" num="0006">Optionally, in any of the preceding aspects, another implementation of the aspect provides that the indicator comprises an @tag attribute.</p><p id="p-0008" num="0007">Optionally, in any of the preceding aspects, another implementation of the aspect provides that the indicator comprises an @value attribute.</p><p id="p-0009" num="0008">Optionally, in any of the preceding aspects, another implementation of the aspect provides that the indicator has a value of &#x201c;PicInPic&#x201d; to indicate that the purpose of the preselection element is for providing the picture-in-picture experience.</p><p id="p-0010" num="0009">Optionally, in any of the preceding aspects, another implementation of the aspect provides that the indicator comprises an @tag attribute having a value of &#x201c;PicInPic&#x201d; to indicate that the purpose of the preselection element is for providing the picture-in-picture experience.</p><p id="p-0011" num="0010">Optionally, in any of the preceding aspects, another implementation of the aspect provides that the indicator comprises an @value attribute having a value of &#x201c;PicInPic&#x201d; to indicate that the purpose of the preselection element is for providing the picture-in-picture experience.</p><p id="p-0012" num="0011">Optionally, in any of the preceding aspects, another implementation of the aspect provides that the indicator comprises an @tag attribute within a CommonAttributesElements element in the preselection element.</p><p id="p-0013" num="0012">Optionally, in any of the preceding aspects, another implementation of the aspect provides that the indicator comprises an @value attribute of a Role element in the preselection element.</p><p id="p-0014" num="0013">Optionally, in any of the preceding aspects, another implementation of the aspect provides that the indicator comprises an @tag attribute that also specifies a representation, adaption set, or the preselection element that may be used for selection purposes toward a decoder.</p><p id="p-0015" num="0014">Optionally, in any of the preceding aspects, another implementation of the aspect provides the indicator comprises an @value attribute of a Role element that specifies information on a role annotation scheme.</p><p id="p-0016" num="0015">Optionally, in any of the preceding aspects, another implementation of the aspect provides that the preselection element includes an @dataUnitsReplacable attribute that specifies whether coded video data units representing the target picture-in-picture region in the main video can be replaced by corresponding video data units of the supplementary video.</p><p id="p-0017" num="0016">Optionally, in any of the preceding aspects, another implementation of the aspect provides that the preselection element only includes an @dataUnitsReplacable attribute when the indicator is an @tag attribute having a value of &#x201c;PicInPic.&#x201d;</p><p id="p-0018" num="0017">Optionally, in any of the preceding aspects, another implementation of the aspect provides that the preselection element includes an @regionsIds attribute that specifies an identifier (ID) for each coded video data unit representing the target picture-in-picture region in the main video.</p><p id="p-0019" num="0018">Optionally, in any of the preceding aspects, another implementation of the aspect provides that the preselection element only includes an @regionsIds attribute when the indicator is an @tag attribute having a value of &#x201c;PicInPic.&#x201d;</p><p id="p-0020" num="0019">Optionally, in any of the preceding aspects, another implementation of the aspect provides that the preselection element is disposed in a Media Presentation Description (MPD) file.</p><p id="p-0021" num="0020">Optionally, in any of the preceding aspects, another implementation of the aspect provides that the preselection element is a Dynamic Adaptive Streaming over Hypertext Transfer Protocol (DASH) preselection element.</p><p id="p-0022" num="0021">Optionally, in any of the preceding aspects, another implementation of the aspect provides that the conversion includes encoding the media data into a bitstream.</p><p id="p-0023" num="0022">Optionally, in any of the preceding aspects, another implementation of the aspect provides that the conversion includes decoding the media data from a bitstream.</p><p id="p-0024" num="0023">An apparatus for processing video data comprising a processor and a non-transitory memory with instructions thereon, wherein the instructions upon execution by the processor, cause the processor to: determine, for a conversion between the media data and a media data file, that a preselection element includes an indicator, wherein the indicator indicates that a purpose of the preselection element is picture-in-picture experience where a supplementary video appears to be overlaid on a target picture-in-picture region in a main video; and perform the conversion between the media data and the media data file based on the indicator.</p><p id="p-0025" num="0024">A non-transitory computer-readable recording medium storing a Media Presentation Description (MPD) of a video which is generated by a method performed by a video processing apparatus, wherein the method comprises: determining that a preselection element includes an indicator, wherein the indicator indicates that a purpose of the preselection element is picture-in-picture experience where a supplementary video appears to be overlaid on a target picture-in-picture region in a main video; and generating the MPD based on the indicator.</p><p id="p-0026" num="0025">For the purpose of clarity, any one of the foregoing embodiments may be combined with any one or more of the other foregoing embodiments to create a new embodiment within the scope of the present disclosure.</p><p id="p-0027" num="0026">These and other features will be more clearly understood from the following detailed description taken in conjunction with the accompanying drawings and claims.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0028" num="0027">For a more complete understanding of this disclosure, reference is now made to the following brief description, taken in connection with the accompanying drawings and detailed description, wherein like reference numerals represent like parts.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic diagram illustrating a description of a video or media data by a media presentation description (MPD) used in DASH.</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is an example of a picture partitioned into tiles, slices, and subpictures.</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is an example of a subpicture-based viewport-dependent 360&#xb0; video delivery scheme.</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is an example of subpicture extraction.</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is an example of picture-in-picture support based on versatile video coding (VVC) subpictures.</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a block diagram showing an example video processing system.</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a block diagram of a video processing apparatus.</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a block diagram that illustrates an example of a video coding system.</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a block diagram illustrating an example of a video encoder.</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a block diagram illustrating an example of a video decoder.</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a method of processing media data according to an embodiment of the disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0040" num="0039">It should be understood at the outset that although an illustrative implementation of one or more embodiments are provided below, the disclosed systems and/or methods may be implemented using any number of techniques, whether currently known or in existence. The disclosure should in no way be limited to the illustrative implementations, drawings, and techniques illustrated below, including the exemplary designs and implementations illustrated and described herein, but may be modified within the scope of the appended claims along with their full scope of equivalents.</p><p id="p-0041" num="0040">H.266 terminology is used in some description only for ease of understanding and not for limiting scope of the disclosed techniques. As such, the techniques described herein are applicable to other video codec protocols and designs also.</p><p id="p-0042" num="0041">Video coding standards have evolved primarily through the development of the well-known International Telecommunication Union-Telecommunication (ITU-T) and International Organization for Standardization (ISO)/International Electrotechnical Commission (IEC) standards. The ITU-T produced H.261 and H.263, ISO/IEC produced Moving Picture Experts Group (MPEG)-1 and MPEG-4 Visual, and the two organizations jointly produced the H.262/MPEG-2 Video and H.264/MPEG-4 Advanced Video Coding (AVC) and H.265/High Efficiency Video Coding (HEVC) standards.</p><p id="p-0043" num="0042">Since H.262, the video coding standards are based on the hybrid video coding structure wherein temporal prediction plus transform coding are utilized. To explore the future video coding technologies beyond HEVC, Joint Video Exploration Team (JVET) was founded by Video Coding Experts Group (VCEG) and MPEG jointly in 2015. Since then, many new methods have been adopted by JVET and put into the reference software named Joint Exploration Model (JEM).</p><p id="p-0044" num="0043">In April 2018, the Joint Video Expert Team (JVET) between VCEG (Q6/16) and ISO/IEC JTC1 SC29/WG11 (MPEG) was created to work on the Versatile Video Coding (VVC) standard targeting at fifty percent (50%) bitrate reduction compared to HEVC. VVC version 1 was finalized in July 2020.</p><p id="p-0045" num="0044">The VVC standard (ITU-T H.266|ISO/IEC 23090-3) and the associated Versatile Supplemental Enhancement Information (VSEI) standard (ITU-T H.274|ISO/IEC 23002-7) were designed for use in a maximally broad range of applications, including both the traditional uses such as television broadcast, video conferencing, or playback from storage media, and also newer and more advanced use cases such as adaptive bit rate streaming, video region extraction, composition and merging of content from multiple coded video bitstreams, multiview video, scalable layered coding, and viewport-adaptive three hundred sixty degree (360&#xb0;) immersive media.</p><p id="p-0046" num="0045">The Essential Video Coding (EVC) standard (ISO/IEC 23094-1) is another video coding standard that has recently been developed by MPEG.</p><p id="p-0047" num="0046">Media streaming applications are typically based on the Internet Protocol (IP), Transmission Control Protocol (TCP), and Hypertext Transfer Protocol (HTTP) transport methods, and typically rely on a file format such as the ISO base media file format (ISOBMFF). One such streaming system is DASH. For using a video format with ISOBMFF and DASH, a file format specification specific to the video format, such as the AVC file format and the HEVC file format in ISO/IEC 14496-15: &#x201c;Information technology&#x2014;Coding of audio-visual objects&#x2014;Part 15: Carriage of network abstraction layer (NAL) unit structured video in the ISO base media file format,&#x201d; would be needed for encapsulation of the video content in ISOBMFF tracks and in DASH representations and segments. Important information about the video bitstreams, e.g., the profile, tier, and level, and many others, would need to be exposed as file format level metadata and/or DASH media presentation description (MPD) for content selection purposes, e.g., for selection of appropriate media segments both for initialization at the beginning of a streaming session and for stream adaptation during the streaming session.</p><p id="p-0048" num="0047">Similarly, for using an image format with ISOBMFF, a file format specification specific to the image format, such as the AVC image file format and the HEVC image file format in ISO/IEC 23008-12: &#x201c;Information technology&#x2014;High efficiency coding and media delivery in heterogeneous environments&#x2014;Part 12: Image File Format,&#x201d; would be needed.</p><p id="p-0049" num="0048">The VVC video file format, the file format for storage of VVC video content based on ISOBMFF, is currently being developed by MPEG. The latest draft specification of the VVC video file format is included in ISO/IEC JTC 1/SC 29/WG 03 output document N0035, &#x201c;Potential improvements on Carriage of VVC and EVC in ISOBMFF,&#x201d; November 2020.</p><p id="p-0050" num="0049">The VVC image file format, the file format for storage of image content coded using VVC based on ISOBMFF, is currently being developed by MPEG. The latest draft specification of the VVC image file format is included in ISO/IEC JTC 1/SC 29/WG 03 output document N0038, &#x201c;Information technology&#x2014;High efficiency coding and media delivery in heterogeneous environments&#x2014;Part 12: Image File Format&#x2014;Amendment 3: Support for VVC, EVC, slideshows and other improvements (CD stage),&#x201d; November 2020.</p><p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic diagram illustrating a description of a video or media data by an MPD <b>100</b> used in DASH. The MPD <b>100</b> describes media streams in terms of periods <b>110</b>, adaptation sets <b>120</b>, representations <b>130</b>, and segments <b>140</b>.</p><p id="p-0052" num="0051">In DASH, there may be multiple representations for video and/or audio data of multimedia content, and different representations may correspond to different coding characteristics (e.g., different profiles or levels of a video coding standard, different bitrates, different spatial resolutions, etc.). The manifest of such representations may be defined in an MPD data structure. A media presentation may correspond to a structured collection of data that is accessible to a DASH streaming client device (e.g., smart phone, electronic tablet, laptop, etc.). The DASH streaming client device may request and download media data information to present a streaming service to a user of the client device. A media presentation may be described in the MPD data structure, which may include updates of the MPD.</p><p id="p-0053" num="0052">A media presentation may contain a sequence of one or more periods <b>110</b>. Each period <b>110</b> may extend until the start of the next period <b>110</b>, or until the end of the media presentation, in the case of the last period <b>110</b>. Each period <b>110</b> may contain one or more representations <b>130</b> for the same media content. A representation <b>130</b> may be one of a number of alternative encoded versions of audio, video, timed text, or other such data. The representations <b>130</b> may differ by encoding types, e.g., by bitrate, resolution, and/or codec for video data and bitrate, language, and/or codec for audio data. The term representation <b>130</b> may be used to refer to a section of encoded audio or video data corresponding to a particular period <b>110</b> of the multimedia content and encoded in a particular way.</p><p id="p-0054" num="0053">Representations <b>130</b> of a particular period <b>110</b> may be assigned to a group indicated by an attribute in the MPD <b>100</b> indicative of an adaptation set <b>120</b> to which the representations <b>130</b> belong. Representations <b>130</b> in the same adaptation set <b>120</b> are generally considered alternatives to each other, in that a client device can dynamically and seamlessly switch between these representations <b>130</b>, e.g., to perform bandwidth adaptation. For example, each representation <b>130</b> of video data for a particular period may be assigned to the same adaptation set <b>120</b>, such that any of the representations <b>130</b> may be selected for decoding to present media data, such as video data or audio data, of the multimedia content for the corresponding period <b>110</b>. The media content within one period <b>110</b> may be represented by either one representation <b>130</b> from group 0, if present, or the combination of at most one representation from each non-zero group, in some examples. Timing data for each representation <b>130</b> of a period <b>110</b> may be expressed relative to the start time of the period <b>110</b>.</p><p id="p-0055" num="0054">A representation <b>130</b> may include one or more segments <b>140</b>. Each representation <b>130</b> may include an initialization segment <b>140</b>, or each segment <b>140</b> of a representation may be self-initializing. When present, the initialization segment <b>140</b> may contain initialization information for accessing the representation <b>130</b>. In general, the initialization segment <b>140</b> does not contain media data. A segment <b>140</b> may be uniquely referenced by an identifier, such as a uniform resource locator (URL), uniform resource name (URN), or uniform resource identifier (URI). The MPD <b>100</b> may provide the identifiers for each segment <b>140</b>. In some examples, the MPD <b>100</b> may also provide byte ranges in the form of a range attribute, which may correspond to the data for a segment <b>140</b> within a file accessible by the URL, URN, or URI.</p><p id="p-0056" num="0055">Different representations <b>130</b> may be selected for substantially simultaneous retrieval for different types of media data. For example, a client device may select an audio representation <b>130</b>, a video representation <b>130</b>, and a timed text representation <b>130</b> from which to retrieve segments <b>140</b>. In some examples, the client device may select particular adaptation sets <b>120</b> for performing bandwidth adaptation. That is, the client device may select an adaptation set <b>120</b> including video representations <b>130</b>, an adaptation set <b>120</b> including audio representations <b>130</b>, and/or an adaptation set <b>120</b> including timed text. Alternatively, the client device may select adaptation sets <b>120</b> for certain types of media (e.g., video), and directly select representations <b>130</b> for other types of media (e.g., audio and/or timed text).</p><p id="p-0057" num="0056">A typical DASH streaming procedure is shown by the following steps.</p><p id="p-0058" num="0057">First, the client gets the MPD. In an embodiment, the client device receives the MPD from a content source after the client device has requested the MPD.</p><p id="p-0059" num="0058">Second, the client estimates the downlink bandwidth and selects a video representation <b>130</b> and an audio representation <b>130</b> according to the estimated downlink bandwidth and the codec, decoding capability, display size, audio language setting, etc.</p><p id="p-0060" num="0059">Third, unless the end of the media presentation is reached, the client requests media segments <b>140</b> of the selected representations <b>130</b> and presents the streaming content to the user.</p><p id="p-0061" num="0060">Fourth, the client keeps estimating the downlink bandwidth. When the bandwidth changes significantly (e.g., becomes lower), the client selects a different video representation <b>130</b> to match the newly estimated bandwidth, and the procedure returns to the third step.</p><p id="p-0062" num="0061">In VVC, a picture is divided (e.g., partitioned) into one or more tile rows and one or more tile columns. A tile is a sequence of coding tree units (CTUs) that cover a rectangular region of a picture. The CTUs in a tile are scanned in raster scan order within that tile.</p><p id="p-0063" num="0062">A slice consists of an integer number of complete tiles or an integer number of consecutive complete CTU rows within a tile of a picture.</p><p id="p-0064" num="0063">Two modes of slices are supported, namely the raster-scan slice mode and the rectangular slice mode. In the raster-scan slice mode, a slice contains a sequence of complete tiles in a tile raster scan of a picture. In the rectangular slice mode, a slice contains either a number of complete tiles that collectively form a rectangular region of the picture or a number of consecutive complete CTU rows of one tile that collectively form a rectangular region of the picture. Tiles within a rectangular slice are scanned in tile raster scan order within the rectangular region corresponding to that slice.</p><p id="p-0065" num="0064">A subpicture contains one or more slices that collectively cover a rectangular region of a picture.</p><p id="p-0066" num="0065"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is an example of a picture <b>200</b> partitioned into tiles <b>202</b>, subpictures/slices <b>204</b>, and CTUs <b>206</b>. As shown, the picture <b>200</b> has been partitioned into eighteen tiles <b>202</b> and twenty-four subpictures/slices <b>204</b>. In VVC, each subpicture consists of one or more complete rectangular slices that collectively cover a rectangular region of the picture as shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. A subpicture may be either specified to be extractable (i.e., coded independently of other subpictures of the same picture and of earlier pictures in decoding order) or not extractable. Regardless of whether a subpicture is extractable or not, the encoder can control whether in-loop filtering (including deblocking, Sample Adaptive Offset (SAO), and adaptive loop filter (ALF)) is applied across the subpicture boundaries individually for each subpicture.</p><p id="p-0067" num="0066">Functionally, subpictures are similar to the motion-constrained tile sets (MCTSs) in HEVC. They both allow independent coding and extraction of a rectangular subset of a sequence of coded pictures for use cases like viewport-dependent 360&#xb0; video streaming optimization and region of interest (ROI) applications.</p><p id="p-0068" num="0067">In streaming of 360&#xb0; video (a.k.a., omnidirectional video), at any particular moment only a subset (e.g., the current viewport) of the entire omnidirectional video sphere would be rendered to the user, while the user can turn his/her head anytime to change the viewing orientation and consequently the current viewport. While it is desirable to have at least some lower-quality representation of the area not covered by the current viewport available at the client and ready to be rendered to the user just in case the user suddenly changes his/her viewing orientation to anywhere on the sphere, a high-quality representation of the omnidirectional video is only needed for the current viewport that is being rendered to the user at any given moment. Splitting the high-quality representation of the entire omnidirectional video into subpictures at an appropriate granularity enables such an optimization as shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref> with twelve high-resolution subpictures on the left-hand side and the remaining twelve subpictures of the omnidirectional video in lower resolution on the right-hand side.</p><p id="p-0069" num="0068"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is an example of a subpicture-based viewport-dependent 360&#xb0; video delivery scheme <b>300</b>. As shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, only a higher-resolution representation of the full video uses subpictures. A lower-resolution representation of the full video does not use subpictures and can be coded with less frequent random access points (RAPs) than the higher-resolution representation. The client receives the full video in the lower-resolution while for the higher-resolution video, the client only receives and decodes the subpictures that cover the current viewport.</p><p id="p-0070" num="0069">There are several design differences between subpictures and MCTSs. First, the subpictures feature in VVC allows motion vectors of a coding block to point outside of the subpicture, even when the subpicture is extractable. To do so, the subpictures feature applies sample padding at subpicture boundaries which, in this case, are similar to picture boundaries. Second, additional changes were introduced for the selection and derivation of motion vectors in the merge mode and in the decoder side motion vector refinement process of VVC. This allows higher coding efficiency compared to the non-normative motion constraints applied at the encoder-side for MCTSs. Third, rewriting the slice headers (SHs) (and picture header (PH) network abstraction layer (NAL) units, when present) is not needed when extracting one or more extractable subpictures from a sequence of pictures to create a sub-bitstream that is a conforming bitstream. In sub-bitstream extractions based on HEVC MCTSs, rewriting of SHs is needed. Note that in both HEVC MCTSs extraction and VVC subpictures extraction, rewriting of sequence parameter sets (SPSs) and picture parameter sets (PPSs) is needed. However, typically there are only a few parameter sets in a bitstream, while each picture has at least one slice. Therefore, the rewriting of SHs can be a significant burden for application systems. Fourth, slices of different subpictures within a picture are allowed to have different NAL unit types. This is the feature often referred to as mixed NAL unit types or mixed subpicture types within a picture, discussed in more detail below. Fifth, VVC specifies a hypothetical reference decoder (HRD) and level definitions for subpicture sequences, thus the conformance of the sub-bitstream of each extractable subpicture sequence can be ensured by encoders.</p><p id="p-0071" num="0070">In AVC and HEVC, all video coding layer (VCL) NAL units in a picture need to have the same NAL unit type. VVC introduces the option to mix subpictures with certain different VCL NAL unit types within a picture, thus providing support for random access not only at the picture level but also at the subpicture level. In VVC, VCL NAL units within a subpicture still need to have the same NAL unit type.</p><p id="p-0072" num="0071">The capability of random accessing from Intra Random Access Pictures (IRAP) subpictures is beneficial for 360&#xb0; video applications. In viewport-dependent 360&#xb0; video delivery schemes similar to the one shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the content of spatially neighboring viewports largely overlaps, i.e. only a fraction of subpictures in a viewport is replaced by new subpictures during a viewport orientation change, while most subpictures remain in the viewport. Subpicture sequences that are newly introduced into the viewport must begin with IRAP slices, but significant reduction in overall transmission bit rate can be achieved when the remaining subpictures are allowed to carry out inter-prediction at viewport changes.</p><p id="p-0073" num="0072">The indication of whether a picture contains just a single type of NAL units or more than one type is provided in the PPS referred to by the picture (i.e., using a flag called pps_mixed_nalu_types_inpic_flag). A picture may comprise subpictures containing IRAP slices and subpictures containing trailing slices at the same time. A few other combinations of different NAL unit types within a picture are also allowed, including leading picture slices of NAL unit types Random Access Skipped Leading (RASL) and Random Access Decodable Leading (RADL), which allows the merging of subpicture sequences with open-group of pictures (GOP) and close-GOP coding structures extracted from different bitstreams into one bitstream.</p><p id="p-0074" num="0073">The layout of subpictures in VVC is signaled in the SPS, thus constant within a coded layer video sequence (CLVS). Each subpicture is signaled by the position of its top-left CTU and its width and height in number of CTUs, therefore ensuring that a subpicture covers a rectangular region of the picture with CTU granularity. The order in which the subpictures are signaled in the SPS determines the index of each subpicture within the picture.</p><p id="p-0075" num="0074">For enabling extraction and merging of subpicture sequences without rewriting of SHs or PHs, the slice addressing scheme in VVC is based on subpicture IDs and a subpicture-specific slice index to associate slices to subpictures. In the slice header (SH), the subpicture identifier (ID) of the subpicture containing the slice and the subpicture-level slice index are signaled. Note that the value of the subpicture ID of a particular subpicture can be different from the value of its subpicture index. A mapping between the two is either signaled in the SPS or PPS (but never both) or implicitly inferred. When present, the subpicture ID mapping needs to be rewritten or added when rewriting the SPSs and PPSs during the subpicture sub-bitstream extraction process. The subpicture ID and the subpicture-level slice index together indicate to the decoder the exact position of the first decoded CTU of a slice within the decoded picture buffer (DPB) slot of the decoded picture. After sub-bitstream extraction, the subpicture ID of a subpicture remains unchanged while the subpicture index may change. Even when the raster-scan CTU address of the first CTU in a slice in the subpicture has changed compared to the value in the original bitstream, the unchanged values of subpicture ID and subpicture-level slice index in the respective SH would still correctly determine the position of each CTU in the decoded picture of the extracted sub-bitstream.</p><p id="p-0076" num="0075"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is an example of subpicture extraction <b>400</b>. In particular, <figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates the usage of a subpicture ID, a subpicture index, and a subpicture-level slice index to enable the subpicture extraction <b>400</b>. In the illustrated embodiment, the subpicture extraction <b>400</b> contains two subpictures and four slices.</p><p id="p-0077" num="0076">Similar to subpicture extraction <b>400</b>, the signaling for subpictures allows merging several subpictures from different bitstreams into a single bitstream by only rewriting the SPSs and PPSs, provided that the different bitstreams are coordinately generated (e.g., using distinct subpicture IDs but otherwise mostly aligned SPS, PPS and PH parameters such as CTU size, chroma format, coding tools, etc.).</p><p id="p-0078" num="0077">While subpictures and slices are independently signaled in the SPS and PPS, respectively, there are inherent reciprocal constraints between the subpicture and slice layouts in order to form a conformant bitstream. First, the presence of subpictures mandates rectangular slices and forbids raster-scan slices. Second, the slices of a given subpicture are consecutive NAL units in decoding order, which means that the subpicture layout constrains the order of coded slice NAL units within the bitstream.</p><p id="p-0079" num="0078">Picture-in-picture services offer the ability to include a picture with a small resolution within a picture with a bigger resolution. Such a service may be beneficial to show two videos to a user at the same time, whereby the video with bigger resolution is considered the main video and the video with a smaller resolution is considered the supplementary video. Such a picture-in-picture service can be used to offer accessibility services where the main video is supplemented by a signage video.</p><p id="p-0080" num="0079">VVC subpictures can be used for picture-in-picture services by using both the extraction and merging properties of VVC subpictures. For such a service, the main video is encoded using a number of subpictures. One of the subpictures is the same size as a supplementary video, which is located at the exact location where the supplementary video is intended to be composited into the main video. The supplementary video is coded independently to enable extraction. When a user chooses to view the version of the service that includes the supplementary video, the subpicture that corresponds to the picture-in-picture area of the main video is extracted from the main video bitstream, and the supplementary video bitstream is merged with the main video bitstream in its place.</p><p id="p-0081" num="0080"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is an example of picture-in-picture support <b>500</b> based on VVC subpictures. The portion of the main video labeled Subpic ID 0 may be referred to herein as a target picture-in-picture region since that portion of the main video will be replaced by a supplementary video with Subpic ID 0. That is, the supplementary video is embedded within or overlayed on the main video as shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>. In the example, the pictures of the main video and the supplementary video share the same video characteristics. In particular, the bit depth, sample aspect ratio, size, frame rate, color space and transfer characteristics, and chroma samples location for the main video and the supplementary video must be the same. The main video and supplementary video bitstreams do not need to use the same NAL unit types within each picture. However, the coding order of the pictures in the main video and supplementary video bitstreams needs to be the same.</p><p id="p-0082" num="0081">Since merging of subpictures is used herein, the subpicture IDs used within the main video and the supplementary video cannot overlap. Even when the supplementary video bitstream is composed of only one subpicture without any further tiles or slice partitioning, subpicture information (e.g., in particular subpicture ID and subpicture ID length) needs to be signaled to enable merging of the supplementary video bitstream with the main video bitstream. The subpicture ID length used to signal the length of the subpicture ID syntax element within the slice NAL units of the supplementary video bitstream has to be the same as the subpicture ID length used to signal the subpicture IDs within the slice NAL units of the main video bitstream. In addition, in order to simplify merging of the supplementary video bitstream with the main video bitstream without the need to rewrite the PPS partitioning information, it may be beneficial to use only one slice and one tile for encoding the supplementary video within the corresponding area of the main video. The main and supplementary video bitstreams have to signal the same coding tools in the SPS, PPS, and picture headers. This includes using the same maximum and minimum allowed sizes for block partitioning, and the same value of initial Quantization Parameter as signaled in the PPS (same value of the pps_init_qp_minus26 syntax element). Coding tools usage can be modified at the slice header level.</p><p id="p-0083" num="0082">When both main and supplementary bitstreams are available via a DASH-based delivery system, a DASH Preselection may be used to signal main and supplementary bitstreams that are intended to be merged and rendered together. A DASH Preselection defines a subset of media components of an MPD that are expected to be consumed jointly by a DASH player, wherein consuming may comprise decoding and rendering. The Adaptation Set that contains the main media component for a DASH Preselection is referred to as a main Adaptation Set. In addition, each DASH Preselection may include one or multiple partial Adaptation Sets. Partial Adaptation Sets may need to be processed in combination with the main Adaptation Set. A main Adaptation Set and partial Adaptation Sets may be indicated by a preselection descriptor or by a preselection element.</p><p id="p-0084" num="0083">Unfortunately, the following issues have been observed when attempting to support picture-in-picture services in DASH. First, while it is possible to use a DASH Preselection for the picture-in-picture experience, an indication of such a purpose is lacking.</p><p id="p-0085" num="0084">Second, while it is possible to use VVC subpictures for the picture-in-picture experience (e.g., as discussed above), it is also possible to use other codecs and methods without being able to replace the coded video data units representing the target picture-in-picture region in the main video with the corresponding video data units of the supplementary video. Therefore, there is a need to indicate whether such replacement is possible.</p><p id="p-0086" num="0085">Third, when the above replacement is possible, the client needs to know which coded video data units in each picture of the main video represent the target picture-in-picture region to be able to perform the replacement. Therefore, this information needs to be signaled.</p><p id="p-0087" num="0086">Fourth, for content selection purposes (and possibly other purposes as well), it would be useful to signal the position and size of the target picture-in-picture region in the main video.</p><p id="p-0088" num="0087">Disclosed herein are techniques that solve one or more of the aforementioned problems. For example, the present disclosure provides techniques that incorporate an indicator in a preselection element. In an embodiment, the indicator indicates that the purpose of the preselection element comprises picture-in-picture (a.k.a, a picture-in-picture experience or picture-in-picture services). Thus, the video coding process is improved relative to conventional video coding techniques.</p><p id="p-0089" num="0088">The detailed embodiments below should be considered as examples to explain general concepts. These embodiments should not be interpreted in a narrow way. Furthermore, these embodiments can be combined in any manner.</p><p id="p-0090" num="0089">In the following discussion, a video unit (a.k.a., video data unit) may be a sequence of pictures, a picture, a slice, a tile, a brick, a subpicture, a CTU/coding tree block (CTB), a CTU/CTB row, one or multiple coding units (CUs)/coding blocks (CBs), one or multiple CTUs/CTBs, one or multiple Virtual Pipeline Data Unit (VPDU), a sub-region within a picture/slice/tile/brick. A father video unit (a.k.a., a parent video unit) represents a unit larger than the video unit. Typically, a father video unit will contain several video units, for example, when the video unit is a CTU, the father video unit could be a slice, CTU row, multiple CTUs, etc. In some embodiments, the video unit may be a sample/pixel.</p><heading id="h-0007" level="1">Example 1</heading><p id="p-0091" num="0090">1). To solve the first problem, an indication is signaled in the MPD. The indication indicates that the purpose of the preselection (a.k.a., Preselection, DASH Preselection, etc.) is for providing the picture-in-picture experience. That is, an indicator in the preselection element indicates that a purpose of the preselection element is for providing a picture-in-picture experience where a supplementary video appears to be overlaid on a target picture-in-picture region in a main video.</p><p id="p-0092" num="0091">a. In one example, the indication is signaled by a particular value of the @tag attribute of the preselection (e.g., through the CommonAttributesElements). In an embodiment, the particular value of @tag is &#x201c;PicInPic.&#x201d;</p><p id="p-0093" num="0092">b. In one example, the indication is signaled by a particular value of the @value attribute of the Role element of the preselection. In an embodiment, the particular value of @value is &#x201c;PicInPic.&#x201d;</p><heading id="h-0008" level="1">Example 2</heading><p id="p-0094" num="0093">2) To solve the second problem, an indication is signaled in the MPD. The indication indicates whether the coded video data units representing the target picture-in-picture region in the main video can be replaced by the corresponding video data units of the supplementary video.</p><p id="p-0095" num="0094">a. In one example, the indication is signaled by an attribute. In an embodiment, the attribute is designated or named @dataUnitsReplacable of the preselection.</p><p id="p-0096" num="0095">b. In one example, the indication is specified to be optional.</p><p id="p-0097" num="0096">i. In one example, the indication can only be present when the @tag attribute of the preselection indicates that the purpose of the preselection is for providing the picture-in-picture experience.</p><p id="p-0098" num="0097">c. When the indication is not present, whether such replacement is possible is unknown.</p><p id="p-0099" num="0098">d. In one example, it is specified that, when @dataUnitsReplacable is true, the client may choose to replace the coded video data units representing the target picture-in-picture region in the main video with the corresponding coded video data units of the supplementary video before sending to the video decoder. This way, separate decoding of the main video and the supplementary video can be avoided.</p><p id="p-0100" num="0099">e. In one example, it is specified that, for a particular picture in the main video, the corresponding video data units of the supplementary video are all the coded video data units in the decoding-time-synchronized sample in the supplemental video representation.</p><heading id="h-0009" level="1">Example 3</heading><p id="p-0101" num="0100">3) To solve the third problem, a list of region IDs is signaled in the MPD. The list of region IDs indicate which coded video data units in each picture of the main video represent the target picture-in-picture region.</p><p id="p-0102" num="0101">a. In one example, the list of region IDs is signaled as an attribute of the preselection. In an embodiment, the attribute is named @regionIds.</p><p id="p-0103" num="0102">b. In one example, the indication is specified to be optional.</p><p id="p-0104" num="0103">i. In one example, the indication can only be present when the @tag attribute of the preselection indicates that the purpose of the preselection is for providing the picture-in-picture experience.</p><p id="p-0105" num="0104">c. In one example, it is specified that the concrete semantics of the region IDs need to be explicitly specified for specific video codecs.</p><p id="p-0106" num="0105">i. In one example, it is specified that, in the case of VVC, the region IDs are subpicture IDs, and coded video data units are VCL NAL units. The VCL NAL units representing the target picture-in-picture region in the main video are those having these subpicture IDs, which are the same as the subpicture IDs in the corresponding VCL NAL units of the supplementary video. Typically, all VCL NAL units of one picture in the supplementary video share the same subpicture ID that is explicitly signaled. In this case, there is only one region ID in the list of region IDs.</p><p id="p-0107" num="0106">A layer is a set of VCL NAL units that all have a particular value of nuh_layer_id and the associated non-VCL NAL units. A network abstraction layer (NAL) unit is a syntax structure containing an indication of the type of data to follow and bytes containing that data in the form of a raw byte sequence payload (RBSP) interspersed as necessary with emulation prevention bytes. A video coding layer (VCL) NAL unit is a collective term for coded slice NAL units and the subset of NAL units that have reserved values of nal_unit_type that are classified as VCL NAL units in the VVC standard.</p><p id="p-0108" num="0107">ii. In one example, it is specified that, in the case of VVC, when the client chooses to replace the coded video data units (which are VCL NAL units) representing the target picture-in-picture region in the main video with the corresponding VCL NAL units of the supplementary video before sending to the video decoder, for each subpicture ID, the VCL NAL units in the main video are replaced with the corresponding VCL NAL units having that subpicture ID in the supplementary video, without changing the order of the corresponding VCL NAL units.</p><heading id="h-0010" level="1">Example 4</heading><p id="p-0109" num="0108">4) To solve the fourth problem, the information on the position and size of the main video is signaled in the MPD. In an embodiment, the position and size information of the main video may be used when embedding/overlaying the supplementary video, which is smaller in size than the main video.</p><p id="p-0110" num="0109">a. In one example, the position and size information is signaled by signaling the four values x, y, width, and height. In this embodiment, x and y specify the location of the top left corner of the region, and the width and height specify the width and height of the region. The units can be in luma samples/pixels.</p><p id="p-0111" num="0110">b. In one example, the position and size information is signaled by an attribute or element of the preselection.</p><p id="p-0112" num="0111">c. In one example, the position and size information is signaled by an attribute or element of the Main Adaptation Set of the preselection.</p><p id="p-0113" num="0112">d. In one example, it is specified that, when @dataUnitsReplacable is true and the position and size information is present, the position and size shall represent exactly the target picture-in-picture region in the main video.</p><p id="p-0114" num="0113">e. In one example, it is specified that, when @dataUnitsReplacable is false and the position and size information is present, the position and size information indicates a preferable region for embedding overlaying the supplementary video (i.e., the client may choose to overlay the supplementary video in a different region of the main video).</p><p id="p-0115" num="0114">f. In one example, it is specified that, when @dataUnitsReplacable is false and the position and size information is not present, no information or recommendation on where to overlay the supplementary video is suggested, and where to overlay the supplementary video is completely up to the client to choose.</p><p id="p-0116" num="0115">Below are some example embodiments corresponding to the examples discussed above. The embodiments can be applied to DASH. The most relevant parts that have been added or modified in the syntax and/or semantics below are shown in italics. There may be some other changes that are editorial in nature and thus not highlighted.</p><p id="p-0117" num="0116">The semantics of a preselection element are provided. As an alternative to the preselection descriptor, preselections may also be defined through a preselection element as provided in Table 1. The selection of preselections is based on the contained attributes and elements in the preselection element.</p><p id="p-0118" num="0000"><tables id="TABLE-US-00001" num="00001"><table frame="none" colsep="0" rowsep="0" pgwide="1"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="273pt" align="center"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE 1</entry></row></thead><tbody valign="top"><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>Semantics of Preselection element</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="1" colwidth="91pt" align="left"/><colspec colname="2" colwidth="42pt" align="left"/><colspec colname="3" colwidth="140pt" align="left"/><tbody valign="top"><row><entry>Element or Attribute Name</entry><entry>Use</entry><entry>Description</entry></row><row><entry namest="1" nameend="3" align="center" rowsep="1"/></row><row><entry>Preselection</entry><entry/><entry/></row><row><entry>@id</entry><entry>OD</entry><entry>specifies the id of the Preselection. This</entry></row><row><entry/><entry>default = 1</entry><entry>shall be unique within one Period.</entry></row><row><entry>@preselectionComponents</entry><entry>M</entry><entry>specifies the ids of the contained Adaptation</entry></row><row><entry/><entry/><entry>Sets or Content Components that belong to this</entry></row><row><entry/><entry/><entry>Preselection as white space separated list in</entry></row><row><entry/><entry/><entry>processing order. The first id defines the Main</entry></row><row><entry/><entry/><entry>Adaptation Set.</entry></row><row><entry>@lang</entry><entry>O</entry><entry>same semantics as in Table 5 for @lang</entry></row><row><entry/><entry/><entry>attribute.</entry></row><row><entry>@order</entry><entry>OD</entry><entry>specifies the conformance rules for</entry></row><row><entry/><entry>Default:</entry><entry>Representations in Adaptation Sets within the</entry></row><row><entry/><entry>&#x2018;undefined&#x2019;</entry><entry>Preselection.</entry></row><row><entry/><entry/><entry>When set to &#x2018;undefined&#x2019;, the Preselection</entry></row><row><entry/><entry/><entry>follows the conformance rules for Multi-</entry></row><row><entry/><entry/><entry>Segment Tracks in subclause 5.3.11.5.1.</entry></row><row><entry/><entry/><entry>When set to &#x2018;time-ordered&#x2019;, the Preselection</entry></row><row><entry/><entry/><entry>follows the conformance rules for Time-</entry></row><row><entry/><entry/><entry>Ordered Segment Tracks in subclause</entry></row><row><entry/><entry/><entry>5.3.11.5.2.</entry></row><row><entry/><entry/><entry>When set to &#x2018;fully-ordered&#x2019;, the Preselection</entry></row><row><entry/><entry/><entry>follows the conformance rules for Fully-</entry></row><row><entry/><entry/><entry>Ordered Segment Tracks in subclause</entry></row><row><entry/><entry/><entry>5.3.11.5.3. In this case, order in the</entry></row><row><entry/><entry/><entry>@preselectionComponents attribute specifies</entry></row><row><entry/><entry/><entry>the component order.</entry></row><row><entry>@dataUnitsReplacable</entry><entry>O</entry><entry>specifies whether the coded video data units</entry></row><row><entry/><entry/><entry>representing the target picture-in-picture</entry></row><row><entry/><entry/><entry>region in the main video can be replaced by</entry></row><row><entry/><entry/><entry>the corresponding video data units of the</entry></row><row><entry/><entry/><entry>supplementary video.</entry></row><row><entry/><entry/><entry>This attribute can only be present when the</entry></row><row><entry/><entry/><entry>@tag attribute of this Preselection (through</entry></row><row><entry/><entry/><entry>the <b>CommonAttributesElements </b>element) is</entry></row><row><entry/><entry/><entry>equal to &#x201c;PicInPic&#x201d;.</entry></row><row><entry/><entry/><entry>When @tag is equal to &#x201c;PicInPic&#x201d; and this</entry></row><row><entry/><entry/><entry>attribute is not present, it is unspecified</entry></row><row><entry/><entry/><entry>whether the coded video data units</entry></row><row><entry/><entry/><entry>representing the target picture-in-picture</entry></row><row><entry/><entry/><entry>region in the main video can be replaced by</entry></row><row><entry/><entry/><entry>the corresponding video data units of the</entry></row><row><entry/><entry/><entry>supplementary video.</entry></row><row><entry/><entry/><entry>When @tag is equal to &#x201c;PicInPic&#x201d; and the</entry></row><row><entry/><entry/><entry>@dataUnitsReplacable attribute is present and</entry></row><row><entry/><entry/><entry>equal to true, the client may choose to replace</entry></row><row><entry/><entry/><entry>the coded video data units representing the</entry></row><row><entry/><entry/><entry>target picture-in-picture region in the main</entry></row><row><entry/><entry/><entry>video with the corresponding coded video data</entry></row><row><entry/><entry/><entry>units of the supplementary video before</entry></row><row><entry/><entry/><entry>sending to the video decoder.</entry></row><row><entry>@regionIds</entry><entry>O</entry><entry>specifies the IDs of the coded video data units</entry></row><row><entry/><entry/><entry>representing the target picture-in-picture</entry></row><row><entry/><entry/><entry>region, as a white space separated list.</entry></row><row><entry/><entry/><entry>This attribute can only be present when the</entry></row><row><entry/><entry/><entry>@tag attribute of this Preselection (through</entry></row><row><entry/><entry/><entry>the <b>CommonAttributesElements </b>element) is</entry></row><row><entry/><entry/><entry>equal to &#x201c;PicInPic&#x201d;.</entry></row><row><entry/><entry/><entry>The concrete semantics of the region IDs need</entry></row><row><entry/><entry/><entry>to be explicitly specified for specific video</entry></row><row><entry/><entry/><entry>codecs.</entry></row><row><entry/><entry/><entry>In the case of VVC, the region IDs are</entry></row><row><entry/><entry/><entry>subpicture IDs, and coded video data units are</entry></row><row><entry/><entry/><entry>VCL NAL units. The VCL NAL units</entry></row><row><entry/><entry/><entry>representing the target picture-in-picture</entry></row><row><entry/><entry/><entry>region in the main video are those having</entry></row><row><entry/><entry/><entry>these subpicture IDs, which are the same as</entry></row><row><entry/><entry/><entry>the subpicture IDs in the corresponding VCL</entry></row><row><entry/><entry/><entry>NAL units of the supplementary video</entry></row><row><entry/><entry/><entry>(typically all VCL NAL units of one picture in</entry></row><row><entry/><entry/><entry>the supplementary video share the same</entry></row><row><entry/><entry/><entry>subpicture ID that is explicitly signalled, and</entry></row><row><entry/><entry/><entry>in this case, there is only one region ID in the</entry></row><row><entry/><entry/><entry>list of region IDs).</entry></row><row><entry><b>Accessibility</b></entry><entry>0 . . . N</entry><entry>specifies information about accessibility</entry></row><row><entry/><entry/><entry>scheme.</entry></row><row><entry/><entry/><entry>For more details, refer to subclauses 5.8.1</entry></row><row><entry/><entry/><entry>and 5.8.4.3.</entry></row><row><entry><b>Role</b></entry><entry>0 . . . N</entry><entry>specifies information on role annotation</entry></row><row><entry/><entry/><entry>scheme.</entry></row><row><entry/><entry/><entry>For more details, refer to subclauses 5.8.1</entry></row><row><entry/><entry/><entry>and 5.8.4.2.</entry></row><row><entry><b>Rating</b></entry><entry>0 . . . N</entry><entry>specifies information on rating scheme.</entry></row><row><entry/><entry/><entry>For more details, refer to subclauses 5.8.1</entry></row><row><entry/><entry/><entry>and 5.8.4.4.</entry></row><row><entry><b>Viewpoint</b></entry><entry>0 . . . N</entry><entry>specifies information on viewpoint annotation</entry></row><row><entry/><entry/><entry>scheme.</entry></row><row><entry/><entry/><entry>For more details, refer to subclauses 5.8.1 and</entry></row><row><entry/><entry/><entry>5.8.4.5.</entry></row><row><entry><b>CommonAttributesElements</b></entry><entry>&#x2014;</entry><entry>specifies the common attributes and elements</entry></row><row><entry/><entry/><entry>(attributes and elements from base type</entry></row><row><entry/><entry/><entry><b>RepresentationBaseType</b>). For details, see</entry></row><row><entry/><entry/><entry>subclause 5.3.7.</entry></row><row><entry namest="1" nameend="3" align="center" rowsep="1"/></row><row><entry namest="1" nameend="3" align="left" id="FOO-00001">Key</entry></row><row><entry namest="1" nameend="3" align="left" id="FOO-00002">For attributes: M = mandatory, O = Optional, OD = optional with default value, CM = conditionally mandatory</entry></row><row><entry namest="1" nameend="3" align="left" id="FOO-00003">For elements: &#x3c;minOccurs&#x3e; . . . &#x3c;maxOccurs&#x3e; (N = unbounded)</entry></row><row><entry namest="1" nameend="3" align="left" id="FOO-00004">Elements are bold; attributes are non-bold and preceded with an @.</entry></row></tbody></tgroup></table></tables></p><p id="p-0119" num="0117">The Extensible Markup Language (XML) Syntax is provided.</p><p id="p-0120" num="0000"><tables id="TABLE-US-00002" num="00002"><table frame="none" colsep="0" rowsep="0" pgwide="1"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="259pt" align="left"/><thead><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry>&#x3c;!-- Preselection --&#x3e;</entry></row><row><entry>&#x3c;xs:complexType name=&#x201c;PreselectionType&#x201d;&#x3e;</entry></row><row><entry>&#x2003;&#x3c;xs:complexContent&#x3e;</entry></row><row><entry>&#x2003;&#x2003;&#x3c;xs:extension base=&#x201c;RepresentationBaseType&#x201d;&#x3e;</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x3c;xs:sequence&#x3e;</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x3c;xs:element name=&#x201c;Accessibility&#x201d; type=&#x201c;DescriptorType&#x201d;</entry></row><row><entry>minOccurs=&#x201c;0&#x201d; maxOccurs=&#x201c;unbounded&#x201d;/&#x3e;</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x3c;xs:element name=&#x201c;Role&#x201d; type=&#x201c;DescriptorType&#x201d; minOccurs=&#x201c;0&#x201d;</entry></row><row><entry>maxOccurs=&#x201c;unbounded&#x201d;/&#x3e;</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x3c;xs:element name=&#x201c;Rating&#x201d; type=&#x201c;DescriptorType&#x201d; minOccurs=&#x201c;0&#x201d;</entry></row><row><entry>maxOccurs=&#x201c;unbounded&#x201d;/&#x3e;</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x3c;xs:element name=&#x201c;Viewpoint&#x201d; type=&#x201c;DescriptorType&#x201d;</entry></row><row><entry>minOccurs=&#x201c;0&#x201d; maxOccurs=&#x201c;unbounded&#x201d;/&#x3e;</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x3c;/xs:sequence&#x3e;</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x3c;xs:attribute name=&#x201c;id&#x201d; type=&#x201c;StringNoWhitespaceType&#x201d; default=&#x201c;1&#x201d;/&#x3e;</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x3c;xs:attribute name=&#x201c;preselectionComponents&#x201d; type=&#x201c;StringVectorType&#x201d;</entry></row><row><entry>use=&#x201c;required&#x201d;/&#x3e;</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x3c;xs:attribute name=&#x201c;lang&#x201d; type=&#x201c;xs:language&#x201d;/&#x3e;</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x3c;xs:attribute name=&#x201c;order&#x201d; type=&#x201c;PreselectionOrderType&#x201d;</entry></row><row><entry>default=&#x201c;undefined&#x201d;/&#x3e;</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x3c;xs:attribute name=&#x201c; dataUnitsReplacable&#x201d; type=&#x201c;xs:boolean&#x201d;/&#x3e;</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x3c;xs:attribute name=&#x201c;regionIds&#x201d; type=&#x201c;StringNoWhitespaceType&#x201d;/&#x3e;</entry></row><row><entry>&#x2003;&#x2003;&#x3c;/xs:extension&#x3e;</entry></row><row><entry>&#x2003;&#x3c;/xs:complexContent&#x3e;</entry></row><row><entry>&#x3c;/xs:complexType&#x3e;</entry></row><row><entry>&#x3c;!--Preselection Order type--&#x3e;</entry></row><row><entry>&#x3c;xs:simpleType name=&#x201c;PreselectionOrderType&#x201d;&#x3e;</entry></row><row><entry>&#x2003;&#x3c;xs:restriction base=&#x201c;xs:string&#x201d;&#x3e;</entry></row><row><entry>&#x2003;&#x2003;&#x3c;xs:enumeration value=&#x201c;undefined&#x201d;/&#x3e;</entry></row><row><entry>&#x2003;&#x2003;&#x3c;xs:enumeration value=&#x201c;time-ordered&#x201d;/&#x3e;</entry></row><row><entry>&#x2003;&#x2003;&#x3c;xs:enumeration value=&#x201c;fully-ordered&#x201d;/&#x3e;</entry></row><row><entry>&#x2003;&#x3c;/xs:restriction&#x3e;</entry></row><row><entry>&#x3c;/xs:simpleType&#x3e;</entry></row><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0121" num="0118">Picture-in-Picture based on preselection is discussed.</p><p id="p-0122" num="0119">When the value of the @tag attribute (through the CommonAttributesElements element) of a Preselection is equal to &#x201c;PicInPic&#x201d;, the purpose of the Preselection is for the picture-in-picture experience. Picture-in-picture services offer the ability to include a video with a small spatial resolution within a video with a bigger spatial resolution. In this case, the different bitstreams/Representations of the main video are included in the Main Adaptation Set of the Preselection, and the different bitstreams/Representations of a supplementary video are included a Partial Adaptation Set of the Preselection.</p><p id="p-0123" num="0120">When @tag is equal to &#x201c;PicInPic&#x201d; and the @dataUnitsReplacable attribute is present and equal to true, the client may choose to replace the coded video data units representing the target picture-in-picture region in the main video with the corresponding coded video data units of the supplementary video before sending to the video decoder. This way, separate decoding of the main video and the supplementary video can be avoided.</p><p id="p-0124" num="0121">For a particular picture in the main video, the corresponding video data units of the supplementary video are all the coded video data units in the decoding-time-synchronized sample in the supplemental video Representation.</p><p id="p-0125" num="0122">In the case of VVC, when the client chooses to replace the coded video data units (which are VCL NAL units) representing the target picture-in-picture region in the main video with the corresponding VCL NAL units of the supplementary video before sending to the video decoder, for each subpicture ID, the VCL NAL units in the main video are replaced with the corresponding VCL NAL units having that subpicture ID in the supplementary video, without changing the order of the corresponding VCL NAL units.</p><p id="p-0126" num="0123"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a block diagram showing an example video processing system <b>600</b> in which various techniques disclosed herein may be implemented. Various implementations may include some or all of the components of the video processing system <b>600</b>. The video processing system <b>600</b> may include input <b>602</b> for receiving video content. The video content may be received in a raw or uncompressed format, e.g., 8 or 10 bit multi-component pixel values, or may be in a compressed or encoded format. The input <b>602</b> may represent a network interface, a peripheral bus interface, or a storage interface. Examples of network interface include wired interfaces such as Ethernet, passive optical network (PON), etc. and wireless interfaces such as Wi-Fi or cellular interfaces.</p><p id="p-0127" num="0124">The video processing system <b>600</b> may include a coding component <b>604</b> that may implement the various coding or encoding methods described in the present document. The coding component <b>604</b> may reduce the average bitrate of video from the input <b>602</b> to the output of the coding component <b>604</b> to produce a coded representation of the video. The coding techniques are therefore sometimes called video compression or video transcoding techniques. The output of the coding component <b>604</b> may be either stored, or transmitted via a communication connected, as represented by the component <b>606</b>. The stored or communicated bitstream (or coded) representation of the video received at the input <b>602</b> may be used by the component <b>608</b> for generating pixel values or displayable video that is sent to a display interface <b>610</b>. The process of generating user-viewable video from the bitstream representation is sometimes called video decompression. Furthermore, while certain video processing operations are referred to as &#x201c;coding&#x201d; operations or tools, it will be appreciated that the coding tools or operations are used at an encoder and corresponding decoding tools or operations that reverse the results of the coding will be performed by a decoder.</p><p id="p-0128" num="0125">Examples of a peripheral bus interface or a display interface may include universal serial bus (USB) or high definition multimedia interface (HDMI) or Displayport, and so on. Examples of storage interfaces include SATA (serial advanced technology attachment), Peripheral Component Interconnect (PCI), Integrated Drive Electronics (IDE) interface, and the like. The techniques described in the present document may be embodied in various electronic devices such as mobile phones, laptops, smartphones or other devices that are capable of performing digital data processing and/or video display.</p><p id="p-0129" num="0126"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a block diagram of a video processing apparatus <b>700</b>. The video processing apparatus <b>700</b> may be used to implement one or more of the methods described herein. The video processing apparatus <b>700</b> may be embodied in a smartphone, tablet, computer, Internet of Things (IoT) receiver, and so on. The video processing apparatus <b>700</b> may include one or more processors <b>702</b>, one or more memories <b>704</b> and video processing hardware <b>706</b> (a.k.a., video processing circuitry). The processor(s) <b>702</b> may be configured to implement one or more methods described in the present document. The memory (memories) <b>704</b> may be used for storing data and code used for implementing the methods and techniques described herein. The video processing hardware <b>706</b> may be used to implement, in hardware circuitry, some techniques described in the present document. In some embodiments, the video processing hardware <b>706</b> may be partly or completely located within the processor <b>702</b>, e.g., a graphics processor.</p><p id="p-0130" num="0127"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a block diagram that illustrates an example of a video coding system <b>800</b> that may utilize the techniques of this disclosure. As shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the video coding system <b>800</b> may include a source device <b>810</b> and a destination device <b>820</b>. Source device <b>810</b> generates encoded video data which may be referred to as a video encoding device. Destination device <b>820</b> may decode the encoded video data generated by source device <b>810</b> which may be referred to as a video decoding device.</p><p id="p-0131" num="0128">Source device <b>810</b> may include a video source <b>812</b>, a video encoder <b>814</b>, and an input/output (I/O) interface <b>816</b>.</p><p id="p-0132" num="0129">Video source <b>812</b> may include a source such as a video capture device, an interface to receive video data from a video content provider, and/or a computer graphics system for generating video data, or a combination of such sources. The video data may comprise one or more pictures. Video encoder <b>814</b> encodes the video data from video source <b>812</b> to generate a bitstream. The bitstream may include a sequence of bits that form a coded representation of the video data. The bitstream may include coded pictures and associated data. The coded picture is a coded representation of a picture. The associated data may include sequence parameter sets, picture parameter sets, and other syntax structures. I/O interface <b>816</b> may include a modulator/demodulator (modem) and/or a transmitter. The encoded video data may be transmitted directly to destination device <b>820</b> via I/O interface <b>816</b> through network <b>830</b>. The encoded video data may also be stored onto a storage medium/server <b>840</b> for access by destination device <b>820</b>.</p><p id="p-0133" num="0130">Destination device <b>820</b> may include an I/O interface <b>826</b>, a video decoder <b>824</b>, and a display device <b>822</b>.</p><p id="p-0134" num="0131">I/O interface <b>826</b> may include a receiver and/or a modem. I/O interface <b>826</b> may acquire encoded video data from the source device <b>810</b> or the storage medium/server <b>840</b>. Video decoder <b>824</b> may decode the encoded video data. Display device <b>822</b> may display the decoded video data to a user. Display device <b>822</b> may be integrated with the destination device <b>820</b>, or may be external to destination device <b>820</b> which may be configured to interface with an external display device.</p><p id="p-0135" num="0132">Video encoder <b>814</b> and video decoder <b>824</b> may operate according to a video compression standard, such as the High Efficiency Video Coding (HEVC) standard, Versatile Video Coding (VVC) standard, and other current and/or further standards.</p><p id="p-0136" num="0133"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a block diagram illustrating an example of a video encoder <b>900</b>, which may be video encoder <b>814</b> in the video coding system <b>800</b> illustrated in <figref idref="DRAWINGS">FIG. <b>8</b></figref>.</p><p id="p-0137" num="0134">Video encoder <b>900</b> may be configured to perform any or all of the techniques of this disclosure. In the example of <figref idref="DRAWINGS">FIG. <b>9</b></figref>, video encoder <b>900</b> includes a plurality of functional components. The techniques described in this disclosure may be shared among the various components of video encoder <b>900</b>. In some examples, a processor may be configured to perform any or all of the techniques described in this disclosure.</p><p id="p-0138" num="0135">The functional components of video encoder <b>900</b> may include a partition unit <b>901</b>, a prediction unit <b>902</b> which may include a mode selection unit <b>903</b>, a motion estimation unit <b>904</b>, a motion compensation unit <b>905</b> and an intra prediction unit <b>906</b>, a residual generation unit <b>907</b>, a transform unit <b>908</b>, a quantization unit <b>909</b>, an inverse quantization unit <b>910</b>, an inverse transform unit <b>911</b>, a reconstruction unit <b>912</b>, a buffer <b>913</b>, and an entropy encoding unit <b>914</b>.</p><p id="p-0139" num="0136">In other examples, video encoder <b>900</b> may include more, fewer, or different functional components. In an example, prediction unit <b>902</b> may include an intra block copy (IBC) unit. The IBC unit may perform prediction in an IBC mode in which at least one reference picture is a picture where the current video block is located.</p><p id="p-0140" num="0137">Furthermore, some components, such as motion estimation unit <b>904</b> and motion compensation unit <b>905</b> may be highly integrated, but are represented in the example of <figref idref="DRAWINGS">FIG. <b>9</b></figref> separately for purposes of explanation.</p><p id="p-0141" num="0138">Partition unit <b>901</b> may partition a picture into one or more video blocks. Video encoder <b>814</b> and video decoder <b>824</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref> may support various video block sizes.</p><p id="p-0142" num="0139">Mode selection unit <b>903</b> may select one of the coding modes, intra or inter, e.g., based on error results, and provide the resulting intra- or inter-coded block to a residual generation unit <b>907</b> to generate residual block data and to a reconstruction unit <b>912</b> to reconstruct the encoded block for use as a reference picture. In some examples, mode selection unit <b>903</b> may select a combination of intra and inter prediction (CIIP) mode in which the prediction is based on an inter prediction signal and an intra prediction signal. Mode selection unit <b>903</b> may also select a resolution for a motion vector (e.g., a sub-pixel or integer pixel precision) for the block in the case of inter-prediction.</p><p id="p-0143" num="0140">To perform inter prediction on a current video block, motion estimation unit <b>904</b> may generate motion information for the current video block by comparing one or more reference frames from buffer <b>913</b> to the current video block. Motion compensation unit <b>905</b> may determine a predicted video block for the current video block based on the motion information and decoded samples of pictures from buffer <b>913</b> other than the picture associated with the current video block.</p><p id="p-0144" num="0141">Motion estimation unit <b>904</b> and motion compensation unit <b>905</b> may perform different operations for a current video block, for example, depending on whether the current video block is an I slice, a P slice, or a B slice. I-slices (or I-frames) are the least compressible but don't require other video frames to decode. P-slices (or P-frames) can use data from previous frames to decompress and are more compressible than I-frames. B-slices (or B-frames) can use both previous and forward frames for data reference to get the highest amount of data compression.</p><p id="p-0145" num="0142">In some examples, motion estimation unit <b>904</b> may perform uni-directional prediction for the current video block, and motion estimation unit <b>904</b> may search reference pictures of list 0 or list 1 for a reference video block for the current video block. Motion estimation unit <b>904</b> may then generate a reference index that indicates the reference picture in list 0 or list 1 that contains the reference video block and a motion vector that indicates a spatial displacement between the current video block and the reference video block. Motion estimation unit <b>904</b> may output the reference index, a prediction direction indicator, and the motion vector as the motion information of the current video block. Motion compensation unit <b>905</b> may generate the predicted video block of the current block based on the reference video block indicated by the motion information of the current video block.</p><p id="p-0146" num="0143">In other examples, motion estimation unit <b>904</b> may perform bi-directional prediction for the current video block, motion estimation unit <b>904</b> may search the reference pictures in list 0 for a reference video block for the current video block and may also search the reference pictures in list 1 for another reference video block for the current video block. Motion estimation unit <b>904</b> may then generate reference indexes that indicate the reference pictures in list 0 and list 1 containing the reference video blocks and motion vectors that indicate spatial displacements between the reference video blocks and the current video block. Motion estimation unit <b>904</b> may output the reference indexes and the motion vectors of the current video block as the motion information of the current video block. Motion compensation unit <b>905</b> may generate the predicted video block of the current video block based on the reference video blocks indicated by the motion information of the current video block.</p><p id="p-0147" num="0144">In some examples, motion estimation unit <b>904</b> may output a full set of motion information for decoding processing of a decoder.</p><p id="p-0148" num="0145">In some examples, motion estimation unit <b>904</b> may not output a full set of motion information for the current video. Rather, motion estimation unit <b>904</b> may signal the motion information of the current video block with reference to the motion information of another video block. For example, motion estimation unit <b>904</b> may determine that the motion information of the current video block is sufficiently similar to the motion information of a neighboring video block.</p><p id="p-0149" num="0146">In one example, motion estimation unit <b>904</b> may indicate, in a syntax structure associated with the current video block, a value that indicates to the video decoder <b>824</b> that the current video block has the same motion information as another video block.</p><p id="p-0150" num="0147">In another example, motion estimation unit <b>904</b> may identify, in a syntax structure associated with the current video block, another video block and a motion vector difference (MVD). The motion vector difference indicates a difference between the motion vector of the current video block and the motion vector of the indicated video block. The video decoder <b>824</b> may use the motion vector of the indicated video block and the motion vector difference to determine the motion vector of the current video block.</p><p id="p-0151" num="0148">As discussed above, video encoder <b>814</b> may predictively signal the motion vector. Two examples of predictive signaling techniques that may be implemented by video encoder <b>814</b> include advanced motion vector prediction (AMVP) and merge mode signaling.</p><p id="p-0152" num="0149">Intra prediction unit <b>906</b> may perform intra prediction on the current video block. When intra prediction unit <b>906</b> performs intra prediction on the current video block, intra prediction unit <b>906</b> may generate prediction data for the current video block based on decoded samples of other video blocks in the same picture. The prediction data for the current video block may include a predicted video block and various syntax elements.</p><p id="p-0153" num="0150">Residual generation unit <b>907</b> may generate residual data for the current video block by subtracting (e.g., indicated by the minus sign) the predicted video block(s) of the current video block from the current video block. The residual data of the current video block may include residual video blocks that correspond to different sample components of the samples in the current video block.</p><p id="p-0154" num="0151">In other examples, there may be no residual data for the current video block, for example in a skip mode, and residual generation unit <b>907</b> may not perform the subtracting operation.</p><p id="p-0155" num="0152">Transform unit <b>908</b> may generate one or more transform coefficient video blocks for the current video block by applying one or more transforms to a residual video block associated with the current video block.</p><p id="p-0156" num="0153">After transform unit <b>908</b> generates a transform coefficient video block associated with the current video block, quantization unit <b>909</b> may quantize the transform coefficient video block associated with the current video block based on one or more quantization parameter (QP) values associated with the current video block.</p><p id="p-0157" num="0154">Inverse quantization unit <b>910</b> and inverse transform unit <b>911</b> may apply inverse quantization and inverse transforms to the transform coefficient video block, respectively, to reconstruct a residual video block from the transform coefficient video block. Reconstruction unit <b>912</b> may add the reconstructed residual video block to corresponding samples from one or more predicted video blocks generated by the prediction unit <b>902</b> to produce a reconstructed video block associated with the current block for storage in the buffer <b>913</b>.</p><p id="p-0158" num="0155">After reconstruction unit <b>912</b> reconstructs the video block, loop filtering operation may be performed to reduce video blocking artifacts in the video block.</p><p id="p-0159" num="0156">Entropy encoding unit <b>914</b> may receive data from other functional components of the video encoder <b>900</b>. When entropy encoding unit <b>914</b> receives the data, entropy encoding unit <b>914</b> may perform one or more entropy encoding operations to generate entropy encoded data and output a bitstream that includes the entropy encoded data.</p><p id="p-0160" num="0157"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a block diagram illustrating an example of a video decoder <b>1000</b>, which may be video decoder <b>824</b> in the video coding system <b>800</b> illustrated in <figref idref="DRAWINGS">FIG. <b>8</b></figref>.</p><p id="p-0161" num="0158">The video decoder <b>1000</b> may be configured to perform any or all of the techniques of this disclosure. In the example of <figref idref="DRAWINGS">FIG. <b>10</b></figref>, the video decoder <b>1000</b> includes a plurality of functional components. The techniques described in this disclosure may be shared among the various components of the video decoder <b>1000</b>. In some examples, a processor may be configured to perform any or all of the techniques described in this disclosure.</p><p id="p-0162" num="0159">In the example of <figref idref="DRAWINGS">FIG. <b>10</b></figref>, video decoder <b>1000</b> includes an entropy decoding unit <b>1001</b>, a motion compensation unit <b>1002</b>, an intra prediction unit <b>1003</b>, an inverse quantization unit <b>1004</b>, an inverse transformation unit <b>1005</b>, a reconstruction unit <b>1006</b> and a buffer <b>1007</b>. Video decoder <b>1000</b> may, in some examples, perform a decoding pass generally reciprocal to the encoding pass described with respect to video encoder <b>814</b> (<figref idref="DRAWINGS">FIG. <b>8</b></figref>).</p><p id="p-0163" num="0160">Entropy decoding unit <b>1001</b> may retrieve an encoded bitstream. The encoded bitstream may include entropy coded video data (e.g., encoded blocks of video data). Entropy decoding unit <b>1001</b> may decode the entropy coded video data, and from the entropy decoded video data, motion compensation unit <b>1002</b> may determine motion information including motion vectors, motion vector precision, reference picture list indexes, and other motion information. Motion compensation unit <b>1002</b> may, for example, determine such information by performing the AMVP and merge mode signaling.</p><p id="p-0164" num="0161">Motion compensation unit <b>1002</b> may produce motion compensated blocks, possibly performing interpolation based on interpolation filters. Identifiers for interpolation filters to be used with sub-pixel precision may be included in the syntax elements.</p><p id="p-0165" num="0162">Motion compensation unit <b>1002</b> may use interpolation filters as used by video encoder <b>814</b> during encoding of the video block to calculate interpolated values for sub-integer pixels of a reference block. Motion compensation unit <b>1002</b> may determine the interpolation filters used by video encoder <b>814</b> according to received syntax information and use the interpolation filters to produce predictive blocks.</p><p id="p-0166" num="0163">Motion compensation unit <b>1002</b> may use some of the syntax information to determine sizes of blocks used to encode frame(s) and/or slice(s) of the encoded video sequence, partition information that describes how each macroblock of a picture of the encoded video sequence is partitioned, modes indicating how each partition is encoded, one or more reference frames (and reference frame lists) for each inter-encoded block, and other information to decode the encoded video sequence.</p><p id="p-0167" num="0164">Intra prediction unit <b>1003</b> may use intra prediction modes for example received in the bitstream to form a prediction block from spatially adjacent blocks. Inverse quantization unit <b>1004</b> inverse quantizes, i.e., de-quantizes, the quantized video block coefficients provided in the bitstream and decoded by entropy decoding unit <b>1001</b>. Inverse transform unit <b>1005</b> applies an inverse transform.</p><p id="p-0168" num="0165">Reconstruction unit <b>1006</b> may sum the residual blocks with the corresponding prediction blocks generated by motion compensation unit <b>1002</b> or intra-prediction unit <b>1003</b> to form decoded blocks. If desired, a deblocking filter may also be applied to filter the decoded blocks in order to remove blockiness artifacts. The decoded video blocks are then stored in buffer <b>1007</b>, which provides reference blocks for subsequent motion compensation/intra prediction and also produces decoded video for presentation on a display device.</p><p id="p-0169" num="0166"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a method <b>1100</b> of processing media data according to an embodiment of the disclosure. The method <b>1100</b> may be performed by a coding apparatus (e.g., an encoder) having a processor and a memory. The method <b>1100</b> may be implemented when picture-in-picture services need to be indicated.</p><p id="p-0170" num="0167">In block <b>1102</b>, the coding apparatus determines, for a conversion between the media data and a media data file, that a preselection element includes an indicator. In an embodiment, the indicator indicates that a purpose of the preselection element is for providing a picture-in-picture experience where a supplementary video appears to be overlaid on a target picture-in-picture region in a main video.</p><p id="p-0171" num="0168">In block <b>1104</b>, the coding apparatus performs a conversion between the media data and the media data file based on the indicator. When implemented in an encoder, converting includes receiving a media file (e.g., a video unit) and encoding the media file into a bitstream. When implemented in a decoder, converting includes receiving a bitstream including the media file, and decoding the bitstream to obtain the media file.</p><p id="p-0172" num="0169">In an embodiment, the indicator comprises an @tag attribute. In an embodiment, the indicator comprises an @value attribute. In an embodiment, the indicator has a value of &#x201c;PicInPic&#x201d; to indicate that the purpose of the preselection element is for providing the picture-in-picture experience.</p><p id="p-0173" num="0170">In an embodiment, the indicator comprises an @tag attribute having a value of &#x201c;PicInPic&#x201d; to indicate that the purpose of the preselection element is for providing the picture-in-picture experience. In an embodiment, the indicator comprises an @value attribute having a value of &#x201c;PicInPic&#x201d; to indicate that the purpose of the preselection element is for providing the picture-in-picture experience.</p><p id="p-0174" num="0171">In an embodiment, the indicator comprises an @tag attribute within a CommonAttributesElements element in the preselection element. In an embodiment, the indicator comprises an @value attribute of a Role element in the preselection element.</p><p id="p-0175" num="0172">In an embodiment, the indicator comprises an @tag attribute that also specifies a representation, adaption set, or the preselection element that may be used for selection purposes toward a decoder. In an embodiment, the indicator comprises an @value attribute of a Role element that specifies information on a role annotation scheme.</p><p id="p-0176" num="0173">In an embodiment, the preselection element includes an @dataUnitsReplacable attribute that specifies whether coded video data units representing the target picture-in-picture region in the main video can be replaced by corresponding video data units of the supplementary video. In an embodiment, the preselection element only includes an @dataUnitsReplacable attribute when the indicator is an @tag attribute having a value of &#x201c;PicInPic.&#x201d;</p><p id="p-0177" num="0174">In an embodiment, the preselection element includes an @regionsIds attribute that specifies an ID for each coded video data unit representing the target picture-in-picture region in the main video. In an embodiment, the preselection element only includes an @regionsIds attribute when the indicator is an @tag attribute having a value of &#x201c;PicInPic.&#x201d;</p><p id="p-0178" num="0175">In an embodiment, the preselection element is disposed in a Media Presentation Description (MPD) file. In an embodiment, the preselection element is a Dynamic Adaptive Streaming over Hypertext Transfer Protocol (DASH) preselection element.</p><p id="p-0179" num="0176">In an embodiment, the conversion includes encoding the media data into a bitstream. In an embodiment, the conversion includes decoding the media data from a bitstream.</p><p id="p-0180" num="0177">In an embodiment, the method <b>1100</b> may utilize or incorporate one or more of the features or processes of the other methods disclosed herein.</p><p id="p-0181" num="0178">A listing of solutions preferred by some embodiments is provided next.</p><p id="p-0182" num="0179">The following solutions show example embodiments of techniques discussed in the present disclosure (e.g., Example 1).</p><p id="p-0183" num="0180">1. A method of processing video data, comprising: performing a conversion between a video data and a descriptor of the video data, wherein the descriptor conforms to a format rule, and wherein the format rule specifies that the descriptor includes a syntax element indicative of a picture-in-picture use of a Preselection syntax structure of the descriptor.</p><p id="p-0184" num="0181">2. The method of claim <b>1</b>, wherein the syntax element is a tag attribute of the Preselection syntax structure.</p><p id="p-0185" num="0182">3. The method of claim <b>1</b>, wherein the syntax element is a role attribute of the Preselection syntax structure.</p><p id="p-0186" num="0183">4. A method of processing video data, comprising: performing a conversion between a video data and a descriptor of the video data, wherein the descriptor conforms to a format rule, and wherein the format rule specifies that the descriptor selectively includes a syntax element indicative of whether video data units of a main video in the video data corresponding to a picture-in-picture region can be replaced with the video data units of a supplementary video in the video data.</p><p id="p-0187" num="0184">5. The method of claim <b>4</b>, wherein the syntax element is an attribute field in the descriptor.</p><p id="p-0188" num="0185">6. The method of claim <b>4</b>, wherein the syntax element is selectively included based on a value of a tag attribute in the descriptor.</p><p id="p-0189" num="0186">7. A method of processing video data, comprising: performing a conversion between a video data and a descriptor of the video data, wherein the descriptor conforms to a format rule, and wherein the format rule specifies that the descriptor includes a list of region identifiers that indicates video data units in pictures of a main video in the video data corresponding to target picture-in-picture regions.</p><p id="p-0190" num="0187">8. The method of claim <b>7</b>, wherein the list is included as an attribute of a Preselection syntax structure in the descriptor.</p><p id="p-0191" num="0188">9. The method of any of claims <b>7</b>-<b>8</b>, wherein the region identifiers correspond to syntax fields used for indicating the video data units according to a coding scheme used for coding the main video.</p><p id="p-0192" num="0189">10. A method of processing video data, comprising: performing a conversion between a video data and a descriptor of the video data, wherein the descriptor conforms to a format rule, and wherein the format rule specifies that the descriptor includes one or more fields indicative of a position and/or a size information of a region in a main video that is used for overlaying or embedding a supplementary video.</p><p id="p-0193" num="0190">11. The method of claim <b>10</b>, wherein the position and the size information comprises four values including a location coordinate, a height and a width of the region.</p><p id="p-0194" num="0191">12. The method of any of claims <b>10</b>-<b>11</b>, wherein the one or more fields comprises an attribute or an element of a Preselection syntax structure.</p><p id="p-0195" num="0192">13. The method of any of claims <b>10</b>-<b>12</b>, wherein whether the region is an exact replaceable region or a preferred replaceable region is determined based on another syntax element.</p><p id="p-0196" num="0193">14. The method of any of claims <b>1</b>-<b>13</b>, wherein the descriptor is a Media Presentation Description (MPD).</p><p id="p-0197" num="0194">15. The method of any of claims <b>1</b>-<b>14</b>, wherein the conversion includes generating the bitstream from the video.</p><p id="p-0198" num="0195">16. The method of any of claims <b>1</b>-<b>14</b>, wherein the conversion includes generating the video from the bitstream.</p><p id="p-0199" num="0196">17. A video decoding apparatus comprising a processor configured to implement a method recited in one or more of claims <b>1</b> to <b>16</b>.</p><p id="p-0200" num="0197">18. A video encoding apparatus comprising a processor configured to implement a method recited in one or more of claims <b>1</b> to <b>16</b>.</p><p id="p-0201" num="0198">19. A computer program product having computer code stored thereon, the code, when executed by a processor, causes the processor to implement a method recited in any of claims <b>1</b> to <b>16</b>.</p><p id="p-0202" num="0199">20. A method of video processing comprising generating a bitstream according to a method recited in any one or more of claims <b>1</b>-<b>16</b> and storing the bitstream on a computer readable medium.</p><p id="p-0203" num="0200">21. A method, an apparatus, or a system described in the present document.</p><p id="p-0204" num="0201">The following documents are incorporated by reference in their entirety:<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0202">[1] ITU-T and ISO/IEC, &#x201c;High efficiency video coding&#x201d;, Rec. ITU-T H.265 ISO/IEC 23008-2 (in force edition).</li>    <li id="ul0001-0002" num="0203">[2] J. Chen, E. Alshina, G. J. Sullivan, J.-R. Ohm, J. Boyce, &#x201c;Algorithm description of Joint Exploration Test Model 7 (JEM7),&#x201d; JVET-G1001, August 2017.</li>    <li id="ul0001-0003" num="0204">[3] Rec. ITU-T H.266|ISO/IEC 23090-3, &#x201c;Versatile Video Coding&#x201d;, 2020.</li>    <li id="ul0001-0004" num="0205">[4] B. Bross, J. Chen, S. Liu, Y.-K. Wang (editors), &#x201c;Versatile Video Coding (Draft 10),&#x201d; JVET-52001.</li>    <li id="ul0001-0005" num="0206">[5] Rec. ITU-T Rec. H.274 ISO/IEC 23002-7, &#x201c;Versatile Supplemental Enhancement Information Messages for Coded Video Bitstreams&#x201d;, 2020.</li>    <li id="ul0001-0006" num="0207">[6] J. Boyce, V. Drugeon, G. Sullivan, Y.-K. Wang (editors), &#x201c;Versatile supplemental enhancement information messages for coded video bitstreams (Draft 5),&#x201d; JVET-52007.</li>    <li id="ul0001-0007" num="0208">[7] ISO/IEC 14496-12: &#x201c;Information technology&#x2014;Coding of audio-visual objects&#x2014;Part 12: ISO base media file format&#x201d;.</li>    <li id="ul0001-0008" num="0209">[8] ISO/IEC 23009-1: &#x201c;Information technology&#x2014;Dynamic adaptive streaming over HTTP (DASH)&#x2014;Part 1: Media presentation description and segment formats&#x201d;. The 4th edition text of the DASH standard specification can be found in MPEG input document m52458.</li>    <li id="ul0001-0009" num="0210">[9] ISO/IEC 14496-15: &#x201c;Information technology&#x2014;Coding of audio-visual objects&#x2014;Part 15: Carriage of network abstraction layer (NAL) unit structured video in the ISO base media file format&#x201d;.</li>    <li id="ul0001-0010" num="0211">[10] ISO/IEC 23008-12: &#x201c;Information technology&#x2014;High efficiency coding and media delivery in heterogeneous environments&#x2014;Part 12: Image File Format&#x201d;.</li>    <li id="ul0001-0011" num="0212">[11] ISO/IEC JTC 1/SC 29/WG 03 output document N0035, &#x201c;Potential improvements on Carriage of VVC and EVC in ISOBMFF&#x201d;, November 2020.</li>    <li id="ul0001-0012" num="0213">[12] ISO/IEC JTC 1/SC 29/WG 03 output document N0038, &#x201c;Information technology&#x2014;High efficiency coding and media delivery in heterogeneous environments&#x2014;Part 12: Image File Format&#x2014;Amendment 3: Support for VVC, EVC, slideshows and other improvements (CD stage)&#x201d;, November 2020.</li></ul></p><p id="p-0205" num="0214">The disclosed and other solutions, examples, embodiments, modules and the functional operations described in this document can be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this document and their structural equivalents, or in combinations of one or more of them. The disclosed and other embodiments can be implemented as one or more computer program products, i.e., one or more modules of computer program instructions encoded on a computer readable medium for execution by, or to control the operation of, data processing apparatus. The computer readable medium can be a machine-readable storage device, a machine-readable storage substrate, a memory device, a composition of matter effecting a machine-readable propagated signal, or a combination of one or more them. The term &#x201c;data processing apparatus&#x201d; encompasses all apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers. The apparatus can include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them. A propagated signal is an artificially generated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus.</p><p id="p-0206" num="0215">A computer program (also known as a program, software, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment. A computer program does not necessarily correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code). A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.</p><p id="p-0207" num="0216">The processes and logic flows described in this document can be performed by one or more programmable processors executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).</p><p id="p-0208" num="0217">Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor will receive instructions and data from a read only memory or a random-access memory or both. The essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks. However, a computer need not have such devices. Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., erasable programmable read-only memory (EPROM), electrically erasable programmable read-only memory (EEPROM), and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and compact disk read-only memory (CD ROM) and digital versatile disc-read only memory (DVD-ROM) disks. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.</p><p id="p-0209" num="0218">While this patent document contains many specifics, these should not be construed as limitations on the scope of any subject matter or of what may be claimed, but rather as descriptions of features that may be specific to particular embodiments of particular techniques. Certain features that are described in this patent document in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Moreover, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.</p><p id="p-0210" num="0219">Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. Moreover, the separation of various system components in the embodiments described in this patent document should not be understood as requiring such separation in all embodiments.</p><p id="p-0211" num="0220">Only a few implementations and examples are described and other implementations, enhancements and variations can be made based on what is described and illustrated in this patent document.</p><p id="p-0212" num="0221">While this patent document contains many specifics, these should not be construed as limitations on the scope of any subject matter or of what may be claimed, but rather as descriptions of features that may be specific to particular embodiments of particular techniques. Certain features that are described in this patent document in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable sub-combination. Moreover, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a sub-combination or variation of a sub-combination.</p><p id="p-0213" num="0222">Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. Moreover, the separation of various system components in the embodiments described in this patent document should not be understood as requiring such separation in all embodiments.</p><p id="p-0214" num="0223">Only a few implementations and examples are described and other implementations, enhancements and variations can be made based on what is described and illustrated in this patent document.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method of processing media data, comprising:<claim-text>determining, for a conversion between the media data and a media data file, that a preselection element includes an indicator, wherein the indicator indicates that a purpose of the preselection element is for providing a picture-in-picture experience where a supplementary video appears to be overlaid on a target picture-in-picture region in a main video; and</claim-text><claim-text>performing the conversion between the media data and the media data file based on the indicator.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the indicator comprises an @tag attribute.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the indicator comprises an @value attribute.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the indicator has a value of &#x201c;PicInPic&#x201d; to indicate that the purpose of the preselection element is for providing the picture-in-picture experience.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the indicator comprises an @tag attribute having a value of &#x201c;PicInPic&#x201d; to indicate that the purpose of the preselection element is for providing the picture-in-picture experience.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the indicator comprises an @value attribute having a value of &#x201c;PicInPic&#x201d; to indicate that the purpose of the preselection element is for providing the picture-in-picture experience.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the indicator comprises an @tag attribute within a CommonAttributesElements element in the preselection element.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the indicator comprises an @value attribute of a Role element in the preselection element.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the indicator comprises an @tag attribute that also specifies a representation, adaption set, or the preselection element that may be used for selection purposes toward a decoder.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the indicator comprises an @value attribute of a Role element that specifies information on a role annotation scheme.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the preselection element includes an @dataUnitsReplacable attribute that specifies whether coded video data units representing the target picture-in-picture region in the main video can be replaced by corresponding video data units of the supplementary video.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the preselection element only includes an @dataUnitsReplacable attribute when the indicator is an @tag attribute having a value of &#x201c;PicInPic.&#x201d;</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the preselection element includes an @regionsIds attribute that specifies an identifier (ID) for each coded video data unit representing the target picture-in-picture region in the main video.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the preselection element only includes an @regionsIds attribute when the indicator is an @tag attribute having a value of &#x201c;PicInPic.&#x201d;</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the preselection element is disposed in a Media Presentation Description (MPD) file.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the preselection element is a Dynamic Adaptive Streaming over Hypertext Transfer Protocol (DASH) preselection element.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the conversion includes encoding the media data into a bitstream.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the conversion includes decoding the media data from a bitstream.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. An apparatus for processing media data comprising a processor and a non-transitory memory with instructions thereon, wherein the instructions upon execution by the processor, cause the processor to:<claim-text>determine, for a conversion between the media data and a media data file, that a preselection element includes an indicator, wherein the indicator indicates that a purpose of the preselection element is for providing a picture-in-picture experience where a supplementary video appears to be overlaid on a target picture-in-picture region in a main video; and</claim-text><claim-text>perform the conversion between the media data and the media data file based on the indicator.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. A non-transitory computer-readable recording medium storing a Media Presentation Description (MPD) of a video which is generated by a method performed by a video processing apparatus, wherein the method comprises:<claim-text>determining that a preselection element includes an indicator, wherein the indicator indicates that a purpose of the preselection element is for providing a picture-in-picture experience where a supplementary video appears to be overlaid on a target picture-in-picture region in a main video; and</claim-text><claim-text>generating the MPD based on the indicator.</claim-text></claim-text></claim></claims></us-patent-application>