<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005152A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005152</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17723767</doc-number><date>20220419</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>KR</country><doc-number>10-2021-0024711</doc-number><date>20210224</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>10</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>10</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20084</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30096</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30016</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>10088</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">APPARATUS AND METHOD FOR IMAGE SEGMENTATION</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>RESEARCH COOPERATION FOUNDATION OF YEUNGNAM UNIVERSITY</orgname><address><city>Gyeongsangbuk-do</city><country>KR</country></address></addressbook><residence><country>KR</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>CHOI</last-name><first-name>Gyu Sang</first-name><address><city>Daegu</city><country>KR</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>SHIN</last-name><first-name>Hyun Kwang</first-name><address><city>Gyeongsangbuk-do</city><country>KR</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">An apparatus for image segmentation according to an embodiment includes an acquirer configured to acquire one or more images in which an object is photographed and a segmentation performer configured to perform segmentation on the one or more images using a segmentation model which is deep learned through a plurality of images, in which the segmentation model is a U-Net-based model including a first type module based on depth-wise separable convolution (DSC) and a second type module based on global context network (GCNet).</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="41.49mm" wi="130.73mm" file="US20230005152A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="54.78mm" wi="132.76mm" file="US20230005152A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="241.30mm" wi="155.70mm" orientation="landscape" file="US20230005152A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="244.60mm" wi="161.88mm" orientation="landscape" file="US20230005152A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="200.24mm" wi="151.47mm" file="US20230005152A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="218.95mm" wi="158.41mm" file="US20230005152A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="78.06mm" wi="138.77mm" file="US20230005152A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="126.66mm" wi="134.79mm" file="US20230005152A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?federal-research-statement description="Federal Research Statement" end="lead"?><heading id="h-0001" level="1">GOVERNMENT LICENSE RIGHTS</heading><p id="p-0002" num="0001">This invention was made with Korean government support under Project No. NRF-2021R1A6A1A03039493 (Task Unique No. 1345334139, Name of Institution Managing Project: National Research Foundation, Research Task Name: Information and Communication Research Institute, Name of Institution Conducting Project: Yeungnam University) awarded by Ministry of Education, Republic of Korea and under Project No. 2019R1A2C1006159 (Task Unique No. 1711142173, Name of Institution Managing Project: National Research Foundation, Research Task Name: on Intelligent Information Analysis Platform for Automatic Fact Checking. Name of Institution Conducting Project: Yeungnam University) awarded by Ministry of Science and ICT, Republic of Korea. The Korean government has certain rights in the invention.</p><?federal-research-statement description="Federal Research Statement" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">CROSS-REFERENCE TO RELATED APPLICATION(S)</heading><p id="p-0003" num="0002">This application claims the benefit under 35 USC &#xa7; 119 of Korean Patent Application No. 10-2021-0024711, filed on Feb. 24, 2021, in the Korean Intellectual Property Office, the entire disclosure of which is incorporated herein by reference for all purposes.</p><heading id="h-0003" level="1">BACKGROUND</heading><heading id="h-0004" level="1">1. Technical Field</heading><p id="p-0004" num="0003">The disclosed embodiments relate to a technology for improving training efficiency and performance of a deep learning model that performs image segmentation.</p><heading id="h-0005" level="1">2. Background Art</heading><p id="p-0005" num="0004">Brain stroke is a symptom in which a part of the brain is damaged by rupture or blockage of a cerebral blood vessel, and is a cause of death that threatens the lives of people around the world. However, until recently, in order to determine a position where the brain stroke has occurred on a medical image, there is no choice but to rely on an opinion of a specialist, and thus it was difficult to respond immediately, and the accuracy was also relatively low.</p><p id="p-0006" num="0005">Therefore, with the recent development of deep learning technology, network models such as SegNet and U-Net have been proposed for segmentation of the medical image, but (1) the number of training parameters was huge, and thus resources for training the model were wasted severely, (2) as training ng deepened, the problems of long-term dependencies became prominent, and (3) due to architectural structural limitations of the model, the results of lesion segmentation were often inaccurate.</p><heading id="h-0006" level="1">SUMMARY</heading><p id="p-0007" num="0006">The disclosed embodiments are intended to improve an architecture of a deep learning model that performs image segmentation to enhance training efficiency and image segmentation performance of the model.</p><p id="p-0008" num="0007">An apparatus for image segmentation according to an embodiment includes an acquirer configured to acquire one or more images in which an object is photographed, and a segmentation performer configured to perform segmentation on the one or more images using a segmentation model which is deep learned through a plurality of images, in which the segmentation model is a U-Net-based model including a first type module based on depth-wise separable convolution (DSC) and a second type module based on a global context network (GCNet).</p><p id="p-0009" num="0008">The one or more images may include a tomographic image of a brain obtained by magnetic resonance imaging (MM).</p><p id="p-0010" num="0009">The segmentation performer may be configured to segment the tomographic image of the brain into a plurality of sections, and determine one or more sections satisfying a preset condition among the plurality of sections as a stroke lesion.</p><p id="p-0011" num="0010">The segmentation model may be a U-Net-based model in which at least some of a plurality of convolution layer blocks in the segmentation model are replaced with the first type module.</p><p id="p-0012" num="0011">The segmentation model may be a U-Net-based model in which the second type module is disposed between an encoder in which down sampling is performed and a decoder in which up sampling is performed in the segmentation model.</p><p id="p-0013" num="0012">The first type module may be configured to include a plurality of depth-wise convolution layer blocks for extracting feature information of a feature map and a plurality of point-wise convolution layer blocks for controlling the number of channels of the feature map.</p><p id="p-0014" num="0013">The first type module may be configured to calculate a map for extracting feature information by repeatedly applying the depth-wise convolution layer block and the point-wise convolution layer block to an input feature map and calculate a map for controlling the number of channels by applying only the point-wise convolution layer block to the input feature map, and sum the map for extracting the feature information and the map for adjusting the number of channels and output a result of the summation.</p><p id="p-0015" num="0014">The second type module may be configured to include a first convolution layer block for extracting feature information of a feature map, a second convolution layer block, and a global context block (GCBlock) based on the global context network.</p><p id="p-0016" num="0015">The second type module may be configured to calculate a global feature map by applying the first convolution layer block, the second convolution layer block, and the global context block to an input feature map, and sum the input feature map and the global feature map and output a result of the summation.</p><p id="p-0017" num="0016">A method for image segmentation according to another embodiment includes acquiring one or more images in which an object is photographed, and performing segmentation on the one or more images using a segmentation model which is deep learned through a plurality of images, in which the segmentation model is a U-Net-based model including a first type module based on depth-wise separable convolution (DSC) and a second type module based on a global context network (GCNet).</p><p id="p-0018" num="0017">The one or more images may include a tomographic image of a brain obtained by magnetic resonance imaging (MM).</p><p id="p-0019" num="0018">In the performing of segmentation, the tomographic image of the brain may be segmented into a plurality of sections, and one or more sections satisfying a preset condition among the plurality of sections may be determined as a stroke lesion.</p><p id="p-0020" num="0019">The segmentation model may be a U-Net-based model in which at least some of a plurality of convolution layer blocks in the segmentation model are replaced with the first type module.</p><p id="p-0021" num="0020">The segmentation model may be a U-Net-based model in which the second type module is disposed between an encoder in which down sampling is performed and a decoder in which up sampling is performed in the segmentation model.</p><p id="p-0022" num="0021">The first type module may be configured to include a plurality of depth-wise convolution layer blocks for extracting feature information of a feature map and a plurality of point-wise convolution layer blocks for controlling the number of channels of the feature map.</p><p id="p-0023" num="0022">The first type module may be configured to calculate a map for extracting feature information by repeatedly applying the depth-wise convolution layer block and the point-wise convolution layer block to an input feature map and calculate a map for controlling the number of channels by applying only the point-wise convolution layer block to the input feature map, and sum the map for extracting the feature information and the map for adjusting the number of channels and output a result of the summation.</p><p id="p-0024" num="0023">The second type module may be configured to include a first convolution layer block for extracting feature information of a feature map, a second convolution layer block, and a global context block (GCBlock) based on the global context network.</p><p id="p-0025" num="0024">The second type module may be configured to calculate a global feature map by applying the first convolution layer block, the second convolution layer block, and the global context block to an input feature map, and sum the input feature map and the global feature map and output a result of the summation.</p><p id="p-0026" num="0025">According to the disclosed embodiments, by performing image segmentation using a layer for extracting feature information of a feature map and a layer for controlling the number of channels of the feature map in combination, the number of parameters to be trained on the model can be drastically reduced to improve training efficiency.</p><p id="p-0027" num="0026">Further, according to the disclosed embodiments, by constructing a model for image segmentation based on the global context network so that non-local features can be reflected in training, the problems of long-term dependencies can be resolved when training the model.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0007" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram illustrating an apparatus for image segmentation according to an embodiment.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is an exemplary diagram for describing a conventional U-Net architecture.</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is an exemplary diagram for describing in detail an architecture of a segmentation model according to an embodiment.</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is an exemplary diagram for describing in detail a first type module according to an embodiment.</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is an exemplary diagram for describing in detail a second type module according to an embodiment.</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flowchart illustrating a method for image segmentation according to an embodiment.</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a block diagram illustratively describing a computing environment including a computing device according to an embodiment.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0008" level="1">DETAILED DESCRIPTION</heading><p id="p-0035" num="0034">Hereinafter, a specific embodiment will be described with reference to the drawings. The following detailed description is provided to aid in a comprehensive understanding of the methods, apparatus and/or systems described herein. However, this is illustrative only, and the disclosed embodiments are not limited thereto.</p><p id="p-0036" num="0035">In describing the embodiments, when it is determined that a detailed description of related known technologies may unnecessarily obscure the subject matter of the present disclosure, a detailed description thereof will be omitted. In addition, terms to be described later are terms defined in consideration of functions in the disclosed embodiments, which may vary according to the intention or custom of users or operators. Therefore, the definition should be made based on the contents throughout this specification. The terms used in the detailed description are only for describing embodiments, and should not be limiting. Unless explicitly used otherwise, expressions in the singular form include the meaning of the plural form. In this description, expressions such as &#x201c;comprising&#x201d; or &#x201c;including&#x201d; are intended to refer to certain features, numbers, steps, actions, elements, some or combination thereof, and it is not to be construed to exclude the presence or possibility of one or more other features, numbers, steps, actions, elements, some or combinations thereof, other than those described.</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram illustrating an apparatus <b>100</b> for image segmentation according to an embodiment.</p><p id="p-0038" num="0037">As illustrated, the apparatus <b>100</b> for image segmentation according to an embodiment includes an acquirer <b>110</b> and a segmentation performer <b>120</b>.</p><p id="p-0039" num="0038">The acquirer <b>110</b> acquires one or more images in which an object is photographed.</p><p id="p-0040" num="0039">According to an embodiment, the acquirer <b>110</b> may acquire one or more images including a tomographic image of a brain obtained by magnetic resonance imaging (MRI).</p><p id="p-0041" num="0040">According to an embodiment, the acquirer <b>110</b> may acquire an image from a separate database (not illustrated) outside the apparatus <b>100</b> for image segmentation, but is not limited thereto, and may acquire an image from a storage space disposed inside the apparatus <b>100</b> for image segmentation (not illustrated). In addition, the acquirer <b>110</b> may acquire an image directly from a user of the apparatus <b>100</b> for image segmentation.</p><p id="p-0042" num="0041">Specifically, a source of the image to be segmented may be different depending on which of a training process, verification process, or actual use process of the segmentation model the image is used in. For example, the apparatus <b>100</b> for image segmentation may use an open source dataset on the web in the case of the image used in the training process and the verification process, and may use an image directly input from the user of the apparatus <b>100</b> for image segmentation in the case of the image used in the actual use process.</p><p id="p-0043" num="0042">The segmentation performer <b>120</b> performs segmentation on one or more images acquired by the acquirer <b>110</b> using a segmentation model which is deep learned through a plurality of images.</p><p id="p-0044" num="0043">In addition, the segmentation model is characterized by being a U-Net-based model including a first type module based on depth-wise separable convolution (DSC) and a second type module based on a global context network (GCNet).</p><p id="p-0045" num="0044">As such, significant advantages can be expected in image segmentation by applying the first type module and the second type module described above to the segmentation model, and thus features and advantages of the segmentation model of the present disclosure compared to the conventional model are as follows.</p><p id="p-0046" num="0045">Conventionally, SegNet, U-Net, <b>2</b>D Dense U-Net, etc. have been proposed as convolutional network-based image segmentation models. In particular, U-Net has become a representative model used for segmentation of medical-related images. However, in the conventional models including U-Net, it is difficult to change the number of channels of a feature map that goes through the convolution layer more than a certain amount and a receptive field does not reflect the non-local feature as down sampling is performed continuously during an encoding process. Accordingly, the conventional models have limitations in that the size and boundary of a segmented region in the image are not clear.</p><p id="p-0047" num="0046">On the other hand, the segmentation model of the apparatus <b>100</b> for image segmentation can easily change the number of channels of the feature map that goes through the convolution layer through the first type module based on the DSC, and may derive segmentation results reflecting the non-local features despite continuous down sampling through the second type module based on the GCNet.</p><p id="p-0048" num="0047">In the following embodiments, the architecture of the segmentation model including the first type module and the second type module will be described in more detail.</p><p id="p-0049" num="0048">According to an embodiment, when the tomographic image of the brain obtained by the magnetic resonance imaging (MRI) is acquired by the acquirer <b>110</b>, the segmentation performer <b>120</b> may segment the tomographic image of the brain into a plurality of sections, and determine one or more sections satisfying a preset condition among the plurality of sections as a stroke lesion.</p><p id="p-0050" num="0049">According to an embodiment, the segmentation model may be the U-Net-based model in which at least some of a plurality of convolution layer blocks in the segmentation model are replaced with the first type module.</p><p id="p-0051" num="0050">Specifically, the first type module may replace all of the plurality of convolution layer blocks constituting the encoder in which down sampling is performed and the decoder in which up sampling is performed in the segmentation model.</p><p id="p-0052" num="0051">Also, according to an embodiment, the first type module may include a plurality of depth-wise convolution layer blocks for extracting feature information of a feature map and a plurality of point-wise convolution layer blocks for controlling the number of channels of the feature map.</p><p id="p-0053" num="0052">Specifically, the first type module may calculate a map for extracting feature information by repeatedly applying the depth-wise convolution layer block and the point-wise convolution layer block to a feature map which is input to the first type module, calculate a map for controlling the number of channels by applying only the point-wise convolution layer block to the feature map which is input to the first type module, and sum the calculated map for extracting the feature information and the calculated map for adjusting the number of channels and output a result of the summation. In this regard, description will be made later in detail with reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref> below.</p><p id="p-0054" num="0053">Meanwhile, according to an embodiment, the segmentation model may be the U-Net-based model in which the second type module is disposed between the encoder and the decoder in the segmentation model.</p><p id="p-0055" num="0054">Specifically, the second type module may be disposed between the first type module disposed in the deepest layer of the encoder and the first type module disposed in the deepest layer of the decoder.</p><p id="p-0056" num="0055">Also, according to one embodiment, the second type module may include a first convolution layer block for extracting feature information of a feature map, a second convolution layer block, and a global context block (GCBlock) based on the global context network.</p><p id="p-0057" num="0056">Specifically, the second type module may calculate a global feature map by applying the first convolution layer block, the second convolution layer block, and the global context block to a feature map which is input to the second type module, and sum the feature map which is input to the second type module and the global feature map and output a result of the summation. In this regard, description will be made later in detail with reference to <figref idref="DRAWINGS">FIG. <b>5</b></figref> below.</p><p id="p-0058" num="0057">In the illustrated embodiment, configurations may respectively have different functions and capabilities other than those described below, and additional configurations may be included in addition to those described below.</p><p id="p-0059" num="0058">In addition, in an embodiment, the acquirer <b>110</b> and the segmentation performer <b>120</b> may be implemented using one or more physically separated devices, or implemented by one or more processors or a combination of one or more processors and software, and may not be clearly distinguished in a specific operation unlike the illustrated example.</p><p id="p-0060" num="0059"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is an exemplary diagram <b>200</b> for describing a conventional U-Net architecture. For convenience of description, the conventional U-Net architecture will be referred to as a &#x2018;conventional architecture&#x2019; hereinafter.</p><p id="p-0061" num="0060">Referring to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, when an image having a size of 224*192 is input to the conventional architecture, a 3*3 convolution operation is performed twice by the first convolution layer block. In this case, padding may be applied to prevent a decrease in the size of the output feature map, and as a result, the number of channels of the feature map output from the first convolution layer block may increase to 64 channels. However, it should be noted that this is an example, and the size and number of convolution filters applied to an input image, a padding value, etc. may be different depending on the embodiment.</p><p id="p-0062" num="0061">After that, the feature map output from the first convolution layer block may be subjected to down sampling and processed to a size of 112*96*64, may be repeatedly subjected to a process of 3*3 convolution operation, padding, and down sampling several times by the convolution layer block at several stages and may be processed into a feature map having a size of 56*48*128, 28*24*256, 14*12*512, etc.</p><p id="p-0063" num="0062">After that, the conventional architecture may generate an output image having the same size as the input image by continuously repeating the process of up sampling, 3*3 convolution operation, and padding on the feature map having the size of 14*12*1024 that was last processed on the encoder.</p><p id="p-0064" num="0063">In this case, in the conventional architecture, a feature map having sizes of 112*96*128, 56*48*256, 28*24*512, etc. before having been subjected to down sampling may be concatenated with an upsampled feature map using a skip connection technique.</p><p id="p-0065" num="0064">However, in such a conventional architecture, since convolution operations should be performed twice for each convolution layer block, there is a disadvantage in that too many training parameters are required according to the size of each convolution filter, the number of filters, and the number of channels of the input feature map (or image). For this reason, a model having the conventional architecture requires excessive resources during training, and thus has an architectural problem in which training efficiency and performance after training are deteriorated.</p><p id="p-0066" num="0065"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is an exemplary diagram <b>300</b> for describing in detail the architecture of the segmentation model according to an embodiment.</p><p id="p-0067" num="0066">Similar to the conventional architecture of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the segmentation model illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref> generates a feature map having the size of 14*12*1024 by repeatedly applying the process of 3*3 convolution operations, padding, and down sampling twice to a received image having a size of 224*192.</p><p id="p-0068" num="0067">However, the process of 3*3 convolution operation, padding, and down sampling is performed by the first type module instead of the conventional convolution layer block, and thus the first type module not only extracts feature information of the feature map through the depth-wise convolution layer block, but also adjusts the number of channels of the feature map through the point-wise convolution layer block, thereby capable of dramatically reducing the the number of training parameters of the segmentation model.</p><p id="p-0069" num="0068">In addition, unlike the conventional architecture in which up sampling is applied to a feature map having the size of 14*12*1024 right away, the segmentation model may generate an output image that effectively reflects the non-local features of the input image by summing the feature map having the size of 14*12*1024 immediately after having gone through the encoder and the feature map having the size of 14*12*1024 that has been additionally gone through the second type module by element and applying up sampling.</p><p id="p-0070" num="0069"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is an exemplary diagram <b>400</b> for describing in detail the first type module according to an embodiment.</p><p id="p-0071" num="0070">In the following embodiments, the depth-wise convolution layer block means a layer block including a 3*3 depth-wise convolution layer block, a batch normalization layer for performing batch normalization, and an activation function layer for applying a ReLU activation function. In addition, the point-wise convolution layer block means a layer block including a 1*1 convolution layer for controlling the number of channels, a batch normalization layer, and an activation function layer.</p><p id="p-0072" num="0071">Referring to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the first type module calculates a map for extracting feature information by repeatedly applying the depth-wise convolution layer block and the point-wise convolution layer block to the input feature map twice, calculates a map for controlling the number of channels by applying only the point-wise convolution layer block once to the input feature map, and then sums the calculated map for extracting feature information and calculated map for controlling the number of channels by element and output a result of the summation as an output of the first type module.</p><p id="p-0073" num="0072">Specifically, the first type module adjusts the number of output channels to C using the point-wise convolution layer block in the process of calculating the map for adjusting the number of channels. Meanwhile, in the process of calculating the map for extracting feature information, the first type module increases the number of output channels to 2C using the first point-wise convolution layer block, and then adjusts the number of output channels to C again using the second point-wise convolution layer block. In this case, it should be noted that the number of output channels increased by using the first point-wise convolution layer block may be set in various ways, and is not necessarily limited to 2 C.</p><p id="p-0074" num="0073">With this configuration, the first type module may extract more various feature information from the feature map according to the number of channels increased in the middle, and at the same time, reduce the number of training parameters compared to the conventional architecture by adjusting the number of output channels using the point-wise convolution layer block.</p><p id="p-0075" num="0074">For example, it is assumed that k is the size of the filter used for convolution operation, C<sub>i </sub>is the number of channels of an input feature map, C<sub>O </sub>is the number of channels of an output feature map, and C<sub>E0 </sub>is the number of channels of a feature map output from the encoder to the decoder. In this case, according to the conventional architecture, the number of training parameters corresponding to one convolution layer block positioned in the encoder is (C<sub>i</sub>*k<sup>2</sup>*C<sub>o</sub>)+(C<sub>o</sub>*k<sup>2</sup>*C<sub>o</sub>), which results in a total of k<sup>2</sup>*C<sub>o</sub>*(C<sub>i</sub>+C<sub>o</sub>), and the number of training parameters corresponding to one convolution layer block positioned in the decoder is (C<sub>i</sub>*k<sup>2</sup>*C<sub>o</sub>)+((C<sub>E0</sub>+C<sub>o</sub>)*k<sup>2</sup>*C<sub>o</sub>)+(C<sub>o</sub>*k<sup>2</sup>*C<sub>o</sub>), which results in a total of k<sup>2</sup>*C<sub>o</sub>*(C<sub>i</sub>+C<sub>o</sub>).</p><p id="p-0076" num="0075">On the other hand, in the case of the segmentation model illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the number of training parameters corresponding to one first type module positioned in the encoder is (C<sub>i</sub>*k<sup>2</sup>)+(C<sub>i</sub>*2C<sub>0</sub>)+(2C<sub>0</sub>*k<sup>2</sup>)+(2C<sub>0</sub>*C<sub>0</sub>)+(C<sub>i</sub>*C<sub>0</sub>), which results in a total of k<sup>2</sup>*(C<sub>i</sub>+2C<sub>0</sub>)+C<sub>0</sub>*(3C<sub>i</sub>+2C<sub>0</sub>), and the number of training parameters corresponding to one first type module positioned in the decoder is ((C<sub>E0</sub>+C<sub>i</sub>)*k<sup>2</sup>)+((C<sub>E0</sub>+C<sub>i</sub>)*2C<sub>0</sub>)+(2C<sub>0</sub>*k<sup>2</sup>)+(2C<sub>0</sub>*C<sub>0</sub>)+((C<sub>E0</sub>+C<sub>i</sub>)*C<sub>0</sub>), which results in a total of k<sup>2</sup>*(C<sub>E0</sub>+C<sub>i</sub>+C<sub>0</sub>)+C<sub>0</sub>*(3C<sub>E0</sub>+3C<sub>i</sub>+2C<sub>0</sub>). In addition, the number of training parameters corresponding to the second type module to be described later is</p><p id="p-0077" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mrow>  <mrow>   <mrow>    <mo>(</mo>    <mrow>     <mi>Ci</mi>     <mo>*</mo>     <mfrac>      <msub>       <mi>C</mi>       <mi>i</mi>      </msub>      <mn>8</mn>     </mfrac>     <mo>*</mo>     <msup>      <mi>k</mi>      <mn>2</mn>     </msup>    </mrow>    <mo>)</mo>   </mrow>   <mo>+</mo>   <mrow>    <mo>(</mo>    <mrow>     <mfrac>      <msub>       <mi>C</mi>       <mi>i</mi>      </msub>      <mn>8</mn>     </mfrac>     <mo>*</mo>     <mn>1</mn>    </mrow>    <mo>)</mo>   </mrow>   <mo>+</mo>   <mrow>    <mo>(</mo>    <mrow>     <mfrac>      <msub>       <mi>C</mi>       <mi>i</mi>      </msub>      <mn>8</mn>     </mfrac>     <mo>*</mo>     <mfrac>      <msub>       <mi>C</mi>       <mi>i</mi>      </msub>      <mn>16</mn>     </mfrac>    </mrow>    <mo>)</mo>   </mrow>   <mo>+</mo>   <mrow>    <mo>(</mo>    <mrow>     <mfrac>      <msub>       <mi>C</mi>       <mi>i</mi>      </msub>      <mn>8</mn>     </mfrac>     <mo>*</mo>     <mfrac>      <msub>       <mi>C</mi>       <mi>i</mi>      </msub>      <mn>16</mn>     </mfrac>    </mrow>    <mo>)</mo>   </mrow>   <mo>+</mo>   <mrow>    <mo>(</mo>    <mrow>     <mi>Ci</mi>     <mo>*</mo>     <mfrac>      <msub>       <mi>C</mi>       <mi>i</mi>      </msub>      <mn>8</mn>     </mfrac>     <mo>*</mo>     <msup>      <mi>k</mi>      <mn>2</mn>     </msup>    </mrow>    <mo>)</mo>   </mrow>  </mrow>  <mo>,</mo> </mrow></math></maths></p><p id="p-0078" num="0000">which results in a total of</p><p id="p-0079" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mrow>  <mrow>   <mfrac>    <mn>1</mn>    <mn>8</mn>   </mfrac>   <mo>&#x2062;</mo>   <msup>    <mi>k</mi>    <mn>2</mn>   </msup>   <mo>*</mo>   <mi>Ci</mi>  </mrow>  <mo>+</mo>  <mrow>   <mfrac>    <mn>1</mn>    <mn>8</mn>   </mfrac>   <mo>&#x2062;</mo>   <mi>Ci</mi>   <mo>*</mo>   <mrow>    <mrow>     <mo>(</mo>     <mrow>      <mn>1</mn>      <mo>+</mo>      <mfrac>       <msub>        <mi>C</mi>        <mi>i</mi>       </msub>       <mn>8</mn>      </mfrac>     </mrow>     <mo>)</mo>    </mrow>    <mo>.</mo>   </mrow>  </mrow> </mrow></math></maths></p><p id="p-0080" num="0076">Accordingly, when the numerical values shown in <figref idref="DRAWINGS">FIGS. <b>2</b> and <b>3</b></figref> are substituted, the total number of training parameters of the conventional architecture is about 34.5 million, whereas the total number of training parameters of the segmentation model of <figref idref="DRAWINGS">FIG. <b>4</b></figref> is about 11.5 million. Therefore, it can be seen that the segmentation model reduces the number of training parameters related to convolution operations performed twice by about 3 times compared to the conventional architecture by replacing the convolution layer block of the conventional architecture with the first type module.</p><p id="p-0081" num="0077"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is an exemplary diagram <b>500</b> for describing in detail the second type module according to an embodiment.</p><p id="p-0082" num="0078">In the following embodiment, the first convolution layer block and the second convolution layer block mean layer blocks including the 3*3 depth-wise convolution layer block, the batch normalization layer for performing batch normalization, and the activation function layer for applying the ReLU activation function.</p><p id="p-0083" num="0079">Referring to <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the second type module calculates a global feature map by applying the first convolution layer block, the second convolution layer block, and the global context block to the input feature map, and then sums the calculated global feature map and the previously input feature map by element and outputs a result of the summation as an output of the second type module.</p><p id="p-0084" num="0080">Specifically, the second type module may input the feature map having the number of channels of C to the first convolution layer block, input the output of the first convolution layer block to the global context block, input the output of the global context block to the second convolution layer block, sums the output of the second convolution layer block and the feature map input to the first convolution layer block by element, and output a result of the summation as the output of the second type module.</p><p id="p-0085" num="0081">Among these, the first convolution layer block and the second convolution layer block may adjust the number of channels of an output feature map. Specifically, the first convolution layer block may reduce the number of channels of the input feature map to reduce an amount of computation, and the second convolution layer block may make the number of channels of the feature map that has gone through the second type module and the number of channels of the feature map input to the second type module coincident with each other.</p><p id="p-0086" num="0082">In this regard, C/r and C<sub>0 </sub>shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref> are the same value, and r may be a hyper parameter set by the user as a ratio of the number of channels to be reduced, or a training parameter determined by learning.</p><p id="p-0087" num="0083">Meanwhile, the global context block may reflect the non-local context feature in the feature map input to the first convolution layer block through the following three configurations as illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0088" num="0084">(1) Context feature extraction (context modeling): After one round of 1*1 convolution operation, the Softmax function is applied to calculate an attention weight, and the result thereof and the output of the first convolution layer block are subjected to matrix multiplication</p><p id="p-0089" num="0085">(2) Bottleneck transformation: After reducing the number of output channels to one half thereof first by using a 1*1 convolution operation, a process of layer normalization and activation function application is performed, and the number of output channels is restored through the 1*1 convolution operation again.</p><p id="p-0090" num="0086">(3) Feature integration: The output of the first convolution layer block and the feature map generated in (2) are again summed by element</p><p id="p-0091" num="0087"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flowchart illustrating a method for image segmentation according to an embodiment.</p><p id="p-0092" num="0088">The method illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref> may be performed, for example, by the apparatus <b>100</b> for image segmentation described above.</p><p id="p-0093" num="0089">First, the apparatus <b>100</b> for image segmentation <b>100</b> acquires one or more images in which an object is photographed (<b>610</b>).</p><p id="p-0094" num="0090">Thereafter, the apparatus <b>100</b> for image segmentation performs segmentation on the one or more images acquired by using the segmentation model which is deep learned through a plurality of images (<b>620</b>).</p><p id="p-0095" num="0091">In this case, the segmentation model is characterized by being the U-Net-based model including the first type module based on the depth-wise convolution layer block and and the second type module based the point-wise convolution layer block.</p><p id="p-0096" num="0092">In the illustrated flowchart, although the method has been described by dividing the method into a plurality of steps, at least some steps may be performed in a different order, performed together in combination with other steps, omitted, performed by dividing the steps into sub-steps, or performed by being added with one or more steps (not illustrated).</p><p id="p-0097" num="0093"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a block diagram illustratively describing a computing environment <b>10</b> including a computing device according to an embodiment. In the illustrated embodiment, respective components may have different functions and capabilities other than those described below, and may include additional components in addition to those described below.</p><p id="p-0098" num="0094">The illustrated computing environment <b>10</b> includes a computing device <b>12</b>. In an embodiment, the computing device <b>12</b> may be the apparatus <b>100</b> for image segmentation.</p><p id="p-0099" num="0095">The computing device <b>12</b> includes at least one processor <b>14</b>, a computer-readable storage medium <b>16</b>, and a communication bus <b>18</b>. The processor <b>14</b> may cause the computing device <b>12</b> to operate according to the exemplary embodiment described above. For example, the processor <b>14</b> may execute one or more programs stored on the computer-readable storage medium <b>16</b>. The one or more programs may include one or more computer-executable instructions, which, when executed by the processor <b>14</b>, may be configured so that the computing device <b>12</b> performs operations according to the exemplary embodiment.</p><p id="p-0100" num="0096">The computer-readable storage medium <b>16</b> is configured so that the computer-executable instruction or program code, program data, and/or other suitable forms of information are stored. A program <b>20</b> stored in the computer-readable storage medium <b>16</b> includes a set of instructions executable by the processor <b>14</b>. In one embodiment, the computer-readable storage medium <b>16</b> may be a memory (volatile memory such as a random access memory, non-volatile memory, or any suitable combination thereof), one or more magnetic disk storage devices, optical disk storage devices, flash memory devices, other types of storage media that are accessible by the computing device <b>12</b> and capable of storing desired information, or any suitable combination thereof.</p><p id="p-0101" num="0097">The communication bus <b>18</b> interconnects various other components of the computing device <b>12</b>, including the processor <b>14</b> and the computer-readable storage medium <b>16</b>.</p><p id="p-0102" num="0098">The computing device <b>12</b> may also include one or more input/output interfaces <b>22</b> that provide an interface for one or more input/output devices <b>24</b>, and one or more network communication interfaces <b>26</b>. The input/output interface <b>22</b> and the network communication interface <b>26</b> are connected to the communication bus <b>18</b>. The input/output device <b>24</b> may be connected to other components of the computing device <b>12</b> through the input/output interface <b>22</b>. The exemplary input/output device <b>24</b> may include a pointing device (such as a mouse or trackpad), a keyboard, a touch input device (such as a touch pad or touch screen), a speech or sound input device, input devices such as various types of sensor devices and/or photographing devices, and/or output devices such as a display device, a printer, a speaker, and/or a network card. The exemplary input/output device <b>24</b> may be included inside the computing device <b>12</b> as a component constituting the computing device <b>12</b>, or may be connected to the computing device <b>12</b> as a separate device distinct from the computing device <b>12</b>.</p><p id="p-0103" num="0099">The embodiments described above may be implemented by a hardware component, a software component, and/or a combination of a hardware component and a software component. For example, the apparatuses, methods, and components described in the embodiments may be implemented using one or more general purpose or special purpose computers, such as, for example, a processor, controller, arithmetic logic unit (ALU), digital signal processor, microcomputer, field programmable gate array (FPGA), programmable logic unit (PLU), microprocessor, or any other device capable of executing and responding to an instruction. A processing device may execute an operating system (OS) and one or more software applications running on the operating system. The processing device may also access, store, manipulate, process, and generate data in response to execution of software. For convenience of understanding, although a description that one processing device is used may have been made, a person of ordinary skill in the art will recognize that the processing device may include a plurality of processing elements and/or a plurality of types of processing elements. For example, the processing device may include a plurality of processors, or one processor and one controller. Other processing configurations, such as parallel processors, are also possible.</p><p id="p-0104" num="0100">Software may include a computer program, code, instruction, or combinations of one or more of these, which may configure the processing device to operate as desired, or instruct the processing device independently or collectively. Software and/or data may be permanently or temporarily embodied in any tangible machine, component, physical equipment, virtual equipment, computer storage medium or device, or signal wave being transmitted, in order to be interpreted by the processing device or to provide instructions or data to the processing device. Software may be distributed over networked computer systems and stored or executed in a distributed manner. Software and data may be stored in one or more computer-readable recording media.</p><p id="p-0105" num="0101">Although representative embodiments of the present disclosure have been described in detail, a person skilled in the art to which the present disclosure pertains will understand that various modifications may be made thereto within the limits that do not depart from the scope of the present disclosure. Therefore, the scope of rights of the present disclosure should not be limited to the described embodiments, but should be defined not only by claims set forth below but also by equivalents to the claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230005152A1-20230105-M00001.NB"><img id="EMI-M00001" he="5.67mm" wi="76.20mm" file="US20230005152A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002" nb-file="US20230005152A1-20230105-M00002.NB"><img id="EMI-M00002" he="5.67mm" wi="76.20mm" file="US20230005152A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An apparatus for image segmentation, which is implemented with a computing device that includes one or more processors and a memory for storing one or more programs executed by the one or more processors, the apparatus comprising:<claim-text>an acquirer configured to acquire one or more images in which an object is photographed; and</claim-text><claim-text>a segmentation performer configured to perform segmentation on the one or more images using a segmentation model which is deep learned through a plurality of images, wherein</claim-text><claim-text>the segmentation model is a U-Net-based model including a first type module based on depth-wise separable convolution (DSC) and a second type module based on global context network (GCNet).</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one or more images include a tomographic image of a brain obtained by magnetic resonance imaging (MM).</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The apparatus of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the segmentation performer is configured to segment the tomographic image of the brain into a plurality of sections, and determine one or more sections satisfying a preset condition among the plurality of sections as a stroke lesion.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the segmentation model is a U-Net-based model in which at least some of a plurality of convolution layer blocks in the segmentation model are replaced with the first type module.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the segmentation model is a U-Net-based model in which the second type module is disposed between an encoder in which down sampling is performed and a decoder in which up sampling is performed in the segmentation model.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first type module is configured to include a plurality of depth-wise convolution layer blocks for extracting feature information of a feature map and a plurality of point-wise convolution layer blocks for controlling the number of channels of the feature map.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The apparatus of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the first type module is configured to:<claim-text>calculate a map for extracting feature information by repeatedly applying the depth-wise convolution layer block and the point-wise convolution layer block to an input feature map;</claim-text><claim-text>calculate a map for controlling the number of channels by applying only the point-wise convolution layer block to the input feature map; and</claim-text><claim-text>sum the map for extracting the feature information and the map for adjusting the number of channels and output a result of the summation.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the second type module is configured to include a first convolution layer block for extracting feature information of a feature map, a second convolution layer block, and a global context block (GCBlock) based on the global context network.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The apparatus of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the second type module is configured to:<claim-text>calculate a global feature map by applying the first convolution layer block, the second convolution layer block, and the global context block to an input feature map; and</claim-text><claim-text>sum the input feature map and the global feature map and output a result of the summation.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. A method for image segmentation performed by a computing device that includes one or more processors and a memory for storing one or more programs executed by the one or more processors, the method comprising:<claim-text>acquiring one or more images in which an object is photographed; and</claim-text><claim-text>performing segmentation on the one or more images using a segmentation model which is deep learned through a plurality of images,</claim-text><claim-text>wherein the segmentation model is a U-Net-based model including a first type module based on depth-wise separable convolution (DSC) and a second type module based on global context network (GCNet).</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the one or more images include a tomographic image of a brain obtained by magnetic resonance imaging (MM).</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein in the performing of segmentation, the tomographic image of the brain is segmented into a plurality of sections, and one or more sections satisfying a preset condition among the plurality of sections is determined as a stroke lesion.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the segmentation model is a U-Net-based model in which at least some of a plurality of convolution layer blocks in the segmentation model are replaced with the first type module.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the segmentation model is a U-Net-based model in which the second type module is disposed between an encoder in which down sampling is performed and a decoder in which up sampling is performed in the segmentation model.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the first type module is configured to include a plurality of depth-wise convolution layer blocks for extracting feature information of a feature map and a plurality of point-wise convolution layer blocks for controlling the number of channels of the feature map.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The method of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the first type module is configured to:<claim-text>calculate a map for extracting feature information by repeatedly applying the depth-wise convolution layer block and the point-wise convolution layer block to an input feature map;</claim-text><claim-text>calculate a map for controlling the number of channels by applying only the point-wise convolution layer block to the input feature map; and</claim-text><claim-text>sum the map for extracting the feature information and the map for adjusting the number of channels and output a result of the summation.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the second type module is configured to include a first convolution layer block for extracting feature information of a feature map, a second convolution layer block, and a global context block (GCBlock) based on the global context network.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The method of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the second type module is configured to:<claim-text>calculate a global feature map by applying the first convolution layer block, the second convolution layer block, and the global context block to an input feature map; and</claim-text><claim-text>sum the input feature map and the global feature map and output a result of the summation.</claim-text></claim-text></claim></claims></us-patent-application>