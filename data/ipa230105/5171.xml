<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005172A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005172</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17940455</doc-number><date>20220908</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>579</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>246</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>44</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>74</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>579</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>246</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>44</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>761</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20016</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">Method and System for Implementing Adaptive Feature Detection for VSLAM Systems</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>PCT/CN2021/076064</doc-number><date>20210208</date></document-id><parent-status>PENDING</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>17940455</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>GUANGDONG OPPO MOBILE TELECOMMUNICATIONS Corp. LTD.</orgname><address><city>Dongguan</city><country>CN</country></address></addressbook><residence><country>CN</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Chen</last-name><first-name>Kevin</first-name><address><city>Palo Alto</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A method includes receiving a first image, receiving a motion dataset, determining a motion level, determining an initialization state, and determining a tracking level. In a first condition, the method includes generating a first image pyramid, detecting a plurality of features in the first image pyramid using a first detector threshold, and generating a first set of detected keypoints from the plurality of features. In a second condition, the method includes generating a second image pyramid, detecting the plurality of features in the second image pyramid using a second detector threshold, the second detector threshold being less restrictive than the first detector threshold, and generating a second set of detected keypoints. In a third condition, the method includes detecting the plurality of features in the first image according to the first detector threshold and generating a third set of detected keypoint.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="110.07mm" wi="149.10mm" file="US20230005172A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="124.38mm" wi="151.13mm" file="US20230005172A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="119.46mm" wi="146.64mm" file="US20230005172A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="180.26mm" wi="119.30mm" file="US20230005172A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="191.09mm" wi="155.62mm" file="US20230005172A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="120.40mm" wi="96.18mm" file="US20230005172A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="123.44mm" wi="92.54mm" file="US20230005172A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="227.84mm" wi="109.22mm" file="US20230005172A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="150.03mm" wi="132.59mm" file="US20230005172A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION(S)</heading><p id="p-0002" num="0001">The present disclosure is a continuation-application of International (PCT) Patent Application No. PCT/CN2021/076064 filed on Feb. 8, 2021, which claims priority of U.S. Provisional Patent Application No. 62/987,028, filed on Mar. 9, 2020, the entire contents of both of which are hereby incorporated by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">The present disclosure relates to the field of augmented reality, and in particular, to a method and system for implementing adaptive feature detection for vSLAM systems, and a non-transitory computer-readable storage medium.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">Augmented Reality (AR) superimposes virtual content over a user's view of the real world. With the development of AR software development kits (SDK), the mobile industry has brought mobile device AR platforms to the mainstream. An AR SDK typically provides six degrees-of-freedom (6 DoF) tracking capability. A user can scan the environment using a camera included in an electronic device (e.g., a smartphone or an AR system), and the electronic device performs visual simultaneous localization and mapping (vSLAM) in real time. Implementing vSLAM in mobile devices can be done using a vSLAM unit to detect features of real-world objects and to track those features as the mobile device moves through its environment in three-dimensions.</p><p id="p-0005" num="0004">Despite the progress made in the field of AR, there is a need in the art for improved methods and systems related to AR.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0006" num="0005">The present disclosure relates generally to methods and systems related to augmented reality applications. More particularly, embodiments of the present disclosure provide methods and systems for adaptive feature detection using variable pyramid level and detector threshold values. The disclosure is applicable to a variety of applications involving vSLAM operations, including, but not limited to, computer vision-based online 3D modeling, AR visualization, facial recognition, robotics, and autonomous vehicles.</p><p id="p-0007" num="0006">A system of one or more computers can be configured to perform particular operations or actions by virtue of having software, firmware, hardware, or a combination of them installed on the system that in operation causes or cause the system to perform the actions. One or more computer programs can be configured to perform particular operations or actions by virtue of including instructions that, when executed by data processing apparatus, cause the apparatus to perform the actions. One general aspect includes a method of adaptive feature detection in visual simultaneous localization and mapping (vSLAM) processing. In such methods, a computer system receives a first image, receives a motion dataset, determines a motion level, determines an initialization state, and determines a tracking level. The method further includes a determination of one of at least three conditions. In a first condition, the method includes generating a first image pyramid, detecting a plurality of features in the first image pyramid using a first detector threshold, and generating a first set of detected keypoints from the plurality of features at least in part by keypoint fusion and selection. In a second condition, the method includes generating a second image pyramid, detecting the plurality of features in the second image pyramid using a second detector threshold, the second detector threshold being less restrictive than the first detector threshold, and generating a second set of detected keypoints at least in part by keypoint fusion and selection. In a third condition, the method includes detecting the plurality of features in the first image according to the first detector threshold and generating a third set of detected keypoints.</p><p id="p-0008" num="0007">Another general aspect includes a computer system including one or more processors and one or more memories storing computer-readable instructions that, upon execution by the one or more processors, configure the computer system to receive a first image, receive a motion dataset, determine a motion level, determine an initialization state, and determine a tracking level. The computer-readable instructions further configure the computer system to determine one of at least three conditions. In a first condition, the computer system is further configured to generate a first image pyramid, detect a plurality of features in the first image pyramid using a first detector threshold, and generate a first set of detected keypoints from the plurality of features at least in part by keypoint fusion and selection. In a second condition, the computer system is further configured to generate a second image pyramid, detect the plurality of features in the second image pyramid using a second detector threshold, the second detector threshold being less restrictive than the first detector threshold, and generate a second set of detected keypoints at least in part by keypoint fusion and selection. In a third condition the computer system is further configured to detect the plurality of features in the first image according to the first detector threshold and generate a third set of detected keypoints.</p><p id="p-0009" num="0008">Another general aspect includes one or more non-transitory computer-storage media storing instructions that, upon execution on a computer system, cause the computer system to perform operations including receiving a first image, receiving a motion dataset, determining a motion level, determining an initialization state, and determining a tracking level. The operations further include determining one of at least three conditions. In a first condition, the operations further include generating a first image pyramid, detecting a plurality of features in the first image pyramid using a first detector threshold, and generating a first set of detected keypoints from the plurality of features at least in part by keypoint fusion and selection. In a second condition, the operations further include generating a second image pyramid, detecting the plurality of features in the second image pyramid using a second detector threshold, the second detector threshold being less restrictive than the first detector threshold, and generating a second set of detected keypoints at least in part by keypoint fusion and selection. In a third condition, the operations further include detecting the plurality of features in the first image according to the first detector threshold and generating a third set of detected keypoints.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0010" num="0009">The drawings herein are incorporated into and form a part of the description, showing embodiments in accordance with the present disclosure, and are configured together with the description to explain the principles of the present disclosure. Apparently, the drawings described below are only some embodiments of the present disclosure. One skilled in the art may acquire other drawings based on these drawings, without making any inventive work.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates an example of a computer system that includes an inertial measurement unit and an RGB optical sensor for feature detection and tracking applications, according to an embodiment of the present disclosure.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a simplified schematic diagram illustrating a vSLAM system according to an embodiment of the present disclosure.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a simplified schematic diagram illustrating a technique for adaptive feature detection according to an embodiment of the present disclosure.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a simplified schematic diagram illustrating a technique for generating a set of detected keypoints according to an embodiment of the present disclosure.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>5</b>A</figref> is a simplified schematic diagram illustrating a technique for generating a set of detected keypoints according to an embodiment of the present disclosure.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>5</b>B</figref> is a simplified schematic diagram illustrating a technique for generating a set of detected keypoints according to an embodiment of the present disclosure.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>5</b>C</figref> is a simplified schematic diagram illustrating a technique for generating a set of detected keypoints according to an embodiment of the present disclosure.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a simplified flowchart illustrating a method of performing adaptive feature detection according to an embodiment of the present disclosure.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates an example computer system, according to embodiments of the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0020" num="0019">In the following description, various embodiments will be described. For purposes of explanation, specific configurations and details are set forth in order to provide a thorough understanding of the embodiments. However, it will also be apparent to one skilled in the art that the embodiments may be practiced without the specific details. Furthermore, well-known features may be omitted or simplified in order not to obscure the embodiment being described.</p><p id="p-0021" num="0020">Embodiments of the present disclosure are directed to, among other things, a vSLAM unit including a detection strategy processor, a motion monitor, and a tracking performance monitor. The robustness of vSLAM unit operations, in particular feature detection and tracking, may be improved by introducing the detection strategy processor, the tracking performance monitor, and the motion monitor in communication with the vSLAM unit. The detection strategy processor may implement a pyramid level detection technique to improve the robustness of detection of features in images received by the vSLAM unit. The detection strategy processor may employ variable detection threshold values and variable pyramid level values during feature detection operations as a function of an initialization state, a motion level, and/or a tracking level. In this way, the detection strategy processor may reduce the effect of initialization errors and motion on feature detection and tracking operations carried out by the vSLAM unit.</p><p id="p-0022" num="0021">In some embodiments, the detection strategy processor may determine an initialization state describing whether the vSLAM unit is initialized. The detection strategy processor may also receive a motion level, based at least in part on motion data received from an inertial measurement unit (IMU), and determined by the motion monitor. The detection strategy processor may also receive a tracking level, based at least in part on the error in tracking features as determined by the tracking performance monitor. Based at least in part on the initialization state, the motion level, and/or the tracking level, the detection strategy processor may implement feature detection (also referred to as keypoint detection) using an image pyramid including a number of levels described by a pyramid level value and applying a detector threshold to feature detection operations. The detection strategy processor may modify the pyramid level value and/or the detector threshold as a function of the initialization state, the motion level, and/or the tracking level. The detection strategy processor may generate a set of detected keypoints for the vSLAM unit to employ in feature tracking operations on subsequent images received by the vSLAM unit.</p><p id="p-0023" num="0022">In an illustrative example, a smart phone app may include an AR function to superimpose animated elements onto objects in the real world. For example, the animated elements could be signs, floral motifs, cartoon animals, etc. The smart phone app may, for example, detect and track specific objects so that a specific animated element only appears on the screen of the phone when a specific object is in the field of view of the camera. To correctly place an animated element in the display field at the appropriate size, perspective, and position to appear as though it were interacting with real world objects, the smart phone app needs information about the surfaces of the objects in the environment around the phone and the position and orientation of the phone (also referred to as the pose). In some cases, this information includes images captured by the camera and information about the motion of the phone in the environment. To determine the pose of the camera, the vSLAM unit may perform an initialization operation, whereby it calculates an initial mapping of three dimensional features into a multi-dimensional coordinate system and further provides an initial pose of the camera relative to the coordinate system.</p><p id="p-0024" num="0023">The vSLAM unit may then initiate feature detection and tracking operations using images as they are received from the camera, such that the objects in the field of view of the camera are tracked. Receiving an image, the vSLAM unit may perform feature tracking on that image using a set of detected keypoints determined either during initialization or in a prior cycle of feature detection. The results of feature tracking may be used to determine a tracking level. Based at least in part on the tracking level, the image may be used in feature detection, such that the set of detected keypoints is updated. In some cases, the feature detection process may be utilized in response to errors in feature tracking exceeding an allowable threshold. The results of feature tracking and feature detection may then be used to optimize the output of the vSLAM unit, for example, by bundle adjustment. This may include motion data from an intertial motion unit (IMU). In some cases, the vSLAM unit may adapt feature detection procedures to correct for deviation from the conditions in which the vSLAM unit was initialized, at least in part by updating the set of detected keypoints.</p><p id="p-0025" num="0024">In this example, the vSLAM unit may include additional units to improve the robustness of feature detection and tracking operations. For example, the vSLAM unit may include a detection strategy processor to modify the process for updating the set of detected keypoints. The detection strategy processor may receive multiple inputs, including a motion level, an initialization state, and/or a tracking level. Each input may be determined by a unit included in the vSLAM unit, and may be used by the detection strategy processor in determining the pyramid level value and the detector threshold to apply to feature detection. The updated set of detected keypoints, produced by the detection strategy processor, may be applied to feature tracking as a technique to reduce error in feature tracking and to improve the output of the vSLAM unit.</p><p id="p-0026" num="0025">In general, vSLAM permits an AR system, as well as other types of systems that use computer vision (CV) to detect features and objects in the real world, to detect and track objects as the system moves relative to the objects. Because initialization, motion, and tracking errors may adversely affect the accuracy and robustness of the vSLAM unit, systems to improve feature detection and tracking reduce that error and improve the output pose generated by vSLAM operations are provided by embodiments of the present disclosure.</p><p id="p-0027" num="0026">In some embodiments, a method implemented by a computer system is provided. The method includes: receiving a first image by a visual simultaneous localization and mapping (vSLAM) unit, the first image being generated by an optical sensor in communication with the computer system; receiving a motion dataset generated by an inertial measurement unit in communication with the vSLAM unit; determining a motion level by the vSLAM unit using a motion monitor; determining an initialization state by the vSLAM unit using an initializer; determining a tracking level by the vSLAM unit using a tracking performance monitor; and in a first condition, using a detection strategy processor of the vSLAM unit generating a first image pyramid; detecting a plurality of features in the first image pyramid using a first detector threshold; and generating a first set of detected keypoints from the plurality of features at least in part by keypoint fusion and selection; in a second condition, using a detection strategy processor of the vSLAM unit generating a second image pyramid; detecting the plurality of features in the second image pyramid using a second detector threshold, the second detector threshold being less restrictive than the first detector threshold; and generating a second set of detected keypoints at least in part by keypoint fusion and selection; and in a third condition, using a detection strategy processor of the vSLAM unit detecting the plurality of features in the first image according to the first detector threshold; and generating a third set of detected keypoints.</p><p id="p-0028" num="0027">In some embodiments, the first condition is a determination that the initialization state is true and the motion level is true or the initialization state is false; the second condition is a determination that the initialization state is true, the motion level is false, and the tracking level is false; and the third condition is a determination that the initialization state is true, the motion level is false, and the tracking level is true.</p><p id="p-0029" num="0028">In some embodiments, the method further includes: receiving a second image; performing feature tracking on the second image at least in part according to the first set of detected keypoints, the second set of detected keypoints, or the third set of detected keypoints; determining a tracking quality; and in accordance with a determination that the tracking quality is false, generating updated keypoints from the second image.</p><p id="p-0030" num="0029">In some embodiments, the determining an initialization state includes: receiving one or more initialization parameters from an initializer in communication with the computer system; determining an initialization quality value, based at least in part on the one or more initialization parameters; comparing the initialization quality value to a threshold criterion; and in accordance with the initialization quality value satisfying the threshold criterion, determining that the initialization state is true; or in accordance with the initialization quality value not satisfying the threshold criterion, determining that the initialization state is false.</p><p id="p-0031" num="0030">In some embodiments, the determining a motion level includes: receiving the motion dataset from an inertial measurement unit in communication with the computer system; determining a displacement value by a motion monitor in communication with the computer system based at least in part on the motion dataset; comparing the displacement value to a threshold criterion; and in accordance with the displacement value satisfying the threshold criterion, determining that the motion level is true; or in accordance with the displacement value not satisfying the threshold criterion, determining that the motion level is false.</p><p id="p-0032" num="0031">In some embodiments, the determining a tracking level includes: receiving a set of keypoints; tracking the set of keypoints in the first image; selecting a set of inliers from the set of keypoints tracked in the first image; determining an error value from the set of inliers; comparing the error value to an error threshold; and in accordance with the error value satisfying the error threshold, determining that the tracking level is true; or in accordance with the error value not satisfying the error threshold, determining that the tracking level is false.</p><p id="p-0033" num="0032">In some embodiments, the generating the first image pyramid includes generating N downscaled images from the first image, each subsequent image after the first image having a lower average pixel-resolution than an image preceding it in the first image pyramid, wherein N is a pyramid level value corresponding to a nonzero integer.</p><p id="p-0034" num="0033">In some embodiments, the first detector threshold is determined at least in part according to a detector threshold used for initializing a vSLAM unit.</p><p id="p-0035" num="0034">In some embodiments, the first image is received from a camera in communication with a vSLAM unit.</p><p id="p-0036" num="0035">In some embodiments, a computer system is provided. The computer system includes one or more processors and one or more memories storing computer-readable instructions. The computer-readable instructions, upon execution by the one or more processors, configure the computer system to: receive a first image by a visual simultaneous localization and mapping (vSLAM) unit, the first image being generated by an optical sensor in communication with the computer system; receive a motion dataset generated by an inertial measurement unit in communication with the vSLAM unit; determine a motion level by the vSLAM unit using a motion monitor; determine an initialization state by the vSLAM unit using an initializer; determine a tracking level by the vSLAM unit using a tracking performance monitor; and in a first condition, using a detection strategy processor of the vSLAM unit generate a first image pyramid; detect a plurality of features in the first image pyramid using a first detector threshold; and generate a first set of detected keypoints from the plurality of features at least in part by keypoint fusion and selection; in a second condition, using a detection strategy processor of the vSLAM unit generate a second image pyramid; detect the plurality of features in the second image pyramid using a second detector threshold, the second detector threshold being less restrictive than the first detector threshold; and generate a second set of detected keypoints at least in part by keypoint fusion and selection; and in a third condition, using a detection strategy processor of the vSLAM unit detect the plurality of features in the first image according to the first detector threshold; and generate a third set of detected keypoints.</p><p id="p-0037" num="0036">In some embodiments, the first condition is a determination that the initialization state is true and the motion level is true or the initialization state is false; the second condition is a determination that the initialization state is true, the motion level is false, and the tracking level is false; and the third condition is a determination that the initialization state is true, the motion level is false, and the tracking level is true.</p><p id="p-0038" num="0037">In some embodiments, the computer-readable instructions further configure the computer system to: receive a second image; perform feature tracking on the second image at least in part according to the first set of detected keypoints, the second set of detected keypoints, or the third set of detected keypoints; determine a tracking quality; and in accordance with a determination that the tracking quality is false, generate updated keypoints from the second image.</p><p id="p-0039" num="0038">In some embodiments, determining an initialization state includes: receiving one or more initialization parameters from an initializer in communication with the computer system; determining an initialization quality value, based at least in part on the one or more initialization parameters; comparing the initialization quality value to a threshold criterion; and in accordance with the initialization quality value satisfying the threshold criterion, determining that the initialization state is true; or in accordance with the initialization quality value not satisfying the threshold criterion, determining that the initialization state is false.</p><p id="p-0040" num="0039">In some embodiments, determining a motion level includes: receiving a motion dataset from an inertial measurement unit in communication with the computer system; determining a displacement value by a motion monitor in communication with the computer system based at least in part on the motion dataset; comparing the displacement value to a threshold criterion; and in accordance with the displacement value satisfying the threshold criterion, determining that the motion level is true; or in accordance with the displacement value not satisfying the threshold criterion, determining that the motion level is false.</p><p id="p-0041" num="0040">In some embodiments, determining a tracking level includes: receiving a set of keypoints; tracking the set of keypoints in the first image; selecting a set of inliers from the set of keypoints tracked in the first image; determining an error value from the set of inliers; comparing the error value to an error threshold; and in accordance with the error value satisfying the error threshold, determining that the tracking level is true; or in accordance with the error value not satisfying the error threshold, determining that the tracking level is false.</p><p id="p-0042" num="0041">In some embodiments, generating the first image pyramid includes generating N downscaled images from the first image, each subsequent image after the first image having a lower average pixel-resolution than an image preceding it in the first image pyramid, wherein N is a pyramid level value corresponding to a nonzero integer.</p><p id="p-0043" num="0042">In some embodiments, one or more non-transitory computer-storage media are provided. The one or more non-transitory computer-storage media store instructions that, upon execution on a computer system, cause the computer system to perform operations including: receiving a first image by a visual simultaneous localization and mapping (vSLAM) unit, the first image being generated by an optical sensor in communication with the computer system; receiving a motion dataset generated by an inertial measurement unit in communication with the vSLAM unit; determining a motion level by the vSLAM unit using a motion monitor; determining an initialization state by the vSLAM unit using an initializer; determining a tracking level by the vSLAM unit using a tracking performance monitor; and in a first condition, using a detection strategy processor of the vSLAM unit generating a first image pyramid; detecting a plurality of features in the first image pyramid using a first detector threshold; and generating a first set of detected keypoints from the plurality of features at least in part by keypoint fusion and selection; in a second condition, using a detection strategy processor of the vSLAM unit generating a second image pyramid; detecting the plurality of features in the second image pyramid using a second detector threshold, the second detector threshold being less restrictive than the first detector threshold; and generating a second set of detected keypoints at least in part by keypoint fusion and selection; and in a third condition, using a detection strategy processor of the vSLAM unit detecting the plurality of features in the first image according to the first detector threshold; and generating a third set of detected keypoints. The first condition is a determination that the initialization state is true and the motion level is true or the initialization state is false; the second condition is a determination that the initialization state is true, the motion level is false, and the tracking level is false; and the third condition is a determination that the initialization state is true, the motion level is false, and the tracking level is true.</p><p id="p-0044" num="0043">In some embodiments, the one or more non-transitory computer-storage media of claim <b>17</b> wherein determining an initialization state includes: receiving one or more initialization parameters from an initializer in communication with the computer system; determining an initialization quality value, based at least in part on the one or more initialization parameters; comparing the initialization quality value to a threshold criterion; and in accordance with the initialization quality value satisfying the threshold criterion, determining that the initialization state is true; or in accordance with the initialization quality value not satisfying the threshold criterion, determining that the initialization state is false.</p><p id="p-0045" num="0044">In some embodiments, determining a motion level includes: receiving a motion dataset from an inertial measurement unit in communication with the computer system; determining a displacement value by a motion monitor in communication with the computer system based at least in part on the motion dataset; comparing the displacement value to a threshold criterion; and in accordance with the displacement value satisfying the threshold criterion, determining that the motion level is true; or in accordance with the displacement value not satisfying the threshold criterion, determining that the motion level is false.</p><p id="p-0046" num="0045">In some embodiments, determining a tracking level includes: receiving a set of keypoints; tracking the set of keypoints in the first image; selecting a set of inliers from the set of keypoints tracked in the first image; determining an error value from the set of inliers; comparing the error value to an error threshold; and in accordance with the error value satisfying the error threshold, determining that the tracking level is true; or in accordance with the error value not satisfying the error threshold, determining that the tracking level is false.</p><p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates an example of a computer system <b>110</b> that includes an inertial measurement unit (IMU) <b>112</b> and an RGB optical sensor <b>114</b> for feature detection and tracking applications, according to an embodiment of the present disclosure. The feature detection and tracking may be implemented by a vSLAM unit <b>116</b> of the computer system <b>110</b>. Generally, the RGB optical sensor <b>114</b> generates an RGB image of a real-world environment that includes, for instance, a real-world object <b>130</b>. In some embodiments, the IMU <b>112</b> generates motion data about the motion of the computer system <b>110</b> in a three-dimensional environment, where this data includes, for instance, rotation and translation of the IMU <b>112</b> with respect to six degrees of freedom (e.g., translation and rotation according to three Cartesian axes). Following an initialization of an AR session (where this initialization can include calibration and tracking), the vSLAM unit <b>116</b> renders an optimized output pose <b>120</b> of the real-world environment in the AR session, where the optimized output pose <b>120</b> describes the pose of the RGB optical sensor <b>114</b> at least in part with respect to a map of features <b>124</b> detected in the real-world object <b>130</b>. The optimized output pose <b>120</b> describes a coordinate system and a map for placing two dimensional AR objects onto a real-world object representation <b>122</b> of the real-world object <b>130</b>.</p><p id="p-0048" num="0047">In an example, the computer system <b>110</b> represents a suitable user device that includes, in addition to the IMU <b>112</b> and the RGB optical sensor <b>114</b>, one or more graphical processing units (GPUs), one or more general purpose processors (GPPs), and one or more memories storing computer-readable instructions that are executable by at least one of the processors to perform various functionalities of the embodiments of the present disclosure. For instance, the computer system <b>110</b> can be any of a smartphone, a tablet, an AR headset, or a wearable AR device, and the like.</p><p id="p-0049" num="0048">The IMU <b>112</b> may have a known sampling rate (e.g., a time-frequency of data point production) and this value may be stored locally and/or be accessible to the vSLAM unit <b>116</b>. The RGB optical sensor <b>114</b> may be a color camera. The RGB optical sensor <b>114</b> and the IMU <b>112</b> may have different sampling rates. Typically, the sampling rate of RGB optical sensor <b>114</b> is lower than that of the IMU <b>112</b>. For instance, the RGB optical sensor <b>114</b> may have a sampling rate of 30 Hz, while the IMU <b>112</b> may have a sampling rate of 100 Hz.</p><p id="p-0050" num="0049">In addition, the IMU <b>112</b> and the RGB optical sensor <b>114</b>, as installed in the computer system <b>110</b>, may be separated by a transformation (e.g., distance offset, field of view angle difference, etc.). This transformation may be known and its value may be stored locally and/or be accessible to the vSLAM unit <b>116</b>. During movement of the computer system <b>110</b>, the RGB optical sensor <b>114</b> and the IMU <b>112</b> may experience disparate motion relative to the centroid, the center of mass, or another point of rotation of the computer system <b>110</b>. In some instances, the transformation may lead to error or mismatch in vSLAM optimized output poses. To that end, the computer system may include calibration data. In some instances, the calibration data may be set based only on the transformation. The calibration data may include data associated at least in part with the resolution of the RGB optical sensor <b>114</b>.</p><p id="p-0051" num="0050">The vSLAM unit <b>116</b> may be implemented as specialized hardware and/or a combination of hardware and software (e.g., general purpose processor and computer-readable instructions stored in memory and executable by the general purpose processor). In addition to initializing an AR session, the computer system <b>110</b> may perform adaptive feature detection techniques as part of vSLAM processes, as described in reference to <figref idref="DRAWINGS">FIGS. <b>2</b>-<b>7</b></figref>.</p><p id="p-0052" num="0051">In an illustrative example of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, a smartphone is used for an AR session that shows the real-world environment. In particular, the AR session includes rendering an AR scene that includes a representation of a real-world table on top of which a vase (or some other real-world object) is placed. A virtual object is to be shown in the AR scene. In particular, the virtual object is to be shown on top of the table. As part of detecting how the smartphone is oriented in the real-world environment relative to the table and the vase, the smartphone may initialize a vSLAM unit using images from RGB optical sensor <b>114</b> or other camera. The vSLAM unit will define a coordinate system in reference to which it will detect features in the table and the vase. After initialization, the vSLAM unit will detect and track the features as part of the overall AR system. While detecting and tracking features, the phone may monitor accuracy of tracking operations of the vSLAM unit <b>116</b>, motion level of the computer system <b>110</b>, and/or initialization quality, and may adapt the feature detection procedure used by the vSLAM unit <b>116</b> to improve the robustness of feature detection and tracking.</p><p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a simplified schematic diagram illustrating a vSLAM system <b>200</b> according to an embodiment of the present disclosure. In some cases, the vSLAM system <b>200</b> performs feature detection and tracking operations following initialization. In some cases, the vSLAM unit <b>116</b> receives a first image <b>202</b> from an RGB optical sensor (e.g., RGB optical sensor <b>114</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>). The first image <b>202</b> may form a part of a set of images received by the vSLAM unit <b>116</b>, such that the vSLAM unit <b>116</b> has generated a set of detected keypoints from a prior-received image of the set of images, either during initialization by an initializer unit <b>220</b> or in prior feature detection operations. The vSLAM unit <b>116</b> may also receive IMU data <b>204</b>.</p><p id="p-0054" num="0053">Features detected in a prior received image of the set of images may be tracked in the first image <b>202</b> by a feature tracking unit <b>240</b>. The output of the feature tracking unit <b>240</b> may include information describing features that are described as inliers or outliers, based at least in part on whether the change in feature position fits a model prediction of coordinated feature shift, based at least in part on the initialization, the coordinate system, and/or motion of the computer system (e.g., computer system <b>110</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>) relative to its environment. For example, initialization of the vSLAM unit <b>116</b> may determine a coordinate system and an output pose such that a feature is predicted to be translated by a given displacement in the first image <b>202</b> relative to the prior-received image of the set of images received by the vSLAM unit <b>116</b>. The displacement may be mapped to the coordinate system generated by the initializer <b>220</b> during initialization of the vSLAM unit <b>116</b>. Based at least in part on a determination of error between the model prediction and the measured displacement, a feature may be designated as an inlier or an outlier.</p><p id="p-0055" num="0054">The feature tracking information generated by the feature tracking unit <b>240</b> may be analyzed by a tracking performance monitor <b>250</b> to determine a tracking level, which may be a value along a range of values, for example, a value between zero and one along a scale ranging from zero to one. In some cases, the tracking performance monitor <b>250</b> may perform one or more operations using inlier data from the feature tracking unit <b>240</b> to determine if feature tracking in the first image <b>202</b> meets a predetermined criterion of the vSLAM system <b>200</b>. For example, the tracking performance monitor <b>250</b> may integrate the error for inliers tracked in the first image <b>202</b>, and compare that integrated error to a threshold value &#x3bb;. In some cases, the tracking performance monitor <b>250</b> may determine a tracking level based on whether the error exceeds &#x3bb;, such that the tracking level is false when the error exceeds &#x3bb; and the tracking level is true when the error does not exceed &#x3bb;.</p><p id="p-0056" num="0055">The feature tracking level output by tracking performance monitor <b>250</b> may be received as an input to detection strategy processor <b>260</b>, which may also receive input from initializer <b>220</b> and from motion monitor <b>230</b>. In some cases, the initializer <b>220</b> may determine an initialization state based at least in part on a measurement of initialization accuracy and/or quality. The initialization state may be represented as a true or false value received by the detection strategy processor <b>260</b>. In some cases, the initialization state may be determined by calculating the error in the current features tracked in the image with respect to the initial output pose and the coordinate system generated during initialization. For example, the computer system (e.g., computer system <b>110</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>) including the vSLAM unit <b>116</b> may move from one environment to another (e.g., from an interior to an exterior environment), such that the initial coordinate system no longer accurately describes the environment surrounding the computer system. In some cases, the vSLAM unit <b>116</b> may determine the initialization state to be false when the initialization accuracy crosses a threshold value.</p><p id="p-0057" num="0056">In some cases, the motion monitor <b>230</b> may receive IMU data <b>204</b>, including translation and rotation data in six degrees of freedom, as described in more detail in reference to <figref idref="DRAWINGS">FIG. <b>1</b></figref>. The motion monitor <b>230</b> may determine a motion level, which may be represented as a true or false value and can be based on accelerometer output and/or gyroscope output. In other embodiments, the motion level may be a value along a range of values, for example, a value between zero and one along a scale ranging from zero to one. In some cases, the motion level may be determined based on one or more operations reducing the IMU data <b>204</b> to a single displacement value, then comparing the displacement value to a threshold value, as described in more detail in reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>. The motion level received by the detection strategy processor <b>260</b> may be used along with the initialization state and/or the tracking level to modify the operation of a feature detection unit <b>270</b> as described in more detail in reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>. The IMU data <b>204</b> along with the output of the feature detection unit <b>270</b> may be received by an optimization unit <b>280</b>, to optimize the output pose of an optical sensor (e.g., RGB optical sensor <b>114</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>). In some cases, optimization may include bundle adjustment (BA) operations. BA operations may adjust an output pose <b>290</b> generated by the vSLAM unit <b>116</b> to minimize a cost function that quantifies an error in fitting a model to parameters including, but not limited to, camera poses and coordinates in the coordinate map associated with features detected in a three dimensional environment (e.g., features <b>124</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>).</p><p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a simplified schematic diagram illustrating a technique <b>300</b> for adaptive feature detection according to an embodiment of the present disclosure. In some cases, motion data <b>342</b> are generated from IMU data <b>204</b> by integration <b>340</b>, where the integration <b>340</b> converts acceleration data in six degrees of freedom to a displacement value. For example, the translational and rotational acceleration measured by accelerometers included in an IMU (e.g., IMU <b>112</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>) may be integrated over three spatial dimensions to generate a displacement value in units of length (e.g., meters). In some cases, the motion data <b>342</b> is received by the detection strategy processor <b>260</b>. The detection strategy processor <b>260</b>, as described in reference to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, may receive an image t&#x2212;1 forming part of a set of images <b>302</b>. The detection strategy processor <b>260</b> may generate a set of detected keypoints <b>312</b> from the image t&#x2212;1 based at least in part on the motion data, the initialization state, and/or the tracking level. The set of detected keypoints <b>312</b> may be applied in feature tracking <b>320</b> of the features in a subsequent image t from the set of images <b>302</b>. A plurality of tracked feature points <b>322</b> produced during feature tracking <b>320</b> are used to determine tracking quality <b>330</b>, as described in more detail in reference to <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0059" num="0058">In some cases, the tracking quality fails to satisfy a predetermined threshold, prompting the detection strategy processor <b>260</b> to repeat detection operations and generate another set of detected keypoints <b>312</b>. As an example, if the tracking quality is poor, for example, because the image contains few elements that can be tracked, the detection threshold can be reduced and/or the pyramid level can be increased as described herein. In some cases, the tracking quality <b>330</b> satisfies the predetermined threshold, following which the vSLAM unit may implement data alignment <b>350</b> to compensate for motion of the computer system as measured by the IMU, and/or may determine an updated initialization state <b>360</b>. In some cases, the initialization state <b>360</b> is false, such that the vSLAM unit may not update the output pose <b>362</b>. In some cases, the initialization state <b>360</b> is true, such that the vSLAM unit may implement optimization <b>370</b> of the output pose as described in more detail in reference to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, thereby generating an optimized pose <b>372</b>.</p><p id="p-0060" num="0059">In some cases, the adaptive feature detection technique <b>300</b> includes multiple iterations of the process, such that each image in the set of images <b>302</b> is processed as image t&#x2212;1 in the detection strategy processor <b>260</b> and subsequently as image tin feature tracking <b>320</b>. In some cases, the feature tracking quality satisfies a predetermined threshold, such that multiple consecutive images in the set of images <b>302</b> are processed in feature tracking <b>320</b> using the same set of detected keypoints <b>312</b>, without updating the set of detected keypoints <b>312</b>, as, for example, when the tracking quality <b>330</b> remains true for multiple tracking cycles. In some cases, motion data <b>342</b> or tracking quality <b>330</b> may necessitate redefining the set of detected keypoints <b>312</b>, such that the detection strategy processor <b>260</b> receives the image t&#x2212;1 in the set of images <b>302</b> and performs keypoint detection operations as described in more detail in reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref> below.</p><p id="p-0061" num="0060"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a simplified schematic diagram illustrating a technique <b>400</b> for generating a set of detected keypoints according to an embodiment of the present disclosure. In some cases, the detection strategy processor <b>260</b>, as described in reference to <figref idref="DRAWINGS">FIGS. <b>2</b>-<b>3</b></figref>, receives an image <b>202</b> (e.g., image <b>202</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref> and the image t&#x2212;1 of <figref idref="DRAWINGS">FIG. <b>3</b></figref>) from a set of images (e.g., set of images <b>302</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>). In some cases, the detection strategy processor <b>260</b> also receives an initialization state <b>360</b>, a motion level <b>422</b>, and a tracking level <b>424</b>, as described in more detail in reference to <figref idref="DRAWINGS">FIG. <b>2</b></figref>. In some cases, the motion level is determined based on comparing the motion data <b>342</b> to a threshold displacement value to generate a true or false value. In <figref idref="DRAWINGS">FIG. <b>4</b></figref>, I represents the initialization state being true, T represents the tracking level being true, and M represents the motion level being true. Strategies described in <figref idref="DRAWINGS">FIG. <b>4</b></figref> are named for which of the determined parameters are true, with false parameters omitted. Operations internal to the detection strategy processor may include, but are not limited to, implementing one or more keypoint detection strategies in accordance with the combination of initialization state <b>360</b>, motion level <b>422</b>, and/or tracking level <b>424</b> values. The detection strategy processor <b>260</b> may determine a pyramid level value and/or a detector threshold value based at least in part on the combination of values, such that the set of detected keypoints <b>312</b> is generated from the image <b>202</b>. In some cases, the set of detected keypoints <b>312</b> may include the output of a single detection strategy per cycle. In some cases, the detection strategy processor <b>260</b> may implement a single strategy per cycle, based at least in part on the combination of values received.</p><p id="p-0062" num="0061">In general, the pyramid level value, as described in more detail in reference to <figref idref="DRAWINGS">FIGS. <b>5</b>A-<b>5</b>C</figref>, describes a number of downscaling steps by which additional images are generated from the image <b>202</b> for subsequent processing in keypoint detection according to a detector threshold. The detector threshold in keypoint detection refers to the standard by which measured features in the image <b>202</b> are either recorded in the set of detected keypoints <b>312</b> or are discarded. In some cases, the detection strategy processor employs as a first detector threshold that the threshold applied during initialization.</p><p id="p-0063" num="0062">In some cases, the initialization state <b>360</b> and the tracking level <b>424</b> are true, while only the motion level <b>422</b> is false, corresponding to satisfactory initialization, tracking and motion. In accordance with this combination of values, the detection strategy processor <b>260</b> may implement a keypoint detection strategy IT <b>430</b> using a pyramid level value of zero and without modifying the detector threshold from a default value or current value. Keypoint detection strategy IT <b>430</b> can be referred to as the default keypoint detection strategy that may be implemented when the vSLAM unit is initialized and when the tracking error and motion level are nominal.</p><p id="p-0064" num="0063">In some cases, only the initialization state <b>360</b> is true, while the tracking level <b>424</b> and the motion level <b>422</b> are false. In accordance with this combination of values, the detection strategy processor <b>260</b> may implement a keypoint detection strategy I <b>432</b> using a pyramid level value N, where N is an integer greater than zero. Keypoint detection strategy I <b>432</b> can be referred to as the tracking-error keypoint detection strategy that may be implemented when the vSLAM unit is initialized and motion level is nominal, but the vSLAM unit measures tracking error outside a predetermined threshold. The pyramid level value may be determined based at least in part on parameters of the hardware making up the computer system (e.g., computer system <b>110</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>). In some cases, in detection strategy I <b>432</b> the detection strategy processor <b>260</b> may reduce the detector threshold to a reduced threshold, being less restrictive than the default or current threshold. In some cases, performing keypoint detection on the N images according at least in part to the reduced detector threshold permits the detection strategy I <b>432</b> to detect a larger number of keypoints relative to keypoint detection strategy IT <b>430</b>, such that the set of detected keypoints <b>312</b> permit improved tracking based on a higher quality detection result, as described in more detail in reference to <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0065" num="0064">In some cases, the initialization state <b>360</b> and the motion level <b>422</b> are true, while the tracking level <b>424</b> is false. In accordance with this combination of values, the detection strategy processor <b>260</b> may implement a keypoint detection strategy IM <b>434</b><i>a </i>using the nonzero integer pyramid level value N and the default value or current value for the detector threshold. Keypoint detection strategy IM <b>434</b><i>a </i>can be referred to as the high-motion keypoint detection strategy that may be implemented when the vSLAM unit is initialized and feature tracking is nominal, but the vSLAM unit determines motion outside a predetermined threshold. In some cases, the motion level being true indicates that the displacement, and thus the motion of the computer system as measured by the IMU, has crossed the threshold value (e.g. the computer system may be moving &#x201c;fast&#x201d; and/or may have experienced non-optimum acceleration during the recent timeframe over which the IMU measurement was generated). Detection strategy IM <b>434</b><i>a </i>may include a nonzero pyramid level value to improve the robustness of feature detection by selecting features that appear across pyramid levels, as described in more detail in reference to <figref idref="DRAWINGS">FIG. <b>5</b></figref>, but leaves the detector threshold value unchanged, at least in part because the tracking level indicates that tracking quality satisfies a predetermined threshold. As an illustrative example, strategy IM <b>434</b> may be applied by the detection strategy processor in response to the motion level crossing from false to true as a result of the environment around the computer system moving with the computer system in coincident motion (e.g., a smart phone held in the passenger compartment of a turning or accelerating vehicle) such that the vSLAM unit (e.g., the vSLAM unit <b>116</b> of <figref idref="DRAWINGS">FIGS. <b>1</b>-<b>2</b></figref>) may track keypoints in the environment, while registering a displacement outside a predetermined threshold.</p><p id="p-0066" num="0065">In some cases, the initialization state <b>360</b> may be false. In accordance with this combination of values, the detection strategy processor <b>260</b> may implement a keypoint detection null strategy <b>434</b><i>b </i>using a nonzero integer pyramid level value N and the default value or current value for the detector threshold. The keypoint detection null strategy <b>434</b><i>b </i>can be referred to as the initialization keypoint detection strategy that may be implemented when the detection strategy processor determines that the vSLAM unit is not initialized. The term null refers to none of the parameters being true, in which case the most robust detection approach may be applied to compensate for inadequate initialization. The keypoint detection null strategy <b>434</b><i>b </i>may correspond to the same parameters as strategy IM <b>434</b><i>a</i>, at least in part to correct for the initialization of the vSLAM unit no longer providing an accurate initial coordinate mapping or initial pose to produce accurate vSLAM operations, including, but not limited to, optimized output poses. As described in more detail in reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the initialization state may be an important parameter at several points in the vSLAM technique <b>300</b>, such that the vSLAM unit may leave the output pose unchanged when the initialization state <b>360</b> is false. As such, in some cases, the detection strategy processor employs the keypoint detection null strategy <b>434</b><i>b </i>until the vSLAM unit completes reinitialization. In some cases, the detection strategy processor may employ the detector threshold used for initialization as the detector threshold employed in the keypoint detection null strategy <b>434</b><i>b. </i></p><p id="p-0067" num="0066"><figref idref="DRAWINGS">FIGS. <b>5</b>A-<b>5</b>C</figref> each are simplified schematic diagrams illustrating a technique for generating a set of detected keypoints (e.g., set of detected keypoints <b>312</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>) according to an embodiment of the present disclosure. As described in more detail in reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the detection strategy processor (e.g., detection strategy processor <b>260</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>) may implement a detection strategy in accordance with the combination of initialization state, motion level, and/or tracking level values. The four strategies described in reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref> may include different combinations of detector threshold values and pyramid level values, as described in more detail, below.</p><p id="p-0068" num="0067"><figref idref="DRAWINGS">FIG. <b>5</b>A</figref> is a simplified schematic diagram illustrating a technique for generating a set of detected keypoints according to an embodiment of the present disclosure. In some cases, as described in reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the detector strategy processor employs detection strategy IT <b>430</b>, in accordance with nominal operation of the vSLAM unit (e.g., vSLAM unit <b>116</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>). As such, the detection strategy IT <b>430</b> may include the detector strategy processor receiving a raw image <b>502</b> (e.g., image <b>202</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, and image t&#x2212;1 of <figref idref="DRAWINGS">FIG. <b>3</b></figref>). In some cases, the detection strategy processor (e.g., detection strategy processor <b>260</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>) generates a set of detected keypoints <b>312</b> by detecting features in the raw image <b>502</b> using the default or current threshold value, according to original-T detection <b>510</b>. In some cases, the original-T detection <b>510</b> does not include a pyramid level, and is included as an option by the detection strategy processor to account for accumulated errors in feature tracking over a number of cycles of feature tracking by the vSLAM system.</p><p id="p-0069" num="0068"><figref idref="DRAWINGS">FIG. <b>5</b>B</figref> is a simplified schematic diagram illustrating a technique for generating a set of detected keypoints according to an embodiment of the present disclosure. In some cases, as described in reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the detector strategy processor employs detection strategy I <b>432</b>, in accordance with operation of the vSLAM unit under conditions where motion is satisfactory but tracking is unsatisfactory. As such, the detection strategy I <b>432</b> may generate one or more images by pyramid construction <b>540</b>, according to a pyramid level value N, where N is a nonzero integer. The detection strategy processor may process the raw image <b>502</b> and a set of N downscaled images <b>542</b> <i>a</i>-<i>n</i>, where a and n are integers greater than zero and n is the pyramid level value. In some cases, N is equal to a. In detection strategy I <b>432</b>, each downscaled image <b>542</b><i>a </i>through downscaled image <b>542</b><i>n </i>may have a pixel-resolution lower than that of the raw image <b>502</b>, and each may have a progressively lower pixel resolution than the preceding downscaled image <b>542</b><i>a</i>-<i>n </i>in the image pyramid. In some cases, downscaling may be based at least in part on a downscaling factor (e.g., binomial filter downscaling) or it may be based on a spatially weighted downscaling to emphasize one or more regions in the raw image <b>502</b>. Pyramid construction <b>540</b> may include, but is not limited to, Gaussian, laplacian, and steerable pyramid construction techniques. Gaussian methods, for example, may employ a contextual smoothing function based on a Gaussian filter. In contrast, steerable pyramid methods may employ multi-scale, multi-orientation band-pass filters to modify the scaling operation for each level of the image pyramid. In some cases, in accordance with strategy I <b>432</b>, the detection strategy processor generates the set of detected keypoints <b>312</b> by detecting features in the raw image <b>502</b> using a reduced detector threshold value, according to reduced-T detection <b>544</b>. Accordingly, the detection strategy processor may use reduced T detection <b>544</b><i>a</i>-<i>n </i>for each of the downscaled images <b>542</b><i>a</i>-<i>n</i>. In some cases, following keypoint detection <b>544</b>-<b>544</b><i>n</i>, the detection strategy I <b>432</b> may include keypoint fusion and selection <b>546</b>. Keypoint fusion and selection <b>546</b> may include selecting the set of detected keypoints <b>312</b> by combining the results of reduced-T detection <b>544</b> with that of each downscaled image <b>542</b><i>a</i>-<i>n</i>, fusing keypoints that are likely to be associated with the same feature across the images in the image pyramid, and selecting keypoints based at least in part on a score for the fused keypoints. In some cases, the fusion of keypoints is based at least in part on spatial localization of keypoints relative to each other in the coordinate system generated during initialization. In some cases, the detection strategy I <b>432</b> employs other techniques, such as keypoint descriptor fusion, which includes comparison of the contextual information for each keypoint in an effort to identify two or more keypoints with each other. Following keypoint fusion and selection the detection strategy I <b>432</b> may produce the set of detected keypoints <b>312</b> for use by the technique <b>300</b>.</p><p id="p-0070" num="0069"><figref idref="DRAWINGS">FIG. <b>5</b>C</figref> is a simplified schematic diagram illustrating a technique for generating a set of detected keypoints according to an embodiment of the present disclosure. In some cases, the detection strategy processor implements detection strategy IM or the null strategy (e.g., strategies <b>434</b><i>a</i>-<i>b </i>of <figref idref="DRAWINGS">FIG. <b>4</b></figref>), according to detection operation <b>534</b>. In some cases, both detection strategies employ a similar approach, using a pyramid of N levels and a default or current detector threshold value. As described in more detail in reference to detection strategy I <b>432</b>, the raw image <b>502</b> may be downscaled by pyramid construction <b>550</b> into m downscaled images <b>552</b><i>a</i>-<i>m</i>, where m is a nonzero integer equivalent to the pyramid level value associated with operation <b>534</b>. In some cases, operation <b>534</b> includes detection of keypoints for the raw image <b>502</b> according to original-T detection <b>554</b> and each of the downscaled images according to original-T detection <b>554</b><i>a</i>-<i>m</i>, following which the detected keypoints are combined by keypoint fusion and selection <b>556</b>, as previously described in reference to detection strategy I <b>432</b>. In some cases, operation <b>534</b> generates the set of detected keypoints <b>312</b>.</p><p id="p-0071" num="0070"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a simplified flowchart illustrating methods of adaptive feature detection using a vSLAM unit according to at least one embodiment of the present disclosure. The flow is described in connection with a computer system that is an example of the computer systems described herein above. Some or all of the operations of the flows can be implemented via specific hardware on the computer system and/or can be implemented as computer-readable instructions stored on a non-transitory computer-readable medium of the computer system. As stored, the computer-readable instructions represent programmable modules that include code executable by a processor of the computer system. The execution of such instructions configures the computer system to perform the respective operations. Each programmable module in combination with the processor represents a means for performing a respective operation (s). While the operations are illustrated in a particular order, it should be understood that no particular order is necessary and that one or more operations may be omitted, skipped, and/or reordered.</p><p id="p-0072" num="0071">The method includes receiving a first image (<b>602</b>). As described in more detail in reference to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the first image may form a part of a set of images received by the vSLAM unit (e.g., vSLAM unit <b>116</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>) from an optical sensor (e.g., RGB optical sensor <b>114</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>). Optionally, the first image is received from a camera in communication with the vSLAM unit. In some cases, the camera may produce images at a raw or native pixel-resolution.</p><p id="p-0073" num="0072">The method further includes receiving a motion dataset (<b>604</b>). As described in more detail in reference to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the computer system (e.g., computer system <b>110</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>) may include an IMU. The IMU may measure motion in six degrees of freedom, and provide motion data to the vSLAM unit. In some cases, the vSLAM unit processes the motion data to determine a displacement value, equivalent to a translational movement in a period of time.</p><p id="p-0074" num="0073">The method further includes determining an initialization state (<b>606</b>). Optionally, determining an initialization state includes receiving one or more initialization parameters from an initializer in communication with the computer system, determining an initialization quality value, based at least in part on the one or more initialization parameters, and comparing the displacement value to a threshold criterion. In accordance with the initialization quality value satisfying the threshold criterion, the method can include determining that the initialization state is true. Alternatively, in accordance with the initialization quality value not satisfying the threshold criterion, the method can include determining that the initialization state is false. As described in more detail in reference to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the initialization may provide an initial output pose and an initial coordinate mapping for the vSLAM unit by which the vSLAM unit detects and tracks features in an image of a set of images.</p><p id="p-0075" num="0074">The method further includes determining a motion level (<b>608</b>). In an embodiment, determining a motion level includes receiving the motion dataset from an inertial measurement unit in communication with the computer system and determining a displacement value by a motion monitor in communication with the computer system based at least in part on the motion dataset. In this embodiment, the method also includes comparing the displacement value to a threshold criterion and, in accordance with the displacement value, satisfying the threshold criterion, determining that the motion level is true. Alternatively, in accordance with the displacement value not satisfying the threshold criterion, the method can include determining that the motion level is false. In some cases, the motion level is determined based on displacement and/or, as described in more detail in reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the motion level may reflect acceleration of the computer system.</p><p id="p-0076" num="0075">The method further includes determining a tracking level (<b>610</b>). In a specific embodiment, determining a tracking level includes receiving a set of keypoints and tracking the set of keypoints in the first image. In this specific embodiment, the method also includes selecting a set of inliers from the set of keypoints tracked in the first image, determining an error value from the set of inliers, and comparing the error value to an error threshold. If the error value satisfies the error threshold the tracking level is determined to be true. If the error value does not satisfy the error threshold, the tracking level is determined to be false. As described in more detail in reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the tracking level may reflect the integrated error based at least in part on inliers tracked in a set of detected keypoints.</p><p id="p-0077" num="0076">The method further includes, in accordance with a determination that the initialization state is true and the motion level is true, or that the initialization state is false (i.e., a first condition), generating a first image pyramid, detecting a plurality of features in the first image pyramid using a first detector threshold, and generating a set of detected keypoints at least in part by keypoint fusion and selection (<b>612</b>). Optionally, generating the first image pyramid includes generating N downscaled images from the first image, each subsequent image after the first image having a lower average pixel-resolution than the image preceding it in the image pyramid, wherein N is a pyramid level value corresponding to a nonzero integer. Optionally, the first detector threshold is determined at least in part according to a detector threshold used for initializing the vSLAM unit.</p><p id="p-0078" num="0077">The method further includes, in accordance with a determination that the initialization state is true, the motion level is false, and the tracking level is false (i.e., a second condition), generating a second image pyramid, detecting the plurality of features in the second image pyramid using a second detector threshold, the second detector threshold being less restrictive than the first detector threshold, and generating a second set of detected keypoints at least in part by keypoint fusion and selection (<b>614</b>).</p><p id="p-0079" num="0078">The method further includes, in accordance with a determination that the initialization state is true, the motion level is false, and the tracking level is true (i.e., a third condition), detecting the plurality of features in the first image according to the first detector threshold; and generating a third set of detected keypoints (<b>616</b>).</p><p id="p-0080" num="0079">In a particular embodiment, the method further includes receiving a second image, performing feature tracking on the second image at least in part according to the set of detected keypoints, determining a tracking quality based at least in part on a plurality of tracked feature points in the second image, and, in accordance with a determination that the tracking quality is false, generating updated keypoints from the second image; and replacing the set of detected keypoints with the updated keypoints.</p><p id="p-0081" num="0080">It should be appreciated that the specific steps illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref> provide a particular method of detecting features in an image according to an embodiment of the present disclosure. Other sequences of steps may also be performed according to alternative embodiments. For example, alternative embodiments of the present disclosure may perform the steps outlined above in a different order. Moreover, the individual steps illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref> may include multiple sub-steps that may be performed in various sequences as appropriate to the individual step. Furthermore, additional steps may be added or removed depending on the particular applications. One of ordinary skill in the art would recognize many variations, modifications, and alternatives.</p><p id="p-0082" num="0081"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates examples of components of a computer system <b>700</b> according to certain embodiments. The computer system <b>700</b> is an example of the computer system described herein above. Although these components are illustrated as belonging to a same computer system <b>700</b>, the computer system <b>700</b> can also be distributed.</p><p id="p-0083" num="0082">The computer system <b>700</b> includes at least a processor <b>702</b>, a memory <b>704</b>, a storage device <b>706</b>, input/output peripherals (I/O) <b>708</b>, communication peripherals <b>710</b>, and an interface bus <b>712</b>. The interface bus <b>712</b> is configured to communicate, transmit, and transfer data, controls, and commands among the various components of the computer system <b>700</b>. The memory <b>704</b> and the storage device <b>706</b> include computer-readable storage media, such as RAM, ROM, electrically erasable programmable read-only memory (EEPROM), hard drives, CD-ROMs, optical storage devices, magnetic storage devices, electronic non-volatile computer storage, for example memory, and other tangible storage media. Any of such computer readable storage media can be configured to store instructions or program codes embodying aspects of the disclosure. The memory <b>704</b> and the storage device <b>706</b> also include computer readable signal media. A computer readable signal medium includes a propagated data signal with computer readable program code embodied therein. Such a propagated signal takes any of a variety of forms including, but not limited to, electromagnetic, optical, or any combination thereof. A computer readable signal medium includes any computer readable medium that is not a computer readable storage medium and that can communicate, propagate, or transport a program for use in connection with the computer system <b>700</b>.</p><p id="p-0084" num="0083">Further, the memory <b>704</b> includes an operating system, programs, and applications. The processor <b>702</b> is configured to execute the stored instructions and includes, for example, a logical processing unit, a microprocessor, a digital signal processor, and other processors. The memory <b>704</b> and/or the processor <b>702</b> can be virtualized and can be hosted within another computer system of, for example, a cloud network or a data center. The I/O peripherals <b>708</b> include user interfaces, such as a keyboard, screen (e.g., a touch screen), microphone, speaker, other input/output devices, and computing components, such as graphical processing units, serial ports, parallel ports, universal serial buses, and other input/output peripherals. The I/O peripherals <b>708</b> are connected to the processor <b>702</b> through any of the ports coupled to the interface bus <b>712</b>. The communication peripherals <b>710</b> are configured to facilitate communication between the computer system <b>700</b> and other computing devices over a communications network and include, for example, a network interface controller, modem, wireless and wired interface cards, antenna, and other communication peripherals.</p><p id="p-0085" num="0084">While the present subject matter has been described in detail with respect to specific embodiments thereof, it will be appreciated that those skilled in the art, upon attaining an understanding of the foregoing may readily produce alterations to, variations of, and equivalents to such embodiments. Accordingly, it should be understood that the present disclosure has been presented for purposes of example rather than limitation, and does not preclude inclusion of such modifications, variations, and/or additions to the present subject matter as would be readily apparent to one of ordinary skill in the art. Indeed, the methods and systems described herein may be embodied in a variety of other forms; furthermore, various omissions, substitutions and changes in the form of the methods and systems described herein may be made without departing from the spirit of the present disclosure. The accompanying claims and their equivalents are intended to cover such forms or modifications as would fall within the scope and spirit of the present disclosure.</p><p id="p-0086" num="0085">Unless specifically stated otherwise, it is appreciated that throughout this specification discussions utilizing terms such as &#x201c;processing,&#x201d; &#x201c;computing,&#x201d; &#x201c;calculating,&#x201d; &#x201c;determining,&#x201d; and &#x201c;identifying&#x201d; or the like refer to actions or processes of a computing device, such as one or more computers or a similar electronic computing device or devices, that manipulate or transform data represented as physical electronic or magnetic quantities within memories, registers, or other information storage devices, transmission devices, or display devices of the computing platform.</p><p id="p-0087" num="0086">The system or systems discussed herein are not limited to any particular hardware architecture or configuration. A computing device can include any suitable arrangement of components that provide a result conditioned on one or more inputs. Suitable computing devices include multipurpose microprocessor-based computer systems accessing stored software that programs or configures the computer system from a general-purpose computing apparatus to a specialized computing apparatus implementing one or more embodiments of the present subject matter. Any suitable programming, scripting, or other type of language or combinations of languages may be used to implement the teachings contained herein in software to be used in programming or configuring a computing device.</p><p id="p-0088" num="0087">Embodiments of the methods disclosed herein may be performed in the operation of such computing devices. The order of the blocks presented in the examples above can be varied-for example, blocks can be re-ordered, combined, and/or broken into sub-blocks. Certain blocks or processes can be performed in parallel.</p><p id="p-0089" num="0088">Conditional language used herein, such as, among others, &#x201c;can,&#x201d; &#x201c;could,&#x201d; &#x201c;might,&#x201d; &#x201c;may,&#x201d; &#x201c;e.g.,&#x201d; and the like, unless specifically stated otherwise, or otherwise understood within the context as used, is generally intended to convey that certain examples include, while other examples do not include, certain features, elements, and/or steps. Thus, such conditional language is not generally intended to imply that features, elements and/or steps are in any way required for one or more examples or that one or more examples necessarily include logic for deciding, with or without author input or prompting, whether these features, elements and/or steps are included or are to be performed in any particular example.</p><p id="p-0090" num="0089">The terms &#x201c;including,&#x201d; &#x201c;having,&#x201d; and the like are synonymous and are used inclusively, in an open-ended fashion, and do not exclude additional elements, features, acts, operations, and so forth. Also, the term &#x201c;or&#x201d; is used in its inclusive sense (and not in its exclusive sense) so that when used, for example, to connect a list of elements, the term &#x201c;or&#x201d; means one, some, or all of the elements in the list. The use of &#x201c;adapted to&#x201d; or &#x201c;configured to&#x201d; herein is meant as open and inclusive language that does not foreclose devices adapted to or configured to perform additional tasks or steps. Additionally, the use of &#x201c;based on&#x201d; is meant to be open and inclusive, in that a process, step, calculation, or other action &#x201c;based on&#x201d; one or more recited conditions or values may, in practice, be based on additional conditions or values beyond those recited. Similarly, the use of &#x201c;based at least in part on&#x201d; is meant to be open and inclusive, in that a process, step, calculation, or other action &#x201c;based at least in part on&#x201d; one or more recited conditions or values may, in practice, be based on additional conditions or values beyond those recited. Headings, lists, and numbering included herein are for ease of explanation only and are not meant to be limiting.</p><p id="p-0091" num="0090">The various features and processes described above may be used independently of one another, or may be combined in various ways. All possible combinations and sub-combinations are intended to fall within the scope of the present disclosure. In addition, certain method or process blocks may be omitted in some implementations. The methods and processes described herein are also not limited to any particular sequence, and the blocks or states relating thereto can be performed in other sequences that are appropriate. For example, described blocks or states may be performed in an order other than that specifically disclosed, or multiple blocks or states may be combined in a single block or state. The example blocks or states may be performed in serial, in parallel, or in some other manner. Blocks or states may be added to or removed from the disclosed examples. Similarly, the example systems and components described herein may be configured differently than described. For example, elements may be added to, removed from, or rearranged compared to the disclosed examples.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method implemented by a computer system, the method comprising:<claim-text>receiving a first image by a visual simultaneous localization and mapping (vSLAM) unit, the first image being generated by an optical sensor in communication with the computer system;</claim-text><claim-text>receiving a motion dataset generated by an inertial measurement unit in communication with the vSLAM unit;</claim-text><claim-text>determining a motion level by the vSLAM unit using a motion monitor;</claim-text><claim-text>determining an initialization state by the vSLAM unit using an initializer;</claim-text><claim-text>determining a tracking level by the vSLAM unit using a tracking performance monitor; and</claim-text><claim-text>in a first condition, using a detection strategy processor of the vSLAM unit:<claim-text>generating a first image pyramid;</claim-text><claim-text>detecting a plurality of features in the first image pyramid using a first detector threshold; and</claim-text><claim-text>generating a first set of detected keypoints from the plurality of features at least in part by keypoint fusion and selection;</claim-text></claim-text><claim-text>in a second condition, using a detection strategy processor of the vSLAM unit:<claim-text>generating a second image pyramid;</claim-text><claim-text>detecting the plurality of features in the second image pyramid using a second detector threshold, the second detector threshold being less restrictive than the first detector threshold; and</claim-text><claim-text>generating a second set of detected keypoints at least in part by keypoint fusion and selection; and</claim-text></claim-text><claim-text>in a third condition, using a detection strategy processor of the vSLAM unit:<claim-text>detecting the plurality of features in the first image according to the first detector threshold; and</claim-text><claim-text>generating a third set of detected keypoints.</claim-text></claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>the first condition is a determination that the initialization state is true and the motion level is true or the initialization state is false;</claim-text><claim-text>the second condition is a determination that the initialization state is true, the motion level is false, and the tracking level is false; and</claim-text><claim-text>the third condition is a determination that the initialization state is true, the motion level is false, and the tracking level is true.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>receiving a second image;</claim-text><claim-text>performing feature tracking on the second image at least in part according to the first set of detected keypoints, the second set of detected keypoints, or the third set of detected keypoints;</claim-text><claim-text>determining a tracking quality; and</claim-text><claim-text>in accordance with a determination that the tracking quality is false, generating updated keypoints from the second image.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein determining an initialization state comprises:<claim-text>receiving one or more initialization parameters from an initializer in communication with the computer system;</claim-text><claim-text>determining an initialization quality value, based at least in part on the one or more initialization parameters;</claim-text><claim-text>comparing the initialization quality value to a threshold criterion; and</claim-text><claim-text>in accordance with the initialization quality value satisfying the threshold criterion, determining that the initialization state is true; or</claim-text><claim-text>in accordance with the initialization quality value not satisfying the threshold criterion, determining that the initialization state is false.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein determining a motion level comprises:<claim-text>receiving the motion dataset from an inertial measurement unit in communication with the computer system;</claim-text><claim-text>determining a displacement value by a motion monitor in communication with the computer system based at least in part on the motion dataset;</claim-text><claim-text>comparing the displacement value to a threshold criterion; and</claim-text><claim-text>in accordance with the displacement value satisfying the threshold criterion, determining that the motion level is true; or</claim-text><claim-text>in accordance with the displacement value not satisfying the threshold criterion, determining that the motion level is false.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein determining a tracking level comprises:<claim-text>receiving a set of keypoints;</claim-text><claim-text>tracking the set of keypoints in the first image;</claim-text><claim-text>selecting a set of inliers from the set of keypoints tracked in the first image;</claim-text><claim-text>determining an error value from the set of inliers;</claim-text><claim-text>comparing the error value to an error threshold; and</claim-text><claim-text>in accordance with the error value satisfying the error threshold, determining that the tracking level is true; or</claim-text><claim-text>in accordance with the error value not satisfying the error threshold, determining that the tracking level is false.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein generating the first image pyramid comprises generating N downscaled images from the first image, each subsequent image after the first image having a lower average pixel-resolution than an image preceding it in the first image pyramid, wherein N is a pyramid level value corresponding to a nonzero integer.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first detector threshold is determined at least in part according to a detector threshold used for initializing a vSLAM unit.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first image is received from a camera in communication with a vSLAM unit.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. A computer system, comprising:<claim-text>one or more processors; and</claim-text><claim-text>one or more memories storing computer-readable instructions that, upon execution by the one or more processors, configure the computer system to:<claim-text>receive a first image by a visual simultaneous localization and mapping (vSLAM) unit, the first image being generated by an optical sensor in communication with the computer system;</claim-text><claim-text>receive a motion dataset generated by an inertial measurement unit in communication with the vSLAM unit;</claim-text><claim-text>determine a motion level by the vSLAM unit using a motion monitor;</claim-text><claim-text>determine an initialization state by the vSLAM unit using an initializer;</claim-text><claim-text>determine a tracking level by the vSLAM unit using a tracking performance monitor; and</claim-text><claim-text>in a first condition, using a detection strategy processor of the vSLAM unit:<claim-text>generate a first image pyramid;</claim-text><claim-text>detect a plurality of features in the first image pyramid using a first detector threshold; and</claim-text><claim-text>generate a first set of detected keypoints from the plurality of features at least in part by keypoint fusion and selection;</claim-text></claim-text><claim-text>in a second condition, using a detection strategy processor of the vSLAM unit:</claim-text></claim-text><claim-text>generate a second image pyramid;<claim-text>detect the plurality of features in the second image pyramid using a second detector threshold, the second detector threshold being less restrictive than the first detector threshold; and</claim-text><claim-text>generate a second set of detected keypoints at least in part by keypoint fusion and selection; and</claim-text><claim-text>in a third condition, using a detection strategy processor of the vSLAM unit:</claim-text><claim-text>detect the plurality of features in the first image according to the first detector threshold; and</claim-text><claim-text>generate a third set of detected keypoints.</claim-text></claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The computer system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein:<claim-text>the first condition is a determination that the initialization state is true and the motion level is true or the initialization state is false;</claim-text><claim-text>the second condition is a determination that the initialization state is true, the motion level is false, and the tracking level is false; and</claim-text><claim-text>the third condition is a determination that the initialization state is true, the motion level is false, and the tracking level is true.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The computer system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the computer-readable instructions further configure the computer system to:<claim-text>receive a second image;</claim-text><claim-text>perform feature tracking on the second image at least in part according to the first set of detected keypoints, the second set of detected keypoints, or the third set of detected keypoints;</claim-text><claim-text>determine a tracking quality; and</claim-text><claim-text>in accordance with a determination that the tracking quality is false, generate updated keypoints from the second image.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The computer system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein determining an initialization state comprises:<claim-text>receiving one or more initialization parameters from an initializer in communication with the computer system;</claim-text><claim-text>determining an initialization quality value, based at least in part on the one or more initialization parameters;</claim-text><claim-text>comparing the initialization quality value to a threshold criterion; and</claim-text><claim-text>in accordance with the initialization quality value satisfying the threshold criterion, determining that the initialization state is true; or</claim-text><claim-text>in accordance with the initialization quality value not satisfying the threshold criterion, determining that the initialization state is false.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The computer system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein determining a motion level comprises:<claim-text>receiving a motion dataset from an inertial measurement unit in communication with the computer system;</claim-text><claim-text>determining a displacement value by a motion monitor in communication with the computer system based at least in part on the motion dataset;</claim-text><claim-text>comparing the displacement value to a threshold criterion; and</claim-text><claim-text>in accordance with the displacement value satisfying the threshold criterion, determining that the motion level is true; or</claim-text><claim-text>in accordance with the displacement value not satisfying the threshold criterion, determining that the motion level is false.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The computer system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein determining a tracking level comprises:<claim-text>receiving a set of keypoints;</claim-text><claim-text>tracking the set of keypoints in the first image;</claim-text><claim-text>selecting a set of inliers from the set of keypoints tracked in the first image;</claim-text><claim-text>determining an error value from the set of inliers;</claim-text><claim-text>comparing the error value to an error threshold; and</claim-text><claim-text>in accordance with the error value satisfying the error threshold, determining that the tracking level is true; or</claim-text><claim-text>in accordance with the error value not satisfying the error threshold, determining that the tracking level is false.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The computer system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein generating the first image pyramid comprises generating N downscaled images from the first image, each subsequent image after the first image having a lower average pixel-resolution than an image preceding it in the first image pyramid, wherein N is a pyramid level value corresponding to a nonzero integer.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. One or more non-transitory computer-storage media storing instructions that, upon execution on a computer system, cause the computer system to perform operations comprising:<claim-text>receiving a first image by a visual simultaneous localization and mapping (vSLAM) unit, the first image being generated by an optical sensor in communication with the computer system;</claim-text><claim-text>receiving a motion dataset generated by an inertial measurement unit in communication with the vSLAM unit;</claim-text><claim-text>determining a motion level by the vSLAM unit using a motion monitor;</claim-text><claim-text>determining an initialization state by the vSLAM unit using an initializer;</claim-text><claim-text>determining a tracking level by the vSLAM unit using a tracking performance monitor; and</claim-text><claim-text>in a first condition, using a detection strategy processor of the vSLAM unit:<claim-text>generating a first image pyramid;</claim-text><claim-text>detecting a plurality of features in the first image pyramid using a first detector threshold; and</claim-text><claim-text>generating a first set of detected keypoints from the plurality of features at least in part by keypoint fusion and selection;</claim-text></claim-text><claim-text>in a second condition, using a detection strategy processor of the vSLAM unit:<claim-text>generating a second image pyramid;</claim-text><claim-text>detecting the plurality of features in the second image pyramid using a second detector threshold, the second detector threshold being less restrictive than the first detector threshold; and</claim-text><claim-text>generating a second set of detected keypoints at least in part by keypoint fusion and selection; and</claim-text></claim-text><claim-text>in a third condition, using a detection strategy processor of the vSLAM unit:<claim-text>detecting the plurality of features in the first image according to the first detector threshold; and</claim-text><claim-text>generating a third set of detected keypoints,</claim-text></claim-text><claim-text>wherein:</claim-text><claim-text>the first condition is a determination that the initialization state is true and the motion level is true or the initialization state is false;</claim-text><claim-text>the second condition is a determination that the initialization state is true, the motion level is false, and the tracking level is false; and</claim-text><claim-text>the third condition is a determination that the initialization state is true, the motion level is false, and the tracking level is true.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The one or more non-transitory computer-storage media of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein determining an initialization state comprises:<claim-text>receiving one or more initialization parameters from an initializer in communication with the computer system;</claim-text><claim-text>determining an initialization quality value, based at least in part on the one or more initialization parameters;</claim-text><claim-text>comparing the initialization quality value to a threshold criterion; and</claim-text><claim-text>in accordance with the initialization quality value satisfying the threshold criterion, determining that the initialization state is true; or</claim-text><claim-text>in accordance with the initialization quality value not satisfying the threshold criterion, determining that the initialization state is false.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The one or more non-transitory computer-storage media of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein determining a motion level comprises:<claim-text>receiving a motion dataset from an inertial measurement unit in communication with the computer system;</claim-text><claim-text>determining a displacement value by a motion monitor in communication with the computer system based at least in part on the motion dataset;</claim-text><claim-text>comparing the displacement value to a threshold criterion; and</claim-text><claim-text>in accordance with the displacement value satisfying the threshold criterion, determining that the motion level is true; or</claim-text><claim-text>in accordance with the displacement value not satisfying the threshold criterion, determining that the motion level is false.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The one or more non-transitory computer-storage media of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein determining a tracking level comprises:<claim-text>receiving a set of keypoints;</claim-text><claim-text>tracking the set of keypoints in the first image;</claim-text><claim-text>selecting a set of inliers from the set of keypoints tracked in the first image;</claim-text><claim-text>determining an error value from the set of inliers;</claim-text><claim-text>comparing the error value to an error threshold; and</claim-text><claim-text>in accordance with the error value satisfying the error threshold, determining that the tracking level is true; or</claim-text><claim-text>in accordance with the error value not satisfying the error threshold, determining that the tracking level is false.</claim-text></claim-text></claim></claims></us-patent-application>