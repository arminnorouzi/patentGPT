<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230007317A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230007317</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17944440</doc-number><date>20220914</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20110101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>21</main-group><subgroup>2343</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20140101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>46</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20140101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>70</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20180101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>30</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20220101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>82</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20220101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>764</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>02</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>13</main-group><subgroup>38</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20220101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>52</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20140101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>50</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>21</main-group><subgroup>2343</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20141101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>46</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20141101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>70</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>30036</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>049</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>82</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>764</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>02</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>13</main-group><subgroup>382</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>52</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20141101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>50</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>2213</main-group><subgroup>0042</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>2213</main-group><subgroup>0026</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">Surveillance Camera Upgrade via Removable Media having Deep Learning Accelerator and Random Access Memory</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16906253</doc-number><date>20200619</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11490135</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17944440</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Micron Technology, Inc.</orgname><address><city>Boise</city><state>ID</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Kale</last-name><first-name>Poorna</first-name><address><city>Folsom</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Lin</last-name><first-name>Te-Chang</first-name><address><city>New Taipei City</city><country>TW</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Systems, devices, and methods related to a deep learning accelerator and memory are described. For example, a removable media (e.g., a memory card, or a USB drive) may be configured to execute instructions with matrix operands and configured with: an interface to receive a video stream; and random access memory to buffer a portion of the video stream as an input to an artificial neural network and to store instructions executable by the deep learning accelerator and matrices of the artificial neural network. Such a removable media can be used to replace an existing removable media used in a surveillance camera to record video or images. The deep learning accelerator can execute the instructions to generate analytics of the buffer portion using the artificial neural network, enabling the surveillance camera that is upgraded via the use of the removable media to provide intelligent services based on the analytics.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="90.59mm" wi="158.75mm" file="US20230007317A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="177.55mm" wi="143.59mm" file="US20230007317A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="152.15mm" wi="111.76mm" file="US20230007317A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="152.15mm" wi="111.76mm" file="US20230007317A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="164.85mm" wi="111.76mm" file="US20230007317A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="209.30mm" wi="123.70mm" file="US20230007317A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="120.40mm" wi="172.13mm" file="US20230007317A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="222.00mm" wi="165.78mm" file="US20230007317A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="193.38mm" wi="143.59mm" file="US20230007317A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="202.95mm" wi="146.73mm" file="US20230007317A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="209.30mm" wi="137.24mm" file="US20230007317A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">RELATED APPLICATIONS</heading><p id="p-0002" num="0001">The present application is a continuation application of U.S. patent application Ser. No. 16/906,253 filed Jun. 19, 2020, the entire disclosures of which application are hereby incorporated herein by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">At least some embodiments disclosed herein relate to surveillance camera in general and more particularly, but not limited to, intelligent surveillance camera powered by artificial neural networks (ANNs), such as ANNs configured through machine learning and/or deep learning.</p><p id="p-0004" num="0003">BACKGROUND</p><p id="p-0005" num="0004">An artificial neural network (ANN) uses a network of neurons to process inputs to the network and to generate outputs from the network.</p><p id="p-0006" num="0005">For example, each neuron in the network receives a set of inputs. Some of the inputs to a neuron may be the outputs of certain neurons in the network; and some of the inputs to a neuron may be the inputs provided to the neural network. The input/output relations among the neurons in the network represent the neuron connectivity in the network.</p><p id="p-0007" num="0006">For example, each neuron can have a bias, an activation function, and a set of synaptic weights for its inputs respectively. The activation function may be in the form of a step function, a linear function, a log-sigmoid function, etc. Different neurons in the network may have different activation functions.</p><p id="p-0008" num="0007">For example, each neuron can generate a weighted sum of its inputs and its bias and then produce an output that is the function of the weighted sum, computed using the activation function of the neuron.</p><p id="p-0009" num="0008">The relations between the input(s) and the output(s) of an ANN in general are defined by an ANN model that includes the data representing the connectivity of the neurons in the network, as well as the bias, activation function, and synaptic weights of each neuron. Based on a given ANN model, a computing device can be configured to compute the output(s) of the network from a given set of inputs to the network.</p><p id="p-0010" num="0009">For example, the inputs to an ANN network may be generated based on camera inputs; and the outputs from the ANN network may be the identification of an item, such as an event or an object.</p><p id="p-0011" num="0010">In general, an ANN may be trained using a supervised method where the parameters in the ANN are adjusted to minimize or reduce the error between known outputs associated with or resulted from respective inputs and computed outputs generated via applying the inputs to the ANN. Examples of supervised learning/training methods include reinforcement learning and learning with error correction.</p><p id="p-0012" num="0011">Alternatively, or in combination, an ANN may be trained using an unsupervised method where the exact outputs resulted from a given set of inputs is not known before the completion of the training. The ANN can be trained to classify an item into a plurality of categories, or data points into clusters.</p><p id="p-0013" num="0012">Multiple training algorithms can be employed for a sophisticated machine learning/training paradigm.</p><p id="p-0014" num="0013">Deep learning uses multiple layers of machine learning to progressively extract features from input data. For example, lower layers can be configured to identify edges in an image; and higher layers can be configured to identify, based on the edges detected using the lower layers, items captured in the image, such as faces, objects, events, etc. Deep learning can be implemented via artificial neural networks (ANNs), such as deep neural networks, deep belief networks, recurrent neural networks, and/or convolutional neural networks.</p><p id="p-0015" num="0014">Deep learning has been applied to many application fields, such as computer vision, speech/audio recognition, natural language processing, machine translation, bioinformatics, drug design, medical image processing, games, etc.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0016" num="0015">The embodiments are illustrated by way of example and not limitation in the figures of the accompanying drawings in which like references indicate similar elements.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows an integrated circuit device having a deep learning accelerator and random access memory configured according to one embodiment.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows a processing unit configured to perform matrix-matrix operations according to one embodiment.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>3</b></figref> shows a processing unit configured to perform matrix-vector operations according to one embodiment.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows a processing unit configured to perform vector-vector operations according to one embodiment.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>5</b></figref> shows a deep learning accelerator and random access memory configured to autonomously apply inputs to a trained artificial neural network according to one embodiment.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates a technique to upgrade a surveillance camera using a removable media according to one embodiment.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIGS. <b>7</b></figref> &#x2014; <b>9</b> illustrate removable media having a deep learning accelerator and random access memory configured according to some embodiments.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>10</b></figref> shows a method implemented in a surveillance camera according to one embodiment.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0004" level="1">DETAILED DESCRIPTION</heading><p id="p-0025" num="0024">At least some embodiments disclosed herein provide a removable media that has a general-purpose integrated circuit configured to perform computations of artificial neural networks (ANNs) with reduced energy consumption and computation time. The integrated circuit includes a deep learning accelerator (DLA) and random access memory. Such a removable media can be inserted into a surveillance camera as a replacement storage device. The insertion of the removable media into the surveillance camera further upgrades the surveillance camera to include the capabilities of generating video analytics using the integrated circuit according to an artificial neural network (ANN).</p><p id="p-0026" num="0025">For example, input video data stored into the removable media can be analyzed by an artificial neural network to generate analytics and/or inference results.</p><p id="p-0027" num="0026">The analytics and/or inference results can be used in video compression performed by a video encoder. The compressed video having a size smaller than the input video data can be stored in the removable media as a replacement of the input video data and can be retrieved to playback the video for a presentation that is substantially the same was the playback of the input video data.</p><p id="p-0028" num="0027">Alternatively, or in combination, the analytics and/or inference results can be used to generate intelligent outputs that are less voluminous than the video data. For example, the artificial neural network (ANN) can be trained to output recognized events, patterns, features, or classifications that are of interest in a particular surveillance application. The outputs can be stored in the removable media, used to generate alerts, transmitted from the surveillance camera to a separate computing system, and/or used to selectively retain and/or transmit video data generated in the surveillance camera.</p><p id="p-0029" num="0028">The deep learning accelerator (DLA) includes a set of general-purpose, programmable hardware computing logic that is specialized and/or optimized to perform parallel vector and/or matrix calculations, including but not limited to multiplication and accumulation of vectors and/or matrices.</p><p id="p-0030" num="0029">Further, the deep learning accelerator (DLA) can include one or more arithmetic-logic units (ALUs) to perform arithmetic and bitwise operations on integer binary numbers.</p><p id="p-0031" num="0030">The deep learning accelerator (DLA) is programmable via a set of instructions to perform the computations of an artificial neural network (ANN).</p><p id="p-0032" num="0031">The granularity of the deep learning accelerator (DLA) operating on vectors and matrices corresponds to the largest unit of vectors/matrices that can be operated upon during the execution of one instruction by the deep learning accelerator (DLA). During the execution of the instruction for a predefined operation on vector/matrix operands, elements of vector/matrix operands can be operated upon by the deep learning accelerator (DLA) in parallel to reduce execution time and/or energy consumption associated with memory/data access. The operations on vector/matrix operands of the granularity of the deep learning accelerator (DLA) can be used as building blocks to implement computations on vectors/matrices of larger sizes.</p><p id="p-0033" num="0032">The implementation of a typical/practical artificial neural network (ANN) involves vector/matrix operands having sizes that are larger than the operation granularity of the deep learning accelerator (DLA). To implement such an artificial neural network (ANN) using the deep learning accelerator (DLA), computations involving the vector/matrix operands of large sizes can be broken down to the computations of vector/matrix operands of the granularity of the deep learning accelerator (DLA). The deep learning accelerator (DLA) can be programmed via instructions to carry out the computations involving large vector/matrix operands. For example, atomic computation capabilities of the deep learning accelerator (DLA) in manipulating vectors and matrices of the granularity of the deep learning accelerator (DLA) in response to instructions can be programmed to implement computations in an artificial neural network (ANN).</p><p id="p-0034" num="0033">In some implementations, the deep learning accelerator (DLA) lacks some of the logic operation capabilities of a typical central processing unit (CPU). However, the deep learning accelerator (DLA) can be configured with sufficient logic units to process the input data provided to an artificial neural network (ANN) and generate the output of the artificial neural network (ANN) according to a set of instructions generated for the deep learning accelerator (DLA). Thus, the deep learning accelerator (DLA) can perform the computation of an artificial neural network (ANN) with little or no help from a central processing unit (CPU) or another processor. Optionally, a conventional general purpose processor can also be configured as part of the deep learning accelerator (DLA) to perform operations that cannot be implemented efficiently using the vector/matrix processing units of the deep learning accelerator (DLA), and/or that cannot be performed by the vector/matrix processing units of the deep learning accelerator (DLA).</p><p id="p-0035" num="0034">A typical artificial neural network (ANN) can be described/specified in a standard format (e.g., open neural network exchange (ONNX)). A compiler can be used to convert the description of the artificial neural network (ANN) into a set of instructions for the deep learning accelerator (DLA) to perform calculations of the artificial neural network (ANN). The compiler can optimize the set of instructions to improve the performance of the deep learning accelerator (DLA) in implementing the artificial neural network (ANN).</p><p id="p-0036" num="0035">The deep learning accelerator (DLA) can have local memory, such as registers, buffers and/or caches, configured to store vector/matrix operands and the results of vector/matrix operations. Intermediate results in the registers can be pipelined/shifted in the deep learning accelerator (DLA) as operands for subsequent vector/matrix operations to reduce time and energy consumption in accessing memory/data and thus speed up typical patterns of vector/matrix operations in implementing a typical artificial neural network (ANN). The capacity of registers, buffers and/or caches in the deep learning accelerator (DLA) is typically insufficient to hold the entire data set for implementing the computation of a typical artificial neural network (ANN). Thus, a random access memory coupled to the deep learning accelerator (DLA) is configured to provide an improved data storage capability for implementing a typical artificial neural network (ANN). For example, the deep learning accelerator (DLA) loads data and instructions from the random access memory and stores results back into the random access memory.</p><p id="p-0037" num="0036">The communication bandwidth between the deep learning accelerator (DLA) and the random access memory is configured to optimize or maximize the utilization of the computation power of the deep learning accelerator (DLA). For example, high communication bandwidth can be provided between the deep learning accelerator (DLA) and the random access memory such that vector/matrix operands can be loaded from the random access memory into the deep learning accelerator (DLA) and results stored back into the random access memory in a time period that is approximately equal to the time for the deep learning accelerator (DLA) to perform the computations on the vector/matrix operands. The granularity of the deep learning accelerator (DLA) can be configured to increase the ratio between the amount of computations performed by the deep learning accelerator (DLA) and the size of the vector/matrix operands such that the data access traffic between the deep learning accelerator (DLA) and the random access memory can be reduced, which can reduce the requirement on the communication bandwidth between the deep learning accelerator (DLA) and the random access memory. Thus, the bottleneck in data/memory access can be reduced or eliminated.</p><p id="p-0038" num="0037">Some video coding techniques are based on the analysis of a video using a deep learning technique. Deep learning-based video coding techniques can include coding schemes that are built primarily on deep neural networks (DNNs) and/or coding tools that use deep neural networks (DNNs) to generate models, predictions, parameters, and/or options for use in traditional coding schemes and/or with traditional coding tools.</p><p id="p-0039" num="0038">For example, a pixel probability model can be computed/estimated using a deep neural network (DNN) to drive a deep learning-based coding scheme. For example, a deep neural network (DNN) can be used to perform or facilitate intra-picture prediction, inter-picture prediction, cross-channel prediction, probability distribution prediction, transform, post-loop filtering, in-loop filtering, down-sampling, up-sampling, encoding optimization, etc. The predictions/optimization can be used with traditional coding schemes and/or traditional coding tools.</p><p id="p-0040" num="0039">At least one embodiment disclosed herein includes a surveillance camera that is configured with a deep learning accelerator (DLA) and random access memory to facilitate intelligent video/image processing using artificial neural networks (ANNs). The deep learning accelerator (DLA) can be provided via a removable media.</p><p id="p-0041" num="0040">For example, an existing surveillance camera may not have a deep learning accelerator (DLA) but has a slot configured to receive a removable media to record video generated by the surveillance camera. Such a slot can be configured as part of a memory card reader of the surveillance camera; and such a removable media can be in the form of a memory card. The memory card can have a form factor and interface of secure digital (SD) cards, microsd cards, compactflash (CF) cards, memory stick cards, etc.</p><p id="p-0042" num="0041">A replacement removable media can be configured with a deep learning accelerator (DLA) and random access memory to perform computations of artificial neural networks (ANNs) autonomously using models of artificial neural networks (ANNs) stored in the removable media. After the replacement removable media is inserted into the slot in the reader of the surveillance camera, the surveillance camera can store video/image data into the replacement removable media in a way same as storing video/image data into a conventional removable media.</p><p id="p-0043" num="0042">In response to the video/image data being stored into the replacement removable media disclosed herein, the deep learning accelerator (DLA) automatically analyzes the video/image data to generate video analytics and/or inference results.</p><p id="p-0044" num="0043">For example, the video analytics and/or inference results can be used to compress the video/image data using a deep learning-based video coding technique. The computation of the deep learning-based video coding can be performed within the removable media. Thus, the removable media can convert/compress an input video autonomously without help from the surveillance camera and/or a host system to which the camera may be connected.</p><p id="p-0045" num="0044">For example, after the removable media device receives a high-resolution video, the deep learning accelerator (DLA) of the removable media performs an analysis of the video to generate video analytics with high accuracy. The video analytics can include pixel probability model, intra-picture prediction, inter-picture prediction, cross-channel prediction, probability distribution prediction, etc. Alternatively, or in combination, the video analytics can identify transform, post-loop filtering, in-loop filtering, down-sampling, up-sampling, encoding optimization, etc. The high-resolution video can be compressed through coding based on the video analytics to generate a compressed video as a replacement of the high-resolution video for storing the video in the device. Optionally, a set of configuration settings can be stored in the removable media to control aspects of the creation and storage of the compressed video. Such configuration settings can include input resolution, storage resolution, storage location, etc.</p><p id="p-0046" num="0045">The removable media can be a memory card that can be plugged into a slot of a reader of memory cards of a host computer system. The memory card can be configured with a form factor and interface of secure digital (SD) cards, microsd cards, compactflash (CF) cards, memory stick cards, etc.</p><p id="p-0047" num="0046">Alternatively, the removable media can be a network storage device that can be connected to a surveillance camera and/or a host computer system via a wired or wireless computer network connection.</p><p id="p-0048" num="0047">Alternatively, the removable media can be a storage device that can be connected to a surveillance camera and/or a host computer system via a standardized port, such as a universal serial bus (USB) port.</p><p id="p-0049" num="0048">Thus, in general, the removable media can be a solid state drive that can be connected to a host computer system via a port, such as a port to accept a universal serial bus (USB) device, a network storage device, or a memory card, or another type of removable data storage device.</p><p id="p-0050" num="0049">For example, the removable media can be a plug and play device that can be plugged into a port or a slot of a running computer system and be recognized and configured by the running computer system on the fly for normal operations as a storage device without requiring the computer system to restart or reboot.</p><p id="p-0051" num="0050">For example, the removable media can be dynamically added to a running computer system via a wired or wireless computer connection without requiring the computer system to restart or reboot.</p><p id="p-0052" num="0051">In some simplified implementations, the removable media may be a data storage device that is not removable and/or may not be hot plugged into a running computer system in order to be configured for normal operations; and after the storage device is connected into the computer system (e.g., using a peripheral bus of the computer system, or a memory bus of the computer system), it may be necessary to restart the computer system to configure the storage device for normal operations in the computer system. In some implementations, the computer system is to be powered down in order to connect the storage device to the computer system.</p><p id="p-0053" num="0052">For example, the removable media can include a host interface for a wired or a wireless connection to a host computer system using a wireless personal area network, a wireless local area network, a wired local area network, a universal serial bus (USB), etc. The host computer system can be a surveillance camera, a personal computer, a mobile computer, a smartphone, a personal media player, a set top box, a hub of internet of things (IoTs), and/or a server computer.</p><p id="p-0054" num="0053">After the removable media is connected to the host computer system, the host computer system can send commands to the removable media to store data, such as a video file. In some instances, the video can be streamed to the removable media in an uncompressed format, or a compressed format. The deep learning accelerator in the storage device analyzes the video file/stream to generate video analytics, such as pixel probability model, intra-picture prediction, inter-picture prediction, cross-channel prediction, probability distribution prediction, transform, post-loop filtering, in-loop filtering, down-sampling, up-sampling, encoding optimization, etc. A video encoder of the removable media codes/encodes/compresses the video file based on the video analytics and generates a compressed video file that is stored in the removable media as a replacement of the video file/stream received from the host computer system.</p><p id="p-0055" num="0054">Optionally, the removable media can perform real-time, on the fly compression when the host computer system streams the video to the removable media. While a subsequent portion of a video is being streamed into the removable media, the removable media provides a previously buffered portion of the video as input to an artificial neural network; and the deep learning accelerator of the removable media performs the computation of the artificial neural network to generate video analytics for the buffered portion; and the video encoder uses the video analytics to code/encode/compress of the buffered portion of the video stream on the fly while the streaming is in progress.</p><p id="p-0056" num="0055">As an alternative to the deep learning-based compression of video data, or in combination, the deep learning accelerator (DLA) and the random access memory with an artificial neural network (ANN) provided in the surveillance camera (e.g., provided via the removable media) can be used for intelligent local processing of the image data captured by the surveillance camera.</p><p id="p-0057" num="0056">For example, to reduce the data storage requirement and/or data communication traffic, the artificial neural network (ANN) implemented in the surveillance camera can convert the video/image data into inference results that are of interest to a specific application and thus eliminate the need to store and/or communicate most of, or all of, the video/image data captured by the image sensor of the surveillance camera.</p><p id="p-0058" num="0057">For example, the artificial neural network (ANN) can be trained to recognize objects and/or events in the images that may be of interest in a surveillance application and convert the images into a description or identification of the objects and events appearing in the images captured by the surveillance camera. The description or identification of the objects and events can be used as a high level summary of the content in the images, which can be used an input to the application running in a computer system that uses the surveillance camera as an input device. Such a computer system can be a mobile computer, a smart phone, a personal media player, a personal computer, a server computer, or a cluster of server computers. Such a high level summary of the content in the images, recognized using an artificial neural network, uses a significantly smaller amount of data storage capacity and/or data transmission bandwidth than the still images or video images that graphically present the corresponding objects and events.</p><p id="p-0059" num="0058">Optionally, images of non-recognized objects and/or events in unusual/unexpected situations or sceneries can be transmitted to a separate computer system for further processing. For example, human operators may inspect the images to identify the objects and events captured in the images of the unusual/unexpected situations or sceneries; and the results can be used to further train the artificial neural network (ANN) (e.g., using a supervised machine learning technique) to improve its capability in identifying objects and events. Subsequently, the further trained artificial neural network (ANN) can be loaded into the surveillance camera to further reduce data that needs to be stored and/or processed outside of the surveillance camera.</p><p id="p-0060" num="0059">Optionally, the surveillance camera can store for a predetermined period of time the image stream that has been converted into a summary or description. The image stream can be annotated with the summary or description and identifications of portions of images showing the objects and evens. Within the predetermined period of time, an application receiving the summary or description as input can select an object or event of interest and request the surveillance camera to transmit the relevant images of the object or event.</p><p id="p-0061" num="0060">In some applications, the surveillance camera is configured (e.g., via the removable media) to monitor the image stream for objects and/or events satisfying predetermined selection criteria. When such objects and/or events are detected in the output of the artificial neural network (ANN), the surveillance camera selects representative portions from the image stream for storing and/or for uploading to a separate computer. Through the analysis performed by the artificial neural network (ANN), the information in the image stream can be compressed into a summary that includes the identifications of the objects of interest, representative images of the objects, and a description of activities of the objects in the scenery captured in the image stream. Information on recognized objects that are not of interest can be reduced and/or discard. Such a summary of the image stream can achieve a compression ratio better than conventional lossy or lossless image/video compression techniques; and in many applications of a surveillance camera, such a summary can be sufficient and/or more useful than the original image stream generated by the image sensor of the surveillance camera.</p><p id="p-0062" num="0061">For example, instead of transmitting the image stream to a separate computer system for processing, the surveillance camera can provide a description of the content in the image stream to the computer system for processing. The representative images of the recognized images can be transmitted with the description, or provided when the computer system sends a request for such images. Further, the computer system may optionally request the surveillance camera to transmit a frame, or a segment of the image stream that contains an object or event of interest.</p><p id="p-0063" num="0062">The description of the content in the image stream can be in the form of identifications of objects, events, features, classifications, locations in images, sizes in images, etc.</p><p id="p-0064" num="0063">For example, a surveillance camera is configured with an image sensor to monitor the health condition of a user. An integrated circuit device having a deep learning accelerator and random access memory is configured in the surveillance camera to process the sensor data locally. Images of the user determined to be not associated with health concerns can be discarded automatically to reduce the need to store a large amount of raw image data and to protect the privacy of the user.</p><p id="p-0065" num="0064">The surveillance camera can be customized for a particular application of intelligent monitoring by storing a model of an artificial neural network (ANN) executable by the deep learning accelerator (DLA). For example, the model can be converted from a description of the artificial neural network (ANN) using a compiler; and the model includes weight/kernel matrices of the artificial neural network (ANN) and instructions with matrix operands, executable by the deep learning accelerator (DLA) to implement the computation of the artificial neural network (ANN) based on the weight/kernel matrices.</p><p id="p-0066" num="0065">For example, a surveillance camera can be configured in a health care facility to monitor the conditions of a patients. Different patients can have different health issues that require the monitoring for the detection of the onset of different symptoms. Thus, a surveillance camera can be customized for the patient currently being monitored by installing a model of an artificial neural network (ANN) trained for the detection of symptoms relevant to the patient.</p><p id="p-0067" num="0066">After the surveillance camera is customized through storing the model in the random access memory to perform the computations of the artificial neural network (ANN), the raw image data generated by the image sensor in the surveillance camera is provided as input to the artificial neural network (ANN); and the outputs of the artificial neural network (ANN) can be used to generate alerts, to selectively retain and/or report sensor data, and/or be provided as the primary output of the surveillance camera.</p><p id="p-0068" num="0067">For example, the surveillance camera can include a wireless transceiver (e.g., a communication device for a wireless personal area network, such as a bluetooth transceiver, or a communication device for a wireless local area network, such as a Wi-Fi transceiver). Through a wireless connection, the output of the artificial neural network (ANN) can be reported to a separate computer system, such as a smartphone, a personal media player, a mobile computer, a personal computer, a hub of internet of things (IoTs), and/or a server computer.</p><p id="p-0069" num="0068">Alternatively, the surveillance camera can have a port for a wired connection to a separate computer system to report the output of the artificial neural network (ANN) or download the outputs stored in the surveillance camera over a period of time.</p><p id="p-0070" num="0069">For example, the artificial neural network (ANN) can be trained to classify whether an image is of interest to a specific application. The output of the artificial neural network (ANN) can be used to selectively store image data for subsequent analysis and/or downloading.</p><p id="p-0071" num="0070">For example, the surveillance camera can provide, to a computer system and without assistance from the computer system, intelligent outputs that are generated locally in the surveillance camera using the artificial neural network (ANN). For example, the surveillance camera can be used to monitor for a health related event and generate an alert when such an event is detected. For example, the surveillance camera can be used to monitor for indications related to the diagnosis of a health problem and record occurrences of such indications and/or associated data for further analysis. For example, the surveillance camera can be used to monitor the user for a fall and generate a call for assistance when detecting that the user is falling. For example, the surveillance camera can be used to detect the appearance of an object in the surrounding of the user of the surveillance camera, and provide an identification of the object for further processing. For example, the surveillance camera can be used to detect a gesture of the user and provide the identification of the gesture to a separate computer (e.g., a smartphone, a game console, a personal media player, a personal computer, a set top box), to control an operation in the separate computer.</p><p id="p-0072" num="0071">The random access memory coupled with the deep learning accelerator can include a portion configured to store input to the artificial neural network (ANN) and another portion configured to store output from the artificial neural network (ANN). The input video data received from the host computer system can be stored in a cyclic way in the input portion of the random access memory. Thus, the input video data for the latest period of the buffering a video stream in the storage device can be found in the input portion of the random access memory for analysis by the deep learning accelerator according to the artificial neural network (ANN) and for encoding by the video encoder. The deep learning accelerator (DLA) can convert in real time, the input video audio data in the input portion into video analytics and/or inference results stored in the output portion of the random access memory. A video encoder can be configured to use video analytics and/or inference results stored in the output portion of the random access memory to compress the input video data stored in the input portion of the random access memory. The compressed video can include a description of the content in the video as recognized by the artificial neural network (ANN) and can be stored in the random access memory or another storage media according to a set of configuration parameters.</p><p id="p-0073" num="0072">For example, a stream of input video data to the artificial neural network (ANN) can be analyzed by the artificial neural network (ANN) into identify segments associated with different scenes depicted in the video stream. Each video segment can be configured to be compressed as a unit. The length of a video segment can be limited by a predetermined number of frames. Further, each video segment can be provided as a set of input to the artificial neural network (ANN) during a time slot. While the deep learning accelerator (DLA) is computing the video analytics from the current set of input, the next video segment can be stored into the random access memory as the next input to the artificial neural network (ANN); and concurrently, the output generated for the previous video segment can be retrieved from the random access memory for the coding/encoding/compression of the previous video segment. The input region of the random access memory can accommodate the storage of multiple video segments at the same time; and after the previous video segment has been encoded and stored, a new video segment can be stored/streamed into the area where the previous video segment is stored.</p><p id="p-0074" num="0073">Thus, the task of compressing a video stream using an artificial neural network (ANN) can be added to a surveillance camera by inserting a removable media according to embodiments disclosed herein. The computation of the artificial neural network (ANN) can be performed locally on the removable media to reduce data storage requirement and/or to reduce computation workload on the surveillance camera and/or the host computer system that uses the surveillance camera.</p><p id="p-0075" num="0074">Further, neuromorphic memory can be used to implement the computations of matrix/vector multiplication and summation to reduce power consumption of the deep learning accelerator (DLA).</p><p id="p-0076" num="0075">For example, neuromorphic memory can be implemented using a crossbar array of memristors that are configured to perform the multiply-and-accumulate (MAC) operations via analog circuitry. Electric currents going through the wordlines through a set of memristors in the crossbar array to a bitline are summed in the bitline, which corresponds to the accumulation operation. The electric currents correspond to the multiplication of the voltages applied on the wordlines and parameters associated with the resistances of the memristors, which corresponds to the multiplication operations. The current in the bitline can be compared with a threshold to determine whether a neuron represented by the bitline is activated under the current input. An array of memristors can be connected to the bitlines respectively and programmed to have thresholds corresponding to the activation level thresholds of the neurons. A current detector can be configured for each memristor connected to the output of a bitline to determine whether the level of electric current in the bitline corresponding to a level that exceeds the threshold of the memristor.</p><p id="p-0077" num="0076"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows an integrated circuit device (<b>101</b>) having a deep learning accelerator (<b>103</b>) and random access memory (<b>105</b>) configured according to one embodiment.</p><p id="p-0078" num="0077">The deep learning accelerator (<b>103</b>) in <figref idref="DRAWINGS">FIG. <b>1</b></figref> includes processing units (<b>111</b>), a control unit (<b>113</b>), and local memory (<b>115</b>). When vector and matrix operands are in the local memory (<b>115</b>), the control unit (<b>113</b>) can use the processing units (<b>111</b>) to perform vector and matrix operations in accordance with instructions. Further, the control unit (<b>113</b>) can load instructions and operands from the random access memory (<b>105</b>) through a memory interface (<b>117</b>) and a high speed/bandwidth connection (<b>119</b>).</p><p id="p-0079" num="0078">The integrated circuit device (<b>101</b>) is configured to be enclosed within an integrated circuit package with pins or contacts for a memory controller interface (<b>107</b>).</p><p id="p-0080" num="0079">The memory controller interface (<b>107</b>) is configured to support a standard memory access protocol such that the integrated circuit device (<b>101</b>) appears to a typical memory controller in a way same as a conventional random access memory device having no deep learning accelerator (DLA) (<b>103</b>). For example, a memory controller external to the integrated circuit device (<b>101</b>) can access, using a standard memory access protocol through the memory controller interface (<b>107</b>), the random access memory (<b>105</b>) in the integrated circuit device (<b>101</b>).</p><p id="p-0081" num="0080">The integrated circuit device (<b>101</b>) is configured with a high bandwidth connection (<b>119</b>) between the random access memory (<b>105</b>) and the deep learning accelerator (DLA) (<b>103</b>) that are enclosed within the integrated circuit device (<b>101</b>). The bandwidth of the connection (<b>119</b>) is higher than the bandwidth of the connection (<b>109</b>) between the random access memory (<b>105</b>) and the memory controller interface (<b>107</b>).</p><p id="p-0082" num="0081">In one embodiment, both the memory controller interface (<b>107</b>) and the memory interface (<b>117</b>) are configured to access the random access memory (<b>105</b>) via a same set of buses or wires. Thus, the bandwidth to access the random access memory (<b>105</b>) is shared between the memory interface (<b>117</b>) and the memory controller interface (<b>107</b>). Alternatively, the memory controller interface (<b>107</b>) and the memory interface (<b>117</b>) are configured to access the random access memory (<b>105</b>) via separate sets of buses or wires. Optionally, the random access memory (<b>105</b>) can include multiple sections that can be accessed concurrently via the connection (<b>119</b>). For example, when the memory interface (<b>117</b>) is accessing a section of the random access memory (<b>105</b>), the memory control interface (<b>107</b>) can concurrently access another section of the random access memory (<b>105</b>). For example, the different sections can be configured on different integrated circuit dies and/or different planes/banks of memory cells; and the different sections can be accessed in parallel to increase throughput in accessing the random access memory (<b>105</b>). For example, the memory controller interface (<b>107</b>) is configured to access one data unit of a predetermined size at a time; and the memory interface (<b>117</b>) is configured to access multiple data units, each of the same predetermined size, at a time.</p><p id="p-0083" num="0082">In one embodiment, the random access memory (<b>105</b>) and the integrated circuit device (<b>101</b>) are configured on different integrated circuit dies configured within a same integrated circuit package. Further, the random access memory (<b>105</b>) can be configured on one or more integrated circuit dies that allows parallel access of multiple data elements concurrently.</p><p id="p-0084" num="0083">In some implementations, the number of data elements of a vector or matrix that can be accessed in parallel over the connection (<b>119</b>) corresponds to the granularity of the deep learning accelerator (DLA) operating on vectors or matrices. For example, when the processing units (<b>111</b>) can operate on a number of vector/matrix elements in parallel, the connection (<b>119</b>) is configured to load or store the same number, or multiples of the number, of elements via the connection (<b>119</b>) in parallel.</p><p id="p-0085" num="0084">Optionally, the data access speed of the connection (<b>119</b>) can be configured based on the processing speed of the deep learning accelerator (DLA) (<b>103</b>). For example, after an amount of data and instructions have been loaded into the local memory (<b>115</b>), the control unit (<b>113</b>) can execute an instruction to operate on the data using the processing units (<b>111</b>) to generate output. Within the time period of processing to generate the output, the access bandwidth of the connection (<b>119</b>) allows the same amount of data and instructions to be loaded into the local memory (<b>115</b>) for the next operation and the same amount of output to be stored back to the random access memory (<b>105</b>). For example, while the control unit (<b>113</b>) is using a portion of the local memory (<b>115</b>) to process data and generate output, the memory interface (<b>117</b>) can offload the output of a prior operation into the random access memory (<b>105</b>) from, and load operand data and instructions into, another portion of the local memory (<b>115</b>). Thus, the utilization and performance of the deep learning accelerator (DLA) are not restricted or reduced by the bandwidth of the connection (<b>119</b>).</p><p id="p-0086" num="0085">The random access memory (<b>105</b>) can be used to store the model data of an artificial neural network (ANN) and to buffer input data for the artificial neural network (ANN). The model data does not change frequently. The model data can include the output generated by a compiler for the deep learning accelerator (DLA) to implement the artificial neural network (ANN). The model data typically includes matrices used in the description of the artificial neural network (ANN) and instructions generated for the deep learning accelerator (DLA) (<b>103</b>) to perform vector/matrix operations of the artificial neural network (ANN) based on vector/matrix operations of the granularity of the deep learning accelerator (DLA) (<b>103</b>). The instructions operate not only on the vector/matrix operations of the artificial neural network (ANN), but also on the input data for the artificial neural network (ANN).</p><p id="p-0087" num="0086">In one embodiment, when the input data is loaded or updated in the random access memory (<b>105</b>), the control unit (<b>113</b>) of the deep learning accelerator (DLA) (<b>103</b>) can automatically execute the instructions for the artificial neural network (ANN) to generate an output of the artificial neural network (ANN). The output is stored into a predefined region in the random access memory (<b>105</b>). The deep learning accelerator (DLA) (<b>103</b>) can execute the instructions without help from a central processing unit (CPU). Thus, communications for the coordination between the deep learning accelerator (DLA) (<b>103</b>) and a processor outside of the integrated circuit device (<b>101</b>) (e.g., a central processing unit (CPU)) can be reduced or eliminated.</p><p id="p-0088" num="0087">Optionally, the logic circuit of the deep learning accelerator (DLA) (<b>103</b>) can be implemented via complementary metal oxide semiconductor (CMOS). For example, the technique of CMOS under the array (CUA) of memory cells of the random access memory (<b>105</b>) can be used to implement the logic circuit of the deep learning accelerator (DLA) (<b>103</b>), including the processing units (<b>111</b>) and the control unit (<b>113</b>). Alternatively, the technique of CMOS in the array of memory cells of the random access memory (<b>105</b>) can be used to implement the logic circuit of the deep learning accelerator (DLA) (<b>103</b>).</p><p id="p-0089" num="0088">In some implementations, the deep learning accelerator (DLA) (<b>103</b>) and the random access memory (<b>105</b>) can be implemented on separate integrated circuit dies and connected using through-silicon vias (TSV) for increased data bandwidth between the deep learning accelerator (DLA) (<b>103</b>) and the random access memory (<b>105</b>). For example, the deep learning accelerator (DLA) (<b>103</b>) can be formed on an integrated circuit die of a field-programmable gate array (FPGA) or application specific integrated circuit (ASIC).</p><p id="p-0090" num="0089">Alternatively, the deep learning accelerator (DLA) (<b>103</b>) and the random access memory (<b>105</b>) can be configured in separate integrated circuit packages and connected via multiple point-to-point connections on a printed circuit board (PCB) for parallel communications and thus increased data transfer bandwidth.</p><p id="p-0091" num="0090">The random access memory (<b>105</b>) can be volatile memory or non-volatile memory, or a combination of volatile memory and non-volatile memory. Examples of non-volatile memory include flash memory, memory cells formed based on negative- and (NAND) logic gates, negative-or (NOR) logic gates, phase-change memory (PCM), magnetic memory (MRAM), resistive random-access memory, cross point storage and memory devices. A cross point memory device can use transistor-less memory elements, each of which has a memory cell and a selector that are stacked together as a column. Memory element columns are connected via two layers of wires running in perpendicular directions, where wires of one layer run in one direction in the layer that is located above the memory element columns, and wires of the other layer run in another direction and are located below the memory element columns. Each memory element can be individually selected at a cross point of one wire on each of the two layers. Cross point memory devices are fast and non-volatile and can be used as a unified memory pool for processing and storage. Further examples of non-volatile memory include read-only memory (ROM), programmable read-only memory (PROM), erasable programmable read-only memory (EPROM) and electronically erasable programmable read-only memory (EEPROM) memory, etc. Examples of volatile memory include dynamic random-access memory (DRAM) and static random-access memory (SRAM).</p><p id="p-0092" num="0091">For example, non-volatile memory can be configured to implement at least a portion of the random access memory (<b>105</b>). The non-volatile memory in the random access memory (<b>105</b>) can be used to store the model data of an artificial neural network (ANN). Thus, after the integrated circuit device (<b>101</b>) is powered off and restarts, it is not necessary to reload the model data of the artificial neural network (ANN) into the integrated circuit device (<b>101</b>). Further, the non-volatile memory can be programmable/rewritable. Thus, the model data of the artificial neural network (ANN) in the integrated circuit device (<b>101</b>) can be updated or replaced to implement an update artificial neural network (ANN), or another artificial neural network (ANN).</p><p id="p-0093" num="0092">The processing units (<b>111</b>) of the deep learning accelerator (DLA) (<b>103</b>) can include vector-vector units, matrix-vector units, and/or matrix-matrix units. Examples of units configured to perform for vector-vector operations, matrix-vector operations, and matrix-matrix operations are discussed below in connection with <figref idref="DRAWINGS">FIGS. <b>2</b>-<b>4</b></figref>.</p><p id="p-0094" num="0093"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows a processing unit (<b>121</b>) configured to perform matrix-matrix operations according to one embodiment. For example, the matrix-matrix unit (<b>121</b>) of <figref idref="DRAWINGS">FIG. <b>2</b></figref> can be used as one of the processing units (<b>111</b>) of the deep learning accelerator (DLA) (<b>103</b>) of <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0095" num="0094">In <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the matrix-matrix unit (<b>121</b>) includes multiple kernel buffers (<b>131</b> to <b>133</b>) and multiple the maps banks (<b>151</b> to <b>153</b>). Each of the maps banks (<b>151</b> to <b>153</b>) stores one vector of a matrix operand that has multiple vectors stored in the maps banks (<b>151</b> to <b>153</b>) respectively; and each of the kernel buffers (<b>131</b> to <b>133</b>) stores one vector of another matrix operand that has multiple vectors stored in the kernel buffers (<b>131</b> to <b>133</b>) respectively. The matrix-matrix unit (<b>121</b>) is configured to perform multiplication and accumulation operations on the elements of the two matrix operands, using multiple matrix-vector units (<b>141</b> to <b>143</b>) that operate in parallel.</p><p id="p-0096" num="0095">A crossbar (<b>123</b>) connects the maps banks (<b>151</b> to <b>153</b>) to the matrix-vector units (<b>141</b> to <b>143</b>). The same matrix operand stored in the maps bank (<b>151</b> to <b>153</b>) is provided via the crossbar (<b>123</b>) to each of the matrix-vector units (<b>141</b> to <b>143</b>); and the matrix-vector units (<b>141</b> to <b>143</b>) receives data elements from the maps banks (<b>151</b> to <b>153</b>) in parallel. Each of the kernel buffers (<b>131</b> to <b>133</b>) is connected to a respective one in the matrix-vector units (<b>141</b> to <b>143</b>) and provides a vector operand to the respective matrix-vector unit. The matrix-vector units (<b>141</b> to <b>143</b>) operate concurrently to compute the operation of the same matrix operand, stored in the maps banks (<b>151</b> to <b>153</b>) multiplied by the corresponding vectors stored in the kernel buffers (<b>131</b> to <b>133</b>). For example, the matrix-vector unit (<b>141</b>) performs the multiplication operation on the matrix operand stored in the maps banks (<b>151</b> to <b>153</b>) and the vector operand stored in the kernel buffer (<b>131</b>), while the matrix-vector unit (<b>143</b>) is concurrently performing the multiplication operation on the matrix operand stored in the maps banks (<b>151</b> to <b>153</b>) and the vector operand stored in the kernel buffer (<b>133</b>).</p><p id="p-0097" num="0096">Each of the matrix-vector units (<b>141</b> to <b>143</b>) in <figref idref="DRAWINGS">FIG. <b>2</b></figref> can be implemented in a way as illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0098" num="0097"><figref idref="DRAWINGS">FIG. <b>3</b></figref> shows a processing unit (<b>141</b>) configured to perform matrix-vector operations according to one embodiment. For example, the matrix-vector unit (<b>141</b>) of <figref idref="DRAWINGS">FIG. <b>3</b></figref> can be used as any of the matrix-vector units in the matrix-matrix unit (<b>121</b>) of <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0099" num="0098">In <figref idref="DRAWINGS">FIG. <b>3</b></figref>, each of the maps banks (<b>151</b> to <b>153</b>) stores one vector of a matrix operand that has multiple vectors stored in the maps banks (<b>151</b> to <b>153</b>) respectively, in a way similar to the maps banks (<b>151</b> to <b>153</b>) of <figref idref="DRAWINGS">FIG. <b>2</b></figref>. The crossbar (<b>123</b>) in <figref idref="DRAWINGS">FIG. <b>3</b></figref> provides the vectors from the maps banks (<b>151</b>) to the vector-vector units (<b>161</b> to <b>163</b>) respectively. A same vector stored in the kernel buffer (<b>131</b>) is provided to the vector-vector units (<b>161</b> to <b>163</b>).</p><p id="p-0100" num="0099">The vector-vector units (<b>161</b> to <b>163</b>) operate concurrently to compute the operation of the corresponding vector operands, stored in the maps banks (<b>151</b> to <b>153</b>) respectively, multiplied by the same vector operand that is stored in the kernel buffer (<b>131</b>). For example, the vector-vector unit (<b>161</b>) performs the multiplication operation on the vector operand stored in the maps bank (<b>151</b>) and the vector operand stored in the kernel buffer (<b>131</b>), while the vector-vector unit (<b>163</b>) is concurrently performing the multiplication operation on the vector operand stored in the maps bank (<b>153</b>) and the vector operand stored in the kernel buffer (<b>131</b>).</p><p id="p-0101" num="0100">When the matrix-vector unit (<b>141</b>) of <figref idref="DRAWINGS">FIG. <b>3</b></figref> is implemented in a matrix-matrix unit (<b>121</b>) of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the matrix-vector unit (<b>141</b>) can use the maps banks (<b>151</b> to <b>153</b>), the crossbar (<b>123</b>) and the kernel buffer (<b>131</b>) of the matrix-matrix unit (<b>121</b>).</p><p id="p-0102" num="0101">Each of the vector-vector units (<b>161</b> to <b>163</b>) in <figref idref="DRAWINGS">FIG. <b>3</b></figref> can be implemented in a way as illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0103" num="0102"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows a processing unit (<b>161</b>) configured to perform vector-vector operations according to one embodiment. For example, the vector-vector unit (<b>161</b>) of <figref idref="DRAWINGS">FIG. <b>4</b></figref> can be used as any of the vector-vector units in the matrix-vector unit (<b>141</b>) of <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0104" num="0103">In <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the vector-vector unit (<b>161</b>) has multiple multiply-accumulate (MAC) units (<b>171</b> to <b>173</b>). Each of the multiply-accumulate (MAC) units (<b>171</b> to <b>173</b>) can receive two numbers as operands, perform multiplication of the two numbers, and add the result of the multiplication to a sum maintained in the multiply-accumulate (MAC) unit.</p><p id="p-0105" num="0104">Each of the vector buffers (<b>181</b> and <b>183</b>) stores a list of numbers. A pair of numbers, each from one of the vector buffers (<b>181</b> and <b>183</b>), can be provided to each of the multiply-accumulate (MAC) units (<b>171</b> to <b>173</b>) as input. The multiply-accumulate (MAC) units (<b>171</b> to <b>173</b>) can receive multiple pairs of numbers from the vector buffers (<b>181</b> and <b>183</b>) in parallel and perform the multiply-accumulate (MAC) operations in parallel. The outputs from the multiply-accumulate (MAC) units (<b>171</b> to <b>173</b>) are stored into the shift register (<b>175</b>); and an accumulator (<b>177</b>) computes the sum of the results in the shift register (<b>175</b>).</p><p id="p-0106" num="0105">When the vector-vector unit (<b>161</b>) of <figref idref="DRAWINGS">FIG. <b>4</b></figref> is implemented in a matrix-vector unit (<b>141</b>) of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the vector-vector unit (<b>161</b>) can use a maps bank (e.g., <b>151</b> or <b>153</b>) as one vector buffer (<b>181</b>), and the kernel buffer (<b>131</b>) of the matrix-vector unit (<b>141</b>) as another vector buffer (<b>183</b>).</p><p id="p-0107" num="0106">The vector buffers (<b>181</b> and <b>183</b>) can have a same length to store the same number/count of data elements. The length can be equal to, or the multiple of, the count of multiply-accumulate (MAC) units (<b>171</b> to <b>173</b>) in the vector-vector unit (<b>161</b>). When the length of the vector buffers (<b>181</b> and <b>183</b>) is the multiple of the count of multiply-accumulate (MAC) units (<b>171</b> to <b>173</b>), a number of pairs of inputs, equal to the count of the multiply-accumulate (MAC) units (<b>171</b> to <b>173</b>), can be provided from the vector buffers (<b>181</b> and <b>183</b>) as inputs to the multiply-accumulate (MAC) units (<b>171</b> to <b>173</b>) in each iteration; and the vector buffers (<b>181</b> and <b>183</b>) feed their elements into the multiply-accumulate (MAC) units (<b>171</b> to <b>173</b>) through multiple iterations.</p><p id="p-0108" num="0107">In one embodiment, the communication bandwidth of the connection (<b>119</b>) between the deep learning accelerator (DLA) (<b>103</b>) and the random access memory (<b>105</b>) is sufficient for the matrix-matrix unit (<b>121</b>) to use portions of the random access memory (<b>105</b>) as the maps banks (<b>151</b> to <b>153</b>) and the kernel buffers (<b>131</b> to <b>133</b>).</p><p id="p-0109" num="0108">In another embodiment, the maps banks (<b>151</b> to <b>153</b>) and the kernel buffers (<b>131</b> to <b>133</b>) are implemented in a portion of the local memory (<b>115</b>) of the deep learning accelerator (DLA) (<b>103</b>). The communication bandwidth of the connection (<b>119</b>) between the deep learning accelerator (DLA) (<b>103</b>) and the random access memory (<b>105</b>) is sufficient to load, into another portion of the local memory (<b>115</b>), matrix operands of the next operation cycle of the matrix-matrix unit (<b>121</b>), while the matrix-matrix unit (<b>121</b>) is performing the computation in the current operation cycle using the maps banks (<b>151</b> to <b>153</b>) and the kernel buffers (<b>131</b> to <b>133</b>) implemented in a different portion of the local memory (<b>115</b>) of the deep learning accelerator (DLA) (<b>103</b>).</p><p id="p-0110" num="0109"><figref idref="DRAWINGS">FIG. <b>5</b></figref> shows a deep learning accelerator and random access memory configured to autonomously apply inputs to a trained artificial neural network according to one embodiment.</p><p id="p-0111" num="0110">An artificial neural network (ANN) (<b>201</b>) that has been trained through machine learning (e.g., deep learning) can be described in a standard format (e.g., open neural network exchange (ONNX)). The description of the trained artificial neural network (ANN) (<b>201</b>) in the standard format identifies the properties of the artificial neurons and their connectivity.</p><p id="p-0112" num="0111">In <figref idref="DRAWINGS">FIG. <b>5</b></figref>, a deep learning accelerator (DLA) compiler (<b>203</b>) converts trained artificial neural network (ANN) (<b>201</b>) by generating instructions (<b>205</b>) for a deep learning accelerator (DLA) (<b>103</b>) and matrices (<b>207</b>) corresponding to the properties of the artificial neurons and their connectivity. The instructions (<b>205</b>) and the matrices (<b>207</b>) generated by the DLA compiler (<b>203</b>) from the trained artificial neural network (ANN) (<b>201</b>) can be stored in random access memory (<b>105</b>) for the deep learning accelerator (DLA) (<b>103</b>).</p><p id="p-0113" num="0112">For example, the random access memory (<b>105</b>) and the deep learning accelerator (DLA) (<b>103</b>) can be connected via a high bandwidth connection (<b>119</b>) in a way as in the integrated circuit device (<b>101</b>) of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. The autonomous computation of <figref idref="DRAWINGS">FIG. <b>5</b></figref> based on the instructions (<b>205</b>) and the matrices (<b>207</b>) can be implemented in the integrated circuit device (<b>101</b>) of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. Alternatively, the random access memory (<b>105</b>) and the deep learning accelerator (DLA) (<b>103</b>) can be configured on a printed circuit board with multiple point to point serial buses running in parallel to implement the connection (<b>119</b>).</p><p id="p-0114" num="0113">In <figref idref="DRAWINGS">FIG. <b>5</b></figref>, after the results of the DLA compiler (<b>203</b>) are stored in the random access memory (<b>105</b>), the application of the trained artificial neural network (ANN) (<b>201</b>) to process an input (<b>211</b>) to the trained artificial neural network (ANN) (<b>201</b>) to generate the corresponding output (<b>213</b>) of the trained artificial neural network (ANN) (<b>201</b>) can be triggered by the presence of the input (<b>211</b>) in the random access memory (<b>105</b>), or another indication provided in the random access memory (<b>105</b>).</p><p id="p-0115" num="0114">In response, the deep learning accelerator (DLA) (<b>103</b>) executes the instructions (<b>205</b>) to combine the input (<b>211</b>) and the matrices (<b>207</b>). The execution of the instructions (<b>205</b>) can include the generation of maps matrices for the maps banks (<b>151</b> to <b>153</b>) of one or more matrix-matrix units (e.g., <b>121</b>) of the deep learning accelerator (DLA) (<b>103</b>).</p><p id="p-0116" num="0115">In some embodiments, the inputs to ANN (<b>211</b>) is in the form of an initial maps matrix. Portions of the initial maps matrix can be retrieved from the random access memory (<b>105</b>) as the matrix operand stored in the maps banks (<b>151</b> to <b>153</b>) of a matrix-matrix unit (<b>121</b>). Alternatively, the DLA instructions (<b>205</b>) also include instructions for the deep learning accelerator (DLA) (<b>103</b>) to generate the initial maps matrix from the input (<b>211</b>).</p><p id="p-0117" num="0116">According to the DLA instructions (<b>205</b>), the deep learning accelerator (DLA) (<b>103</b>) loads matrix operands into the kernel buffers (<b>131</b> to <b>133</b>) and maps banks (<b>151</b> to <b>153</b>) of its matrix-matrix unit (<b>121</b>). The matrix-matrix unit (<b>121</b>) performs the matrix computation on the matrix operands. For example, the DLA instructions (<b>205</b>) break down matrix computations of the trained artificial neural network (ANN) (<b>201</b>) according to the computation granularity of the deep learning accelerator (DLA) (<b>103</b>) (e.g., the sizes/dimensions of matrices that loaded as matrix operands in the matrix-matrix unit (<b>121</b>)) and applies the input feature maps to the kernel of a layer of artificial neurons to generate output as the input for the next layer of artificial neurons.</p><p id="p-0118" num="0117">Upon completion of the computation of the trained artificial neural network (ANN) (<b>201</b>) performed according to the instructions (<b>205</b>), the deep learning accelerator (DLA) (<b>103</b>) stores the output (<b>213</b>) of the artificial neural network (ANN) (<b>201</b>) at a pre-defined location in the random access memory (<b>105</b>), or at a location specified in an indication provided in the random access memory (<b>105</b>) to trigger the computation.</p><p id="p-0119" num="0118">When the technique of <figref idref="DRAWINGS">FIG. <b>5</b></figref> is implemented in the integrated circuit device (<b>101</b>) of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, an external device connected to the memory controller interface (<b>107</b>) can write the input (<b>211</b>) into the random access memory (<b>105</b>) and trigger the autonomous computation of applying the input (<b>211</b>) to the trained artificial neural network (ANN) (<b>201</b>) by the deep learning accelerator (DLA) (<b>103</b>). After a period of time, the output (<b>213</b>) is available in the random access memory (<b>105</b>); and the external device can read the output (<b>213</b>) via the memory controller interface (<b>107</b>) of the integrated circuit device (<b>101</b>).</p><p id="p-0120" num="0119">For example, a predefined location in the random access memory (<b>105</b>) can be configured to store an indication to trigger the autonomous execution of the instructions (<b>205</b>) by the deep learning accelerator (DLA) (<b>103</b>). The indication can optionally include a location of the input (<b>211</b>) within the random access memory (<b>105</b>). Thus, during the autonomous execution of the instructions (<b>205</b>) to process the input (<b>211</b>), the external device can retrieve the output generated during a previous run of the instructions (<b>205</b>), and/or store another set of input for the next run of the instructions (<b>205</b>).</p><p id="p-0121" num="0120">Optionally, a further predefined location in the random access memory (<b>105</b>) can be configured to store an indication of the progress status of the current run of the instructions (<b>205</b>). Further, the indication can include a prediction of the completion time of the current run of the instructions (<b>205</b>) (e.g., estimated based on a prior run of the instructions (<b>205</b>)). Thus, the external device can check the completion status at a suitable time window to retrieve the output (<b>213</b>).</p><p id="p-0122" num="0121">In some embodiments, the random access memory (<b>105</b>) is configured with sufficient capacity to store multiple sets of inputs (e.g., <b>211</b>) and outputs (e.g., <b>213</b>). Each set can be configured in a predetermined slot/area in the random access memory (<b>105</b>).</p><p id="p-0123" num="0122">The deep learning accelerator (DLA) (<b>103</b>) can execute the instructions (<b>205</b>) autonomously to generate the output (<b>213</b>) from the input (<b>211</b>) according to matrices (<b>207</b>) stored in the random access memory (<b>105</b>) without helps from a processor or device that is located outside of the integrated circuit device (<b>101</b>).</p><p id="p-0124" num="0123">In a method according to one embodiment, random access memory (<b>105</b>) of a computing device (e.g., <b>101</b>) can be accessed using an interface (<b>107</b>) of the computing device (e.g., <b>101</b>) to a memory controller. The computing device (e.g., <b>101</b>) can have processing units (e.g., <b>111</b>) configured to perform at least computations on matrix operands, such as a matrix operand stored in maps banks (<b>151</b> to <b>153</b>) and a matrix operand stored in kernel buffers (<b>131</b> to <b>133</b>).</p><p id="p-0125" num="0124">For example, the computing device (e.g., <b>101</b>) can be enclosed within an integrated circuit package; and a set of connections can connect the interface (<b>107</b>) to the memory controller that is located outside of the integrated circuit package.</p><p id="p-0126" num="0125">Instructions (<b>205</b>) executable by the processing units (e.g., <b>111</b>) can be written into the random access memory (<b>105</b>) through the interface (<b>107</b>).</p><p id="p-0127" num="0126">Matrices (<b>207</b>) of an artificial neural network (<b>201</b>) can be written into the random access memory (<b>105</b>) through the interface (<b>107</b>). The matrices (<b>207</b>) identify the property and/or state of the artificial neural network (<b>201</b>).</p><p id="p-0128" num="0127">Optionally, at least a portion of the random access memory (<b>105</b>) is non-volatile and configured to store the instructions (<b>205</b>) and the matrices (<b>07</b>) of the artificial neural network (<b>201</b>).</p><p id="p-0129" num="0128">First input (<b>211</b>) to the artificial neural network can be written into the random access memory (<b>105</b>) through the interface (<b>107</b>).</p><p id="p-0130" num="0129">An indication is provided in the random access memory (<b>105</b>) to cause the processing units (<b>111</b>) to start execution of the instructions (<b>205</b>). In response to the indication, the processing units (<b>111</b>) execute the instructions to combine the first input (<b>211</b>) with the matrices (<b>207</b>) of the artificial neural network (<b>201</b>) to generate first output (<b>213</b>) from the artificial neural network (<b>201</b>) and store the first output (<b>213</b>) in the random access memory (<b>105</b>).</p><p id="p-0131" num="0130">For example, the indication can be an address of the first input (<b>211</b>) in the random access memory (<b>105</b>); and the indication can be stored a predetermined location in the random access memory (<b>105</b>) to cause the initiation of the execution of the instructions (<b>205</b>) for the input (<b>211</b>) identified by the address. Optionally, the indication can also include an address for storing the output (<b>213</b>).</p><p id="p-0132" num="0131">The first output (<b>213</b>) can be read, through the interface (<b>107</b>), from the random access memory (<b>105</b>).</p><p id="p-0133" num="0132">For example, the computing device (e.g., <b>101</b>) can have a deep learning accelerator (<b>103</b>) formed on a first integrated circuit die and the random access memory (<b>105</b>) formed on one or more second integrated circuit dies. The connection (<b>119</b>) between the first integrated circuit die and the one or more second integrated circuit dies can include through-silicon vias (TSVs) to provide high bandwidth for memory access.</p><p id="p-0134" num="0133">For example, a description of the artificial neural network (<b>201</b>) can be converted using a compiler (<b>203</b>) into the instructions (<b>205</b>) and the matrices (<b>207</b>). The combination of the instructions (<b>205</b>) and the matrices (<b>207</b>) stored in the random access memory (<b>105</b>) and the deep learning accelerator (<b>103</b>) provides an autonomous implementation of the artificial neural network (<b>201</b>) that can automatically convert input (<b>211</b>) to the artificial neural network (<b>201</b>) to its output (<b>213</b>).</p><p id="p-0135" num="0134">For example, during a time period in which the deep learning accelerator (<b>103</b>) executes the instructions (<b>205</b>) to generate the first output (<b>213</b>) from the first input (<b>211</b>) according to the matrices (<b>207</b>) of the artificial neural network (<b>201</b>), the second input to artificial neural network (<b>201</b>) can be written into the random access memory (<b>105</b>) through the interface (<b>107</b>) at an alternative location. After the first output (<b>213</b>) is stored in the random access memory (<b>105</b>), an indication can be provided in the random access memory to cause the deep learning accelerator (<b>103</b>) to again start the execution of the instructions and generate second output from the second input.</p><p id="p-0136" num="0135">During the time period in which the deep learning accelerator (<b>103</b>) executes the instructions (<b>205</b>) to generate the second output from the second input according to the matrices (<b>207</b>) of the artificial neural network (<b>201</b>), the first output (<b>213</b>) can be read from the random access memory (<b>105</b>) through the interface (<b>107</b>); and a further input can be written into the random access memory to replace the first input (<b>211</b>), or written at a different location. The process can be repeated for a sequence of inputs.</p><p id="p-0137" num="0136">The deep learning accelerator (<b>103</b>) can include at least one matrix-matrix unit (<b>121</b>) that can execute an instruction on two matrix operands. The two matrix operands can be a first matrix and a second matrix. Each of two matrices has a plurality of vectors. The matrix-matrix unit (<b>121</b>) can include a plurality of matrix-vector units (<b>141</b> to <b>143</b>) configured to operate in parallel. Each of the matrix-vector units (<b>141</b> to <b>143</b>) are configured to operate, in parallel with other matrix-vector units, on the first matrix and one vector from second matrix. Further, each of the matrix-vector units (<b>141</b> to <b>143</b>) can have a plurality of vector-vector units (<b>161</b> to <b>163</b>) configured to operate in parallel. Each of the vector-vector units (<b>161</b> to <b>163</b>) is configured to operate, in parallel with other vector-vector units, on a vector from the first matrix and a common vector operand of the corresponding matrix-vector unit. Further, each of the vector-vector units (<b>161</b> to <b>163</b>) can have a plurality of multiply-accumulate units (<b>171</b> to <b>173</b>) configured to operate in parallel.</p><p id="p-0138" num="0137">The deep learning accelerator (<b>103</b>) can have local memory (<b>115</b>) and a control unit (<b>113</b>) in addition to the processing units (<b>111</b>). The control unit (<b>113</b>) can load instructions (<b>205</b>) and matrix operands (e.g., <b>207</b>) from the random access memory (<b>105</b>) for execution by the processing units (<b>111</b>). The local memory can cache matrix operands used by the matrix-matrix unit. The connection (<b>119</b>) can be configured with a bandwidth sufficient to load a set of matrix operands from the random access memory (<b>105</b>) to the local memory (<b>115</b>) during a time period in which the matrix-matrix unit performs operations on two other matrix operands. Further, during the time period, the bandwidth is sufficient to store a result, generated by the matrix-matrix unit (<b>121</b>) in a prior instruction execution, from the local memory (<b>115</b>) to the random access memory (<b>105</b>).</p><p id="p-0139" num="0138">The deep learning accelerator (<b>103</b>) and the random access memory (<b>105</b>) can be configured in a data storage device, such as a removable media, to calculate video analytics for compressing video data. For example, the removable media configured in a surveillance camera can calculate and use the video analytics to compress the video captured by its image sensor using a deep learning-based coding technique. Alternatively, or in combination, the video analytics can include inference results of identifications of items and/or events of interest such that the input video is compressed as a description of content recognized by an artificial neural network in the video and optionally representative images/clips of the recognized items and/or events.</p><p id="p-0140" num="0139">For example, the data storage device (e.g., in the form of a removable media) can receive a video stream from the image sensor of the surveillance camera for recording. In response, the data storage device automatically generates a compressed video file storing the video stream using a deep learning-based video coding technique and/or as a description of recognized content in the video stream.</p><p id="p-0141" num="0140">For example, a set of configuration parameters can be stored into the data storage device to control aspects of the compress of video files/streams provided to the data storage device. The configuration parameters can include identifications of attributes of the video files/streams to be compressed for storing in the data storage device, such as the format and resolution of the input video files/streams, the resolution of the compressed video file, the location in the data storage device to store the compressed video file, etc.</p><p id="p-0142" num="0141">After the configuration parameters are stored in the data storage device, the data storage device can automatically convert a video file/stream received from the image sensor and/or a controller of the surveillance camera into a compressed video file using a deep learning-based video coding technique.</p><p id="p-0143" num="0142">For example, the deep learning accelerator (<b>103</b>) executes the instructions (<b>205</b>) to perform the computation of the artificial neural network (<b>201</b>) that receives the video file/stream as the input (<b>211</b>) and generating the video analytics as the output (<b>213</b>); and a video encoder uses the video analytics to compress the video file/stream and generate a compressed video file that is stored in the data storage device. Subsequently, a computer system may retrieve the compressed video file for playing back the video content, or request the data storage device to decompress the video file for streaming from the data storage device in another format for playback.</p><p id="p-0144" num="0143">For example, the data storage device in a form of a removable media can have an interface to a reader (e.g., memory card reader) or a port (e.g., USB port). Alternatively, the data storage device can have a transceiver or a host interface for a wired or wireless communication connection to a separate computer system such as a surveillance camera, a personal computer, a set top box, a hub of internet of things (IoT), a server computer, a smartphone, a personal medial player, a mobile computer, a tablet computer, etc. The computer system may store a video into the data storage device, which uses a deep learning-based video coding technique to compress it for storage in the device.</p><p id="p-0145" num="0144">The video compression capability of the data storage device can be customized, updated, and/or upgraded via receiving, through the transceiver or host interface, and storing into the random access memory (<b>105</b>), the matrices (<b>207</b>) and instructions (<b>205</b>) of an artificial neural network (<b>201</b>).</p><p id="p-0146" num="0145">The artificial neural network (<b>201</b>), implemented via the deep learning accelerator (<b>103</b>) executing the instructions (<b>205</b>), provides analytics for the compression of the video received in the transceiver or host interface for storing in the data storage device. The compression improves the capability of the data storage device in storing video content, reduces the communication bandwidth to transmit the video content from the data storage device, and/or reduces the computation workloads of the computer system in which the data storage device is installed.</p><p id="p-0147" num="0146">Alternatively, or in combination, the deep learning accelerator (<b>103</b>) and the random access memory (<b>105</b>) configured in the data storage device can be configured to convert video/image data into a description of content recognized by an artificial neural network from the video/image data. For example, the surveillance camera having the data storage device can be used to monitor the condition and/or surrounding of a user or location to generate alerts and/or selectively store image data based on the description.</p><p id="p-0148" num="0147">For example, the data storage device can store the description with representative images/clips of recognized items and/or events as a version of the video stream from the image sensor of the surveillance camera. The surveillance camera can provide the version of the video stream to a computer system to implement a specific surveillance application. The description can provide identifications of recognized items or events that are identified by an artificial neural network (<b>201</b>). The separate computer system further processes the identifications to generate an alert, to provide a display, to selective request image data from the surveillance camera, and/or to selectively store image data.</p><p id="p-0149" num="0148">The surveillance camera is customizable, updatable, and/or upgradable via receiving and storing into the random access memory (<b>105</b>) the matrices (<b>207</b>) and instructions (<b>205</b>) of an artificial neural network (<b>201</b>).</p><p id="p-0150" num="0149">The artificial neural network (<b>201</b>), implemented via the deep learning accelerator (<b>103</b>) executing the instructions (<b>205</b>), converts the image stream from an image sensor into inference results. The conversion improves the quality of outputs of the surveillance camera, reduces the communication bandwidth requirement for the connection to the computer system, and/or reduces the computation workloads of the computer system.</p><p id="p-0151" num="0150"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates a technique to upgrade a surveillance camera (<b>231</b>) using a removable media (<b>191</b>) according to one embodiment.</p><p id="p-0152" num="0151">The surveillance camera (<b>231</b>) may not have the capability to perform the computation of an artificial neural network (<b>201</b>) when the removable media A (<b>192</b>) is in the surveillance camera (<b>231</b>).</p><p id="p-0153" num="0152">However, the surveillance camera (<b>231</b>) is configured to record a video in the removable media A (<b>192</b>).</p><p id="p-0154" num="0153">For example, the surveillance camera (<b>231</b>) has an image sensor (<b>233</b>) that can generate a video stream of a scene as seen through a lens (<b>239</b>) of the surveillance camera (<b>231</b>).</p><p id="p-0155" num="0154">Examples of image sensors include a charge coupled device (CCD) image sensor, and a complementary metal oxide semiconductor (CMOS) image sensor. The image sensor (<b>233</b>) can be formed on an integrated circuit die.</p><p id="p-0156" num="0155">A controller (<b>235</b>) of the surveillance camera (<b>231</b>) can control the operations of the surveillance camera (<b>231</b>) in response to commands received in a communication interface (<b>237</b>) over a wired or wireless connection (<b>228</b>) to a computer system (<b>223</b>).</p><p id="p-0157" num="0156">For example, the controller (<b>235</b>) can record the video stream into the removable media (<b>192</b>) in response to a command from the computer system (<b>223</b>) and provide the video stream from the removable media A (<b>192</b>) to the computer system (<b>223</b>) in response to another command from the computer system (<b>223</b>).</p><p id="p-0158" num="0157">In <figref idref="DRAWINGS">FIG. <b>6</b></figref>, a removable media B (<b>191</b>) is configured with a deep learning accelerator (DLA) (<b>103</b>) and random access memory (<b>105</b>). When the removable media A (<b>192</b>) is replaced with the removable media B (<b>191</b>), the surveillance camera (<b>231</b>) is upgraded to have the capability to perform the computation of an artificial neural network (<b>201</b>).</p><p id="p-0159" num="0158">Once the removable media B (<b>191</b>) is in the surveillance camera (<b>231</b>), the controller (<b>235</b>) can record a video/image stream generated by the image sensor (<b>233</b>) into the removable media B (<b>191</b>). In response to recording the video/image stream in the removable media (<b>191</b>), the deep learning accelerator (<b>103</b>) executes instructions (<b>205</b>) stored in the random access memory (<b>105</b>) to perform the computations of the artificial neural network (<b>201</b>) using the matrices (<b>207</b>) stored in the random access memory (<b>105</b>). The output of the artificial neural network (<b>201</b>) can be used to generate intelligent output usable by the computer system (<b>223</b>). The output can include a compressed video coded using a deep learning-based coding technique, and/or a description of content recognized by the artificial neural network (<b>201</b>) from the video.</p><p id="p-0160" num="0159">For example, the removable media (<b>191</b>) illustrated in <figref idref="DRAWINGS">FIGS. <b>7</b></figref> &#x2014; <b>9</b> can be used as the removable media B (<b>191</b>) to upgrade the surveillance camera (<b>231</b>).</p><p id="p-0161" num="0160"><figref idref="DRAWINGS">FIGS. <b>7</b></figref> &#x2014; <b>9</b> illustrate removable media (<b>191</b>) having a deep learning accelerator (<b>103</b>) and random access memory (<b>105</b>) configured according to some embodiments.</p><p id="p-0162" num="0161">A removable media (<b>191</b>) in <figref idref="DRAWINGS">FIG. <b>7</b>, <b>8</b></figref>, or <b>9</b> includes a video encoder (<b>215</b>). The video encoder (<b>215</b>) can be implemented using software executed by a general purpose processor, or using an encoder logic circuit. In some embodiments, an encoding-specific logic circuit is used to accelerate a portion of the encoding operations; and a remaining portion of the encoding operations is performed by executing a set of software instructions. Thus, the video encoders (<b>215</b>) in <figref idref="DRAWINGS">FIG. <b>7</b>, <b>8</b></figref>, or <b>9</b> are not limited to a specific hardware/software implementation.</p><p id="p-0163" num="0162">The operations of the video encoder (<b>215</b>) are based at least in part on the video analytics (<b>102</b>) in the output (<b>213</b>) of the artificial neural network (<b>201</b>) that receives the video (<b>104</b>) as input (<b>211</b>).</p><p id="p-0164" num="0163">The input video (<b>104</b>) can be initially received from an image sensor (<b>233</b>) of a surveillance camera (<b>231</b>) in an uncompressed format, in a format compressed using a lossless compression technique, or in a format compressed using a lossy compression technique. Configuration parameters (<b>221</b>) are stored into the random access memory (<b>105</b>) to identify the format of the input video (<b>104</b>) and/or desired compression operations to be performed on the input video (<b>104</b>).</p><p id="p-0165" num="0164">When the input video (<b>104</b>) is initially in a compressed format, the removable media (<b>191</b>) uses a corresponding decoder to generate the video content in an uncompressed format where the image attributes of individual pixels in individual frames are explicitly specified. When a video is in a compressed format, at least some pixels of some frames in the video are implicitly specified using data associated with other pixels and/or other frames. The decoder can be implemented in the removable media (<b>191</b>) using hardware and/or software.</p><p id="p-0166" num="0165">In some implementations, the decoder decompresses the input video (<b>104</b>) on the fly when the input video (<b>104</b>) is stored into the random access memory as the input (<b>211</b>) to the artificial neural network (<b>201</b>), or when the input video (<b>104</b>) is retrieved from the random access memory as the input (<b>211</b>) to the artificial neural network (<b>201</b>).</p><p id="p-0167" num="0166">The deep learning accelerator (<b>103</b>) executes the instructions (<b>205</b>) to generate the video analytics (<b>102</b>) of the input video (<b>104</b>).</p><p id="p-0168" num="0167">For example, depending on the deep learning-based coding technique used with video encoder (<b>215</b>), the video analytics (<b>102</b>) can include pixel probability model, intra-picture prediction, inter-picture prediction, cross-channel prediction, probability distribution prediction, transform, post-loop filtering, in-loop filtering, down-sampling, up-sampling, encoding optimization, etc.</p><p id="p-0169" num="0168">For example, the video analytics (<b>102</b>) can include identifications, classifications or categories of items, objects, persons, features and/or events shown in images in the input video (<b>104</b>). Further, the video analytics (<b>102</b>) in the output (<b>213</b>) of the artificial neural network (<b>201</b>) can include a location and size of an object, person, or feature recognized from the input video (<b>104</b>). The video encoder (<b>215</b>) can annotate the compressed video (<b>227</b>) using the information/description of the items, objects, persons, features and/or events shown in images in the input video (<b>104</b>).</p><p id="p-0170" num="0169">For example, the video analytics (<b>102</b>) can include representative images/clips of the recognized items, objects, persons, features and/or events shown in images in the input video (<b>104</b>).</p><p id="p-0171" num="0170">In some implementations, the video encoder (<b>215</b>) constructs and generates the compressed video (<b>227</b>) using the representative images/clips and annotate the compressed video (<b>227</b>) with the descriptions of the recognized items, objects, persons, features and/or events shown in images in the input video (<b>104</b>).</p><p id="p-0172" num="0171">For example, the output (<b>213</b>) of the artificial neural network (<b>201</b>) can include an identification of an event associated with the object, person, or feature; and a description coded into the compressed video (<b>227</b>) can include the identification of the event.</p><p id="p-0173" num="0172">Thus, using the video analytics (<b>102</b>) and/or the input video (<b>104</b>), the video encoder (<b>215</b>) generates a compressed video (<b>227</b>). The removable media (<b>191</b>) stores the compressed video (<b>227</b>) as a replacement of the input video (<b>104</b>).</p><p id="p-0174" num="0173">In one embodiment, the input video (<b>104</b>) is a portion of a video stream. After the input video (<b>104</b>) is compressed and stored, another portion of the video stream can be stored into the random access memory (<b>105</b>) in the portion of the random access memory (<b>105</b>) that was previously occupied by the input video (<b>104</b>). Thus, it is not necessary to store the entire uncompressed version of the compressed video (<b>227</b>) in the random access memory (<b>105</b>) at the same time.</p><p id="p-0175" num="0174">In some implementations, the removable media (<b>191</b>) is configured to generate the compressed video (<b>227</b>) on the fly in real time with the streaming of video into the removable media (<b>191</b>). For example, while the input video (<b>104</b>) is being analyzed to generate the video analytics (<b>102</b>), a subsequent portion of the video stream is stored into the random access memory (<b>105</b>) as the next input to the artificial neural network (<b>201</b>). After the subsequent portion of the video stream is received in the removable media (<b>191</b>), the video encoder (<b>215</b>) completes the generation of the compressed video (<b>227</b>) for the input video (<b>104</b>) using the video analytics (<b>102</b>). Thus, the deep learning accelerator (<b>103</b>) can re-execute the instructions to analyze the subsequent portion of the video stream for the video encoder (<b>215</b>) to compress the subsequent portion, while a further portion is being received in the removable media (<b>191</b>) and/or the random access memory (<b>105</b>).</p><p id="p-0176" num="0175">In <figref idref="DRAWINGS">FIG. <b>7</b></figref> an integrated circuit device (<b>101</b>) includes not only a deep learning accelerator (<b>103</b>) and random access memory (<b>105</b>) but also a controller (<b>127</b>), and the logic circuit of a host interface (<b>106</b>) for a wired or wireless connection to the surveillance camera (<b>231</b>) and/or the computer system (<b>223</b>).</p><p id="p-0177" num="0176">In some embodiments, the host interface (<b>106</b>) includes an interface to a memory card reader. The removable media (<b>191</b>) can be configured as a memory card.</p><p id="p-0178" num="0177">In some embodiments, the host interface (<b>106</b>) includes an interface to a plug and play port, such as a universal serial bus (USB) port. The removable media (<b>191</b>) can be configured as a USB drive.</p><p id="p-0179" num="0178">In some embodiments, the host interface (<b>106</b>) includes a transceiver for wired or wireless communications, such as a local area network transceiver, a wireless personal area network transceiver (e.g., a bluetooth transceiver), or a wireless local area network transceiver (e.g., a Wi-Fi transceiver). The removable media (<b>191</b>) can be configured as a network storage drive.</p><p id="p-0180" num="0179">For example, the controller (<b>127</b>), and the host interface (<b>106</b>) can be formed on an integrated circuit die that is stacked on and connected to the integrated circuit die(s) of the random access memory (<b>105</b>). Thus, the video data received in the host interface (<b>106</b>) can be stored by the controller (<b>127</b>) into the random access memory (<b>105</b>) as the input (<b>211</b>) to the artificial neural network (<b>201</b>) implemented using the deep learning accelerator (<b>103</b>).</p><p id="p-0181" num="0180">The integrated circuit device (<b>101</b>) of <figref idref="DRAWINGS">FIG. <b>7</b></figref> has a controller (<b>127</b>) that is configured to control the operations of the host interface (<b>106</b>), the video encoder (<b>215</b>), and/or the deep learning accelerator (<b>103</b>).</p><p id="p-0182" num="0181">The controller (<b>127</b>) can be implemented, for example, using a microcontroller or a sequencer that controls the timing of the operations of loading the input video (<b>104</b>) into the random access memory (<b>105</b>) and the generation of the compressed video (<b>227</b>) from the input video (<b>104</b>) and the video analytics (<b>102</b>).</p><p id="p-0183" num="0182">Optionally, the controller (<b>127</b>) can be implemented using a microprocessor that runs an application stored in the random access memory (<b>105</b>) as firmware to coordinate the operations among the video encoder (<b>215</b>), the random access memory (<b>105</b>), the deep learning accelerator (<b>103</b>), and/or a host interface (<b>106</b>).</p><p id="p-0184" num="0183">After a set of frames of input video (<b>104</b>) is stored into the random access memory (<b>105</b>) as the input (<b>211</b>) to the artificial neural network (<b>201</b>), the controller (<b>127</b>) can cause the deep learning accelerator (<b>103</b>) to execute the instructions (<b>205</b>) and generate the video analytics (<b>102</b>) as the output (<b>213</b>) of the artificial neural network (<b>201</b>).</p><p id="p-0185" num="0184">For example, the controller (<b>127</b>) can instruct the deep learning accelerator (<b>103</b>) to start the execution of the instructions (<b>205</b>) by writing the address of the input (<b>211</b>) at a predefined location in the random access memory (<b>105</b>). When the deep learning accelerator (<b>103</b>) is in an idle state, the deep learning accelerator (<b>103</b>) can periodically read the address stored at the predefined location in the random access memory (<b>105</b>). When a new and/or valid address is retrieved from the predefined location, the deep learning accelerator (<b>103</b>) starts the execution of the instructions (<b>205</b>). Optionally, after starting the execution of the instructions (<b>205</b>), the deep learning accelerator (<b>103</b>) can optionally clear, erase or invalidate the address previously stored at the predefined location in the random access memory (<b>105</b>).</p><p id="p-0186" num="0185">Alternatively, the controller (<b>127</b>) is configured to send a signal or a message to the deep learning accelerator (<b>103</b>) to instruct the deep learning accelerator (<b>103</b>) to execute the instructions (<b>205</b>). The signal or a message can be transmitted from the controller (<b>127</b>) to the deep learning accelerator (<b>103</b>) using a direct connection that does not go through the memory cells of the random access memory (<b>105</b>).</p><p id="p-0187" num="0186">In some implementations, the controller (<b>127</b>) and the deep learning accelerator (<b>103</b>) have separate connections (<b>109</b> and <b>119</b>) to the random access memory (<b>105</b>). When the controller (<b>127</b>) and the deep learning accelerator (<b>103</b>) are not accessing a same block or address of the random access memory (<b>105</b>), the connections (<b>109</b> and <b>119</b>) can be used by the controller (<b>127</b>) and the deep learning accelerator (<b>103</b>) in parallel to access different portions of the random access memory (<b>105</b>) simultaneously.</p><p id="p-0188" num="0187">In other implementations, the control unit (<b>113</b>) and the controller (<b>127</b>) can share at least a portion of their circuity in the deep learning accelerator (<b>103</b>) and use the same memory interface (<b>117</b>) to access the random access memory (<b>105</b>).</p><p id="p-0189" num="0188">A portion of the processing units (<b>111</b>) can be implemented using neuromorphic memory (<b>225</b>). For example, the neuromorphic memory (<b>225</b>) can include a crossbar array of memristors configured to perform multiply-and-accumulate (MAC) operations via analog circuitry. For example, a multiply-accumulate units (e.g., <b>171</b> or <b>173</b>) in a vector-vector unit (e.g., <b>161</b>) of the deep learning accelerator (<b>103</b>) can be implemented using a crossbar array of memristors. The memristors can be connected in an array with wordlines and bitlines configured to address the memristors as memory cells. A typical memristor is connected to one of the wordlines and one of the bitlines in the array. Electric currents going through the wordlines through a set of memristors in the crossbar array to a bitline are summed in the bitline, which corresponds to the accumulation operation. The electric currents correspond to the multiplication of the voltages applied on the wordlines and parameters associated with the resistances of the memristors, which corresponds to the multiplication operations. The current in the bitline can be compared with a threshold to determine whether a neuron represented by the bitline is activated under the current input. An array of memristors can be connected to the bitlines respectively and programmed to have thresholds corresponding to the activation level thresholds of the neurons. A current detector can be configured for each memristor connected to the output of a bitline to determine whether the level of electric current in the bitline corresponding to a level that exceeds the threshold of the memristor. The neuromorphic memory (<b>225</b>) can perform the multiply-and-accumulate (MAC) operations in a way similar to a memory device reading an array of memory cells and thus with low energy cost and high computation speed.</p><p id="p-0190" num="0189">Through a connection (<b>108</b>) the controller (<b>127</b>) operates the host interface (<b>106</b>) of the integrated circuit device (<b>101</b>) of <figref idref="DRAWINGS">FIG. <b>7</b></figref> to communicate with a surveillance camera (<b>231</b>) and/or a separate computer system (<b>223</b>) through a wired or wireless connection.</p><p id="p-0191" num="0190">For example, the host interface (<b>106</b>) can be configured to communicate according to a communication protocol for a memory card interface, a universal serial bus (USB), a peripheral component interconnect (PCI) bus, a PCI express (PCIe) bus, a local area network, a peripheral bus, a mobile industry processor interface (MIPI), a wireless personal area network or a wireless local area network, or a communication protocol of internet of things (IoTs). For example, the host interface (<b>106</b>) can be formed on a radio frequency (RF) complementary metal oxide semiconductor (CMOS) integrated circuit chip.</p><p id="p-0192" num="0191">The host interface (<b>106</b>) can be used by the removable media (<b>191</b>) to receive data and/or instructions from the surveillance camera (<b>231</b>) and/or the computer system (<b>223</b>), such as the configuration parameters (<b>221</b>), the matrices (<b>207</b>) and the instructions (<b>205</b>) of the artificial neural network (<b>201</b>). The host interface (<b>106</b>) can be used by the removable media (<b>191</b>) to provide the compressed video (<b>227</b>) to the computer system (<b>223</b>), or another device.</p><p id="p-0193" num="0192">In some implementations, the host interface (<b>106</b>) includes an interface for receiving a video stream from the surveillance camera (<b>231</b>) for recording, and another interface for direct communication with the computer system (<b>223</b>) without going through the communication interface (<b>237</b>). Thus, the computer system (<b>223</b>) can use the additional interface to access the random access memory (<b>105</b>) in a way that may not be supported by the communication interface (<b>237</b>) of the surveillance camera (<b>231</b>). For example, the computer system (<b>223</b>) can use the additional interface to update the matrices (<b>207</b>), DLA instructions (<b>205</b>), configuration parameters (<b>221</b>), and/or the video encoder (<b>215</b>) in the random access memory (<b>105</b>) without going through the communication interface (<b>237</b>) of the surveillance camera (<b>231</b>). For example, the computer system (<b>223</b>) can use the additional interface to access a portion of the compressed video (<b>227</b>), the input video (<b>104</b>) and/or the video analytics (<b>102</b>).</p><p id="p-0194" num="0193">For example, the computer system (<b>223</b>) can request the removable media (<b>191</b>) and/or the surveillance camera (<b>231</b>) to stream the compressed video (<b>227</b>) to the computer system (<b>223</b>) or another device for playback. Optionally, the controller (<b>127</b>) can decompress the compressed video (<b>227</b>) and stream the video in a resolution and/or format identified in the configuration parameters (<b>221</b>).</p><p id="p-0195" num="0194">In some implementations, the control unit (<b>113</b>) of the deep learning accelerator (<b>103</b>) can include the controller (<b>127</b>); and the logic circuit of the host interface (<b>106</b>) can be implemented on the integrated circuit die of the deep learning accelerator (<b>103</b>), as illustrated in <figref idref="DRAWINGS">FIG. <b>8</b></figref>.</p><p id="p-0196" num="0195">In <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the deep learning accelerator (<b>103</b>) is configured on an integrated circuit die; and the random access memory (<b>105</b>) is configured on one or more integrated circuit dies. The control unit (<b>113</b>) controls not only the execution of the instructions (<b>205</b>) of the artificial neural network (<b>201</b>), but also the communications of the host interface (<b>106</b>) with the surveillance camera (<b>231</b>) and/or the computer system (<b>223</b>) and the operations of the video encoder (<b>215</b>).</p><p id="p-0197" num="0196">For example, the control unit (<b>113</b>) controls the host interface (<b>106</b>) to receive a video stream from the surveillance camera (<b>231</b>) and stores the input video (<b>104</b>) into the random access memory (<b>105</b>) through the high bandwidth connection (<b>119</b>).</p><p id="p-0198" num="0197">In some implementations, a portion of the video encoder (<b>215</b>) is implemented using logic circuit that is enclosed in an integrated circuit package of an integrated circuit device (<b>101</b>) (e.g., as illustrated in <figref idref="DRAWINGS">FIG. <b>7</b></figref>).</p><p id="p-0199" num="0198">Alternatively, the logic circuit of the video encoder (<b>215</b>) can be a separate component (e.g., an integrated circuit chip) that is outside of an integrated circuit package that encloses the deep learning accelerator (<b>103</b>) and the random access memory (<b>105</b>).</p><p id="p-0200" num="0199">For example, an integrated circuit chip of the video encoder (<b>215</b>), and an integrated circuit device (<b>101</b>) having the deep learning accelerator (<b>103</b>) and the random access memory (<b>105</b>) can be mounted on a printed circuit board configured in the removable media (<b>191</b>).</p><p id="p-0201" num="0200">The removable media (<b>191</b>) of <figref idref="DRAWINGS">FIG. <b>9</b></figref> has a substrate (<b>229</b>) that provides connections among its components, such as a deep learning accelerator (<b>103</b>), random access memory (<b>105</b>), a video encoder (<b>215</b>), a controller (<b>127</b>), and a host interface (<b>106</b>).</p><p id="p-0202" num="0201">In some implementations, the substrate (<b>229</b>) includes an integrated circuit die having wires for connecting the components. Some of the components (e.g., the integrated circuit die(s) of the random access memory (<b>105</b>), the deep learning accelerator (<b>103</b>), the controller (<b>127</b>), the video encoder (<b>215</b>), and/or the host interface (<b>106</b>)) can be connected to the integrated circuit die of the substrate (<b>229</b>) via through silicon vias (TSVs). Other components can be connected to the substrate (<b>229</b>) via wire bonding, die attach, or another technique.</p><p id="p-0203" num="0202">In some implementations, the substrate (<b>229</b>) further includes a printed circuit board having wires for connecting the components and other components, such as a power source (e.g., battery), a display, a light-emitting diode (LED) indicator, etc.</p><p id="p-0204" num="0203">In some implementations, the circuits of the host interface (<b>106</b>), the controller (<b>127</b>), the video encoder (<b>215</b>) are integrated in a same integrated circuit chip; and the output of the integrated circuit chip includes a video stream from the surveillance camera (<b>231</b>) to the random access memory (<b>105</b>). In other implementations, the host interface (<b>106</b>) is separate from the integrated circuit chip of the controller (<b>127</b>) and/or the video encoder (<b>215</b>).</p><p id="p-0205" num="0204">In some implementations, the logic circuit of the host interface (<b>106</b>) and/or the controller (<b>127</b>) are configured on the integrated circuit die of the deep learning accelerator (<b>103</b>), or another integrated circuit die.</p><p id="p-0206" num="0205"><figref idref="DRAWINGS">FIG. <b>10</b></figref> shows a method implemented in a surveillance camera according to one embodiment. For example, the method of <figref idref="DRAWINGS">FIG. <b>9</b></figref> can be implemented in a surveillance camera of <figref idref="DRAWINGS">FIG. <b>6</b></figref> with removable media (<b>191</b>) of <figref idref="DRAWINGS">FIG. <b>7</b></figref>, <figref idref="DRAWINGS">FIG. <b>8</b></figref>, or <figref idref="DRAWINGS">FIG. <b>9</b></figref>.</p><p id="p-0207" num="0206">At block <b>301</b>, a removable data storage device (<b>191</b>) is provided to have random access memory (<b>105</b>) and at least one processing unit (<b>111</b>) configured to perform matrix operations.</p><p id="p-0208" num="0207">For example, the removable data storage device can be a memory card, a universal serial bus (USB) drive, a solid state drive (SSD), a network storage device, etc.</p><p id="p-0209" num="0208">For example, the removable storage media can be a plug and play device that is connectable to a computer system (<b>223</b>) for normal operation and dis-connectable from the computer system (<b>223</b>) without restarting the computer system (<b>223</b>).</p><p id="p-0210" num="0209">For example, the at least one processing unit (<b>111</b>) can be formed on an integrated circuit die of a field-programmable gate array (FPGA) or application specific integrated circuit (ASIC) implementing a deep learning accelerator (<b>103</b>). The deep learning accelerator (<b>103</b>) can include the at least one processing unit (<b>111</b>) for matrix instruction execution, local memory (<b>115</b>) to buffer matrix operands and results, a control unit (<b>113</b>) that can load the instructions (<b>205</b>) from random access memory (<b>105</b>) for execution, and a memory interface (<b>117</b>) to access the random access memory (<b>105</b>).</p><p id="p-0211" num="0210">For example, an integrated circuit package configured to enclose at least the integrated circuit die of FPGA or ASIC and one or more integrated circuit dies of the random access memory. In some implementations, the integrated circuit package further encloses the controller and/or the host interface (<b>106</b>).</p><p id="p-0212" num="0211">For example, the random access memory (<b>105</b>) and the deep learning accelerator (<b>103</b>) are formed on separate integrated circuit dies and connected by through-silicon vias (TSVs).</p><p id="p-0213" num="0212">At block <b>303</b>, matrices (<b>207</b>) of an artificial neural network (<b>201</b>) and instructions (<b>205</b>) executable to implement computations of the artificial neural network (<b>201</b>) using the matrices (<b>207</b>) are stored into the random access memory (<b>105</b>) of the removable data storage device (<b>191</b>).</p><p id="p-0214" num="0213">For example, the matrices (<b>207</b>) and the instructions (<b>205</b>) can be stored into the removable data storage device (<b>191</b>) before the removable data storage device (<b>191</b>) is installed into a surveillance camera (e.g., <b>231</b>).</p><p id="p-0215" num="0214">For example, the removable data storage device (<b>191</b>) can be connected to a computer system (<b>223</b>) to store the matrices (<b>207</b>) and the instructions (<b>205</b>) in preparation of the upgrading of the surveillance camera (e.g., <b>231</b>). To upgrade the surveillance camera (<b>231</b>), an existing removable media (e.g., <b>192</b>) can be removed from the surveillance camera (<b>231</b>); and then, the removable data storage device (<b>191</b>) having the deep learning accelerator (<b>103</b>) and the random access memory (<b>105</b>) storing the matrices (<b>207</b>) and the instructions (<b>205</b>) can be connected to the surveillance camera (<b>231</b>) as a replacement of the previously installed removable media (<b>192</b>).</p><p id="p-0216" num="0215">At block <b>305</b>, in response to the removable data storage device (<b>191</b>) being detected in the surveillance camera (<b>231</b>), the surveillance camera (<b>231</b>) configures the removable data storage device (<b>191</b>) as part of the storage capacity of the surveillance camera (<b>231</b>).</p><p id="p-0217" num="0216">For example, the surveillance camera (<b>231</b>) can store a video stream into the removable data storage device (<b>191</b>) in a way same as recording a video stream into the previously installed removable media (<b>192</b>) that does not have a deep learning accelerator (<b>103</b>).</p><p id="p-0218" num="0217">At block <b>307</b>, a host interface (<b>106</b>) of the removable data storage device (<b>191</b>) received a portion of a video stream (e.g., <b>104</b>) from the surveillance camera (<b>231</b>). The removable media (<b>191</b>) can store the video stream in its random access memory (<b>105</b>) in a cyclic way where an oldest portion is erased and replaced with a newest portion of the video stream.</p><p id="p-0219" num="0218">For example, the host interface (<b>106</b>) can be configured to communicate with a host (e.g., the surveillance camera (<b>231</b>), the computer system (<b>223</b>)) in accordance with a protocol of a memory card interface, a universal serial bus (USB), a peripheral component interconnect (PCI) bus, a PCI express (PCIe) bus, a peripheral bus, a local area network, a mobile industry processor interface (MIPI), a wireless personal area network, a wireless local area network, or internet of things (IoTs), or any combination thereof.</p><p id="p-0220" num="0219">At block <b>309</b>, at least one processing unit (<b>111</b>) of the removable data storage device (<b>191</b>) executes the instructions (<b>205</b>) to compute output (<b>213</b>) from the artificial neural network (<b>201</b>) that has the portion of the video stream (e.g., <b>104</b>) as the input (<b>211</b>) to the artificial neural network (<b>201</b>). The output (<b>213</b>) can include video analytics (<b>102</b>) determined by the artificial neural network (<b>201</b>) from the portion of the video stream.</p><p id="p-0221" num="0220">At block <b>311</b>, the removable data storage device (<b>191</b>) and/or the surveillance camera (<b>231</b>) can provide, based on the video analytics, input to a computer system (<b>223</b>) that is connected to the surveillance camera (<b>231</b>).</p><p id="p-0222" num="0221">For example, the input provided to the computer system (<b>223</b>) can include a compressed video (<b>227</b>) generated based on the video analytics (<b>102</b>) and/or the portion of the video stream (e.g., <b>104</b>).</p><p id="p-0223" num="0222">For example, the compressed video (<b>227</b>) can be provided to the computer system (<b>223</b>) via the communication interface (<b>237</b>) of the surveillance camera (<b>231</b>) in a way similar to the computer system (<b>223</b>) retrieving a video in the previously installed removable media (<b>192</b>) that does not have a deep learning accelerator (<b>103</b>).</p><p id="p-0224" num="0223">Alternatively, the removable data storage device (<b>191</b>) can include an additional interface for communicating to the computer system (<b>223</b>) directly without going through the existing communication interface (<b>237</b>) of the surveillance camera (<b>231</b>). The additional interface can facilitate a way of accessing the data (e.g., video analytics (<b>102</b>) and/or the input video (<b>104</b>)) in the random access memory (<b>105</b>) that is not supported by the communication interface (<b>237</b>) of the surveillance camera (<b>231</b>).</p><p id="p-0225" num="0224">The content of the compressed video (<b>227</b>) can be substantially the same as the video stream from the image sensor (<b>233</b>) of the surveillance camera (<b>231</b>). For example, a deep learning-based video coding technique implemented based on the video analytics (<b>102</b>) can be used to compress the video stream and generate the compressed video (<b>227</b>).</p><p id="p-0226" num="0225">For example, the removable data storage device (<b>191</b>) can include a video encoder configured to code and generate the compressed video (<b>227</b>) based on the video analytics (<b>102</b>) identifying a pixel probability model, an intra-picture prediction, an inter-picture prediction, a cross-channel prediction, a probability distribution prediction, a transform, a post-loop filtering option, an in-loop filtering option, a down-sampling option, an up-sampling option, or an encoding optimization option, or any combination thereof.</p><p id="p-0227" num="0226">Alternatively, the content of the compressed video (<b>227</b>) can be selectively generated from the video stream captured by the image sensor (<b>233</b>) of the surveillance camera (<b>231</b>), based on what is recognized by the artificial neural network (<b>201</b>) from the video stream.</p><p id="p-0228" num="0227">For example, the video encoder (<b>215</b>) can extract images or clips from the portion of the video stream (e.g., <b>104</b>) based on the video analytics (<b>102</b>) identifying an object, person, or feature recognized from the portion of the video stream (e.g., <b>104</b>), and a location and size of the object, person, or feature. The video encoder (<b>215</b>) can construct the compressed video (<b>227</b>) using the images or clips extracted from the portion of the video stream (e.g., <b>104</b>) and thus discard the remaining video content of the video stream from the image sensor (<b>233</b>).</p><p id="p-0229" num="0228">Optionally, the video encoder (<b>215</b>) can generate a description of content in the portion of the video stream (e.g., <b>104</b>) based on the video analytics (<b>102</b>) identifying an identification, a classification, or a category of an object, person, or feature recognized by the artificial neural network (<b>201</b>) from the portion of the video stream (e.g., <b>104</b>), and a location and size of the object, person, or feature. The video encoder (<b>215</b>) can annotate the compressed video (<b>227</b>) according to the description.</p><p id="p-0230" num="0229">In some implementations, the video encoder (<b>215</b>) uses the description of the content of the portion of the video stream (e.g., <b>104</b>) as the compressed video (<b>227</b>) and as a replacement of the portion of the video stream (e.g., <b>104</b>). For example, the removable media (<b>191</b>) can provide the description to the computer system (<b>223</b>) without the images or clips extracted from the portion of the video stream (e.g., <b>104</b>) to show the object, person, or feature recognized by the artificial neural network (<b>201</b>). Optionally, the computer system (<b>223</b>) can optionally request the removable media (<b>191</b>) to provide the images or clips extracted from the portion of the video stream (e.g., <b>104</b>). In some implementations, the configuration parameters (<b>221</b>) determines whether the removable data storage device (<b>191</b>) provides the compressed video (<b>227</b>) coded using a deep learning-based video coding technique (with or without the description of the content), or the compressed video (<b>227</b>) reconstructed from images/clips of recognized objects, persons, or features that are of interest in a particular surveillance application (with or without the description of the content).</p><p id="p-0231" num="0230">In some implementations, the video encoder (<b>215</b>) is implemented at least in part by a controller (<b>127</b>) executing software instructions.</p><p id="p-0232" num="0231">The video analytics (<b>102</b>) can be generated for the portion of the video stream (e.g., input video (<b>104</b>)) and used to compress the portion (<b>104</b>) during a time period in which a further portion of the video stream is received in the host interface (<b>106</b>). Thus, the compression of the video stream is performed on the fly during the streaming of the video stream from the image sensor (<b>233</b>) of the surveillance camera (<b>231</b>) computer system (<b>223</b>) to the host interface (<b>106</b>).</p><p id="p-0233" num="0232">For example, while a first portion of the video stream is being analyzed using the artificial neural network (<b>201</b>) to generate the video analytics (<b>102</b>) that is then used to compress the first portion, a second portion of the video stream is received in the host interface (<b>106</b>) from the image sensor (<b>233</b>) of the surveillance camera (<b>231</b>) and buffered into the random access memory (<b>105</b>). The compression of the first portion is completed during the streaming of the second portion. After the second portion is buffered in the random access memory (<b>105</b>), the deep learning accelerator (<b>103</b>) executes the instructions (<b>205</b>) again to generate the video analytics (<b>102</b>) of the second portion, while a third portion is received in the host interface (<b>106</b>) from the image sensor (<b>233</b>) of the surveillance camera (<b>231</b>). Since the first portion has been stored as the compressed video, the storage capacity in the random access memory (<b>105</b>) previously used to store/buffer the first portion can be freed to store/buffer the third portion.</p><p id="p-0234" num="0233">The present disclosure includes methods and apparatuses which perform the methods described above, including data processing systems which perform these methods, and computer readable media containing instructions which when executed on data processing systems cause the systems to perform these methods.</p><p id="p-0235" num="0234">A typical data processing system may include an inter-connect (e.g., bus and system core logic), which interconnects a microprocessor(s) and memory. The microprocessor is typically coupled to cache memory.</p><p id="p-0236" num="0235">The inter-connect interconnects the microprocessor(s) and the memory together and also interconnects them to input/output (I/O) device(s) via I/O controller(s). I/O devices may include a display device and/or peripheral devices, such as mice, keyboards, modems, network interfaces, printers, scanners, video cameras and other devices known in the art. In one embodiment, when the data processing system is a server system, some of the I/O devices, such as printers, scanners, mice, and/or keyboards, are optional.</p><p id="p-0237" num="0236">The inter-connect can include one or more buses connected to one another through various bridges, controllers and/or adapters. In one embodiment the I/O controllers include a universal serial bus (USB) adapter for controlling USB peripherals, and/or an IEEE-<b>1394</b> bus adapter for controlling IEEE-<b>1394</b> peripherals.</p><p id="p-0238" num="0237">The memory may include one or more of: read only memory (ROM), volatile random access memory (RAM), and non-volatile memory, such as hard drive, flash memory, etc.</p><p id="p-0239" num="0238">Volatile RAM is typically implemented as dynamic RAM (DRAM) which requires power continually in order to refresh or maintain the data in the memory. Non-volatile memory is typically a magnetic hard drive, a magnetic optical drive, an optical drive (e.g., a DVD RAM), or other type of memory system which maintains data even after power is removed from the system. The non-volatile memory may also be a random access memory.</p><p id="p-0240" num="0239">The non-volatile memory can be a local device coupled directly to the rest of the components in the data processing system. A non-volatile memory that is remote from the system, such as a network storage device coupled to the data processing system through a network interface such as a modem or ethernet interface, can also be used.</p><p id="p-0241" num="0240">In the present disclosure, some functions and operations are described as being performed by or caused by software code to simplify description. However, such expressions are also used to specify that the functions result from execution of the code/instructions by a processor, such as a microprocessor.</p><p id="p-0242" num="0241">Alternatively, or in combination, the functions and operations as described here can be implemented using special purpose circuitry, with or without software instructions, such as using application-specific integrated circuit (ASIC) or field-programmable gate array (FPGA). Embodiments can be implemented using hardwired circuitry without software instructions, or in combination with software instructions. Thus, the techniques are limited neither to any specific combination of hardware circuitry and software, nor to any particular source for the instructions executed by the data processing system.</p><p id="p-0243" num="0242">While one embodiment can be implemented in fully functioning computers and computer systems, various embodiments are capable of being distributed as a computing product in a variety of forms and are capable of being applied regardless of the particular type of machine or computer-readable media used to actually effect the distribution.</p><p id="p-0244" num="0243">At least some aspects disclosed can be embodied, at least in part, in software. That is, the techniques may be carried out in a computer system or other data processing system in response to its processor, such as a microprocessor, executing sequences of instructions contained in a memory, such as ROM, volatile RAM, non-volatile memory, cache or a remote storage device.</p><p id="p-0245" num="0244">Routines executed to implement the embodiments may be implemented as part of an operating system or a specific application, component, program, object, module or sequence of instructions referred to as &#x201c;computer programs.&#x201d; The computer programs typically include one or more instructions set at various times in various memory and storage devices in a computer, and that, when read and executed by one or more processors in a computer, cause the computer to perform operations necessary to execute elements involving the various aspects.</p><p id="p-0246" num="0245">A machine readable medium can be used to store software and data which when executed by a data processing system causes the system to perform various methods. The executable software and data may be stored in various places including for example ROM, volatile RAM, non-volatile memory and/or cache. Portions of this software and/or data may be stored in any one of these storage devices. Further, the data and instructions can be obtained from centralized servers or peer to peer networks. Different portions of the data and instructions can be obtained from different centralized servers and/or peer to peer networks at different times and in different communication sessions or in a same communication session. The data and instructions can be obtained in entirety prior to the execution of the applications. Alternatively, portions of the data and instructions can be obtained dynamically, just in time, when needed for execution. Thus, it is not required that the data and instructions be on a machine readable medium in entirety at a particular instance of time.</p><p id="p-0247" num="0246">Examples of computer-readable media include but are not limited to non-transitory, recordable and non-recordable type media such as volatile and non-volatile memory devices, read only memory (ROM), random access memory (RAM), flash memory devices, floppy and other removable disks, magnetic disk storage media, optical storage media (e.g., compact disk read-only memory (CD ROM), digital versatile disks (DVDs), etc.), among others. The computer-readable media may store the instructions.</p><p id="p-0248" num="0247">The instructions may also be embodied in digital and analog communication links for electrical, optical, acoustical or other forms of propagated signals, such as carrier waves, infrared signals, digital signals, etc. However, propagated signals, such as carrier waves, infrared signals, digital signals, etc. are not tangible machine readable medium and are not configured to store instructions.</p><p id="p-0249" num="0248">In general, a machine readable medium includes any mechanism that provides (i.e., stores and/or transmits) information in a form accessible by a machine (e.g., a computer, network device, personal digital assistant, manufacturing tool, any device with a set of one or more processors, etc.).</p><p id="p-0250" num="0249">In various embodiments, hardwired circuitry may be used in combination with software instructions to implement the techniques. Thus, the techniques are neither limited to any specific combination of hardware circuitry and software nor to any particular source for the instructions executed by the data processing system.</p><p id="p-0251" num="0250">The above description and drawings are illustrative and are not to be construed as limiting. Numerous specific details are described to provide a thorough understanding. However, in certain instances, well known or conventional details are not described in order to avoid obscuring the description. References to one or an embodiment in the present disclosure are not necessarily references to the same embodiment; and, such references mean at least one.</p><p id="p-0252" num="0251">In the foregoing specification, the disclosure has been described with reference to specific exemplary embodiments thereof. It will be evident that various modifications may be made thereto without departing from the broader spirit and scope as set forth in the following claims. The specification and drawings are, accordingly, to be regarded in an illustrative sense rather than a restrictive sense.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A device, comprising:<claim-text>a removable storage media, having:<claim-text>a host interface operable to receive commands to write video data into the removable storage media;</claim-text><claim-text>a memory operable to store the video data as an input to an artificial neural network; and</claim-text><claim-text>at least one processing unit coupled with the memory to perform computations of the artificial neural network to generate an output of the artificial neural network responsive to the input;</claim-text></claim-text><claim-text>wherein the device is configured to provide input data to a computer system based on the output.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the memory is further configured to store data representative of weights of the artificial neural network and data representative of instructions having matrix operand and executable by the at least one processing unit to perform the computations of the artificial neural network.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>a digital camera connected to the host interface to write the video data into the removable storage media.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The device of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the output includes analytics of the video data.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The device of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the input data to the computer system includes a description of content in the video data.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The device of <claim-ref idref="CLM-00004">claim 4</claim-ref>, further comprising:<claim-text>a video encoder coupled to the memory and configured to generate a compressed video based on the video data and the analytics.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The device of <claim-ref idref="CLM-00006">claim 6</claim-ref>, configured to store the compressed video as a replacement of the video data.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The device of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the removable storage media is configured in a form of a solid state drive, a memory card, a network storage device, or a universal serial bus (USB) drive.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The device of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the at least one processing unit includes a matrix-matrix unit configured to operate on two matrix operands of an instruction;<claim-text>wherein the matrix-matrix unit includes a plurality of matrix-vector units configured to operate in parallel;</claim-text><claim-text>wherein each of the plurality of matrix-vector units includes a plurality of vector-vector units configured to operate in parallel;</claim-text><claim-text>wherein each of the plurality of vector-vector units includes a plurality of multiply-accumulate units configured to operate in parallel; and</claim-text><claim-text>wherein each of the plurality of multiply-accumulate units includes neuromorphic memory configured to perform multiply-accumulate operations via analog circuitry.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The device of <claim-ref idref="CLM-00009">claim 9</claim-ref>, further comprising:<claim-text>an integrated circuit package configured to enclose at least the memory and the at least one processing unit.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. A method, comprising:<claim-text>receiving, in a host interface of a removable storage media, commands to write video data into the removable storage media;</claim-text><claim-text>storing, in a memory of the removable storage media, the video data as an input to an artificial neural network;</claim-text><claim-text>performing, by at least one processing unit of the device, computations of the artificial neural network to generate an output of the artificial neural network responsive to the input; and</claim-text><claim-text>providing, by the device, input data to a computer system based on the output.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, further comprising:<claim-text>storing, into the memory, data representative of weights of the artificial neural network and data representative of instructions having matrix operand and executable by the at least one processing unit to perform the computations of the artificial neural network.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, further comprising:<claim-text>writing, by a digital camera connected to the host interface, the video data into the removable storage media.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the output includes analytics of the video data; and the input data to the computer system includes a description of content in the video data.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref>, further comprising:<claim-text>generating, by a video encoder of the device, a compressed video based on the video data and the analytics; and</claim-text><claim-text>storing the compressed video as a replacement of the video data.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. A non-transitory computer storage medium storing instructions which, when executed in a device, cause the device to perform a method, the method comprising:<claim-text>writing, through a host interface of a removable storage media, data into the removable storage media;</claim-text><claim-text>storing, in a memory of the removable storage media, the video data as an input to an artificial neural network;</claim-text><claim-text>performing, by at least one processing unit of the device, computations of the artificial neural network to generate an output of the artificial neural network responsive to the input; and</claim-text><claim-text>providing, by the device, input data to a computer system based on the output.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The non-transitory computer storage medium of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the method further comprises:<claim-text>storing, into the memory, data representative of weights of the artificial neural network and data representative of instructions having matrix operand and executable by the at least one processing unit to perform the computations of the artificial neural network.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The non-transitory computer storage medium of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the method further comprises:<claim-text>writing, by a digital camera connected to the host interface, the video data into the removable storage media.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The non-transitory computer storage medium of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the output includes analytics of the video data.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The non-transitory computer storage medium of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein the method further comprises:<claim-text>generating, by a video encoder of the device, a compressed video based on the video data and the analytics; and</claim-text><claim-text>storing the compressed video as a replacement of the video data.</claim-text></claim-text></claim></claims></us-patent-application>