<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005362A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005362</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17662998</doc-number><date>20220511</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>08</class><subclass>G</subclass><main-group>1</main-group><subgroup>01</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>W</subclass><main-group>4</main-group><subgroup>46</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>S</subclass><main-group>13</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>08</class><subclass>G</subclass><main-group>1</main-group><subgroup>0108</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20180201</date></cpc-version-indicator><section>H</section><class>04</class><subclass>W</subclass><main-group>4</main-group><subgroup>46</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>S</subclass><main-group>13</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">Accuracy of Predictions on Radar Data using Vehicle-to-Vehicle Technology</invention-title><us-related-documents><us-provisional-application><document-id><country>US</country><doc-number>63303347</doc-number><date>20220126</date></document-id></us-provisional-application><us-provisional-application><document-id><country>US</country><doc-number>63272090</doc-number><date>20211026</date></document-id></us-provisional-application><us-provisional-application><document-id><country>US</country><doc-number>63218248</doc-number><date>20210702</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Aptiv Technologies Limited</orgname><address><city>St. Michael</city><country>BB</country></address></addressbook><residence><country>BB</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Billapati</last-name><first-name>Rajashekar</first-name><address><city>Kokomo</city><state>IN</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Tyagi</last-name><first-name>Kanishka</first-name><address><city>Agoura Hills</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Manukian</last-name><first-name>Narbik</first-name><address><city>Los Angeles</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">This document describes techniques and systems for improving accuracy of predictions on radar data using vehicle-to-vehicle (V2V) technology. V2V communications data and the matching sensor data related to one or more vehicles in the vicinity of a host vehicle are collected. The V2V data is used as label data and the radar data is used as the input data for training the model. The training may either occur onboard the host vehicle or remotely. Further, multiple host vehicles may contribute data to train the model. Once the model has been updated with the included training, the updated model is deployed to the sensor tracking system of the host vehicle. By using the dataset that includes the V2V communications data and the matching sensor data, the updated model may accurately track other vehicles and enable the host vehicle to utilize advanced driver-assistance systems safely and reliably.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="232.66mm" wi="148.00mm" file="US20230005362A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="257.56mm" wi="150.03mm" file="US20230005362A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="254.76mm" wi="157.65mm" file="US20230005362A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="254.76mm" wi="158.33mm" file="US20230005362A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="256.79mm" wi="146.39mm" file="US20230005362A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="257.47mm" wi="156.55mm" file="US20230005362A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="251.88mm" wi="127.51mm" file="US20230005362A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="257.64mm" wi="164.34mm" file="US20230005362A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application claims the benefit of U.S. Provisional Patent Application No. 63/303,347, filed Jan. 26, 2022, U.S. Provisional Patent Application No. 63/272,090, filed Oct. 26, 2021, and U.S. Provisional Patent Application No. 63/218,248, filed Jul. 2, 2021, which are incorporated by reference herein in their entireties.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">Advanced safety or driving systems for vehicles may use sensors to track nearby objects. These objects may include other vehicles, pedestrians, and animals, as well as inanimate objects, such as trees and street signs. The sensors (e.g., optical cameras, radar, lidar) collect low-level data that is processed in different ways to estimate positions, trajectories, and movements of the objects. Often, machine-learned models are used to estimate objects in or near a road; predictions are made while sensor data is collected and input to the models. For machine-learned models to quickly and accurately predict object whereabouts and movements in and around a road, the models need to be trained using large complex datasets that address as many different driving scenarios as possible. Manually generating a huge or intricate dataset that is sufficiently detailed and complex to be used for training a machine-learned model that assists with sensor based driving can be a challenging and time consuming task.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0004" num="0003">This document describes techniques, systems, and methods for improving accuracy of predictions on radar data using vehicle-to-vehicle (V2V) technology. In one example, a method includes receiving, from a V2V communications platform of a host vehicle, V2V communications data from one or more other vehicles in a vicinity of the host vehicle. The method also includes receiving sensor data generated by a sensor system of the host vehicle indicative of the one or more other vehicles. The method further includes updating a model to generate an updated model by inputting the V2V communications data from the one or more other vehicles as label data for the sensor data, inputting the sensor data to the model, and training the model based on the sensor data and the V2V communications data. The method further includes deploying the updated model to the host vehicle for detecting and tracking objects in the vicinity of the host vehicle.</p><p id="p-0005" num="0004">In another example, a system includes one or more processors configured to receive, from a V2V communications platform of a first vehicle, V2V communications data from one or more other vehicles in a vicinity of the first vehicle. The one or more processors are also configured to receive, from a sensor system of the first host vehicle, sensor data related to the one or more other vehicles. The one or more processors are further configured to update a model to generate an updated model by inputting, to the model, the V2V communications data from the one or more other vehicles as label data for the sensor data, inputting the sensor data to the model, and training the model based on the sensor data and the V2V communications data. The one or more processors are further configured to deploy the updated model to the first vehicle for detecting and tracking objects in the vicinity of the first vehicle. The system may be located remotely from the vehicle.</p><p id="p-0006" num="0005">In another example, a system includes one or more processors configured to receive, from a V2V communications platform of a host vehicle, V2V communications data from one or more other vehicles in a vicinity of the host vehicle. The one or more processors are also configured to receive, from a sensor system of the first host vehicle, sensor data related to the one or more other vehicles. The one or more processors are further configured to update a model to generate an updated model by inputting, to the model, the V2V communications data from the one or more other vehicles as label data for the sensor data, inputting the sensor data to the model, and training the model based on the sensor data and the V2V communications data. The one or more processors are further configured to output the updated model to the host vehicle for detecting and tracking objects in the vicinity of the host vehicle. The system may be located on the host vehicle.</p><p id="p-0007" num="0006">This Summary introduces simplified concepts related to improving accuracy of predictions on radar data using V2V technology, further described in the Detailed Description and Drawings. This Summary is not intended to identify essential features of the claimed subject matter, nor is it intended for use in determining the scope of the claimed subject matter. Although primarily described in the context of improving the accuracy of predictions on radar data using V2V communications data, the techniques for using V2V communications data as ground truth data for training sensor-based models can be applied to other applications where accuracy of trained models are desired. Further, these techniques may also be applied to other communications data such as vehicle-to-infrastructure (V2I), vehicle-to-pedestrian (V2P), and vehicle-to-everything data (V2X).</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0008" num="0007">The details of one or more aspects of improving accuracy of predictions on radar data using V2V technology are described in this document with reference to the following figures, in which same numbers are used throughout to indicate like components:</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates an example operating environment of a vehicle that is configured to improve accuracy of predictions on radar data using V2V technology, in accordance with techniques of this disclosure;</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>2</b>-<b>1</b></figref> illustrates an example of an automotive system for improving accuracy of predictions on radar data using V2V technology, in accordance with techniques of this disclosure;</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>2</b>-<b>2</b></figref> illustrates another example of an automotive system for improving accuracy of predictions on radar data using V2V technology, in accordance with techniques of this disclosure;</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates an example implementation using a local model training system for improving accuracy of predictions on radar data using V2V technology, in accordance with techniques of this disclosure;</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates another example implementation using a remote model training system for improving accuracy of predictions on radar data using V2V technology, in accordance with techniques of this disclosure;</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates an example flowchart for improving accuracy of predictions on radar data using V2V technology, in accordance with techniques of this disclosure; and</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates an example method for improving accuracy of predictions on radar data using V2V technology, in accordance with techniques of this disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0016" num="0015">Overview</p><p id="p-0017" num="0016">As vehicles are more often being equipped with autonomous and semi-autonomous systems, machine-learned trained models are being used to assist these systems in object recognition, identification, and/or object tracking applications. The trained models provide a minimum level of accuracy to the sensor systems (e.g., optical cameras, radar, lidar, ultrasonic sensors) and enable the sensor systems to reliably detect and track various objects (e.g., other vehicles, pedestrians, animals, stationary objects such as road signs and vegetation). However, accuracy of the trained models can improve, for example, as the trained models are provided better training data sets (e.g., a larger dataset can account for more object types and driving scenarios). Generating datasets that improve the training of models has been tried, however, several issues remain as obstacles to the creation of the datasets.</p><p id="p-0018" num="0017">One issue is gathering reliable information for the datasets. Sensor data alone may not provide enough accuracy to reliably train the models. However, as recognized by this disclosure, pairing the sensor data with other highly accurate data may overcome this deficiency. For example, sensor data and V2V communications data can be used in combination to improve the accuracy of datasets used to train machine-learned models. While this disclosure primarily describes these training datasets in relation to radar data combined with V2V communications data, the concepts disclosed herein may, likewise, be applied to other combinations of sensor data (e.g., lidar, camera) and other vehicle communications data (e.g., V2I, V2P).</p><p id="p-0019" num="0018">Another issue is finding information sources that can provide values that populate these very large datasets. A solution to this issue, as described herein, is to leverage information gathering already provided by other vehicles or infrastructure near or on the roads. By using data collected by many vehicles in many different environmental and driving scenarios (e.g., urban environments versus rural environments, city scenarios versus non-city scenarios), large quantities of data can be acquired to encompass many different real-world scenarios. By training models with sensor data, in combination with accurate V2V communications data, the models of vehicles may more-accurately or more-quickly detect and track objects and avoid collisions with the objects, than when using models trained in other ways.</p><p id="p-0020" num="0019">This document describes techniques and systems for improving accuracy of predictions on radar data using V2V technology. V2V communications data and the matching sensor data related to one or more vehicles in the vicinity of a host vehicle are collected. The V2V data is used as label data and the radar data is used as the input data for training the model. The training may either occur onboard the host vehicle or remotely (e.g., in a cloud). Further, multiple host vehicles may contribute data to train the model. Once the model has been updated with the included training, the updated model is deployed to the sensor tracking system of the host vehicle. By using the dataset that includes the V2V communications data and the matching sensor data, the updated model may accurately track other vehicles and enable the host vehicle to utilize advanced driver-assistance systems safely and reliably.</p><heading id="h-0006" level="1">Example Environment</heading><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates an example operating environment <b>100</b> of a host vehicle <b>102</b> that is configured to improve accuracy of predictions on radar data using V2V technology, in accordance with techniques of this disclosure. In the depicted environment <b>100</b>, a V2V system <b>104</b> and a radar system <b>106</b> are mounted to, or integrated within, the host vehicle <b>102</b>. Although illustrated as a car, the host vehicle <b>102</b> can represent other types of vehicles and machinery (e.g., a motorcycle, a bus, a tractor, a semi-trailer truck, watercraft, aircraft, or other heavy equipment) including manned and unmanned systems that may be used for a variety of purposes. The host vehicle <b>102</b> may operate in an autonomous or semi-autonomous mode.</p><p id="p-0022" num="0021">In general, manufacturers can mount V2V and radar sensors/antennas to any moving platform that can travel in the environment <b>100</b>. The sensors/antennas can project their respective field-of-view (FOV) from any exterior surface of the host vehicle <b>102</b>. For example, vehicle manufacturers can integrate at least a part of the radar system <b>106</b> (e.g., the radar sensors/antennas) into a side mirror, bumper, roof, or any other interior or exterior location where the FOV includes a portion of the environment <b>100</b> and objects moving or stationary that are in the environment <b>100</b>. Manufacturers can design the location of the sensors/antennas to provide a particular FOV that sufficiently encompasses portions of the environment <b>100</b> in which the host vehicle <b>102</b> may be traveling. In the depicted implementation, a portion of the V2V system <b>104</b> and a portion of the radar system <b>106</b> are mounted near the front bumper section of the host vehicle <b>102</b>.</p><p id="p-0023" num="0022">The V2V system <b>104</b> can communicate with other vehicles in the environment <b>100</b> that are also equipped with V2V systems. For example, a vehicle <b>108</b> in the environment <b>100</b> includes a V2V system <b>110</b>. The V2V system <b>104</b> and the V2V system <b>110</b> can communicate to one another via a wireless communication link <b>112</b>. The data conveyed via the wireless communication link <b>112</b> by the V2V system <b>104</b> and the V2V system <b>110</b> to one another may include vehicle information such as speed, location, and heading information of their respective vehicles. Likewise, other vehicles (not illustrated) in the environment <b>100</b> that are equipped with V2V systems can transmit similar data and receive similar data from other vehicles with V2V systems. Generally, the V2V communications data represents accurate and precise information about the vehicle from which the data originates.</p><p id="p-0024" num="0023">The radar system <b>106</b> can transmit radar signals (e.g., electromagnetic radiation) that can be reflected off of objects in the environment <b>100</b> (e.g., the vehicle <b>108</b>) and receive the reflected signals (e.g., as receive signals <b>114</b>). In this example, the receive signals <b>114</b> includes radar data (e.g., range data, range rate data, and azimuth data) describing the vehicle <b>108</b>. The radar system <b>106</b> may use this data, often by utilizing a trained model, to predict future driving actions (e.g., acceleration and braking actions, turning actions) of the vehicle <b>108</b>.</p><p id="p-0025" num="0024">In the environment <b>100</b>, the radar system <b>106</b> of the host vehicle <b>102</b> is one type of a sensor-tracking system. In other examples, the techniques described herein may be applied to other types of sensor-tracking systems including camera systems, lidar systems, ultrasonic systems, or any other sensor systems used to identify, track, and/or avoid objects by the host vehicle <b>102</b> in the environment <b>100</b>. Additionally, for simplicity in describing this example, the radar system <b>106</b> includes the sensor and any object detection and tracking features (e.g., object-tracking modules, sensor-fusion modules, models assisting in the implementation of these features). In some examples, the sensor and the object detection and tracking features may be represented by different and/or separate systems.</p><p id="p-0026" num="0025">The V2V communications data received by the V2V system <b>104</b> from the V2V system <b>110</b> and the radar data related to the vehicle <b>108</b> received by the radar system <b>106</b> can be collected by a data set collector module <b>116</b>. The V2V communications data is collected as label data set <b>118</b>. Similarly, the radar data is collected and stored as radar data set <b>120</b>. A model training system <b>122</b> can receive the label data set <b>118</b> and the radar data set <b>120</b>. The model training system <b>122</b> trains or retrains a model to be used by the radar system <b>106</b> in assisting the radar system <b>106</b> in predicting and tracking objects in the environment <b>100</b>. The label data set <b>118</b> can serve as the ground truth and the radar data can be used as input data for training (or retraining) a model. The model training system <b>122</b> can include instructions to be executed by a processor (e.g., electronic control unit (ECU)) installed on the host vehicle <b>102</b>. Alternatively, or additionally, the model training system <b>122</b> (or portions thereof) may be executed remotely (e.g., a cloud environment) to the host vehicle <b>102</b>.</p><p id="p-0027" num="0026">The model training system <b>122</b> uses the label data set <b>118</b> and the radar data set <b>120</b> to update the model for the radar system <b>106</b>. In some implementations, the label data set <b>118</b> and the radar data set <b>120</b> can be an aggregate of label data sets and radar data sets collected by multiple host vehicles. After the model is updated, it can be deployed to the radar system <b>106</b>. Using the updated model, the radar system <b>106</b> may be more accurate and reliable in predicting the actions of objects in the environment <b>100</b>.</p><heading id="h-0007" level="1">Example Systems</heading><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>2</b>-<b>1</b></figref> illustrates an example of an automotive system <b>200</b>-<b>1</b> for improving accuracy of predictions on radar data using V2V technology, in accordance with techniques of this disclosure. The automotive system <b>200</b>-<b>1</b> can be integrated within the host vehicle <b>102</b>. For example, the automotive system <b>200</b>-<b>1</b> includes a controller <b>202</b>, a V2V system <b>104</b>-<b>1</b>, a radar system <b>106</b>-<b>1</b>, and a model training system <b>122</b>-<b>1</b>. The V2V system <b>104</b>-<b>1</b> is an example of the V2V system <b>104</b>, the radar system <b>106</b>-<b>1</b> is an example of the radar system <b>106</b>, and the model training system <b>122</b>-<b>1</b> is an example of the model training system <b>122</b>. The V2V system <b>104</b>-<b>1</b> and the radar system <b>106</b>-<b>1</b> can be integrated into an automotive or other vehicular environment. In this example, the model training system <b>122</b>-<b>1</b> is integrated into the host vehicle <b>102</b>. In some implementations, an additional model training system may be remote (e.g., in the cloud) to the vehicle and can train a sensor model concurrent with the model training system <b>122</b>-<b>1</b>, e.g., in an offline fashion.</p><p id="p-0029" num="0028">The V2V system <b>104</b>-<b>1</b>, the radar system <b>106</b>-<b>1</b>, the model training system <b>122</b>-<b>1</b>, and the controller <b>202</b> communicate over a link <b>230</b>. The link <b>230</b> may be a wired or wireless link and in some cases includes a communication bus.</p><p id="p-0030" num="0029">The controller <b>202</b> performs operations based on information received over the link <b>230</b>, such as data output from the V2V system <b>104</b>-<b>1</b> or the radar system <b>106</b>-<b>1</b> as objects in an environment (e.g., the environment <b>100</b>) are identified from the data. The controller <b>202</b> includes an over-the-air (OTA) interface <b>204</b> (optional in this implementation), a processor <b>206</b>-<b>1</b>, and a computer-readable storage media (CRM) <b>208</b>-<b>1</b> (e.g., a memory, long-term storage, short-term storage), which stores instructions for an automotive module <b>210</b>.</p><p id="p-0031" num="0030">The CRM <b>208</b>-<b>1</b> can include a data set collector module <b>212</b>. The data set collector module <b>212</b> can store object information derived from data obtained by the V2V system <b>104</b>-<b>1</b> and the radar system <b>106</b>-<b>1</b>, including the label data set <b>118</b> and the radar data set <b>120</b> illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. Alternatively, the data set collector module <b>212</b> may reside on any CRM <b>208</b> integrated within the host vehicle <b>102</b>.</p><p id="p-0032" num="0031">The V2V system <b>104</b>-<b>1</b> includes a V2V antenna <b>214</b>, a processor <b>206</b>-<b>2</b>, and a CRM <b>208</b>-<b>2</b>, which stores host vehicle data <b>216</b> and object data <b>218</b>. The V2V system <b>104</b>-<b>1</b> can transmit, via the V2V antenna <b>214</b>, the host vehicle data <b>216</b> as V2V communications data to other vehicles or objects equipped with V2V systems through the wireless communication link <b>112</b>. Likewise, the V2V system can receive, via the V2V antenna <b>214</b>, V2V communications data from other vehicles equipped with V2V systems as the object data <b>218</b>.</p><p id="p-0033" num="0032">The host vehicle data <b>216</b> and the object data <b>218</b> can include information (e.g., vehicle speed, location, heading, classification) about the respective vehicle and is considered more accurate and precise than sensor data (e.g., radar data). The label data set <b>118</b> of the data set collector module <b>212</b> can be derived from the object data <b>218</b>.</p><p id="p-0034" num="0033">The radar system <b>106</b>-<b>1</b> includes one or more radar sensors <b>220</b>, a processor <b>206</b>-<b>3</b>, and a CRM <b>208</b>-<b>3</b>, which includes radar data <b>222</b> and a trained model <b>224</b>. The CRM <b>208</b>-<b>3</b> also includes instructions for performing sensor operations. The radar system <b>106</b>-<b>1</b> can receive signals (e.g., the receive signals <b>114</b>) reflected from nearby objects and can store the data as the radar data <b>222</b>. The radar system <b>106</b>-<b>1</b> can detect and track objects in the FOV using the radar data <b>222</b>. The radar data <b>222</b>, when input to the trained model <b>224</b>, can provide more accurate and reliable detections and classifications of objects, which the radar system <b>106</b>-<b>1</b> can output to the automotive module <b>210</b>.</p><p id="p-0035" num="0034">The radar data set <b>120</b> of the data set collector module <b>212</b> can be derived from the radar data <b>222</b>. The trained model <b>224</b> may initially be trained using a generic radar data set and a label data set not derived from V2V communications data. As described in this disclosure, subsequent trained models <b>224</b> may include training using the label data set <b>118</b> and the radar data set <b>120</b>.</p><p id="p-0036" num="0035">The model training system <b>122</b>-<b>1</b> includes a processor <b>206</b>-<b>4</b> and a CRM <b>208</b>-<b>4</b>, which stores instructions for a machine learning module <b>226</b>-<b>1</b>. The dedicated processor <b>206</b>-<b>4</b> enables the model training system <b>122</b>-<b>1</b> to train the trained model <b>224</b> without causing disruption to the radar system <b>106</b>-<b>1</b> or any other systems of the host vehicle <b>102</b>. The machine learning module <b>226</b>-<b>1</b> receives a label data set <b>118</b>-<b>1</b> and a radar data set <b>120</b>-<b>1</b> from the data set collector module <b>212</b>. In some aspects, the training will not start until the quantity of data passes a threshold.</p><p id="p-0037" num="0036">Using the label data set <b>118</b>-<b>1</b> as ground truth data and the radar data set <b>120</b>-<b>1</b> as input, the machine learning module <b>226</b>-<b>1</b> can train the trained model <b>224</b> or retrain a previously trained model <b>224</b> and can deploy the updated trained model <b>224</b> to the radar system <b>106</b>-<b>1</b>. In some aspects, before the trained model <b>224</b> is deployed, the trained model <b>224</b> can be compared to another model (e.g., an initially trained model based on generic radar data and labels, an earlier trained model) for accuracy. If the updated trained model <b>224</b> does not outperform the other model, the updated trained model <b>224</b> may not be deployed.</p><p id="p-0038" num="0037">The machine learning module <b>226</b>-<b>1</b> may train the trained model <b>224</b> using machine learning techniques, such as supervised learning, to perform object detection, object tracking, and/or object classification. The trained model <b>224</b> may include one or more artificial neural networks (e.g., long short-term memory (LSTM) networks, recurrent neural networks (RNN), convolution neural networks (CNN)). The output of the trained model <b>224</b> (e.g., predictions about objects) can be used by the automotive module <b>210</b> for driving applications.</p><p id="p-0039" num="0038">Generally, the automotive system <b>200</b>-<b>1</b> executes the automotive module <b>210</b> to perform an automotive function, which may include using output from the radar system <b>106</b>-<b>1</b>. For example, the automotive module <b>210</b> can provide automatic cruise control and monitor the radar system <b>106</b>-<b>1</b> for output that indicates the presence of objects in or near the FOV, for instance, to slow the speed and prevent a rear-end collision with the vehicle <b>108</b>. In such an example, the trained model <b>224</b> can output object information to the automotive module <b>210</b>. The automotive module <b>210</b> may provide alerts or perform a specific maneuver when the data obtained from the radar system <b>106</b>-<b>1</b> indicates that one or more objects are crossing in front of the host vehicle <b>102</b>. By training the trained model <b>224</b> using the radar data <b>222</b> as input and the object data <b>218</b> as ground truth data, the accuracy and reliability of the radar system <b>106</b>-<b>1</b> may be improved.</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>2</b>-<b>2</b></figref> illustrates another example of an automotive system <b>200</b>-<b>2</b> for improving accuracy of predictions on radar data using V2V technology, in accordance with techniques of this disclosure. Similar to the automotive system <b>200</b>-<b>1</b>, the automotive system <b>200</b>-<b>2</b> includes the controller <b>202</b>, the V2V system <b>104</b>-<b>1</b>, and the radar system <b>106</b>-<b>1</b>. The controller <b>202</b> includes the OTA interface <b>204</b> through which the controller <b>202</b> can communicate to a model training system <b>122</b>-<b>2</b>. The controller <b>202</b> and the model training system <b>122</b>-<b>2</b> can communicate through wireless link <b>232</b> (e.g., WiFi link, cellular link). In other aspects, the OTA interface <b>204</b> and the data set collector module <b>212</b> may be in a different system or module (e.g., the V2V system <b>104</b>-<b>1</b>, the radar system <b>106</b>-<b>1</b>, the model training system <b>122</b>-<b>1</b>, a dedicated controller (not illustrated) integrated within the host vehicle <b>102</b>). Optionally, the automotive system <b>200</b>-<b>2</b> can, likewise, include a model training system <b>122</b>-<b>1</b> that operates according to the description with respect to <figref idref="DRAWINGS">FIG. <b>2</b>-<b>1</b></figref>.</p><p id="p-0041" num="0040">As illustrated in <figref idref="DRAWINGS">FIG. <b>2</b>-<b>2</b></figref>, the model training system <b>122</b>-<b>2</b> is remote (e.g., resides on a cloud-based server) to the rest of the automotive system <b>200</b>-<b>2</b>. The model training system <b>122</b>-<b>2</b> includes an OTA interface <b>228</b> used to communicate on the wireless link <b>232</b>. Similar to the model training system <b>122</b>-<b>1</b>, the model training system <b>122</b>-<b>2</b> includes a processor <b>206</b>-<b>5</b>, a CRM <b>208</b>-<b>5</b> which stores instructions for a machine learning module <b>226</b>-<b>2</b> to train or retrain a trained model <b>224</b>, and the model training system <b>122</b>-<b>2</b> may use the same training methods and techniques as the model training system <b>122</b>-<b>1</b>. Different than the model training system <b>122</b>-<b>1</b>, the machine learning module <b>226</b>-<b>2</b> of the model training system <b>122</b>-<b>2</b> includes a label data set <b>118</b>-<b>2</b> (e.g., truth data) and a radar data set <b>120</b>-<b>2</b> (e.g., input data) that includes more data than that of the model training system <b>122</b>-<b>1</b>. By residing remotely to the automotive system <b>200</b>-<b>2</b>, the model training system <b>122</b>-<b>2</b> can receive input from a plurality of automotive systems <b>200</b>-<b>2</b>. Further, the model training system <b>122</b>-<b>2</b> can include an artificial neural network that is common to each radar system <b>106</b>-<b>1</b> of each automotive system <b>200</b>-<b>2</b> of the plurality of automotive systems <b>200</b>-<b>2</b>. This enables the model training system <b>122</b>-<b>2</b> to use data acquired from each radar system <b>106</b>-<b>1</b> even if each radar system <b>106</b>-<b>1</b> is different or has different properties due to discretions of mounting or calibration between each radar system <b>106</b>-<b>1</b>. The label data set <b>118</b>-<b>2</b> and the radar data set <b>120</b>-<b>2</b> can include an aggregate of data from the plurality of automotive systems <b>200</b>-<b>2</b>. Because each automotive system <b>200</b>-<b>2</b> of the plurality of automotive systems <b>200</b>-<b>2</b> may experience different driving conditions (e.g., dense metropolitan conditions, sparse rural conditions, city streets, interstates, and others), the label data set <b>118</b>-<b>2</b> and the radar data set <b>120</b>-<b>2</b> can contain large amounts of data that includes the different driving conditions. In this manner, the machine learning module <b>226</b>-<b>2</b> can train the trained model <b>224</b> with the large amounts of data that includes the different driving conditions. The updated trained model <b>224</b> may be highly accurate in relation to many different driving conditions in many different environments. This may improve reliability and safety for any vehicle <b>108</b> with the updated trained model <b>224</b> regardless of the environment in which it travels.</p><heading id="h-0008" level="1">Example Implementations</heading><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates an example implementation <b>300</b> using a local model training system <b>302</b> for improving accuracy of predictions on radar data using V2V technology, in accordance with techniques of this disclosure. A host vehicle <b>304</b> has an automotive system, similar to the automotive system <b>200</b>-<b>1</b>, that includes a V2V system <b>306</b>, a radar system <b>308</b>, and a model training system <b>302</b> (e.g., the model training system <b>122</b>-<b>1</b>) is traveling in an environment that includes objects <b>310</b>-<b>1</b> and <b>310</b>-<b>2</b>. Objects <b>310</b>-<b>1</b> and <b>310</b>-<b>2</b> include V2V systems <b>312</b>-<b>1</b> and <b>312</b>-<b>2</b>, respectively. The V2V systems <b>306</b> and <b>312</b>-<b>1</b> can communicate via the wireless link <b>414</b>-<b>1</b>, and the V2V systems <b>306</b> and <b>312</b>-<b>2</b> can communicate via the wireless link <b>414</b>-<b>2</b>. The radar system <b>308</b> receives reflected signals <b>316</b>-<b>1</b> and <b>316</b>-<b>2</b> from objects <b>310</b>-<b>1</b> and <b>310</b>-<b>2</b>, respectively.</p><p id="p-0043" num="0042">The V2V system <b>306</b> can receive V2V communications data from the objects <b>310</b>-<b>1</b> and <b>310</b>-<b>2</b> that can include the vehicle velocity, location, heading, and classification of the objects <b>310</b>-<b>1</b> and <b>310</b>-<b>2</b>. Likewise, the radar system <b>308</b> can detect radar data from the objects <b>310</b>-<b>1</b> and <b>310</b>-<b>2</b> that complements the V2V communications data of the objects <b>310</b>-<b>1</b> and <b>310</b>-<b>2</b>.</p><p id="p-0044" num="0043">The V2V communications data and the radar data can be collected and retained in a data set collector module. As the host vehicle <b>304</b> encounters different objects <b>310</b>, data from each of the objects <b>310</b> can be collected in the data set collector module. In some aspects when enough data is collected to surpass a threshold, the model training system <b>302</b> can begin the training process on a radar model of the radar system <b>308</b>. The model training system <b>302</b> may not disrupt the operations of the radar system <b>308</b> as the training is performed. Once the radar model is updated, the model training system <b>302</b> can deploy the updated model to the radar system <b>308</b>. The updated model may contain data that accounts for the different driving environments and conditions that the host vehicle <b>304</b> has experienced to the point of the training process. By accumulating V2V communications data and radar data from other vehicles as the host vehicle <b>304</b> encounters them, the data sets used to train the radar model can continue to grow. Larger data sets enable more accurate training. More accurate training may increase the reliability of the radar system <b>308</b> and the safety of the host vehicle <b>304</b>.</p><p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates another example implementation <b>400</b> using a remote model training system <b>402</b> for improving accuracy of predictions on radar data using V2V technology, in accordance with techniques of this disclosure. The model training system <b>402</b> resides remotely from host vehicles <b>404</b>, including a first group of host vehicles <b>404</b>-<b>1</b> and a second group of host vehicles <b>404</b>-<b>2</b>. That is, the model training system <b>402</b> may reside on a cloud-based server or other computational device that is located outside of any host vehicle <b>404</b>. The host vehicles <b>404</b> may communicate with the model training system <b>402</b> via wireless communications such as a WiFi connection or a cellular connection (e.g., 4G, 5G). The model training system <b>402</b>, similar to the model training system <b>122</b>-<b>2</b> described with reference to <figref idref="DRAWINGS">FIG. <b>2</b>-<b>2</b></figref>, includes a machine learning module <b>406</b> that stores instructions to train a radar model using a label data set <b>408</b> as label data and a radar data set <b>410</b> as input to the machine learning module <b>406</b>.</p><p id="p-0046" num="0045">In the example implementation <b>400</b>, the host vehicles <b>404</b>-<b>1</b> are traveling in a metropolitan environment <b>412</b>-<b>1</b> with dense traffic, and the host vehicles <b>404</b>-<b>2</b> are traveling in rural environment <b>412</b>-<b>2</b> with less traffic density than the environment <b>412</b>-<b>1</b>. The environments <b>412</b> can represent any environments with many different levels of traffic density and driving conditions. For example, the environments <b>412</b> may represent interstate travel versus traveling on city streets. As the host vehicles <b>404</b> travel in their respective environments <b>412</b>, they can encounter other host vehicles <b>404</b> and other vehicles that include V2V systems (not illustrated) but are not host vehicles.</p><p id="p-0047" num="0046">The host vehicles <b>404</b> are able to communicate with the model training system <b>402</b>. The host vehicles <b>404</b> can collect V2V communications data and radar data from one another and from the other vehicles. The collected V2V communications data and radar data can be transmitted to the model training system <b>402</b>. In some aspects, the data is transmitted from the host vehicles to the model training system after a quantity of data has been collected that exceeds a threshold. The model training system <b>402</b> can receive the data from each of the host vehicles to use as a label data set <b>408</b> and a radar data set <b>410</b>. The label data set <b>408</b> is derived from the collected V2V communications data from each of the host vehicles <b>404</b>, and the radar data set <b>410</b> is derived from the collected radar data from each of the host vehicles <b>404</b>. By collecting data from a plurality of host vehicles <b>404</b>, the label data set <b>408</b> and the radar data set <b>410</b> may each grow to be large. Additionally, because the host vehicles <b>404</b> are traveling in different environments and under different driving conditions, the label data set <b>408</b> and the radar data set <b>410</b> can include a wide variety of traffic conditions.</p><p id="p-0048" num="0047">After the machine learning module <b>406</b> updates a radar model (used by the radar systems of the host vehicles) using the label data set <b>408</b> and the radar data set <b>410</b>, the model training system <b>402</b> can deploy the updated radar model to each of the host vehicles <b>404</b>. The updated radar model can include traffic scenarios not normally traveled by each of the host vehicles <b>404</b>. In other words, the updated radar model can include rural traffic conditions for the host vehicles <b>404</b>-<b>1</b> that may not generally travel in rural areas, and the updated radar model can include metropolitan travel conditions for the host vehicles <b>404</b>-<b>2</b> that may not generally travel in dense traffic. The updated radar model may include a wide variety of traffic conditions and densities. Thus, the updated radar model may provide more reliability and safety for each of the host vehicles <b>404</b>, regardless of the environment in which they may encounter.</p><heading id="h-0009" level="1">Example Methods</heading><p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates an example flowchart <b>500</b> for improving accuracy of predictions on radar data using V2V technology, in accordance with techniques of this disclosure. The operations (or steps) <b>502</b> through <b>514</b> are performed but are not necessarily limited to the order or combinations in which the operations are shown herein. Further, any of one or more of the operations may be repeated, combined, or reorganized to provide other operations.</p><p id="p-0050" num="0049">At step <b>502</b>, V2V communications data and radar data related to an object are received by a host vehicle. The V2V communications data can include classification, location, heading, and speed of the object (e.g., another vehicle). The radar data can include range, range rate, and azimuth of the object.</p><p id="p-0051" num="0050">At step <b>504</b>, the V2V communications data and the radar data are collected and retained in a CRM. The V2V communications data and the radar data are retained until a minimum quantity of V2V communications data and the radar data is collected.</p><p id="p-0052" num="0051">At step <b>506</b>, the quantity of V2V communications data and the radar data is checked. If the quantity exceeds a threshold, then the V2V communications data and the radar data may be used in step <b>508</b>. If the quantity of V2V communications data and the radar data is below the threshold, then more V2V communications data and radar data are collected and the quantity can be rechecked.</p><p id="p-0053" num="0052">At step <b>508</b>, a radar model is updated by training the radar model. The training may be performed using machine learning techniques. The V2V communications are used as truth data (e.g., a label data set). The radar data is used as input for training the model.</p><p id="p-0054" num="0053">At step <b>510</b>, the updated radar model is tested for accuracy. A safety check is performed on the updated radar model with respect to a production model. The safety check includes comparing the updated radar model and the production model for maximum deviation. If the updated radar model outperforms the production model, then the updated model passes the safety check.</p><p id="p-0055" num="0054">At step <b>512</b>, if the updated radar model passes the safety check, the updated model is deemed deployable. If the updated radar model fails the safety check, then the process is repeated from step <b>502</b> until an updated radar model passes the safety check.</p><p id="p-0056" num="0055">At step <b>514</b>, the updated radar model is deployed to the host vehicle. By using a large data set (e.g., the V2V communications data and the radar data) the updated radar model may accurately predict the path and speed of objects in the same environment as the host vehicle. The radar model may enable automotive systems of the host vehicle to make safe driving decisions as it travels in an environment.</p><p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates an example method for improving accuracy of predictions on radar data using V2V technology, in accordance with techniques of this disclosure. The operations (or steps) <b>602</b> through <b>608</b> are performed but are not necessarily limited to the order or combinations in which the operations are shown herein. Further, any of one or more of the operations may be repeated, combined, or reorganized to provide other operations.</p><p id="p-0058" num="0057">At step <b>602</b>, V2V communications data from one or more other vehicles in a vicinity of a host vehicle is received by the host vehicle. The V2V communications data is generally considered to be accurate and precise data and can include location data, heading data, velocity data, and classification of each of the one or more other vehicles.</p><p id="p-0059" num="0058">At step <b>604</b>, sensor data related to the one or more other vehicles is received by the host vehicle. This disclosure generally describes the sensor data in context to a radar sensor; however, the sensor may be any sensor used by a vehicle to safely travel through an environment. Some examples of other sensors include lidar, cameras, and ultrasonic sensors.</p><p id="p-0060" num="0059">At step <b>606</b>, a model, used by the sensor, is updated to generate an updated model. The model is updated by applying steps <b>606</b>-<b>1</b> through <b>606</b>-<b>3</b>. At step <b>606</b>-<b>1</b>, the V2V communications data is input as label data for the sensor data. At step <b>606</b>-<b>2</b>, the sensor data is input into the model. At step <b>606</b>-<b>3</b>, the model is trained based on the sensor data and the V2V communications data. In some aspects, the model is trained on a dedicated ECU integrated within the host vehicle. A model trained in this manner may include traffic-related information for the environments in which the host vehicle travels. In other aspects, the model is trained remotely to the host vehicle. The V2V communications data and the sensor data used to train the model can include data received by multiple host vehicles traveling in a variety of environments. By including data of multiple host vehicles, the data set (e.g., the label data set and the sensor data set) used to train the model can be very large compared to other available data sets.</p><p id="p-0061" num="0060">At step <b>608</b>, the updated model is deployed to the host vehicle. By receiving large quantities of data and training the model with the large quantities of data, the updated model may accurately predict object-tracking in a variety of driving environments. The host vehicle may use the updated model to safely travel in an environment.</p><heading id="h-0010" level="1">Additional Examples</heading><p id="p-0062" num="0061">Example 1: A method comprising: receiving, from a V2V communications platform of a host vehicle, V2V communications data from one or more other vehicles in a vicinity of the host vehicle; receiving sensor data generated by a sensor system of the host vehicle indicative of the one or more other vehicles; updating a model to generate an updated model by: inputting the V2V communications data from the one or more other vehicles as label data for the sensor data; inputting the sensor data to the model; and training the model based on the sensor data and the V2V communications data; and deploying the updated model to the host vehicle for detecting and tracking objects in the vicinity of the host vehicle.</p><p id="p-0063" num="0062">Example 2: The method of example 1, further comprising: operating the host vehicle in an autonomous or semi-autonomous mode based on the updated model.</p><p id="p-0064" num="0063">Example 3: The method of any one of the preceding examples, wherein the updated model is generated by the host vehicle.</p><p id="p-0065" num="0064">Example 4: The method of any one of the preceding examples, wherein updating the model to generate the updated model is performed: utilizing a first processor and a first computer-readable storage media, the first processor and the computer-readable storage media being different from a second processor and a second computer-readable storage media, the second processor and the second computer-readable storage being used for sensor operations of the host vehicle.</p><p id="p-0066" num="0065">Example 5: The method of any one of the preceding examples, wherein: the host vehicle represents a plurality of host vehicles; the updated model is trained, using a common artificial neural network to each sensor system of each host vehicle of the plurality of host vehicles, remotely in relation to the plurality of host vehicles; and the updated model is deployed to the plurality of host vehicles.</p><p id="p-0067" num="0066">Example 6: The method of any one of the preceding examples, wherein the updated model is deployed to the plurality of host vehicles via an over-the-air update.</p><p id="p-0068" num="0067">Example 7: The method of any one of the preceding examples, wherein the updated model is also trained at each of the plurality of host vehicles.</p><p id="p-0069" num="0068">Example 8: A system comprising: one or more processors configured to: receive, from a V2V communications platform of a first vehicle, V2V communications data from one or more other vehicles in a vicinity of the first vehicle; receive, from a sensor system of the first vehicle, sensor data related to the one or more other vehicles; update a model to generate an updated model by: inputting, to the model, the V2V communications data from the one or more other vehicles as label data for the sensor data; inputting the sensor data to the model; and training the model based on the sensor data and the V2V communications data; and deploy the updated model to the first vehicle for detecting and tracking objects in the vicinity of the first vehicle.</p><p id="p-0070" num="0069">Example 9: The system of any one of the preceding examples, wherein: the first vehicle represents a plurality of host vehicles; the updated model is trained remotely in relation to the plurality of host vehicles; and the updated model is deployed to the plurality of host vehicles.</p><p id="p-0071" num="0070">Example 10: The system of any one of the preceding examples, wherein the updated model is deployed to the plurality of host vehicles via an over-the-air update.</p><p id="p-0072" num="0071">Example 11: The system of any one of the preceding examples, wherein the updated model is also trained on a separate system installed on each of the plurality of host vehicles.</p><p id="p-0073" num="0072">Example 12: The system of any one of the preceding examples, wherein at least a first group of host vehicles of the plurality of host vehicles operates in a first environment different from a second environment in which at least a second group of host vehicles of the plurality of host vehicles operates.</p><p id="p-0074" num="0073">Example 13: The system of any one of the preceding examples, wherein the model is updated based on a quantity of the V2V communications data received and the sensor data received exceeding a threshold.</p><p id="p-0075" num="0074">Example 14: The system of any one of the preceding examples, wherein the one or more processors are further configured to: compare the updated model to a production model; and responsive to the updated model outperforming the production model, deploy the updated model.</p><p id="p-0076" num="0075">Example 15: The system of any one of the preceding examples, wherein the V2V communications data for each respective other vehicle comprises: location data; heading data; and velocity data.</p><p id="p-0077" num="0076">Example 16: The system of any one of the preceding examples, wherein the model was previously trained, and wherein the model is continuously trained.</p><p id="p-0078" num="0077">Example 17: The system of any one of the preceding examples, wherein the sensor system comprises a radar system.</p><p id="p-0079" num="0078">Example 18: The system of any one of the preceding examples, wherein the sensor data comprises: range data; range rate data; and azimuth data.</p><p id="p-0080" num="0079">Example 19: A system comprising: one or more processors configured to: receive, from a V2V communications platform of a host vehicle, V2V communications data from one or more other vehicles in a vicinity of the host vehicle; receive, from a sensor system of the host vehicle, sensor data related to the one or more other vehicles; update a model to generate an updated model by: inputting, to the model, the V2V communications data from the one or more other vehicles as label data for the sensor data; inputting the sensor data to the model; and training the model based on the sensor data and the V2V communications data; and output the updated model to the host vehicle for detecting and tracking objects in the vicinity of the host vehicle.</p><p id="p-0081" num="0080">Example 20: The system of any one of the preceding examples, wherein the system is part of the host vehicle.</p><p id="p-0082" num="0081">Example 21: A system comprising means for performing the method of any of the preceding examples.</p><p id="p-0083" num="0082">Example 22: A system comprising at least one processor configured to perform the method of any of the preceding examples.</p><p id="p-0084" num="0083">Example 23: A computer-readable storage media comprising instructions that, when executed, cause a processor to perform the method of any of the preceding examples.</p><heading id="h-0011" level="1">CONCLUSION</heading><p id="p-0085" num="0084">While various embodiments of the disclosure are described in the foregoing description and shown in the drawings, it is to be understood that this disclosure is not limited thereto but may be variously embodied to practice within the scope of the following claims. From the foregoing description, it will be apparent that various changes may be made without departing from the spirit and scope of the disclosure as defined by the following claims. Problems associated with collecting large data sets for model training can occur in other systems. Therefore, although described as a way to improve accuracy of predictions on radar data using V2V technology, the techniques of the foregoing description can be applied to other systems that would benefit from building large data sets to be used to train models. Further, these techniques may also be applied to other communications data, such as V2I, V2P, and V2X.</p><p id="p-0086" num="0085">The use of &#x201c;or&#x201d; and grammatically related terms indicates non-exclusive alternatives without limitation unless the context clearly dictates otherwise. As used herein, a phrase referring to &#x201c;at least one of&#x201d; a list of items refers to any combination of those items, including single members. As an example, &#x201c;at least one of: a, b, or c&#x201d; is intended to cover a, b, c, a-b, a-c, b-c, and a-b-c, as well as any combination with multiples of the same element (e.g., a-a, a-a-a, a-a-b, a-a-c, a-b-b, a-c-c, b-b, b-b-b, b-b-c, c-c, and c-c-c or any other ordering of a, b, and c).</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method comprising:<claim-text>receiving, from a vehicle-to-vehicle (V2V) communications platform of a host vehicle, V2V communications data from one or more other vehicles in a vicinity of the host vehicle;</claim-text><claim-text>receiving sensor data generated by a sensor system of the host vehicle indicative of the one or more other vehicles;</claim-text><claim-text>updating a model to generate an updated model by:<claim-text>inputting the V2V communications data from the one or more other vehicles as label data for the sensor data;</claim-text><claim-text>inputting the sensor data to the model; and</claim-text><claim-text>training the model based on the sensor data and the V2V communications data; and</claim-text></claim-text><claim-text>deploying the updated model to the host vehicle for detecting and tracking objects in the vicinity of the host vehicle.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>operating the host vehicle in an autonomous or semi-autonomous mode based on the updated model.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the updated model is generated by the host vehicle.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein updating the model to generate the updated model is performed:<claim-text>utilizing a first processor and a first computer-readable storage media, the first processor and the computer-readable storage media being different from a second processor and a second computer-readable storage media, the second processor and the second computer-readable storage being used for sensor operations of the host vehicle.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>the host vehicle represents a plurality of host vehicles;</claim-text><claim-text>the updated model is trained, using a common artificial neural network to each sensor system of each host vehicle of the plurality of host vehicles, remotely in relation to the plurality of host vehicles; and</claim-text><claim-text>the updated model is deployed to the plurality of host vehicles.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the updated model is deployed to the plurality of host vehicles via an over-the-air update.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the updated model is also trained at each of the plurality of host vehicles.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. A system comprising:<claim-text>one or more processors configured to:<claim-text>receive, from a vehicle-to-vehicle (V2V) communications platform of a first vehicle, V2V communications data from one or more other vehicles in a vicinity of the first vehicle;</claim-text><claim-text>receive, from a sensor system of the first vehicle, sensor data related to the one or more other vehicles;</claim-text><claim-text>update a model to generate an updated model by:<claim-text>inputting, to the model, the V2V communications data from the one or more other vehicles as label data for the sensor data;</claim-text><claim-text>inputting the sensor data to the model; and</claim-text><claim-text>training the model based on the sensor data and the V2V communications data; and</claim-text></claim-text><claim-text>deploy the updated model to the first vehicle for detecting and tracking objects in the vicinity of the first vehicle.</claim-text></claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein:<claim-text>the first vehicle represents a plurality of host vehicles;</claim-text><claim-text>the updated model is trained remotely in relation to the plurality of host vehicles; and</claim-text><claim-text>the updated model is deployed to the plurality of host vehicles.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the updated model is deployed to the plurality of host vehicles via an over-the-air update.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the updated model is also trained on a separate system installed on each of the plurality of host vehicles.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein at least a first group of host vehicles of the plurality of host vehicles operates in a first environment different from a second environment in which at least a second group of host vehicles of the plurality of host vehicles operates.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the model is updated based on a quantity of the V2V communications data received and the sensor data received exceeding a threshold.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the one or more processors are further configured to:<claim-text>compare the updated model to a production model; and</claim-text><claim-text>responsive to the updated model outperforming the production model, deploy the updated model.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the V2V communications data for each respective other vehicle comprises:<claim-text>location data;</claim-text><claim-text>heading data; and</claim-text><claim-text>velocity data.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the model was previously trained, and wherein the model is continuously trained.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the sensor system comprises a radar system.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the sensor data comprises:<claim-text>range data;</claim-text><claim-text>range rate data; and</claim-text><claim-text>azimuth data.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. A system comprising:<claim-text>one or more processors configured to:<claim-text>receive, from a vehicle-to-vehicle (V2V) communications platform of a host vehicle, V2V communications data from one or more other vehicles in a vicinity of the host vehicle;</claim-text><claim-text>receive, from a sensor system of the host vehicle, sensor data related to the one or more other vehicles;</claim-text><claim-text>update a model to generate an updated model by:<claim-text>determining, based on the V2V communications data from the one or more other vehicles, ground truth data for the sensor data;</claim-text><claim-text>updating the model based on the ground truth data for the sensor data; and</claim-text><claim-text>inputting the sensor data to the model; and</claim-text></claim-text></claim-text><claim-text>output the updated model to the host vehicle for detecting and tracking objects in the vicinity of the host vehicle.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The system of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein the system is part of the host vehicle.</claim-text></claim></claims></us-patent-application>