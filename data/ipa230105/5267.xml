<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005268A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005268</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17784472</doc-number><date>20201010</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>JP</country><doc-number>2019-232512</doc-number><date>20191224</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>52</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>64</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>82</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>52</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>647</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>82</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>2201</main-group><subgroup>07</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">OBJECT SEARCH DEVICE AND OBJECT SEARCH METHOD</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Hitachi, Ltd.</orgname><address><city>Tokyo</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>TAMURA</last-name><first-name>Masato</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>YOSHINAGA</last-name><first-name>Tomoaki</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>HIROIKE</last-name><first-name>Atsushi</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>NAKAMAE</last-name><first-name>Hiromu</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>YANASHIMA</last-name><first-name>Yuta</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>Hitachi, Ltd.</orgname><role>03</role><address><city>Tokyo</city><country>JP</country></address></addressbook></assignee></assignees><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/JP2020/038659</doc-number><date>20201010</date></document-id><us-371c12-date><date>20220610</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">An object of the invention is to configure an object search device capable of expressing information on shapes and irregularities as features only by images, in a search for an object that is characteristic in shape or irregularity, and performing an accurate search.</p><p id="p-0002" num="0000">The object search device includes: an image feature extraction unit that is configured with a first neural network, and is configured to input an image to extract an image feature; a three-dimensional data feature extraction unit that is configured with a second neural network, and is configured to input three-dimensional data to extract a three-dimensional data feature; a learning unit that is configured to extract an image feature and a three-dimensional data feature from an image and three-dimensional data of an object obtained from a same individual, respectively, and update an image feature extraction parameter so as to reduce a difference between the image feature and the three-dimensional data feature; and a search unit that is configured to extract image features of a query image and a gallery image of the object by the image feature extraction unit using the updated image feature extraction parameter, and calculate a similarity between the image features of both images to search for the object.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="207.26mm" wi="150.88mm" file="US20230005268A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="189.74mm" wi="146.05mm" file="US20230005268A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="95.25mm" wi="124.04mm" file="US20230005268A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="216.58mm" wi="152.91mm" file="US20230005268A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="190.92mm" wi="152.74mm" file="US20230005268A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="173.65mm" wi="128.78mm" file="US20230005268A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="104.39mm" wi="140.21mm" file="US20230005268A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="205.91mm" wi="147.40mm" file="US20230005268A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="95.08mm" wi="137.75mm" file="US20230005268A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="205.82mm" wi="126.15mm" file="US20230005268A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="207.01mm" wi="152.91mm" file="US20230005268A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="111.00mm" wi="141.39mm" file="US20230005268A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0001">The present invention relates to an object search device and an object search method.</p><heading id="h-0002" level="1">BACKGROUND ART</heading><p id="p-0004" num="0002">There are monitoring systems as applications for searching for similar objects. For example, when a target to be searched is a person, a person search can be used to assist a surveillant in tracking a suspicious person. For example, when the target to be searched is an item of baggage, a baggage search can be used to track an item of left-behind baggage or suspicious baggage from the baggage shown in an image of a camera at another place, and identify the owner. In such an application, it is preferable that a same individual such as the same person or the same baggage appears at a higher rank of a search result, and a highly accurate search is required.</p><p id="p-0005" num="0003">PTL 1 discloses a background art of this technical field. This related art discloses that: &#x201c;A video monitoring system includes a video acquisition unit that acquires signals from imaging devices such as one or more cameras, and an image data unit that holds an input image acquired by the video acquisition unit. The image monitoring system further includes a condition designating unit that designates a personal characteristic, time, and a camera from the input image, an image search unit that searches for, by image recognition, an image matching conditions designated by the condition designating unit from an input image group stored in the image data unit, and a result displaying unit that displays results of the image search unit. A means is provided for selecting, by user designation or a person tracking method, a plurality of personal characteristics designated by the condition designating unit and for adding such personal characteristics to search conditions of the condition designating unit&#x201d;.</p><heading id="h-0003" level="1">CITATION LIST</heading><heading id="h-0004" level="1">Patent Literature</heading><p id="p-0006" num="0004">PTL 1: JP-A-2009-27393</p><p id="p-0007" num="0005">PTL 2: JP-A-2015-176484</p><heading id="h-0005" level="1">SUMMARY OF INVENTION</heading><heading id="h-0006" level="1">Technical Problem</heading><p id="p-0008" num="0006">As a method for performing a highly accurate search as required by a monitoring system, there are methods disclosed in PTL 1 and PTL 2. PTL 1 has proposed a search method using an image. In recent years, a method of inputting images into a convolutional neural network (CNN) to extract feature, and calculating a similarity of the feature is often used. In this method, for a same individual, various images are prepared in which lighting environments or positions and sizes of objects are different, and the CNN is subjected to learning to extract similar features for those images. With the learning, the CNN is robust to various image changes and is able to perform highly accurate searches. PTL 2 has proposed a search method using a three-dimensional model. In the search using a three-dimensional model, for images including depth information and point cloud data, features are extracted by using classical feature extraction such as a local binary pattern (LBP) or a neural network (NN), and a similarity of the features is calculated in the same manner as for images. By using the three-dimensional model, it is possible to extract features of shapes and irregularities for which extraction of the features from images thereof is difficult, and it is possible to improve the accuracy of the search.</p><p id="p-0009" num="0007">The search using images disclosed in PTL 1 and the search using three-dimensional data disclosed in PTL 2 are useful, but there are still problems. In the search using images, since the images do not include information on three-dimensional shapes and irregularities of objects, it is not possible to perform a search including the information. Thus, for example, when there are a plurality of items of baggage having similar colors to a target to be searched but having different irregularity patterns from the target to be searched, there is a high possibility that an erroneous search result will be obtained in the search using images. The search using three-dimensional data requires accurate three-dimensional information in order to perform the highly accurate search. Depth cameras are limited in a distance at which accurate three-dimensional data of an imaging target can be acquired, and a large number of depth cameras are required depending on a monitoring range. This causes a problem of installation cost of the depth cameras. In addition, since a data amount to be processed of the three-dimensional data is larger than a data amount to be processed of images, feature extraction for three-dimensional data takes much time.</p><p id="p-0010" num="0008">To solve such a problem, the invention proposes a method of estimating, from images, features including information on shapes and irregularities obtained from three-dimensional data and performing feature extraction. With the invention, an object is to configure an object search device capable of expressing information on shapes and irregularities as features only by images, in a search for an object that is characteristic in shape or irregularity, and performing an accurate search.</p><heading id="h-0007" level="1">Solution to Problem</heading><p id="p-0011" num="0009">A preferred example of an object search device of the invention includes: an image feature extraction unit that is configured with a first neural network and is configured to input an image to extract an image feature; a three-dimensional data feature extraction unit that is configured with a second neural network and is configured to input three-dimensional data to extract a three-dimensional data feature; a learning unit that is configured to extract an image feature and a three-dimensional data feature from an image and three-dimensional data of an object that are obtained from a same individual, respectively, and update an image feature extraction parameter so as to reduce a difference between the image feature and the three-dimensional data feature; and a search unit that is configured to extract image features of a query image and a gallery image of the object by the image feature extraction unit using the updated image feature extraction parameter, and calculate a similarity between the image features of both images to search for the object.</p><p id="p-0012" num="0010">As another characteristic of the invention, the object search device further includes: a tracking unit that is configured with a third neural network, and is configured to input a plurality of images that are continuous in chronological order, and connect objects of a same type as objects, which are spatially close to each other in distance, based on detection results of the images, so as to generate, as tracking data, a plurality of images of a same individual that are continuous in chronological order; and a tracking data storage unit that is configured to store the tracking data generated by the tracking unit, in which the search unit is configured to perform object search by using the tracking data, which is obtained from the tracking unit and the tracking data storage unit, and the images.</p><p id="p-0013" num="0011">A preferred example of an object search method of the invention includes: performing learning processing of inputting an image and three-dimensional data of an object that are obtained from a same individual, extracting an image feature from the image by a first neural network, extracting a three-dimensional data feature from the three-dimensional data by a second neural network, and updating an image feature extraction parameter so as to reduce a difference between the image feature and the three-dimensional data feature; and extracting image features of a query image and a gallery image of the object by the first neural network of an image feature extraction unit using the updated image feature extraction parameter, and calculating a similarity between the image features of both images to search for the object.</p><p id="p-0014" num="0012">As another characteristic of the invention, in the object search method, the image feature extraction parameter is a weight and a bias of each neuron constituting the first neural network.</p><heading id="h-0008" level="1">Advantageous Effect</heading><p id="p-0015" num="0013">According to the invention, it is possible to provide a highly accurate object search device that expresses three-dimensional information such as shapes and irregularities as a feature only based on an image. It is not necessary to handle three-dimensional data at the time of a search, and an effect is obtained that the problem of a higher installation cost of depth cameras and the problem of much processing time of the feature extraction do not occur. Other effects will be described in each embodiment.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0009" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0016" num="0014"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a configuration diagram of an object search device according to a first embodiment.</p><p id="p-0017" num="0015"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a functional block diagram of functional units controlled by a learning control unit or a search control unit of the object search device.</p><p id="p-0018" num="0016"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flowchart of an example of learning processing in the first embodiment.</p><p id="p-0019" num="0017"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flowchart of an example of search processing in the first embodiment.</p><p id="p-0020" num="0018"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram showing an example of an image and three-dimensional data required for the learning processing used in the present embodiment.</p><p id="p-0021" num="0019"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a table showing annotation data required for the learning processing used in the first embodiment.</p><p id="p-0022" num="0020"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a configuration diagram of an object search device according to a second embodiment.</p><p id="p-0023" num="0021"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a functional block diagram of functional units controlled by a learning control unit or a search control unit of the object search device of the second embodiment.</p><p id="p-0024" num="0022"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a flowchart of an example of tracking processing used in the second embodiment.</p><p id="p-0025" num="0023"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a flowchart of an example of search processing used in the second embodiment.</p><p id="p-0026" num="0024"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a table showing annotation data required for the learning processing used in the second embodiment.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0010" level="1">DESCRIPTION OF EMBODIMENTS</heading><p id="p-0027" num="0025">Hereinafter, embodiments of the invention will be described with reference to the drawings.</p><heading id="h-0011" level="1">First Embodiment</heading><p id="p-0028" num="0026">In the present embodiment, an example in which an object search device serving as an example searches for baggage such as a suitcase will be described. A target to be searched does not have to be the baggage, and may be any object that can be imaged.</p><p id="p-0029" num="0027"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a configuration diagram of the object search device according to the present embodiment.</p><p id="p-0030" num="0028">An object search device <b>100</b> can be configured on a general-purpose computer, and a hardware configuration thereof includes: an arithmetic unit <b>110</b> including a central processing unit (CPU), a random access memory (RAM), etc.; a storage unit <b>120</b> including a read only memory (ROM), a hard disk drive (HDD), a solid state drive (SSD) using a flash memory or the like, etc.; an input unit <b>130</b> including an input device such as a keyboard or mouse; a display unit <b>140</b> including a display device such as a liquid crystal display (LCD) or organic EL display; a communication unit <b>150</b> including a network interface card (NIC) or the like; and the like.</p><p id="p-0031" num="0029">The communication unit <b>150</b> is connected, via a network <b>160</b>, to a monitoring image database_A<b>171</b>, a monitoring image database_Z<b>172</b>, and cameras <b>181</b> and <b>182</b> installed in various places, which are shared with an external monitoring system.</p><p id="p-0032" num="0030">The arithmetic unit <b>110</b> implements the following functional units by loading an object search program <b>121</b> stored in the storage unit <b>120</b> into the RAM and executing the object search program <b>121</b> by the CPU. The arithmetic unit <b>110</b> includes a learning control unit <b>111</b>, a search control unit <b>112</b>, an image/three-dimensional data acquisition unit <b>113</b>, a learning unit <b>114</b>, a search unit <b>115</b>, an image feature extraction unit <b>116</b>, and a three-dimensional data feature extraction unit <b>117</b>.</p><p id="p-0033" num="0031">The learning control unit <b>111</b> is activated by an instruction of a learning executor (a human or a computer that controls learning), and controls each of the following functional units in order to execute learning processing requested by the learning executor.</p><p id="p-0034" num="0032">The search control unit <b>112</b> is activated by an instruction of a search executor (a human or a computer that performs a search), and controls the following functional units in order to execute search processing requested by the search executor.</p><p id="p-0035" num="0033"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows a functional block diagram of the functional units controlled by the learning control unit <b>111</b> or the search control unit <b>112</b>.</p><p id="p-0036" num="0034">The image/three-dimensional data acquisition unit <b>113</b> processes input and output of data when executing the learning processing or the search processing. When executing the learning processing, the image/three-dimensional data acquisition unit <b>113</b> accepts images, three-dimensional data, and annotation data used for the learning processing from the learning executor, or receives images and three-dimensional data requested by the learning executor from the outside, and transmits these types of data to the learning unit <b>114</b>. Here, the annotation data is correct answer data for performing learning.</p><p id="p-0037" num="0035">When executing the search processing, the image/three-dimensional data acquisition unit <b>113</b> accepts a query image and a gallery image from the search executor, or alternatively acquires both or either of a query image and a gallery image requested by the search executor from the outside, and transmits those images to the search unit <b>115</b>. Here, the query image is an image showing the baggage to be searched, and the gallery image is an image that is to be compared with based on the query image to search for an image that shows the same individual baggage as the baggage to be searched.</p><p id="p-0038" num="0036">The learning unit <b>114</b> updates, by learning, parameters (weights and biases of neurons constituting a neural network) used when the image feature extraction unit <b>116</b> and the three-dimensional data feature extraction unit <b>117</b> extract features. At the time of executing the learning, the learning unit <b>114</b> receives images, three-dimensional data, and annotation data used for the learning from the image/three-dimensional data acquisition unit <b>113</b>. The learning unit <b>114</b> transmits the received image to the image feature extraction unit <b>116</b>, and receives a feature extracted by the image feature extraction unit <b>116</b>. The learning unit <b>114</b> transmits the received three-dimensional data to the three-dimensional data feature extraction unit <b>117</b>, and receives a feature extracted by the three-dimensional data feature extraction unit <b>117</b>. The learning unit <b>114</b> receives the parameters for feature extraction from the image feature extraction unit <b>116</b> and the three-dimensional data feature extraction unit <b>117</b>, and transmits the updated parameters to the image feature extraction unit <b>116</b> and the three-dimensional data feature extraction unit <b>117</b>.</p><p id="p-0039" num="0037">The search unit <b>115</b> performs input and output processing, calculation, and display management for a search. At the time of executing the search, the search unit <b>115</b> receives a query image and a gallery image used for the search from the image/three-dimensional data acquisition unit <b>113</b>, and transmits these images to the image feature extraction unit <b>116</b>. The search unit <b>115</b> receives a feature from the image feature extraction unit <b>116</b>, and transmits a search result calculated based on the feature to the display unit <b>140</b>.</p><p id="p-0040" num="0038">The display unit <b>140</b> displays the search result. At the time of executing the search, the display unit <b>140</b> acquires the query image, the gallery image, and a display order from the search unit <b>115</b>, and displays the images according to the display order.</p><p id="p-0041" num="0039">The image feature extraction unit <b>116</b> extracts features required for similarity calculation from images. At the time of executing the learning, the image feature extraction unit <b>116</b> receives the images from the learning unit <b>114</b>, and transmits the extracted features to the learning unit <b>114</b>. In addition, the image feature extraction unit <b>116</b> transmits the parameters for feature extraction to the learning unit <b>114</b>, and receives the updated parameters from the learning unit <b>114</b>. At the time of executing the search, the image feature extraction unit <b>116</b> receives the images from the search unit <b>115</b>, and transmits the extracted feature to the search unit <b>115</b>.</p><p id="p-0042" num="0040">The three-dimensional data feature extraction unit <b>117</b> extracts features required for the similarity calculation from three-dimensional data. At the time of executing the learning, the three-dimensional data feature extraction unit <b>117</b> receives the three-dimensional data from the learning unit <b>114</b>, and transmits the extracted features to the learning unit <b>114</b>. The three-dimensional data feature extraction unit <b>117</b> transmits the parameters for feature extraction to the learning unit <b>114</b>, and receives the updated feature from the learning unit <b>114</b>.</p><p id="p-0043" num="0041">An image feature extraction parameter <b>122</b> of the storage unit <b>120</b> stores parameters (weights and biases of neurons constituting the neural network) used when the image feature extraction unit <b>116</b> extracts an image feature.</p><p id="p-0044" num="0042">A three-dimensional data feature extraction parameter <b>123</b> of the storage unit <b>120</b> stores parameters (weights and biases of neurons constituting the neural network) used when the three-dimensional data feature extraction unit <b>117</b> extracts a three-dimensional data feature.</p><p id="p-0045" num="0043"><figref idref="DRAWINGS">FIG. <b>3</b></figref> shows a flowchart of an example of the learning processing used in the present embodiment.</p><p id="p-0046" num="0044">In step S<b>100</b>, the learning control unit <b>111</b> is activated by a learning start operation of the learning executor to start the learning processing.</p><p id="p-0047" num="0045">In step S<b>101</b>, the learning executor inputs images, three-dimensional data, and annotation data required for the learning into the image/three-dimensional data acquisition unit. Here, when the images include color information, three-dimensional data including color information is used.</p><p id="p-0048" num="0046">In step S<b>102</b>, the image/three-dimensional data acquisition unit transmits the images, the three-dimensional data, and the annotation data received in step S<b>101</b> to the learning unit <b>114</b>.</p><p id="p-0049" num="0047">In step S<b>103</b>, the learning unit transmits the images received in step S<b>102</b> to the image feature extraction unit <b>116</b>.</p><p id="p-0050" num="0048">In step S<b>104</b>, the image feature extraction unit <b>116</b> performs feature extraction on the images received in step S<b>103</b>. For example, a CNN is used for the feature extraction. The CNN accepts the images as an input, converts the images into numerical vectors as features, and outputs the features. This conversion is performed by repeating a convolution operation by using a parameter held by the CNN (using the image feature extraction parameter <b>122</b>). The parameter can be initially set to any value, and the value of the parameter is updated by learning.</p><p id="p-0051" num="0049">In step S<b>105</b>, the image feature extraction unit transmits the features extracted in step S<b>104</b> and the parameter used for the feature extraction (image feature extraction parameter <b>122</b>) to the learning unit <b>114</b>.</p><p id="p-0052" num="0050">In step S<b>106</b>, the learning unit <b>114</b> stores the image features received in step S<b>105</b> and the image feature extraction parameter.</p><p id="p-0053" num="0051">In step S<b>107</b>, the learning unit transmits the three-dimensional data received in step S<b>102</b> to the three-dimensional data feature extraction unit <b>117</b>.</p><p id="p-0054" num="0052">In step S<b>108</b>, the three-dimensional data feature extraction unit <b>117</b> performs the feature extraction on the three-dimensional data received in step S<b>107</b>. For the feature extraction, for example, a neural network (NN) for three-dimensional data is used. For example, a point net or the like is used. The NN for three-dimensional data accepts the three-dimensional data as an input, converts the three-dimensional data into numerical vectors as features, and outputs the features. This conversion is performed by repeating processing of: using a parameter held by the NN for three-dimensional data (using the three-dimensional data feature extraction parameter <b>123</b>) to linearly convert the three-dimensional data and inputting the converted three-dimensional data to a nonlinear function. The parameter can be initially set to any value, and the value of the parameter is updated by learning.</p><p id="p-0055" num="0053">In step S<b>109</b>, the three-dimensional data feature extraction unit <b>117</b> transmits the features extracted in step S<b>108</b> and the parameter used for the feature extraction (the three-dimensional data feature extraction parameter <b>123</b>) to the learning unit.</p><p id="p-0056" num="0054">In step S<b>110</b>, the learning unit <b>114</b> stores the three-dimensional data features received in step <b>3109</b> and the three-dimensional data feature extraction parameter.</p><p id="p-0057" num="0055">In step S<b>111</b>, the learning unit <b>114</b> performs parameter update work by using the image features and the image feature extraction parameter stored in step S<b>106</b>, and the three-dimensional data features and the three-dimensional data feature extraction parameter stored in step S<b>110</b>. For example, the stochastic gradient descent method is used to update the parameters.</p><p id="p-0058" num="0056">The stochastic gradient descent method requires an objective function, and an objective function E<sub>image </sub>used for updating the image feature extraction parameter is calculated as in Formula (1) using, for example, an image feature f<sub>image </sub>of the same individual and a three-dimensional data feature f<sub>3d</sub>.</p><p id="p-0059" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>[Formula 1]<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0060" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>E</i><sub>image</sub><i>=&#x2225;f</i><sub>image</sub><i>&#x2212;f</i><sub>3d</sub>&#x2225;<sub>2</sub>&#x2003;&#x2003;(1)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0061" num="0057">This is a squared error between the image feature f<sub>image </sub>and the three-dimensional data feature f<sub>3d</sub>. The image feature extraction parameter is updated in a direction of minimizing the objective function E<sub>image</sub>. When calculating the objective function for a plurality of individuals, for example, the E<sub>image</sub>, can be calculated for the plurality of individuals, and a mean square error can be used, which takes the average thereof.</p><p id="p-0062" num="0058">For example, the objective function E<sub>3d </sub>used for updating the parameters for three-dimensional data feature extraction uses Softmax cross-entropy by individual identification. The three-dimensional data feature extraction parameter is updated in a direction of minimizing the objective function E<sub>3d</sub>. When calculating the objective function for a plurality of individuals, for example, the E<sub>3d </sub>is calculated for the plurality of individuals and an average is taken.</p><p id="p-0063" num="0059">In step S<b>112</b>, the learning unit <b>114</b> transmits the updated values of the parameters calculated in step S<b>111</b> to the image feature extraction unit <b>116</b> and the three-dimensional data feature extraction unit <b>117</b>.</p><p id="p-0064" num="0060">In step S<b>113</b>, by using the updated values of the parameters received in step S<b>112</b>, the image feature extraction unit <b>116</b> and the three-dimensional data feature extraction unit <b>117</b> update each of the feature extraction parameters (the image feature extraction parameter <b>122</b>, and the three-dimensional data feature extraction parameter <b>123</b>).</p><p id="p-0065" num="0061">In step S<b>114</b>, the learning control unit <b>111</b> determines whether the learning executor continues or ends the learning, based on initial request contents and a processing result. When it is determined that the learning executor continues the learning, the process proceeds to S<b>101</b>, and when it is determined that the learning executor ends the learning, the process proceeds to S<b>115</b>.</p><p id="p-0066" num="0062">In step S<b>115</b>, the learning processing is ended.</p><p id="p-0067" num="0063">As illustrated in the present embodiment, with the learning processing, it is made possible for the three-dimensional data feature extraction unit to extract similar features for the same individual, and it is made possible for the image feature extraction unit to extract a feature close to the features output by the three-dimensional data feature extraction unit. Accordingly, the image feature extraction unit can extract, from images, features as output by the three-dimensional data feature extraction unit. This makes it possible to perform extraction of similar features in consideration of a three-dimensional structure, in the image feature extraction at the time of the search. By using these features, it is possible to improve the accuracy for searching for an object that is characteristic in shape. When performing a search using three-dimensional data, it is generally necessary to install both an RGB camera and a depth camera, or an RGB-D camera in the entire monitoring area, which increases the installation cost. However, if the present method is used, it is sufficient to prepare the three-dimensional data only at the time of the learning, and it is possible to prevent the increase in the cost of installing the cameras.</p><p id="p-0068" num="0064">Regarding a timing of learning the image feature extraction parameter and the three-dimensional data feature extraction parameter in this flow, the parameters may be learned at the same time, or the three-dimensional data feature extraction parameter may be learned first, and then the image feature extraction parameter may be learned later.</p><p id="p-0069" num="0065">Although a method for searching for the same individual is described in the present embodiment, this method can be used not only for searching for the same individual but also for object identification and object detection, and the accuracy can be improved.</p><p id="p-0070" num="0066"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows a flowchart of an example of the search processing used in the present embodiment.</p><p id="p-0071" num="0067">In step S<b>200</b>, the search control unit <b>112</b> is activated by a search start operation of the search executor to start the search processing.</p><p id="p-0072" num="0068">In step S<b>201</b>, the search executor inputs query images and gallery images required for the search processing to the image/three-dimensional data acquisition unit <b>113</b>. Alternatively, the search control unit <b>112</b>, at the request of the search executor, causes the image/three-dimensional data acquisition unit to acquire the query images and the gallery images necessary for the search processing from the cameras <b>181</b> and <b>182</b> installed in various places, the external monitoring image databases <b>171</b> and <b>172</b>, or the like.</p><p id="p-0073" num="0069">In step S<b>202</b>, the image/three-dimensional data acquisition unit <b>113</b> transmits the images received in step S<b>201</b> to the search unit <b>115</b>.</p><p id="p-0074" num="0070">In step S<b>203</b>, the search unit <b>115</b> transmits the images received in step S<b>202</b> to the image feature extraction unit <b>116</b>.</p><p id="p-0075" num="0071">In step S<b>204</b>, the image feature extraction unit performs the feature extraction on the images received in step S<b>203</b>. As the feature extraction method, the same method as used in step S<b>104</b> in the learning processing flow is used. In the learning processing, when the learning is performed by using an image including color information and three-dimensional data including color information, the feature extraction can be performed on the image including color information.</p><p id="p-0076" num="0072">In step S<b>205</b>, the image feature extraction unit <b>116</b> transmits features extracted in step S<b>204</b> to the search unit <b>115</b>.</p><p id="p-0077" num="0073">In step S<b>206</b>, the search unit <b>115</b> determines a display order of search results by using the features received in step S<b>205</b>. When determining the display order using the features, it is necessary to calculate a similarity of the features. For the calculation of the similarity, for example, there is a method using the Euclidean distance of numerical vectors representing the features. Assuming that a feature of the query image is f<sub>q </sub>and a feature of the gallery image is f<sub>g</sub>, the similarity s can be calculated as in Formula (2).</p><p id="p-0078" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mrow>  <mo>[</mo>  <mrow>   <mi>Formula</mi>   <mo>&#x2062;</mo>   <mtext>   </mtext>   <mn>2</mn>  </mrow>  <mo>]</mo> </mrow></math></maths><maths id="MATH-US-00001-2" num="00001.2"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mi>s</mi>     <mo>=</mo>     <mrow>      <mn>1.</mn>      <mo>-</mo>      <mfrac>       <msub>        <mrow>         <mo>&#xf605;</mo>         <mrow>          <msub>           <mi>f</mi>           <mi>q</mi>          </msub>          <mo>-</mo>          <msub>           <mi>f</mi>           <mi>g</mi>          </msub>         </mrow>         <mo>&#xf606;</mo>        </mrow>        <mn>2</mn>       </msub>       <mn>2.</mn>      </mfrac>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>2</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0079" num="0074">The similarity takes a value from 0.0 to 1.0, and the larger the value, the higher the similarity. The display order is determined such that, for example, those having a high similarity are preferentially displayed.</p><p id="p-0080" num="0075">In step S<b>207</b>, the search unit <b>115</b> transmits the images received in step S<b>202</b> and the display order determined in step S<b>206</b> to the display unit <b>140</b>.</p><p id="p-0081" num="0076">In step S<b>208</b>, the display unit displays the search results using the images and the display order received in step S<b>207</b>. In the display, for example, the search results are displayed from the top of a screen in a descending order of the display order.</p><p id="p-0082" num="0077">In step S<b>209</b>, the search control unit <b>112</b> determines whether the search executor continues or ends the search, based on initial request contents and a processing result. When it is determined that the search executor continues the search, the process proceeds to S<b>201</b>, and when it is determined that the search executor ends the search, the process proceeds to S<b>210</b>.</p><p id="p-0083" num="0078">In step S<b>210</b>, the search process is ended.</p><p id="p-0084" num="0079">As illustrated in the present embodiment, since only the features of the images are used at the time of the search processing, the above-described camera for acquiring the three-dimensional data is not required in the monitoring area, and the increase in the installation cost of the camera can be prevented. Since the data amount in the three-dimensional data is larger than the data amount in the images, it takes more time to process the three-dimensional data. Therefore, when the feature extraction is performed by using the three-dimensional data at the time of the search processing, the search accuracy is improved, but the search speed is lowered. However, if the present method is used, features obtained from the three-dimensional data can be reproduced by processing only the images, and thus, the search accuracy can be improved without causing a decrease in the processing speed.</p><p id="p-0085" num="0080"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram showing an example of an image and three-dimensional data required for the learning processing used in the present embodiment. Each item in the drawing will be described below.</p><p id="p-0086" num="0081"><b>500</b> indicates an item of baggage to be photographed.</p><p id="p-0087" num="0082"><b>501</b> indicates a camera that photographs the baggage and generates an image and three-dimensional data. As this camera, one camera capable of acquiring both an image and three-dimensional data such as an RGB-D camera may be used, or both an RGB camera and a depth camera may be used.</p><p id="p-0088" num="0083"><b>502</b> indicates an image that can be acquired when the baggage <b>500</b> is photographed by the camera <b>501</b>.</p><p id="p-0089" num="0084"><b>503</b> indicates three-dimensional data that can be acquired when the baggage <b>500</b> is photographed by the camera <b>501</b>. As for an expression method of the three-dimensional data, any method such as a point cloud or Voxel that can express spatial information may be used.</p><p id="p-0090" num="0085"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a table showing annotation data required for the learning processing used in the present embodiment. The format of the annotation data will be described below.</p><p id="p-0091" num="0086"><b>600</b> indicates a data table stored as the annotation data. Items of an image ID, a three-dimensional data ID, and a baggage ID are stored in the data table in association with one another. The image ID is an ID assigned to uniquely identify an image. For example, the ID is incremented by one each time an image is acquired, and then is assigned. The three-dimensional data ID is an ID assigned to uniquely identify the three-dimensional data. For example, the ID is incremented by one each time the three-dimensional data is acquired, and then is assigned. The image and the three-dimensional data are not necessarily acquired at the same time-point, but it is desirable to have a combination of the image and the three-dimensional data that are acquired at possibly closest time-points. Regarding a method of determining the combination, for example, for images and three-dimensional data of the same individual acquired at close time-points, a computer may assign a same ID for the image ID and the three-dimensional data ID at the time of the acquisition to determine the combination, or a person may later confirm the images and the three-dimensional data to determine the combination. The baggage ID is an ID for uniquely identifying the same individual, and is assigned to the acquired images and three-dimensional data by annotation work. In the data table <b>600</b>, two rows from the top represent the images and the three-dimensional data of the same individual.</p><p id="p-0092" num="0087">By performing the learning by using the annotation data as illustrated in the present embodiment, it is possible to associate the features extracted from the images with the features extracted from the three-dimensional data at the time of the learning. This method can be implemented by performing learning with the learning processing flow shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref> using this association.</p><heading id="h-0012" level="1">Second Embodiment</heading><p id="p-0093" num="0088">In the present embodiment, an example in which the object search device serving as an example searches for baggage such as a suitcase will be described. The target to be searched does not have to be the baggage, and may be any object that can be imaged. In contrast to the first embodiment, the present embodiment is an example in which, as for images used in the learning processing and the search processing, by tracking an object in a camera (capturing the same object in frames continuous in chronological order in one camera), it is known in advance that the object is the same individual in a plurality of images, and as for the three-dimensional data used for the learning, the three-dimensional data is made into one piece of data in advance by a method such as three-dimensional reconstruction (by capturing an object while rotating around the object with a three-dimensional camera and obtaining data of the entire object to reconstruct three dimensions of the object). The present embodiment is the same as the first embodiment except that a plurality of images and three-dimensionally reconstructed three-dimensional data are used for each individual during the learning processing and the search processing.</p><p id="p-0094" num="0089"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a configuration diagram of an object search device <b>200</b> according to the second embodiment. The object search device <b>200</b> of the present embodiment is configured by adding a tracking unit <b>118</b>, a tracking data storage unit <b>124</b>, and a tracking image feature extraction parameter <b>125</b> to the object search device shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref> of the first embodiment.</p><p id="p-0095" num="0090"><figref idref="DRAWINGS">FIG. <b>8</b></figref> shows a functional block diagram of functional units of the object search device <b>200</b> of the second embodiment that are controlled by the learning control unit <b>111</b> or the search control unit <b>112</b>. This functional block diagram is configured by adding the tracking unit <b>118</b> and the tracking data storage unit <b>124</b> to the functional block diagram shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref> of the first embodiment.</p><p id="p-0096" num="0091">The tracking unit <b>118</b> performs tracking processing (of grouping together images detected to be capturing the same object and assigning the same tracking ID thereto) on a plurality of input images that are continuous in chronological order. When executing the search processing, the tracking unit <b>118</b> receives the plurality of images that are continuous in chronological order from the image/three-dimensional data acquisition unit <b>113</b>, and transmits tracking data that is a tracking result and the images to the search unit <b>115</b>.</p><p id="p-0097" num="0092">The tracking data storage unit <b>124</b> stores the tracking data. When executing the tracking, the tracking data storage unit <b>124</b> receives the tracking data and the images from the tracking unit <b>118</b>. When executing the search processing, the tracking data storage unit <b>124</b> transmits the tracking data and the images to the search unit <b>115</b>.</p><p id="p-0098" num="0093"><figref idref="DRAWINGS">FIG. <b>9</b></figref> shows a flowchart of an example of the tracking processing used in the present embodiment.</p><p id="p-0099" num="0094">In step S<b>300</b>, the tracking unit starts the tracking processing according to a tracking start instruction of a tracking executor. The tracking executor is a person or a computer that controls the tracking. In the present embodiment, the tracking executor is the learning control unit <b>111</b> or the search control unit <b>112</b> that is activated by the operation of the learning executor or the search executor.</p><p id="p-0100" num="0095">In step S<b>301</b>, the tracking executor inputs the plurality of images continuous in chronological order for performing the tracking to the image/three-dimensional data acquisition unit <b>113</b>.</p><p id="p-0101" num="0096">In step S<b>302</b>, the image/three-dimensional data acquisition unit transmits the plurality of images continuous in chronological order that are received in step S<b>301</b> to the tracking unit <b>118</b>.</p><p id="p-0102" num="0097">In step S<b>303</b>, the tracking unit <b>118</b> performs the tracking processing on the plurality of images continuous in chronological order that are received in step S<b>302</b>. For the tracking processing, for example, the following processing procedure is performed. First, a CNN detector is used to detect baggage (tracking target) in each image (the CNN detector is subjected to the learning processing in advance. Parameters used for detections by the CNN detector are stored in the tracking image feature extraction parameter <b>125</b>). A detection result includes a type of the baggage and a position and a size of the baggage in an image. Next, regarding the detection results of images that are adjacent to each other in chronological order, the detection results of baggage of the same type are linked to each other as detection results spatially close to each other in distance. For the calculation of the distance, for example, the Euclidean distance of a detection position is used. From a tracking result obtained in this way, a tracking ID, which uniquely determines the same individual, and a plurality of images continuous in chronological order of the individual indicated by the tracking ID are generated as the tracking data.</p><p id="p-0103" num="0098">In step S<b>304</b>, the tracking unit <b>118</b> stores the tracking data generated in step S<b>303</b> in the tracking data storage unit.</p><p id="p-0104" num="0099">In step S<b>305</b>, the tracking executor determines whether to continue or end the tracking.</p><p id="p-0105" num="0100">In step S<b>306</b>, the tracking is ended.</p><p id="p-0106" num="0101">By performing the tracking processing as illustrated in the present embodiment, the plurality of images can be obtained in advance for the same individual, and feature extraction can be performed by using the plurality of images at the time of the search processing.</p><p id="p-0107" num="0102"><figref idref="DRAWINGS">FIG. <b>10</b></figref> shows a flowchart of an example of the search processing used in the present embodiment. This flow is the same as the flow of the first embodiment (<figref idref="DRAWINGS">FIG. <b>4</b></figref>) except that the tracking processing is performed. Hereinafter, only the flow of the tracking processing will be described.</p><p id="p-0108" num="0103">In step S<b>401</b>, the search executor inputs, to the image/three-dimensional data acquisition unit <b>113</b>, a plurality of images continuous in chronological order showing an object as a target to be searched.</p><p id="p-0109" num="0104">In step S<b>402</b>, the image/three-dimensional data acquisition unit <b>113</b> transmits the images received in step S<b>401</b> to the tracking unit <b>118</b>.</p><p id="p-0110" num="0105">In step S<b>403</b>, the tracking unit <b>118</b> performs the tracking processing on the images received in step S<b>402</b>.</p><p id="p-0111" num="0106">In step S<b>404</b>, the tracking unit transmits tracking data obtained as a result of the tracking processing performed in step S<b>403</b> and the images to the search unit <b>115</b>.</p><p id="p-0112" num="0107">In step S<b>405</b>, the search unit transmits the tracking data and the images received in step S<b>404</b> to the image feature extraction unit <b>116</b>.</p><p id="p-0113" num="0108">In step S<b>406</b>, the tracking data and the images stored in the tracking data storage unit <b>124</b> are transmitted to the image feature extraction unit <b>116</b>.</p><p id="p-0114" num="0109">In step S<b>407</b>, the image feature extraction unit <b>116</b> performs the feature extraction by using the tracking data and the images received in step S<b>405</b> and step S<b>406</b>. To extract features from the plurality of images of the same individual, for example, a 3DCNN is used. In this case, the 3DCNN is also used for the image feature extraction unit in the learning flow.</p><p id="p-0115" num="0110">In this flow, the tracking processing is performed on a plurality of items of baggage in advance, and the tracking data and the images thereof are stored in the tracking data storage unit <b>124</b>. When executing the search processing, the images stored in the tracking data storage unit <b>124</b> in advance are searched as gallery images.</p><p id="p-0116" num="0111">As illustrated in the present embodiment, by performing the tracking processing at the time of executing the search processing, a plurality of images taken from various directions for the same individual can be obtained. By using these images, which are obtained from a plurality of viewpoints, in the feature extraction, it is easy to reproduce the features obtained from the three-dimensional data, and the accuracy can be improved.</p><p id="p-0117" num="0112"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a table showing annotation data required for the learning processing used in the present embodiment. The format of the annotation data will be described below.</p><p id="p-0118" num="0113"><b>700</b> indicates a data table stored as the annotation data. An image ID, a three-dimensional data ID, a tracking ID, and a baggage ID are stored in the data table in association with one another. The image ID, the three-dimensional data ID, and the baggage ID are the same as those in the data table <b>600</b> shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. As the tracking ID, an ID assigned to uniquely identify an object as the same individual and as the result of the tracking processing is used. As for the three-dimensional data, one piece of three-dimensional data is created by using, for example, three-dimensional reconstruction. Thus, one piece of three-dimensional data is associated with a plurality of images.</p><p id="p-0119" num="0114">By performing the learning processing by using the annotation data as shown in the present embodiment, the learning processing can be performed so as to extract a characteristic of an object when the object is viewed from various directions, as a feature, and the accuracy of the search processing can be improved.</p><p id="p-0120" num="0115">The invention is not limited to the above embodiments, and includes various modifications. For example, the embodiments described above have been described in detail for easy understanding of the invention, and the invention is not necessarily limited to those including all of the configurations described above. A part of a configuration according to a certain embodiment can be replaced with a configuration according to another embodiment, and a configuration according to a certain embodiment can be added to a configuration according to another embodiment. A part of the configuration of each embodiment may be added to, deleted from, or replaced with another configuration.</p><heading id="h-0013" level="1">REFERENCE SIGN LIST</heading><p id="p-0121" num="0000"><ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0000">    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0116"><b>100</b> object search device</li>        <li id="ul0002-0002" num="0117"><b>110</b> arithmetic unit</li>        <li id="ul0002-0003" num="0118"><b>111</b> learning control unit</li>        <li id="ul0002-0004" num="0119"><b>112</b> search control unit</li>        <li id="ul0002-0005" num="0120"><b>113</b> image/three-dimensional data acquisition unit</li>        <li id="ul0002-0006" num="0121"><b>114</b> learning unit</li>        <li id="ul0002-0007" num="0122"><b>115</b> search unit</li>        <li id="ul0002-0008" num="0123"><b>116</b> image feature extraction unit</li>        <li id="ul0002-0009" num="0124"><b>117</b> three-dimensional data feature extraction unit</li>        <li id="ul0002-0010" num="0125"><b>118</b> tracking unit</li>        <li id="ul0002-0011" num="0126"><b>120</b> storage unit</li>        <li id="ul0002-0012" num="0127"><b>121</b> object search program</li>        <li id="ul0002-0013" num="0128"><b>122</b> image feature extraction parameter</li>        <li id="ul0002-0014" num="0129"><b>123</b> three-dimensional data feature extraction parameter</li>        <li id="ul0002-0015" num="0130"><b>124</b> tracking data storage unit</li>        <li id="ul0002-0016" num="0131"><b>125</b> tracking image feature extraction parameter</li>        <li id="ul0002-0017" num="0132"><b>130</b> input unit</li>        <li id="ul0002-0018" num="0133"><b>140</b> display unit</li>        <li id="ul0002-0019" num="0134"><b>150</b> communication unit</li>        <li id="ul0002-0020" num="0135"><b>160</b> network</li>        <li id="ul0002-0021" num="0136"><b>171</b>, <b>172</b> monitoring image database</li>        <li id="ul0002-0022" num="0137"><b>181</b>, <b>182</b> cameras installed in various places</li>        <li id="ul0002-0023" num="0138"><b>200</b> object search device of second embodiment</li>        <li id="ul0002-0024" num="0139"><b>500</b> baggage to be photographed</li>        <li id="ul0002-0025" num="0140"><b>501</b> camera for photographing baggage</li>        <li id="ul0002-0026" num="0141"><b>502</b> image that can be acquired when baggage <b>500</b> is photographed by camera <b>501</b></li>        <li id="ul0002-0027" num="0142"><b>503</b> three-dimensional data that can be acquired when baggage <b>500</b> is photographed by camera <b>501</b></li>        <li id="ul0002-0028" num="0143"><b>600</b> data table stored as annotation data</li>        <li id="ul0002-0029" num="0144"><b>700</b> data table stored as annotation data of second embodiment</li>    </ul>    </li></ul></p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001 MATH-US-00001-2" nb-file="US20230005268A1-20230105-M00001.NB"><img id="EMI-M00001" he="9.91mm" wi="76.20mm" file="US20230005268A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An object search device, comprising:<claim-text>an image feature extraction unit that is configured with a first neural network, and is configured to input an image to extract an image feature;</claim-text><claim-text>a three-dimensional data feature extraction unit that is configured with a second neural network, and is configured to input three-dimensional data to extract a three-dimensional data feature;</claim-text><claim-text>a learning unit that is configured to extract an image feature and a three-dimensional data feature from an image and three-dimensional data of an object that are obtained from a same individual, respectively, and update an image feature extraction parameter so as to reduce a difference between the image feature and the three-dimensional data feature; and</claim-text><claim-text>a search unit that is configured to extract image features of a query image and a gallery image of the object by the image feature extraction unit using the updated image feature extraction parameter, and calculate a similarity between the image features of both images to search for the object.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The object search device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the learning unit is configured to accept an image, three-dimensional data, and annotation data of an object that are used for learning, and identify, by using the annotation data, an association between an image and three-dimensional data that are obtained from a same individual.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The object search device according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein<claim-text>the learning unit is configured to extract an image feature and a three-dimensional data feature from an image and three-dimensional data respectively that are identified, by using the annotation data, to be obtained from a same individual, and update an image feature extraction parameter and a three-dimensional data feature extraction parameter based on the extracted image feature and the three-dimensional data feature.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The object search device according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein<claim-text>the image feature extraction parameter and the three-dimensional data feature extraction parameter are a weight and a bias of each neuron constituting the first or second neural network, respectively.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The object search device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the search unit is configured to extract image features of a query image and a plurality of gallery images of an object, calculate a similarity between the query image and each gallery image based on the image feature of each image, and display the gallery images on a display unit in descending order of the similarity.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The object search device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the learning unit is configured to extract an image feature and a three-dimensional data feature by using an image including color information and three-dimensional data including color information, respectively, and update the image feature extraction parameter, and</claim-text><claim-text>the search unit is configured to extract image features of a query image and a gallery image of an object including color information, and calculate a similarity between the image features of both images to perform searching.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The object search device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>a tracking unit that is configured with a third neural network, and is configured to input a plurality of images that are continuous in chronological order, and connect objects of a same type as objects, which are spatially close to each other in distance, based on detection results of the images, so as to generate, as tracking data, a plurality of images of a same individual that are continuous in chronological order; and</claim-text><claim-text>a tracking data storage unit that is configured to store the tracking data generated by the tracking unit, wherein</claim-text><claim-text>the search unit is configured to perform object search by using the tracking data, which is obtained from the tracking unit and the tracking data storage unit, and the images.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The object search device according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein<claim-text>the learning unit is configured to perform learning processing by using the tracking data, which includes the plurality of images of the same individual and is obtained from the tracking unit, and annotation data that is associated with one piece of three-dimensional data obtained by three-dimensional reconstruction.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. An object search method, comprising:<claim-text>performing learning processing of: inputting an image and three-dimensional data of an object that are obtained from a same individual, extracting an image feature from the image by a first neural network, extracting a three-dimensional data feature from the three-dimensional data by a second neural network, and updating an image feature extraction parameter so as to reduce a difference between the image feature and the three-dimensional data feature, and</claim-text><claim-text>extracting image features of a query image and a gallery image of the object by the first neural network using the updated image feature extraction parameter, and calculating a similarity between the image features of both images to search the object.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The object search method according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein<claim-text>the image feature extraction parameter is a weight and a bias of each neuron constituting the first neural network.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The object search method according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein<claim-text>a plurality of images that are continuous in chronological order are input into a third neural network, and objects of a same type are connected as objects, which are spatially close to each other in distance, based on detection results of the images, so as to generate, as tracking data, a plurality of images of a same individual that are continuous in chronological order,</claim-text><claim-text>the plurality of images, which are continuous in chronological order of the same individual and collected as the tracking data, and one piece of three-dimensional data obtained by three-dimensional reconstruction of plural pieces of three-dimensional data of the individual are input, image features are extracted by the first neural network from the plurality of images that are continuous in chronological order of the same individual and collected as the tracking data, a three-dimensional data feature is extracted by the second neural network from the one piece of three-dimensional data obtained by the three-dimensional reconstruction, and the learning processing for updating the image feature extraction parameter is performed so as to reduce a difference between the image features and the three-dimensional data feature, and</claim-text><claim-text>the tracking data that is obtained by collecting the plurality of images continuous in chronological order of the same individual is input as the query image and gallery image of the object to search for the object.</claim-text></claim-text></claim></claims></us-patent-application>