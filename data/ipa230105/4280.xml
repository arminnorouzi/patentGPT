<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004281A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004281</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17781090</doc-number><date>20210517</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>CN</country><doc-number>202010573140.3</doc-number><date>20200622</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>04845</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>0488</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>0481</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>04845</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>0488</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>0481</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>2203</main-group><subgroup>04806</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>2203</main-group><subgroup>04803</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">INTELLIGENT INTERACTION METHOD AND DEVICE, AND STORAGE MEDIUM</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>BOE Technology Group Co., Ltd.</orgname><address><city>Beijing</city><country>CN</country></address></addressbook><residence><country>CN</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>LEI</last-name><first-name>Liping</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>ZHANG</last-name><first-name>Honglei</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>DONG</last-name><first-name>Wenchu</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>BOE Technology Group Co., Ltd.</orgname><role>03</role><address><city>Beijing</city><country>CN</country></address></addressbook></assignee></assignees><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/CN2021/094221</doc-number><date>20210517</date></document-id><us-371c12-date><date>20220531</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Provided are an intelligent interaction device and method, and a non-transitory computer readable storage medium. The method includes: displaying, on a touch screen, a current window of a multimedia file in a playing state; displaying, in response to an instruction from a user for zooming the current window, a zoomed window of the current window at a first predetermined position of the current window, the zoomed window being smaller than the current window; and displaying, in response to an annotation operation performed by the user for the zoomed window, an annotation in the zoomed window, and updating the current window by displaying the annotation in the current window.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="72.14mm" wi="141.22mm" file="US20230004281A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="189.57mm" wi="149.35mm" file="US20230004281A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="187.20mm" wi="140.80mm" file="US20230004281A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="199.05mm" wi="152.15mm" file="US20230004281A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="160.95mm" wi="159.00mm" file="US20230004281A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="208.36mm" wi="164.08mm" file="US20230004281A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">The present disclosure is a national phase entry under 35 U.S.C. &#xa7; 371 of International Application No. PCT/CN2021/094221, filed on May 17, 2021 and claims the priority of the Chinese Patent Application No. 202010573140.3, filed on Jun. 22, 2020, and titled &#x201c;INTELLIGENT INTERACTION METHOD, DEVICE, AND STORAGE MEDIUM,&#x201d; the entire contents of which are hereby incorporated by reference in their entireties.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">The present disclosure relates to the field of information display technology, and in particular, to an intelligent interaction method, device, and non-transitory computer readable storage medium.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">With the rapid development of display technology, the application of larger touch screens is becoming more and more widespread.</p><p id="p-0005" num="0004">It is difficult for the users to annotate or perform operation for the content displayed on the larger touch screen due to the large touch screen, and the operation is inefficient, if the user annotates directly with the cursor.</p><p id="p-0006" num="0005">It should be noted that the information disclosed in the &#x201c;BACKGROUND&#x201d; section is intended only to enhance the understanding of the context of this disclosure and may therefore include information that does not constitute prior art known to those of ordinary skill in the art.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0007" num="0006">The present disclosure provides an intelligent interaction method, device, and non-transitory computer readable storage medium.</p><p id="p-0008" num="0007">Other features and advantages of the present disclosure will become apparent through the following detailed description, or will be learned in part through the practice of the present disclosure.</p><p id="p-0009" num="0008">According to a first aspect of the present disclosure, there is provided an intelligent interaction method for annotation scenarios, executed by a processor, including: displaying, by the processor, on a touch screen a current window of a multimedia file in a playing state; displaying, by the processor, in response to an instruction from a user for zooming the current window, a zoomed window of the current window at a first predetermined position of the current window, the zoomed window being smaller than the current window; and displaying, by the processor, in response to an annotation operation performed by the user for the zoomed window, an annotation in the zoomed window, and updating the current window by displaying the annotation in the current window.</p><p id="p-0010" num="0009">According to a second aspect of the present disclosure, there is provided an intelligent interaction method, including: displaying, by a processor, on a touch screen a current window of a multimedia file in a playing state; displaying, by the processor, in response to an instruction from a user for zooming the current window, a zoomed window of the current window on the touch screen with the current window maintained; and switching, by the processor, in response to a control operation performed by the user for the zoomed window, from displaying a first content in the current window to displaying a second content in the current window, wherein the second content is displayed in the zoomed window corresponding to the control operation.</p><p id="p-0011" num="0010">According to a third aspect of the present disclosure, there is provided an intelligent interaction device for annotation scenarios, including: a touch screen, one or more processors coupled to the touch screen, and a memory for storing one or more programs that, when executed by the one or more processors, cause the one or more processors to implement the intelligent interaction method as described in any one of the foregoing embodiments.</p><p id="p-0012" num="0011">According to a fourth aspect of the present disclosure, there is provided a non-transitory computer readable storage medium, storing a computer program, where the computer program, when executed by one or more processors, implements the intelligent interaction method as described in any one of the foregoing embodiments.</p><p id="p-0013" num="0012">It should be understood that both the foregoing general description and the following detailed description are exemplary and explanatory only and not for limiting the present disclosure.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE ACCOMPANYING DRAWINGS</heading><p id="p-0014" num="0013">Accompanying drawings herein, which are incorporated into and constitute a part of the specification, illustrate embodiments conforming to the present disclosure, and are used to explain the principles of the present disclosure in conjunction with the specification. It will be apparent that the accompanying drawings in the following description only show some of the embodiments of the present disclosure, and that other drawings may be obtained from them without any creative efforts by one of ordinary skill in the art.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>1</b></figref> schematically illustrates a flow chart of steps performed by a processor of an intelligent interaction device in one exemplary embodiment of the present disclosure.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>2</b></figref> schematically illustrates a diagram of generating a zoomed window at a current user interface in exemplary embodiments of the present disclosure.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>3</b></figref> schematically illustrates a diagram of a position of a user in relation to a touch screen in exemplary embodiments of the present disclosure.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>4</b></figref> schematically illustrates a diagram in which a zoom icon is displayed in a current window in exemplary embodiments of the present disclosure.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>5</b></figref> schematically illustrates a diagram in which a predetermined region is enlarged and annotated with comments in exemplary embodiments of the present disclosure.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>6</b></figref> schematically illustrates a flow chart of the steps performed by the processor of the intelligent interaction device in another exemplary embodiment of the present disclosure.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>7</b></figref> schematically illustrates a schematic diagram in which the zoomed window is displayed on the touch screen other than the current window in one exemplary embodiment of the present disclosure.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>8</b></figref> schematically illustrates a schematic diagram in which the zoomed window is displayed on the touch screen other than the current window in another exemplary embodiment of the present disclosure.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>9</b></figref> schematically illustrates a structural schematic diagram of a computer system adapted to the intelligent interaction device for implementing the exemplary embodiments of the present disclosure.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>10</b></figref> schematically illustrates a diagram of a computer-readable storage medium according to some embodiments of the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0025" num="0024">Exemplary embodiments will now be described more fully with reference to the accompanying drawings. However, the exemplary embodiments may be implemented in various forms and should not be construed as limited to the examples set forth herein; rather, these embodiments are provided so that the present disclosure will be more comprehensive and complete, and will fully convey the concept of exemplary embodiments to those skilled in the art. The described features, structures, or characteristics may be combined in any suitable manner in one or more embodiments.</p><p id="p-0026" num="0025">In addition, the accompanying drawings are merely schematic illustrations of the present disclosure and are not necessarily drawn to scale. The same reference numerals in the drawings indicate the same or similar structures, and thus their detailed description will be omitted. Some of the block diagrams shown in the accompanying drawings are functional entities and do not necessarily have to correspond to physically or logically separate entities. These functional entities may be implemented in software form, or in one or more hardware modules or integrated circuits, or in different network and/or processor devices and/or microcontroller devices.</p><p id="p-0027" num="0026">In the exemplary embodiments, there is first provided an intelligent interaction method that can be applied to annotation scenarios of the content displayed on the touch screen. With reference to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the method includes the following steps performed by the processor of the device.</p><p id="p-0028" num="0027">S<b>110</b>, displaying on a touch screen a current window of a multimedia file in a playing state.</p><p id="p-0029" num="0028">S<b>120</b>, displaying, in response to an instruction from a user for zooming the current window, a zoomed window of the current window at a first predetermined position of the current window, the zoomed window being smaller than the current window.</p><p id="p-0030" num="0029">S<b>130</b>, displaying, in response to an annotation operation performed by the user for the zoomed window, an annotation in the zoomed window, and updating the current window by displaying the annotation in the current window.</p><p id="p-0031" num="0030">In the intelligent interaction device provided according to the exemplary embodiments, the annotation of a certain area of the current window is completed by setting the zoomed window at the first predetermined position, and the area of the zoomed window is smaller than the current window, so that the user can directly annotate the zoomed window by touch, thus completing the annotation of the current window, which is simple to operate and improves the user experience. Users can annotate in the small window without having to move to the large window, making it easier for users who are not tall enough or cannot easily walk to the large window.</p><p id="p-0032" num="0031">In an exemplary embodiment of the present disclosure, the touch screen is used to display a multimedia file, and is capable of receiving a touch operation performed by the user, or receiving an instruction of a gesture operation performed by the user for the touch screen, or receiving a trigger operation performed by the user for a key on the touch screen. The processor may be integrated in the touch screen, or may be connected to the touch screen through the network information, or may be connected through the circuits, which is not specifically limited in this exemplary embodiment.</p><p id="p-0033" num="0032">The various steps of the intelligent interaction method in the exemplary embodiments will be described below in more detail in conjunction with the accompanying drawings and examples.</p><p id="p-0034" num="0033">At step S<b>110</b>, a current window of a multimedia file in a playing state is displayed on a touch screen.</p><p id="p-0035" num="0034">In an exemplary embodiment of the present disclosure, the processor may respond to the user's operation of opening a file on the touch screen, and display the multimedia file on the touch screen. The multimedia file may include text (such as Word, PPT, and other types of documents), video, images (such as photos or screenshots, etc.), etc., without specific limitation in this exemplary embodiment. The user can open the file by touch, gesture, and other open operations for multimedia files, so that the multimedia files are displayed on the touch screen.</p><p id="p-0036" num="0035">In another exemplary embodiment of the present disclosure, displaying the current window of the multimedia file in the playing state on the touch screen can be performed by the processor displaying the multimedia file of other devices on the touch screen by receiving and responding to a screen projection command from other devices. The other devices can be screen transmitters or mobile terminals. The mobile terminals can be cell phones, tablet computers, computers, and other terminal devices, or devices with similar functionality and application scenarios as the touch screen, such as a screen projection between two electronic whiteboards or between two conference machines, which are not specifically limited in this exemplary embodiment.</p><p id="p-0037" num="0036">The touch screen and processor can be the display board and processor of an intelligent interaction device. The intelligent interaction device is the electronic whiteboard, conference machine, educational machine and other intelligent terminal equipment.</p><p id="p-0038" num="0037">In this exemplary embodiment, the other device sends a screen projection command to the intelligent interaction device, and the processor receives the command and displays the content to be projected from the other device on the touch screen.</p><p id="p-0039" num="0038">At step S<b>120</b>, a zoomed window of the current window is displayed at a first predetermined position of the current window in response to an instruction from a user for zooming the current window, the zoomed window being smaller than the current window.</p><p id="p-0040" num="0039">In an exemplary embodiment of the present disclosure, referring to <figref idref="DRAWINGS">FIG. <b>2</b></figref> and <figref idref="DRAWINGS">FIG. <b>3</b></figref>, displaying, in response to the instruction from the user <b>310</b> for zooming the current window, the zoomed window <b>220</b> of the current window <b>210</b> at the first predetermined position of the current window <b>210</b> may include displaying, in response to a swipe operation performed by the user <b>310</b> in a first direction on the current window <b>210</b>, the zoomed window <b>220</b> of the current window <b>210</b> at the first predetermined position.</p><p id="p-0041" num="0040">The touch screen is at a certain distance from the ground <b>320</b> when in use, such as 1 m, 1.2 m, 2 m, etc., and the distance is fixed if the position of the touch screen or the position of the remote control command to control the lift is fixed.</p><p id="p-0042" num="0041">The first predetermined position may be preset by the user, for example, the system default position, or set by the user according to the settings page before starting the function of this application. The first predetermined position may be selected by the user from multiple pre-stored positions, or selected by the user according to the visual virtual desktop display.</p><p id="p-0043" num="0042">In some other embodiments of the present disclosure, the first predetermined position may be determined based on the current position of the user <b>310</b>, i.e., the processor detects the current position of the user <b>310</b> and determines a position at a preset distance from the current position of the user <b>310</b> as the first predetermined position. Specifically, the zoomed window <b>220</b> may be generated at a position less than 1 meter from the user <b>310</b> based on the current position of the user <b>310</b>, so that the user <b>310</b> can more easily make annotations in the zoomed window <b>220</b>. In another exemplary embodiment, the processor collects the current position of the user by means of image capture and determines the user's orthographic projection area on the touch screen based on the user's current position, and the center of the orthographic projection area on the touch screen can be used as the first predetermined position.</p><p id="p-0044" num="0043">In the exemplary embodiments, the predetermined position may also be set directly by the user in the processor. For example, the first predetermined position is set in the area close to the lower left corner or the area close to the lower right corner of the current window <b>210</b>, or close to the lower left corner or the lower right corner of the touch screen, or close to the middle of the side frame.</p><p id="p-0045" num="0044">In the exemplary embodiments, the first direction described above may be a direction towards the first predetermined position, i.e., the direction in which the user <b>310</b> slides on the screen towards the desired zoomed window position, and then the zoomed window is generated at the first predetermined position.</p><p id="p-0046" num="0045">In the exemplary embodiments, the area of the zoomed window is smaller than the area of the current window. The size of the zoomed window may be one-third, one-fourth, one-fifth, one-eighth, one-sixteenth, etc. of the current window, or it may be customized according to the current window and the user's height, which is not specifically limited in this exemplary embodiment.</p><p id="p-0047" num="0046">In another exemplary embodiment of the present disclosure, referring to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, displaying, in response to an instruction from a user for zooming the current window <b>210</b>, a zoomed window of the current window <b>210</b> at a first predetermined position of the current window <b>210</b> may be that the processor generates the zoomed window at the first predetermined position in response to a trigger operation performed by the user on a zoom icon <b>410</b> displayed in the current window <b>210</b>.</p><p id="p-0048" num="0047">In the exemplary embodiments, the display position of the zoom icon <b>410</b> may be the first predetermined position, or may be set according to the current position of the user, or may be set directly by the processor. For example, the zoom icon <b>410</b> is set in the lower left or lower right corner of the current window <b>210</b>. When the user needs to open the zoomed window, the user can click on the zoom icon <b>410</b> to display the zoomed window at the first predetermined position, and then the zoom icon <b>410</b> is no longer displayed. After the user closes the zoomed window, the display of the zoom icon <b>410</b> is restored.</p><p id="p-0049" num="0048">In an exemplary embodiment of the present disclosure, after generating the zoomed window, the processor controls the current window <b>210</b> to stop receiving user instructions, i.e., the user's operation on the current window <b>210</b> is invalidated, and the processor receives only the user's operation on the zoomed window, which can prevent misuse of the current window <b>210</b> when operating on the zoomed window.</p><p id="p-0050" num="0049">In a further exemplary embodiment of the present disclosure, the zoomed window displays only a portion of the current window, and the displayed portion may be determined according to the user's operation. For example, the user frames a portion of the content by the cursor, and when the processor receives the instruction from the user for zooming the current window, it displays the framed portion of the content in the zoomed window.</p><p id="p-0051" num="0050">In the exemplary embodiments, the content displayed in the zoomed window can also be set directly by the processor, for example, the processor divides the current window into a preset number of sub-display regions of equal size, and displays the content of the first sub-display region by default when receiving the instruction from the user for zooming the current window. In this case, the processor can respond to the user's switching operation to display the content of other sub-display regions, for example, the user double-clicks a position of the current page with the cursor, the processor detects which sub-display region the user double-clicks and displays the content of the sub-display region where the double-click position is located in the zoomed window.</p><p id="p-0052" num="0051">It should be noted that there are various ways of switching operations by the user, and the above description is exemplary and is not specifically limited in the exemplary embodiments.</p><p id="p-0053" num="0052">At step S<b>130</b>, in response to an annotation operation performed by the user for the zoomed window, an annotation is displayed in the zoomed window and the current window is updated by displaying the annotation in the current window.</p><p id="p-0054" num="0053">In an exemplary embodiment of the present disclosure, in response to the annotation operation performed by the user for the zoomed window, displaying the annotation in the zoomed window and updating the current window by displaying the annotation in the current window may be synchronizing the content of the annotation in the current window when the user annotates. Specifically, the processor may intercept the display content of the zoomed window at intervals of a third predetermined time and replace the display content of the current window with the intercepted display content of the zoomed window at intervals of the third predetermined. In the replacement, the proportional relationship between the sizes of the zoomed window and the current window may be determined first, the display content of the zoomed window is zoomed in according to the proportional relationship to replace the display content of the current window.</p><p id="p-0055" num="0054">In the exemplary embodiments, the third predetermined time can be set by the user and may be 0.01 seconds, 0.02 seconds, etc. The third predetermined time can also be customized according to the processing accuracy and processing efficiency of the processor and can be set smaller when the processing accuracy of the processor is high to improve the display accuracy and display effect of the current window.</p><p id="p-0056" num="0055">In another exemplary embodiment of the present disclosure, referring to <figref idref="DRAWINGS">FIG. <b>5</b></figref>, in response to the annotation operation performed by the user <b>310</b> for the zoomed window, displaying the annotation in the zoomed window, and updating the current window by displaying the annotation in the current window <b>210</b> may be that the current window is updated by displaying the annotation after the user <b>310</b> completes the annotation. Specifically, the processor detects the operation information of user <b>310</b> on the zoomed window in real time, and enlarges the content of the zoomed window and displays it in the current window <b>210</b> when no operation information of the user <b>310</b> is detected at the first predetermined time. That is, the display content of the zoomed window is displayed in the current window <b>210</b> when the user <b>310</b> does not perform any operation on the zoomed window at the first predetermined time. By displaying the annotation to update the current window after the user completes the annotation, the user <b>310</b> can make changes in time when annotates incorrectly without displaying it in the current window <b>210</b>. For example, in a teaching scenario, it can prevent a large number of students from recording wrong notes due to errors in the instructor's annotation.</p><p id="p-0057" num="0056">In the exemplary embodiments, the first predetermined time can be set by the user or automatically set by the processor, e.g., 3 seconds, 4 seconds, or can be customized according to the needs, which is not specifically limited in the exemplary embodiments.</p><p id="p-0058" num="0057">In the exemplary embodiments, the processor may respond to the zoom-in operation performed by the user <b>310</b> on the preset area of the zoomed window, and display the content of the zoomed-in preset area on the zoomed window. Zooming in on the preset area enables the user <b>310</b> to annotate the preset area more clearly. For example, when it is necessary to annotate a smaller part of the zoomed window, it is not easy to complete the annotation due to the smaller part, so that the zoom-in operation helps to complete the annotation more conveniently. Since the annotation is still in progress, the current window <b>210</b> does not display the content of the zoomed window.</p><p id="p-0059" num="0058">In the exemplary embodiments, when the processor does not detect the operation of the user <b>310</b> for the zoomed-in preset area within the second predetermined time, it retains the annotation of the preset area and restores the preset area to the initial size. The second predetermined time is less than the first predetermined time described above. The second predetermined time may be set by the user or automatically set by the processor, and may be 2 seconds, 1 second, etc., or may be customized according to the needs, which is not specifically limited in the exemplary embodiments.</p><p id="p-0060" num="0059">In an exemplary embodiment of the present disclosure, after displaying the zoomed window of the current window at the first predetermined position of the current window, when the number of users is multiple, i.e., when there are multiple users using the intelligent interaction device at the same time, for example, when two or more users need to annotate the current window, the processor may, in response to a copy operation performed by the user for the zoomed window or another instruction from the user for zooming the current window, generate at least one sub-zoomed window in the current window. Specifically, the user can long press the zoomed window to copy the zoomed window, and the processor, upon receiving the user's long press operation, feeds back to the user a signal that the zoomed window has been copied and generates a sub-zoomed window misaligned with the zoomed window. The sub-zoomed window can move on the current window with the drag operation performed by the user, i.e. the sub-zoomed window is a floating window. The end of the drag operation is the display position of the sub-zoomed window.</p><p id="p-0061" num="0060">In the exemplary embodiments, the copy operation may also be an operation in which the user swipes in the opposite direction of the zoomed window to generate a sub-zoomed window. Alternatively, the sub-zoomed window may be generated by the processor receiving and responding to the user's gesture of swiping outwards with both hands on the zoomed window. There are various ways to perform the copy operation, which are not specifically limited in the exemplary embodiments.</p><p id="p-0062" num="0061">In the exemplary embodiments, the sub-zoomed window and the zoomed window display the same content at all times, which can make it easier and faster for multiple users to complete the cooperative annotation. For example, in the teaching scenario, when the lecturer asks two students to solve the two topics displayed on the current window, they can start solving them at the same time, instead of solving them one by one, which can save time.</p><p id="p-0063" num="0062">In another exemplary embodiment, the display contents of the sub-zoomed window and the zoomed window are not synchronized, and after the annotation is completed in the sub-zoomed window, the annotation contents of the sub-zoomed window are also displayed in the current window. For example, in the teaching scenario, the instructor asks two students to do the same topic, in order to prevent the students from copying each other, the contents of the sub-zoomed window and the zoomed window can be unsynchronized, and after all the annotations are completed, the contents of the annotations are displayed in the current window. In this case, in order to prevent the overlapping of the two annotations from causing confusion, the annotation position can be pre-defined during annotation.</p><p id="p-0064" num="0063">After the user finishes annotating, the processor can close the display of the sub-zoomed window or the zoomed window in response to a close operation performed by the user for the zoomed window or the sub-zoomed window.</p><p id="p-0065" num="0064">In the exemplary embodiments, the close operation may be a gesture operation of the user pressing down on the sub-zoomed window or zoomed window, or may be that there is a close icon displayed in the upper right corner of the zoomed window and sub-zoomed window, and the user triggers the close icon to close the sub-zoomed window or the zoomed window.</p><p id="p-0066" num="0065">It should be noted that there are various ways to perform the close operation, which may be customized according to the needs and preferences of the user and are not specifically limited in the exemplary embodiments.</p><p id="p-0067" num="0066">The present disclosure also provides an intelligent interaction method that can be applied to a scenario of annotating content displayed on a touch screen and performed by an intelligent interaction device. The intelligent interaction device can include the touch screen and a processor. With reference to <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the intelligent interaction method can include the following steps.</p><p id="p-0068" num="0067">S<b>610</b>, displaying, by the processor, on the touch screen a current window of a multimedia file in a playing state.</p><p id="p-0069" num="0068">S<b>620</b>, displaying, by the processor, in response to an instruction from a user for zooming the current window, a zoomed window of the current window on the touch screen with the current window maintained.</p><p id="p-0070" num="0069">S<b>630</b>, switching, by the processor, in response to a control operation performed by the user for the zoomed window, from displaying a first content in the current window to displaying a second content in the current window, wherein the second content is displayed in the zoomed window corresponding to the control operation.</p><p id="p-0071" num="0070">In the intelligent interaction method provided in the exemplary embodiments, compared to the prior art, the annotation of the current window is completed by setting the zoomed window at the first predetermined position, and the area of the zoomed window is smaller than that of the current window, so that the user can directly annotate the zoomed window by touching, thus completing the annotation of the current window, which is simple to operate and enhances the user experience.</p><p id="p-0072" num="0071">In an exemplary embodiment of the present disclosure, the touch screen is used to display a multimedia file, which can receive the touch operation performed by the user, or receive an instruction of gesture operation performed by the user for the touch screen, or receive a trigger operation performed by the user for a key on the touch screen. The processor can be integrated in the touch screen, or can be connected to the touch screen through the network information, or can be connected through the circuits, which is not specifically limited in the exemplary embodiment.</p><p id="p-0073" num="0072">The steps in the intelligent interaction method in the exemplary embodiments will be described in more detail below in conjunction with the accompanying drawings and examples.</p><p id="p-0074" num="0073">At step S<b>610</b>, a current window of a multimedia file in a playing state is displayed on a touch screen.</p><p id="p-0075" num="0074">In an exemplary embodiment of the present disclosure, the processor may respond to the user's operation of opening a file on the touch screen, and display the multimedia file on the touch screen. The multimedia file may include text, video, images, etc., without specific limitation in this exemplary embodiment. The user can open the file by touch, gesture, and other open operations for multimedia files, so that the multimedia files are displayed on the touch screen.</p><p id="p-0076" num="0075">In another exemplary embodiment of the present disclosure, displaying the current window of the multimedia file in the playing state on the touch screen can be performed by the processor displaying the multimedia file of other devices on the touch screen by receiving and responding to a screen projection command from other devices. The other devices can be cell phones, tablets, computers and other terminal devices with touch screens.</p><p id="p-0077" num="0076">At S<b>620</b>, in response to an instruction from a user for zooming the current window, a zoomed window of the current window is displayed on the touch screen with the current window maintained.</p><p id="p-0078" num="0077">In an exemplary embodiment of the present disclosure, the zoomed window is generated at the first predetermined position of the current window in response to the instruction form the user for zooming the current window, and the zoom operation and the determination of the first predetermined position have been explained in detail above and will not be repeated here.</p><p id="p-0079" num="0078">In the exemplary embodiments, the specific way to perform the zoom operation has been explained in detail above and will not be repeated here.</p><p id="p-0080" num="0079">In another exemplary embodiment of the present disclosure, the current window is displayed in full screen on the touch screen, and during the formation of the zoomed window, the display area of the current window on the touch screen may be zoomed out first, so that the zoomed window can be displayed at a second predetermined position of the touch screen. The relationship between the sizes of the zoomed window and the current window has been explained in detail above and will not be repeated here.</p><p id="p-0081" num="0080">In the exemplary embodiment, the touch screen can be set perpendicular to the ground, the processor can detect the current position of the user and determine the second predetermined position based on the current position of the user. Specifically, the processor can set the second predetermined position in a more convenient place for the user based on the current position of the user. For example, the second predetermined position is set at the edge of the touch screen which is close to user. The second predetermined position is not specifically limited in the exemplary embodiment.</p><p id="p-0082" num="0081">In another exemplary embodiment of the present disclosure, referring to <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the second predetermined position can be directly customized by the processor. Specifically, after zooming out the display area of the current window <b>210</b>, the zoomed window <b>220</b> may be formed in the lower left or lower right corner of the touch screen <b>710</b>, the diagonal lines of the zoomed window <b>220</b> and the current window <b>210</b> are located in the same line and overlap with the diagonal line of the touch screen <b>710</b>. In this case, the length and width of the current window are scaled down proportionally, and thus the display effect is not changed.</p><p id="p-0083" num="0082">In the exemplary embodiment, the second predetermined position may be preset by the user, for example, the system default position, or set by the user according to the settings page before starting the function of this application. The second predetermined position may be selected by the user from multiple pre-stored positions, or selected by the user according to the visual virtual desktop display.</p><p id="p-0084" num="0083">In a further exemplary embodiment of the present disclosure, with reference to <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the second predetermined position may also be set directly by the processor. Specifically, after zooming out the current window <b>210</b>, the current window <b>210</b> is displayed on one side of the touch screen <b>710</b>, and the display area of the current window may be compressed to the side away from the ground, or to the left, or to the right. The zoomed window <b>220</b> is formed on the area formed after the compression.</p><p id="p-0085" num="0084">In other exemplary embodiments of the present disclosure, the current window is displayed on the touch screen <b>710</b> in a non-full screen, that is, when forming the zoomed window <b>220</b>, instead of performing the above-mentioned operation of zooming out the current window <b>210</b>, the zoomed window <b>220</b> is formed in the area other than the area where the current window <b>210</b> is displayed on the touch screen <b>710</b>. The specific formation method has been described in detail above and will not be repeated here.</p><p id="p-0086" num="0085">At step S<b>630</b>, in response to a control operation performed by the user for the zoomed window, the current window is switched from displaying a first content to displaying a second content, where the second content is displayed in the zoomed window corresponding to the control operation.</p><p id="p-0087" num="0086">In an exemplary embodiment of the present disclosure, the control operation may include annotation operation, zoom-in operation, deletion operation, etc. The annotation operation is to annotate the content of the playback. The zoom-in operation can be applied when the user needs to zoom in on the displayed content for viewing, and the content displayed on the zoomed window is displayed in the current window by the user performing the zoom-in operation for the zoomed window. The zoom-in operation can be an operation where the user uses two fingers to click and stretch outward at the same time for a predetermined part of the zoomed window, or it can be customized according to the user's needs, and is not specifically limited in the exemplary embodiment.</p><p id="p-0088" num="0087">In the exemplary embodiment, the delete operation is used to delete part of the multimedia file, which can be used in conjunction with the annotation operation, i.e., when an error occurs in the annotation, or when one of the annotations is no longer needed, the delete operation can be used to delete the unwanted part of the annotation, or delete part of the multimedia file, and the delete operation can be done by the user clicking on the part to be deleted and jiggling, or can be customized according to the user's needs, and is not specifically limited in the exemplary embodiment.</p><p id="p-0089" num="0088">In the exemplary embodiment, the processor can display the content of the zoomed window on the current window after completing all operations on the zoomed window, or it can synchronize the operations on the zoomed window to the current window directly. The specific execution process of the two display methods has been explained in detail at step S<b>130</b> above and will not be repeated here.</p><p id="p-0090" num="0089">In the exemplary embodiment, the processor of the intelligent interaction device may also perform a restore operation for the current window, store the changed contents of the current window, i.e., the changed display contents after the completion of the annotation operation and other operations, and restore the display contents of the current window to the initially displayed contents, i.e., the initial display contents when the annotation operation and other operations were not performed.</p><p id="p-0091" num="0090">In the exemplary embodiment, after hiding the contents corresponding to the control operation, when the user needs to call out the contents corresponding to the control operation again, the changed display content matched the initial display content can be obtained from the storage library using the initial display content, and the initial display content can be replaced with the changed display content in response to the user's recall operation, and then the display of the contents corresponding to the control operation can be completed.</p><p id="p-0092" num="0091">In the exemplary embodiment, the restore operation can be the operation of the user to swipe up for the current window, and the recall operation can be the operation of the user to swipe down for the current window. The restore operation and recall operation can also be customized according to the needs of the user, which are not specifically limited in the exemplary embodiment.</p><p id="p-0093" num="0092">The intelligent interaction device <b>900</b> according to embodiments of the present disclosure is described below with reference to <figref idref="DRAWINGS">FIG. <b>9</b></figref>. The intelligent interaction device <b>900</b> shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref> is merely an example and shall not constitute a limitation on the functionality and scope of use of the embodiments of the present disclosure.</p><p id="p-0094" num="0093">As shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref>, the components of the intelligent interaction device <b>900</b> may include, but are not limited to, at least one processing unit <b>910</b>, at least one storage unit <b>920</b>, a bus <b>930</b> connecting the different system components (including the storage unit <b>920</b> and the processing unit <b>910</b>), and a display unit <b>940</b>.</p><p id="p-0095" num="0094">The storage unit <b>920</b> stores program code, the program code may be executed by the processing unit <b>910</b> causing the processing unit <b>910</b> to perform the steps according to various exemplary embodiments of the present disclosure as described above. For example, the processing unit <b>910</b> may perform the steps as shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref> or <figref idref="DRAWINGS">FIG. <b>6</b></figref>.</p><p id="p-0096" num="0095">The memory unit <b>920</b> may include readable media in the form of volatile memory units, such as random access memory units (RAM) <b>921</b> and/or cache memory units <b>922</b>, and may further include read-only memory units (ROM) <b>923</b>.</p><p id="p-0097" num="0096">The storage unit <b>920</b> may also include a program/utility <b>924</b> having a set of (at least one) program modules <b>925</b>, such program modules <b>925</b> including, but not limited to an operating system, one or more applications, other program modules, and program data, each of these examples or some combination may be applied in a network environment.</p><p id="p-0098" num="0097">The bus <b>930</b> may be a local bus that represents one or more of several types of bus structures, including a memory cell bus or memory cell controller, a peripheral bus, a graphics acceleration port, a processing unit, or any bus structure using any of the multiple bus structures.</p><p id="p-0099" num="0098">The intelligent interaction device <b>900</b> may also communicate with one or more external devices <b>970</b> (e.g., keyboards, pointing devices, Bluetooth devices, etc.), with one or more devices that enable the user to interact with the intelligent interaction device <b>900</b>, and/or with any device that enables the intelligent interaction device <b>900</b> to communicate with one or more other computing devices (e.g., routers, modems, etc.). This communication may be via input/output (I/O) interface <b>950</b>. And, the intelligent interaction device <b>900</b> may also communicate with one or more networks (e.g., local area network (LAN), wide area network (WAN), and/or public networks, such as the Internet) through the network adapter <b>960</b>. As shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref>, network adapter <b>960</b> communicates with other modules of intelligent interaction device <b>900</b> via bus <b>930</b>. It should be appreciated that although not shown in the figures, other hardware and/or software modules may be used in conjunction with the intelligent interaction device <b>900</b>, including, but not limited to, microcode, device drives, redundant processing units, external disk drive arrays, RAID systems, tape drives, and data backup storage systems, among others.</p><p id="p-0100" num="0099">According to the above description of embodiments, it is readily understood by those skilled in the art that the exemplary embodiments described herein can be implemented by means of software or by means of software combined with the necessary hardware. Thus, a technical solution according to the embodiments of the present disclosure may be embodied in the form of a software product that may be stored in a non-volatile storage medium (which may be a CD-ROM, USB flash drive, removable hard drive, etc.) or on a network, including a number of instructions to cause a computing device (which may be a personal computer, server, terminal device, or network device, etc.) to perform the method of the amendments of the present disclosure.</p><p id="p-0101" num="0100">In exemplary embodiments of the present disclosure, there is also provided a non-transitory computer readable storage medium having stored thereon a program product capable of implementing the methods described above in this specification. In some possible embodiments, aspects of the present disclosure may also be implemented in the form of a program product including program code that, when the program product is run on a terminal device, is used to cause the terminal device to perform the steps described above according to various exemplary embodiments of the present disclosure.</p><p id="p-0102" num="0101">Referring to <figref idref="DRAWINGS">FIG. <b>10</b></figref>, a program product <b>1000</b> for implementing the above methods according to embodiments of the present disclosure is illustrated, which may employ a portable compact disk read-only memory (CD-ROM) and include program code and may run on a terminal device, such as a personal computer. However, the program products of the present disclosure are not limited thereto, and for the purposes of this document, a readable storage medium may be any tangible medium that contains or stores a program that may be used by or in combination with an instruction execution system, apparatus, or device.</p><p id="p-0103" num="0102">The program product may employ any combination of one or more readable media. The readable medium may be a readable signal medium or a readable storage medium. The readable storage medium may be, for example, but not limited to, an electrical, magnetic, optical, electromagnetic, infrared, or semiconductor system, apparatus, or device, or any combination of the above. Specific examples of readable storage media include (non-exhaustive list): electrical connections with one or more wires, portable disks, hard disks, random access memory (RAM), read-only memory (ROM), erasable programmable read-only memory (EPROM or flash memory), optical fiber, portable compact disk read-only memory (CD-ROM), optical storage devices, magnetic memory devices, or any suitable combination of the above.</p><p id="p-0104" num="0103">The computer readable signal medium may include a data signal propagated in the baseband or as part of a carrier wave that carries readable program code. This propagated data signal can take a variety of forms, including, but not limited to, electromagnetic signals, optical signals or any suitable combination of the above. The readable signal medium may also be any readable medium other than a readable storage medium that sends, propagates, or transmits a program for use by or in conjunction with an instruction execution system, apparatus, or device.</p><p id="p-0105" num="0104">The program code contained on the readable media may be transmitted by any suitable medium, including, but not limited to, wireless, wired, fiber optic, RF, etc., or any suitable combination of the above.</p><p id="p-0106" num="0105">Program code for performing the operations of the present disclosure may be written in any combination of one or more programming languages, the programming languages including object-oriented programming languages, such as Java, C++, etc., and also including conventional procedural programming languages, such as &#x201c;C&#x201d; language or similar programming languages. The program code may be executed entirely on the user computing device, partially on the user device, as a stand-alone package, partially on the user computing device and partially on the remote computing device, or entirely on the remote computing device or server. In the case involving a remote computing device, the remote computing device may be connected to the user computing device via any kind of network, including a local area network (LAN) or a wide area network (WAN), or, alternatively, may be connected to an external computing device (e.g., using an Internet service provider to connect via the Internet).</p><p id="p-0107" num="0106">Further, the appended drawings are merely schematic illustrations of the processing included in the methods according to exemplary embodiments of the present disclosure, and are not intended to be limiting. It should be understood that the processing shown in the accompanying drawings does not indicate or limit the temporal order of such processing. It also should be understood that these processes may be performed, for example, in multiple modules, either synchronously or asynchronously.</p><p id="p-0108" num="0107">Other embodiments of the present disclosure will readily come to the mind of one skilled in the art upon consideration of the specification and practice of the invention disclosed herein. This disclosure is intended to cover any variation, use, or adaptation of the present disclosure that follows the general principles of the present disclosure and includes commonly known or customary technical means in the art that are not disclosed herein. The specification and embodiments are to be considered exemplary only, and the true scope and spirit of the present disclosure is indicated by the claims.</p><p id="p-0109" num="0108">It should be understood that the present disclosure is not limited to the precise construction already described above and illustrated in the accompanying drawings, and that various modifications and changes may be made without departing from its scope. The scope of the present disclosure is limited only by the appended claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An intelligent interaction method for annotation scenarios, comprising:<claim-text>displaying, by a processor, on a touch screen a current window of a multimedia file in a playing state;</claim-text><claim-text>displaying, by the processor, in response to an instruction from a user for zooming the current window, a zoomed window of the current window at a first predetermined position of the current window, the zoomed window being smaller than the current window; and</claim-text><claim-text>displaying, by the processor, in response to an annotation operation performed by the user for the zoomed window, an annotation in the zoomed window, and updating the current window by displaying the annotation in the current window.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein displaying, in response to an instruction from a user for zooming the current window, a zoomed window of the current window at a first predetermined position of the current window, comprises:<claim-text>displaying, in response to a touch operation performed by the user with a set gesture on the touch screen, the zoomed window of the current window at the first predetermined position of the current window; or</claim-text><claim-text>displaying, in response to a swipe operation performed by the user in a first direction, the zoomed window of the current window at the first predetermined position of the current window; or</claim-text><claim-text>displaying, in response to a trigger operation performed by the user on a zoom icon, the zoomed window of the current window at the first predetermined position of the current window, wherein the zoom icon is provided in the current window.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein displaying a zoomed window of the current window at a first predetermined position of the current window, comprises:<claim-text>detecting a current position of the user relative to the touch screen, and determining the first predetermined position based on the current position; and</claim-text><claim-text>displaying the zoomed window of the current window at the first predetermined position of the current window, wherein a center of the zoomed window is at the first predetermined position.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein determining the first predetermined position based on the current position, comprises:<claim-text>obtaining an orthographic projection area of the user on the touch screen based on the current position; and</claim-text><claim-text>determining a center of the orthographic projection area as the first predetermined position.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein after displaying a zoomed window of the current window at a first predetermined position of the current window, the method further comprises:<claim-text>disabling a touch operation performed by the user in the current window.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein displaying the multimedia file on the touch screen, comprises:<claim-text>displaying, in response to an operation performed by the user for opening a file on the touch screen, the multimedia file on the touch screen; or</claim-text><claim-text>displaying, in response a screen projection command from another device, the multimedia file on the touch screen.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein after displaying a zoomed window of the current window at a first predetermined position of the current window, the method further comprises:<claim-text>generating, in response to a copy operation performed by the user for the zoomed window or another instruction from the user for zooming the current window, a sub-zoomed window,</claim-text><claim-text>wherein content of the zoomed window is displayed simultaneously with content of the sub-zoomed window, the sub-zoomed window is configured to move on the current window in response to a drag operation performed by the user, and the sub-zoomed window and the zoomed window are in different positions.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein types of the multimedia file comprise text, video or image.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. An intelligent interaction method, comprising:<claim-text>displaying, by a processor, on a touch screen a current window of a multimedia file in a playing state;</claim-text><claim-text>displaying, by the processor, in response to an instruction from a user for zooming the current window, a zoomed window of the current window on the touch screen with the current window maintained; and</claim-text><claim-text>switching, by the processor, in response to a control operation performed by the user for the zoomed window, from displaying a first content in the current window to displaying a second content in the current window, wherein the second content is displayed in the zoomed window corresponding to the control operation.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the current window is displayed full screen on the touch screen, and displaying a zoomed window of the current window on the touch screen comprises:<claim-text>zooming out display area of the current window on the touch screen; and</claim-text><claim-text>forming the zoomed window of the current window at a second predetermined position other than the display area of the current window on the touch screen.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the current window is displayed on the touch screen in a non-full screen, and displaying a zoomed window of the current window on the touch screen comprises:<claim-text>forming the zoomed window of the current window at a second predetermined position other than an area where the current window is displayed on the touch screen.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein after switching, in response to a control operation performed by the user for the zoomed window, from displaying a first content in the current window to displaying a second content in the current window, the method further comprises:<claim-text>receiving and responding to a restore operation performed by the user for the current window,</claim-text><claim-text>storing the second content of the current window, and</claim-text><claim-text>displaying the first content in the current window.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein when displaying the first content in the current window, the method further comprises:<claim-text>receiving and responding to a recall operation performed by the user for the current window, obtaining the second content matched the first content from a storage library using the first content; and</claim-text><claim-text>replacing the first content with the second content.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. An intelligent interaction device for annotation scenarios, comprising:<claim-text>a touch screen,</claim-text><claim-text>one or more processors coupled to the touch screen, and</claim-text><claim-text>a memory for storing one or more programs that, when executed by the one or more processors, cause the one or more processors to implement an intelligent interaction method, wherein the method comprises:</claim-text><claim-text>displaying, on a touch screen, a current window of a multimedia file in a playing state;</claim-text><claim-text>displaying, in response to an instruction from a user for zooming the current window, a zoomed window of the current window at a first predetermined position of the current window, the zoomed window being smaller than the current window; and</claim-text><claim-text>displaying, in response to an annotation operation performed by the user for the zoomed window, an annotation in the zoomed window, and updating the current window by displaying the annotation in the current window.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. A non-transitory computer readable storage medium, storing a computer program, wherein the computer program, when executed by one or more processors, implements the intelligent interaction method of <claim-ref idref="CLM-00001">claim 1</claim-ref>.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The intelligent interaction device of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the one or more processors are configured to display a zoomed window of the current window at a first predetermined position of the current window by:<claim-text>detecting a current position of the user relative to the touch screen, and determining the first predetermined position based on the current position; and</claim-text><claim-text>displaying the zoomed window of the current window at the first predetermined position of the current window, wherein a center of the zoomed window is at the first predetermined position.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The intelligent interaction device of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the method further comprises:<claim-text>generating, in response to a copy operation performed by the user for the zoomed window or another instruction from the user for zooming the current window, a sub-zoomed window,</claim-text><claim-text>wherein content of the zoomed window is displayed simultaneously with content of the sub-zoomed window, the sub-zoomed window is configured to move on the current window in response to a drag operation performed by the user, and the sub-zoomed window and the zoomed window are in different positions.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. An intelligent interaction device, comprising:<claim-text>a touch screen,</claim-text><claim-text>one or more processors coupled to the touch screen, and</claim-text><claim-text>a memory for storing one or more programs that, when executed by the one or more processors, cause the one or more processors to implement the method of <claim-ref idref="CLM-00009">claim 9</claim-ref>.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. An intelligent interaction device, comprising:<claim-text>a touch screen,</claim-text><claim-text>one or more processors coupled to the touch screen, and</claim-text><claim-text>a memory for storing one or more programs that, when executed by the one or more processors, cause the one or more processors to implement the method of <claim-ref idref="CLM-00012">claim 12</claim-ref>.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. A non-transitory computer readable storage medium, storing a computer program, wherein the computer program, when executed by one or more processors, implements the intelligent interaction method of <claim-ref idref="CLM-00009">claim 9</claim-ref>.</claim-text></claim></claims></us-patent-application>