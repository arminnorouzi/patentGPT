<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230000304A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230000304</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17941829</doc-number><date>20220909</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>KR</country><doc-number>10-2020-0047672</doc-number><date>20200420</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>47</class><subclass>L</subclass><main-group>11</main-group><subgroup>40</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>05</class><subclass>D</subclass><main-group>1</main-group><subgroup>02</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>47</class><subclass>L</subclass><main-group>11</main-group><subgroup>4011</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>05</class><subclass>D</subclass><main-group>1</main-group><subgroup>0274</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>05</class><subclass>D</subclass><main-group>1</main-group><subgroup>0238</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>05</class><subclass>D</subclass><main-group>1</main-group><subgroup>0246</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>47</class><subclass>L</subclass><main-group>2201</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>05</class><subclass>D</subclass><main-group>2201</main-group><subgroup>0203</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">ROBOT DEVICE AND CONTROL METHOD THEREFOR</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>PCT/KR2021/003212</doc-number><date>20210316</date></document-id><parent-status>PENDING</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>17941829</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>SAMSUNG ELECTRONICS CO., LTD.</orgname><address><city>Suwon-si</city><country>KR</country></address></addressbook><residence><country>KR</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>MOON</last-name><first-name>Kyungjin</first-name><address><city>Suwon-si</city><country>KR</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>MOON</last-name><first-name>Boseok</first-name><address><city>Suwon-si</city><country>KR</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>BAE</last-name><first-name>Sanghyun</first-name><address><city>Suwon-si</city><country>KR</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>SAMSUNG ELECTRONICS CO., LTD.</orgname><role>03</role><address><city>Suwon-si</city><country>KR</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A robot device includes: a sensor configured to generate sensing data related to an action of the robot device; a communication interface configured to communicate with a server; a memory storing instructions; and a processor configured to execute the instructions to: based on the action of the robot device changing, store action data in the memory, the action data including instruction data corresponding to the action, the sensing data related to the action, and map data related to the action, transmit, to the server via the communication interface, the action data stored in the memory, receive, from the server via the communication interface, threshold data corresponding to the action, and based on identifying that the sensing data is outside of a threshold range based on the threshold data received from the server, generate an event.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="170.69mm" wi="142.24mm" file="US20230000304A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="104.39mm" wi="118.53mm" file="US20230000304A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="95.33mm" wi="125.05mm" file="US20230000304A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="137.24mm" wi="120.65mm" file="US20230000304A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="184.32mm" wi="147.15mm" file="US20230000304A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="184.32mm" wi="147.15mm" file="US20230000304A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="223.60mm" wi="147.74mm" file="US20230000304A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="157.82mm" wi="107.44mm" file="US20230000304A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="148.25mm" wi="106.43mm" file="US20230000304A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="78.74mm" wi="76.03mm" file="US20230000304A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="160.44mm" wi="141.39mm" file="US20230000304A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="199.22mm" wi="144.27mm" file="US20230000304A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application is a continuation application of International Application No. PCT/KR2021/003212 filed on Mar. 16, 2021, which is based on and claims priority to Korean Patent Application 10-2020-0047672, filed on Apr. 20, 2020, in the Korean Intellectual Property Office, the disclosures of which are incorporated by reference herein in their entireties.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND</heading><heading id="h-0003" level="1">1. Field</heading><p id="p-0003" num="0002">The disclosure relates to a robot device and a control method therefor, and more particularly, to a robot device that moves, and a control method therefor.</p><heading id="h-0004" level="1">2. Description of Related Art</heading><p id="p-0004" num="0003">Electronic devices of various types are being developed and distributed, and in particular, along with active research going on in the field of robots, the distribution rate of household robots of various types other than industrial robots is on the increase.</p><p id="p-0005" num="0004">When developing a robot device, in a logging method based on texts, there may be a problem in that the amount of logs may increase exponentially according to the operation time, as there may be a great variety of components included in the robot device, and each component may create and add entries to their respective logs.</p><p id="p-0006" num="0005">As a result, replay of the operation history of a related robot device may not be straightforward using a log file generated according to a logging method based on texts. Alternatively or additionally, the log file may be inappropriate for searching for logging information regarding a problem that occurred between operations.</p><p id="p-0007" num="0006">Furthermore, a log file generated according to a related logging method may be unsuitable for a developing environment for a related robot device, such as, but not limited to, requiring a separate compiling process to be performed.</p><p id="p-0008" num="0007">Accordingly, there exists a need for further improvements in logging methods. In particular, a logging method that allows for replay of the operation history in a straightforward and fast manner without requiring a separate compiling process. Improvements are presented herein. These improvements may also be applicable to other technologies that employ these logging methods.</p><heading id="h-0005" level="1">SUMMARY</heading><p id="p-0009" num="0008">Provided are a robot device that generates and stores data according to the operation of the robot device, and replays the operation based on the data, and a control method therefor.</p><p id="p-0010" num="0009">According to an aspect of the disclosure, a robot device includes: a sensor configured to generate sensing data related to an action of the robot device; a communication interface configured to communicate with a server; a memory storing instructions; and a processor configured to execute the instructions to: based on the action of the robot device changing, store action data in the memory, the action data including instruction data corresponding to the action, the sensing data related to the action, and map data related to the action, transmit, to the server via the communication interface, the action data stored in the memory, receive, from the server via the communication interface, threshold data corresponding to the action, and based on identifying that the sensing data is outside of a threshold range based on the threshold data received from the server, generate an event.</p><p id="p-0011" num="0010">The instruction data may be in a robot programming language format, and may be configured to control at least one of a driving distance, a driving direction, or a driving speed of the robot device, and the processor may be further configured to execute the instructions to control the robot device, according to the instruction data, to generate the action.</p><p id="p-0012" num="0011">The sensing data related to the action includes operation data generated in hardware elements of the robot device during performance of the action by the robot device, the map data may include information on an object located in a specific place that the robot device is located, and the information on the object may include at least one of location information of the object, size information of the object, shape information of the object, or characteristic information of the object.</p><p id="p-0013" num="0012">The threshold data may include threshold range values of sensing data for each action of the robot device, the threshold range values having been acquired according to an analysis of the action data transmitted to the server, and the processor may be further configured to execute the instructions to identify whether the sensing data related to the action is within the threshold range.</p><p id="p-0014" num="0013">The threshold data may indicate that the sensing data related to the action is outside of the threshold range, and the processor may be further configured to execute the instructions to generate the event based on the threshold data.</p><p id="p-0015" num="0014">The event may include at least one of: a first feedback notifying that the sensing data related to the action is outside of the threshold range, an instruction configured to control hardware elements of the robot device such that the sensing data related to the action is within the threshold range, and a second feedback identifying a hardware element from among the hardware elements of the robot device wherein an error occurred, based on the sensing data related to the action.</p><p id="p-0016" num="0015">The processor may be further configured to execute the instructions to, based on a test mode being executed, replay the action based on the instruction data stored in the memory and the sensing data stored in the memory corresponding to the instruction data, or replay the action based on the instruction data stored in the memory and other sensing data received from the server corresponding to the instruction data.</p><p id="p-0017" num="0016">The processor may be further configured to execute the instructions to: convert the instruction data and the sensing data into converted data in a robot programming language format, store the converted data in the memory, and transmit the converted data to the server via the communication interface.</p><p id="p-0018" num="0017">According to an aspect of the disclosure, a robot device system includes: a robot device configured to, based on an action of the robot device changing, store action data including instruction data corresponding to the action, sensing data related to the action and generated by a sensor of the robot device, and map data related to the action; and a server configured to identify a threshold range of the sensing data related to the action based on the action data received from the robot device, and transmit, to the robot device, threshold data related to the identified threshold range, wherein the robot device is further configured to generate an event, based on identifying that the sensing data is outside of the threshold range based on the threshold data received from the server.</p><p id="p-0019" num="0018">The server may be further configured to: receive, from the robot device, instruction data and sensing data corresponding to each of a plurality of actions, and identify a threshold range of each of the plurality of actions, based on the received instruction data and sensing data.</p><p id="p-0020" num="0019">The threshold data may include information regarding the threshold range of the sensing data corresponding to each of a plurality of actions of the robot device.</p><p id="p-0021" num="0020">The threshold data may include information indicating that the sensing data related to the action is outside of the threshold range.</p><p id="p-0022" num="0021">According to an aspect of the disclosure, a control method for a robot device includes: storing, in a memory of the robot device, action data corresponding to an action of the robot device, based on the action of the robot device changing, the action data including instruction data corresponding to the action, sensing data related to the action, and map data related to the action; transmitting the stored data to a server; receiving, from the server, threshold data corresponding to the action data; and based on identifying that the sensing data is outside of a threshold range based on the threshold data received from the server, generating an event.</p><p id="p-0023" num="0022">The instruction data may be in a robot programming language format, and may be configured to control at least one of a driving distance, a driving direction, or a driving speed of the robot device, and the control method may further include controlling the robot device to generate the action based on the instruction data.</p><p id="p-0024" num="0023">The sensing data may include operation data generated in hardware elements of the robot device during performing of the action by the robot device, the map data may include information on an object located in a specific place that the robot device is located, and the information on the object may include at least one of location information of the object, size information of the object, shape information of the object, or characteristic information of the object.</p><p id="p-0025" num="0024">The threshold data received from the server may include threshold range values of sensing data for each action of the robot device, the threshold range values having been acquired according to an analysis of the action data transmitted to the server, and the generating of the event may include identifying whether the sensing data related to the action is within the threshold range.</p><p id="p-0026" num="0025">The threshold data may indicate that the sensing data related to the action is outside of the threshold range, and the generating of the event may include generating the event based on the threshold data.</p><p id="p-0027" num="0026">The event may include at least one of: a first feedback notifying that the sensing data related to the action is outside of the threshold range, an instruction configured to control hardware elements of the robot device such that the sensing data related to the action is within the threshold range, and a second feedback identifying a hardware element from among the hardware elements of the robot device wherein an error occurred, based on the sensing data related to the action.</p><p id="p-0028" num="0027">The control method may further include, based on a test mode being executed, replaying the action based on the instruction data stored in the memory and the sensing data stored in the memory corresponding to the instruction data, or replaying the action based on the instruction data stored in the memory and other sensing data received from the server corresponding to the instruction data.</p><p id="p-0029" num="0028">The storing of the action data in the memory may include: converting the instruction data and the sensing data into converted data in a robot programming language format; storing the converted data in the memory; and transmitting the converted data to the server.</p><p id="p-0030" num="0029">According to the various embodiments of the disclosure, a log file may be stored when an action of the robot device occurs.</p><p id="p-0031" num="0030">Also, by using a log file of a low capacity, an operation part of the robot device that drove differently from what was predicted may be replayed easily.</p><p id="p-0032" num="0031">In addition, by using a robot programming language, the driving history of the robot device may be replayed without a separate interpreter.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0006" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0033" num="0032">The above and other aspects, features, and advantages of certain embodiments of the present disclosure will be more apparent from the following description taken in conjunction with the accompanying drawings, in which:</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram illustrating a configuration of a robot device according to an embodiment of the disclosure;</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a detailed block diagram of a robot device according to an embodiment of the disclosure;</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram for illustrating data according to an embodiment of the disclosure;</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram for illustrating instruction data and sensing data according to an embodiment of the disclosure;</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram for illustrating a threshold range according to an embodiment of the disclosure;</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a diagram for illustrating map data according to an embodiment of the disclosure;</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a diagram for illustrating a case of replaying driving of a robot device according to an embodiment of the disclosure;</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a flow chart for illustrating a control method for a robot device according to an embodiment of the disclosure;</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a block diagram illustrating a configuration of a robot device system according to an embodiment of the disclosure;</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a diagram for illustrating a robot device system according to an embodiment of the disclosure; and</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a sequence diagram for illustrating receipt and transmission of data between a server and a robot device according to an embodiment of the disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0007" level="1">DETAILED DESCRIPTION</heading><p id="p-0045" num="0044">Hereinafter, embodiments of the disclosure will be described in detail with reference to the accompanying drawings. As terms used in the embodiments of the disclosure, general terms that are currently used widely were selected as far as possible, in consideration of the functions described in the disclosure. However, the terms may vary depending on the intention of those skilled in the art who work in the pertinent field, previous court decisions, or emergence of new technologies, etc. In particular cases, there may be terms that were designated by the Applicant on his own, and in such cases, the meaning of the terms are be described in detail in the relevant descriptions in the disclosure. Accordingly, the terms used in the disclosure should be defined based on the meaning of the terms and the overall content of the disclosure, but not just based on the names of the terms.</p><p id="p-0046" num="0045">Herein, expressions such as &#x201c;have,&#x201d; &#x201c;may have,&#x201d; &#x201c;include,&#x201d; and &#x201c;may include&#x201d; denote the existence of such characteristics (e.g., elements such as numbers, functions, operations, and components), and do not exclude the existence of additional characteristics.</p><p id="p-0047" num="0046">In addition, the expression &#x201c;at least one of A and/or B&#x201d; should be interpreted to mean any one of &#x201c;A&#x201d; or &#x201c;B&#x201d; or &#x201c;A and B.&#x201d;</p><p id="p-0048" num="0047">Further, the expressions &#x201c;first,&#x201d; &#x201c;second&#x201d; and the like as used in the disclosure may be used to describe various elements regardless of any order and/or degree of importance. Such expressions may be used only to distinguish one element from another element, and are not intended to limit the elements.</p><p id="p-0049" num="0048">A description in the disclosure that one element (e.g., a first element) is &#x201c;(operatively or communicatively) coupled with/to&#x201d; or &#x201c;connected to&#x201d; another element (e.g., a second element) should be interpreted to include both the case where the one element is directly coupled to the another element, and the case where the one element is coupled to the another element through still another element (e.g., a third element).</p><p id="p-0050" num="0049">In addition, singular expressions include plural expressions, unless defined differently in the context. Further, in the disclosure, terms such as &#x201c;include&#x201d; and &#x201c;consist of&#x201d; should be construed as designating that there are such characteristics, numbers, steps, operations, elements, components, or a combination thereof described in the specification, but not as excluding in advance the existence or possibility of adding one or more of other characteristics, numbers, steps, operations, elements, components, or a combination thereof.</p><p id="p-0051" num="0050">In the disclosure, &#x201c;a module&#x201d; or &#x201c;a part&#x201d; performs at least one function or operation, and may be implemented as hardware or software, or as a combination of hardware and software. Further, a plurality of &#x201c;modules&#x201d; or &#x201c;parts&#x201d; may be integrated into at least one module and implemented by at least one processor, except &#x201c;modules&#x201d; or &#x201c;parts&#x201d; which need to be implemented by specific hardware.</p><p id="p-0052" num="0051">In addition, herein, the term &#x201c;user&#x201d; may refer to a person who uses an electronic device or a device using an electronic device (e.g., an artificial intelligence electronic device).</p><p id="p-0053" num="0052">Hereinafter, various embodiments of the disclosure are described in more detail with reference to the accompanying drawings.</p><p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram illustrating a configuration of a robot device according to an embodiment of the disclosure.</p><p id="p-0055" num="0054">As shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, a robot device <b>100</b> according to an embodiment of the disclosure may refer to machines in various forms that have the capability of performing a work function by themselves. As an example, a robot may be a smart device that detects the surrounding environment in real time, based on a sensor, a camera, etc., and collects information and operates autonomously, performing operations other than simple repetitive functions. However, this is merely an example, and the disclosure is not limited thereto.</p><p id="p-0056" num="0055">The various embodiments of the disclosure may be implemented through an electronic device. Here, the electronic device may be implemented as at least one device in one or more of various forms, such as, but not limited to, a user terminal device, a display device, a set-top box, a tablet personal computer (PC), a smart phone, an e-book reader, a desktop PC, a laptop PC, a workstation, a server, a personal digital assistant (PDA), a portable multimedia player (PMP), a Moving Picture Experts Group (MPEG) Audio Layer-3 (MP3) player, a kiosk, etc. However, these are merely examples, and the electronic device may also be implemented as electronic devices in other various types such as a wearable device falling under at least one form among an accessory-type device (e.g., a watch, a ring, a bracelet, an ankle bracelet, a necklace, glasses, a contact lens, and/or a head-mounted-device (HMD)), a device integrated with fabrics or clothing (e.g., electronic clothing), a robot including an operation part, a projector, a server, etc. Hereinafter, for the convenience of explanation, description will be made on the assumption of a case wherein the various embodiments of the disclosure are implemented by the robot device <b>100</b>.</p><p id="p-0057" num="0056">The robot device <b>100</b> may include an operation part including an actuator or a motor (e.g., operation part <b>150</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>). Here, the operation part <b>150</b> may include a wheel, a brake, etc. In some embodiments, the robot device <b>100</b> may be implemented as a mobile robot that may move in a specific place (e.g., a building, a home, a business such as, but not limited to, a restaurant, a hotel, a store, a hospital, a warehouse) by itself by using the operation part <b>150</b>. For example, if the robot device <b>100</b> determines that there is an obstacle in the front, the robot device <b>100</b> may rotate its main body to the right side or the left side and then make the main body go forward, or make the main body go backward, by using the operation part <b>150</b>.</p><p id="p-0058" num="0057">Also, the operation part <b>150</b> may move a component (e.g., a robot joint) of the robot device <b>100</b> for replacing the functions of a human arm or hand, other than the main body. The robot device <b>100</b>, according to an embodiment of the disclosure, may generate or change an action of the robot device <b>100</b> by using the operation part <b>150</b>. Here, the action may include the moving and the movement of the robot device <b>100</b>. For example, the robot device <b>100</b> may drive or rotate the robot device <b>100</b> by controlling the operation part <b>150</b>. As another example, the robot device <b>100</b> may perform operations such as moving an external object, or gripping an external object, by controlling the operation part <b>150</b>. That is, the action may include various operations and functions that the robot device <b>100</b> may perform.</p><p id="p-0059" num="0058">In some embodiments, the robot device <b>100</b> may be divided into an industrial use, a medical use, a household use, a military use, an exploration use, etc., according to fields or performable functions. According to an embodiment, industrial robots may be subdivided into a robot used in a product manufacturing process in a factory, a robot performing serving of guests (e.g., receiving orders, serving, etc., in a shop, a restaurant, etc.), and the like. For example, the robot device <b>100</b>, according to an embodiment of the disclosure, may be implemented as a serving robot that may carry service goods to a location desired by a user, and/or a specific location in various places such as a restaurant, a hotel, a mart, a hospital, a clothing shop, etc. However, these are merely examples, and the robot device <b>100</b> may be classified in various ways according to fields of application, functions, and purposes of uses, and are not limited to the aforementioned examples.</p><p id="p-0060" num="0059">Continuing to refer to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the robot device <b>100</b>, according to an embodiment of the disclosure, includes a sensor <b>110</b>, a communication interface <b>120</b>, a memory <b>130</b>, and a processor <b>140</b>.</p><p id="p-0061" num="0060">In some embodiments, the sensor <b>110</b> may be implemented as a detection sensor. The sensor <b>110</b> may sense actual operations of the operation part <b>150</b> and the components according to an action of the robot device <b>100</b>, and generate sensing data. The sensing data may include operation data generated by hardware elements provided in the robot device <b>100</b> as an action of the robot device <b>100</b> occurs. Here, the sensing data may also be referred to as a log file or operation data, but is generally referred to as sensing data, for the convenience of explanation.</p><p id="p-0062" num="0061">For example, when the robot device <b>100</b> performs an action of a left side rotation, the sensor <b>110</b> may sense actual operations of the operation part <b>150</b> according to the action of the left side rotation (e.g., the rotation angle, the rotation speed, the moving directions of each of the left side wheel and the right side wheel, etc.), and generate sensing data. As another example, if the robot device <b>100</b> performs an action of forward driving, the sensor <b>110</b> may sense actual operations of the operation part <b>150</b> according to the action of forward driving (e.g., the driving direction, the driving speed, etc.), and generate sensing data. However, these are merely examples, and the sensor <b>110</b> may sense actual operations of each of the components included in the robot device <b>100</b> according to an action of the robot device <b>100</b>, and generate sensing data.</p><p id="p-0063" num="0062">In some embodiments, the sensor <b>110</b> may be a component for acquiring one or more images from the surroundings. For example, the sensor <b>110</b> may be a red-green-blue (RGB) camera, a three-dimensional (3D) camera, etc. The processor <b>140</b>, according to an embodiment, may acquire map data based on the sensing data of the sensor <b>110</b>. That is, the map data may include information on an object in a specific place (e.g., geographical location) based on where the robot device <b>100</b> is located.</p><p id="p-0064" num="0063">In some embodiments, the sensor <b>110</b> may be implemented as an ultrasonic sensor, an infrared sensor, etc. In a case that the detection sensor is implemented as an ultrasonic sensor, the processor <b>140</b> may control the ultrasonic sensor to radiate an ultrasonic pulse. In such a case, when a reflective wave is received from the ultrasonic pulse being reflected on an object, the processor <b>140</b> may measure the elapsed time between the radiation of the ultrasonic pulse and the reception of the reflective wave, and thereby measure the distance between the object and the processor <b>140</b>. Alternatively or additionally, the ultrasonic sensor may include an ultrasonic proximity sensor, and may be implemented in various ways. An infrared sensor may refer to an element that detects infrared light information included by an object. The processor <b>140</b> may identify an object based on the sensing data (e.g., infrared light information, distance information) of the sensor <b>110</b>.</p><p id="p-0065" num="0064">The disclosure is not limited thereto, and the sensor <b>110</b> may be implemented as sensors in various forms. For example, the sensor <b>110</b> may include an radio-frequency (RF) sensor, a geomagnetic sensor, a position sensitive device (PSD) sensor, a sensor that detects a cliff within a driving path, a light detection and ranging (LIDAR) sensor, etc.</p><p id="p-0066" num="0065">In some embodiments, the processor <b>140</b> may acquire map data by identifying the location of the robot device <b>100</b> within a specific place (e.g., a building, a home, a business such as, but not limited to, a restaurant, a hotel, a store, a hospital, a warehouse), an object near the robot device <b>100</b>, etc., based on the sensing data of the sensor <b>110</b>.</p><p id="p-0067" num="0066">In some embodiments, the map data may include object information such as, at least one of location information of the object within a specific place, size information, shape information, and characteristic information. The size information may include at least one of information on the width, the height, and the length of the object, and the shape information may include a representative image, a plan-view shape (or a top-view), etc., of the object. Also, the characteristic information of the object may include information on whether the object may be climbed, or a threshold distance between the robot device <b>100</b> and the object in case the robot device <b>100</b> drives while avoiding the object (or, drives to follow the object), etc.</p><p id="p-0068" num="0067">In some embodiments, the processor <b>140</b> may analyze whether there is an object, a location of an object, a distance with an object, etc., based on the sensing data of the sensor <b>110</b>, and generate instruction data for generating an action of the robot device <b>100</b> based on the analysis result.</p><p id="p-0069" num="0068">In some embodiments, the instruction data may be data in a robot programming language format for controlling at least one of a driving distance, a driving direction, and a driving speed of the robot device <b>100</b>. However, this is merely an example, and the disclosure is not limited thereto. For example, the instruction data may be data in a robot programming language format for controlling a rotation direction, a rotation angle, or a rotation speed of the robot device <b>100</b>, or movements of each of the plurality of components provided in the robot device <b>100</b>.</p><p id="p-0070" num="0069">For example, the processor <b>140</b> may generate instruction data in a robot programming language format that may be interpreted by the robot device <b>100</b> such that the robot device <b>100</b> performs a specific action. As an example, when an object (e.g., an obstacle) is identified in the front side of the robot device <b>100</b>, the processor <b>140</b> may generate instruction data for making the robot device <b>100</b> drive to rotate to the right side or the left side, or for making the robot device <b>100</b> go backward. In such an example, the instruction data may include instructions regarding an action of driving to rotate to the right side, an action of driving to rotate to the left side, or an action of driving to go backward, etc.</p><p id="p-0071" num="0070">Then, the processor <b>140</b> may control the operation part <b>150</b> and/or the components of the robot device <b>100</b>, etc., such that the robot device <b>100</b> performs a specific action corresponding to the instruction data. In some embodiments, the instruction data may be referred to as command data including instructions, but may be generally referred to as instruction data for the convenience of explanation.</p><p id="p-0072" num="0071">The communication interface <b>120</b>, according to an embodiment of the disclosure, may transmit and/or receive data by performing communications with an external device (e.g., a source device, an external user terminal), an external storage medium (e.g., a universal serial bus (USB) memory), an external server (e.g., a web hard server), etc., through communication methods such as Wireless-Fidelity (Wi-Fi) based on an access point (AP) (e.g., a wireless local area network (LAN)), Bluetooth, Zigbee, a wired/wireless LAN, a wide area network (WAN), Ethernet, an Institute of Electrical and Electronics Engineers (IEEE) 1394 interface, a high-definition multimedia interface (HDMI), a USB interface, a Mobile High-Definition Link (MHL), AES/EBU (Audio Engineering Society/European Broadcasting Union), optical interface, coaxial interface, etc.</p><p id="p-0073" num="0072">In some embodiments, the robot device <b>100</b> may perform communication in a peer-to-peer (P2P) form with another electronic device through the communication interface <b>120</b>, and share communication. As an example, the robot device <b>100</b> may perform communication with another electronic device in an ad-hoc mode of transmitting and/or receiving information in a P2P form between devices without an AP.</p><p id="p-0074" num="0073">The processor <b>140</b>, according to an embodiment of the disclosure, may transmit data to a server (e.g., server <b>200</b> of <figref idref="DRAWINGS">FIG. <b>9</b></figref>) through the communication interface <b>120</b>, and receive data from the server. A more detailed explanation in this regard is made with reference to <figref idref="DRAWINGS">FIG. <b>9</b></figref>.</p><p id="p-0075" num="0074">According to an embodiment of the disclosure, the memory <b>130</b> provided in the robot device <b>100</b> may be implemented as an internal memory such as a read-only memory (ROM) (e.g., an electrically erasable programmable read-only memory (EEPROM)), a random-access memory (RAM), etc., included in the processor <b>140</b>, or implemented as a separate memory from the processor <b>140</b>. The memory <b>130</b> may be implemented in the form of a memory embedded in the robot device <b>100</b>, and/or implemented in the form of a memory that may be attached to and/or detached from the robot device <b>100</b> according to the use of stored data. For example, in the case of data for operating the robot device <b>100</b>, the data may be stored in a memory embedded in the robot device <b>100</b>, and in the case of data for an extended function of the robot device <b>100</b>, the data may be stored in a memory that may be attached to and/or detached from the robot device <b>100</b>.</p><p id="p-0076" num="0075">In the case of a memory embedded in the robot device <b>100</b>, the memory may be implemented as at least one of a volatile memory (e.g., a dynamic RAM (DRAM), a static RAM (SRAM), or a synchronous dynamic RAM (SDRAM), etc.) and a non-volatile memory (e.g., an one time programmable ROM (OTPROM), a programmable ROM (PROM), an erasable and programmable ROM (EPROM), an EEPROM, a mask ROM, a flash ROM, a flash memory (e.g., NAND flash, NOR flash, etc.), a hard drive, or a solid state drive (SSD)). In the case of a memory that may be attached to and/or detached from the robot device <b>100</b>, the memory may be implemented as forms such as a memory card (e.g., compact flash (CF), secure digital (SD), micro secure digital (Micro-SD), mini secure digital (Mini-SD), extreme digital (xD), a multi-media card (MMC), etc.), an external memory that may be connected to a USB port (e.g., a USB memory), etc.</p><p id="p-0077" num="0076">In some embodiments, if an action of the robot device <b>100</b> is changed according to control by the processor <b>140</b>, the processor <b>140</b> may store instruction data corresponding to the action, sensing data of the sensor <b>110</b> related to the action, and map data related to the action in the memory <b>130</b>. Here, the sensing data of the sensor <b>110</b> related to the action and the map data related to the action may respectively refer to sensing data and map data on the time point when the action of the robot device <b>100</b> was changed. For example, if an action of the robot device <b>100</b> is changed, the processor <b>140</b> may store instruction data corresponding to the action, and sensing data and map data on the time point when the action was changed in the memory <b>130</b>. Here, the map data may include information on an object adjacent to the robot device <b>100</b> on the time point when the action was changed.</p><p id="p-0078" num="0077">The processor <b>140</b> controls the overall operations of the robot device <b>100</b>.</p><p id="p-0079" num="0078">According to an embodiment, the processor <b>140</b> may be implemented as a digital signal processor (DSP) processing digital image signals, a microprocessor, an artificial intelligence (AI) processor, and/or a timing controller (T-CON). However, the disclosure is not limited thereto, and the processor <b>140</b> may include one or more of a central processing unit (CPU), a micro controller unit (MCU), a micro processing unit (MPU), a controller, an application processor (AP), or a communication processor (CP), and an advanced reduced instruction set computer (RISC) machine (ARM) processor. Alternative or additionally, the processor <b>140</b> may be implemented as a system on chip (SoC) having a processing algorithm stored therein or large scale integration (LSI), or in the form of a field programmable gate array (FPGA).</p><p id="p-0080" num="0079">The processor <b>140</b> according to an embodiment of the disclosure may control the operation of the robot device <b>100</b> based on map data for a specific place (e.g., geographical location) sensed by the sensor <b>110</b>. For example, the map data for a specific place may refer to data indicating the physical geography of the place wherein the robot device <b>100</b> is operated, and it may be an image form, but is not limited thereto.</p><p id="p-0081" num="0080">The processor <b>140</b> may generate instruction data for controlling the action of the robot device <b>100</b> based on information on an object included in the map data, section information of each of a plurality of sections constituting a place, etc. For example, the instruction data may be data in a robot programming language format for controlling at least one of a driving distance, a driving direction, and a driving speed of the robot device <b>100</b>.</p><p id="p-0082" num="0081">As an example, if a place is a restaurant, an object may be a table, a seat on which a user may sit, or physical obstacles in various forms. The map data may include identification information and location information of a table, a seat, etc., provided in the place. As another example, if a place is a home, an object may refer to furniture, a home appliance, or obstacles in various forms.</p><p id="p-0083" num="0082">Here, an obstacle may refer to various kinds of objects or situations that may interfere with the driving of the robot device <b>100</b>, that may cause stopping of the operation between driving of the robot device <b>100</b>, or that may cause damage and/or breakage of the robot device <b>100</b>. For example, an obstacle may include various objects such as a rug, clothes, a wall surface, a step, a threshold, etc., other than furniture and electronic devices.</p><p id="p-0084" num="0083">In some embodiments, section information of each of a plurality of sections constituting a place may refer to information for identifying each of the plurality of sections. For example, in case a specific place is a restaurant, section information may include identification information, location information, and size information, etc., regarding the kitchen, the payment area, and the hall area provided in the restaurant.</p><p id="p-0085" num="0084">Map data may be received from an external server (not shown) and stored in the memory <b>130</b>. Alternatively or additionally, the map data may be acquired based on sensing data (e.g., an image) acquired through the sensor <b>110</b> (e.g., a camera) included in the robot device <b>100</b>.</p><p id="p-0086" num="0085">The processor <b>140</b> according to an embodiment of the disclosure may control the hardware elements of the robot device <b>100</b> based on instruction data (e.g., data in a robot programming language format) and generate an action. Here, the action may include various movements that the robot device <b>100</b> may perform according to the purpose of the robot device <b>100</b>, such as an action for driving while avoiding an obstacle, etc., based on map data, an action for performing cleaning, an action for carrying an object, etc. A more detailed description in this regard is made with reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0087" num="0086"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram for illustrating data according to an embodiment of the disclosure.</p><p id="p-0088" num="0087">Referring to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, if an object <b>10</b> (e.g., an obstacle) is identified in the front side of the robot device <b>100</b> based on sensing data, the processor <b>140</b> according to an embodiment of the disclosure may generate instruction data for making the robot device <b>100</b> drive to rotate to the right side or the left side, or for making the robot device <b>100</b> go backward.</p><p id="p-0089" num="0088">The processor <b>140</b> may control the driving part, the components, etc. such that the robot device <b>100</b> performs a specific action corresponding to the instruction data. Referring to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the processor <b>140</b> may control the hardware elements of the robot device <b>100</b> based on the instruction data, and generate an action of driving to rotate to the right side.</p><p id="p-0090" num="0089">Here, if the action of the robot device <b>100</b> changes from an action of driving straight to an action of driving to rotate to the right side, the processor <b>140</b> may store the instruction data on the time point when the action changed, the sensing data including the operation data generated in the hardware elements on the time point when the action changed, and the map data on the time point when the action changed in the memory <b>130</b>.</p><p id="p-0091" num="0090">As another example, if the action of the robot device <b>100</b> changes from an action of driving straight to an action of driving to rotate to the right side, the processor <b>140</b> may store the instruction data corresponding to the changed action (i.e., the action of driving to rotate to the right side), the operation data generated in the hardware elements according to performing of the changed action, and the map data related to the changed action in the memory <b>130</b>. Here, the operation data may refer to data generated as the hardware elements such as the motor, the wheel, etc. are driven for the robot device <b>100</b> to perform the action.</p><p id="p-0092" num="0091">In some embodiments, the processor <b>140</b> may selectively and/or optionally store at least one of the instruction data, the sensing data, and the map data. For example, the processor <b>140</b> may store only the sensing data, or map the instruction data and the sensing data and store them. As another example, the processor <b>140</b> may map the instruction data, the sensing data, and the map data, and store them in the memory <b>130</b>.</p><p id="p-0093" num="0092"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram for illustrating instruction data and sensing data according to an embodiment of the disclosure.</p><p id="p-0094" num="0093">Referring to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the processor <b>140</b> according to an embodiment of the disclosure may acquire, based on map data, instruction data in a robot programming language format that may be interpreted by the robot device <b>100</b>.</p><p id="p-0095" num="0094">As an example, the processor <b>140</b> may generate &#x201c;moving&#x201d; instruction data <b>410</b> such that the robot device <b>100</b> performs an action of forward driving. Then, if an obstacle <b>20</b> is detected in the front side based on map data, the processor <b>140</b> may generate &#x201c;avoiding_obstacle&#x201d; instruction data <b>420</b> such that the robot device <b>100</b> performs an action of driving to avoid the obstacle <b>20</b>.</p><p id="p-0096" num="0095">In some embodiments, the processor <b>140</b> may control the hardware elements based on the instruction data, and perform an action. Then, the processor <b>140</b> may acquire operation data generated in the hardware elements of the robot device <b>100</b>. Here, the operation data may refer to data related to actual operations of the hardware elements according to performing of the action.</p><p id="p-0097" num="0096">If the action of the robot device <b>100</b> changes, the processor <b>140</b> according to an embodiment of the disclosure may store the instruction data, the sensing data, and the map data in the memory <b>130</b>. Then, the processor <b>140</b> may transmit the data stored in the memory <b>130</b> to the server through the communication interface <b>120</b>. For example, if the action of the robot device <b>100</b> changes from &#x201c;moving&#x201d; to &#x201c;avoiding_obstacle,&#x201d; the processor <b>140</b> may store the instruction data corresponding to the changed action, the sensing data including the operation data of the hardware elements according to performing of the action, and the map data related to the action in the memory <b>130</b>.</p><p id="p-0098" num="0097">If it is identified that the sensing data corresponding to an action that occurred in the robot device <b>100</b> does not belong to (e.g., is not within, is outside of) a threshold range based on data received from the server, the processor <b>140</b> according to an embodiment of the disclosure may generate an event. For example, the data received from the server may include the data for a threshold range of the sensing data for each action that may occur in the robot device <b>100</b>, which was acquired by analyzing the data transmitted from the robot device <b>100</b> to the server. The threshold range may refer to an error range and/or a normal range, but for the convenience of explanation, it is generally referred to as a threshold range.</p><p id="p-0099" num="0098">For example, the processor <b>140</b> may transmit instruction data (e.g., <b>420</b>) corresponding to an action &#x201c;avoiding_obstacle,&#x201d; sensing data including the operation data of the hardware elements according to performing of the action &#x201c;avoiding_obstacle,&#x201d; and map data related to the action &#x201c;avoiding_obstacle&#x201d; to the server. Then, the processor <b>140</b> may acquire data for the threshold range of the sensing data related to the action &#x201c;avoiding_obstacle&#x201d; based on the data received from the server.</p><p id="p-0100" num="0099">For example, the server may receive a plurality of sensing data for each action from a plurality of robot devices. Then, the server may analyze the plurality of sensing data and acquire data for the threshold range of the sensing data for each action.</p><p id="p-0101" num="0100">As an example, if sensing data corresponding to the action &#x201c;avoiding_obstacle&#x201d; is received from each of the plurality of robot devices, the server may analyze the plurality of sensing data, and acquire standard sensing data corresponding to the action &#x201c;avoiding_obstacle.&#x201d; For example, the server may analyze the plurality of sensing data, and acquire standard sensing data through combination of the most probable operation data, or acquire sensing data received from a specific robot device <b>100</b> as standard sensing data. Then, the server may transmit the standard sensing data for each action that may occur in the robot device <b>100</b> and threshold similarity with the standard sensing data to the robot device <b>100</b>.</p><p id="p-0102" num="0101">Then, the robot device <b>100</b> may measure a similarity between the received standard sensing data and the sensing data stored in the memory <b>130</b>, and identify whether the similarity is greater than or equal to the threshold similarity (e.g., whether the similarity belongs to the threshold range). For example, if the threshold similarity is 95%, the robot device <b>100</b> may compare the standard sensing data and the sensing data stored in the memory <b>130</b>, and determine whether the similarity is greater than or equal to 95% according to the comparison result. Here, as a method for determining similarity, a known method of measuring similarity may be used. However, this is merely an example, and the disclosure is not limited thereto.</p><p id="p-0103" num="0102">For example, if a plurality of sensing data for a specific action is collected, the server may analyze the plurality of sensing data, and acquire standard sensing data according to normal operations of the hardware elements included in the robot device <b>100</b>, wherein errors did not occur. As an example, the server may acquire standard sensing data from a plurality of sensing data based on a learning network model or an algorithm, and transmit the data to the robot device <b>100</b>. The learning network model may be an artificial intelligence model which went through machine learning (e.g., trained) based on the plurality of sensing data for each action.</p><p id="p-0104" num="0103">Functions related to artificial intelligence according to the disclosure may be operated through the processor <b>140</b> and the memory <b>130</b>. The processor <b>140</b> may consist of one or a plurality of processors. Here, the one or plurality of processors may be generic-purpose processors such as a CPU, an AP, a DSP, etc., graphic-dedicated processors such as a GPU and a vision processing unit (VPU), and/or artificial intelligence-dedicated processors such as a neural processing unit (NPU). The one or plurality of processors may perform control to process input data according to predefined operation rules and/or an artificial intelligence model stored in the memory. Alternatively, in case the one or plurality of processors are artificial intelligence-dedicated processors, the artificial intelligence-dedicated processors may be designed as a hardware structure specific for processing of a specific artificial intelligence model.</p><p id="p-0105" num="0104">The predefined operation rules and/or the artificial intelligence model may be characterized in that they are made through learning (e.g., machine learning, deep learning). That is, being made through learning may refer to a basic artificial intelligence model being trained by using a plurality of learning data by a learning algorithm, and the predefined operations rules and/or the artificial intelligence model set being created to perform desired characteristics (e.g., purposes). Such learning may be performed in a device itself wherein artificial intelligence is performed according to the disclosure, and/or through a separate server and/or a system. As examples of learning algorithms, there are supervised learning, unsupervised learning, semi-supervised learning, or reinforcement learning, but learning algorithms in the disclosure are not limited to the aforementioned examples.</p><p id="p-0106" num="0105">An artificial intelligence model may consist of a plurality of neural network layers. Each of the plurality of neural network layers may have a plurality of weight values, and may perform a neural network operation through an operation result of the previous layer and an operation among the plurality of weight values. The plurality of weight values included by the plurality of neural network layers may be optimized by the learning result of the artificial intelligence model. For example, the plurality of weight values may be updated such that a loss value and/or a cost value acquired at the artificial intelligence model during a learning process is reduced or minimized. An artificial neural network may include a deep neural network (DNN), a convolutional neural network (CNN), a recurrent neural network (RNN), a restricted Boltzmann Machine (RBM), a deep belief network (DBN), a bidirectional recurrent deep neural network (BRDNN), deep Q-networks, etc., but the disclosure is not limited to the aforementioned examples.</p><p id="p-0107" num="0106">Then, if it is identified that sensing data corresponding to an action occurred in the robot device <b>100</b> does not belong to (e.g., is not within, is outside of) the threshold range based on the data received from the server, the robot device <b>100</b> according to an embodiment of the disclosure may generate an event. A more detailed description in this regard is made with reference to <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0108" num="0107"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram for illustrating a threshold range according to an embodiment of the disclosure.</p><p id="p-0109" num="0108">Referring to <figref idref="DRAWINGS">FIG. <b>5</b></figref>, as the robot device <b>100</b> performs an action, the processor <b>140</b> according to an embodiment of the disclosure may acquire sensing data including data regarding operation of the hardware elements (e.g., the operation data).</p><p id="p-0110" num="0109">In some embodiments, the processor <b>140</b> may receive data from the server. Here, the data received from the server may include data for the threshold range of the sensing data for each action.</p><p id="p-0111" num="0110">The data for the threshold range of the sensing data for each action may include the standard sensing data for each action and the data regarding the threshold similarity. In some embodiments, the processor <b>140</b> may compare sensing data related to an action and the standard data related to the action, and identify similarity. Then, if the similarity is smaller than the threshold similarity, the processor <b>140</b> may identify that the sensing data corresponding to the action occurred in the robot device <b>100</b> does not belong to (e.g., is not within, is outside of) the threshold range.</p><p id="p-0112" num="0111">Referring to <figref idref="DRAWINGS">FIG. <b>5</b></figref>, if a similarity between the sensing data corresponding to the action &#x201c;avoiding_obstacle&#x201d; performed in the robot device <b>100</b> and the standard sensing data of the action &#x201c;avoiding_obstacle&#x201d; is smaller than the threshold similarity, the processor <b>140</b>, according to an embodiment of the disclosure, may identify that the sensing data does not belong to (e.g., is not within, is outside of) the threshold range, and generate an event <b>530</b>.</p><p id="p-0113" num="0112">The event <b>530</b> may include at least one of a feedback notifying that the sensing data corresponding to the action occurred in the robot device <b>100</b> does not belong to (e.g., is not within, is outside of) the threshold range, an instruction (e.g., <b>510</b> or <b>520</b>) controlling the hardware elements of the robot device <b>100</b> such that the sensing data corresponding to the action belongs to (e.g., is within) the threshold range, and an event notifying an element wherein an error occurred among the hardware elements of the robot device <b>100</b> based on the sensing data corresponding to the action.</p><p id="p-0114" num="0113">As an example, if the sensing data corresponding to a specific action performed in the robot device <b>100</b> has a similarity smaller than the threshold similarity with the standard sensing data corresponding to the specific action, the processor <b>140</b> may identify that an error occurred in the robot device <b>100</b>. The processor <b>140</b> may provide at least one of a visual feedback and an auditory feedback notifying that the sensing data corresponding to the action does not belong to (e.g., is not within, is outside of) the threshold range.</p><p id="p-0115" num="0114">As another example, the processor <b>140</b> may generate an action controlling the hardware elements such that the sensing data corresponding to the action belongs to (e.g., is within) the threshold range. For example, if the driving speed in the standard sensing data of an action &#x201c;moving&#x201d; is 0.5 meters per second (m/s), and the driving speed in the sensing data generated as the robot device <b>100</b> performs the action &#x201c;moving&#x201d; is 0.1 m/s, the processor <b>140</b> may generate an instruction for changing the driving speed to 0.5 m/s by controlling the wheel and the motor provided in the driving part.</p><p id="p-0116" num="0115">As still another example, if the sensing data corresponding to a specific action performed in the robot device <b>100</b> has a similarity smaller than the threshold similarity with the standard sensing data corresponding to the specific action, the processor <b>140</b> may generate an event <b>530</b> notifying that an error (or, an obstacle <b>20</b>) occurred in the hardware elements of which control is required for performing the specific action. For example, if the driving speed in the standard sensing data of an action &#x201c;moving&#x201d; is 0.5 m/s, and the driving speed in the sensing data generated as the robot device <b>100</b> performs the action &#x201c;moving&#x201d; is 0.1 m/s, the processor <b>140</b> may provide a visual feedback or an auditory feedback notifying that an error occurred in the wheel, the motor, etc. provided in the driving part.</p><p id="p-0117" num="0116"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a diagram for illustrating map data according to an embodiment of the disclosure.</p><p id="p-0118" num="0117">Referring to <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the processor <b>140</b> according to an embodiment of the disclosure may acquire map data including information on an object based on the sensing data of the sensor <b>110</b> while the robot device <b>100</b> is moving (or, driving).</p><p id="p-0119" num="0118">In some embodiments, the information on the object may include at least one of location information of the object within a specific place, size information, shape information, and characteristic information. The size information may include at least one of information on the width, the height, and the length of the object, and the shape information may include a representative image, a plan-view shape (and/or a top-view), etc., of the object. Also, the characteristic information of the object may include information on whether the object may be climbed, or a threshold distance between the robot device <b>100</b> and the object in case the robot device <b>100</b> drives while avoiding the object (or, drives to follow the object), etc.</p><p id="p-0120" num="0119">The processor <b>140</b>, according to an embodiment of the disclosure, may acquire information on an object by using an artificial intelligence model stored in the memory <b>130</b>. For example, an artificial intelligence model trained to identify an object in an input image may be stored in the memory <b>130</b>. In some embodiments, the artificial intelligence model may be a model trained by using a plurality of sample images including various objects. Identifying an object may be understood to refer to acquiring information on the object such as the name, the type, the size information, the shape information, the characteristic information, etc., of the object.</p><p id="p-0121" num="0120">In some embodiments, the processor <b>140</b> may acquire instruction data corresponding to the object based on the information on the object.</p><p id="p-0122" num="0121">For example, if an obstacle that cannot be climbed is detected in the front side, the processor <b>140</b> may acquire instruction data for driving while avoiding the obstacle. As another example, if an obstacle that may be climbed is detected in the front side, the processor <b>140</b> may acquire instruction data for driving to climb over the obstacle.</p><p id="p-0123" num="0122">If an action of the robot device <b>100</b> changes based on information on an object (e.g., map data), the processor <b>140</b> may transmit instruction data corresponding to the action, sensing data including operation data generated as the action is performed, and map data including the information on the object to the server.</p><p id="p-0124" num="0123"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a diagram for illustrating a case of replaying driving of a robot device <b>100</b> according to an embodiment of the disclosure.</p><p id="p-0125" num="0124">Referring to <figref idref="DRAWINGS">FIG. <b>7</b></figref>, a case wherein the robot device <b>100</b> according to an embodiment of the disclosure is implemented as a robot cleaner <b>700</b> may be assumed.</p><p id="p-0126" num="0125">The robot cleaner <b>700</b> may identify objects such as a point where there is a dividing line or a raised spot on the bottom, a point where the movable width narrows, a point where there is a wall, a point where a wall starts, a point where a wall ends, a point where there is a door, etc., based on the sensing data of the sensor <b>110</b>. The processor <b>140</b> may divide a place (e.g., a home) into a plurality of sections (e.g., a living room, a bedroom, a toilet, or a kitchen, etc.) with the identified points as boundaries among the sections.</p><p id="p-0127" num="0126">In some embodiments, the robot cleaner <b>700</b> may perform cleaning while moving in the plurality of sections sequentially.</p><p id="p-0128" num="0127">Referring to <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the robot cleaner <b>700</b>, according to an embodiment of the disclosure, may perform cleaning by cleaning the first area <b>710</b> (e.g., the kitchen), and then moving to the next section.</p><p id="p-0129" num="0128">According to an embodiment of the disclosure, the processor <b>140</b> may make the robot cleaner <b>700</b> drive based on the sensing data of the sensor <b>110</b>. Referring to <figref idref="DRAWINGS">FIG. <b>7</b></figref>, a case wherein the robot cleaner <b>700</b> cleans the first area <b>710</b> (e.g., the kitchen), and then performs an action to rotate to the left side for moving to the second area <b>720</b> may be assumed.</p><p id="p-0130" num="0129">As the action of the robot cleaner <b>700</b> was changed from an action of driving straight to an action of rotating to the left side (e.g., as illustrated by arrow <b>30</b>), the processor <b>140</b> may store instruction data corresponding to the action of rotating to the left side, and the sensing data and the map data on the time point when the action of rotating to the left side is performed in the memory <b>130</b>.</p><p id="p-0131" num="0130">Afterwards, when the test mode is executed, the robot cleaner <b>700</b> may replay the action based on the instruction data stored in the memory <b>130</b> and the sensing data corresponding to the instruction data.</p><p id="p-0132" num="0131">The robot cleaner <b>700</b>, according to an embodiment of the disclosure, may receive sensing data corresponding to the instruction data stored in the memory <b>130</b> from the server, and replay an action corresponding to the instruction data based on the instruction data stored in the memory <b>130</b> and the sensing data received from the server.</p><p id="p-0133" num="0132">The robot cleaner <b>700</b>, according to an embodiment of the disclosure, may receive instruction data and sensing data corresponding to the instruction data from the server, and replay an action based on the instruction data and the sensing data received from the server.</p><p id="p-0134" num="0133">For example, another robot device <b>100</b> may perform communication with the server, and receive instruction data and sensing data corresponding to the instruction data from the server. In such an example, the another robot device <b>100</b> may replay an action based on the instruction data and the sensing data received from the server. The action replayed by the another robot device <b>100</b> may refer to the action performed by the robot cleaner <b>700</b>. The another robot device <b>100</b> may replay the action performed in the robot cleaner <b>700</b> in the past, and/or simulate an action that may be performed. Here, the another robot device <b>100</b> may referred to as a simulator.</p><p id="p-0135" num="0134"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a detailed block diagram of a robot device <b>100</b> according to an embodiment of the disclosure.</p><p id="p-0136" num="0135">Referring to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the robot device <b>100</b> according to an embodiment of the disclosure may include a sensor <b>110</b>, a communication interface <b>120</b>, a memory <b>130</b>, a processor <b>140</b>, and an operation part <b>150</b>. Regarding components that overlap with the components illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, explanation will be omitted.</p><p id="p-0137" num="0136">The robot device <b>100</b> according to an embodiment may include an operation part <b>150</b> including an actuator or a motor. In some embodiments, the operation part <b>150</b> may include a wheel, a brake, etc. The robot device <b>100</b> may be implemented as a moving robot that may move in a specific place (e.g., a building, a home, a business such as, but not limited to, a restaurant, a hotel, a store, a hospital, a warehouse) by itself using the operation part <b>150</b>.</p><p id="p-0138" num="0137">The operation part <b>150</b> may include all components that are provided in the robot device <b>100</b> and make the robot device <b>100</b> capable of performing various operations and functions, other than components that make the robot device <b>100</b> movable. For example, the operation part <b>150</b> may include a robot joint and allow the robot device <b>100</b> to carry an external object.</p><p id="p-0139" num="0138"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a flow chart for illustrating a control method <b>800</b> for a robot device <b>100</b> according to an embodiment of the disclosure.</p><p id="p-0140" num="0139">In the control method <b>800</b> for a robot device <b>100</b>, if an action of the robot device <b>100</b> changes, instruction data corresponding to the action, sensing data related to the action, and map data related to the action are stored in operation S<b>810</b>.</p><p id="p-0141" num="0140">Then, the stored data is transmitted to a server in operation S<b>820</b>.</p><p id="p-0142" num="0141">In operation S<b>830</b>, if it is identified that the sensing data corresponding to the action occurred in the robot device <b>100</b> does not belong to (e.g., is not within, is outside of) a threshold range based on the data received from the server, an event is generated.</p><p id="p-0143" num="0142">The instruction data according to an embodiment of the disclosure may be data in a robot programming language format for controlling at least one of a driving distance, a driving direction, and a driving speed of the robot device <b>100</b>.</p><p id="p-0144" num="0143">In some embodiments, the control method <b>800</b> may further include controlling hardware elements of the robot device <b>100</b> based on the data in the robot programming language format and generating the action.</p><p id="p-0145" num="0144">The sensing data may include operation data generated in the hardware elements as the action occurs, and the map data may include information on an object in a specific place wherein the robot device <b>100</b> is located, and the information on the object may include at least one of location information, size information, shape information, and characteristic information of the object.</p><p id="p-0146" num="0145">Also, the data received from the server may include data for a threshold range of sensing data for each action that may occur in the robot device <b>100</b>, which was acquired by analyzing the data transmitted from the robot device <b>100</b> to the server, and the operation S<b>830</b> of generating an event may include identifying whether the sensing data corresponding to the action occurred in the robot device <b>100</b> belongs to (e.g., is within) the threshold range based on the data for the threshold range.</p><p id="p-0147" num="0146">In some embodiments, the data received from the server may indicate that the sensing data corresponding to the action occurred in the robot device <b>100</b> does not belong to (e.g., is not within, is outside of) the threshold range, and the operation S<b>830</b> of generating an event may include generating the event based on the received data.</p><p id="p-0148" num="0147">Further, the event may include at least one of a feedback notifying that the sensing data corresponding to the action occurred in the robot device <b>100</b> does not belong to (e.g., is not within, is outside of) the threshold range, an instruction controlling the hardware elements of the robot device <b>100</b> such that the sensing data corresponding to the action belongs to (e.g., is within) the threshold range, and an event notifying an element wherein an error occurred among the hardware elements of the robot device <b>100</b> based on the sensing data corresponding to the action.</p><p id="p-0149" num="0148">In some embodiments, the control method <b>800</b> may further include the step of, based on a test mode being executed, replaying the action based on the stored instruction data and the stored sensing data corresponding to the instruction data, or replaying the action based on the instruction data stored in the memory and the sensing data received from the server corresponding to the instruction data.</p><p id="p-0150" num="0149">In some embodiments, the operation S<b>810</b> of storing the instruction data may include the steps of converting the instruction data and the sensing data into data of a robot programming language format and storing the converted data in the memory <b>130</b>.</p><p id="p-0151" num="0150"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a block diagram illustrating a configuration of a robot device system according to an embodiment of the disclosure.</p><p id="p-0152" num="0151">As shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref>, a robot device system <b>1000</b> includes a robot device <b>100</b> and a server <b>200</b>.</p><p id="p-0153" num="0152">If an action changes, the robot device <b>100</b>, according to an embodiment, may store instruction data corresponding to the action, sensing data of the sensor related to the action, and map data related to the action, and transmit the stored data to a server. Also, if it is identified that the sensing data corresponding to the action occurred in the robot device <b>100</b> does not belong to (e.g., is not within, is outside of) a threshold range based on the data received from the server <b>200</b>, the robot device <b>100</b> may generate an event.</p><p id="p-0154" num="0153">The server <b>200</b>, according to an embodiment of the disclosure, may identify the threshold range of the sensing data corresponding to the action based on the instruction data, the sensing data, and the map data received from the robot device <b>100</b>. Then, the server <b>200</b> may transmit data related to the identified threshold range to the robot device <b>100</b>.</p><p id="p-0155" num="0154">A more detailed description in this regard is made with reference to <figref idref="DRAWINGS">FIG. <b>10</b></figref>.</p><p id="p-0156" num="0155"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a diagram for illustrating a robot device system <b>1000</b> according to an embodiment of the disclosure.</p><p id="p-0157" num="0156">As shown in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, the robot device system <b>1000</b> may include a plurality of robot devices (e.g., first robot device <b>100</b>-<b>1</b>, and second robot device <b>100</b>-<b>2</b>) communicatively coupled to a server <b>200</b> via a network <b>300</b>.</p><p id="p-0158" num="0157">Referring to <figref idref="DRAWINGS">FIG. <b>10</b></figref>, each of the plurality of robot devices (e.g., <b>100</b>-<b>1</b>, <b>100</b>-<b>2</b>), according to an embodiment of the disclosure, may transmit data to the server <b>200</b>. As an example, if a first action changes, the first robot device <b>100</b>-<b>1</b> may transmit instruction data corresponding to the first action, sensing data related to the first action, and map data related to the first action to the server <b>200</b>. Also, if a second action changes, the second robot device <b>100</b>-<b>2</b> may transmit instruction data corresponding to the second action, sensing data related to the second action, and map data related to the second action to the server <b>200</b>. As data is received from the plurality of robot devices (e.g., <b>100</b>-<b>1</b>, <b>100</b>-<b>2</b>), the server <b>200</b> may acquire the instruction data, the sensing data, and the map data corresponding to each of the plurality of actions (e.g., the first action, the second action).</p><p id="p-0159" num="0158">As an example, the server <b>200</b> may receive, from the first robot device <b>100</b>-<b>1</b>, instruction data corresponding to the first action, and sensing data and map data generated as the first robot device <b>100</b>-<b>1</b> performs the first action. Also, the server <b>200</b> may receive, from the second robot device <b>100</b>-<b>2</b>, instruction data corresponding to the second action, and sensing data and map data generated as the second robot device <b>100</b>-<b>2</b> performs the second action. Then, the server <b>200</b> may acquire standard sensing data for the first action based on the plurality of sensing data corresponding to the first action.</p><p id="p-0160" num="0159">For example, the server <b>200</b> may analyze the sensing data received from the first robot device <b>100</b>-<b>1</b> and the sensing data received from the second robot device <b>100</b>-<b>2</b>, and acquire sensing data by assuming a case wherein the first action is performed in a normal state of the hardware elements provided in a robot device <b>100</b>. Here, the sensing data acquired by the server <b>200</b> may include standard sensing data corresponding to the first action.</p><p id="p-0161" num="0160">In some embodiments, the server <b>200</b> may acquire standard sensing data for each action that may be performed in a robot device <b>100</b>, and/or a threshold range of the sensing data. For example, the server <b>200</b> may acquire sensing data corresponding to the first action and the threshold range of the sensing data, and sensing data corresponding to the second action and the threshold range of the sensing data. The threshold range may refer to a range of determining whether a robot device <b>100</b> performed a specific action without an error (and/or without an error of the hardware elements) based on the sensing data received from a robot device <b>100</b>.</p><p id="p-0162" num="0161">As an example, the server <b>200</b>, according to an embodiment of the disclosure, may transmit data for the threshold range of the sensing data corresponding to each of a plurality of actions that may occur in a robot device <b>100</b> to the first robot device <b>100</b>-<b>1</b>. Then, the first robot device <b>100</b>-<b>1</b> may compare sensing data corresponding to a specific action performed in the first robot device <b>100</b>-<b>1</b> and standard sensing data corresponding to the specific action based on the data received from the server <b>200</b>, and determine whether the similarity belongs within a threshold range. Then, if it is identified that the similarity is outside the threshold range according to the determination result, the first robot device <b>100</b>-<b>1</b> may provide feedback indicating that the hardware elements of the first robot device <b>100</b>-<b>1</b> are in a disabled state, or feedback notifying a hardware element related to the specific action as an event.</p><p id="p-0163" num="0162">In some embodiments, a case wherein the similarity belongs within the threshold range may indicate that the specific action performed in the first robot device <b>100</b>-<b>1</b> was performed within a normal range. For example, the first robot device <b>100</b>-<b>1</b> may perform comparison between the sensing data generated as the first robot device <b>100</b>-<b>1</b> performed an action and the standard sensing data corresponding to the action, and determine whether the first robot device <b>100</b>-<b>1</b> performed the action within the normal range according to the comparison result. The normal range may refer to standard sensing data generated by assuming a case wherein the first robot device <b>100</b>-<b>1</b> performed the action in a state wherein the hardware elements are not in a disabled state, or standard sensing data including information on an error range that was predetermined for the action.</p><p id="p-0164" num="0163">As another example, the server <b>200</b> may determine whether sensing data received from the first robot device <b>100</b>-<b>1</b> is outside a threshold range, and transmit the determination result to the first robot device <b>100</b>-<b>1</b>. For example, the determination result may be data indicating that sensing data corresponding to an action occurred in the first robot device <b>100</b>-<b>1</b> does not belong to (e.g., is not within, is outside of) a threshold range, and when the data is received, the robot device <b>100</b> may provide a feedback indicating that the hardware elements are in a disabled state, or a feedback notifying a hardware element related to the specific action as an event.</p><p id="p-0165" num="0164"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a sequence diagram for illustrating receipt and transmission of data between a server and a robot device <b>100</b> according to an embodiment of the disclosure.</p><p id="p-0166" num="0165">Referring to <figref idref="DRAWINGS">FIG. <b>11</b></figref>, if an action of the robot device <b>100</b> changes, the robot device <b>100</b> may store, in operation S<b>1110</b>, instruction data corresponding to the action, sensing data of the sensor related to the action, and map data related to the action.</p><p id="p-0167" num="0166">Then, the robot device <b>100</b> may transmit the stored data to a server in operation S<b>1120</b>.</p><p id="p-0168" num="0167">In operation S<b>1130</b>, the server <b>200</b>, according to an embodiment, may identify a threshold range corresponding to the action based on the instruction data, the sensing data, and the map data received from the robot device <b>100</b>.</p><p id="p-0169" num="0168">Then, the server <b>200</b> may transmit data related to the identified threshold range to the robot device <b>100</b> in operation S<b>1140</b>.</p><p id="p-0170" num="0169">In operation S<b>1150</b>, if it is identified that the sensing data corresponding to the action occurred in the robot device <b>100</b> does not belong to (e.g., is not within, is outside of) the threshold range based on the received data, the robot device <b>100</b> may generate an event.</p><p id="p-0171" num="0170">The various embodiments of the disclosure may be applied to all electronic devices, other than a robot device <b>100</b>.</p><p id="p-0172" num="0171">The various embodiments described above may be implemented by a recording medium that may be read by a computer or a device similar to a computer, by using software, hardware, or a combination thereof. In some cases, the embodiments described in this specification may be implemented as a processor itself. According to implementation by software, the embodiments such as procedures and functions described in this specification may be implemented as separate software modules. Each of the software modules may perform one or more functions and operations described in this specification.</p><p id="p-0173" num="0172">Computer instructions for performing processing operations of a robot cleaner according to the various embodiments of the disclosure described above may be stored in a non-transitory computer-readable medium. Such computer instructions stored in a non-transitory computer-readable medium make the processing operations at the robot device <b>100</b> according to the various embodiments described above performed by a specific machine, when they are executed by the processor of the specific machine.</p><p id="p-0174" num="0173">A non-transitory computer-readable medium refers to a medium that stores data semi-permanently, and is readable by machines, but not a medium that stores data for a short moment such as a register, a cache, and a memory. As specific examples of a non-transitory computer-readable medium, there may be a compact disc (CD), a digital video disc (DVD), a hard disc, a blue-ray disc, a USB, a memory card, a ROM and the like.</p><p id="p-0175" num="0174">While embodiments of the disclosure have been shown and described, the disclosure is not limited to the aforementioned specific embodiments, and it is apparent that various modifications may be made by those having ordinary skill in the art to which the disclosure belongs, without departing from the gist of the disclosure as claimed by the appended claims. Further, it is intended that such modifications are not to be interpreted independently from the technical idea or prospect of the disclosure.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A robot device comprising:<claim-text>a sensor configured to generate sensing data related to an action of the robot device;</claim-text><claim-text>a communication interface configured to communicate with a server;</claim-text><claim-text>a memory storing instructions; and</claim-text><claim-text>a processor configured to execute the instructions to:<claim-text>based on the action of the robot device changing, store action data in the memory, the action data comprising instruction data corresponding to the action, the sensing data related to the action, and map data related to the action,</claim-text><claim-text>transmit, to the server via the communication interface, the action data stored in the memory,</claim-text><claim-text>receive, from the server via the communication interface, threshold data corresponding to the action, and</claim-text><claim-text>based on identifying that the sensing data is outside of a threshold range based on the threshold data received from the server, generate an event.</claim-text></claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The robot device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the instruction data is in a robot programming language format, and is configured to control at least one of a driving distance, a driving direction, or a driving speed of the robot device, and<claim-text>the processor is further configured to execute the instructions to control the robot device, according to the instruction data, to generate the action.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The robot device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the sensing data related to the action comprises operation data generated in hardware elements of the robot device during performance of the action by the robot device,<claim-text>the map data comprises information on an object located in a specific place that the robot device is located, and</claim-text><claim-text>the information on the object comprises at least one of location information of the object, size information of the object, shape information of the object, or characteristic information of the object.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The robot device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the threshold data comprises threshold range values of sensing data for each action of the robot device, the threshold range values having been acquired according to an analysis of the action data transmitted to the server, and<claim-text>the processor is further configured to execute the instructions to identify whether the sensing data related to the action is within the threshold range.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The robot device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the threshold data indicates that the sensing data related to the action is outside of the threshold range, and<claim-text>the processor is further configured to execute the instructions to generate the event based on the threshold data.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The robot device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the event comprises at least one of:<claim-text>a first feedback notifying that the sensing data related to the action is outside of the threshold range,</claim-text><claim-text>an instruction configured to control hardware elements of the robot device such that the sensing data related to the action is within the threshold range, and</claim-text><claim-text>a second feedback identifying a hardware element from among the hardware elements of the robot device wherein an error occurred, based on the sensing data related to the action.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The robot device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the processor is further configured to execute the instructions to:<claim-text>based on a test mode being executed, replay the action based on the instruction data stored in the memory and the sensing data stored in the memory corresponding to the instruction data, or replay the action based on the instruction data stored in the memory and other sensing data received from the server corresponding to the instruction data.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The robot device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the processor is further configured to execute the instructions to:<claim-text>convert the instruction data and the sensing data into converted data in a robot programming language format,</claim-text><claim-text>store the converted data in the memory, and</claim-text><claim-text>transmit the converted data to the server via the communication interface.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. A robot device system comprising:<claim-text>a robot device configured to, based on an action of the robot device changing, store action data comprising instruction data corresponding to the action, sensing data related to the action and generated by a sensor of the robot device, and map data related to the action; and</claim-text><claim-text>a server configured to identify a threshold range of the sensing data related to the action based on the action data received from the robot device, and transmit, to the robot device, threshold data related to the identified threshold range,</claim-text><claim-text>wherein the robot device is further configured to generate an event, based on identifying that the sensing data is outside of the threshold range based on the threshold data received from the server.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The robot device system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the server is further configured to:<claim-text>receive, from the robot device, instruction data and sensing data corresponding to each of a plurality of actions, and</claim-text><claim-text>identify a threshold range of each of the plurality of actions, based on the received instruction data and sensing data.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The robot device system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the threshold data comprises information regarding the threshold range of the sensing data corresponding to each of a plurality of actions of the robot device.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The robot device system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the threshold data comprises information indicating that the sensing data related to the action is outside of the threshold range.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. A control method for a robot device, the control method comprising:<claim-text>storing, in a memory of the robot device, action data corresponding to an action of the robot device, based on the action of the robot device changing, the action data comprising instruction data corresponding to the action, sensing data related to the action, and map data related to the action;</claim-text><claim-text>transmitting the stored data to a server;</claim-text><claim-text>receiving, from the server, threshold data corresponding to the action data; and</claim-text><claim-text>based on identifying that the sensing data is outside of a threshold range based on the threshold data received from the server, generating an event.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The control method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the instruction data is in a robot programming language format, and is configured to control at least one of a driving distance, a driving direction, or a driving speed of the robot device, and<claim-text>the control method further comprises controlling the robot device to generate the action based on the instruction data.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The control method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the sensing data comprises operation data generated in hardware elements of the robot device during performing of the action by the robot device,<claim-text>the map data comprises information on an object located in a specific place that the robot device is located, and</claim-text><claim-text>the information on the object comprises at least one of location information of the object, size information of the object, shape information of the object, or characteristic information of the object.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The control method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the threshold data received from the server comprises threshold range values of sensing data for each action of the robot device, the threshold range values having been acquired according to an analysis of the action data transmitted to the server, and<claim-text>the generating of the event comprises identifying whether the sensing data related to the action is within the threshold range.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The control method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the threshold data indicates that the sensing data related to the action is outside of the threshold range, and<claim-text>the generating of the event comprises generating the event based on the threshold data.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The control method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the event comprises at least one of:<claim-text>a first feedback notifying that the sensing data related to the action is outside of the threshold range,</claim-text><claim-text>an instruction configured to control hardware elements of the robot device such that the sensing data related to the action is within the threshold range, and</claim-text><claim-text>a second feedback identifying a hardware element from among the hardware elements of the robot device wherein an error occurred, based on the sensing data related to the action.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The control method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, further comprising:<claim-text>based on a test mode being executed, replaying the action based on the instruction data stored in the memory and the sensing data stored in the memory corresponding to the instruction data, or replaying the action based on the instruction data stored in the memory and other sensing data received from the server corresponding to the instruction data.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The control method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the storing of the action data in the memory comprises:<claim-text>converting the instruction data and the sensing data into converted data in a robot programming language format;</claim-text><claim-text>storing the converted data in the memory; and</claim-text><claim-text>transmitting the converted data to the server.</claim-text></claim-text></claim></claims></us-patent-application>