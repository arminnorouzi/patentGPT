<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005294A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005294</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17745668</doc-number><date>20220516</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>40</main-group><subgroup>16</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>50</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>50</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>17</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>5</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>22</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>40</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>75</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>40</main-group><subgroup>168</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>50</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>50</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>17</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>5</main-group><subgroup>008</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>22</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>40</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>758</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30201</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>10024</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20208</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20012</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">SYSTEM, METHOD, AND COMPUTER PROGRAM FOR CAPTURING AN IMAGE WITH CORRECT SKIN TONE EXPOSURE</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>17694458</doc-number><date>20220314</date></document-id><parent-status>PENDING</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>17745668</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16796497</doc-number><date>20200220</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11455829</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17694458</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16290763</doc-number><date>20190301</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>10586097</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>16796497</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16215351</doc-number><date>20181210</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>10372971</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>16290763</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>15976756</doc-number><date>20180510</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>10558848</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>16215351</doc-number></document-id></child-doc></relation></continuation><us-provisional-application><document-id><country>US</country><doc-number>62599940</doc-number><date>20171218</date></document-id></us-provisional-application><us-provisional-application><document-id><country>US</country><doc-number>62568553</doc-number><date>20171005</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Duelight LLC</orgname><address><city>Sunnyvale</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Rivard</last-name><first-name>William Guie</first-name><address><city>Menlo Park</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Kindle</last-name><first-name>Brian J.</first-name><address><city>Sunnyvale</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Feder</last-name><first-name>Adam Barry</first-name><address><city>Mountain View</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A system and method are provided for capturing an image with correct skin tone exposure. In use, one or more faces having threshold skin tone are detected within a scene. Based on the detected one or more faces, a high dynamic range (HDR) capture mode is enabled. Further, the scene image is captured using the HDR capture mode.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="194.90mm" wi="153.75mm" file="US20230005294A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="211.07mm" wi="155.79mm" file="US20230005294A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="233.34mm" wi="140.12mm" file="US20230005294A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="163.91mm" wi="118.03mm" file="US20230005294A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="163.91mm" wi="118.03mm" file="US20230005294A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="163.91mm" wi="118.03mm" file="US20230005294A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="143.68mm" wi="157.99mm" file="US20230005294A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="213.78mm" wi="164.76mm" file="US20230005294A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="194.06mm" wi="133.69mm" file="US20230005294A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="243.92mm" wi="174.16mm" file="US20230005294A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="235.03mm" wi="151.64mm" file="US20230005294A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="198.20mm" wi="165.69mm" file="US20230005294A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="230.72mm" wi="153.75mm" file="US20230005294A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="235.37mm" wi="161.37mm" file="US20230005294A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="242.49mm" wi="171.70mm" file="US20230005294A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="244.77mm" wi="173.23mm" file="US20230005294A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="174.67mm" wi="161.12mm" file="US20230005294A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="185.76mm" wi="156.04mm" file="US20230005294A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCES TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">The present application is a continuation of and claims priority to U.S. patent application Ser. No. 17/694,458, titled &#x201c;SYSTEM, METHOD, AND COMPUTER PROGRAM FOR CAPTURING AN IMAGE WITH CORRECT SKIN TONE EXPOSURE,&#x201d; filed Mar. 14, 2022, which in turn, is a continuation of and claims priority to U.S. patent application Ser. No. 16/796,497, titled &#x201c;SYSTEM, METHOD, AND COMPUTER PROGRAM FOR CAPTURING AN IMAGE WITH CORRECT SKIN TONE EXPOSURE,&#x201d; filed Feb. 20, 2020, which in turn, is a continuation of and claims priority to U.S. patent application Ser. No. 16/290,763, titled &#x201c;SYSTEM, METHOD, AND COMPUTER PROGRAM FOR CAPTURING AN IMAGE WITH CORRECT SKIN TONE EXPOSURE,&#x201d; filed Mar. 1, 2019, which in turn, is a continuation of and claims priority to U.S. patent application Ser. No. 16/215,351, titled &#x201c;SYSTEM, METHOD, AND COMPUTER PROGRAM FOR CAPTURING AN IMAGE WITH CORRECT SKIN TONE EXPOSURE,&#x201d; filed Dec. 10, 2018, which in turn, is a continuation of and claims priority to U.S. patent application Ser. No. 15/976,756, titled &#x201c;SYSTEM, METHOD, AND COMPUTER PROGRAM FOR CAPTURING AN IMAGE WITH CORRECT SKIN TONE EXPOSURE,&#x201d; filed May 10, 2018, which claims priority to U.S. Provisional Patent Application No. 62/568,553, titled &#x201c;SYSTEM, METHOD, AND COMPUTER PROGRAM FOR CAPTURING AN IMAGE,&#x201d; filed Oct. 5, 2017, as well as U.S. Provisional Patent Application No. 62/599,940, titled &#x201c;SYSTEM, METHOD, AND COMPUTER PROGRAM FOR CAPTURING AN IMAGE WITH CORRECT SKIN TONE EXPOSURE SETTINGS,&#x201d; filed Dec. 18, 2017, all of which are hereby incorporated by reference for all purposes.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">FIELD OF THE INVENTION</heading><p id="p-0003" num="0002">The present invention relates to capturing an image, and more particularly to capturing an image with correct skin tone exposure.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">Conventional photographic systems currently capture images according to scene level exposure settings, with one or more points of interest used to specify corresponding regions used to meter and/or focus the scene for capture by a photographic system. However, a certain region (or regions) may have overall intensity levels that are too dark or too light to provide sufficient contrast using conventional capture techniques, resulting in a poor quality capture of significant visual features within the region. Consequently, conventional photographic systems commonly fail to capture usable portrait images of individuals with very dark skin tone or very light skin tone because the subject's skin tone is at one extreme edge of the dynamic range for the photographic system.</p><p id="p-0005" num="0004">There is thus a need for addressing these and/or other issues associated with the prior art.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0006" num="0005">A system and method are provided for capturing an image with correct skin tone exposure. In use, one or more faces having threshold skin tone are detected within a scene. Based on the detected one or more faces, a high dynamic range (HDR) capture mode is enabled. Further, the scene image is captured using the HDR capture mode.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. <b>1</b>A</figref> illustrates a first exemplary method for capturing an image, in accordance with one possible embodiment.</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>1</b>B</figref> illustrates a second exemplary method for capturing an image, in accordance with one possible embodiment.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>1</b>C</figref> illustrates an exemplary scene segmentation into a face region and non-face regions, in accordance with one possible embodiment.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>1</b>D</figref> illustrates a face region mask of a scene, in accordance with one possible embodiment.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>1</b>E</figref> illustrates a face region mask of a scene including a transition region, in accordance with one possible embodiment.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates an exemplary transition in mask value from a non-face region to a face region, in accordance with one possible embodiment.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>3</b>A</figref> illustrates a digital photographic system, in accordance with an embodiment.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>3</b>B</figref> illustrates a processor complex within the digital photographic system, according to one embodiment.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>3</b>C</figref> illustrates a digital camera, in accordance with an embodiment.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>3</b>D</figref> illustrates a wireless mobile device, in accordance with another embodiment.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>3</b>E</figref> illustrates a camera module configured to sample an image, according to one embodiment.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>3</b>F</figref> illustrates a camera module configured to sample an image, according to another embodiment.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>3</b>G</figref> illustrates a camera module in communication with an application processor, in accordance with an embodiment.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates a network service system, in accordance with another embodiment.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates capturing an image with correct skin tone exposure, in accordance with another embodiment.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates capturing an image with correct skin tone exposure, in accordance with another embodiment.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates capturing an image with correct skin tone exposure, in accordance with another embodiment.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates a network architecture, in accordance with one possible embodiment.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>9</b></figref> illustrates an exemplary system, in accordance with one embodiment.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>1</b>A</figref> illustrates an exemplary method <b>100</b> for capturing an image, in accordance with one possible embodiment. Method <b>100</b> may be performed by any technically feasible digital photographic system (e.g., a digital camera or digital camera subsystem). In one embodiment, method <b>100</b> is performed by digital photographic system <b>300</b> of <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>.</p><p id="p-0027" num="0026">At step <b>102</b>, the digital photographic system detects one or more faces within a scene. It is to be appreciated that although the present description describes detecting one or more faces, one or more other body parts (e.g. arms, hands, legs, feet, chest, neck, etc.) may be detected and used within the context of method <b>100</b>. Any technically feasible technique may be used to detect the one or more faces (or the one or more body parts). If, at step <b>104</b>, at least one of the one or more faces (or the one or more body parts) has a threshold skin tone, then the method proceeds to step <b>108</b>. In the context of the present description, skin tone refers to a shade of skin (e.g., human skin). For example, a skin tone may be light, medium, or dark, or a meld between light and medium, or medium and dark, according to a range of natural human skin colors.</p><p id="p-0028" num="0027">If, at step <b>104</b>, no face within the scene has a threshold skin tone, the method proceeds to step <b>106</b>. In the context of the present description, a threshold skin tone is defined to be a dark skin tone below a defined low intensity threshold or a light skin tone above a defined intensity high threshold. For dark skin tones, an individual's face may appear to be highly underexposed, while for light skin tones, and individual's face may appear to be washed out and overexposed. Such thresholds may be determined according to any technically feasible technique, including quantitative techniques and/or techniques using subjective assessment of captured images from a given camera system or systems.</p><p id="p-0029" num="0028">Additionally, a threshold skin tone may include a predefined shade of skin. For example, a threshold skin tone may refer to a skin tone of light shade, medium shade, or dark shade, or a percentage of light shade and/or medium shade and/or dark shade. Such threshold skin tone may be predefined by a user, by an application, an operating system, etc. Additionally, the threshold skin tone may function in a static manner (i.e. it does not change, etc.) or in a dynamic manner. For example, a threshold skin tone may be tied to a context of the capturing device (e.g. phone, camera, etc.) and/or of the environment. In this manner, a default threshold skin tone may be applied contingent upon specific contextual or environmental conditions (e.g. brightness is of predetermined range, etc.), and if such contextual and/or environmental conditions change, the threshold skin tone may be accordingly modified. For example, a default threshold skin tone may be tied to a &#x2018;normal&#x2019; condition of ambient lighting, but if the environment is changed to bright sunlight outside, the threshold skin tone may account for the brighter environment and modify the threshold skin tone.</p><p id="p-0030" num="0029">A low threshold skin tone may be any technically feasible threshold for low-brightness appearance within a captured scene. In one embodiment, the low threshold skin tone is defined as a low average intensity (e.g., below 15% of an overall intensity range) for a region for a detected face. In another embodiment, the low threshold skin tone is defined as a low contrast for the region for the detected face. In yet another embodiment, the low threshold is defined as a low histogram median (e.g., 20% of the overall intensity range) for the region for the detected face. Similarly, a high threshold may be any technically feasible threshold for high-brightness appearance within a captured scene. In one embodiment, the high threshold is defined as a high average intensity (e.g., above 85% of an overall intensity range) for a region for a detected face. In another embodiment, the high threshold is defined as high intensity (bright) but low contrast for the region for the detected face. In yet another embodiment, the high threshold is defined as a high histogram median (e.g., 80% of the overall intensity range) for the region for the detected face.</p><p id="p-0031" num="0030">If, at step <b>106</b>, the scene includes regions having collectively high dynamic range intensity, then the method proceeds to step <b>108</b>. Otherwise, the method proceeds to step <b>110</b>.</p><p id="p-0032" num="0031">At step <b>108</b>, the digital photographic system enables high dynamic range (HDR) capture. At step <b>110</b>, the digital photographic system captures an image of the scene according to a capture mode. For example, if the capture mode specified that HDR is enabled, then the digital photographic system captures an HDR image.</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>1</b>B</figref> illustrates an exemplary method <b>120</b> for capturing an image, in accordance with one possible embodiment. Method <b>120</b> may be performed by any technically feasible digital photographic system (e.g., a digital camera or digital camera subsystem). In one embodiment, method <b>120</b> is performed by digital photographic system <b>300</b> of <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>.</p><p id="p-0034" num="0033">At step <b>122</b>, the digital photographic system detects one or more faces within a scene having threshold skin tone, as described herein. Of course, it is to be appreciated that method <b>120</b> may be applied additionally to one or more other body parts (e.g. arm, neck, chest, leg, hand, etc.).</p><p id="p-0035" num="0034">At step <b>124</b>, the digital photographic system segments the scene into one or more face region(s) and one or more non-face region(s). Any technically feasible techniques may be implemented to provide scene segmentation, including techniques that surmise coverage for a segment/region based on appearance, as well as techniques that also include a depth image (z-map) captured in conjunction with a visual image. In an alternative embodiment, step <b>124</b> may include edge detection between one part (e.g. head, etc.) and a second part (e.g. neck, etc.). In certain embodiments, machine learning techniques (e.g., a neural network classifier) may be used to detect image pixels that are part of a face region(s), or skin associated with other body parts.</p><p id="p-0036" num="0035">At step <b>126</b>, the one or more images of the scene are captured. The camera module and/or digital photographic system may be used to capture such one or more images of the scene. In one embodiment, the digital photographic system may capture a single, high dynamic range image. For example, the digital photographic system may capture a single image, which may have a dynamic range of fourteen or more bits per color channel per pixel. In another embodiment, the digital photographic system captures two or more images, each of which may provide a relatively high dynamic range (e.g., twelve or more bits per color channel per pixel) or a dynamic range of less than twelve bits per color channel per pixel. The two or more images are exposed to capture detail of at least the face region(s) and the non-face region(s). For example, a first of the two or more images may be exposed so that the median intensity of the face region(s) defines the mid-point intensity of the first image. Furthermore, a second of the two or more images may be exposed so that the median intensity of the non-face region(s) defines a mid-point intensity of the second image.</p><p id="p-0037" num="0036">At step <b>128</b>, the digital photographic system processes the one or more face regions to generate a final image. In one embodiment, to process the one or more face regions, the digital photographic system applies a high degree of HDR effect to final image pixels within the face region(s). In certain embodiments, a degree of HDR effect is tapered down for pixels along a path leading from an outside boundary of a given face region through a transition region, to a boundary of a surrounding non-face region. The transition region may have an arbitrary thickness (e.g., one pixel to many pixels). In one embodiment, the degree of HDR effect is proportional to a strength coefficient, as defined in co-pending U.S. patent application Ser. No. 14/823,993, now U.S. Pat. No. 9,918,017, filed Aug. 11, 2015, entitled &#x201c;IMAGE SENSOR APPARATUS AND METHOD FOR OBTAINING MULTIPLE EXPOSURES WITH ZERO INTERFRAME TIME,&#x201d; which is incorporated herein by reference for all purposes. In other embodiments, other HDR techniques may be implemented, with the degree of HDR effect defined according to the particular technique. For example, a basic alpha blend may be used to blend between a conventionally exposed (ev 0) image and an HDR image, with the degree of zero HDR effect for non-face region pixels, a degree of one for face regions pixels, and a gradual transition (see <figref idref="DRAWINGS">FIG. <b>2</b></figref>) between one and zero for pixels within a transition region. In general, applying an HDR effect to pixels within a face region associated with an individual with a dark skin tone provides greater contrast at lower light levels and remaps the darker skin tones closer to an image intensity mid-point. Applying the HDR effect to pixels within the face region can provide greater contrast for pixels within the face region, thereby providing greater visual detail. Certain HDR techniques implement tone (intensity) mapping. In one embodiment, conventional HDR tone mapping is modified to provide greater range to pixels within the face region. For example, when capturing an image of an individual with dark skin tone, a darker captured intensity range may be mapped by the modified tone mapping to have a greater output range (final image) for pixels within the face region, while a conventional mapping is applied for pixels within the non-face region. In one embodiment, an HDR pixel stream (with correct tone mapping) may be created, as described in U.S. patent application Ser. No. 14/536,524, now U.S. Pat. No. 9,160,936, entitled &#x201c;SYSTEMS AND METHODS FOR GENERATING A HIGH-DYNAMIC RANGE (HDR) PIXEL STREAM,&#x201d; filed Nov. 7, 2014, which is hereby incorporated by reference for all purposes. Additionally, a video stream (with correct tone mapping) may be generated by applying the methods described herein.</p><p id="p-0038" num="0037">In another embodiment, to process the one or more images, the digital photographic system may perform a local equalization on pixels within the face region (or the selected body region). The local equalization may be applied with varying degrees within the transition region. In one embodiment, local equalization techniques, including contrast limited adaptive histogram, equalization (CLAHE) may be applied separately or in combination with an HDR technique. In such embodiments, one or more images may be captured according to method <b>120</b>, or one or more images may be captured according to any other technically feasible image capture technique.</p><p id="p-0039" num="0038">In certain embodiments, a depth map image and associated visual image(s) may be used to construct a model of one or more individuals within a scene. One or more texture maps may be generated from the visual image(s). For example, the depth map may be used, in part, to construct a three-dimensional (3D) model of an individual's face (photographic subject), while the visual image(s) may provide a surface texture for the 3D model. In one embodiment, a surface texture may include colors and/or features (e.g. moles, cuts, scars, freckles, facial fair, etc.). The surface texture may be modified to provide an average intensity that is closer to an image mid-point intensity, while preserving skin color and individually-unique skin texture (e.g. moles, cuts, scars, freckles, facial fair, etc.). The 3D model may then be rendered to generate a final image. The rendered image may include surmised natural scene lighting, natural scene lighting in combination with added synthetic illumination sources in the rendering process, or a combination thereof. For example, a soft side light may be added to provide depth cues from highlights and shadows on the individual's face. Furthermore, a gradient light may be added in the rendering process to provide additional highlights and shadows.</p><p id="p-0040" num="0039">In certain other embodiments, techniques disclosed herein for processing face regions may be implemented as post-processing rather than in conjunction with image capture.</p><p id="p-0041" num="0040">More illustrative information will now be set forth regarding various optional architectures and uses in which the foregoing method may or may not be implemented, per the desires of the user. It should be strongly noted that the following information is set forth for illustrative purposes and should not be construed as limiting in any manner. Any of the following features may be optionally incorporated with or without the exclusion of other features described.</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>1</b>C</figref> illustrates an exemplary scene segmentation into face region(s) <b>142</b> and non-face region(s) <b>144</b>, in accordance with one possible embodiment. As shown, an image <b>140</b> is segmented into a face region <b>142</b> and a non-face region <b>144</b>. Any technically feasible technique may be used to perform the scene segmentation. The technique may operate solely on visual image information, depth map information, or a combination thereof. As an option, <figref idref="DRAWINGS">FIG. <b>1</b>C</figref> may be implemented in the context of any of the other figures, as described herein. In particular, <figref idref="DRAWINGS">FIG. <b>1</b>C</figref> may be implemented within the context of steps <b>122</b>-<b>128</b> of <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>.</p><p id="p-0043" num="0042">In another embodiment, a selected body-part region may be distinguished and separately identified from a non-selected body-part region. For example, a hand may be distinguished from the surroundings, an arm from a torso, a foot from a leg, etc.</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. <b>1</b>D</figref> illustrates a face region mask <b>141</b> of a scene, in accordance with one possible embodiment. In one embodiment, a pixel value within face region mask <b>141</b> is set to a value of one (1.0) if a corresponding pixel location within image <b>140</b> is within face region <b>142</b>, and a pixel value within face region mask <b>141</b> is set to a value of zero (0.0) if a corresponding pixel location within image <b>140</b> is outside face region <b>142</b>. In one embodiment, a substantially complete face region mask <b>141</b> is generated and stored in memory. In another embodiment, individual mask elements are computed prior to use, without storing a complete face region mask <b>141</b> in memory. As an option, <figref idref="DRAWINGS">FIG. <b>1</b>D</figref> may be implemented in the context of any of the other figures, as described herein. In particular, <figref idref="DRAWINGS">FIG. <b>1</b>D</figref> may be implemented within the context of steps <b>122</b>-<b>128</b> of <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>, or within the context of <figref idref="DRAWINGS">FIG. <b>1</b>C</figref>.</p><p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. <b>1</b>E</figref> illustrates a face region mask <b>141</b> of a scene including a transition region <b>146</b>, in accordance with one possible embodiment. As shown, transition region <b>146</b> is disposed between face region <b>142</b> and non-face region <b>144</b>. Mask values within face region mask <b>141</b> increase from zero to one along path <b>148</b> from non-face region <b>144</b> to face region <b>142</b>. A gradient of increasing mask values from non-face region <b>144</b> to face region <b>142</b> is indicated along path <b>148</b>. For example, a mask value may increase from a value of zero (0.0) in non-face region <b>144</b> to a value of one (1.0) in face region <b>142</b> along path <b>148</b>. Any technically feasible technique may be used to generate the gradient. As an option, <figref idref="DRAWINGS">FIG. <b>1</b>E</figref> may be implemented in the context of any of the other figures, as described herein. In particular, <figref idref="DRAWINGS">FIG. <b>1</b>E</figref> may be implemented within the context of steps <b>122</b>-<b>128</b> of <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>, or within the context of <figref idref="DRAWINGS">FIGS. <b>1</b>C-<b>1</b>D</figref>.</p><p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates an exemplary transition in mask value from a non-face region (e.g., non-face region <b>144</b>) to a face region (e.g., face region <b>142</b>), in accordance with one possible embodiment. As an option, <figref idref="DRAWINGS">FIG. <b>2</b></figref> may be implemented in the context of the details of any of the Figures disclosed herein. Of course, however, <figref idref="DRAWINGS">FIG. <b>2</b></figref> may be implemented in any desired environment.</p><p id="p-0047" num="0046">As shown, a face region mask pixel value <b>202</b> (e.g., mask value at a given pixel within face region mask <b>141</b>) increases from non-face region <b>201</b>A pixels to face region <b>201</b>C pixels along path <b>200</b>, which starts out in a non-face region <b>201</b>A, traverses through a transition region <b>201</b>B, and continues into a face region <b>201</b>C. For example, path <b>200</b> may correspond to at least a portion of path <b>148</b> of <figref idref="DRAWINGS">FIG. <b>1</b>E</figref>. Of course, it is to be appreciated that <figref idref="DRAWINGS">FIG. <b>2</b></figref> may be implemented with respect to other and/or multiple body regions. For example, <figref idref="DRAWINGS">FIG. <b>2</b></figref> may be implemented to indicate mask values that include face regions and neck regions. Furthermore, <figref idref="DRAWINGS">FIG. <b>2</b></figref> may be implemented to indicate mask values that include any body part regions with skin that is visible.</p><p id="p-0048" num="0047">In other embodiments, a depth map may be used in conjunction with, or to determine, a face region. Additionally, contrast may be modified (e.g., using CLAHE or any similar technique) to determine a skin tone. In one embodiment, if the skin tone is lighter (or darker), additional contrast may be added (or removed). In other embodiments, if the contours of a face are known (e.g., from 3D mapping of the face), lighting, shadowing, and/or other lighting effects may be added or modified in one or more post processing operations. Further, voxelization (and/or 3d mapping of 2d images) or other spatial data (e.g., a surface mesh of a subject constructed from depth map data) may be used to model a face and/or additional body parts, and otherwise determine depth values associated with an image.</p><p id="p-0049" num="0048">In one embodiment, depth map information may be obtained from a digital camera (e.g. based on parallax calculated from more than one lens perspectives, more than one image of the same scene but from different angles an/or zoom levels, near-simultaneous capture of the image, dual pixel/focus pixel phase detection, etc.). Additionally, depth map information may also be obtained (e.g., from a depth map sensor). As an example, if a face is found in an image, and a depth map of the image is used to model the face, then synthetic lighting (e.g., a lighting gradient) could be added to the face to modify lighting conditions on the face (e.g., in real-time or in post-processing). Further, a texture map (sampled from the face in the image) may be used in conjunction with the depth map to generate a 3D model of the face. In this manner, not only can synthetic lighting be applied with correct perspective on an arbitrary face, but additionally, the lighting color may be correct for the ambient conditions and skin tone on the face (or whatever skin section is shown), according to a measured color balance for ambient lighting. Any technically feasible technique may be implemented for measuring color balance of ambient lighting, including identifying illumination sources in a scene and estimating color balance for one or more of the illumination sources. Alternatively, color balance for illumination on a subject's face may be determined based on matching sampled color to a known set of human skin tones.</p><p id="p-0050" num="0049">In one embodiment, a texture map may be created from the face in the image. Furthermore, contrast across the texture map may be selectively modified to correct for skin tone. For example, a scene(s) may be segmented to include regions of subject skin (e.g., face) and regions that are not skin. Additionally, the scene(s) may include other skin body parts (e.g. arm, neck, etc.). In such an example, all exposed skin may be included in the texture map (either as separate texture maps or one inclusive texture map) and corrected (e.g., equalized, tone mapped, etc.) together. The corrected texture map is then applied to a 3D model of the face and any visible skin associated with visible body parts. The 3D model may then be rendered in place in a scene to generate an image of the face and any other body parts of an individual in the scene. By performing contrast correction/adjustment on visible skin of the same individual, the generated image may appear to be more natural overall because consistent skin tone is preserved for the individual.</p><p id="p-0051" num="0050">In certain scenarios, non-contiguous regions of skin may be corrected separately. As an example, a news broadcaster may have light projected onto their face, while their neck, hands, or arms may be in shadows. Such face may therefore be very light, whereas the neck, hands, arms may all be of a separate and different hue and light intensity. As such, the scene may be segmented into several physical regions having different hue and light intensity. Each region may therefore be corrected in context for a more natural overall appearance.</p><p id="p-0052" num="0051">In one embodiment, an object classifier and/or recognition techniques (e.g. machine learning, etc.) may be used to detect a body part (hand, arm, face, leg) and associate all body parts with exposed skin such that contrast correcting (according to the texture map) may be applied to the detected body part or parts. In one embodiment, a neural-network classification engine is configured to identify individual pixels as being affiliated with exposed skin of a body part. Pixels that are identified as being exposed skin may be aggregated into segments (regions), and the regions may be corrected (e.g., equalized, tone-mapped, etc.).</p><p id="p-0053" num="0052">In one embodiment, a hierarchy of the scene may be built and a classification engine may be used to segment the scene. An associated texture map(s) may be generated from a scene segment(s). The texture map(s) may be corrected, rendered in place, and applied to such hierarchy. In another embodiment, the hierarchy may include extracting exposure values for each skin-exposed body part, and correlating the exposure values with a correction value based on the texture map.</p><p id="p-0054" num="0053">In some embodiments, skin tone may be different based on the determined body part. For example, facial skin tone may differ from the hand/arm/etc. In such an embodiment, a 3D model including the texture map and the depth map, may be generated and rendered to separately correct and/or equalize pixels for one or more of each different body part. In one embodiment, a reference tone (e.g., one of a number of discrete, known human skin tones) may be used as a basis for correction (e.g., equalization, tone mapping, hue adjustment) of pixels within an image that are affiliated with exposed skin. In other embodiments, correction/skin tone may be separately applied to different, visually non-contiguous body parts.</p><p id="p-0055" num="0054"><figref idref="DRAWINGS">FIG. <b>3</b>A</figref> illustrates a digital photographic system <b>300</b>, in accordance with one embodiment. As an option, the digital photographic system <b>300</b> may be implemented in the context of the details of any of the Figures disclosed herein. Of course, however, the digital photographic system <b>300</b> may be implemented in any desired environment. Further, the aforementioned definitions may equally apply to the description below.</p><p id="p-0056" num="0055">As shown, the digital photographic system <b>300</b> may include a processor complex <b>310</b> coupled to a camera module <b>330</b> via an interconnect <b>334</b>. In one embodiment, the processor complex <b>310</b> is coupled to a strobe unit <b>336</b>. The digital photographic system <b>300</b> may also include, without limitation, a display unit <b>312</b>, a set of input/output devices <b>314</b>, non-volatile memory <b>316</b>, volatile memory <b>318</b>, a wireless unit <b>340</b>, and sensor devices <b>342</b>, each coupled to the processor complex <b>310</b>. In one embodiment, a power management subsystem <b>320</b> is configured to generate appropriate power supply voltages for each electrical load element within the digital photographic system <b>300</b>. A battery <b>322</b> may be configured to supply electrical energy to the power management subsystem <b>320</b>. The battery <b>322</b> may implement any technically feasible energy storage system, including primary or rechargeable battery technologies. Of course, in other embodiments, additional or fewer features, units, devices, sensors, or subsystems may be included in the system.</p><p id="p-0057" num="0056">In one embodiment, a strobe unit <b>336</b> may be integrated into the digital photographic system <b>300</b> and configured to provide strobe illumination <b>350</b> during an image sample event performed by the digital photographic system <b>300</b>. In another embodiment, a strobe unit <b>336</b> may be implemented as an independent device from the digital photographic system <b>300</b> and configured to provide strobe illumination <b>350</b> during an image sample event performed by the digital photographic system <b>300</b>. The strobe unit <b>336</b> may comprise one or more LED devices, a gas-discharge illuminator (e.g. a Xenon strobe device, a Xenon flash lamp, etc.), or any other technically feasible illumination device. In certain embodiments, two or more strobe units are configured to synchronously generate strobe illumination in conjunction with sampling an image. In one embodiment, the strobe unit <b>336</b> is controlled through a strobe control signal <b>338</b> to either emit the strobe illumination <b>350</b> or not emit the strobe illumination <b>350</b>. The strobe control signal <b>338</b> may be implemented using any technically feasible signal transmission protocol. The strobe control signal <b>338</b> may indicate a strobe parameter (e.g. strobe intensity, strobe color, strobe time, etc.), for directing the strobe unit <b>336</b> to generate a specified intensity and/or color of the strobe illumination <b>350</b>. The strobe control signal <b>338</b> may be generated by the processor complex <b>310</b>, the camera module <b>330</b>, or by any other technically feasible combination thereof. In one embodiment, the strobe control signal <b>338</b> is generated by a camera interface unit within the processor complex <b>310</b> and transmitted to both the strobe unit <b>336</b> and the camera module <b>330</b> via the interconnect <b>334</b>. In another embodiment, the strobe control signal <b>338</b> is generated by the camera module <b>330</b> and transmitted to the strobe unit <b>336</b> via the interconnect <b>334</b>.</p><p id="p-0058" num="0057">Optical scene information <b>352</b>, which may include at least a portion of the strobe illumination <b>350</b> reflected from objects in the photographic scene, is focused as an optical image onto an image sensor <b>332</b> within the camera module <b>330</b>. The image sensor <b>332</b> generates an electronic representation of the optical image. The electronic representation comprises spatial color intensity information, which may include different color intensity samples (e.g. red, green, and blue light, etc.). In other embodiments, the spatial color intensity information may also include samples for white light. The electronic representation is transmitted to the processor complex <b>310</b> via the interconnect <b>334</b>, which may implement any technically feasible signal transmission protocol.</p><p id="p-0059" num="0058">In one embodiment, input/output devices <b>314</b> may include, without limitation, a capacitive touch input surface, a resistive tablet input surface, one or more buttons, one or more knobs, light-emitting devices, light detecting devices, sound emitting devices, sound detecting devices, or any other technically feasible device for receiving user input and converting the input to electrical signals, or converting electrical signals into a physical signal. In one embodiment, the input/output devices <b>314</b> include a capacitive touch input surface coupled to a display unit <b>312</b>. A touch entry display system may include the display unit <b>312</b> and a capacitive touch input surface, also coupled to processor complex <b>310</b>.</p><p id="p-0060" num="0059">Additionally, in other embodiments, non-volatile (NV) memory <b>316</b> is configured to store data when power is interrupted. In one embodiment, the NV memory <b>316</b> comprises one or more flash memory devices (e.g. ROM, PCM, FeRAM, FRAM, PRAM, MRAM, NRAM, etc.). The NV memory <b>316</b> comprises a non-transitory computer-readable medium, which may be configured to include programming instructions for execution by one or more processing units within the processor complex <b>310</b>. The programming instructions may implement, without limitation, an operating system (OS), UI software modules, image processing and storage software modules, one or more input/output devices <b>314</b> connected to the processor complex <b>310</b>, one or more software modules for sampling an image stack through camera module <b>330</b>, one or more software modules for presenting the image stack or one or more synthetic images generated from the image stack through the display unit <b>312</b>. As an example, in one embodiment, the programming instructions may also implement one or more software modules for merging images or portions of images within the image stack, aligning at least portions of each image within the image stack, or a combination thereof. In another embodiment, the processor complex <b>310</b> may be configured to execute the programming instructions, which may implement one or more software modules operable to create a high dynamic range (HDR) image.</p><p id="p-0061" num="0060">Still yet, in one embodiment, one or more memory devices comprising the NV memory <b>316</b> may be packaged as a module configured to be installed or removed by a user. In one embodiment, volatile memory <b>318</b> comprises dynamic random access memory (DRAM) configured to temporarily store programming instructions, image data such as data associated with an image stack, and the like, accessed during the course of normal operation of the digital photographic system <b>300</b>. Of course, the volatile memory may be used in any manner and in association with any other input/output device <b>314</b> or sensor device <b>342</b> attached to the process complex <b>310</b>.</p><p id="p-0062" num="0061">In one embodiment, sensor devices <b>342</b> may include, without limitation, one or more of an accelerometer to detect motion and/or orientation, an electronic gyroscope to detect motion and/or orientation, a magnetic flux detector to detect orientation, a global positioning system (GPS) module to detect geographic position, or any combination thereof. Of course, other sensors, including but not limited to a motion detection sensor, a proximity sensor, an RGB light sensor, a gesture sensor, a 3-D input image sensor, a pressure sensor, and an indoor position sensor, may be integrated as sensor devices. In one embodiment, the sensor devices may be one example of input/output devices <b>314</b>.</p><p id="p-0063" num="0062">Wireless unit <b>340</b> may include one or more digital radios configured to send and receive digital data. In particular, the wireless unit <b>340</b> may implement wireless standards (e.g. WiFi, Bluetooth, NFC, etc.), and may implement digital cellular telephony standards for data communication (e.g. CDMA, 3G, 4G, 5G, LTE, LTE-Advanced, etc.). Of course, any wireless standard or digital cellular telephony standards may be used.</p><p id="p-0064" num="0063">In one embodiment, the digital photographic system <b>300</b> is configured to transmit one or more digital photographs to a network-based (online) or &#x201c;cloud-based&#x201d; photographic media service via the wireless unit <b>340</b>. The one or more digital photographs may reside within the NV memory <b>316</b>, the volatile memory <b>318</b>, or any other memory device associated with the processor complex <b>310</b>. In one embodiment, a user may possess credentials to access an online photographic media service and to transmit one or more digital photographs for storage to, retrieval from, and presentation by the online photographic media service. The credentials may be stored or generated within the digital photographic system <b>300</b> prior to transmission of the digital photographs. The online photographic media service may comprise a social networking service, photograph sharing service, or any other network-based service that provides storage of digital photographs, processing of digital photographs, transmission of digital photographs, sharing of digital photographs, or any combination thereof. In certain embodiments, one or more digital photographs are generated by the online photographic media service based on image data (e.g. image stack, HDR image stack, image package, etc.) transmitted to servers associated with the online photographic media service. In such embodiments, a user may upload one or more source images from the digital photographic system <b>300</b> for processing and/or storage by the online photographic media service.</p><p id="p-0065" num="0064">In one embodiment, the digital photographic system <b>300</b> comprises at least one instance of a camera module <b>330</b>. In another embodiment, the digital photographic system <b>300</b> comprises a plurality of camera modules <b>330</b>. Such an embodiment may also include at least one strobe unit <b>336</b> configured to illuminate a photographic scene, sampled as multiple views by the plurality of camera modules <b>330</b>. The plurality of camera modules <b>330</b> may be configured to sample a wide angle view (e.g., greater than forty-five degrees of sweep among cameras) to generate a panoramic photograph. In one embodiment, a plurality of camera modules <b>330</b> may be configured to sample two or more narrow angle views (e.g., less than forty-five degrees of sweep among cameras) to generate a stereoscopic photograph. In other embodiments, a plurality of camera modules <b>330</b> may be configured to generate a 3-D image or to otherwise display a depth perspective (e.g. a z-component, etc.) as shown on the display unit <b>312</b> or any other display device. In still other embodiments, two or more different camera modules <b>330</b> are configured to have different optical properties, such as different optical zoom levels. In one embodiment, a first camera module <b>330</b> is configured to sense intensity at each pixel, while a second camera module <b>330</b> is configured to sense color at each pixel. In such an embodiment, pixel intensity information from the first camera module and pixel color information from the second camera module may be fused together to generate an output image. In one embodiment, a first camera module <b>330</b> with a higher zoom factor is configured to capture a central image, while at least one camera module <b>330</b> with a wider zoom factor is configured to capture a wider image; the central image and the wider image are then fused together to generate a visual image, while parallax between the central image and the wider image is used to estimate a depth image (depth map). The visual image and the depth map may be used to generate a corrected portrait image according to the techniques disclosed herein.</p><p id="p-0066" num="0065">In one embodiment, a display unit <b>312</b> may be configured to display a two-dimensional array of pixels to form an image for display. The display unit <b>312</b> may comprise a liquid-crystal (LCD) display, a light-emitting diode (LED) display, an organic LED display, or any other technically feasible type of display. In certain embodiments, the display unit <b>312</b> may only be able to display a narrower dynamic range of image intensity values than a complete range of intensity values sampled from a photographic scene, such as the dynamic range of a single HDR image, the total dynamic range sampled over a set of two or more images comprising a multiple exposure (e.g., an HDR image stack), or an image and/or image set captured to combine ambient illumination and strobe illumination (e.g., strobe illumination <b>350</b>). In one embodiment, images comprising an image stack may be merged according to any technically feasible HDR blending technique to generate a synthetic image for display within dynamic range constraints of the display unit <b>312</b>. In one embodiment, the limited dynamic range of display unit <b>312</b> may specify an eight-bit per color channel binary representation of corresponding color intensities. In other embodiments, the limited dynamic range may specify more than eight-bits (e.g., 10 bits, 12 bits, or 14 bits, etc.) per color channel binary representation.</p><p id="p-0067" num="0066"><figref idref="DRAWINGS">FIG. <b>3</b>B</figref> illustrates a processor complex <b>310</b> within the digital photographic system <b>300</b> of <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>, in accordance with one embodiment. As an option, the processor complex <b>310</b> may be implemented in the context of the details of any of the Figures disclosed herein. Of course, however, the processor complex <b>310</b> may be implemented in any desired environment. Further, the aforementioned definitions may equally apply to the description below.</p><p id="p-0068" num="0067">As shown, the processor complex <b>310</b> includes a processor subsystem <b>360</b> and may include a memory subsystem <b>362</b>. In one embodiment, processor complex <b>310</b> may comprise a system on a chip (SoC) device that implements processor subsystem <b>360</b>, and memory subsystem <b>362</b> comprises one or more DRAM devices coupled to the processor subsystem <b>360</b>. In another embodiment, the processor complex <b>310</b> may comprise a multi-chip module (MCM) encapsulating the SoC device and the one or more DRAM devices comprising the memory subsystem <b>362</b>.</p><p id="p-0069" num="0068">The processor subsystem <b>360</b> may include, without limitation, one or more central processing unit (CPU) cores <b>370</b>, a memory interface <b>380</b>, input/output interfaces unit <b>384</b>, and a display interface unit <b>382</b>, each coupled to an interconnect <b>374</b>. The one or more CPU cores <b>370</b> may be configured to execute instructions residing within the memory subsystem <b>362</b>, volatile memory <b>318</b>, NV memory <b>316</b>, or any combination thereof. Each of the one or more CPU cores <b>370</b> may be configured to retrieve and store data through interconnect <b>374</b> and the memory interface <b>380</b>. In one embodiment, each of the one or more CPU cores <b>370</b> may include a data cache, and an instruction cache. Additionally, two or more of the CPU cores <b>370</b> may share a data cache, an instruction cache, or any combination thereof. In one embodiment, a cache hierarchy is implemented to provide each CPU core <b>370</b> with a private cache layer, and a shared cache layer.</p><p id="p-0070" num="0069">In some embodiments, processor subsystem <b>360</b> may include one or more graphics processing unit (GPU) cores <b>372</b>. Each GPU core <b>372</b> may comprise a plurality of multi-threaded execution units that may be programmed to implement, without limitation, graphics acceleration functions. In various embodiments, the GPU cores <b>372</b> may be configured to execute multiple thread programs according to well-known standards (e.g. OpenGL&#x2122;, WebGL&#x2122;, OpenCL&#x2122;, CUDA&#x2122;, etc.), and/or any other programmable rendering graphic standard. In certain embodiments, at least one GPU core <b>372</b> implements at least a portion of a motion estimation function, such as a well-known Harris detector or a well-known Hessian-Laplace detector. Such a motion estimation function may be used at least in part to align images or portions of images within an image stack. For example, in one embodiment, an HDR image may be compiled based on an image stack, where two or more images are first aligned prior to compiling the HDR image. In another example, the motion estimation function may be used to stabilize video frames, either during real-time recording/previews or post-capture.</p><p id="p-0071" num="0070">As shown, the interconnect <b>374</b> is configured to transmit data between and among the memory interface <b>380</b>, the display interface unit <b>382</b>, the input/output interfaces unit <b>384</b>, the CPU cores <b>370</b>, and the GPU cores <b>372</b>. In various embodiments, the interconnect <b>374</b> may implement one or more buses, one or more rings, a cross-bar, a mesh, or any other technically feasible data transmission structure or technique. The memory interface <b>380</b> is configured to couple the memory subsystem <b>362</b> to the interconnect <b>374</b>. The memory interface <b>380</b> may also couple NV memory <b>316</b>, volatile memory <b>318</b>, or any combination thereof to the interconnect <b>374</b>. The display interface unit <b>382</b> may be configured to couple a display unit <b>312</b> to the interconnect <b>374</b>. The display interface unit <b>382</b> may implement certain frame buffer functions (e.g. frame refresh, etc.). Alternatively, in another embodiment, the display unit <b>312</b> may implement certain frame buffer functions (e.g. frame refresh, etc.). The input/output interfaces unit <b>384</b> may be configured to couple various input/output devices to the interconnect <b>374</b>.</p><p id="p-0072" num="0071">In certain embodiments, a camera module <b>330</b> is configured to store exposure parameters for sampling each image associated with an image stack. For example, in one embodiment, when directed to sample a photographic scene, the camera module <b>330</b> may sample a set of images comprising the image stack according to stored exposure parameters. A software module comprising programming instructions executing within a processor complex <b>310</b> may generate and store the exposure parameters prior to directing the camera module <b>330</b> to sample the image stack. In other embodiments, the camera module <b>330</b> may be used to meter an image or an image stack, and the software module comprising programming instructions executing within a processor complex <b>310</b> may generate and store metering parameters prior to directing the camera module <b>330</b> to capture the image. Of course, the camera module <b>330</b> may be used in any manner in combination with the processor complex <b>310</b>.</p><p id="p-0073" num="0072">In one embodiment, exposure parameters associated with images comprising the image stack may be stored within an exposure parameter data structure that includes exposure parameters for one or more images. In another embodiment, a camera interface unit (not shown in <figref idref="DRAWINGS">FIG. <b>3</b>B</figref>) within the processor complex <b>310</b> may be configured to read exposure parameters from the exposure parameter data structure and to transmit associated exposure parameters to the camera module <b>330</b> in preparation of sampling a photographic scene. After the camera module <b>330</b> is configured according to the exposure parameters, the camera interface may direct the camera module <b>330</b> to sample the photographic scene; the camera module <b>330</b> may then generate a corresponding image stack. The exposure parameter data structure may be stored within the camera interface unit, a memory circuit within the processor complex <b>310</b>, volatile memory <b>318</b>, NV memory <b>316</b>, the camera module <b>330</b>, or within any other technically feasible memory circuit. Further, in another embodiment, a software module executing within processor complex <b>310</b> may generate and store the exposure parameter data structure.</p><p id="p-0074" num="0073"><figref idref="DRAWINGS">FIG. <b>3</b>C</figref> illustrates a digital camera <b>302</b>, in accordance with one embodiment. As an option, the digital camera <b>302</b> may be implemented in the context of the details of any of the Figures disclosed herein. Of course, however, the digital camera <b>302</b> may be implemented in any desired environment. Further, the aforementioned definitions may equally apply to the description below.</p><p id="p-0075" num="0074">In one embodiment, the digital camera <b>302</b> may be configured to include a digital photographic system, such as digital photographic system <b>300</b> of <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>. As shown, the digital camera <b>302</b> includes a camera module <b>330</b>, which may include optical elements configured to focus optical scene information representing a photographic scene onto an image sensor, which may be configured to convert the optical scene information to an electronic representation of the photographic scene.</p><p id="p-0076" num="0075">Additionally, the digital camera <b>302</b> may include a strobe unit <b>336</b>, and may include a shutter release button <b>315</b> for triggering a photographic sample event, whereby digital camera <b>302</b> samples one or more images comprising the electronic representation. In other embodiments, any other technically feasible shutter release mechanism may trigger the photographic sample event (e.g. such as a timer trigger or remote control trigger, etc.).</p><p id="p-0077" num="0076"><figref idref="DRAWINGS">FIG. <b>3</b>D</figref> illustrates a wireless mobile device <b>376</b>, in accordance with one embodiment. As an option, the mobile device <b>376</b> may be implemented in the context of the details of any of the Figures disclosed herein. Of course, however, the mobile device <b>376</b> may be implemented in any desired environment. Further, the aforementioned definitions may equally apply to the description below.</p><p id="p-0078" num="0077">In one embodiment, the mobile device <b>376</b> may be configured to include a digital photographic system (e.g. such as digital photographic system <b>300</b> of <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>), which is configured to sample a photographic scene. In various embodiments, a camera module <b>330</b> may include optical elements configured to focus optical scene information representing the photographic scene onto an image sensor, which may be configured to convert the optical scene information to an electronic representation of the photographic scene. Further, a shutter release command may be generated through any technically feasible mechanism, such as a virtual button, which may be activated by a touch gesture on a touch entry display system comprising display unit <b>312</b>, or a physical button, which may be located on any face or surface of the mobile device <b>376</b>. Of course, in other embodiments, any number of other buttons, external inputs/outputs, or digital inputs/outputs may be included on the mobile device <b>376</b>, and which may be used in conjunction with the camera module <b>330</b>.</p><p id="p-0079" num="0078">As shown, in one embodiment, a touch entry display system comprising display unit <b>312</b> is disposed on the opposite side of mobile device <b>376</b> from camera module <b>330</b>. In certain embodiments, the mobile device <b>376</b> includes a user-facing camera module <b>331</b> and may include a user-facing strobe unit (not shown). Of course, in other embodiments, the mobile device <b>376</b> may include any number of user-facing camera modules or rear-facing camera modules, as well as any number of user-facing strobe units or rear-facing strobe units.</p><p id="p-0080" num="0079">In some embodiments, the digital camera <b>302</b> and the mobile device <b>376</b> may each generate and store a synthetic image based on an image stack sampled by camera module <b>330</b>. The image stack may include one or more images sampled under ambient lighting conditions, one or more images sampled under strobe illumination from strobe unit <b>336</b>, one or more images sampled under an accumulated exposure of both ambient and strobe illumination, or a combination thereof.</p><p id="p-0081" num="0080"><figref idref="DRAWINGS">FIG. <b>3</b>E</figref> illustrates camera module <b>330</b>, in accordance with one embodiment. As an option, the camera module <b>330</b> may be implemented in the context of the details of any of the Figures disclosed herein. Of course, however, the camera module <b>330</b> may be implemented in any desired environment. Further, the aforementioned definitions may equally apply to the description below.</p><p id="p-0082" num="0081">In one embodiment, the camera module <b>330</b> may be configured to control strobe unit <b>336</b> through strobe control signal <b>338</b>. As shown, a lens <b>390</b> is configured to focus optical scene information <b>352</b> onto image sensor <b>332</b> to be sampled. In one embodiment, image sensor <b>332</b> advantageously controls detailed timing of the strobe unit <b>336</b> though the strobe control signal <b>338</b> to reduce inter-sample time between an image sampled with the strobe unit <b>336</b> enabled, and an image sampled with the strobe unit <b>336</b> disabled. For example, the image sensor <b>332</b> may enable the strobe unit <b>336</b> to emit strobe illumination <b>350</b> less than one microsecond (or any desired length) after image sensor <b>332</b> completes an exposure time associated with sampling an ambient image and prior to sampling a strobe image.</p><p id="p-0083" num="0082">In other embodiments, the strobe illumination <b>350</b> may be configured based on a desired one or more target points. For example, in one embodiment, the strobe illumination <b>350</b> may light up an object in the foreground, and depending on the length of exposure time, may also light up an object in the background of the image. In one embodiment, once the strobe unit <b>336</b> is enabled, the image sensor <b>332</b> may then immediately begin exposing a strobe image. The image sensor <b>332</b> may thus be able to directly control sampling operations, including enabling and disabling the strobe unit <b>336</b> associated with generating an image stack, which may comprise at least one image sampled with the strobe unit <b>336</b> disabled, and at least one image sampled with the strobe unit <b>336</b> either enabled or disabled. In one embodiment, data comprising the image stack sampled by the image sensor <b>332</b> is transmitted via interconnect <b>334</b> to a camera interface unit <b>386</b> within processor complex <b>310</b>. In some embodiments, the camera module <b>330</b> may include an image sensor controller (e.g., controller <b>333</b> of <figref idref="DRAWINGS">FIG. <b>3</b>G</figref>), which may be configured to generate the strobe control signal <b>338</b> in conjunction with controlling operation of the image sensor <b>332</b>.</p><p id="p-0084" num="0083"><figref idref="DRAWINGS">FIG. <b>3</b>F</figref> illustrates a camera module <b>330</b>, in accordance with one embodiment. As an option, the camera module <b>330</b> may be implemented in the context of the details of any of the Figures disclosed herein. Of course, however, the camera module <b>330</b> may be implemented in any desired environment. Further, the aforementioned definitions may equally apply to the description below.</p><p id="p-0085" num="0084">In one embodiment, the camera module <b>330</b> may be configured to sample an image based on state information for strobe unit <b>336</b>. The state information may include, without limitation, one or more strobe parameters (e.g. strobe intensity, strobe color, strobe time, etc.), for directing the strobe unit <b>336</b> to generate a specified intensity and/or color of the strobe illumination <b>350</b>. In one embodiment, commands for configuring the state information associated with the strobe unit <b>336</b> may be transmitted through a strobe control signal <b>338</b>, which may be monitored by the camera module <b>330</b> to detect when the strobe unit <b>336</b> is enabled. For example, in one embodiment, the camera module <b>330</b> may detect when the strobe unit <b>336</b> is enabled or disabled within a microsecond or less of the strobe unit <b>336</b> being enabled or disabled by the strobe control signal <b>338</b>. To sample an image requiring strobe illumination, a camera interface unit <b>386</b> may enable the strobe unit <b>336</b> by sending an enable command through the strobe control signal <b>338</b>. In one embodiment, the camera interface unit <b>386</b> may be included as an interface of input/output interfaces <b>384</b> in a processor subsystem <b>360</b> of the processor complex <b>310</b> of <figref idref="DRAWINGS">FIG. <b>3</b>B</figref>. The enable command may comprise a signal level transition, a data packet, a register write, or any other technically feasible transmission of a command. The camera module <b>330</b> may sense that the strobe unit <b>336</b> is enabled and then cause image sensor <b>332</b> to sample one or more images requiring strobe illumination while the strobe unit <b>336</b> is enabled. In such an implementation, the image sensor <b>332</b> may be configured to wait for an enable signal destined for the strobe unit <b>336</b> as a trigger signal to begin sampling a new exposure.</p><p id="p-0086" num="0085">In one embodiment, camera interface unit <b>386</b> may transmit exposure parameters and commands to camera module <b>330</b> through interconnect <b>334</b>. In certain embodiments, the camera interface unit <b>386</b> may be configured to directly control strobe unit <b>336</b> by transmitting control commands to the strobe unit <b>336</b> through strobe control signal <b>338</b>. By directly controlling both the camera module <b>330</b> and the strobe unit <b>336</b>, the camera interface unit <b>386</b> may cause the camera module <b>330</b> and the strobe unit <b>336</b> to perform their respective operations in precise time synchronization. In one embodiment, precise time synchronization may be less than five hundred microseconds of event timing error. Additionally, event timing error may be a difference in time from an intended event occurrence to the time of a corresponding actual event occurrence.</p><p id="p-0087" num="0086">In another embodiment, camera interface unit <b>386</b> may be configured to accumulate statistics while receiving image data from camera module <b>330</b>. In particular, the camera interface unit <b>386</b> may accumulate exposure statistics for a given image while receiving image data for the image through interconnect <b>334</b>. Exposure statistics may include, without limitation, one or more of an intensity histogram, a count of over-exposed pixels, a count of under-exposed pixels, an intensity-weighted sum of pixel intensity, or any combination thereof. The camera interface unit <b>386</b> may present the exposure statistics as memory-mapped storage locations within a physical or virtual address space defined by a processor, such as one or more of CPU cores <b>370</b>, within processor complex <b>310</b>. In one embodiment, exposure statistics reside in storage circuits that are mapped into a memory-mapped register space, which may be accessed through the interconnect <b>334</b>. In other embodiments, the exposure statistics are transmitted in conjunction with transmitting pixel data for a captured image. For example, the exposure statistics for a given image may be transmitted as in-line data, following transmission of pixel intensity data for the captured image. Exposure statistics may be calculated, stored, or cached within the camera interface unit <b>386</b>. In other embodiments, an image sensor controller within camera module <b>330</b> is configured to accumulate the exposure statistics and transmit the exposure statistics to processor complex <b>310</b>, such as by way of camera interface unit <b>386</b>. In one embodiments, the exposure statistics are accumulated within the camera module <b>330</b> and transmitted to the camera interface unit <b>386</b>, either in conjunction with transmitting image data to the camera interface unit <b>386</b>, or separately from transmitting image data.</p><p id="p-0088" num="0087">In one embodiment, camera interface unit <b>386</b> may accumulate color statistics for estimating scene white-balance. Any technically feasible color statistics may be accumulated for estimating white balance, such as a sum of intensities for different color channels comprising red, green, and blue color channels. The sum of color channel intensities may then be used to perform a white-balance color correction on an associated image, according to a white-balance model such as a gray-world white-balance model. In other embodiments, curve-fitting statistics are accumulated for a linear or a quadratic curve fit used for implementing white-balance correction on an image. As with the exposure statistics, the color statistics may be presented as memory-mapped storage locations within processor complex <b>310</b>. In one embodiment, the color statistics are mapped in a memory-mapped register space, which may be accessed through interconnect <b>334</b>. In other embodiments, the color statistics may be transmitted in conjunction with transmitting pixel data for a captured image. For example, in one embodiment, the color statistics for a given image may be transmitted as in-line data, following transmission of pixel intensity data for the image. Color statistics may be calculated, stored, or cached within the camera interface <b>386</b>. In other embodiments, the image sensor controller within camera module <b>330</b> is configured to accumulate the color statistics and transmit the color statistics to processor complex <b>310</b>, such as by way of camera interface unit <b>386</b>. In one embodiments, the color statistics are accumulated within the camera module <b>330</b> and transmitted to the camera interface unit <b>386</b>, either in conjunction with transmitting image data to the camera interface unit <b>386</b>, or separately from transmitting image data.</p><p id="p-0089" num="0088">In one embodiment, camera interface unit <b>386</b> may accumulate spatial color statistics for performing color-matching between or among images, such as between or among an ambient image and one or more images sampled with strobe illumination. As with the exposure statistics, the spatial color statistics may be presented as memory-mapped storage locations within processor complex <b>310</b>. In one embodiment, the spatial color statistics are mapped in a memory-mapped register space. In another embodiment the camera module is configured to accumulate the spatial color statistics, which may be accessed through interconnect <b>334</b>. In other embodiments, the color statistics may be transmitted in conjunction with transmitting pixel data for a captured image. For example, in one embodiment, the color statistics for a given image may be transmitted as in-line data, following transmission of pixel intensity data for the image. Color statistics may be calculated, stored, or cached within the camera interface <b>386</b>.</p><p id="p-0090" num="0089">In one embodiment, camera module <b>330</b> may transmit strobe control signal <b>338</b> to strobe unit <b>336</b>, enabling the strobe unit <b>336</b> to generate illumination while the camera module <b>330</b> is sampling an image. In another embodiment, camera module <b>330</b> may sample an image illuminated by strobe unit <b>336</b> upon receiving an indication signal from camera interface unit <b>386</b> that the strobe unit <b>336</b> is enabled. In yet another embodiment, camera module <b>330</b> may sample an image illuminated by strobe unit <b>336</b> upon detecting strobe illumination within a photographic scene via a rapid rise in scene illumination. In one embodiment, a rapid rise in scene illumination may include at least a rate of increasing intensity consistent with that of enabling strobe unit <b>336</b>. In still yet another embodiment, camera module <b>330</b> may enable strobe unit <b>336</b> to generate strobe illumination while sampling one image, and disable the strobe unit <b>336</b> while sampling a different image.</p><p id="p-0091" num="0090"><figref idref="DRAWINGS">FIG. <b>3</b>G</figref> illustrates camera module <b>330</b>, in accordance with one embodiment. As an option, the camera module <b>330</b> may be implemented in the context of the details of any of the Figures disclosed herein. Of course, however, the camera module <b>330</b> may be implemented in any desired environment. Further, the aforementioned definitions may equally apply to the description below.</p><p id="p-0092" num="0091">In one embodiment, the camera module <b>330</b> may be in communication with an application processor <b>335</b>. The camera module <b>330</b> is shown to include image sensor <b>332</b> in communication with a controller <b>333</b>. Further, the controller <b>333</b> is shown to be in communication with the application processor <b>335</b>.</p><p id="p-0093" num="0092">In one embodiment, the application processor <b>335</b> may reside outside of the camera module <b>330</b>. As shown, the lens <b>390</b> may be configured to focus optical scene information to be sampled onto image sensor <b>332</b>. The optical scene information sampled by the image sensor <b>332</b> may then be communicated from the image sensor <b>332</b> to the controller <b>333</b> for at least one of subsequent processing and communication to the application processor <b>335</b>. In another embodiment, the controller <b>333</b> may control storage of the optical scene information sampled by the image sensor <b>332</b>, or storage of processed optical scene information.</p><p id="p-0094" num="0093">In another embodiment, the controller <b>333</b> may enable a strobe unit to emit strobe illumination for a short time duration (e.g. less than ten milliseconds) after image sensor <b>332</b> completes an exposure time associated with sampling an ambient image. Further, the controller <b>333</b> may be configured to generate strobe control signal <b>338</b> in conjunction with controlling operation of the image sensor <b>332</b>.</p><p id="p-0095" num="0094">In one embodiment, the image sensor <b>332</b> may be a complementary metal oxide semiconductor (CMOS) sensor or a charge-coupled device (CCD) sensor. In another embodiment, the controller <b>333</b> and the image sensor <b>332</b> may be packaged together as an integrated system, multi-chip module, multi-chip stack, or integrated circuit. In yet another embodiment, the controller <b>333</b> and the image sensor <b>332</b> may comprise discrete packages. In one embodiment, the controller <b>333</b> may provide circuitry for receiving optical scene information from the image sensor <b>332</b>, processing of the optical scene information, timing of various functionalities, and signaling associated with the application processor <b>335</b>. Further, in another embodiment, the controller <b>333</b> may provide circuitry for control of one or more of exposure, shuttering, white balance, and gain adjustment. Processing of the optical scene information by the circuitry of the controller <b>333</b> may include one or more of gain application, amplification, and analog-to-digital conversion. After processing the optical scene information, the controller <b>333</b> may transmit corresponding digital pixel data, such as to the application processor <b>335</b>.</p><p id="p-0096" num="0095">In one embodiment, the application processor <b>335</b> may be implemented on processor complex <b>310</b> and at least one of volatile memory <b>318</b> and NV memory <b>316</b>, or any other memory device and/or system. The application processor <b>335</b> may be previously configured for processing of received optical scene information or digital pixel data communicated from the camera module <b>330</b> to the application processor <b>335</b>.</p><p id="p-0097" num="0096"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates a network service system <b>400</b>, in accordance with one embodiment. As an option, the network service system <b>400</b> may be implemented in the context of the details of any of the Figures disclosed herein. Of course, however, the network service system <b>400</b> may be implemented in any desired environment. Further, the aforementioned definitions may equally apply to the description below.</p><p id="p-0098" num="0097">In one embodiment, the network service system <b>400</b> may be configured to provide network access to a device implementing a digital photographic system. As shown, network service system <b>400</b> includes a wireless mobile device <b>376</b>, a wireless access point <b>472</b>, a data network <b>474</b>, a data center <b>480</b>, and a data center <b>481</b>. The wireless mobile device <b>376</b> may communicate with the wireless access point <b>472</b> via a digital radio link <b>471</b> to send and receive digital data, including data associated with digital images. The wireless mobile device <b>376</b> and the wireless access point <b>472</b> may implement any technically feasible transmission techniques for transmitting digital data via digital radio link <b>471</b> without departing the scope and spirit of the present invention. In certain embodiments, one or more of data centers <b>480</b>, <b>481</b> may be implemented using virtual constructs so that each system and subsystem within a given data center <b>480</b>, <b>481</b> may comprise virtual machines configured to perform data processing and network data transmission tasks. In other implementations, one or more of data centers <b>480</b>, <b>481</b> may be physically distributed over a plurality of physical sites.</p><p id="p-0099" num="0098">The wireless mobile device <b>376</b> may comprise a smart phone configured to include a digital camera, a digital camera configured to include wireless network connectivity, a reality augmentation device, a laptop configured to include a digital camera and wireless network connectivity, or any other technically feasible computing device configured to include a digital photographic system and wireless network connectivity.</p><p id="p-0100" num="0099">In various embodiments, the wireless access point <b>472</b> may be configured to communicate with wireless mobile device <b>376</b> via the digital radio link <b>471</b> and to communicate with the data network <b>474</b> via any technically feasible transmission media, such as any electrical, optical, or radio transmission media. For example, in one embodiment, wireless access point <b>472</b> may communicate with data network <b>474</b> through an optical fiber coupled to the wireless access point <b>472</b> and to a router system or a switch system within the data network <b>474</b>. A network link <b>475</b>, such as a wide area network (WAN) link, may be configured to transmit data between the data network <b>474</b> and the data center <b>480</b>.</p><p id="p-0101" num="0100">In one embodiment, the data network <b>474</b> may include routers, switches, long-haul transmission systems, provisioning systems, authorization systems, and any technically feasible combination of communications and operations subsystems configured to convey data between network endpoints, such as between the wireless access point <b>472</b> and the data center <b>480</b>. In one implementation scenario, wireless mobile device <b>376</b> may comprise one of a plurality of wireless mobile devices configured to communicate with the data center <b>480</b> via one or more wireless access points coupled to the data network <b>474</b>.</p><p id="p-0102" num="0101">Additionally, in various embodiments, the data center <b>480</b> may include, without limitation, a switch/router <b>482</b> and at least one data service system <b>484</b>. The switch/router <b>482</b> may be configured to forward data traffic between and among a network link <b>475</b>, and each data service system <b>484</b>. The switch/router <b>482</b> may implement any technically feasible transmission techniques, such as Ethernet media layer transmission, layer <b>2</b> switching, layer <b>3</b> routing, and the like. The switch/router <b>482</b> may comprise one or more individual systems configured to transmit data between the data service systems <b>484</b> and the data network <b>474</b>.</p><p id="p-0103" num="0102">In one embodiment, the switch/router <b>482</b> may implement session-level load balancing among a plurality of data service systems <b>484</b>. Each data service system <b>484</b> may include at least one computation system <b>488</b> and may also include one or more storage systems <b>486</b>. Each computation system <b>488</b> may comprise one or more processing units, such as a central processing unit, a graphics processing unit, or any combination thereof. A given data service system <b>484</b> may be implemented as a physical system comprising one or more physically distinct systems configured to operate together. Alternatively, a given data service system <b>484</b> may be implemented as a virtual system comprising one or more virtual systems executing on an arbitrary physical system. In certain scenarios, the data network <b>474</b> may be configured to transmit data between the data center <b>480</b> and another data center <b>481</b>, such as through a network link <b>476</b>.</p><p id="p-0104" num="0103">In another embodiment, the network service system <b>400</b> may include any networked mobile devices configured to implement one or more embodiments of the present invention. For example, in some embodiments, a peer-to-peer network, such as an ad-hoc wireless network, may be established between two different wireless mobile devices. In such embodiments, digital image data may be transmitted between the two wireless mobile devices without having to send the digital image data to a data center <b>480</b>.</p><p id="p-0105" num="0104"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates capturing <b>500</b> an image with correct skin tone exposure, in accordance with one embodiment. As an option, the capturing <b>500</b> may be implemented in the context of any one or more of the embodiments set forth in any previous and/or subsequent figure(s) and/or description thereof. Of course, however, the capturing <b>500</b> may be implemented in the context of any desired environment. Further, the aforementioned definitions may equally apply to the description below.</p><p id="p-0106" num="0105">As shown, image <b>502</b> represents a conventional captured image using standard exposure and capture techniques. As can be seen, portions of image <b>502</b> lack sufficient contrast, making the subject's facial features difficult to distinguish. Image <b>504</b> represents a conventional flash captured image using standard exposure and flash capture technique. As shown in image <b>504</b>, turning on the flash aggravates a lack of contrast, making the subject's facial features even more difficult to distinguish. In contrast, image <b>506</b> represents one or more techniques described herein, wherein such techniques provide improved image quality for portraits of individuals with very dark skin tone or very light skin tone. In particular, image <b>506</b> was captured according to method <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>, where a face was detected in the scene, and the face was determined to have a threshold dark skin tone. Having detected the face with threshold skin tone, the camera was configured to capture an HDR image, where default camera behavior would have captured a non-HDR image. The HDR image was tone-mapped and equalized according to CLAHE techniques to generate image <b>506</b>.</p><p id="p-0107" num="0106"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates capturing <b>600</b> an image with correct skin tone exposure, in accordance with one embodiment. As an option, the capturing <b>600</b> may be implemented in the context of any one or more of the embodiments set forth in any previous and/or subsequent figure(s) and/or description thereof. Of course, however, the capturing <b>600</b> may be implemented in the context of any desired environment. Further, the aforementioned definitions may equally apply to the description below.</p><p id="p-0108" num="0107">As shown, image <b>602</b> represents a conventional captured image at EV+3. As visually apparent and indicated by the accompanying histogram, image <b>602</b> is &#x201c;blown out&#x201d; and generally over-exposed (note the large spike on the far right side of the histogram). However, image <b>602</b> does include useful visual detail in the region of the subject's face, despite such detail appearing incorrectly over exposed. Image <b>604</b> represents a conventional captured image at EV&#x2212;0. As visually apparent, image <b>602</b> balances overall image exposure, the goal of conventional exposure; however, as a consequence, there is insufficient visual detail or contrast captured in the region of the subject's face. The accompanying histogram is centered, with no excessively dark regions and only a narrow spike of excessively bright pixels. As such, image <b>604</b> meets conventional goals of balanced exposure, despite the subject's face lacking sufficient visual detail. Image <b>602</b> and image <b>604</b> both fail to capture important visual detail in different regions, with image <b>604</b> failing to capture visual detail in the region of the subject's face and image <b>602</b> failing to capture visual detail surrounding the subject's face. In contrast, image <b>606</b> provides visual detail while preserving a natural exposure appearance (e.g., correct exposure) in the region of the subjects face, and furthermore provides visual detail in regions surrounding the subject's face. In the histogram accompanying image <b>606</b>, three clusters of pixel values are apparent. A leftmost cluster is associated with the region of the subject's face and some background regions. This cluster has a higher peak and breadth than that of image <b>604</b> or image <b>602</b>, indicating greater detail in this intensity range for image <b>606</b>. Furthermore, a mid-range group associate mostly with surrounding regions has increased in magnitude and breadth, indicating greater detail in the mid range. Note that while there are a significant number of bright pixels (e.g., in the sky region) in the rightmost cluster, few overall pixels are actually saturated (&#x201c;blown out&#x201d;). Beyond the visual superiority of detail in image <b>606</b>, the accompanying histogram objectively illustrates that image <b>606</b> provides more detail in appropriate intensity ranges. Image <b>606</b> was captured using one or more techniques described herein, wherein such techniques provide improved image quality for portraits of individuals with very dark skin tone (or very light skin tone). In particular, image <b>606</b> was captured according to method <b>120</b> of <figref idref="DRAWINGS">FIG. <b>2</b>B</figref>, with a high degree of HDR effect applied, in conjunction with equalization, to pixels within the face region. In this manner, the lighting and visual detail of the captured image is corrected, both with respect to the environment (shown around the face), and the face skin tone of the subject.</p><p id="p-0109" num="0108"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates capturing <b>700</b> an image with correct skin tone exposure, in accordance with one embodiment. As an option, the capturing <b>700</b> may be implemented in the context of any one or more of the embodiments set forth in any previous and/or subsequent figure(s) and/or description thereof. Of course, however, the capturing <b>700</b> may be implemented in the context of any desired environment. Further, the aforementioned definitions may equally apply to the description below.</p><p id="p-0110" num="0109">As shown, image <b>702</b> represents a conventional captured image at EV+3. As is visually apparent, image <b>702</b> is over exposed; furthermore, the accompanying histogram shows a large spike of saturated pixels at the far right. At this exposure level, useful visual detail is captured in the region of the subject's face. However, the subject appears unnaturally bright and over exposed. Image <b>704</b> represents a conventional captured image at EV&#x2212;0. The histogram accompanying image <b>704</b> indicates conventionally proper exposure that would not cause a conventional camera system to select an HDR capture mode. Despite the conventionally proper exposure, the subject's face lacks visual detail. In contrast, image <b>706</b> was captured according to method <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>, where a face was detected in the scene, and the face was determined to have a threshold dark skin tone. Having detected the face with threshold skin tone, the camera was configured to capture an HDR image, where default camera behavior would have captured a non-HDR image. The HDR image was tone-mapped and equalized according to CLAHE techniques to generate image <b>706</b>. In this manner, the lighting of the captured image is corrected, both respect to the environment (shown around the face), and the face skin tone.</p><p id="p-0111" num="0110"><figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates a network architecture <b>800</b>, in accordance with one possible embodiment. As shown, at least one network <b>802</b> is provided. In the context of the present network architecture <b>800</b>, the network <b>802</b> may take any form including, but not limited to a telecommunications network, a local area network (LAN), a wireless network, a wide area network (WAN) such as the Internet, peer-to-peer network, cable network, etc. While only one network is shown, it should be understood that two or more similar or different networks <b>802</b> may be provided.</p><p id="p-0112" num="0111">Coupled to the network <b>802</b> is a plurality of devices. For example, a server computer <b>812</b> and an end user computer <b>808</b> may be coupled to the network <b>802</b> for communication purposes. Such end user computer <b>808</b> may include a desktop computer, lap-top computer, and/or any other type of logic. Still yet, various other devices may be coupled to the network <b>802</b> including a personal digital assistant (PDA) device <b>810</b>, a mobile phone device <b>806</b>, a television <b>804</b>, a camera <b>814</b>, etc.</p><p id="p-0113" num="0112"><figref idref="DRAWINGS">FIG. <b>9</b></figref> illustrates an exemplary system <b>900</b>, in accordance with one embodiment. As an option, the system <b>900</b> may be implemented in the context of any of the devices of the network architecture <b>800</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>. Of course, the system <b>900</b> may be implemented in any desired environment.</p><p id="p-0114" num="0113">As shown, a system <b>900</b> is provided including at least one central processor <b>902</b> which is connected to a communication bus <b>912</b>. The system <b>900</b> also includes main memory <b>904</b> [e.g. random access memory (RAM), etc.]. The system <b>900</b> also includes a graphics processor <b>908</b> and a display <b>910</b>.</p><p id="p-0115" num="0114">The system <b>900</b> may also include a secondary storage <b>906</b>. The secondary storage <b>906</b> includes, for example, a hard disk drive and/or a removable storage drive, representing a floppy disk drive, a magnetic tape drive, a compact disk drive, etc. The removable storage drive reads from and/or writes to a removable storage unit in a well known manner.</p><p id="p-0116" num="0115">Computer programs, or computer control logic algorithms, may be stored in the main memory <b>904</b>, the secondary storage <b>906</b>, and/or any other memory, for that matter. Such computer programs, when executed, enable the system <b>900</b> to perform various functions (as set forth above, for example). Memory <b>904</b>, storage <b>906</b> and/or any other storage are possible examples of non-transitory computer-readable media. In one embodiment, digital photographic system <b>300</b> includes system <b>900</b>.</p><p id="p-0117" num="0116">It is noted that the techniques described herein, in an aspect, are embodied in executable instructions stored in a computer readable medium for use by or in connection with an instruction execution machine, apparatus, or device, such as a computer-based or processor-containing machine, apparatus, or device. It will be appreciated by those skilled in the art that for some embodiments, other types of computer readable media are included which may store data that is accessible by a computer, such as magnetic cassettes, flash memory cards, digital video disks, Bernoulli cartridges, random access memory (RAM), read-only memory (ROM), and the like.</p><p id="p-0118" num="0117">As used here, a &#x201c;computer-readable medium&#x201d; includes one or more of any suitable media for storing the executable instructions of a computer program such that the instruction execution machine, system, apparatus, or device may read (or fetch) the instructions from the computer readable medium and execute the instructions for carrying out the described methods. Suitable storage formats include one or more of an electronic, magnetic, optical, and electromagnetic format. A non-exhaustive list of conventional exemplary computer readable medium includes: a portable computer diskette; a RAM; a ROM; an erasable programmable read only memory (EPROM or flash memory); optical storage devices, including a portable compact disc (CD), a portable digital video disc (DVD), a high definition DVD (HD-DVD&#x2122;), a BLU-RAY disc; and the like.</p><p id="p-0119" num="0118">It should be understood that the arrangement of components illustrated in the Figures described are exemplary and that other arrangements are possible. It should also be understood that the various system components (and means) defined by the claims, described below, and illustrated in the various block diagrams represent logical components in some systems configured according to the subject matter disclosed herein.</p><p id="p-0120" num="0119">For example, one or more of these system components (and means) may be realized, in whole or in part, by at least some of the components illustrated in the arrangements illustrated in the described Figures. In addition, while at least one of these components are implemented at least partially as an electronic hardware component, and therefore constitutes a machine, the other components may be implemented in software that when included in an execution environment constitutes a machine, hardware, or a combination of software and hardware.</p><p id="p-0121" num="0120">More particularly, at least one component defined by the claims is implemented at least partially as an electronic hardware component, such as an instruction execution machine (e.g., a processor-based or processor-containing machine) and/or as specialized circuits or circuitry (e.g., discreet logic gates interconnected to perform a specialized function). Other components may be implemented in software, hardware, or a combination of software and hardware. Moreover, some or all of these other components may be combined, some may be omitted altogether, and additional components may be added while still achieving the functionality described herein. Thus, the subject matter described herein may be embodied in many different variations, and all such variations are contemplated to be within the scope of what is claimed.</p><p id="p-0122" num="0121">In the description above, the subject matter is described with reference to acts and symbolic representations of operations that are performed by one or more devices, unless indicated otherwise. As such, it will be understood that such acts and operations, which are at times referred to as being computer-executed, include the manipulation by the processor of data in a structured form. This manipulation transforms the data or maintains it at locations in the memory system of the computer, which reconfigures or otherwise alters the operation of the device in a manner well understood by those skilled in the art. The data is maintained at physical locations of the memory as data structures that have particular properties defined by the format of the data. However, while the subject matter is being described in the foregoing context, it is not meant to be limiting as those of skill in the art will appreciate that various of the acts and operations described hereinafter may also be implemented in hardware.</p><p id="p-0123" num="0122">To facilitate an understanding of the subject matter described herein, many aspects are described in terms of sequences of actions. At least one of these aspects defined by the claims is performed by an electronic hardware component. For example, it will be recognized that the various actions may be performed by specialized circuits or circuitry, by program instructions being executed by one or more processors, or by a combination of both. The description herein of any sequence of actions is not intended to imply that the specific order described for performing that sequence must be followed. All methods described herein may be performed in any suitable order unless otherwise indicated herein or otherwise clearly contradicted by context.</p><p id="p-0124" num="0123">The use of the terms &#x201c;a&#x201d; and &#x201c;an&#x201d; and &#x201c;the&#x201d; and similar referents in the context of describing the subject matter (particularly in the context of the following claims) are to be construed to cover both the singular and the plural, unless otherwise indicated herein or clearly contradicted by context. Recitation of ranges of values herein are merely intended to serve as a shorthand method of referring individually to each separate value falling within the range, unless otherwise indicated herein, and each separate value is incorporated into the specification as if it were individually recited herein. Furthermore, the foregoing description is for the purpose of illustration only, and not for the purpose of limitation, as the scope of protection sought is defined by the claims as set forth hereinafter together with any equivalents thereof entitled to. The use of any and all examples, or exemplary language (e.g., &#x201c;such as&#x201d;) provided herein, is intended merely to better illustrate the subject matter and does not pose a limitation on the scope of the subject matter unless otherwise claimed. The use of the term &#x201c;based on&#x201d; and other like phrases indicating a condition for bringing about a result, both in the claims and in the written description, is not intended to foreclose any other conditions that bring about that result. No language in the specification should be construed as indicating any non-claimed element as essential to the practice of the invention as claimed.</p><p id="p-0125" num="0124">The embodiments described herein included the one or more modes known to the inventor for carrying out the claimed subject matter. Of course, variations of those embodiments will become apparent to those of ordinary skill in the art upon reading the foregoing description. The inventor expects skilled artisans to employ such variations as appropriate, and the inventor intends for the claimed subject matter to be practiced otherwise than as specifically described herein. Accordingly, this claimed subject matter includes all modifications and equivalents of the subject matter recited in the claims appended hereto as permitted by applicable law. Moreover, any combination of the above-described elements in all possible variations thereof is encompassed unless otherwise indicated herein or otherwise clearly contradicted by context.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A device, comprising:<claim-text>a non-transitory memory storing instructions;</claim-text><claim-text>a camera module; and</claim-text><claim-text>one or more processors in communication with the non-transitory memory and the camera module, wherein the one or more processors execute the instructions to:<claim-text>detect one or more faces having threshold skin tone within a scene;</claim-text><claim-text>based on the detected one or more faces, enable a high dynamic range (HDR) capture mode; and</claim-text><claim-text>capture the scene image using the HDR capture mode.</claim-text></claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the threshold skin tone is detected for one or more regions of the one or more faces detected.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The device of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the threshold skin tone is a medium natural human skin color.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The device of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the threshold skin tone is a dark natural human skin color.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The device of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the threshold skin tone is a light natural human skin color.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The device of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the threshold skin tone is a predefined average intensity or histogram median.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The device of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the predefined average intensity or histogram median is based on one or more face regions associated with the one or more faces.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the device is operable such that the capturing includes performing local equalization to generate the scene image.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The device of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the local equalization comprises contrast limited adaptive histogram equalization.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the device is operable to further execute the instructions to correct a color of light associated with the skin tone associated with each of the one or more faces based on a measurement of an ambient lighting color balance.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the device is operable such that the capturing includes enabling a flash illuminator.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the device is operable such that the capturing is of a video stream.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the device is operable such that the capturing includes performing tone mapping to generate the scene image.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The device of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the tone mapping is performed on pixels affiliated with exposed skin.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein contrast is adjusted is performed on pixels associated with exposed skin.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one or more processors execute the instructions to determine a separate exposure value for each of the one or more faces.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one or more processors execute the instructions to determine a first exposure for at least one of the one or more faces, and a second exposure for the scene which does not contain the one or more faces.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, the one or more processors execute the instructions to process each of the one or more faces to determine a face region, a transition region, and a non face region.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. A non-transitory computer-readable medium storing one or more programs containing instructions that, when executed by one or more processors of a device, cause the device to:<claim-text>detect one or more faces having threshold skin tone within a scene;</claim-text><claim-text>based on the detected one or more faces, enable a high dynamic range (HDR) capture mode; and</claim-text><claim-text>capture the scene image using the HDR capture mode.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. A method, comprising:<claim-text>detecting, using an image processor, one or more faces having threshold skin tone within a scene;</claim-text><claim-text>based on the detected one or more faces, enabling, using the image processor, a high dynamic range (HDR) capture mode; and</claim-text><claim-text>capturing, using the image processor, the scene image using the HDR capture mode.</claim-text></claim-text></claim></claims></us-patent-application>