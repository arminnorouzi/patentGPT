<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230000508A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230000508</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17780440</doc-number><date>20201120</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>17</main-group><subgroup>17</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>34</main-group><subgroup>10</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20161101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>17</main-group><subgroup>1778</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>17</main-group><subgroup>1703</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20160201</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>34</main-group><subgroup>10</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>17</main-group><subgroup>8897</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">TARGETING TOOL FOR VIRTUAL SURGICAL GUIDANCE</invention-title><us-related-documents><us-provisional-application><document-id><country>US</country><doc-number>62942984</doc-number><date>20191203</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Howmedica Osteonics Corp.</orgname><address><city>Mahwah</city><state>NJ</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Poltaretskyi</last-name><first-name>Sergii</first-name><address><city>Ependes FR</city><country>CH</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Boileau</last-name><first-name>Pascal</first-name><address><city>Nice</city><country>FR</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>Howmedica Osteonics Corp.</orgname><role>02</role><address><city>Mahwah</city><state>NJ</state><country>US</country></address></addressbook></assignee></assignees><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/US2020/061585</doc-number><date>20201120</date></document-id><us-371c12-date><date>20220526</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">An example physical targeting tool includes a main body defining a channel configured to receive a tool, the channel having a longitudinal axis; a first physical targeting feature attached to the main body; and a second physical targeting feature attached to the main body, wherein the first physical targeting feature and the second physical targeting feature are displaced along the longitudinal axis of the channel.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="203.03mm" wi="154.43mm" file="US20230000508A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="214.29mm" wi="171.28mm" file="US20230000508A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="175.43mm" wi="174.24mm" file="US20230000508A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="210.31mm" wi="145.88mm" file="US20230000508A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="238.08mm" wi="173.65mm" file="US20230000508A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="168.23mm" wi="171.70mm" file="US20230000508A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="221.57mm" wi="169.33mm" file="US20230000508A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="237.32mm" wi="160.10mm" file="US20230000508A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="183.90mm" wi="142.16mm" orientation="landscape" file="US20230000508A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="174.41mm" wi="143.17mm" orientation="landscape" file="US20230000508A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="190.50mm" wi="158.50mm" file="US20230000508A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="220.81mm" wi="138.94mm" orientation="landscape" file="US20230000508A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="206.08mm" wi="161.54mm" file="US20230000508A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="214.12mm" wi="162.73mm" file="US20230000508A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="191.01mm" wi="161.54mm" orientation="landscape" file="US20230000508A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="172.80mm" wi="134.96mm" orientation="landscape" file="US20230000508A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="191.26mm" wi="157.06mm" orientation="landscape" file="US20230000508A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="110.41mm" wi="132.50mm" file="US20230000508A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><p id="p-0002" num="0001">This patent application claims the benefit of U.S. Provisional Patent Application No. 62/942,984, filed Dec. 3, 2019, the entirety of which is incorporated by reference herein.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">BACKGROUND</heading><p id="p-0003" num="0002">Surgical joint repair procedures involve repair and/or replacement of a damaged or diseased joint. Many times, a surgical joint repair procedure, such as joint arthroplasty as an example, involves replacing the damaged joint with a prosthetic, or set of prosthetics, that is implanted into the patient's bone. To assist with positioning, the surgical procedure often involves the use of surgical instruments to control the shaping of the surface of the damaged bone and cutting or drilling of bone to accept the prosthetic.</p><heading id="h-0002" level="1">SUMMARY</heading><p id="p-0004" num="0003">In some orthopedic surgical procedures, a surgeon may implant one or more implant devices in a patient. The surgeon may perform various surgical steps to prepare the patient's bone to receive the implant device. These surgical steps may include insertion of guide pins, modifications to a surface of the bone (e.g., via reaming), removal of portions of the bone (e.g., resection), creating anchorage points, or other surgical steps.</p><p id="p-0005" num="0004">A visualization device may display virtual guidance that assists a surgeon in performing one or more of the surgical steps to prepare the patient's bone to receive the implant device. For instance, the visualization device may display a virtual axis to indicate a physical axis along which the surgeon is to position a rotating tool (e.g., a drill bit, a screw, a self-tapping guide pin, or other rotating tool) in order to perform a surgical step. The surgeon may achieve correct performance of the surgical step by aligning a shaft of the rotating tool with the displayed virtual axis, activating a driver of the rotating tool, and advancing the shaft of the rotating tool along the displayed virtual axis. However, in some scenarios, various tools used by the surgeon to perform the surgical step may obscure a portion of the virtual axis or otherwise interfere with the display of the virtual axis. For instance, from the surgeon's perspective, the driver of the rotating tool may obscure a portion of the virtual axis.</p><p id="p-0006" num="0005">In accordance with one or more techniques of this disclosure, a visualization device may display virtual guidance that guides performance of a surgical step with the use of a physical targeting tool that attaches to a tool. The physical targeting tool may include a channel configured to receive a tool, and physical targeting features that are laterally offset from the channel. Where the tool is a rotating tool the channel may be configured to receive a shaft of the rotating tool. The visualization device may display the virtual guidance such that the surgeon may achieve correct performance of the surgical step by aligning one or more physical targeting features with the displayed virtual guidance. As the physical targeting features are laterally offset from the channel, the physical targeting features may not be obscured by the driver of the tool. Accordingly, the physical targeting features may aid the surgeon and can be a visible proxy to the actual channel. Moreover, virtual targeting features can be displayed relative to the physical targeting features, and virtual targeting features can be aligned with the physical targeting features so as to deliver non-obstructed views of such virtual targeting features. In this way, the techniques of this disclosure enable a surgeon to utilize tools, and associated drivers, to perform surgical steps with the assistance of virtual guidance.</p><p id="p-0007" num="0006">The details of various examples of the disclosure are set forth in the accompanying drawings and the description below. Various features, objects, and advantages will be apparent from the description, drawings, and claims.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram of an orthopedic surgical system according to an example of this disclosure.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram of an orthopedic surgical system that includes a mixed reality (MR) system, according to an example of this disclosure.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flowchart illustrating example phases of a surgical lifecycle.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flowchart illustrating preoperative, intraoperative and postoperative workflows in support of an orthopedic surgical procedure.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a schematic representation of a visualization device for use in a mixed reality (MR) system, according to an example of this disclosure.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a block diagram illustrating example components of a visualization device for use in a mixed reality (MR) system, according to an example of this disclosure.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIGS. <b>7</b> and <b>8</b></figref> are conceptual diagrams illustrating an MR system providing virtual guidance for installation of a guide pin in a bone, in accordance with one or more techniques of this disclosure.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a conceptual diagram of virtual guidance that may be provided by an MR system.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a conceptual diagram of tools obscuring a portion of virtual guidance provided by an MR system.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIGS. <b>11</b>A-<b>11</b>C</figref> illustrate various views of one example of a physical targeting tool, in accordance with one or more techniques of this disclosure.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>12</b>A and <b>12</b>B</figref> are conceptual diagrams illustrating an example physical targeting tool being attached to a tool, in accordance with one or more techniques of this disclosure.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a conceptual diagram of virtual guidance that may be provided by an MR system to guide a surgeon performing a surgical step using a physical targeting tool, in accordance with one or more techniques of this disclosure.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a conceptual diagram of virtual guidance that may be provided by an MR system to guide a surgeon performing a surgical step using a physical targeting tool, in accordance with one or more techniques of this disclosure.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a conceptual diagram of virtual guidance that may be provided by an MR system to guide a surgeon performing a surgical step using a physical targeting tool, in accordance with one or more techniques of this disclosure.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>16</b></figref> is a flowchart illustrating example techniques for providing virtual guidance to guide a surgeon performing a surgical step using a physical targeting tool, in accordance with one or more techniques of this disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0004" level="1">DETAILED DESCRIPTION</heading><p id="p-0023" num="0022">Orthopedic surgery can involve implanting one or more implant devices to repair or replace a patient's damaged or diseased joint. Virtual surgical planning tools that use image data of the diseased or damaged joint may be used to generate an accurate three-dimensional bone model that can be viewed and manipulated preoperatively by the surgeon. These tools can enhance surgical outcomes by allowing the surgeon to simulate the surgery, select or design an implant that more closely matches the contours of the patient's actual bone, and select or design surgical instruments and guide tools that are adapted specifically for repairing the bone of a particular patient. Use of these planning tools typically results in generation of a preoperative surgical plan, complete with an implant and surgical instruments that are selected or manufactured for the individual patient.</p><p id="p-0024" num="0023">A surgeon may want to view details of the preoperative surgical plan relative to the patient's real bone during the actual procedure in order to more efficiently and accurately position and orient the implant components. For example, the surgeon may want to obtain intraoperative visualization that provides guidance for positioning and orientation of implant components, guidance for preparation of bone or tissue to receive the implant components, guidance for reviewing the details of a procedure or procedural step, and/or guidance for selection of tools or implants and tracking of surgical procedure workflow.</p><p id="p-0025" num="0024">Accordingly, this disclosure describes systems and methods for using a mixed reality (MR) visualization system to assist with creation, implementation, verification, and/or modification of a surgical plan before and during a surgical procedure. Because MR may be used to interact with the surgical plan, this disclosure may also refer to the surgical plan as a &#x201c;virtual&#x201d; surgical plan. Visualization tools other than or in addition to mixed reality visualization systems may be used in accordance with techniques of this disclosure. A surgical plan, e.g., as generated by the BLUEPRINT&#x2122; system, available from Wright Medical Group, N.V., or another surgical planning platform, may include information defining a variety of features of a surgical procedure, such as features of particular surgical procedure steps to be performed on a patient by a surgeon according to the surgical plan including, for example, bone or tissue preparation steps and/or steps for selection, modification and/or placement of implant components. Such information may include, in various examples, dimensions, shapes, angles, surface contours, and/or orientations of implant components to be selected or modified by surgeons, dimensions, shapes, angles, surface contours and/or orientations to be defined in bone or tissue by the surgeon in bone or tissue preparation steps, and/or positions, axes, planes, angle and/or entry points defining placement of implant components by the surgeon relative to patient bone or tissue. Information such as dimensions, shapes, angles, surface contours, and/or orientations of anatomical features of the patient may be derived from imaging (e.g., x-ray, CT, MRI, ultrasound or other images), direct observation, or other techniques.</p><p id="p-0026" num="0025">In this disclosure, the term &#x201c;mixed reality&#x201d; (MR) refers to the presentation of virtual objects such that a user sees images that include both real, physical objects and virtual objects. Virtual objects may include text, 2-dimensional surfaces, 3-dimensional models, or other user-perceptible elements that are not actually present in the physical, real-world environment in which they are presented as coexisting. In addition, virtual objects described in various examples of this disclosure may include graphics, images, animations or videos, e.g., presented as 3D virtual objects or 2D virtual objects. Virtual objects may also be referred to as virtual elements. Such elements may or may not be analogs of real-world objects. In some examples, in mixed reality, a camera may capture images of the real world and modify the images to present virtual objects in the context of the real world. In such examples, the modified images may be displayed on a screen, which may be head-mounted, handheld, or otherwise viewable by a user. This type of mixed reality is increasingly common on smartphones, such as where a user can point a smartphone's camera at a sign written in a foreign language and see in the smartphone's screen a translation in the user's own language of the sign superimposed on the sign along with the rest of the scene captured by the camera. In some examples, in mixed reality, see-through (e.g., transparent) holographic lenses, which may be referred to as waveguides, may permit the user to view real-world objects, i.e., actual objects in a real-world environment, such as real anatomy, through the holographic lenses and also concurrently view virtual objects.</p><p id="p-0027" num="0026">The Microsoft HOLOLENS&#x2122; headset, available from Microsoft Corporation of Redmond, Washington, is an example of a MR device that includes see-through holographic lenses, sometimes referred to as waveguides, that permit a user to view real-world objects through the lens and concurrently view projected 3D holographic objects. The Microsoft HOLOLENS&#x2122; headset, or similar waveguide-based visualization devices, are examples of an MR visualization device that may be used in accordance with some examples of this disclosure. Some holographic lenses may present holographic objects with some degree of transparency through see-through holographic lenses so that the user views real-world objects and virtual, holographic objects. In some examples, some holographic lenses may, at times, completely prevent the user from viewing real-world objects and instead may allow the user to view entirely virtual environments. The term mixed reality may also encompass scenarios where one or more users are able to perceive one or more virtual objects generated by holographic projection. In other words, &#x201c;mixed reality&#x201d; may encompass the case where a holographic projector generates holograms of elements that appear to a user to be present in the user's actual physical environment.</p><p id="p-0028" num="0027">In some examples, in mixed reality, the positions of some or all presented virtual objects are related to positions of physical objects in the real world. For example, a virtual object may be tethered to a table in the real world, such that the user can see the virtual object when the user looks in the direction of the table but does not see the virtual object when the table is not in the user's field of view. In some examples, in mixed reality, the positions of some or all presented virtual objects are unrelated to positions of physical objects in the real world. For instance, a virtual item may always appear in the top right of the user's field of vision, regardless of where the user is looking.</p><p id="p-0029" num="0028">Augmented reality (AR) is similar to MR in the presentation of both real-world and virtual elements, but AR generally refers to presentations that are mostly real, with a few virtual additions to &#x201c;augment&#x201d; the real-world presentation. For purposes of this disclosure, MR is considered to include AR. For example, in AR, parts of the user's physical environment that are in shadow can be selectively brightened without brightening other areas of the user's physical environment. This example is also an instance of MR in that the selectively-brightened areas may be considered virtual objects superimposed on the parts of the user's physical environment that are in shadow.</p><p id="p-0030" num="0029">Furthermore, in this disclosure, the term &#x201c;virtual reality&#x201d; (VR) refers to an immersive artificial environment that a user experiences through sensory stimuli (such as sights and sounds) provided by a computer. Thus, in virtual reality, the user may not see any physical objects as they exist in the real world. Video games set in imaginary worlds are a common example of VR. The term &#x201c;VR&#x201d; also encompasses scenarios where the user is presented with a fully artificial environment in which some virtual object's locations are based on the locations of corresponding physical objects as they relate to the user. Walk-through VR attractions are examples of this type of VR.</p><p id="p-0031" num="0030">The term &#x201c;extended reality&#x201d; (XR) is a term that encompasses a spectrum of user experiences that includes virtual reality, mixed reality, augmented reality, and other user experiences that involve the presentation of at least some perceptible elements as existing in the user's environment that are not present in the user's real-world environment. Thus, the term &#x201c;extended reality&#x201d; may be considered a genus for MR and VR. XR visualizations may be presented in any of the techniques for presenting mixed reality discussed elsewhere in this disclosure or presented using techniques for presenting VR, such as VR goggles.</p><p id="p-0032" num="0031">Visualization tools may utilize patient image data to generate three-dimensional models of bone contours to facilitate preoperative planning for joint repairs and replacements. These tools allow surgeons to design and/or select surgical guides and implant components that closely match the patient's anatomy. These tools can improve surgical outcomes by customizing a surgical plan for each patient. An example of such a visualization tool for shoulder repairs is the BLUEPRINT&#x2122; system available from Wright Medical Group, N.V. The BLUEPRINT&#x2122; system provides the surgeon with two-dimensional planar views of the bone repair region as well as a three-dimensional virtual model of the repair region. The surgeon can use the BLUEPRINT&#x2122; system to select, design or modify appropriate implant components, determine how best to position and orient the implant components and how to shape the surface of the bone to receive the components, and design, select or modify surgical guide tool(s) or instruments to carry out the surgical plan. The information generated by the BLUEPRINT&#x2122; system is compiled in a preoperative surgical plan for the patient that is stored in a database at an appropriate location (e.g., on a server in a wide area network, a local area network, or a global network) where it can be accessed by the surgeon or other care provider, including before and during the actual surgery.</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram of an orthopedic surgical system <b>100</b> according to an example of this disclosure. Orthopedic surgical system <b>100</b> includes a set of subsystems. In the example of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the subsystems include a virtual planning system <b>102</b>, a planning support system <b>104</b>, a manufacturing and delivery system <b>106</b>, an intraoperative guidance system <b>108</b>, a medical education system <b>110</b>, a monitoring system <b>112</b>, a predictive analytics system <b>114</b>, and a communications network <b>116</b>. In other examples, orthopedic surgical system <b>100</b> may include more, fewer, or different subsystems. For example, orthopedic surgical system <b>100</b> may omit medical education system <b>110</b>, monitoring system <b>112</b>, predictive analytics system <b>114</b>, and/or other subsystems. In some examples, orthopedic surgical system <b>100</b> may be used for surgical tracking, in which case orthopedic surgical system <b>100</b> may be referred to as a surgical tracking system. In other cases, orthopedic surgical system <b>100</b> may be generally referred to as a medical device system.</p><p id="p-0034" num="0033">Users of orthopedic surgical system <b>100</b> may use virtual planning system <b>102</b> to plan orthopedic surgeries. Users of orthopedic surgical system <b>100</b> may use planning support system <b>104</b> to review surgical plans generated using orthopedic surgical system <b>100</b>. Manufacturing and delivery system <b>106</b> may assist with the manufacture and delivery of items needed to perform orthopedic surgeries. Intraoperative guidance system <b>108</b> provides guidance to assist users of orthopedic surgical system <b>100</b> in performing orthopedic surgeries. Medical education system <b>110</b> may assist with the education of users, such as healthcare professionals, patients, and other types of individuals. Pre- and postoperative monitoring system <b>112</b> may assist with monitoring patients before and after the patients undergo surgery. Predictive analytics system <b>114</b> may assist healthcare professionals with various types of predictions. For example, predictive analytics system <b>114</b> may apply artificial intelligence techniques to determine a classification of a condition of an orthopedic joint, e.g., a diagnosis, determine which type of surgery to perform on a patient and/or which type of implant to be used in the procedure, determine types of items that may be needed during the surgery, and so on.</p><p id="p-0035" num="0034">The subsystems of orthopedic surgical system <b>100</b> (i.e., virtual planning system <b>102</b>, planning support system <b>104</b>, manufacturing and delivery system <b>106</b>, intraoperative guidance system <b>108</b>, medical education system <b>110</b>, pre- and postoperative monitoring system <b>112</b>, and predictive analytics system <b>114</b>) may include various systems. The systems in the subsystems of orthopedic surgical system <b>100</b> may include various types of computing systems, computing devices, including server computers, personal computers, tablet computers, smartphones, display devices, Internet of Things (IoT) devices, visualization devices (e.g., mixed reality (MR) visualization devices, virtual reality (VR) visualization devices, holographic projectors, or other devices for presenting extended reality (XR) visualizations), surgical tools, and so on. A holographic projector, in some examples, may project a hologram for general viewing by multiple users or a single user without a headset, rather than viewing only by a user wearing a headset. For example, virtual planning system <b>102</b> may include a MR visualization device and one or more server devices, planning support system <b>104</b> may include one or more personal computers and one or more server devices, and so on. A computing system is a set of one or more computing systems configured to operate as a system. In some examples, one or more devices may be shared between two or more of the subsystems of orthopedic surgical system <b>100</b>. For instance, in the previous examples, virtual planning system <b>102</b> and planning support system <b>104</b> may include the same server devices.</p><p id="p-0036" num="0035">In the example of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the devices included in the subsystems of orthopedic surgical system <b>100</b> may communicate using communications network <b>116</b>. Communications network <b>116</b> may include various types of communication networks including one or more wide-area networks, such as the Internet, local area networks, and so on. In some examples, communications network <b>116</b> may include wired and/or wireless communication links.</p><p id="p-0037" num="0036">Many variations of orthopedic surgical system <b>100</b> are possible in accordance with techniques of this disclosure. Such variations may include more or fewer subsystems than the version of orthopedic surgical system <b>100</b> shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. For example, <figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram of an orthopedic surgical system <b>200</b> that includes one or more mixed reality (MR) systems, according to an example of this disclosure. Orthopedic surgical system <b>200</b> may be used for creating, verifying, updating, modifying and/or implementing a surgical plan. In some examples, the surgical plan can be created preoperatively, such as by using a virtual surgical planning system (e.g., the BLUEPRINT&#x2122; system), and then verified, modified, updated, and viewed intraoperatively, e.g., using MR visualization of the surgical plan. In other examples, orthopedic surgical system <b>200</b> can be used to create the surgical plan immediately prior to surgery or intraoperatively, as needed. In some examples, orthopedic surgical system <b>200</b> may be used for surgical tracking, in which case orthopedic surgical system <b>200</b> may be referred to as a surgical tracking system. In other cases, orthopedic surgical system <b>200</b> may be generally referred to as a medical device system.</p><p id="p-0038" num="0037">In the example of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, orthopedic surgical system <b>200</b> includes a preoperative surgical planning system <b>202</b>, a healthcare facility <b>204</b> (e.g., a surgical center or hospital), a storage system <b>206</b>, and a network <b>208</b> that allows a user at healthcare facility <b>204</b> to access stored patient information, such as medical history, image data corresponding to the damaged joint or bone and various parameters corresponding to a surgical plan that has been created preoperatively (as examples). Preoperative surgical planning system <b>202</b> may be equivalent to virtual planning system <b>102</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> and, in some examples, may generally correspond to a virtual planning system similar or identical to the BLUEPRINT&#x2122; system.</p><p id="p-0039" num="0038">In the example of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, healthcare facility <b>204</b> includes a mixed reality (MR) system <b>212</b>. In some examples of this disclosure, MR system <b>212</b> includes one or more processing device(s) (P) <b>210</b> to provide functionalities that will be described in further detail below. Processing device(s) <b>210</b> may also be referred to as processor(s). In addition, one or more users of MR system <b>212</b> (e.g., a surgeon, nurse, or other care provider) can use processing device(s) (P) <b>210</b> to generate a request for a particular surgical plan or other patient information that is transmitted to storage system <b>206</b> via network <b>208</b>. In response, storage system <b>206</b> returns the requested patient information to MR system <b>212</b>. In some examples, the users can use other processing device(s) to request and receive information, such as one or more processing devices that are part of MR system <b>212</b>, but not part of any visualization device, or one or more processing devices that are part of a visualization device (e.g., visualization device <b>213</b>) of MR system <b>212</b>, or a combination of one or more processing devices that are part of MR system <b>212</b>, but not part of any visualization device, and one or more processing devices that are part of a visualization device (e.g., visualization device <b>213</b>) that is part of MR system <b>212</b>.</p><p id="p-0040" num="0039">In some examples, multiple users can simultaneously use MR system <b>212</b>. For example, MR system <b>212</b> can be used in a spectator mode in which multiple users each use their own visualization devices so that the users can view the same information at the same time and from the same point of view. In some examples, MR system <b>212</b> may be used in a mode in which multiple users each use their own visualization devices so that the users can view the same information from different points of view. Different users may be located locally or remotely relative to one another, while interacting within MR system <b>212</b>. If one or more users are remote, then those remote users may view similar virtual information to that of other local users while viewing different real-world views than the local users.</p><p id="p-0041" num="0040">In some examples, processing device(s) <b>210</b> can provide a user interface to display data and receive input from users at healthcare facility <b>204</b>. Processing device(s) <b>210</b> may be configured to control visualization device <b>213</b> to present a user interface. Furthermore, processing device(s) <b>210</b> may be configured to control visualization device <b>213</b> to present virtual images, such as 3D virtual models, 2D images, and so on. Processing device(s) <b>210</b> can include a variety of different processing or computing devices, such as servers, desktop computers, laptop computers, tablets, mobile phones and other electronic computing devices, or processors within such devices. In some examples, one or more of processing device(s) <b>210</b> can be located remote from healthcare facility <b>204</b>. In some examples, processing device(s) <b>210</b> reside within visualization device <b>213</b>. In some examples, at least one of processing device(s) <b>210</b> is external to visualization device <b>213</b>. In some examples, one or more processing device(s) <b>210</b> reside within visualization device <b>213</b> and one or more of processing device(s) <b>210</b> are external to visualization device <b>213</b>.</p><p id="p-0042" num="0041">In the example of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, MR system <b>212</b> also includes one or more memory or storage device(s) (M) <b>215</b> for storing data and instructions of software that can be executed by processing device(s) <b>210</b>. The instructions of software can correspond to the functionality of MR system <b>212</b> described herein. In some examples, the functionalities of a virtual surgical planning application, such as the BLUEPRINT&#x2122; system, can also be stored and executed by processing device(s) <b>210</b> in conjunction with memory storage device(s) (M) <b>215</b>. For instance, memory or storage system <b>215</b> may be configured to store data corresponding to at least a portion of a virtual surgical plan. In some examples, storage system <b>206</b> may be configured to store data corresponding to at least a portion of a virtual surgical plan. In some examples, memory or storage device(s) (M) <b>215</b> reside within visualization device <b>213</b>. In some examples, memory or storage device(s) (M) <b>215</b> are external to visualization device <b>213</b>. In some examples, memory or storage device(s) (M) <b>215</b> include a combination of one or more memory or storage devices within visualization device <b>213</b> and one or more memory or storage devices external to the visualization device.</p><p id="p-0043" num="0042">Network <b>208</b> may be equivalent to network <b>116</b>. Network <b>208</b> can include one or more wide area networks, local area networks, and/or global networks (e.g., the Internet) that connect preoperative surgical planning system <b>202</b> and MR system <b>212</b> to storage system <b>206</b>. Storage system <b>206</b> can include one or more databases that can contain patient information, medical information, patient image data, and parameters that define the surgical plans. For example, medical images of the patient's diseased or damaged bone typically are generated preoperatively in preparation for an orthopedic surgical procedure. The medical images can include images of the relevant bone(s) taken along the sagittal plane and the coronal plane of the patient's body. The medical images can include X-ray images, magnetic resonance imaging (MRI) images, computerized tomography (CT) images, ultrasound images, and/or any other type of 2D or 3D image that provides information about the relevant surgical area. Storage system <b>206</b> also can include data identifying the implant components selected for a particular patient (e.g., type, size, etc.), surgical guides selected for a particular patient, and details of the surgical procedure, such as entry points, cutting planes, drilling axes, reaming depths, etc. Storage system <b>206</b> can be a cloud-based storage system (as shown) or can be located at healthcare facility <b>204</b> or at the location of preoperative surgical planning system <b>202</b> or can be part of MR system <b>212</b> or visualization device (VD) <b>213</b>, as examples.</p><p id="p-0044" num="0043">MR system <b>212</b> can be used by a surgeon before (e.g., preoperatively) or during the surgical procedure (e.g., intraoperatively) to create, review, verify, update, modify and/or implement a surgical plan. In some examples, MR system <b>212</b> may also be used after the surgical procedure (e.g., postoperatively) to review the results of the surgical procedure, assess whether revisions are required, or perform other postoperative tasks. To that end, MR system <b>212</b> may include a visualization device <b>213</b> that may be worn by the surgeon and (as will be explained in further detail below) is operable to display a variety of types of information, including a 3D virtual image of the patient's diseased, damaged, or postsurgical joint and details of the surgical plan, such as a 3D virtual image of the prosthetic implant components selected for the surgical plan, 3D virtual images of entry points for positioning the prosthetic components, alignment axes and cutting planes for aligning cutting or reaming tools to shape the bone surfaces, or drilling tools to define one or more holes in the bone surfaces, in the surgical procedure to properly orient and position the prosthetic components, surgical guides and instruments and their placement on the damaged joint, and any other information that may be useful to the surgeon to implement the surgical plan. MR system <b>212</b> can generate images of this information that are perceptible to the user of the visualization device <b>213</b> before and/or during the surgical procedure.</p><p id="p-0045" num="0044">In some examples, MR system <b>212</b> includes multiple visualization devices (e.g., multiple instances of visualization device <b>213</b>) so that multiple users can simultaneously see the same images and share the same 3D scene. In some such examples, one of the visualization devices can be designated as the master device and the other visualization devices can be designated as observers or spectators. Any observer device can be re-designated as the master device at any time, as may be desired by the users of MR system <b>212</b>.</p><p id="p-0046" num="0045">In this way, <figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates a surgical planning system that includes a preoperative surgical planning system <b>202</b> to generate a virtual surgical plan customized to repair an anatomy of interest of a particular patient. For example, the virtual surgical plan may include a plan for an orthopedic joint repair surgical procedure, such as one of a standard total shoulder arthroplasty or a reverse shoulder arthroplasty. In this example, details of the virtual surgical plan may include details relating to at least one of preparation of glenoid bone or preparation of humeral bone. In some examples, the orthopedic joint repair surgical procedure is one of a stemless standard total shoulder arthroplasty, a stemmed standard total shoulder arthroplasty, a stemless reverse shoulder arthroplasty, a stemmed reverse shoulder arthroplasty, an augmented glenoid standard total shoulder arthroplasty, and an augmented glenoid reverse shoulder arthroplasty.</p><p id="p-0047" num="0046">The virtual surgical plan may include a 3D virtual model corresponding to the anatomy of interest of the particular patient and a 3D model of a prosthetic component matched to the particular patient to repair the anatomy of interest or selected to repair the anatomy of interest. Furthermore, in the example of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the surgical planning system includes a storage system <b>206</b> to store data corresponding to the virtual surgical plan. The surgical planning system of <figref idref="DRAWINGS">FIG. <b>2</b></figref> also includes MR system <b>212</b>, which may comprise visualization device <b>213</b>. In some examples, visualization device <b>213</b> is wearable by a user. In some examples, visualization device <b>213</b> is held by a user, or rests on a surface in a place accessible to the user. MR system <b>212</b> may be configured to present a user interface via visualization device <b>213</b>. The user interface is visually perceptible to the user using visualization device <b>213</b>. For instance, in one example, a screen of visualization device <b>213</b> may display real-world images and the user interface on a screen. In some examples, visualization device <b>213</b> may project virtual, holographic images onto see-through holographic lenses and also permit a user to see real-world objects of a real-world environment through the lenses. In other words, visualization device <b>213</b> may comprise one or more see-through holographic lenses and one or more display devices that present imagery to the user via the holographic lenses to present the user interface to the user.</p><p id="p-0048" num="0047">In some examples, visualization device <b>213</b> is configured such that the user can manipulate the user interface (which is visually perceptible to the user when the user is wearing or otherwise using visualization device <b>213</b>) to request and view details of the virtual surgical plan for the particular patient, including a 3D virtual model of the anatomy of interest (e.g., a 3D virtual bone of the anatomy of interest) and a 3D model of the prosthetic component selected to repair an anatomy of interest. In some such examples, visualization device <b>213</b> is configured such that the user can manipulate the user interface so that the user can view the virtual surgical plan intraoperatively, including (at least in some examples) the 3D virtual model of the anatomy of interest (e.g., a 3D virtual bone of the anatomy of interest). In some examples, MR system <b>212</b> can be operated in an augmented surgery mode in which the user can manipulate the user interface intraoperatively so that the user can visually perceive details of the virtual surgical plan projected in a real environment, e.g., on a real anatomy of interest of the particular patient. In this disclosure, the terms real and real world may be used in a similar manner. For example, MR system <b>212</b> may present one or more virtual objects that provide guidance for preparation of a bone surface and placement of a prosthetic implant on the bone surface. Visualization device <b>213</b> may present one or more virtual objects in a manner in which the virtual objects appear to be overlaid on an actual, real anatomical object of the patient, within a real-world environment, e.g., by displaying the virtual object(s) with actual, real-world patient anatomy viewed by the user through holographic lenses. For example, the virtual objects may be 3D virtual objects that appear to reside within the real-world environment with the actual, real anatomical object.</p><p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flowchart illustrating example phases of a surgical lifecycle <b>300</b>. In the example of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, surgical lifecycle <b>300</b> begins with a preoperative phase (<b>302</b>). During the preoperative phase, a surgical plan is developed. The preoperative phase may be followed by a manufacturing and delivery phase (<b>304</b>). During the manufacturing and delivery phase, patient-specific items, such as parts and equipment, needed for executing the surgical plan are manufactured and delivered to a surgical site. For instance, a patient specific implant may be manufactured based on a design generated during the preoperative phase. An intraoperative phase follows the manufacturing and delivery phase (<b>306</b>). The surgical plan is executed during the intraoperative phase. In other words, one or more persons perform the surgery on the patient during the intraoperative phase. The intraoperative phase is followed by the postoperative phase (<b>308</b>). The postoperative phase includes activities occurring after the surgical plan is complete. For example, the patient may be monitored during the postoperative phase for complications.</p><p id="p-0050" num="0049">As described in this disclosure, orthopedic surgical system <b>100</b> (<figref idref="DRAWINGS">FIG. <b>1</b></figref>) may be used in one or more of preoperative phase <b>302</b>, the manufacturing and delivery phase <b>304</b>, the intraoperative phase <b>306</b>, and the postoperative phase <b>308</b>. For example, virtual planning system <b>102</b> and planning support system <b>104</b> may be used in preoperative phase <b>302</b>. Manufacturing and delivery system <b>106</b> may be used in the manufacturing and delivery phase <b>304</b>. Intraoperative guidance system <b>108</b> may be used in intraoperative phase <b>306</b>. Some of the systems of <figref idref="DRAWINGS">FIG. <b>1</b></figref> may be used in multiple phases of <figref idref="DRAWINGS">FIG. <b>3</b></figref>. For example, medical education system <b>110</b> may be used in one or more of preoperative phase <b>302</b>, intraoperative phase <b>306</b>, and postoperative phase <b>308</b>; pre- and postoperative monitoring system <b>112</b> may be used in preoperative phase <b>302</b> and postoperative phase <b>308</b>. Predictive analytics system <b>114</b> may be used in preoperative phase <b>302</b> and postoperative phase <b>308</b>.</p><p id="p-0051" num="0050">Various workflows may exist within the surgical process of <figref idref="DRAWINGS">FIG. <b>3</b></figref>. For example, different workflows within the surgical process of <figref idref="DRAWINGS">FIG. <b>3</b></figref> may be appropriate for different types of surgeries. <figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flowchart illustrating preoperative, intraoperative and postoperative workflows in support of an orthopedic surgical procedure. In the example of <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the surgical process begins with a medical consultation (<b>400</b>). During the medical consultation (<b>400</b>), a healthcare professional evaluates a medical condition of a patient. For instance, the healthcare professional may consult the patient with respect to the patient's symptoms. During the medical consultation (<b>400</b>), the healthcare professional may also discuss various treatment options with the patient. For instance, the healthcare professional may describe one or more different surgeries to address the patient's symptoms.</p><p id="p-0052" num="0051">Furthermore, the example of <figref idref="DRAWINGS">FIG. <b>4</b></figref> includes a case creation step (<b>402</b>). In other examples, the case creation step occurs before the medical consultation step. During the case creation step, the medical professional or other user establishes an electronic case file for the patient. The electronic case file for the patient may include information related to the patient, such as data regarding the patient's symptoms, patient range of motion observations, data regarding a surgical plan for the patient, medical images of the patients, notes regarding the patient, billing information regarding the patient, and so on.</p><p id="p-0053" num="0052">The example of <figref idref="DRAWINGS">FIG. <b>4</b></figref> includes a preoperative patient monitoring phase (<b>404</b>). During the preoperative patient monitoring phase, the patient's symptoms may be monitored. For example, the patient may be suffering from pain associated with arthritis in the patient's shoulder. In this example, the patient's symptoms may not yet rise to the level of requiring an arthroplasty to replace the patient's shoulder. However, arthritis typically worsens over time. Accordingly, the patient's symptoms may be monitored to determine whether the time has come to perform a surgery on the patient's shoulder. Observations from the preoperative patient monitoring phase may be stored in the electronic case file for the patient. In some examples, predictive analytics system <b>114</b> may be used to predict when the patient may need surgery, to predict a course of treatment to delay or avoid surgery or make other predictions with respect to the patient's health.</p><p id="p-0054" num="0053">Additionally, in the example of <figref idref="DRAWINGS">FIG. <b>4</b></figref>, a medical image acquisition step occurs during the preoperative phase (<b>406</b>). During the image acquisition step, medical images of the patient are generated. The medical images may be generated in a variety of ways. For instance, the images may be generated using a Computed Tomography (CT) process, a Magnetic Resonance Imaging (MRI) process, an ultrasound process, or another imaging process. The medical images generated during the image acquisition step include images of an anatomy of interest of the patient. For instance, if the patient's symptoms involve the patient's shoulder, medical images of the patient's shoulder may be generated. The medical images may be added to the patient's electronic case file. Healthcare professionals may be able to use the medical images in one or more of the preoperative, intraoperative, and postoperative phases. In some examples, the medical images may be segmented into anatomical parts. For instance, medical images of the patient's shoulder may be segmented into a scapula, a humerus, etc. Three-dimensional (3D) models of the anatomical parts may be generated.</p><p id="p-0055" num="0054">Furthermore, in the example of <figref idref="DRAWINGS">FIG. <b>4</b></figref>, an automatic processing step may occur (<b>408</b>). During the automatic processing step, virtual planning system <b>102</b> (<figref idref="DRAWINGS">FIG. <b>1</b></figref>) may automatically develop a preliminary surgical plan for the patient. In some examples of this disclosure, virtual planning system <b>102</b> may use machine learning techniques to develop the preliminary surgical plan based on information in the patient's virtual case file.</p><p id="p-0056" num="0055">The example of <figref idref="DRAWINGS">FIG. <b>4</b></figref> also includes a manual correction step (<b>410</b>). During the manual correction step, one or more human users may check and correct the determinations made during the automatic processing step. In some examples of this disclosure, one or more users may use mixed reality or virtual reality visualization devices during the manual correction step. In some examples, changes made during the manual correction step may be used as training data to refine the machine learning techniques applied by virtual planning system <b>102</b> during the automatic processing step.</p><p id="p-0057" num="0056">A virtual planning step (<b>412</b>) may follow the manual correction step in <figref idref="DRAWINGS">FIG. <b>4</b></figref>. During the virtual planning step, a healthcare professional may develop a surgical plan for the patient. In some examples of this disclosure, one or more users may use mixed reality or virtual reality visualization devices during development of the surgical plan for the patient. As discussed in further detail below, during the virtual planning step, virtual planning system <b>102</b> may design a patient matched implant.</p><p id="p-0058" num="0057">Furthermore, in the example of <figref idref="DRAWINGS">FIG. <b>4</b></figref>, intraoperative guidance may be generated (<b>414</b>). The intraoperative guidance may include guidance to a surgeon on how to execute the surgical plan. In some examples of this disclosure, virtual planning system <b>102</b> may generate at least part of the intraoperative guidance. In some examples, the surgeon or other user may contribute to the intraoperative guidance.</p><p id="p-0059" num="0058">Additionally, in the example of <figref idref="DRAWINGS">FIG. <b>4</b></figref>, a step of selecting and manufacturing surgical items is performed (<b>416</b>). During the step of selecting and manufacturing surgical items, manufacturing and delivery system <b>106</b> (<figref idref="DRAWINGS">FIG. <b>1</b></figref>) may manufacture surgical items for use during the surgery described by the surgical plan. For example, the surgical items may include surgical implants (e.g., generic and/or patient specific), surgical tools, and other items required to perform the surgery described by the surgical plan.</p><p id="p-0060" num="0059">In the example of <figref idref="DRAWINGS">FIG. <b>4</b></figref>, a surgical procedure may be performed with guidance from intraoperative system <b>108</b> (<figref idref="DRAWINGS">FIG. <b>1</b></figref>) (<b>418</b>). For example, a surgeon may perform the surgery while wearing a head-mounted MR visualization device of intraoperative system <b>108</b> that presents guidance information to the surgeon. The guidance information may help guide the surgeon through the surgery, providing guidance for various surgical steps in a surgical workflow, including sequence of surgical steps, details of individual surgical steps, and tool or implant selection, implant placement and position, and bone surface preparation for various surgical steps in the surgical procedure workflow.</p><p id="p-0061" num="0060">Postoperative patient monitoring may occur after completion of the surgical procedure (<b>420</b>). During the postoperative patient monitoring step, healthcare outcomes of the patient may be monitored. Healthcare outcomes may include relief from symptoms, ranges of motion, complications, performance of implanted surgical items, and so on. Pre- and postoperative monitoring system <b>112</b> (<figref idref="DRAWINGS">FIG. <b>1</b></figref>) may assist in the postoperative patient monitoring step.</p><p id="p-0062" num="0061">The medical consultation, case creation, preoperative patient monitoring, image acquisition, automatic processing, manual correction, and virtual planning steps of <figref idref="DRAWINGS">FIG. <b>4</b></figref> are part of preoperative phase <b>302</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>. The surgical procedures with guidance steps of <figref idref="DRAWINGS">FIG. <b>4</b></figref> is part of intraoperative phase <b>306</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>. The postoperative patient monitoring step of <figref idref="DRAWINGS">FIG. <b>4</b></figref> is part of postoperative phase <b>308</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0063" num="0062">As mentioned above, one or more of the subsystems of orthopedic surgical system <b>100</b> may include one or more mixed reality (MR) systems, such as MR system <b>212</b> (<figref idref="DRAWINGS">FIG. <b>2</b></figref>). Each MR system may include a visualization device. For instance, in the example of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, MR system <b>212</b> includes visualization device <b>213</b>. In some examples, in addition to including a visualization device, an MR system may include external computing resources that support the operations of the visualization device. For instance, the visualization device of an MR system may be communicatively coupled to a computing device (e.g., a personal computer, backpack computer, smartphone, etc.) that provides the external computing resources. Alternatively, adequate computing resources may be provided on or within visualization device <b>213</b> to perform necessary functions of the visualization device.</p><p id="p-0064" num="0063"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a schematic representation of visualization device <b>213</b> for use in an MR system, such as MR system <b>212</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, according to an example of this disclosure. As shown in the example of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, visualization device <b>213</b> can include a variety of electronic components found in a computing system, including one or more processor(s) <b>514</b> (e.g., microprocessors or other types of processing units) and memory <b>516</b> that may be mounted on or within a frame <b>518</b>. Furthermore, in the example of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, visualization device <b>213</b> may include a transparent screen <b>520</b> that is positioned at eye level when visualization device <b>213</b> is worn by a user. In some examples, screen <b>520</b> can include one or more liquid crystal displays (LCDs) or other types of display screens on which images are perceptible to a surgeon who is wearing or otherwise using visualization device <b>213</b> via screen <b>520</b>. Other display examples include organic light emitting diode (OLED) displays. In some examples, visualization device <b>213</b> can operate to project 3D images onto the user's retinas using techniques known in the art.</p><p id="p-0065" num="0064">In some examples, screen <b>520</b> includes see-through holographic lenses, sometimes referred to as waveguides, that permit a user to see real-world objects through (e.g., beyond) the lenses and also see holographic imagery projected into the lenses and onto the user's retinas by displays, such as liquid crystal on silicon (LCoS) display devices, which are sometimes referred to as light engines or projectors, operating as an example of a holographic projection system <b>538</b> within visualization device <b>213</b>. In other words, visualization device <b>213</b> may include one or more see-through holographic lenses to present virtual images to a user. Hence, in some examples, visualization device <b>213</b> can operate to project 3D images onto the user's retinas via screen <b>520</b>, e.g., formed by holographic lenses. In this manner, visualization device <b>213</b> may be configured to present a 3D virtual image to a user within a real-world view observed through screen <b>520</b>, e.g., such that the virtual image appears to form part of the real-world environment. In some examples, visualization device <b>213</b> may be a Microsoft HOLOLENS&#x2122; headset, available from Microsoft Corporation, of Redmond, Washington, USA, or a similar device, such as, for example, a similar MR visualization device that includes waveguides. The HOLOLENS&#x2122; device can be used to present 3D virtual objects via holographic lenses, or waveguides, while permitting a user to view actual objects in a real-world scene, i.e., in a real-world environment, through the holographic lenses.</p><p id="p-0066" num="0065">Although the example of <figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates visualization device <b>213</b> as a head-wearable device, visualization device <b>213</b> may have other forms and form factors. For instance, in some examples, visualization device <b>213</b> may be a handheld smartphone or tablet.</p><p id="p-0067" num="0066">Visualization device <b>213</b> can also generate a virtual user interface (UI) <b>522</b> that is visible to the user, e.g., as holographic imagery projected into see-through holographic lenses as described above. For example, UI <b>522</b> can include a variety of selectable virtual widgets <b>524</b> that allow the user to interact with a mixed reality (MR) system, such as MR system <b>212</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>. Imagery presented by visualization device <b>213</b> may include, for example, one or more 3D virtual objects. Details of an example of UI <b>522</b> are described elsewhere in this disclosure. Visualization device <b>213</b> also can include a speaker or other sensory devices <b>526</b> that may be positioned adjacent the user's ears. Sensory devices <b>526</b> can convey audible information or other perceptible information (e.g., vibrations) to assist the user of visualization device <b>213</b>.</p><p id="p-0068" num="0067">Visualization device <b>213</b> can also include a transceiver <b>528</b> to connect visualization device <b>213</b> to a processing device <b>510</b> and/or to network <b>208</b> and/or to a computing cloud, such as via a wired communication protocol or a wireless protocol, e.g., Wi-Fi, Bluetooth, etc. Visualization device <b>213</b> also includes a variety of sensors to collect sensor data, such as one or more optical camera(s) <b>530</b> (or other optical sensors) and one or more depth camera(s) <b>532</b> (or other depth sensors), mounted to, on or within frame <b>518</b>. In some examples, the optical sensor(s) <b>530</b> are operable to scan the geometry of the physical environment in which a user of MR system <b>212</b> is located (e.g., an operating room) and collect two-dimensional (2D) optical image data (either monochrome or color). Depth sensor(s) <b>532</b> are operable to provide 3D image data, such as by employing time of flight, stereo or other known or future-developed techniques for determining depth and thereby generating image data in three dimensions. Other sensors can include motion sensors <b>533</b> (e.g., Inertial Mass Unit (IMU) sensors, accelerometers, etc.) to assist with tracking movement.</p><p id="p-0069" num="0068">MR system <b>212</b> processes the sensor data so that geometric, environmental, textural, or other types of landmarks (e.g., corners, edges or other lines, walls, floors, objects) in the user's environment or &#x201c;scene&#x201d; can be defined and movements within the scene can be detected. As an example, the various types of sensor data can be combined or fused so that the user of visualization device <b>213</b> can perceive 3D images that can be positioned, or fixed and/or moved within the scene. When a 3D image is fixed in the scene, the user can walk around the 3D image, view the 3D image from different perspectives, and manipulate the 3D image within the scene using hand gestures, voice commands, gaze line (or direction) and/or other control inputs. As another example, the sensor data can be processed so that the user can position a 3D virtual object (e.g., a bone model) on an observed physical object in the scene (e.g., a surface, the patient's real bone, etc.) and/or orient the 3D virtual object with other virtual images displayed in the scene. In some examples, the sensor data can be processed so that the user can position and fix a virtual representation of the surgical plan (or other widget, image or information) onto a surface, such as a wall of the operating room. Yet further, in some examples, the sensor data can be used to recognize surgical instruments and the position and/or location of those instruments.</p><p id="p-0070" num="0069">Visualization device <b>213</b> may include one or more processors <b>514</b> and memory <b>516</b>, e.g., within frame <b>518</b> of the visualization device. In some examples, one or more external computing resources <b>536</b> process and store information, such as sensor data, instead of or in addition to in-frame processor(s) <b>514</b> and memory <b>516</b>. In this way, data processing and storage may be performed by one or more processors <b>514</b> and memory <b>516</b> within visualization device <b>213</b> and/or some of the processing and storage requirements may be offloaded from visualization device <b>213</b>. Hence, in some examples, one or more processors that control the operation of visualization device <b>213</b> may be within visualization device <b>213</b>, e.g., as processor(s) <b>514</b>. Alternatively, in some examples, at least one of the processors that controls the operation of visualization device <b>213</b> may be external to visualization device <b>213</b>, e.g., as processor(s) <b>210</b>. Likewise, operation of visualization device <b>213</b> may, in some examples, be controlled in part by a combination one or more processors <b>514</b> within the visualization device and one or more processors <b>210</b> external to visualization device <b>213</b>.</p><p id="p-0071" num="0070">For instance, in some examples, when visualization device <b>213</b> is in the context of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, processing of the sensor data can be performed by processing device(s) <b>210</b> in conjunction with memory or storage device(s) (M) <b>215</b>. In some examples, processor(s) <b>514</b> and memory <b>516</b> mounted to frame <b>518</b> may provide sufficient computing resources to process the sensor data collected by cameras <b>530</b>, <b>532</b> and motion sensors <b>533</b>. In some examples, the sensor data can be processed using a Simultaneous Localization and Mapping (SLAM) algorithm, or other known or future-developed algorithms for processing and mapping 2D and 3D image data and tracking the position of visualization device <b>213</b> in the 3D scene. In some examples, image tracking may be performed using sensor processing and tracking functionality provided by the Microsoft HOLOLENS&#x2122; system, e.g., by one or more sensors and processors <b>514</b> within a visualization device <b>213</b> substantially conforming to the Microsoft HOLOLENS&#x2122; device or a similar mixed reality (MR) visualization device.</p><p id="p-0072" num="0071">In some examples, MR system <b>212</b> can also include user-operated control device(s) <b>534</b> that allow the user to operate MR system <b>212</b>, use MR system <b>212</b> in spectator mode (either as master or observer), interact with UI <b>522</b> and/or otherwise provide commands or requests to processing device(s) <b>210</b> or other systems connected to network <b>208</b>. As examples, control device(s) <b>534</b> can include a microphone, a touch pad, a control panel, a motion sensor or other types of control input devices with which the user can interact.</p><p id="p-0073" num="0072"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a block diagram illustrating example components of visualization device <b>213</b> for use in a MR system. In the example of <figref idref="DRAWINGS">FIG. <b>6</b></figref>, visualization device <b>213</b> includes processors <b>514</b>, a power supply <b>600</b>, display device(s) <b>602</b>, speakers <b>604</b>, microphone(s) <b>606</b>, input device(s) <b>608</b>, output device(s) <b>610</b>, storage device(s) <b>612</b>, sensor(s) <b>614</b>, and communication devices <b>616</b>. In the example of <figref idref="DRAWINGS">FIG. <b>6</b></figref>, sensor(s) <b>616</b> may include depth sensor(s) <b>532</b>, optical sensor(s) <b>530</b>, motion sensor(s) <b>533</b>, and orientation sensor(s) <b>618</b>. Optical sensor(s) <b>530</b> may include cameras, such as Red-Green-Blue (RGB) video cameras, infrared cameras, or other types of sensors that form images from light. Display device(s) <b>602</b> may display imagery to present a user interface to the user.</p><p id="p-0074" num="0073">Speakers <b>604</b>, in some examples, may form part of sensory devices <b>526</b> shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>. In some examples, display devices <b>602</b> may include screen <b>520</b> shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>. For example, as discussed with reference to <figref idref="DRAWINGS">FIG. <b>5</b></figref>, display device(s) <b>602</b> may include see-through holographic lenses, in combination with projectors, that permit a user to see real-world objects, in a real-world environment, through the lenses, and also see virtual 3D holographic imagery projected into the lenses and onto the user's retinas, e.g., by a holographic projection system. In this example, virtual 3D holographic objects may appear to be placed within the real-world environment. In some examples, display devices <b>602</b> include one or more display screens, such as LCD display screens, OLED display screens, and so on. The user interface may present virtual images of details of the virtual surgical plan for a particular patient.</p><p id="p-0075" num="0074">In some examples, a user may interact with and control visualization device <b>213</b> in a variety of ways. For example, microphones <b>606</b>, and associated speech recognition processing circuitry or software, may recognize voice commands spoken by the user and, in response, perform any of a variety of operations, such as selection, activation, or deactivation of various functions associated with surgical planning, intra-operative guidance, or the like. As another example, one or more cameras or other optical sensors <b>530</b> of sensors <b>614</b> may detect and interpret gestures (such as hand motions, hand gestures, finger motions, finger gestures, eye blinks, or other physical gestures) in order to perform operations as described above. As a further example, sensors <b>614</b> may sense gaze direction and perform various operations as described elsewhere in this disclosure. In some examples, input devices <b>608</b> may receive manual input from a user, e.g., via a handheld controller including one or more buttons, a keypad, a touchscreen, joystick, trackball, and/or other manual input media, and perform, in response to the manual user input, various operations as described above.</p><p id="p-0076" num="0075">As discussed above, surgical lifecycle <b>300</b> may include a preoperative phase <b>302</b> (<figref idref="DRAWINGS">FIG. <b>3</b></figref>). One or more users may use orthopedic surgical system <b>100</b> in preoperative phase <b>302</b>. For instance, orthopedic surgical system <b>100</b> may include virtual planning system <b>102</b> to help the one or more users generate a virtual surgical plan that may be customized to an anatomy of interest of a particular patient. As described herein, the virtual surgical plan may include a 3-dimensional virtual model that corresponds to the anatomy of interest of the particular patient and a 3-dimensional model of one or more prosthetic components matched to the particular patient to repair the anatomy of interest or selected to repair the anatomy of interest. The virtual surgical plan also may include a 3-dimensional virtual model of guidance information to guide a surgeon in performing the surgical procedure, e.g., in preparing bone surfaces or tissue and placing implantable prosthetic hardware relative to such bone surfaces or tissue.</p><p id="p-0077" num="0076">A visualization system, such as MR visualization system <b>212</b>, may be configured to display virtual guidance including one or more virtual guides for performing work on a portion of a patient's anatomy. For instance, the visualization system may display virtual guidance that guides performance of a surgical step with the use of a physical targeting tool that attaches to the rotating tool. In some examples, a user such as a surgeon may view real-world objects in a real-world scene. The real-world scene may be in a real-world environment such as a surgical operating room. In this disclosure, the terms real and real-world may be used in a similar manner. The real-world objects viewed by the user in the real-world scene may include the patient's actual, real anatomy, such as an actual glenoid or humerus, exposed during surgery. The user may view the real-world objects via a see-through (e.g., transparent) screen, such as see-through holographic lenses, of a head-mounted MR visualization device, such as visualization device <b>213</b>, and also see virtual guidance such as virtual MR objects that appear to be projected on the screen or within the real-world scene, such that the MR guidance object(s) appear to be part of the real-world scene, e.g., with the virtual objects appearing to the user to be integrated with the actual, real-world scene. For example, the virtual guidance may be projected on the screen of a MR visualization device, such as visualization device <b>213</b>, such that the virtual guidance is overlaid on, and appears to be placed within, an actual, observed view of the patient's actual bone viewed by the surgeon through the transparent screen, e.g., through see-through holographic lenses. Hence, in this example, the virtual guidance may be a virtual 3D object that appears to be part of the real-world environment, along with actual, real-world objects.</p><p id="p-0078" num="0077">A screen through which the surgeon views the actual, real anatomy and also observes the virtual objects, such as virtual anatomy and/or virtual surgical guidance, may include one or more see-through holographic lenses. The holographic lenses, sometimes referred to as &#x201c;waveguides,&#x201d; may permit the user to view real-world objects through the lenses and display projected holographic objects for viewing by the user. As discussed above, an example of a suitable head-mounted MR device for visualization device <b>213</b> is the Microsoft HOLOLENS&#x2122; headset, available from Microsoft Corporation, of Redmond, Wash., USA. The HOLOLENS&#x2122; headset includes see-through, holographic lenses, also referred to as waveguides, in which projected images are presented to a user. The HOLOLENS&#x2122; headset also includes an internal computer, cameras and sensors, and a projection system to project the holographic content via the holographic lenses for viewing by the user. In general, the Microsoft HOLOLENS&#x2122; headset or a similar MR visualization device may include, as mentioned above, LCoS display devices that project images into holographic lenses, also referred to as waveguides, e.g., via optical components that couple light from the display devices to optical waveguides. The waveguides may permit a user to view a real-world scene through the waveguides while also viewing a 3D virtual image presented to the user via the waveguides. In some examples, the waveguides may be diffraction waveguides.</p><p id="p-0079" num="0078">The visualization system (e.g., MR system <b>212</b>/visualization device <b>213</b>) may be configured to display different types of virtual guidance. Examples of virtual guidance include, but are not limited to, a virtual point, a virtual axis, a virtual angle, a virtual path, a virtual plane, virtual reticle, and a virtual surface or contour. As discussed above, the visualization system (e.g., MR system <b>212</b>/visualization device <b>213</b>) may enable a user to directly view the patient's anatomy via a lens by which the virtual guides are displayed, e.g., projected. The virtual guidance may guide or assist various aspects of the surgery. For instance, a virtual guide may guide at least one of preparation of anatomy for attachment of the prosthetic or attachment of the prosthetic to the anatomy.</p><p id="p-0080" num="0079">The visualization system may obtain parameters for the virtual guides from a virtual surgical plan, such as the virtual surgical plan described herein. Example parameters for the virtual guides include, but are not necessarily limited to, guide location, guide orientation, guide type, guide color, etc.</p><p id="p-0081" num="0080">The visualization system may display a virtual guide in a manner in which the virtual guide appears to be overlaid on an actual, real object, within a real-world environment, e.g., by displaying the virtual guide(s) with actual, real-world objects (e.g., at least a portion of the patient's anatomy) viewed by the user through holographic lenses. For example, the virtual guidance may be 3D virtual objects that appear to reside within the real-world environment with the actual, real object.</p><p id="p-0082" num="0081">The techniques of this disclosure are described below with respect to a shoulder arthroplasty surgical procedure. Examples of shoulder arthroplasties include, but are not limited to, reversed arthroplasty, augmented reverse arthroplasty, standard total shoulder arthroplasty, augmented total shoulder arthroplasty, and hemiarthroplasty. However, the techniques are not so limited, and the visualization system may be used to provide virtual guidance information, including virtual guides in any type of surgical procedure. Other example procedures in which a visualization system, such as MR system <b>212</b>, may be used to provide virtual guidance include, but are not limited to, other types of orthopedic surgeries; any type of procedure with the suffix &#x201c;plasty,&#x201d; &#x201c;stomy,&#x201d; &#x201c;ectomy,&#x201d; &#x201c;clasia,&#x201d; or &#x201c;centesis,&#x201d;; orthopedic surgeries for other joints, such as elbow, wrist, finger, hip, knee, ankle or toe, or any other orthopedic surgical procedure in which precision guidance is desirable. For instance, a visualization system may be used to provide virtual guidance for an ankle arthroplasty surgical procedure.</p><p id="p-0083" num="0082">As discussed above, a MR system (e.g., MR system <b>212</b>, MR system <b>1800</b>A of <figref idref="DRAWINGS">FIG. <b>18</b></figref>, etc.) may receive a virtual surgical plan for attaching an implant to a patient and/or preparing bones, soft tissue or other anatomy of the patient to receive the implant. The virtual surgical plan may specify various surgical steps to be performed and various parameters for the surgical steps to be performed. As one example, the virtual surgical plan may specify a location on the patient's bone (e.g., glenoid, humerus, tibia, talus, etc.) for attachment of a guide pin. As another example, the virtual surgical plan may specify locations and/or orientations of one or more anchorage locations (e.g., screws, stems, pegs, keels, etc.).</p><p id="p-0084" num="0083"><figref idref="DRAWINGS">FIGS. <b>7</b> and <b>8</b></figref> are conceptual diagrams illustrating an MR system providing virtual guidance for installation of a guide pin in a bone, in accordance with one or more techniques of this disclosure. In <figref idref="DRAWINGS">FIGS. <b>7</b> and <b>8</b></figref> and other FIGS., for purposes of illustration, some of the surrounding tissue and some bone has been omitted for ease of illustration. As shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, MR system <b>212</b> may display virtual axis <b>3400</b> on or relative to humeral head <b>3204</b> of humerus <b>3200</b>. <figref idref="DRAWINGS">FIG. <b>7</b></figref> and subsequent figures illustrate one example of what the surgeon, or other user, would see when viewing via visualization device <b>213</b>. In particular, when viewing via visualization device <b>213</b> from the view shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the surgeon may see a portion of humerus <b>3200</b> and virtual axis <b>3400</b> (and/or other virtual guidance) overlaid on the portion of humerus <b>3200</b>.</p><p id="p-0085" num="0084">To display virtual axis <b>3400</b>, MR system <b>212</b> may determine a location on a virtual model of humerus <b>3200</b> at which a guide is to be installed. MR system <b>212</b> may obtain the location from a virtual surgical plan (e.g., the virtual surgical plan described above as generated by virtual planning system <b>202</b>). The location obtained by MR system <b>212</b> may specify one or both of coordinates of a point on the virtual model and a vector. The point may be the position at which the guide is to be installed and the vector may indicate the angle/slope at which the guide is to be installed. As such, MR system <b>212</b> may display a virtual drilling axis having parameters obtained from the virtual surgical plan, and the virtual drilling axis may be configured to guide drilling of one or more holes in the glenoid (e.g., for attachment of a guide pin to the scapula).</p><p id="p-0086" num="0085">A virtual model of humerus <b>3200</b> may be registered with humerus <b>3200</b> such that coordinates on the virtual model approximately correspond to coordinates on humerus <b>3200</b>. For instance, MR system <b>212</b> may generate a transformation matrix between the virtual model of humerus <b>3200</b> and an observed portion of humerus <b>3200</b>. This transformation matrix may allow for translation along the x, y, and z axes of the virtual model and rotation about the x, y and z axes in order to achieve and maintain alignment between the virtual and observed bones. In some examples, after registration is complete, MR system <b>212</b> utilize the results of the registration to perform simultaneous localization and mapping (SLAM) (or any other tracking algorithm) to maintain alignment of the virtual model to the corresponding observed object. As such, by displaying virtual axis <b>3400</b> at the determined location on the virtual model, MR system <b>212</b> may display virtual axis <b>3400</b> at the planned position on humerus <b>3200</b>.</p><p id="p-0087" num="0086">The surgeon may attach a guide pin to humerus <b>3200</b> using the displayed virtual guidance. For instance, where the guide pin includes a self-tapping threaded distal tip, the surgeon may align the guide pin with the displayed virtual axis <b>3400</b> and utilize a drill or other instrument to install the guide pin in humerus <b>3200</b>.</p><p id="p-0088" num="0087"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a conceptual diagram illustrating guide <b>3500</b> as installed in humeral head <b>3204</b>. Guide <b>3500</b> may take the form of an elongated pin to be mounted in a hole formed in the humeral head. As shown in <figref idref="DRAWINGS">FIGS. <b>7</b> and <b>8</b></figref>, by displaying virtual axis <b>3400</b>, a surgeon may install guide <b>3500</b> at the planned position on humeral head <b>3204</b>.</p><p id="p-0089" num="0088">As discussed above, <figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates an example of what the surgeon, or other user, would see when viewing via visualization device <b>213</b> from the view shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>. In particular, <figref idref="DRAWINGS">FIG. <b>7</b></figref> shows what the surgeon would see when the surgeon's gaze line is from a side view/substantially orthogonal to the axis of the surgical step being performed (e.g., virtual axis <b>3400</b>). However, the surgeon is not likely to view the patient from such an angle when operating a driver of a rotating tool (e.g., a drill or motor that rotates the guide pin, a drill bit, a reamer, or the like). Instead, when operating the driver of the rotating tool, the surgeon is likely to view the patient from behind the drill or motor while operating the drill or motor, with a gaze line substantially parallel to an axis of the surgical step being performed.</p><p id="p-0090" num="0089"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a conceptual diagram of virtual guidance that may be provided by an MR system according to one or more examples of this disclosure. As shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref>, a surgeon may view a portion of scapula <b>5100</b> through visualization device <b>213</b> with a gaze line substantially parallel (e.g., closer to parallel than perpendicular) to an axis of the surgical step being performed. For instance, as shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref> where visualization device <b>213</b> displays virtual axis <b>902</b> to guide use of a rotating tool to perform a surgical step on scapula <b>5100</b>, the axis of the surgical step being performed may correspond to virtual axis <b>902</b>. As such, the surgeon may view scapula <b>5100</b> through visualization device <b>213</b> with a gaze line substantially parallel to an axis of virtual axis <b>902</b>.</p><p id="p-0091" num="0090">As discussed above, in some examples, the surgeon may utilize one or more tools to perform work on portion of a patient's anatomy (e.g., scapula <b>5100</b>, humerus <b>3200</b>, etc.). For instance, the surgeon may utilize a driver to drive (e.g., provide rotational power to) a rotating tool. Examples of rotating tools include, but are not limited to, guide pins (e.g., self-tapping guide pins, such as guide <b>3500</b>), reaming tools, drill bits, and screw drivers.</p><p id="p-0092" num="0091">As also discussed above, MR system <b>212</b> may provide virtual guidance to assist the surgeon in performing surgical steps. For instance, as shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref>, visualization device <b>213</b> of MR system <b>212</b> may display virtual axis <b>902</b> to guide use of a rotating tool to perform a surgical step on scapula <b>5100</b>. The surgeon may achieve correct performance of the surgical step by aligning a shaft of the rotating tool with virtual axis <b>902</b>, activating a driver of the rotating tool, and advancing the shaft of the rotating tool along the displayed virtual axis. However, in some scenarios, the rotating tool, the driver of the rotating tool, and/or various tools used by the surgeon may obscure or otherwise interfere with a portion of the virtual guidance being presented by visualization device <b>213</b>.</p><p id="p-0093" num="0092"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a conceptual diagram of tools obscuring a portion of virtual guidance provided by an MR system. As shown in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, to utilize virtual axis <b>902</b>, the surgeon may align a shaft of rotating tool <b>1004</b> with virtual axis view a portion of scapula <b>5100</b> through visualization device <b>213</b> with a gaze line substantially parallel (e.g., closer to parallel than perpendicular) to an axis of the surgical step being performed. As can be seen in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, when viewing with a gaze line substantially parallel to the axis of the surgical step being performed (i.e., substantially parallel to virtual axis <b>902</b>) driver <b>1002</b> obscures the displayed virtual guidance (i.e., virtual axis <b>902</b>). With the virtual guidance obscured, it may be difficult for the surgeon to find and/or maintain alignment between a tool being used and the virtual guidance. For instance, it may be difficult for the surgeon to maintain alignment between a shaft of rotating tool <b>1004</b> and virtual axis <b>902</b>.</p><p id="p-0094" num="0093">In accordance with one or more techniques of this disclosure, MR system <b>212</b> may display virtual guidance that guides performance of a surgical step with the use of a physical targeting tool that attaches to a tool. Where the tool is a rotating tool, the physical targeting tool may include a channel through which the shaft of the rotating tool is inserted, and one or more physical targeting features that are offset from the channel. Further details of one example of a physical targeting tool are discussed below with reference to <figref idref="DRAWINGS">FIGS. <b>11</b>A-<b>11</b>C</figref>. MR system <b>212</b> may display the virtual guidance such that the surgeon may achieve correct performance of the surgical step by aligning the one or more physical targeting features of the physical targeting tool with the displayed virtual guidance. As the physical targeting features are offset from the channel, the physical targeting features may not be obscured by the driver of the rotating tool. As such, MR system <b>212</b> may display virtual guidance to be aligned with the physical targeting features without the virtual guidance being obscured. In this way, the techniques of this disclosure enable a surgeon to utilize tools, and associated drivers, to perform surgical steps with the assistance of virtual guidance.</p><p id="p-0095" num="0094"><figref idref="DRAWINGS">FIGS. <b>11</b>A-<b>11</b>C</figref> illustrate various views of one example of a physical targeting tool, in accordance with one or more techniques of this disclosure. As shown in <figref idref="DRAWINGS">FIGS. <b>11</b>A-<b>11</b>C</figref>, physical targeting tool <b>1100</b> may include main body <b>1102</b>, first physical sight <b>1108</b>, second physical sight <b>1110</b>, and handle <b>1112</b>.</p><p id="p-0096" num="0095">Main body <b>1102</b> may define a channel configured to receive a tool. For instance, main body <b>1102</b> may define channel <b>1114</b> that is configured to receive a shaft of a rotating tool, such as rotating tool <b>1004</b> of <figref idref="DRAWINGS">FIG. <b>10</b></figref>. Channel <b>1114</b> may have a primary axis that controls movement of a received tool. For instance, where channel <b>1114</b> is configured to receive a shaft of a rotating tool, channel <b>1114</b> may have a longitudinal axis (e.g., longitudinal axis <b>1116</b>) about-which the shaft may rotate. Channel <b>1114</b> may be considered to be configured to receive the tool by being sized such that an inner dimension of channel <b>1114</b> is slightly larger than an outer dimension of the tool. For instance, where the tool is a rotating tool, channel <b>1114</b> may be cylindrical and have an inner diameter that is slightly larger than an outer diameter of a shaft of the rotating tool. In this way, the shaft of the rotating tool may spin within channel <b>1114</b> but may be confined to rotation about longitudinal axis <b>1116</b>.</p><p id="p-0097" num="0096">Channel <b>1114</b> may extend all the way through main body <b>1102</b> such that channel <b>1114</b> is open at both distal end <b>1104</b> and proximal end <b>1106</b>. Therefore, a rotating tool may be inserted into proximal end <b>1106</b>, advanced through channel <b>1114</b>, and come out of distal end <b>1104</b>.</p><p id="p-0098" num="0097">Physical targeting tool <b>1100</b> may include one or more physical targeting features attached to main body <b>1102</b>. In the example of <figref idref="DRAWINGS">FIGS. <b>11</b>A-<b>11</b>C</figref>, the physical targeting features include first physical sight <b>1108</b> and second physical sight <b>1110</b>. As shown in <figref idref="DRAWINGS">FIGS. <b>11</b>A-<b>11</b>C</figref>, first physical sight <b>1108</b> and second physical sight <b>1110</b> may be displaced along main body <b>1102</b> such that first physical sight <b>1108</b> and second physical sight <b>1110</b> are displaced along longitudinal axis <b>1116</b> of channel <b>1114</b>. Each of first physical sight <b>1108</b> and second physical sight <b>1110</b> may include a focal point. As shown in <figref idref="DRAWINGS">FIGS. <b>11</b>A-<b>11</b>C</figref>, first physical sight <b>1108</b> may include focal point <b>1118</b> and second physical sight <b>1110</b> may include focal point <b>1120</b>. While shown in <figref idref="DRAWINGS">FIGS. <b>11</b>A-<b>11</b>C</figref> as reticles with crosshairs, each of first physical sight <b>1108</b> and second physical sight <b>1110</b>, and their respective focal points, may be any shape capable of being aligned with displayed virtual guidance (e.g., as displayed by a visualization device, such as visualization device <b>213</b> of <figref idref="DRAWINGS">FIG. <b>5</b></figref>). Other examples shapes include, but are not limited to, spheres, circles, triangles, and the like. In some examples, both first physical sight <b>1108</b> and second physical sight <b>1110</b> may be the same shape. In other examples, first physical sight <b>1108</b> and second physical sight <b>1110</b> may be different shapes.</p><p id="p-0099" num="0098">Focal points <b>1118</b> and <b>1120</b> may be laterally displaced from channel <b>1114</b>. For instance, focal point <b>1118</b> of first physical sight <b>1108</b> may be laterally displaced from longitudinal axis <b>1116</b> of channel <b>1114</b> by distance Di and focal point <b>1120</b> of second physical sight <b>1110</b> may be laterally displaced from longitudinal axis <b>1116</b> of channel <b>1114</b> by distance D<sub>2</sub>. In some examples, such as the example of <figref idref="DRAWINGS">FIGS. <b>11</b>A-<b>11</b>C</figref>, D<sub>1 </sub>and D<sub>2 </sub>may be different such axis <b>1122</b>, which connects focal point <b>1118</b> of first physical sight <b>1108</b> and focal point <b>1120</b> of second physical sight <b>1110</b>, is not parallel to longitudinal axis <b>1116</b> of channel <b>1114</b>. Where D<sub>1 </sub>and D<sub>2 </sub>are different, physical targeting tool <b>1100</b> may be considered to have asymmetric sights. In other examples, D<sub>1 </sub>and D<sub>2 </sub>may be equal such axis <b>1122</b> is parallel to longitudinal axis <b>1116</b> of channel <b>1114</b>. Where D<sub>1 </sub>and D<sub>2 </sub>are equal, physical targeting tool <b>1100</b> may be considered to have symmetric sights.</p><p id="p-0100" num="0099">Both symmetric and asymmetric cases present various advantages. For instance, physical targeting tools with asymmetric sights may enable a surgeon to maintain a gaze closer to a point of contact between a tool being used and a portion of a patient's anatomy on which the tool is being used.</p><p id="p-0101" num="0100">In some examples, physical targeting tool <b>1100</b> may include at least two of the physical targeting features. As discussed herein, during use, a surgeon may attempt to align virtual guidance with the physical targeting features. If only a single physical targeting feature were used, the directionality of the tool guided by physical targeting tool <b>1100</b> may not be certain. For instance, if only a single physical targeting feature at a particular position along longitudinal axis <b>1116</b> is aligned with corresponding virtual guidance, that particular point may be correctly positioned but the remainder of points may or may not be correctly positioned (i.e., as it takes two different points to define a line).</p><p id="p-0102" num="0101">Handle <b>1112</b> may be attached to main body <b>1102</b> and be configured to be gripped by a hand of an operator of the tool that channel <b>1114</b> is configured to receive. For instance, where the tool is a rotating tool connected to a driver, handle <b>1112</b> may be configured to be gripped by a first hand of a surgeon while a second hand of the surgeon operates the driver. By gripping handle <b>1112</b> while a rotating tool is actively rotating within channel <b>1114</b>, the surgeon may prevent rotation of main body <b>1102</b> that would otherwise occur. Handle <b>1112</b> may be mounted to main body <b>1102</b> at an angle (e.g., relative to longitudinal axis <b>1116</b>) desirable for proper gripping of handle <b>1112</b> while a tool within channel <b>1114</b> is being operated.</p><p id="p-0103" num="0102">In some examples, physical targeting tool <b>1100</b> may be ambidextrous such that physical targeting tool <b>1100</b> can be similarly operated by right-handed and left-handed surgeons. In other examples, physical targeting tool <b>1100</b> may be not be ambidextrous (i.e., physical targeting tool <b>1100</b> may come in &#x201c;righty&#x201d; or &#x201c;leftie&#x201d; configurations). Where physical targeting tool <b>1100</b> is not ambidextrous, handle <b>1112</b> of physical targeting tool <b>1100</b> may be configured to be gripped by a non-dominant hand of the surgeon. In the example of <figref idref="DRAWINGS">FIGS. <b>11</b>A-<b>11</b>C</figref> where handle <b>1112</b> is located on the left when looking down first physical sight <b>1108</b> and second physical sight <b>1110</b>, handle <b>1112</b> may be considered to be configured to be gripped by a left hand.</p><p id="p-0104" num="0103">Physical targeting tool <b>1100</b> may be made from any suitable material, or combination of materials. In some examples, physical targeting tool <b>1100</b> may be unitarily constructed such that it is one continuous piece. For instance, physical targeting tool <b>1100</b> may be 3D printed as a single part. In some examples, physical targeting tool <b>1100</b> may be constructed from multiple parts.</p><p id="p-0105" num="0104">In some examples, as opposed to utilizing a separate physical targeting tool, one or more of the physical targeting features may be integrated onto a tool or a driver of the tool. For instance, first physical sight <b>1108</b> and/or second physical sight <b>1110</b> may be attached to driver <b>1002</b>.</p><p id="p-0106" num="0105"><figref idref="DRAWINGS">FIG. <b>12</b>A and <b>12</b>B</figref> are conceptual diagrams illustrating an example physical targeting tool being attached to a tool, in accordance with one or more techniques of this disclosure. As shown in <figref idref="DRAWINGS">FIG. <b>12</b>A</figref>, rotating tool <b>1004</b> includes distal end <b>1202</b>, proximal end <b>1206</b>, and shaft <b>1204</b>. In the example of <figref idref="DRAWINGS">FIG. <b>12</b>A</figref>, rotating tool <b>1004</b> is a self-tapping guide pin as distal end <b>1202</b> is configured to drill and secure rotating tool <b>1004</b> in an object (e.g., a patient's bone). Other examples of rotating tools include, but are not limited to, guide pins (e.g., self-tapping guide pins, such as guide <b>3500</b>), reaming tools, drill bits, and screw drivers. As shown in <figref idref="DRAWINGS">FIG. <b>12</b>A</figref>, proximal end <b>1206</b> may be attached to driver <b>1002</b> (e.g., via a chuck or some other mechanical linkage), which may provide rotational power to rotating tool <b>1004</b>. Examples of driver <b>1002</b> include, but are not limited to, drills, surgical motors, and the like. Driver <b>1002</b> may be electrically powered, pneumatically powered, or manually powered.</p><p id="p-0107" num="0106">In operation, a surgeon may insert rotating tool <b>1004</b> into channel <b>1114</b> of physical targeting tool <b>1100</b>. For instance, the surgeon may insert distal end <b>1202</b> of rotating tool <b>1004</b> into proximal end <b>1106</b> of channel <b>1114</b> and advance distal end <b>1202</b> through channel <b>1114</b> toward distal end <b>1104</b> of channel <b>1114</b> such that distal end <b>1202</b> of rotating tool <b>1004</b> emerges from distal end <b>1104</b> of channel <b>1114</b>. While described as rotating tool <b>1004</b> being inserted into physical targeting tool <b>1004</b>, it is noted that a similar outcome may be achieved by sliding physical targeting tool <b>1004</b> down rotating tool <b>1004</b>.</p><p id="p-0108" num="0107">In some examples, the surgeon may continue to advance rotating tool <b>1004</b> until proximal end <b>1106</b> of channel <b>1114</b> comes into contact with driver <b>1002</b>. In such examples, the surgeon may perform a surgical step using rotating tool <b>1004</b> while proximal end <b>1106</b> of channel <b>1114</b> remains in contact with driver <b>1002</b>. In other examples, the surgeon may perform a surgical step using rotating tool <b>1004</b> while distal end <b>1104</b> of channel <b>1114</b> remains in contact with anatomy on which rotating tool <b>1004</b> is being used.</p><p id="p-0109" num="0108"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a conceptual diagram of virtual guidance that may be provided by an MR system to guide a surgeon performing a surgical step using a physical targeting tool, in accordance with one or more techniques of this disclosure. As shown in <figref idref="DRAWINGS">FIG. <b>13</b></figref>, the surgeon may bring the combined assembly of driver <b>1002</b>, rotating tool <b>1004</b>, and physical targeting tool <b>1004</b> into a position near a current surgical step to be performed. In the example of <figref idref="DRAWINGS">FIG. <b>13</b></figref>, the current surgical step is installing a guide pin in scapula <b>5100</b>. However, the techniques of this disclosure are equally applicable to other steps of surgical procedures.</p><p id="p-0110" num="0109">As discussed above, in some examples, a tool, or a driver of a tool, being used to perform a step may obscure or otherwise interfere with a portion of virtual guidance being presented by visualization device <b>213</b> to assist with operation of the tool. In accordance with one or more techniques of this disclosure, MR system <b>212</b> may display virtual guidance <b>1302</b> that guides performance of a surgical step with the use of a physical targeting tool that attaches to a tool. Virtual guidance <b>1302</b> may enable the surgeon to achieve correct performance of the surgical step by aligning portions of the one or more physical targeting features of physical targeting tool <b>1100</b> with the displayed virtual guidance <b>1302</b>. Virtual guidance <b>1302</b> may be in the form of shapes that correspond to the shapes of the physical targeting features of physical targeting tool <b>1100</b>. For instance, in the example of <figref idref="DRAWINGS">FIG. <b>13</b></figref> where the physical targeting features include first physical sight <b>1108</b> and second physical sight <b>1110</b>, both in the shape of reticles, virtual guidance <b>1302</b> may be in the form of virtually displayed reticles <b>1304</b> and <b>1306</b>, which may be referred to as virtual guidance elements.</p><p id="p-0111" num="0110">As first physical sight <b>1108</b> and second physical sight <b>1110</b> are offset from channel <b>1114</b> of rotating tool <b>1004</b>, first physical sight <b>1108</b> and second physical sight <b>1110</b> may not be obscured by driver <b>1002</b> during operation of rotating tool <b>1004</b>. In other words, first physical sight <b>1108</b> and second physical sight <b>1110</b> may remain visible to the surgeon while the surgeon is operating rotating tool <b>1004</b> (e.g., causing driver <b>1002</b> to apply rotational force to rotating tool <b>1004</b>). As such, the surgeon may be able to align, and maintain alignment of, reticles <b>1304</b> and <b>1306</b> of virtual guidance <b>1302</b> with first physical sight <b>1108</b> and second physical sight <b>1110</b> while the surgeon is operating rotating tool <b>1004</b>. In other words, MR system <b>212</b> may display virtual guidance to be aligned with physical targeting features of the physical targeting tool without the virtual guidance being obscured. In this way, the techniques of this disclosure enable a surgeon to utilize tools, and associated drivers, to perform surgical steps with the assistance of virtual guidance.</p><p id="p-0112" num="0111">While illustrated in the example of <figref idref="DRAWINGS">FIG. <b>13</b></figref> as shapes that correspond to the shapes of the physical targeting features of physical targeting tool <b>1100</b>, MR system <b>212</b> may display the virtual guidance in any manner that enables the surgeon to align physical targeting features of a physical targeting tool with the virtual guidance. Other examples of virtual guidance that MR system <b>212</b> may display include, but are not limited to, axes (e.g., an axis connecting focal points of the physical targeting features), points, circles, rings, polygons, planes, X shapes, crosses, or any other shape or combination of shapes.</p><p id="p-0113" num="0112">In some examples, MR system <b>212</b> may display the virtual guidance at a same depth as the physical targeting features. For instance, MR system <b>212</b> may display reticles <b>1304</b> and <b>1306</b> of virtual guidance <b>1302</b> at the same depths (e.g., positions along the longitudinal axis) as first physical sight <b>1108</b> and second physical sight <b>1110</b>. In some examples, MR system <b>212</b> may display the virtual guidance at a different depth than the physical targeting features. For instance, MR system <b>212</b> may display reticles <b>1304</b> and <b>1306</b> of virtual guidance <b>1302</b> at positions displaced along the longitudinal axis (e.g., by 10 cm, 20 cm, 50 cm, etc.) from first physical sight <b>1108</b> and second physical sight <b>1110</b>. In either case, the surgeon may be considered to have aligned the physical targeting features with the virtual guidance where the physical targeting features and the virtual guidance at least align in dimensions (e.g., in x and y dimensions, where the z dimension extends along the longitudinal axis).</p><p id="p-0114" num="0113"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a conceptual diagram of virtual guidance that may be provided by an MR system to guide a surgeon performing a surgical step using a physical targeting tool, in accordance with one or more techniques of this disclosure. <figref idref="DRAWINGS">FIG. <b>14</b></figref> illustrates a similar scenario as <figref idref="DRAWINGS">FIG. <b>13</b></figref> from a slightly offset perspective. In the example of <figref idref="DRAWINGS">FIG. <b>14</b></figref>, first physical sight <b>1108</b> and second physical sight <b>1110</b> may not yet be aligned with virtual guidance <b>1302</b> (e.g., reticles <b>1304</b> and <b>1306</b>). As discussed above, to achieve correct performance of the surgical step, the surgeon may align portions of the one or more physical targeting features of physical targeting tool <b>1100</b> with the displayed virtual guidance <b>1302</b>. In this case, the surgeon may align portions of the one or more physical targeting features of physical targeting tool <b>1100</b> with the displayed virtual guidance <b>1302</b> by rotating physical targeting tool <b>1100</b> clockwise about the axis of rotating tool <b>1004</b>.</p><p id="p-0115" num="0114">For instance, as shown in the example of <figref idref="DRAWINGS">FIG. <b>14</b></figref>, MR system <b>212</b> may display virtual guidance <b>1302</b> as including offset virtual axis <b>1402</b>. In the example of <figref idref="DRAWINGS">FIG. <b>14</b></figref>, a surgeon may enable the surgeon to achieve correct performance of the surgical step by focal points <b>1118</b> and <b>1120</b> with offset virtual axis <b>1402</b>. Where targeting tool <b>1100</b> has symmetric sights, offset virtual axis <b>1402</b> may be parallel to an axis virtual axis <b>902</b>, through virtual axis <b>902</b> may or may not be contemporaneously displayed with virtual guidance <b>1302</b>.</p><p id="p-0116" num="0115"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a flowchart illustrating example techniques for providing virtual guidance to guide a surgeon performing a surgical step using a physical targeting tool, in accordance with one or more techniques of this disclosure. For purposes of explanation, the techniques of <figref idref="DRAWINGS">FIG. <b>14</b></figref> are described as being performed by MR system <b>212</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. However, other mixed-reality systems may perform the techniques of <figref idref="DRAWINGS">FIG. <b>15</b></figref>.</p><p id="p-0117" num="0116">MR system <b>212</b> may perform a registration process that registers a virtual bone object with a corresponding bone of the patient (<b>1502</b>) in the field of view presented to the surgeon by visualization device <b>213</b>. For instance, MR system <b>212</b> may obtain a virtual glenoid object (e.g., a virtual model of a portion of an anatomy of the patient) from storage system <b>206</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>. The virtual glenoid object may be generated based on pre-operative imaging (e.g., CT imaging) of the patient's scapula/glenoid. MR system <b>212</b> may perform the registration using any suitable process. The registration may produce a transformation matrix between the virtual bone object with the patient's actual bone.</p><p id="p-0118" num="0117">In some examples, MR system <b>212</b> may perform a registration process that registers at least a portion of a virtual representation of a physical targeting tool object with a corresponding portion of a physical targeting tool (<b>1604</b>) in the field of view presented to the surgeon by visualization device <b>213</b>. For instance, MR system <b>212</b> may obtain a virtual targeting tool object (e.g., a virtual model of a physical targeting tool, such as physical targeting tool <b>1100</b> of <figref idref="DRAWINGS">FIGS. <b>11</b>A-<b>11</b>C</figref>) from storage system <b>206</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>. The virtual targeting tool object may be obtained from a manufacturer of the physical targeting tool. As one example, the virtual targeting tool object may be a CAD model or other virtual representation of the physical targeting tool. As discussed above, the physical targeting tool may include two or more physical targeting features. In some examples, the virtual targeting tool object may be a representation of an entire targeting tool. For instance, the virtual targeting tool object may be a CAD model or other virtual representation of physical targeting tool <b>1100</b>. In some examples, the virtual targeting tool object may be a representation of a portion (e.g., a physical targeting feature, handle, etc.) of a targeting tool. For instance, the virtual targeting tool object may be a CAD model or other virtual representation of physical targeting features of the physical targeting tool (e.g., first physical sight <b>1108</b> and second physical sight <b>1110</b> of physical targeting tool <b>1100</b>).</p><p id="p-0119" num="0118">As part of the registration(s), MR system <b>212</b> may map positions of the virtual representation of the targeting tool object and positions of the virtual bone object into a common coordinate system. As the positions of the virtual representation of the targeting tool object and the virtual bone object correspond to positions of the physical targeting tool object and the patient's actual bone, points on the virtual representation of the targeting tool object and the virtual bone object may correspond to actual positions on the physical targeting tool object and the patient's actual bone. In other words, the positions of the virtual representation of the targeting tool object and the virtual bone object may function as proxies for the positions of the physical targeting tool and the actual bone.</p><p id="p-0120" num="0119">MR system <b>212</b> may display virtual guidance that guides performing the surgical step on the anatomy when the physical targeting features of the physical targeting tool are aligned with the displayed virtual guidance (<b>1606</b>). For instance, visualization device <b>213</b> of MR system <b>212</b> may display virtual guidance <b>1302</b> of <figref idref="DRAWINGS">FIG. <b>13</b> or <b>14</b></figref>. As discussed above, virtual guidance <b>1302</b> may be in the form of shapes that correspond to the shapes of the physical targeting features of the physical targeting tool. For instance, visualization device <b>213</b> may display first virtual guidance with a shape that corresponds to a shape of a first physical targeting feature of the physical targeting tool, and display second virtual guidance with a shape that corresponds to a shape of a second physical targeting feature of the physical targeting tool (e.g., display reticles <b>1304</b> and <b>1306</b> of virtual guidance <b>1302</b> of <figref idref="DRAWINGS">FIG. <b>13</b></figref> that have shapes corresponding to first physical sight <b>1108</b> and second physical sight <b>1110</b> of physical targeting tool <b>1100</b>).</p><p id="p-0121" num="0120">MR system <b>212</b> may display the virtual guidance at a position/orientation relative to the virtual bone object. For instance, where the virtual guidance includes an offset virtual axis (e.g., offset virtual axis <b>1402</b> of <figref idref="DRAWINGS">FIG. <b>14</b></figref>), MR system <b>212</b> may obtain (e.g., from a virtual surgical plan) parameters for a current surgical step to be performed. The parameters may specify a location on the virtual bone object at which the step is to be performed and/or an orientation relative to the virtual bone object at which the step is to be performed. For instance, where the step is performed along an axis, the parameters may specify the location as coordinates of a point that the axis passes through (e.g., an x,y,z coordinate set) and the orientation as angles that define the axis (e.g., a polar angle and an azimuth angle). Based on the obtained parameters and dimensions of the physical targeting tool (e.g., D<sub>1 </sub>and D<sub>2 </sub>of <figref idref="DRAWINGS">FIG. <b>11</b>C</figref>), MR system <b>212</b> may determine parameters of the offset virtual axis. For instance, where the physical targeting tool has symmetric sights (e.g., where D<sub>1 </sub>and D<sub>2 </sub>are equal), MR system <b>212</b> may determine the parameters of the offset virtual axis by adding the lateral displacement distance of the physical targeting features to a coordinate of the axis defined by the parameters (e.g., to a coordinate of a plane orthogonal to the axis).</p><p id="p-0122" num="0121">The displayed virtual guidance may enable the surgeon to achieve correct performance of the surgical step by aligning the physical targeting features of the physical targeting tool with the displayed virtual guidance. For instance, the surgeon may align first physical sight <b>1108</b> and second physical sight <b>1110</b> of physical targeting tool <b>1100</b> with reticles <b>1304</b> and <b>1306</b> of virtual guidance <b>1302</b> of <figref idref="DRAWINGS">FIG. <b>13</b></figref>. When the physical targeting features of the physical targeting tool with the displayed virtual guidance, the surgeon may activate a driver of a tool physically guided by the physical targeting tool (e.g., activate driver <b>1002</b> of rotating tool <b>1004</b>) and perform the surgical step. As discussed above, the virtual guidance is displayed at a position/orientation, based on the physical targeting tool, relative to an axis for a current surgical step and relative to the virtual bone object. As the virtual bone object is a proxy for the patient's actual bone, this results in the virtual guidance being displayed at a position/orientation relative to the patient's actual bone. Furthermore, as the physical targeting tool controls the position/orientation of the tool being used to perform the surgical step, aligning the physical targeting features of the physical targeting tool with the displayed virtual guidance results in the tool being placed at the desired position/orientation relative to the patient's actual bone.</p><p id="p-0123" num="0122">In some examples, MR system <b>212</b> may output an indication of whether the surgeon has aligned the physical targeting features of the physical targeting tool with the displayed virtual guidance. For instance, MR system <b>212</b> may output a haptic, audio, graphical, textual, or any other indication perceptible to the surgeon to indicate that the surgeon has aligned the physical targeting features of the physical targeting tool with the displayed virtual guidance.</p><p id="p-0124" num="0123">In some examples, MR system <b>212</b> may positively control operation of surgical tools (e.g., driver <b>1002</b>) based on whether the surgeon has aligned the physical targeting features of the physical targeting tool with the displayed virtual guidance. For instance, if the surgeon has aligned the physical targeting features of the physical targeting tool with the displayed virtual guidance, MR system <b>212</b> may enable operation of driver <b>1002</b>. Similarly, if the surgeon has not aligned the physical targeting features of the physical targeting tool with the displayed virtual guidance, MR system <b>212</b> may disable operation of driver <b>1002</b>.</p><p id="p-0125" num="0124">While the techniques been disclosed with respect to a limited number of examples, those skilled in the art, having the benefit of this disclosure, will appreciate numerous modifications and variations there from. For instance, it is contemplated that any reasonable combination of the described examples may be performed. It is intended that the appended claims cover such modifications and variations as fall within the true spirit and scope of the invention.</p><p id="p-0126" num="0125">It is to be recognized that depending on the example, certain acts or events of any of the techniques described herein can be performed in a different sequence, may be added, merged, or left out altogether (e.g., not all described acts or events are necessary for the practice of the techniques). Moreover, in certain examples, acts or events may be performed concurrently, e.g., through multi-threaded processing, interrupt processing, or multiple processors, rather than sequentially.</p><p id="p-0127" num="0126">In one or more examples, the functions described may be implemented in hardware, software, firmware, or any combination thereof If implemented in software, the functions may be stored on or transmitted over as one or more instructions or code on a computer-readable medium and executed by a hardware-based processing unit. Computer-readable media may include computer-readable storage media, which corresponds to a tangible medium such as data storage media, or communication media including any medium that facilitates transfer of a computer program from one place to another, e.g., according to a communication protocol. In this manner, computer-readable media generally may correspond to (1) tangible computer-readable storage media which is non-transitory or (2) a communication medium such as a signal or carrier wave. Data storage media may be any available media that can be accessed by one or more computers or one or more processors to retrieve instructions, code and/or data structures for implementation of the techniques described in this disclosure. A computer program product may include a computer-readable medium.</p><p id="p-0128" num="0127">By way of example, and not limitation, such computer-readable storage media can comprise RAM, ROM, EEPROM, CD-ROM or other optical disk storage, magnetic disk storage, or other magnetic storage devices, flash memory, or any other medium that can be used to store desired program code in the form of instructions or data structures and that can be accessed by a computer. Also, any connection is properly termed a computer-readable medium. For example, if instructions are transmitted from a website, server, or other remote source using a coaxial cable, fiber optic cable, twisted pair, digital subscriber line (DSL), or wireless technologies such as infrared, radio, and microwave, then the coaxial cable, fiber optic cable, twisted pair, DSL, or wireless technologies such as infrared, radio, and microwave are included in the definition of medium. It should be understood, however, that computer-readable storage media and data storage media do not include connections, carrier waves, signals, or other transitory media, but are instead directed to non-transitory, tangible storage media. Disk and disc, as used herein, includes compact disc (CD), laser disc, optical disc, digital versatile disc (DVD), floppy disk and Blu-ray disc, where disks usually reproduce data magnetically, while discs reproduce data optically with lasers. Combinations of the above should also be included within the scope of computer-readable media.</p><p id="p-0129" num="0128">Operations described in this disclosure may be performed by one or more processors, which may be implemented as fixed-function processing circuits, programmable circuits, or combinations thereof, such as one or more digital signal processors (DSPs), general purpose microprocessors, application specific integrated circuits (ASICs), field programmable gate arrays (FPGAs), or other equivalent integrated or discrete logic circuitry. Fixed-function circuits refer to circuits that provide particular functionality and are preset on the operations that can be performed. Programmable circuits refer to circuits that can programmed to perform various tasks and provide flexible functionality in the operations that can be performed. For instance, programmable circuits may execute instructions specified by software or firmware that cause the programmable circuits to operate in the manner defined by instructions of the software or firmware. Fixed-function circuits may execute software instructions (e.g., to receive parameters or output parameters), but the types of operations that the fixed-function circuits perform are generally immutable. Accordingly, the terms &#x201c;processor&#x201d; and &#x201c;processing circuity,&#x201d; as used herein may refer to any of the foregoing structures or any other structure suitable for implementation of the techniques described herein.</p><p id="p-0130" num="0129">Various examples have been described. These and other examples are within the scope of the following claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A physical targeting tool comprising:<claim-text>a main body defining a channel configured to receive a tool, the channel having a longitudinal axis;</claim-text><claim-text>a first physical targeting feature attached to the main body; and</claim-text><claim-text>a second physical targeting feature attached to the main body, wherein the first physical targeting feature and the second physical targeting feature are displaced along the longitudinal axis of the channel.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The physical targeting tool of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the tool is a rotating tool, and wherein the channel is configured to receive a shaft of the rotating tool.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The physical targeting tool of <claim-ref idref="CLM-00002">claim 2</claim-ref>, further comprising:<claim-text>a handle configured to prevent rotation of the main body caused by rotation of the shaft.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The physical targeting tool of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the handle is configured to be gripped by a first hand of a user of the rotating tool, and wherein a second hand of the user operates a driver of the rotating tool.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The physical targeting tool of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein a proximal end of the channel is configured to contact the driver of the rotating tool.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The physical targeting tool of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first physical targeting feature includes a focal point, wherein the second physical targeting feature includes a focal point, wherein a distance between the focal point of the first physical targeting feature and the longitudinal axis of the channel is greater than a distance between the focal point of the second physical targeting feature and the longitudinal axis of the channel.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The physical targeting tool of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein an axis connecting the focal point of the first physical targeting feature and the focal point of the second physical targeting feature is not parallel to the longitudinal axis of the channel.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The physical targeting tool of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first physical targeting feature includes a focal point, wherein the second physical targeting feature includes a focal point, wherein a distance between the focal point of the first physical targeting feature and the longitudinal axis of the channel is equal to a distance between the focal point of the second physical sight and the longitudinal axis of the channel.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The physical targeting tool of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein an axis connecting the focal point of the first physical targeting feature and the focal point of the second physical targeting feature is parallel to the longitudinal axis of the channel.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The physical targeting tool of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first physical targeting feature and the second physical targeting feature each comprise a physical sight.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The physical targeting tool of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each of the physical sights comprise a reticle.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The physical targeting tool of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the device is unitarily constructed.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The physical targeting tool of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the channel is open at a proximal end and a distal end such that, when the shaft is inserted into the channel, the shaft passes through both the proximal end and the distal end.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The physical targeting tool of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the distal end of the channel is configured to contact anatomy of a patient on which the tool is being used.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The physical targeting tool of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the shaft is a shaft of a self-tapping guide pin.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The physical targeting tool of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the shaft is a shaft of a screw.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The physical targeting tool of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the shaft is a shaft of a drill bit.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. A method comprising:<claim-text>registering, via a visualization device, a virtual model of a portion of an anatomy of a patient to a corresponding portion of the anatomy, the virtual model obtained from a virtual surgical plan; and</claim-text><claim-text>displaying, via the visualization device, virtual guidance that guides performing work on the anatomy when two or more physical targeting features of a physical targeting tool are aligned with the displayed virtual guidance.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The method of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein displaying the virtual guidance comprises:<claim-text>displaying a first virtual guidance element with a shape that corresponds to a shape of a first physical targeting feature of the two or more physical targeting features; and</claim-text><claim-text>displaying a second virtual guidance element with a shape that corresponds to a shape of a second physical targeting feature of the two or more physical targeting features.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The method of <claim-ref idref="CLM-00018">claim 18</claim-ref>, further comprising:<claim-text>registering, via the visualization device, a virtual model of a portion of the physical targeting tool to a corresponding portion of the physical targeting tool; and</claim-text><claim-text>outputting, via the visualization device, an indication of whether the physical targeting features are aligned with the displayed virtual guidance.</claim-text></claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. (canceled)</claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. (canceled)</claim-text></claim><claim id="CLM-00023" num="00023"><claim-text><b>23</b>. (canceled)</claim-text></claim></claims></us-patent-application>