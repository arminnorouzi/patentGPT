<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230007225A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230007225</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17780504</doc-number><date>20201202</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>CN</country><doc-number>201911231206.4</doc-number><date>20191205</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>13</main-group><subgroup>257</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>13</main-group><subgroup>246</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>13</main-group><subgroup>383</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20180501</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>13</main-group><subgroup>257</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20180501</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>13</main-group><subgroup>246</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20180501</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>13</main-group><subgroup>383</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">EYE POSITIONING APPARATUS AND METHOD, AND 3D DISPLAY DEVICE AND METHOD</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Beijing Ivisual 3D Technology Co., Ltd.</orgname><address><city>Beijing</city><country>CN</country></address></addressbook><residence><country>CN</country></residence></us-applicant><us-applicant sequence="01" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>VISIOTECH VENTURES PTE. LTD.</orgname><address><city>Singapore</city><country>SG</country></address></addressbook><residence><country>SG</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>DIAO</last-name><first-name>Honghao</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>HUANG</last-name><first-name>Lingxi</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor></inventors></us-parties><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/CN2020/133328</doc-number><date>20201202</date></document-id><us-371c12-date><date>20220526</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">An eye positioning apparatus is provided, comprising: an eye positioner, comprising a first black-and-white camera configured to shoot first black-and-white images and a second black-and-white camera configured to shoot second black-and-white images; and an eye positioning image processor, configured to identify presence of eyes based on at least one of the first black-and-white images and the second black-and-white images and determine eye space positions based on the eyes identified in the first black-and-white images and the second black-and-white images. The apparatus can determine the eye space positions of a user at high accuracy, thereby improving 3D display quality. An eye positioning method, a 3D display device and method, a computer-readable storage medium, and a computer program product are also provided.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="107.70mm" wi="158.75mm" file="US20230007225A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="233.00mm" wi="165.95mm" orientation="landscape" file="US20230007225A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="245.19mm" wi="162.14mm" orientation="landscape" file="US20230007225A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="62.82mm" wi="66.89mm" file="US20230007225A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="244.77mm" wi="172.30mm" file="US20230007225A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="167.47mm" wi="171.03mm" file="US20230007225A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="150.37mm" wi="163.66mm" file="US20230007225A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="241.05mm" wi="160.36mm" file="US20230007225A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="151.55mm" wi="157.99mm" file="US20230007225A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="240.54mm" wi="104.22mm" file="US20230007225A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="215.48mm" wi="88.73mm" file="US20230007225A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="124.29mm" wi="163.58mm" file="US20230007225A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><p id="p-0002" num="0001">The present disclosure is a National Stage Filing of the PCT International Application No. PCT/CN2020/133328 filed on Dec. 2, 2020, which claims priority to the Chinese Patent Application with an application number of 201911231206.4 and a title of &#x201c;Eye Tracking Apparatus and Method, and 3D Display Device and Method&#x201d;, filed to China National Intellectual Property Administration on Dec. 5, 2019, the disclosures of which are hereby incorporated by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">The present disclosure relates to a 3D display technology, and for example, relates to an eye positioning apparatus and method, and a 3D display device and method.</p><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0004" num="0003">In some conventional face or eye positioning apparatuses, only a distance between a face and a screen is detected; and positions of viewpoints, at which eyes are located, are determined by a preset or default pupil distance. Thus, the recognition accuracy is not high; a viewpoint calculation error may be caused; and high-quality 3D display cannot be satisfied.</p><p id="p-0005" num="0004">The present background is only for the convenience of understanding the related technologies in the field, and is not regarded as an admission of the existing technologies.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0006" num="0005">In order to provide a basic understanding of some aspects of the disclosed embodiments, a brief summary is given below. The summary is not intended to identify key/important components or describe the scope of protection of the present disclosure, but to be a preface to the following detailed description.</p><p id="p-0007" num="0006">Embodiments of the present disclosure are intended to provide an eye positioning apparatus and method, a 3D display device and method, a computer-readable storage medium, and a computer program product.</p><p id="p-0008" num="0007">In one solution, an eye positioning apparatus is provided, comprising: an eye positioner, comprising a first black-and-white camera configured to shoot first black-and-white images and a second black-and-white camera configured to shoot second black-and-white images; and an eye positioning image processor, configured to identify the presence of eyes based on at least one of the first black-and-white images and the second black-and-white images and determine eye space positions based on the eyes identified in the first black-and-white images and the second black-and-white images.</p><p id="p-0009" num="0008">By adopting the eye positioning apparatus, the eye space positions of the user can be determined at high accuracy, to improve 3D display quality.</p><p id="p-0010" num="0009">In some embodiments, the eye positioning apparatus further comprises an eye positioning data interface, configured to transmit eye space position information which indicates the eye space positions.</p><p id="p-0011" num="0010">In some embodiments, the eye positioner further comprises an infrared emitting apparatus. In some embodiments, the infrared emitting apparatus is configured to emit infrared light with a wavelength greater than or equal to 1.5 microns.</p><p id="p-0012" num="0011">In some embodiments, the first black-and-white camera and the second black-and-white camera are configured to respectively shoot a first black-and-white image sequence comprising the first black-and-white images and a second black-and-white image sequence comprising the second black-and-white images.</p><p id="p-0013" num="0012">In some embodiments, the eye positioning image processor comprises a synchronizer, configured to determine time-synchronized first black-and-white images and second black-and-white images, so as to conduct identification of eyes and determination of the eye space positions.</p><p id="p-0014" num="0013">In some embodiments, the eye positioning image processor comprises: a buffer, configured to buffer a plurality of first black-and-white images and second black-and-white images in the first black-and-white image sequence and the second black-and-white image sequence; a comparer, configured to compare a plurality of previous or subsequent first black-and-white images and second black-and-white images in the first black-and-white image sequence and the second black-and-white image sequence; and a determiner, wherein when the comparer does not identify the presence of eyes in a current first black-and-white image and a current second black-and-white image in the first black-and-white image sequence and the second black-and-white image sequence and identifies the presence of eyes in a previous or subsequent first black-and-white image and second black-and-white image through comparison, the determiner is configured to take the eye space positions, determined based on the previous or subsequent first black-and-white image and second black-and-white image, as current eye space positions.</p><p id="p-0015" num="0014">Based on this, for example, in the case of stalling or frame skipping of the first or second black-and-white camera, a more coherent display picture can be provided for the user, to ensure viewing experience.</p><p id="p-0016" num="0015">In another solution, a 3D display device is provided, comprising: a multi-viewpoint 3D display screen, comprising a plurality of subpixels corresponding to a plurality of viewpoints; the eye positioning apparatus as described above, configured to obtain the eye space positions; and a 3D processing apparatus, configured to determine corresponding viewpoints according to the eye space positions acquired by the eye positioning apparatus, and render the subpixels, corresponding to the viewpoints, of the multi-viewpoint 3D display screen based on 3D signals.</p><p id="p-0017" num="0016">In some embodiments, the multi-viewpoint 3D display screen comprises a plurality of composite pixels; each composite pixel of the plurality of composite pixels comprises a plurality of composite subpixels; and each composite subpixel of the plurality of composite subpixels comprises a plurality of subpixels corresponding to the plurality of viewpoints.</p><p id="p-0018" num="0017">In some embodiments, the 3D processing apparatus is in communication connection with the eye positioning apparatus.</p><p id="p-0019" num="0018">In some embodiments, the 3D display device further comprises: a 3D shooting apparatus, configured to collect 3D images; and the 3D shooting apparatus comprises a depth-of-field (DOF) camera and at least two color cameras.</p><p id="p-0020" num="0019">In some embodiments, the eye positioning apparatus is integrated with the 3D shooting apparatus.</p><p id="p-0021" num="0020">In some embodiments, the 3D shooting apparatus is placed in front of the 3D display device.</p><p id="p-0022" num="0021">In another solution, an eye positioning method is provided, comprising: shooting first black-and-white images and second black-and-white images; identifying the presence of eyes based on at least one of the first black-and-white images and the second black-and-white images; and determining eye space positions based on the eyes identified in the first black-and-white images and the second black-and-white images.</p><p id="p-0023" num="0022">In some embodiments, the eye positioning method further comprises: transmitting eye space position information which indicates the eye space positions.</p><p id="p-0024" num="0023">In some embodiments, the eye positioning method further comprises: emitting infrared light by utilizing an infrared emitting apparatus, when the first black-and-white camera or the second black-and-white camera works.</p><p id="p-0025" num="0024">In some embodiments, the eye positioning method further comprises: shooting a first black-and-white image sequence comprising the first black-and-white images and a second black-and-white image sequence comprising the second black-and-white images respectively.</p><p id="p-0026" num="0025">In some embodiments, the eye positioning method further comprises: determining time-synchronized first black-and-white images and second black-and-white images.</p><p id="p-0027" num="0026">In some embodiments, the eye positioning method further comprises: buffering a plurality of first black-and-white images and second black-and-white images in the first black-and-white image sequence and the second black-and-white image sequence; comparing a plurality of previous or subsequent first black-and-white images and second black-and-white images in the first black-and-white image sequence and the second black-and-white image sequence; and taking, when the presence of eyes is not identified in a current first black-and-white image and a current second black-and-white image in the first black-and-white image sequence and the second black-and-white image sequence and the presence of eyes is identified in a previous or subsequent first black-and-white image and second black-and-white image through comparison, the eye space positions determined based on the previous or subsequent first black-and-white image and second black-and-white image as current eye space positions.</p><p id="p-0028" num="0027">In another solution, a 3D display method is provided, comprising: acquiring eye space positions of a user; determining corresponding viewpoints according to the eye space positions; and rendering subpixels, corresponding to the viewpoints, of a multi-viewpoint 3D display screen based on 3D signals.</p><p id="p-0029" num="0028">In some embodiments, the 3D display method further comprises: providing the multi-viewpoint 3D display screen, which comprises a plurality of composite pixels, wherein each composite pixel of the plurality of composite pixels comprises a plurality of composite subpixels; and each composite subpixel of the plurality of composite subpixels comprises a plurality of subpixels corresponding to the plurality of viewpoints.</p><p id="p-0030" num="0029">The computer-readable storage medium provided by the embodiments of the present disclosure stores computer-executable instructions; and the computer-executable instructions are configured to execute the eye positioning method and the 3D display method.</p><p id="p-0031" num="0030">The computer program product provided by the embodiments of the present disclosure comprises computer programs stored on a computer-readable storage medium; the computer programs comprise program instructions; and when the program instructions are executed by a computer, the computer executes the eye positioning method and the 3D display method.</p><p id="p-0032" num="0031">The above general description and the following description are exemplary and explanatory only, and are not intended to limit the present disclosure.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">DESCRIPTION OF DRAWINGS</heading><p id="p-0033" num="0032">One or more embodiments are illustrated by the corresponding drawings, and the illustrations and drawings do not limit the embodiments. Elements having the same reference numerals in the drawings are shown as similar elements, and the drawings are not intended to limit the scale, wherein:</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref> are structural schematic diagrams of a 3D display device according to embodiments of the present disclosure;</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>1</b>C</figref> is a structural schematic diagram of an eye positioning apparatus according to an embodiment of the present disclosure;</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a structural schematic diagram of hardware of a 3D display device according to an embodiment of the present disclosure;</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a structural schematic diagram of software of the 3D display device shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>;</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a schematic diagram of determining eye space positions by utilizing an eye positioning apparatus according to an embodiment of the present disclosure;</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIGS. <b>5</b>A to <b>5</b>C</figref> are front schematic diagrams of a 3D display device according to embodiments of the present disclosure;</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIGS. <b>6</b>A and <b>6</b>B</figref> are schematic diagrams of positional relationship between a face of a user and a 3D display device according to embodiments of the present disclosure;</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a schematic diagram of steps of an eye positioning method according to an embodiment of the present disclosure;</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a schematic diagram of steps of a 3D display method according to an embodiment of the present disclosure; and</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a schematic diagram of realizing the display of a multi-viewpoint 3D display screen of a 3D display device with a 3D display method according to an embodiment of the present disclosure, wherein each eye of a user corresponds to one viewpoint.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">REFERENCE NUMERALS</heading><p id="p-0044" num="0043"><b>100</b>: 3D display device; <b>101</b>: processor; <b>122</b>: register; <b>110</b>: multi-viewpoint 3D display screen; <b>120</b>: 3D shooting apparatus; <b>121</b>: camera assembly; <b>121</b><i>a</i>: first color camera; <b>121</b><i>b</i>: second color camera; <b>121</b><i>c</i>: DOF camera; <b>125</b>: 3D image output interface; <b>126</b>: 3D image processor; <b>130</b>: 3D processing apparatus; <b>131</b>: buffer; <b>140</b>: signal interface; <b>150</b>: eye positioning apparatus; <b>151</b>: eye positioner; <b>151</b><i>a</i>: first black-and-white camera; <b>151</b><i>b</i>: second black-and-white camera; <b>154</b>: infrared emitting apparatus; <b>152</b>: eye positioning image processor; <b>155</b>: synchronizer; <b>156</b>: buffer; <b>157</b>: comparer; <b>153</b>: eye positioning data interface; CP: composite pixel; CSP: composite subpixel; <b>200</b>: 3D display device; <b>201</b>: processor; <b>202</b>: external memory interface; <b>203</b>: memory; <b>204</b>: universal serial bus (USB) interface; <b>205</b>: charging management module; <b>206</b>: power management module; <b>207</b>: battery; <b>210</b>: multi-viewpoint 3D display screen; <b>212</b>: audio module; <b>213</b>: loudspeaker; <b>214</b>: phone receiver; <b>215</b>: microphone; <b>216</b>: earphone interface; <b>217</b>: button; <b>218</b>: motor; <b>219</b>: indicator; <b>220</b>: 3D shooting apparatus; <b>221</b>: camera assembly; <b>222</b>: register; <b>223</b>: graphics processing unit (GPU); <b>224</b>: codec; <b>225</b>: 3D image output interface; <b>226</b>: 3D image processor; <b>230</b>: 3D processing apparatus; <b>240</b>: signal interface; <b>250</b>: eye positioning apparatus; <b>260</b>: subscriber identity module (SIM) card interface; <b>270</b>: sensor module; <b>2701</b>: proximity light sensor; <b>2702</b>: ambient light sensor; <b>2703</b>: pressure sensor; <b>2704</b>: air pressure sensor; <b>2705</b>: magnetic sensor; <b>2706</b>: gravity sensor; <b>2707</b>: gyro sensor; <b>2708</b>: acceleration sensor; <b>2709</b>: distance sensor; <b>2710</b>: temperature sensor; <b>2711</b>: fingerprint sensor; <b>2712</b>: touch sensor; <b>2713</b>: bone conduction sensor; <b>281</b>: mobile communication module; <b>282</b>: antenna; <b>283</b>: wireless communication module; <b>284</b>: antenna; <b>310</b>: application program layer; <b>320</b>: framework layer; <b>330</b>: core class library and runtime; <b>340</b>: kernel layer; T: distance between two black-and-white cameras; <b>401</b><i>a</i>: focal plane of the first black-and-white camera <b>151</b><i>a</i>; <b>401</b><i>b</i>: focal plane of the second black-and-white camera <b>151</b><i>b</i>; f: focal length; Oa: lens center of the first black-and-white camera <b>151</b><i>a</i>; Ob: lens center of the second black-and-white camera <b>151</b><i>b</i>; Za: optical axis of the first black-and-white camera <b>151</b><i>a</i>; Zb: optical axis of the second black-and-white camera <b>151</b><i>b</i>; R: right eye of the user; L: left eye of the user; P: pupil distance of the user; &#x3b1;: tilt angle of the face of the user relative to the multi-viewpoint 3D display screen; XRa: X-axis coordinate of imaging of the right eye of the user R in the focal plane <b>401</b><i>a </i>of the first black-and-white camera <b>151</b><i>a</i>; XRb: X-axis coordinate of imaging of the right eye of the user R in the focal plane <b>401</b><i>b </i>of the second black-and-white camera <b>151</b><i>b</i>; XLa: X-axis coordinate of imaging of the left eye of the user L in the focal plane <b>401</b><i>a </i>of the first black-and-white camera <b>151</b><i>a</i>; XLb: X-axis coordinate of imaging of the left eye of the user L in the focal plane <b>401</b><i>b </i>of the second black-and-white camera <b>151</b><i>b</i>; DR: distance between the right eye of the user R and the multi-viewpoint 3D display screen; DL: distance between the left eye of the user L and the multi-viewpoint 3D display screen; <b>500</b>: 3D display device; and <b>600</b>: 3D display device.</p><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0045" num="0044">For more detailed understanding of characteristics and technical contents of embodiments of the present disclosure, the implementation of the embodiments of the present disclosure will be described in detail below with reference to the accompanying drawings, and the accompanying drawings are used for reference only, instead of limiting the embodiments of the present disclosure.</p><p id="p-0046" num="0045">In one solution, an eye positioning apparatus is provided, comprising: an eye positioner, comprising a first black-and-white camera configured to shoot first black-and-white images and a second black-and-white camera configured to shoot second black-and-white images; an eye positioning image processor, configured to identify the presence of eyes based on at least one of the first black-and-white images and the second black-and-white images and determine eye space positions based on positions of the eyes present in the first black-and-white images and the second black-and-white images; and an eye positioning data interface, configured to transmit eye space information of the eye space positions.</p><p id="p-0047" num="0046">By adopting the eye positioning apparatus, the eye space positions of the user can be determined at high accuracy.</p><p id="p-0048" num="0047">In some embodiments, the eye positioner further comprises an infrared emitting apparatus.</p><p id="p-0049" num="0048">In some embodiments, the infrared emitting apparatus is configured to emit infrared light with a wavelength greater than or equal to 1.5 microns.</p><p id="p-0050" num="0049">In some embodiments, the first black-and-white camera and the second black-and-white camera are configured to respectively shoot a first black-and-white image sequence and a second black-and-white image sequence.</p><p id="p-0051" num="0050">In some embodiments, the eye positioning image processor comprises a synchronizer, configured to determine time-synchronized first black-and-white images and second black-and-white images.</p><p id="p-0052" num="0051">In some embodiments, the eye positioning image processor comprises: a buffer, configured to buffer a plurality of first black-and-white images and second black-and-white images in the first black-and-white image sequence and the second black-and-white image sequence; and a comparer, configured to compare a plurality of previous or subsequent first black-and-white images and second black-and-white images in the first black-and-white image sequence and the second black-and-white image sequence.</p><p id="p-0053" num="0052">In some embodiments, when the presence of eyes is not identified in a current first black-and-white image and a current second black-and-white image in the first black-and-white image sequence and the second black-and-white image sequence and the presence of eyes is identified in a previous or subsequent first black-and-white image and second black-and-white image, the eye positioning image processor is configured to take eye space position information, determined based on the previous or subsequent first black-and-white image and second black-and-white image, as current eye space position information.</p><p id="p-0054" num="0053">Based on this, for example, in the case of stalling or frame skipping of the first or second black-and-white camera, a more coherent display picture can be provided for the user, to ensure the viewing experience.</p><p id="p-0055" num="0054">In some embodiments, the first black-and-white camera and the second black-and-white camera are configured to shoot the first black-and-white image sequence and the second black-and-white image sequence at a frequency of 24 fps or more.</p><p id="p-0056" num="0055">In another solution, a 3D display device is provided, comprising: a multi-viewpoint 3D display screen (such as a multi-viewpoint naked-eye 3D display screen), a video signal interface (signal interface) configured to receive video frames of 3D video signals (3D signals), a 3D processing apparatus in communication connection with the video signal interface, and an eye positioning apparatus as described above; the multi-viewpoint 3D display screen comprises a plurality of subpixels corresponding to a plurality of viewpoints; the 3D processing apparatus is configured to render subpixels relevant to predetermined viewpoints based on the video frames of the 3D video signals; and the predetermined viewpoints are determined by the eye space position information of the user.</p><p id="p-0057" num="0056">In some embodiments, the multi-viewpoint 3D display screen comprises a plurality of composite pixels; each composite pixel of the plurality of composite pixels comprises a plurality of composite subpixels; and each composite subpixel comprises a plurality of homochromatic subpixels corresponding to the plurality of viewpoints.</p><p id="p-0058" num="0057">In some embodiments, the 3D processing apparatus is in communication connection with an eye positioning data interface of the eye positioning apparatus.</p><p id="p-0059" num="0058">In some embodiments, the 3D display device further comprises a 3D shooting apparatus configured to collect 3D images; the 3D shooting apparatus comprises a camera assembly and a 3D image processor; and the camera assembly comprises a first color camera, a second color camera, and a DOF camera.</p><p id="p-0060" num="0059">In some embodiments, the eye positioning apparatus is integrated with the 3D shooting apparatus.</p><p id="p-0061" num="0060">In some embodiments, the 3D shooting apparatus is a front-mounted shooting apparatus.</p><p id="p-0062" num="0061">In another solution, an eye positioning method is provided, comprising: shooting first black-and-white images at a first position; shooting second black-and-white images at a second position, wherein the second position is different from the first position; identifying the presence of eyes based on at least one of the first black-and-white images and the second black-and-white images; determining eye space positions based on positions of the eyes present in the first black-and-white images and the second black-and-white images; and transmitting eye space position information of the eye space positions.</p><p id="p-0063" num="0062">In some embodiments, the eye positioning method further comprises: emitting infrared light by utilizing an infrared emitting apparatus, when the first black-and-white camera or the second black-and-white camera works.</p><p id="p-0064" num="0063">In some embodiments, the eye positioning method further comprises: shooting a first black-and-white image sequence and a second black-and-white image sequence respectively.</p><p id="p-0065" num="0064">In some embodiments, the eye positioning method further comprises: determining time-synchronized first black-and-white images and second black-and-white images.</p><p id="p-0066" num="0065">In some embodiments, the eye positioning method further comprises: buffering a plurality of first black-and-white images and second black-and-white images in the first black-and-white image sequence and the second black-and-white image sequence; and comparing a plurality of previous or subsequent first black-and-white images and second black-and-white images in the first black-and-white image sequence and the second black-and-white image sequence.</p><p id="p-0067" num="0066">In some embodiments, the eye positioning method further comprises: taking, when the presence of eyes is not identified in a current first black-and-white image and a current second black-and-white image in the first black-and-white image sequence and the second black-and-white image sequence and the presence of eyes is identified in a previous or subsequent first black-and-white image and second black-and-white image, eye space position information determined based on the previous or subsequent first black-and-white image and second black-and-white image as current eye space position information.</p><p id="p-0068" num="0067">In some embodiments, the eye positioning method further comprises: shooting a first black-and-white image sequence and a second black-and-white image sequence at a frequency of 24 fps or more.</p><p id="p-0069" num="0068">In another solution, a 3D display method is provided, which is suitable for a 3D display device. The 3D display device comprises a multi-viewpoint 3D display screen, comprising a plurality of subpixels corresponding to a plurality of viewpoints; and the 3D display method comprises: transmitting video frames of 3D video signals; receiving or reading eye space position information of the user, wherein the eye space position information is determined by the above eye positioning method; determining viewpoints, at which the eyes are located, based on the eye space position information; and rendering relevant subpixels according to the received video frames of the 3D video signals, based on the viewpoints.</p><p id="p-0070" num="0069">In some embodiments, the 3D display method further comprises: providing the multi-viewpoint display screen, which comprises a plurality of composite pixels, wherein each composite pixel of the plurality of composite pixels comprises a plurality of composite subpixels; and each composite subpixel comprises a plurality of homochromatic subpixels corresponding to a plurality of viewpoints.</p><p id="p-0071" num="0070">In another solution, a 3D display device is provided, comprising a processor and a memory storing program instructions, and further comprising a multi-viewpoint 3D display screen, wherein the processor is configured to execute the above 3D display method when executing the program instructions.</p><p id="p-0072" num="0071"><figref idref="DRAWINGS">FIG. <b>1</b>A</figref> shows a structural schematic diagram of a 3D display device <b>100</b> according to an embodiment of the present disclosure. Referring to <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>, an embodiment of the present disclosure provides the 3D display device <b>100</b>, which comprises a multi-viewpoint 3D display screen <b>110</b>, a signal interface <b>140</b> configured to receive video frames of 3D video signals, a 3D processing apparatus <b>130</b> in communication connection with the signal interface <b>140</b>, and an eye positioning apparatus <b>150</b>. The eye positioning apparatus <b>150</b> is in communication connection to the 3D processing apparatus <b>130</b>, so that the 3D processing apparatus <b>130</b> may directly receive eye positioning data.</p><p id="p-0073" num="0072">The multi-viewpoint 3D display screen <b>110</b> may comprise a display panel and gratings (not shown) covering the display panel. In an embodiment shown in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>, the multi-viewpoint 3D display screen <b>110</b> may comprise m&#xd7;n composite pixels CP and thus define a display resolution of m&#xd7;n. In an embodiment shown in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>, the multi-viewpoint 3D display screen <b>110</b> may comprise m columns and n rows of composite pixels and thus define a display resolution of m&#xd7;n.</p><p id="p-0074" num="0073">In some embodiments, the resolution of m&#xd7;n may be higher than full high definition (FHD), comprising but not limited to 1920&#xd7;1080, 1920&#xd7;1200, 2048&#xd7;1280, 2560&#xd7;1440, 3840&#xd7;2160 and the like.</p><p id="p-0075" num="0074">In some embodiments, the 3D processing apparatus is in communication connection with the multi-viewpoint 3D display screen.</p><p id="p-0076" num="0075">In some embodiments, the 3D processing apparatus is in communication connection with a driving apparatus of the multi-viewpoint 3D display screen.</p><p id="p-0077" num="0076">By way of explanation but not limitation, each composite pixel CP comprises a plurality of composite subpixels CSP; each composite subpixel comprises i homochromatic subpixels corresponding to i viewpoints; and i&#x2265;3. In the embodiments shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, i=6; but conceivably, i may be other values. In the illustrated embodiments, the multi-viewpoint 3D display screen may have i (i=6) viewpoints (V1-V6) correspondingly, but conceivably may have more or fewer viewpoints correspondingly.</p><p id="p-0078" num="0077">By way of explanation but not limitation, in the embodiments shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, each composite pixel comprises three composite subpixels; and each composite subpixel comprises six homochromatic subpixels corresponding to six viewpoints (i=6). The three composite subpixels respectively correspond to three colors, i.e., red (R), green (G) and blue (B). In other words, the three composite subpixels of each composite pixel respectively have six red subpixels, six green subpixels or six blue subpixels. In the embodiments shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, all composite subpixels in the composite pixels are arranged in parallel. Each composite subpixel comprises subpixels in a single row. However, different arrangements of the composite subpixels in the composite pixels or different arrangements of the subpixels in the composite subpixels are conceivable; for example, each composite subpixel comprises subpixels in a single row or an array form.</p><p id="p-0079" num="0078">By way of explanation but not limitation, for example, as shown in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>, the 3D display device <b>100</b> may be provided with a single 3D processing apparatus <b>130</b>. The single 3D processing apparatus <b>130</b> simultaneously processes the rendering of subpixels of each composite subpixel of each composite pixel of the 3D display screen <b>110</b>. In other embodiments, the 3D display device <b>100</b> may also be provided with more than one 3D processing apparatus <b>130</b>, which processes the rendering of subpixels of each composite subpixel of each composite pixel of the 3D display screen <b>110</b> in parallel, series or a combination of series and parallel. Persons skilled in the field will understand that more than one 3D processing apparatus may be allocated in other ways and process multiple rows and columns of composite pixels or composite subpixels of the 3D display screen <b>110</b> in parallel, which falls within the scope of embodiments of the present disclosure.</p><p id="p-0080" num="0079">In some embodiments, the 3D processing apparatus <b>130</b> may optionally comprise a buffer <b>131</b>, to buffer the received video frames.</p><p id="p-0081" num="0080">In some embodiments, the 3D processing apparatus is an FPGA or ASIC chip or an FPGA or ASIC chipset.</p><p id="p-0082" num="0081">Continuing to refer to <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>, the 3D display device <b>100</b> may further comprise a processor <b>101</b> in communication connection to the 3D processing apparatus <b>130</b> through the signal interface <b>140</b>. In some embodiments shown herein, the processor <b>101</b> is contained in a computer or an intelligent terminal such as a mobile terminal, or serves as a processor unit. However, conceivably, in some embodiments, the processor <b>101</b> may be arranged outside the 3D display device, for example, the 3D display device may be a multi-viewpoint 3D display with a 3D processing apparatus, such as a non-smart 3D TV.</p><p id="p-0083" num="0082">For the sake of simplicity, the following exemplary embodiment of the 3D display device internally comprises a processor. Based on this, the signal interface <b>140</b> is configured as an internal interface for connecting the processor <b>101</b> with the 3D processing apparatus <b>130</b>; and the structure may be more clearly defined with reference to the 3D display device <b>200</b> implemented as a mobile terminal shown in <figref idref="DRAWINGS">FIGS. <b>2</b> and <b>3</b></figref>. In some embodiments shown herein, the signal interface as the internal interface of the 3D display device may be a mobile industry processor interface (MIPI), a mini-MIPI, a low voltage differential signaling (LVDS) interface, a min-LVDS interface or a Display Port interface. In some embodiments, as shown in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>, the processor <b>101</b> of the 3D display device <b>100</b> may further comprise a register <b>122</b>. The register <b>122</b> may be configured to temporarily store instructions, data and addresses. In some embodiments, the register <b>122</b> may be configured to receive information about display requirements of the multi-viewpoint 3D display screen <b>110</b>.</p><p id="p-0084" num="0083">In some embodiments, the 3D display device <b>100</b> may further comprise a codec, configured to decompress and encode compressed 3D video signals and transmit the decompressed 3D video signals to the 3D processing apparatus <b>130</b> through the signal interface <b>140</b>.</p><p id="p-0085" num="0084">Referring to <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>, an embodiment shown in <figref idref="DRAWINGS">FIG. <b>1</b>B</figref> is different from an embodiment shown in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref> in that, the 3D display device <b>100</b> further comprises a 3D shooting apparatus <b>120</b> configured to collect 3D images, the eye positioning apparatus <b>150</b> is integrated in the 3D shooting apparatus <b>120</b> and may also be integrated into a conventional shooting apparatus of a processing terminal or a display device. As shown in <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>, the 3D shooting apparatus <b>120</b> is configured as a front-mounted shooting apparatus. The 3D shooting apparatus <b>120</b> comprises a camera assembly <b>121</b>, a 3D image processor <b>126</b>, and a 3D image output interface <b>125</b>. The 3D shooting apparatus <b>120</b> is integrated with the eye positioning apparatus <b>150</b>.</p><p id="p-0086" num="0085">As shown in <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>, the camera assembly <b>121</b> comprises a first color camera <b>121</b><i>a</i>, a second color camera <b>121</b><i>b</i>, and a DOF camera <b>121</b><i>c</i>. In other embodiments not shown, the 3D image processor <b>126</b> may be integrated into the camera assembly <b>121</b>. In some embodiments, the first color camera <b>121</b><i>a </i>is configured to acquire a first color image of a shot object, the second color camera <b>121</b><i>b </i>is configured to acquire a second color image of the shot object, and the two color images are composited to acquire a composite color image of a middle point; and the DOF camera <b>121</b><i>c </i>is configured to acquire a DOF image of the shot object. The composite color image and the DOF image form a video frame of a 3D video signal. In embodiments of the present disclosure, the first color camera and the second color camera are the same color camera. In other embodiments, the first color camera and the second color camera may be different color cameras. In this case, in order to acquire a color composite image, the first color image and the second color image may be calibrated or corrected. The DOF camera <b>121</b><i>c </i>may be a time-of-flight (TOF) camera or a structured light camera. The DOF camera <b>121</b><i>c </i>may be arranged between the first color camera and the second color camera.</p><p id="p-0087" num="0086">In some embodiments, the 3D image processor <b>126</b> is configured to composite the first color image and the second color image into a composite color image, and form a 3D image from the composite color image and the DOF image. The formed 3D image is transmitted to the processor <b>101</b> of the 3D display device <b>100</b> through the 3D image output interface <b>125</b>.</p><p id="p-0088" num="0087">Optionally, the first color image and the second color image as well as the DOF image are directly transmitted to the processor <b>101</b> of the 3D display device <b>100</b> through the 3D image output interface <b>125</b>; and the processes of compositing the color image and forming the 3D image are performed by the processor <b>101</b>.</p><p id="p-0089" num="0088">Optionally, the 3D image output interface <b>125</b> may also be in communication connection to the 3D processing apparatus <b>130</b> of the 3D display device <b>100</b>, so that the processes of compositing the color image and forming the 3D image may be performed by the 3D processing apparatus <b>130</b>.</p><p id="p-0090" num="0089">In some embodiments, at least one of the first color camera and the second color camera is a wide-angle color camera.</p><p id="p-0091" num="0090">Continuing to refer to <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>, the eye positioning apparatus <b>150</b> is integrated in the 3D shooting apparatus <b>120</b> and comprises an eye positioner <b>151</b>, an eye positioning image processor <b>152</b> and an eye positioning data interface <b>153</b>.</p><p id="p-0092" num="0091">The eye positioner <b>151</b> comprises a first black-and-white camera <b>151</b><i>a </i>and a second black-and-white camera <b>151</b><i>b</i>. The first black-and-white camera <b>151</b><i>a </i>is configured to shoot first black-and-white images; and the second black-and-white camera <b>151</b><i>b </i>is configured to shoot second black-and-white images. When the 3D shooting apparatus <b>120</b> is front-mounted and the eye positioning apparatus <b>150</b> is integrated in the 3D shooting apparatus <b>120</b>, the eye positioning apparatus <b>150</b> is also front-mounted, and the shot object of the first black-and-white camera and the second black-and-white camera is a face of the user.</p><p id="p-0093" num="0092">In some embodiments, the eye positioning data interface <b>153</b> of the eye positioning apparatus <b>150</b> is in communication connection to the 3D processing apparatus <b>130</b> of the 3D display device <b>100</b>, so that the 3D processing apparatus <b>130</b> may directly receive eye positioning data. In other embodiments, the eye positioning image processor <b>152</b> of the eye positioning apparatus <b>150</b> may be in communication connection to the processor <b>101</b> of the 3D display device <b>100</b>, so that the eye positioning data may be transmitted from the processor <b>101</b> to the 3D processing apparatus <b>130</b> through the eye positioning data interface <b>153</b>.</p><p id="p-0094" num="0093">In some embodiments, the eye positioning apparatus <b>150</b> is in communication connection with a camera assembly <b>221</b>, so that the eye positioning data may be used when the 3D images are shot.</p><p id="p-0095" num="0094">Optionally, the eye positioner <b>151</b> is further provided with an infrared emitting apparatus <b>154</b>. When the first black-and-white camera or the second black-and-white camera works, the infrared emitting apparatus <b>154</b> is configured to selectively emit infrared light, to play a role of supplementing light when the ambient light is insufficient, for example, when shooting at night, so that the first black-and-white images or the second black-and-white images available for identifying the face and eyes of the user can also be shot even under the condition of weak ambient light.</p><p id="p-0096" num="0095">In some embodiments, the eye positioning apparatus <b>150</b> or the processing terminal or display device integrated with the eye positioning apparatus may be configured to, when the first black-and-white camera or the second black-and-white camera works, based on a received light sensing signal, for example, control the turn-on or adjust the size of the infrared emitting apparatus when the light sensing signal is detected to be lower than a given threshold. In some embodiments, the light sensing signal is received from an ambient light sensor, integrated in the processing terminal or the display device, such as an ambient light sensor <b>2702</b>.</p><p id="p-0097" num="0096">Optionally, the infrared emitting apparatus <b>154</b> is configured to emit infrared light with a wavelength greater than or equal to 1.5 microns, i.e., long-wave infrared light. Compared with short-wave infrared light, the ability of the long-wave infrared light to penetrate the skin is weak, so the long-wave infrared light is less harmful to the eyes.</p><p id="p-0098" num="0097">The shot first black-and-white images and second black-and-white images are transmitted to the eye positioning image processor <b>152</b>. Exemplarily, the eye positioning image processor is configured to have a visual identification function, such as a face identification function, and is configured to identify the face and the eyes based on at least one of the two black-and-white images and to determine eye space positions based on positions of eyes present in the two black-and-white images. In embodiments of the present disclosure, the first black-and-white camera and the second black-and-white camera are the same black-and-white camera. In other embodiments, the first black-and-white camera and the second black-and-white camera may be different black-and-white cameras. In this case, in order to determine the eye space position, the first black-and-white image and the second black-and-white image may be calibrated or corrected.</p><p id="p-0099" num="0098">In some embodiments, at least one of the first black-and-white camera and the second black-and-white camera is a wide-angle black-and-white camera.</p><p id="p-0100" num="0099"><figref idref="DRAWINGS">FIG. <b>4</b></figref> schematically shows a vertical view of a geometric relationship model for determining eye space positions by using two black-and-white cameras. In an embodiment shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the first black-and-white camera and the second black-and-white camera are the same black-and-white camera, thereby having the same focal length f; an optical axis Za of the first black-and-white camera <b>151</b><i>a </i>is parallel to an optical axis Zb of the second black-and-white camera <b>151</b><i>b</i>; and a focal plane <b>401</b><i>a </i>of the first black-and-white camera <b>151</b><i>a </i>and a focal plane <b>401</b><i>b </i>of the second black-and-white camera <b>151</b><i>b </i>are in the same plane and perpendicular to the optical axes of the two black-and-white cameras. Based on the above arrangements, a connecting line of lens centers Oa and Ob of the two black-and-white cameras is parallel to the focal planes of the two black-and-white cameras. In the embodiment shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, a vertical view of a geometric relationship model of an XZ plane is shown by taking a direction of the connecting line of the lens centers Oa to Ob of two black-and-white cameras as an X-axis direction and a direction of the optical axes of the two black-and-white cameras as a Z-axis direction.</p><p id="p-0101" num="0100">In the embodiment shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the lens center Oa of the first black-and-white camera <b>151</b><i>a </i>is taken as an origin, and the lens center Ob of the second black-and-white camera <b>151</b><i>b </i>is taken as an origin. R and L respectively represent a right eye and a left eye of the user; XRa and XRb respectively represent X-axis coordinates of imaging of the right eye R of the user in the focal planes <b>401</b><i>a </i>and <b>401</b><i>b </i>of the two black-and-white cameras; and XLa and XLb respectively represent X-axis coordinates of imaging of the left eye L of the user in the focal planes <b>401</b><i>a </i>and <b>401</b><i>b </i>of the two black-and-white cameras. In addition, a distance T between the two black-and-white cameras and the focal lengths f of the two cameras are also known. According to a geometric relationship of similar triangles, distances DR and DL between the right eye R and the left eye L and the planes, in which the two black-and-white cameras set as above are, may be respectively solved as follows:</p><p id="p-0102" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mrow>  <mrow>   <mi>DR</mi>   <mo>=</mo>   <mfrac>    <mrow>     <mi>T</mi>     <mo>&#xb7;</mo>     <mi>f</mi>    </mrow>    <mrow>     <mi>XRb</mi>     <mo>-</mo>     <mi>XRa</mi>    </mrow>   </mfrac>  </mrow>  <mo>&#x2062;</mo>  <mtext></mtext>  <mrow>   <mi>DL</mi>   <mo>=</mo>   <mfrac>    <mrow>     <mi>T</mi>     <mo>&#xb7;</mo>     <mi>f</mi>    </mrow>    <mrow>     <mi>XLb</mi>     <mo>-</mo>     <mi>XLa</mi>    </mrow>   </mfrac>  </mrow> </mrow></math></maths></p><p id="p-0103" num="0101">In addition, a tilt angle &#x3b1; formed by a connecting line between both eyes of the user and the planes, in which the two black-and-white cameras set as above are, and a distance or a pupil distance P between both eyes of the user may be respectively solved as follows:</p><p id="p-0104" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mrow>  <mrow>   <mi>&#x3b1;</mi>   <mo>=</mo>   <mrow>    <mi>arctan</mi>    <mo>(</mo>    <mfrac>     <mrow>      <mi>DL</mi>      <mo>-</mo>      <mi>DR</mi>     </mrow>     <mrow>      <mfrac>       <mrow>        <mrow>         <mi>XLb</mi>         <mo>&#xb7;</mo>         <mi>DL</mi>        </mrow>        <mo>-</mo>        <mrow>         <mi>XRa</mi>         <mo>&#xb7;</mo>         <mi>DR</mi>        </mrow>       </mrow>       <mi>f</mi>      </mfrac>      <mo>+</mo>      <mi>T</mi>     </mrow>    </mfrac>    <mo>)</mo>   </mrow>  </mrow>  <mo>&#x2062;</mo>  <mtext></mtext>  <mrow>   <mi>P</mi>   <mo>=</mo>   <mfrac>    <mrow>     <mi>DL</mi>     <mo>-</mo>     <mi>DR</mi>    </mrow>    <mrow>     <mi>sin</mi>     <mo>&#x2062;</mo>     <mi>&#x3b1;</mi>    </mrow>   </mfrac>  </mrow> </mrow></math></maths></p><p id="p-0105" num="0102">In the embodiment shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the connecting line between both eyes of the user, i.e., the face of the user, and the planes, in which the two black-and-white cameras set as above are, are tilted to each other, and the tilt angle is a; when the face of the user is parallel to the planes, in which the two black-and-white cameras set as above are, i.e., when the user looks at the two black-and-white cameras head-on, the tilt angle &#x3b1; is zero.</p><p id="p-0106" num="0103">As described above, in some embodiments herein, the 3D display device <b>100</b> may be a computer or an intelligent terminal, such as a mobile terminal. However, conceivably, in some embodiments, the 3D display device <b>100</b> may also be a non-smart display terminal, such as a non-smart 3D TV. <figref idref="DRAWINGS">FIGS. <b>5</b>A, <b>5</b>B and <b>5</b>C</figref> show schematic diagrams of a 3D display device <b>500</b>, which is respectively configured as a smart phone, a tablet personal computer (PC) and a non-smart display, and has a multi-viewpoint 3D display screen <b>510</b> and a front-mounted 3D shooting apparatus which is integrated with an eye positioning apparatus. In embodiments shown in <figref idref="DRAWINGS">FIGS. <b>5</b>A to <b>5</b>C</figref>, the 3D shooting apparatus <b>120</b> comprising two color cameras <b>121</b><i>a </i>and <b>121</b><i>b </i>and the DOF camera <b>121</b><i>c </i>and the integrated eye positioning apparatus <b>150</b> comprising two black-and-white cameras <b>151</b><i>a </i>and <b>151</b><i>b </i>are arranged in the same plane as the multi-viewpoint 3D display screen <b>510</b> of the 3D display device <b>500</b>. Therefore, in the embodiment shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the distances DR and DL between the right eye R and left eye L of the user and the planes, in which the two black-and-white cameras set as above are, are distances between the right eye R and left eye L of the user and the multi-viewpoint 3D display screen; and the tilt angle &#x3b1; formed by the face of the user and the planes, in which the two black-and-white cameras set as above are, is a tilt angle formed between the face of the user and the multi-viewpoint 3D display screen.</p><p id="p-0107" num="0104">Referring to <figref idref="DRAWINGS">FIG. <b>6</b>A</figref>, a schematic diagram of the user looking at a multi-viewpoint 3D display screen of a 3D display device <b>600</b> squarely or head-on is shown, i.e., a plane, in which the face of the user is, and a plane, in which the display screen is, are parallel to each other; the distances DR and DL between both eyes of the user and the display screen are the same; and the tilt angle &#x3b1; is zero.</p><p id="p-0108" num="0105">Referring to <figref idref="DRAWINGS">FIG. <b>6</b>B</figref>, a schematic diagram of the face of the user tilts relative to the multi-viewpoint 3D display screen of the 3D display device <b>600</b> is shown, i.e., the plane, in which the face of the user is, and the plane, in which the display screen is, are not parallel to each other; the distances DR and DL between both eyes of the user and the display screen are different; and the tilt angle &#x3b1; is not zero.</p><p id="p-0109" num="0106">In some embodiments, the eye positioning data interface <b>153</b> is configured to transmit the tilt angle or parallelism of both eyes of the user relative to the eye positioning apparatus <b>150</b> or the multi-viewpoint 3D display screen <b>110</b>. This may make for more accurately presenting the 3D images, which will be described hereinafter.</p><p id="p-0110" num="0107">For example, the eye space position information DR, DL, a and P obtained as an example above is transmitted to the 3D processing apparatus <b>130</b> through the eye positioning data interface <b>153</b>. The 3D processing apparatus <b>130</b>, based on the received eye space position information, determines viewpoints, at which both eyes of the user are located and which are provided by the multi-viewpoint 3D display screen <b>110</b>, i.e., predetermined viewpoints.</p><p id="p-0111" num="0108">For example, the eye space position information DR, DL, a and P obtained as an example above may also be directly transmitted to the processor <b>101</b> of the 3D display device <b>100</b>; and the 3D processing apparatus <b>130</b> receives/reads the eye space position information from the processor <b>101</b> through the eye positioning data interface <b>153</b>.</p><p id="p-0112" num="0109">In some embodiments, the first black-and-white camera <b>151</b><i>a </i>is configured to shoot a first black-and-white image sequence, which comprises a plurality of first black-and-white images arranged in time sequence; and the second black-and-white camera <b>151</b><i>b </i>is configured to shoot a second black-and-white image sequence, which comprises a plurality of second black-and-white images arranged in time sequence.</p><p id="p-0113" num="0110">In some embodiments, the eye positioning image processor <b>152</b> comprises a synchronizer <b>155</b>, configured to determine time-synchronized first black-and-white images and second black-and-white images in the first black-and-white image sequence and the second black-and-white image sequence. The first black-and-white images and the second black-and-white images, which are determined to be time-synchronized, are used for identification of the eyes and determination of the eye space positions.</p><p id="p-0114" num="0111">In some embodiments, the eye positioning image processor <b>152</b> comprises a buffer <b>156</b> and a comparer <b>157</b>. The buffer <b>156</b> is configured to buffer a plurality of first black-and-white images and second black-and-white images, respectively arranged in time sequence, in the first black-and-white image sequence and the second black-and-white image sequence. The comparer <b>157</b> is configured to compare a plurality of first black-and-white images and second black-and-white images, shot in time sequence, in the first black-and-white image sequence and the second black-and-white image sequence. By comparison, for example, it may be judged whether the eye space positions are changed or whether the eyes are still in a viewing range.</p><p id="p-0115" num="0112">In some embodiments, the eye positioning image processor <b>152</b> further comprises a determiner (not shown), wherein when the comparer does not identify the presence of eyes in a current first black-and-white image and a current second black-and-white image in the first black-and-white image sequence and the second black-and-white image sequence through comparison and identifies the presence of eyes in a previous or subsequent first black-and-white image and second black-and-white image, the determiner is configured to take eye space positions, determined based on the previous or subsequent first black-and-white image and second black-and-white image, as current eye space positions. This is a case, for example, when the user turns his head briefly. In this case, the face and eyes of the user may not be identified for a short time.</p><p id="p-0116" num="0113">Exemplarily, several first black-and-white images and second black-and-white images in the first black-and-white image sequence and second black-and-white image sequence are stored in a buffer section of the buffer <b>156</b>. In some cases, the face and eyes cannot be identified from the buffered current first black-and-white image and second black-and-white image; but the face and eyes may be identified from the buffered previous or subsequent first black-and-white image and second black-and-white image. In this case, eye space position information, determined based on a first black-and-white image and second black-and-white image subsequent to, i.e., shot later after the current first black-and-white image and second black-and-white image, is taken as current eye space position information; and eye space position information, determined based on a first black-and-white image and second black-and-white image previous to, i.e., shot earlier before the current first black-and-white image and second black-and-white image, is taken as the current eye space position information. In addition, the eye space position information determined based on the above previous and subsequent first black-and-white images and second black-and-white images available for identifying the face and the eyes may be averaged, data-fitted, interpolated or processed by other methods; and the obtained results may be taken as the current eye space position information.</p><p id="p-0117" num="0114">In some embodiments, the first black-and-white camera and the second black-and-white camera are configured to shoot the first black-and-white image sequence and the second black-and-white image sequence at a frequency of 24 fps or more. Exemplarily, the sequences are shot at a frequency of 30 fps. Exemplarily, the sequences are shot at a frequency of 60 fps.</p><p id="p-0118" num="0115">In some embodiments, the first black-and-white camera and the second black-and-white camera are configured to shoot at the same frequency as a refresh frequency of the display screen of the 3D display device.</p><p id="p-0119" num="0116">As described above, the 3D display device provided by embodiments of the present disclosure may be a 3D display device containing a processor. In some embodiments, the 3D display device may be configured as a smart cell phone, a tablet PC, a smart TV, a wearable device, a vehicle-mounted device, a notebook computer, an ultra-mobile personal computer (UMPC), a netbook, a personal digital assistant (PDA) and the like.</p><p id="p-0120" num="0117">Exemplarily, <figref idref="DRAWINGS">FIG. <b>2</b></figref> shows a structural schematic diagram of hardware of a 3D display device <b>200</b> implemented as a mobile terminal, such as a tablet PC or a smart cell phone. The 3D display device <b>200</b> may comprise a processor <b>201</b>, an external memory interface <b>202</b>, an (internal) memory <b>203</b>, a USB interface <b>204</b>, a charging management module <b>205</b>, a power management module <b>206</b>, a battery <b>207</b>, a mobile communication module <b>281</b>, a wireless communication module <b>283</b>, antennas <b>282</b> and <b>284</b>, an audio module <b>212</b>, a loudspeaker <b>213</b>, a receiver <b>233</b>, a microphone <b>215</b>, an earphone interface <b>216</b>, a button <b>217</b>, a motor <b>218</b>, an indicator <b>219</b>, an SIM card interface <b>260</b>, a multi-viewpoint 3D display screen <b>210</b>, a 3D processing apparatus <b>230</b>, a signal interface <b>240</b>, a 3D shooting apparatus <b>220</b>, a sensor module <b>230</b> and the like. The 3D shooting apparatus <b>220</b> may comprise a camera assembly <b>221</b>, a 3D image output interface <b>225</b> and an eye positioning apparatus <b>250</b>. The sensor module <b>270</b> may comprise a proximity light sensor <b>2701</b>, an ambient light sensor <b>2702</b>, a pressure sensor <b>2703</b>, an air pressure sensor <b>2704</b>, a magnetic sensor <b>2705</b>, a gravity sensor <b>2706</b>, a gyro sensor <b>2707</b>, an acceleration sensor <b>2708</b>, a distance sensor <b>2709</b>, a temperature sensor <b>2710</b>, a fingerprint sensor <b>2711</b>, a touch sensor <b>2712</b>, a bone conduction sensor <b>2713</b> and the like.</p><p id="p-0121" num="0118">Understandably, the schematic structures of embodiments of the present disclosure do not constitute a limitation on the 3D display device <b>200</b>. In other embodiments of the present disclosure, the 3D display device <b>200</b> may comprise more or fewer components than shown in diagrams, or combine some components, or split some components, or use different component arrangements. The components shown in the diagrams may be implemented by hardware, software or a combination of software and hardware.</p><p id="p-0122" num="0119">The processor <b>201</b> may comprise one or more processing units, for example, the processor <b>201</b> may comprise an application processor (AP), a modem processor, a baseband processor, a register <b>222</b>, a GPU <b>223</b>, an image signal processor (ISP), a controller, a memory, a video codec <b>224</b>, a digital signal processor (DSP), a baseband processor, a neural network processor (NPU) or a combination thereof. Different processing units may be independent elements, and may also be integrated in one or more processors.</p><p id="p-0123" num="0120">The processor <b>201</b> may further be provided with a cache, configured to store instructions or data just used or recycled by the processor <b>201</b>. When the processor <b>201</b> intends to reuse the instructions or data, the instructions or data can be directly called from the memory.</p><p id="p-0124" num="0121">In some embodiments, the processor <b>201</b> may comprise one or more interfaces. Interfaces may comprise an integrated circuit (I2C) interface, an integrated circuit built-in audio (I2S) interface, a pulse code modulation (PCM) interface, a universal asynchronous receiver-transmitter (UART) interface, a mobile industry processor interface (MIPI), a general purpose input-output (GPIO) interface, an SIM interface, a USB interface and the like.</p><p id="p-0125" num="0122">The I2C interface is a bidirectional synchronous serial bus, and comprises a serial data line (SDA) and a serial clock line (SCL). In some embodiments, the processor <b>201</b> may comprise multiple groups of I2C buses. The processor <b>201</b> may be in communication connection with a touch sensor <b>2712</b>, a charger, a flash lamp, the 3D shooting apparatus <b>220</b> or the camera assembly <b>221</b>, the eye positioning apparatus <b>250</b> and the like through different I2C bus interfaces, respectively.</p><p id="p-0126" num="0123">Both the I2S interface and the PCM interface may be used for audio communication.</p><p id="p-0127" num="0124">The UART interface is a universal serial data bus, used for asynchronous communication. The bus may be a bidirectional communication bus. The bus converts to-be-transmitted data between serial communication and parallel communication. In some embodiments, the DART interface is configured to connect the processor <b>201</b> with the wireless communication module <b>283</b>.</p><p id="p-0128" num="0125">In an embodiment shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the MIPI may be configured to connect the processor <b>201</b> with the multi-viewpoint 3D display screen <b>202</b>. In addition, the MIPI may further be configured to connect peripheral elements, such as the camera assembly <b>221</b> and the eye positioning apparatus <b>250</b>.</p><p id="p-0129" num="0126">The GPIO interface may be configured by software. The GPIO interface may be configured as a control signal, and may also be configured as a data signal. In some embodiments, the GPIO interface may be configured to connect the processor <b>201</b> with the 3D shooting apparatus <b>220</b>, the multi-viewpoint 3D display screen <b>110</b>, the wireless communication module <b>283</b>, the audio module <b>212</b>, the sensor module <b>270</b> and the like.</p><p id="p-0130" num="0127">The USB interface <b>204</b> is an interface compliant with USB standard specifications, and may be a Mini USB interface, a Micro USB interface, a USB Type C interface or the like. The USB interface <b>204</b> may be configured to connect with the charger to charge the 3D display device <b>200</b>, and may also be used for transmitting data between the 3D display device <b>200</b> and the peripheral devices. The USB interface <b>204</b> may also be configured to connect with earphones, and play audio through the earphones.</p><p id="p-0131" num="0128">Understandably, an interface connection relationship, among various modules, illustrated in embodiments of the present disclosure is only used for schematic illustration, without constructing structural limitation to the 3D display device <b>200</b>.</p><p id="p-0132" num="0129">A wireless communication function of the 3D display device <b>200</b> may be realized by the antennas <b>282</b> and <b>284</b>, the mobile communication module <b>281</b>, the wireless communication module <b>283</b>, the modem processor, the baseband processor or the like.</p><p id="p-0133" num="0130">The antennas <b>282</b> and <b>284</b> are configured to transmit and receive electromagnetic wave signals. Each antenna in the 3D display device <b>200</b> may be configured to cover a single or multiple communication frequency bands. Different antennas may further be reused, to improve a utilization rate of antennas.</p><p id="p-0134" num="0131">The mobile communication module <b>281</b> may provide solutions for wireless communication, comprising 2G/3G/4G/5G, applied to the 3D display device <b>200</b>. The mobile communication module <b>281</b> may comprise at least one filter, a switch, a power amplifier, a low noise amplifier (LNA) and the like. The mobile communication module <b>281</b> may receive electromagnetic waves through the antenna <b>282</b>, filter and amplify the received electromagnetic waves, and transmit the processed electromagnetic waves to the modem processor for demodulation. The mobile communication module <b>281</b> may further amplify a signal modulated by the modem processor, and then convert the amplified signal into an electromagnetic wave through the antenna <b>282</b> for radiation. In some embodiments, at least part of functional modules of the mobile communication module <b>281</b> may be arranged in the processor <b>201</b>. In some embodiments, at least part of functional modules of the mobile communication module <b>282</b> may be arranged in the same element together with at least part of modules of the processor <b>201</b>.</p><p id="p-0135" num="0132">The wireless communication module <b>283</b> may provide solutions for wireless communication, comprising a wireless local area network (WLAN), a Bluetooth (BT), a global navigation satellite system (GNSS), a frequency modulation (FM), a near field communication technology (NFC), an infrared technology (IR) and the like, applied to the 3D display device <b>200</b>. The wireless module <b>283</b> may be one or more elements for integrating at least one communication processing module. The wireless communication module <b>283</b> receives an electromagnetic wave through the antenna <b>284</b>, modulates and filters an electromagnetic wave signal, and transmits the processed signal to the processor <b>201</b>. The wireless communication module <b>283</b> may further receive a to-be-transmitted signal from the processor <b>201</b>, modulate and amplify the received signal, and convert the processed signal into an electromagnetic wave through the antenna <b>284</b> for radiation.</p><p id="p-0136" num="0133">In some embodiments, the antenna <b>282</b> of the 3D display device <b>200</b> is coupled with the mobile communication module <b>281</b>, and the antenna <b>284</b> is coupled with the wireless communication module <b>283</b>, so that the 3D display device <b>200</b> may communicate with the network and other devices through wireless communication technology. The wireless communication technology may comprise at least one of global system for mobile communications (GSM), general packet radio service (GPRS), code division multiple access (CDMA), wideband code division multiple access (WCDMA), time division code division multiple access (TD-SCDMA), long term evolution (LTE), BT, GNSS, WLAN, NFC, FM and IR technologies. The GNSS may comprise at least one of a global positioning system (GPS), a global navigation satellite system (GLONASS), a Beidou satellite navigation system (BDS), a quasi-zenith satellite system (QZSS), and a satellite-based augmentation system (SBAS).</p><p id="p-0137" num="0134">In some embodiments, the external interface configured to receive 3D video signals may comprise the USB interface <b>204</b>, the mobile communication module <b>281</b>, the wireless communication module <b>283</b>, or a combination thereof. In addition, other possible interfaces configured to receive 3D video signals, such as the above interfaces, are conceivable.</p><p id="p-0138" num="0135">The memory <b>203</b> may be configured to store computer-executable program codes, which comprise instructions. The processor <b>201</b> implements application of various functions and data processing of the 3D display device <b>200</b> by running the instructions stored in the memory <b>203</b>. The memory <b>203</b> may comprise a program storage region and a data storage region, wherein the program storage region may store an operating system, application programs required by at least one function (such as a sound playing function and an image playing function) and the like. The data storage region may store data (such as audio data and phonebook) created during use of the 3D display device <b>200</b> and the like. In addition, the memory <b>203</b> may comprise a high-speed random access memory (RAM), and may further comprise a nonvolatile memory (NVM), such as at least one disk storage, flash memory, and universal flash storage (UFS).</p><p id="p-0139" num="0136">The external memory interface <b>202</b> may be configured to connect with an external memory card, such as a Micro SD card, to expand storage capacity of the 3D display device <b>200</b>. The external memory card communicates with the processor <b>201</b> through the external memory interface <b>202</b>, to realize a data storage function.</p><p id="p-0140" num="0137">In some embodiments, memories of the 3D display device may comprise the (internal) memory <b>203</b>, an external memory card connected with the external memory interface <b>202</b>, or a combination thereof. In other embodiments of the present disclosure, the signal interface may also adopt internal interface connection modes or combinations thereof different from connection modes in the above embodiments.</p><p id="p-0141" num="0138">In embodiments of the present disclosure, the camera assembly <b>221</b> may collect images or videos in 2D or 3D, and output the collected videos through the 3D image output interface <b>225</b>. The eye positioning apparatus <b>250</b> may determine eye space positions of the user. The camera assembly <b>221</b>, the 3D image output interface <b>225</b> and the eye positioning apparatus <b>250</b> together form the 3D shooting apparatus <b>220</b>.</p><p id="p-0142" num="0139">In some embodiments, the 3D display device <b>200</b> realizes a display function through the video signal interface <b>204</b>, the 3D processing apparatus <b>203</b>, the eye positioning apparatus <b>250</b>, the multi-viewpoint 3D display screen <b>210</b>, and the application processor.</p><p id="p-0143" num="0140">In some embodiments, the 3D display device <b>200</b> may comprise a GPU, for example, may be configured to process 3D video images in the processor <b>201</b>, and may also be configured to process 2D video images.</p><p id="p-0144" num="0141">In some embodiments, the 3D display device <b>200</b> further comprises a video codec <b>224</b>, configured to compress or decompress digital videos.</p><p id="p-0145" num="0142">In some embodiments, the video signal interface <b>240</b> is configured to output video frames of a 3D video signal, such as a decompressed 3D video signal, processed by the GPU or the codec <b>224</b> or both to the 3D processing apparatus <b>230</b>.</p><p id="p-0146" num="0143">In some embodiments, the GPU or the codec <b>224</b> is integrated with a format adjuster.</p><p id="p-0147" num="0144">The multi-viewpoint 3D display screen <b>210</b> is configured to display 3D images or videos. The multi-viewpoint 3D display screen <b>210</b> comprises a display panel. The display panel may be a liquid crystal display (LCD), an organic light-emitting diode (OLED), an active matrix organic light-emitting diode or initiative matrix organic light-emitting diode (AMOLED), a flexible light-emitting diode (FLED), a Mini-LED, a Micro-LED, a Micro-OLED, a quantum dot light-emitting diode (QLED) or the like.</p><p id="p-0148" num="0145">In some embodiments, the eye positioning apparatus <b>250</b> is in communication connection to the 3D processing apparatus <b>230</b>, so that the 3D processing apparatus <b>230</b> may render the corresponding subpixels in the composite pixels (composite subpixels), based on the eye positioning data. In some embodiments, the eye positioning apparatus <b>250</b> may further be connected with the processor <b>201</b>, for example, be in by-passing connection with the processor <b>201</b>.</p><p id="p-0149" num="0146">In some embodiments, the 3D image output interface <b>225</b> of the 3D shooting apparatus <b>220</b> may be in communication connection to the processor <b>201</b> or the 3D processing apparatus <b>230</b>.</p><p id="p-0150" num="0147">The 3D display device <b>200</b> may realize audio functions through the audio module <b>212</b>, the loudspeaker <b>213</b>, the receiver <b>214</b>, the microphone <b>215</b>, the earphone interface <b>216</b>, the application processor and the like, such as music playing and recording. The audio module <b>212</b> is configured to convert digital audio information into analog audio signal output, and is also configured to convert analog audio input into digital audio signals. The audio module <b>212</b> may further be configured to encode and decode audio signals. In some embodiments, the audio module <b>212</b> may be arranged in the processor <b>201</b>, or some functional modules of the audio module <b>212</b> may be arranged in the processor <b>201</b>. The loudspeaker <b>213</b> is configured to convert audio electrical signals into sound signals. The 3D display device <b>200</b> may listen to music or hands-free conversation through the loudspeaker <b>213</b>. The receiver <b>214</b>, also called &#x201c;telephone handset&#x201d;, is configured to convert audio electrical signals into sound signals. When the 3D display device <b>200</b> answers calls or receives voice messages, the 3D display device <b>200</b> may receive voice by placing the receiver <b>214</b> close to an ear. The microphone <b>215</b> is configured to convert sound signals into electrical signals. The earphone interface <b>216</b> is configured to connect with a wired earphone. The earphone interface <b>216</b> may be a USB interface, and may also be a 3.5 mm open mobile 3D display device platform (OMTP) standard interface or a cellular telecommunications industry association (CTIA) standard interface.</p><p id="p-0151" num="0148">The button <b>217</b> comprises a power button, a volume button and the like. The button <b>217</b> may be a mechanical button. The button <b>217</b> may also be a touch button. The 3D display device <b>200</b> may receive button input, and generate button signal input related to user settings and function control of the 3D display device <b>200</b>.</p><p id="p-0152" num="0149">The motor <b>218</b> may generate a vibration alert. The motor <b>218</b> may be configured as a call vibration alert, and may also be configured as a touch vibration feedback.</p><p id="p-0153" num="0150">The SIM card interface <b>260</b> is configured to connect with an SIM card. In some embodiments, the 3D display device <b>200</b> is an eSIM, i.e., an embedded SIM card.</p><p id="p-0154" num="0151">The ambient light sensor <b>2702</b> is configured to sense ambient light brightness. The 3D display device <b>200</b> may adjust the brightness of the multi-viewpoint 3D display screen <b>210</b> or assist eye positioning according to the sensed ambient light brightness, for example, when the ambient light is relatively dim, the eye positioning apparatus <b>250</b> starts the infrared emitting apparatus. The ambient light sensor <b>2702</b> may also be configured to adjust white balance when a black-and-white camera shoots.</p><p id="p-0155" num="0152">The pressure sensor <b>2703</b> is configured to sense pressure signals, and may convert the pressure signals into electrical signals. In some embodiments, the pressure sensor <b>2703</b> may be arranged on the multi-viewpoint 3D display screen <b>210</b>, which falls within the scope of embodiments of the present disclosure.</p><p id="p-0156" num="0153">The air pressure sensor <b>2704</b> is configured to measure air pressure. In some embodiments, the 3D display device <b>200</b> calculates an altitude by the air pressure value measured by the air pressure sensor <b>2704</b>, and assists in positioning and navigation.</p><p id="p-0157" num="0154">The magnetic sensor <b>2705</b> comprises a Hall sensor.</p><p id="p-0158" num="0155">The gravity sensor <b>2706</b>, as a sensor converting motion or gravity into electrical signals, is mainly configured to measure parameters, such as tilt angle, inertia force, impact and vibration.</p><p id="p-0159" num="0156">The gyro sensor <b>2707</b> may be configured to determine a motion attitude of the 3D display device <b>200</b>.</p><p id="p-0160" num="0157">The acceleration sensor <b>2708</b> may detect acceleration of the 3D display device <b>200</b> in various directions (generally three axes).</p><p id="p-0161" num="0158">The distance sensor <b>2709</b> may be configured to measure a distance.</p><p id="p-0162" num="0159">The temperature sensor <b>2710</b> may be configured to detect a temperature.</p><p id="p-0163" num="0160">The fingerprint sensor <b>2711</b> may be configured to collect fingerprints. The 3D display device <b>200</b> may utilize collected fingerprint characteristics to unlock with fingerprints, access an application lock, shoot with fingerprints, answer an incoming call with fingerprints and the like.</p><p id="p-0164" num="0161">The touch sensor <b>2712</b> may be arranged in the multi-viewpoint 3D display screen <b>210</b>; and the touch sensor <b>2712</b> and the multi-viewpoint 3D display screen <b>210</b> form a touch screen, also called a &#x201c;touch panel&#x201d;.</p><p id="p-0165" num="0162">The bone conduction sensor <b>2713</b> may acquire vibration signals.</p><p id="p-0166" num="0163">The charging management module <b>205</b> is configured to receive charging input from the charger. The charger may be a wireless charger, and may also be a wired charger. In some embodiments of wired charging, the charging management module <b>205</b> may receive the charging input of the wired charger through the USB interface <b>204</b>. In some embodiments of wireless charging, the charging management module <b>205</b> may receive wireless charging input through a wireless charging coil of the 3D display device <b>200</b>.</p><p id="p-0167" num="0164">The power management module <b>206</b> is configured to connect with the battery <b>207</b>, the charging management module <b>205</b> and the processor <b>201</b>. The power management module <b>206</b> receives input from at least one of the battery <b>207</b> and the charging management module <b>205</b>, and supplies power to the processor <b>201</b>, the memory <b>203</b>, the external memory, the multi-viewpoint 3D display screen <b>210</b>, the camera assembly <b>221</b>, the wireless communication module <b>283</b> and the like. In other embodiments, the power management module <b>206</b> and the charging management module <b>205</b> may be provided in the same device.</p><p id="p-0168" num="0165">A software system of the 3D display device <b>200</b> may adopt a hierarchical architecture, an event-driven architecture, a microkernel architecture, a micro-service architecture or a cloud architecture. In embodiments shown in the present disclosure, an Android system with the hierarchical architecture is taken as an example, to illustrate a software structure of the 3D display device <b>200</b>. However, conceivably, the embodiments of the present disclosure may be implemented in different software systems, such as an operating system.</p><p id="p-0169" num="0166"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a structural schematic diagram of software of the 3D display device <b>200</b> shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. The hierarchical architecture divides software into several layers. The layers communicate with each other through a software interface. In some embodiments, the Android system is divided into four layers, from top to bottom, comprising an application program layer <b>310</b>, a framework layer <b>320</b>, core class library and runtime <b>330</b>, and a kernel layer <b>340</b>.</p><p id="p-0170" num="0167">The application program layer <b>310</b> may comprise a series of application packages. As shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the application packages may comprise application programs, such as Bluetooth, WLAN, navigation, music, camera, calendar, call, video, gallery, map and short message. The 3D video display method according to embodiments of the present disclosure, for example, may be executed in a video application.</p><p id="p-0171" num="0168">The framework layer <b>320</b> provides an application programming interface (API) and a programming framework for application programs in the application program layer. The framework layer comprises some predefined functions. For example, in some embodiments of the present disclosure, functions or algorithms for recognizing the acquired 3D video images and algorithms for processing images may be contained in the framework layer.</p><p id="p-0172" num="0169">As shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the framework layer <b>320</b> may comprise a resource manager, a phone manager, a content manager, a notification manager, a window manager, a view system installation package and manager and the like.</p><p id="p-0173" num="0170">Android Runtime comprises a core library and a virtual machine. The Android Runtime is responsible for scheduling and management of an Android system.</p><p id="p-0174" num="0171">The core library comprises two parts: one is performance functions to be called by java language, and the other is the core library of Android.</p><p id="p-0175" num="0172">The application program layer and the framework layer run in the virtual machine. The virtual machine executes java files of the application program layer and the framework layer as binary files. The virtual machine is configured to implement functions of object life cycle management, stack management, thread management, security and exception management, garbage collection and the like.</p><p id="p-0176" num="0173">The core class library may comprise a plurality of functional modules, such as a 3D graphics processing library (such as OpenGL ES), a surface manager, an image processing library, a media library and a graphics engine (such as SGL).</p><p id="p-0177" num="0174">The kernel layer <b>340</b> is a layer between hardware and software. The kernel layer at least comprises a camera driver, an audio and video interface, a call interface, a Wifi interface, a sensor driver, a power manager and a GPS interface.</p><p id="p-0178" num="0175">Here, an embodiment of 3D video transmission and display in a 3D display device is described by taking the 3D display device, as a mobile terminal, with the structures shown in <figref idref="DRAWINGS">FIGS. <b>2</b> and <b>3</b></figref> as an example; however, conceivably, in other embodiments, more or fewer characteristics may be included or the characteristics therein may be changed.</p><p id="p-0179" num="0176">In some embodiments, for example, the 3D display device <b>200</b> implemented as a mobile terminal, such as a tablet PC or a smart cell phone, receives, for example, a compressed 3D video signal from a network, such as a cellular network, a WLAN or Bluetooth, for example, by means of the mobile communication module <b>281</b> and the antenna <b>282</b> or the wireless communication module <b>283</b> and the antenna <b>284</b> as external interfaces; the compressed 3D video signal, for example, is subjected to image processing of the GPU <b>223</b> as well as coding and decoding and decompression of the codec <b>224</b>; and then, for example, a decompressed 3D video signal is transmitted to the 3D processing apparatus <b>230</b> through the video signal interface <b>240</b> as an internal interface, such as the MIPI or the mini-MIPI. In addition, the eye space position information of the user is acquired by the eye positioning apparatus <b>250</b>. The predetermined viewpoints are determined based on the eye space position information. The 3D processing apparatus <b>230</b> correspondingly renders the subpixels of the display screen for the predetermined viewpoints, thereby realizing the 3D video playing.</p><p id="p-0180" num="0177">In other embodiments, the 3D display device <b>200</b> reads the compressed 3D video signal stored in the (internal) memory <b>203</b>, or stored in an external memory card through the external memory interface <b>202</b>, and realizes 3D video playing through corresponding processing, transmission and rendering.</p><p id="p-0181" num="0178">In some other embodiments, the 3D display device <b>200</b> receives 3D images shot by the camera assembly <b>221</b> and transmitted by the 3D image output interface <b>225</b>, and realizes 3D video playing through corresponding processing, transmission and rendering.</p><p id="p-0182" num="0179">In some embodiments, the playing of 3D images is implemented in a video application in the Android application program layer <b>310</b>.</p><p id="p-0183" num="0180">Embodiments of the present disclosure may also provide an eye positioning method, which is realized by using the eye positioning apparatus in the above embodiments.</p><p id="p-0184" num="0181">Referring to <figref idref="DRAWINGS">FIG. <b>7</b></figref>, in some embodiments, the eye positioning method comprises:</p><p id="p-0185" num="0182">S<b>701</b>: shooting first black-and-white images and second black-and-white images;</p><p id="p-0186" num="0183">S<b>702</b>: identifying the presence of eyes based on at least one of the first black-and-white images and the second black-and-white images;</p><p id="p-0187" num="0184">S<b>703</b>: determining eye space positions based on the eyes identified in the first black-and-white images and the second black-and-white images.</p><p id="p-0188" num="0185">Exemplarily, the first black-and-white images are shot at a first position; the second black-and-white images are shot at a second position; and the first position is different from the second position.</p><p id="p-0189" num="0186">In some embodiments, the eye positioning method further comprises: transmitting eye space position information indicating the eye space positions.</p><p id="p-0190" num="0187">In some embodiments, the eye positioning method further comprises: when the first black-and-white camera or the second black-and-white camera works, utilizing an infrared emitting apparatus to emit infrared light.</p><p id="p-0191" num="0188">In some embodiments, the eye positioning method further comprises: shooting a first black-and-white image sequence comprising the first black-and-white images and a second black-and-white image sequence comprising the second black-and-white images respectively.</p><p id="p-0192" num="0189">In some embodiments, the eye positioning method further comprises: determining time-synchronized first black-and-white images and second black-and-white images.</p><p id="p-0193" num="0190">In some embodiments, the eye positioning method further comprises: buffering a plurality of first black-and-white images and second black-and-white images in the first black-and-white image sequence and the second black-and-white image sequence; comparing a plurality of previous or subsequent first black-and-white images and second black-and-white images in the first black-and-white image sequence and the second black-and-white image sequence; and taking, when the presence of eyes is not identified in a current first black-and-white image and second black-and-white image in the first black-and-white image sequence and the second black-and-white image sequence and the presence of eyes is identified in a previous or subsequent first black-and-white image and second black-and-white image through comparison, the eye space positions determined based on the previous or subsequent first black-and-white image and second black-and-white image as current eye space positions.</p><p id="p-0194" num="0191">In some embodiments, the eye positioning method comprises: shooting a first black-and-white image sequence and a second black-and-white image sequence at a frequency of 24 fps or more.</p><p id="p-0195" num="0192">Embodiments of the present disclosure may further provide a 3D display method.</p><p id="p-0196" num="0193">Referring to <figref idref="DRAWINGS">FIG. <b>8</b></figref>, in some embodiments, the 3D display method comprises:</p><p id="p-0197" num="0194">S<b>801</b>: acquiring eye space positions of the user;</p><p id="p-0198" num="0195">S<b>802</b>: determining corresponding viewpoints according to the eye space positions;</p><p id="p-0199" num="0196">S<b>803</b>: rendering subpixels, corresponding to the viewpoints, of a multi-viewpoint 3D display screen based on 3D signals.</p><p id="p-0200" num="0197">In some embodiments, the 3D display method further comprises: providing the multi-viewpoint 3D display screen, which comprises a plurality of composite pixels, wherein each composite pixel of the plurality of composite pixels comprises a plurality of composite subpixels; and each composite subpixel of the plurality of composite subpixels comprises a plurality of subpixels corresponding to the plurality of viewpoints.</p><p id="p-0201" num="0198">Exemplarily, when it is determined that each eye of the user corresponds to one viewpoint based on the eye space positions, images of two viewpoints, at which both eyes of the user are located, are generated based on video frames of 3D video signals; and the subpixels, corresponding to the two viewpoints, in the composite subpixels are rendered.</p><p id="p-0202" num="0199">Referring to <figref idref="DRAWINGS">FIG. <b>9</b></figref>, in the illustrated embodiment, the right eye of the user is at a second viewpoint V2; the left eye is at a fifth viewpoint V5; images of the two viewpoints V2 and V5 are generated based on the video frames of the 3D video signals; and the subpixels, corresponding to the two viewpoints, in the composite subpixels are rendered.</p><p id="p-0203" num="0200">In some embodiments, when the tilt angle or parallelism of both eyes of the user relative to the multi-viewpoint 3D display screen is determined based on the eye space positions, targeted or customized display images may be provided for the user, to improve viewing experience of the user.</p><p id="p-0204" num="0201">The above eye space positions may be acquired or determined in real time, and may also be acquired or determined periodically or randomly.</p><p id="p-0205" num="0202">The computer-readable storage medium provided by the embodiments of the present disclosure stores computer-executable instructions; and the computer-executable instructions are set to execute the eye positioning method and the 3D display method.</p><p id="p-0206" num="0203">The computer program product provided by the embodiments of the present disclosure comprises computer programs stored on a computer-readable storage medium; the computer programs comprise program instructions; and when the program instructions are executed by a computer, the computer executes the eye positioning method and the 3D display method.</p><p id="p-0207" num="0204">Technical solutions of embodiments of the present disclosure may be reflected in the form of a software product, which is stored in a storage medium and comprises one or more instructions for enabling computer equipment (which may be a personal computer, a server, network equipment or the like) to perform all or some steps of the method in embodiments of the present disclosure. The storage medium may be a non-transient storage medium, comprising a plurality of media capable of storing program codes, such as a U disk, a mobile hard disk, a read-only memory (ROM), a RAM, a diskette or an optical disk, and may also be a transient storage medium.</p><p id="p-0208" num="0205">The system, the devices, the modules or the units illustrated in the above embodiments may be realized by various possible entities. A typical realizing entity is the computer or the processor thereof or other components. The computer, for example, may be the personal computer, a laptop computer, vehicle-mounted human-computer interaction equipment, the cell phone, a camera phone, an intelligent phone, the PDA, a media player, navigation equipment, E-mail equipment, a game console, the tablet personal computer, the wearable equipment, the smart television, an Internet of Things (IoT) system, smart home, an industrial computer, a singlechip system or a combination thereof. In a typical configuration, the computer may comprise one or more Central Processing Units (CPUs), an input/output interface, a network interface and a memory. The memory probably comprises a volatile memory, an RAM and/or a nonvolatile memory and other forms in a computer readable medium, such as a Read Only Memory (ROM) or a flash RAM.</p><p id="p-0209" num="0206">The method, the programs, the system, the devices and the like in the embodiments of the present disclosure may be executed or realized in one or more networked computers, and may also be implemented in distributed computing environments. In the embodiments of the description, in the distributed computing environments, tasks are executed by remote processing equipment connected by a communication network.</p><p id="p-0210" num="0207">Those skilled in the art should understand that the embodiments of the description may provide the method, the system or computer program products. Therefore, the embodiments of the description may adopt forms of full-hardware embodiments, full-software embodiments or embodiments combining software and hardware aspects.</p><p id="p-0211" num="0208">Those skilled in the art may contemplate that the functional modules/units or the controller and related method steps, illustrated in the above embodiments, may be realized in a software manner, a hardware manner and a software/hardware combination manner, and for example, may be realized in a pure computer readable program code manner, and logic programming can also be performed for part or all of the method steps to enable the controller to realize same functions by the hardware, comprising but not limited to a logic gate, a switch, a special integrated circuit, a Programmable Logic Controller (PLC) (such as the FPGA) and an embedded microcontroller.</p><p id="p-0212" num="0209">In some embodiments of the present disclosure, the components of the devices are described in a form of the functional modules/units. It may be contemplated that a plurality of functional modules/units are realized in one or more &#x2018;combined&#x2019; functional modules/units and/or one or more software and/or hardware. It may also be contemplated that the single functional module/unit is realized by the combination of a plurality of sub-functional modules/sub-units and/or multiple software and/or hardware. The division of the functional modules/units may be only a logic function division, and in a realizing manner, the plurality of functional modules/units may be combined or may be integrated into another system. Additionally, the connection of the modules, the units, the devices, the systems and the components thereof in the text comprises direct or indirect connection, covering feasible electrical, mechanical and communication connection, especially comprising wired or wireless connection between various interfaces, comprising but not limited to a High-Definition Multimedia Interface (HDMI), thunders, the USB, the WiFi and the cellular network.</p><p id="p-0213" num="0210">In the embodiments of the present disclosure, the technical features, the flow charts and/or the block diagrams of the method and the programs may be applied in the corresponding devices, equipment and systems as well as the modules, the units and the components thereof. On the contrary, all the embodiments and features of the devices, the equipment, the systems as well as the modules, the units and the components thereof may be applied in the method and the programs according to the embodiments of the present disclosure. For example, a computer program instruction may be loaded in a general-purpose computer, a special computer, an embedded processor or a processor of other programmable data processing equipment to generate a machine which has corresponding functions or features realized in one program or more programs of the flow charts and/or one block or more blocks of the block diagrams.</p><p id="p-0214" num="0211">The method and the programs according to the embodiments of the present disclosure may be stored in a computer readable memory or medium which can guide the computer or other programmable data processing equipment to work in a specific manner by way of the computer program instructions or programs. The embodiments of the present disclosure also relate to the readable memory or medium which stores the method, the programs and the instructions which can implement the embodiments of the present disclosure.</p><p id="p-0215" num="0212">Unless clearly pointed out, actions or steps of the method and the programs recorded according to the embodiments of the present disclosure are not necessarily executed according to a specific sequence, and an expected result may still be realized. In some implementation manners, multitasking and parallel processing are also permissible or are probably favorable.</p><p id="p-0216" num="0213">The exemplary system and method of the present disclosure are shown and described with reference to the above embodiments, and are only examples for executing the system and the method. Those skilled in the art may understand that when the system and/or the method is executed, various changes may be made to the embodiments of the system and the method described here, without departing from the spirit and the scope, defined in the attached claims, of the present disclosure.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230007225A1-20230105-M00001.NB"><img id="EMI-M00001" he="12.36mm" wi="76.20mm" file="US20230007225A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002" nb-file="US20230007225A1-20230105-M00002.NB"><img id="EMI-M00002" he="16.59mm" wi="76.20mm" file="US20230007225A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An eye positioning apparatus, comprising:<claim-text>an eye positioner, comprising a first black-and-white camera configured to shoot first black-and-white images and a second black-and-white camera configured to shoot second black-and-white images;</claim-text><claim-text>an eye positioning image processor, configured to identify presence of eyes based on at least one of the first black-and-white images and second black-and-white images, and determine eye space positions based on the eyes identified in the first black-and-white images and second black-and-white images.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The eye positioning apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising an eye positioning data interface, configured to transmit eye space position information which indicates the eye space positions.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The eye positioning apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the eye positioner further comprises an infrared emitting apparatus.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The eye positioning apparatus according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the infrared emitting apparatus is configured to emit infrared light with a wavelength greater than or equal to 1.5 microns.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The eye positioning apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first black-and-white camera and the second black-and-white camera are configured to respectively shoot a first black-and-white image sequence comprising the first black-and-white images and a second black-and-white image sequence comprising the second black-and-white images.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The eye positioning apparatus according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the eye positioning image processor comprises a synchronizer, configured to determine time-synchronized first black-and-white images and second black-and-white images, so as to conduct identification of eyes and determination of eye space positions.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The eye positioning apparatus according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the eye positioning image processor comprises:<claim-text>a buffer, configured to buffer a plurality of first black-and-white images and second black-and-white images in the first black-and-white image sequence and second black-and-white image sequence;</claim-text><claim-text>a comparer, configured to compare a plurality of previous or subsequent first black-and-white images and second black-and-white images in the first black-and-white image sequence and second black-and-white image sequence;</claim-text><claim-text>a determiner, wherein when the comparer does not identify presence of eyes in a current first black-and-white image and a current second black-and-white image in the first black-and-white image sequence and the second black-and-white image sequence and identifies presence of eyes in a previous or subsequent first black-and-white image and second black-and-white image through comparison, the determiner is configured to take eye space positions, determined based on the previous or subsequent first black-and-white image and second black-and-white image, as current eye space positions.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. A 3D display device, comprising:<claim-text>a multi-viewpoint 3D display screen, comprising a plurality of subpixels corresponding to a plurality of viewpoints;</claim-text><claim-text>the eye positioning apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, configured to obtain eye space positions;</claim-text><claim-text>a 3D processing apparatus, configured to determine corresponding viewpoints according to eye space positions obtained by the eye positioning apparatus, and render subpixels, corresponding to the viewpoints, of the multi-viewpoint 3D display screen based on 3D signals.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The 3D display device according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the multi-viewpoint 3D display screen comprises a plurality of composite pixels, each composite pixel of the plurality of composite pixels comprises a plurality of composite subpixels, and each composite subpixel of the plurality of composite subpixels comprises a plurality of subpixels corresponding to a plurality of viewpoints.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The 3D display device according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the 3D processing apparatus is in communication connection with the eye positioning apparatus.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The 3D display device according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, further comprising:<claim-text>a 3D shooting apparatus, configured to collect 3D images;</claim-text><claim-text>the 3D shooting apparatus comprises a depth-of-field (DOF) camera and at least two color cameras.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The 3D display device according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the eye positioning apparatus is integrated with the 3D shooting apparatus.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The 3D display device according to <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the 3D shooting apparatus is placed in front of the 3D display device.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. An eye positioning method, comprising:<claim-text>shooting first black-and-white images and second black-and-white images;</claim-text><claim-text>identifying presence of eyes based on at least one of the first black-and-white images and second black-and-white images;</claim-text><claim-text>determining eye space positions based on the eyes identified in the first black-and-white images and second black-and-white images.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The eye positioning method according to <claim-ref idref="CLM-00014">claim 14</claim-ref>, further comprising:<claim-text>transmitting eye space position information which indicates the eye space positions;</claim-text><claim-text>or,</claim-text><claim-text>emitting infrared light by utilizing an infrared emitting apparatus when the first black-and-white camera or the second black-and-white camera works;</claim-text><claim-text>or,</claim-text><claim-text>shooting a first black-and-white image sequence comprising the first black-and-white images and a second black-and-white image sequence comprising the second black-and-white images respectively.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. (canceled)</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. (canceled)</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The eye positioning method according to <claim-ref idref="CLM-00015">claim 15</claim-ref>, further comprising: determining time-synchronized first black-and-white images and second black-and-white images.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The eye positioning method according to <claim-ref idref="CLM-00018">claim 18</claim-ref>, further comprising:<claim-text>buffering a plurality of first black-and-white images and second black-and-white images in the first black-and-white image sequence and second black-and-white image sequence;</claim-text><claim-text>comparing a plurality of previous or subsequent first black-and-white images and second black-and-white images in the first black-and-white image sequence and the second black-and-white image sequence;</claim-text><claim-text>taking, when presence of eyes is not identified in a current first black-and-white image and second black-and-white image in the first black-and-white image sequence and the second black-and-white image sequence and presence of eyes is identified in a previous or subsequent first black-and-white image and second black-and-white image through comparison, eye space positions determined based on the previous or subsequent first black-and-white image and second black-and-white image as current eye space positions.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. A 3D display method, comprising:<claim-text>acquiring eye space positions of a user;</claim-text><claim-text>determining corresponding viewpoints according to the eye space positions;</claim-text><claim-text>rendering subpixels, corresponding to the viewpoints, of a multi-viewpoint 3D display screen based on 3D signals.</claim-text></claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. The 3D display method according to <claim-ref idref="CLM-00020">claim 20</claim-ref>, further comprising: providing the multi-viewpoint 3D display screen, which comprises a plurality of composite pixels, wherein each composite pixel of the plurality of composite pixels comprises a plurality of composite subpixels, and each composite subpixel of the plurality of composite subpixels comprises a plurality of subpixels corresponding to a plurality of viewpoints.</claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. A non-transitory computer-readable storage medium, storing computer-executable instructions, wherein the computer-executable instructions are configured to execute the method of <claim-ref idref="CLM-00014">claim 14</claim-ref>.</claim-text></claim><claim id="CLM-00023" num="00023"><claim-text><b>23</b>. A computer program product, comprising computer programs stored on a non-transitory computer-readable storage medium, wherein the computer programs comprise program instructions, and make a computer execute the method of <claim-ref idref="CLM-00014">claim 14</claim-ref> when the program instructions are executed by the computer.</claim-text></claim></claims></us-patent-application>