<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005468A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005468</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17779518</doc-number><date>20191126</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>13</main-group><subgroup>10</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>268</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>13</main-group><subgroup>047</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>13</main-group><subgroup>06</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>13</main-group><subgroup>10</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20200101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>268</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>13</main-group><subgroup>047</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>13</main-group><subgroup>06</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">POSE ESTIMATION MODEL LEARNING APPARATUS, POSE ESTIMATION APPARATUS, METHODS AND PROGRAMS FOR THE SAME</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>NIPPON TELEGRAPH AND TELEPHONE CORPORATION</orgname><address><city>Tokyo</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>NAGANO</last-name><first-name>Mizuki</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>IJIMA</last-name><first-name>Yusuke</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>KOBAYASHI</last-name><first-name>Nozomi</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>NIPPON TELEGRAPH AND TELEPHONE CORPORATION</orgname><role>03</role><address><city>Tokyo</city><country>JP</country></address></addressbook></assignee></assignees><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/JP2019/046148</doc-number><date>20191126</date></document-id><us-371c12-date><date>20220524</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A pause estimation model learning apparatus includes: a morphological analysis unit configured to perform morphological analysis on training text data to provide M types of information, M being an integer that is equal to or larger than 2; a feature selection unit configured to combine N pieces of information, among the M pieces of information, to be an input feature when a predetermined certain condition is satisfied, and select predetermined one of the N pieces of information to be the input feature when the certain condition is not satisfied, N being an integer that is equal to or larger than 2 and equal to or smaller than M; and a learning unit configured to learn a pause estimation model by using the input feature selected by the feature selection unit and a pause correct label.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="128.02mm" wi="142.41mm" file="US20230005468A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="151.64mm" wi="144.27mm" orientation="landscape" file="US20230005468A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="222.25mm" wi="146.90mm" orientation="landscape" file="US20230005468A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="180.42mm" wi="147.74mm" orientation="landscape" file="US20230005468A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="143.59mm" wi="149.18mm" orientation="landscape" file="US20230005468A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="67.56mm" wi="150.45mm" orientation="landscape" file="US20230005468A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="239.44mm" wi="96.10mm" orientation="landscape" file="US20230005468A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="205.23mm" wi="148.34mm" orientation="landscape" file="US20230005468A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="179.83mm" wi="146.98mm" orientation="landscape" file="US20230005468A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="190.92mm" wi="147.32mm" orientation="landscape" file="US20230005468A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="193.29mm" wi="149.01mm" orientation="landscape" file="US20230005468A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="194.56mm" wi="147.40mm" orientation="landscape" file="US20230005468A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="243.67mm" wi="99.23mm" orientation="landscape" file="US20230005468A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="240.20mm" wi="46.06mm" orientation="landscape" file="US20230005468A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="173.23mm" wi="149.18mm" orientation="landscape" file="US20230005468A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="74.42mm" wi="149.27mm" orientation="landscape" file="US20230005468A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="165.86mm" wi="146.98mm" orientation="landscape" file="US20230005468A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="76.45mm" wi="148.00mm" orientation="landscape" file="US20230005468A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="197.27mm" wi="147.24mm" orientation="landscape" file="US20230005468A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0002" num="0001">The present disclosure relates to a pause estimation model learning apparatus that learns a pause estimation model for estimating a timing of putting an intermission (hereinafter, also referred to as &#x201c;pause&#x201d;) for implementing speech synthesis, a pause estimation apparatus that uses the pause estimation model to estimate a pause position, a method for these, and a program.</p><heading id="h-0002" level="1">BACKGROUND ART</heading><p id="p-0003" num="0002">When a text is read using synthesized speech, processing is executed to estimate a timing of putting an intermission in a sentence.</p><p id="p-0004" num="0003">When this processing is executed using a pause estimation model learned by machine learning such as Conditional random field (CRF) or Deep Neural Network (DNN), training data for the pause estimation model requires data as a result of morphologically analyzed a huge amount of text data and a pause correct label indicating a pause position in the text data read. Generally, the writing, part-of-speech, conjugation, and the like of a morpheme from the morphologically analyzed data are used as input features (feature amount) for the learning (see NPL 1).</p><heading id="h-0003" level="1">CITATION LIST</heading><heading id="h-0004" level="1">Non Patent Literature</heading><p id="p-0005" num="0000"><ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0004">NPL 1: Masayuki Suzuki, Ryo Kuroiwa, Keisuke Innami, Shumpei Kobayashi, Shinya Shimizu, Nobuaki Minematsu, and Keikichi Hirose, &#x201c;Accent Sandhi Estimation of Tokyo Dialect of Japanese Using Conditional Random Fields&#x201d;, IEICE Transactions, Vol. J 96&#x2014; D, No. 3, pp. 644-654, 2013</li></ul></p><heading id="h-0005" level="1">SUMMARY OF THE INVENTION</heading><heading id="h-0006" level="1">Technical Problem</heading><p id="p-0006" num="0005">Various features of morphemes, such as writing, part-of-speech, and conjugation, can be used for the learning. However, an increase in the number of features in an attempt to increase the coverage leads to a higher cost for creating training data. Furthermore, the increased features are not necessarily effective for the pause estimation.</p><p id="p-0007" num="0006">In particular, when the writing is used as one of the features, morphemes with different writings are regarded as different features, giving rise to a complex combination between features leading to a larger model size, resulting in a problem in that the used amount of a read only memory (ROM) or a random access memory (RAM) is increased and the execution speed is compromised.</p><p id="p-0008" num="0007">Patterns in which a pause is inserted into a sentence are limited. In view of this, the estimation is desirably performed with the smallest possible calculation amount using some features effective for the estimation, instead of thoughtlessly using a large number of features.</p><p id="p-0009" num="0008">An object of the present disclosure is to provide a pause estimation model learning apparatus, a method thereof, and a program with which the model size of the pause estimation model can be reduced and the learning processing speed can be improved, without compromising the accuracy of estimation of a pause in a sentence.</p><heading id="h-0007" level="1">Means for Solving the Problem</heading><p id="p-0010" num="0009">In order to solve the above problem, according to one aspect of the present disclosure, a pause estimation model learning apparatus includes: a morphological analysis unit configured to perform morphological analysis on training text data to provide M types of information, M being an integer that is equal to or larger than 2; a feature selection unit configured to combine N pieces of information, among the M pieces of information, to be an input feature when a predetermined certain condition is satisfied, and select predetermined one of the N pieces of information to be the input feature when the certain condition is not satisfied, N being an integer that is equal to or larger than 2 and equal to or smaller than M; and a learning unit configured to learn a pause estimation model by using the input feature selected by the feature selection unit and a pause correct label.</p><heading id="h-0008" level="1">Effects of the Invention</heading><p id="p-0011" num="0010">The present disclosure provides an effect that the model size of the pause estimation model can be reduced and the learning processing speed can be improved, without compromising the accuracy of estimation of a pause in a sentence.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0009" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram illustrating an example of input features of a related-art method.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a diagram illustrating an example of input features of a first embodiment.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram illustrating an example of an overall configuration of speech synthesis processing.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a functional block diagram of a pause estimation model learning apparatus according to the first embodiment.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flowchart of an example of processing flow of the pause estimation model learning apparatus according to the first embodiment.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a diagram illustrating an example of a configuration in which four types of information &#x201c;writing&#x201d;, &#x201c;part-of-speech&#x201d;, &#x201c;conjugation&#x201d;, and &#x201c;reading&#x201d; are given to a morpheme.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a diagram illustrating an example of a part of the training text data divided into morphemes, input features of related art, and input features of the present embodiment.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a diagram illustrating an example of a combination between features.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a diagram illustrating an example of a combination between features.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a diagram illustrating an example of a combination between features.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a diagram illustrating an example of a combination between features.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a diagram illustrating features used in an experiment.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a diagram illustrating sizes of models according to a related-art method and according to the present embodiment, and accuracy of pause estimation relative to the same test data.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a functional block diagram of a pause estimation model learning apparatus according to a second embodiment.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a flowchart of an example of processing flow of the pause estimation model learning apparatus according to the second embodiment.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>16</b></figref> is a functional block diagram of a pause estimation apparatus according to a third embodiment.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is a flowchart of an example of processing flow of the pause estimation apparatus according to the third embodiment.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>18</b></figref> is a diagram illustrating a configuration example of a computer to which the present method is applied.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0010" level="1">DESCRIPTION OF EMBODIMENTS</heading><p id="p-0030" num="0029">Hereinafter, embodiments of the present disclosure will be described. In the drawings used in the following description, the same reference signs are given to components having the same function or the steps of performing the same processing, and duplicate description is omitted. Furthermore, in the following description, it is assumed that processing performed for each element of a vector or a matrix is applied to all elements of the vector or the matrix unless otherwise specified.</p><heading id="h-0011" level="1">Points of First Embodiment</heading><p id="p-0031" num="0030">The training data for the pause estimation model in the speech synthesis requires the data as a result of morphological analysis on a huge amount of text data and the pause correct label. Of these, the morphologically analyzed data is used as the input features, for learning the pause estimation model by machine learning. Here, the present embodiment is mainly different from related-art methods in the following points. Specifically, in the related art, information, such as the part-of-speech and writing obtained by the morphological analysis (see <figref idref="DRAWINGS">FIG. <b>1</b></figref>), is directly used for as the input features for the learning. On the other hand, in the present embodiment, such information is not directly used, and features are combined under a certain condition designated in advance, to be used as the input feature (see <figref idref="DRAWINGS">FIG. <b>2</b></figref>). With this configuration, the features with a huge amount of information can be locally used, without the combinations among the features being complex.</p><p id="p-0032" num="0031">In the present embodiment, the amount of training data can be reduced, with the features combined under a certain condition to narrow down the features for selecting input features effective for the pause estimation. Furthermore, the model size can be reduced and processing speed can be improved without compromising pause estimation accuracy.</p><heading id="h-0012" level="1">First Embodiment</heading><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates an example of an overall configuration of speech synthesis processing.</p><p id="p-0034" num="0033">Conversion of text data into synthesized speech is performed through processing roughly divided into three stages of processes respectively executed by a language processing unit <b>110</b>, a prosody generation unit <b>120</b>, and a voice waveform generation unit <b>130</b>.</p><p id="p-0035" num="0034">First of all, the language processing unit <b>110</b> receives the text data as input, analyzes the input text data, provides information such as how the text is read, with what accent, and where the pause is put, and outputs the information as a context of a synthesized text.</p><p id="p-0036" num="0035">Next, the prosody generation unit <b>120</b> receives the context of the synthesized text as an input, provides information such as intonation, inflection, and rhythm of sound, and outputs the information as a voice parameter.</p><p id="p-0037" num="0036">Finally, the voice waveform generation unit <b>130</b> receives the voice parameter as an input, and generates a voice waveform from the voice parameter, and outputs the voice waveform as synthesized speech data.</p><p id="p-0038" num="0037">In the present embodiment, of the above processes, a focus is given on a pause estimation process executed by the language processing unit <b>110</b>.</p><p id="p-0039" num="0038">If the pause estimation process is executed by machine learning, a pause estimation model needs to be learned. In this process, with a related-art method, a feature selection unit selects a part of features obtained by morphological analysis, to be directly used as the input feature. In the present embodiment, the features are selected, and then are combined with other features under a certain condition designated in advance, to be used as the input feature. The processes thereafter can be performed through a procedure similar to that in the related-art method.</p><p id="p-0040" num="0039">Pause estimation model learning apparatus according to first embodiment In the present embodiment, an apparatus learning a pause estimation model will be described.</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a functional block diagram of the pause estimation model learning apparatus according to the present embodiment, and <figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates an example of the processing flow.</p><p id="p-0042" num="0041">The pause estimation model learning apparatus includes a morphological analysis unit <b>111</b>, a feature selection unit <b>112</b>, and a learning unit <b>113</b>.</p><p id="p-0043" num="0042">The pause estimation model learning apparatus receives training text data and a pause correct label as inputs, learns the pause estimation model, and outputs the learned pause estimation model. Note that the learned pause estimation model is used, for example, in the language processing unit <b>110</b> described above.</p><p id="p-0044" num="0043">Note that the training text data is a huge amount of text data used for learning, and the pause correct label is a label indicating a position where a pause is inserted when such text data is read. The correct label may be generated with an appropriate pause position manually provided to the text data or may be generated from a pair of the text data and corresponding spoken voice data. For example, voice recognition processing is executed on the spoken voice data to detect a pause (for example, a section in which the volume continues to be lower than a predetermined threshold for more than a predetermined period of time), and a label indicating the corresponding position in the text data is generated as a pause correct label. Examples of the text data and pause correct label (&#x3c;P&#x3e;) are as follows.</p><heading id="h-0013" level="1">Example: &#x201c;Maa Kore Wa&#x3c;P&#x3e;Jibun <i>Shidai </i>Na Wake De&#x3c;P&#x3e;Koko Ni Ka I Te Mo Shikatana I&#x3c;P&#x3e;&#x201d;</heading><p id="p-0045" num="0044">The pause estimation model learning apparatus is a special apparatus constituted by, for example, a known or dedicated computer including a central processing unit (CPU), a main storage apparatus (random access memory (RAM)), and the like into which a special program is read. The pause estimation model learning apparatus, for example, executes each processing under control of the central processing unit. The data input to the pause estimation model learning apparatus and the data obtained in each processing, for example, are stored in the main storage apparatus, and the data stored in the main storage apparatus is read out, as needed, to the central processing unit to be used for other processing. At least a portion of each processing unit of the pause estimation model learning apparatus may be constituted with hardware such as an integrated circuit. Each storage unit included in the pause estimation model learning apparatus can be configured, for example, by a main storage device such as a random access memory (RAM) or by middleware such as a relational database or a key-value store. However, each storage unit does not need to be included inside the pause estimation model learning apparatus and may be configured by an auxiliary storage apparatus constituted with a hard disk, an optical disk, or a semiconductor memory element such as a flash memory, and may be provided outside the pause estimation model learning apparatus.</p><p id="p-0046" num="0045">Each unit will be described below.</p><p id="p-0047" num="0000">Morphological Analysis Unit <b>111</b> The morphological analysis unit <b>111</b> receives the training text data as an input, performs morphological analysis on the training text data (S<b>111</b>), provides M types of information to a morpheme, and outputs the result. Note that M is an integer that is equal to or larger than 2. The result of providing the M types of information to a morpheme is also referred to as a morphological analysis result.</p><p id="p-0048" num="0046">Note that the morphological analysis includes dividing text data into morphemes, which are smallest meaningful units; and providing each of the morphemes with information, such as the part-of-speech or conjugate thereof. This morphological analysis may be performed manually or by using a morphological analyzer. The information (feature) obtained differs depending on the morphological analyzer used. In this example, it is assumed that information such as &#x201c;writing&#x201d;, &#x201c;part-of-speech&#x201d; &#x201c;conjugation&#x201d;, and &#x201c;reading&#x201d; is obtainable. Essentially, information to be used as an input feature for the feature selection unit <b>112</b> described below may be provided to a morpheme. <figref idref="DRAWINGS">FIG. <b>6</b></figref> shows an example of a configuration in which four types of information &#x201c;writing&#x201d;, &#x201c;part-of-speech&#x201d;, &#x201c;conjugation&#x201d;, and &#x201c;reading&#x201d; are given to a morpheme.</p><p id="p-0049" num="0047">Feature Selection Unit <b>112</b></p><p id="p-0050" num="0000">The feature selection unit <b>112</b> receives the morpheme that has been provided with the M types of information as an input, and outputs an input feature. The input feature is a result of combining N pieces of information among the M pieces of information when a predetermined certain condition is satisfied, or predetermined one of the N pieces of information is selected when the certain condition is not satisfied (S<b>112</b>). Note that N is an integer that is equal to or larger than 2 and equal to or smaller than M. Note that a configuration may be employed in which N is the same as M, and the morphological analysis unit <b>111</b> provides the morpheme with only the information used by the feature selection unit <b>112</b>.</p><p id="p-0051" num="0048">The input feature used for learning is selected from the information (features) obtained by the morphological analysis. In this process, in a related-art method, predetermined types of features have been selected from the features obtained by the morphological analysis, to be directly used as the input features (see <figref idref="DRAWINGS">FIG. <b>1</b></figref>). However, such features may include a feature having no impact on the pause estimation process. For example, if the part-of-speech of a morpheme is a noun, replacing the morpheme with &#x201c;a morpheme with a different writing, the part-of-speech of which is a noun&#x201d; would almost never lead to a change in the position of the pause. Thus, a combination of features noun+writing has a limited impact on the pause estimation process, and thus can be regarded to be not so important.</p><p id="p-0052" num="0049">In the present embodiment, the features are combined only under a certain condition determined by an administrator of the pause estimation model learning apparatus in advance, to be used as input features, instead of directly using all the features as the input features. With this configuration, the size of the pause estimation model created by machine learning can be reduced from that obtained by the related-art method. Furthermore, the processing speed can be improved. <figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates an example of a combination between features.</p><p id="p-0053" num="0050"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates an example of a part of the training text data divided into morphemes, input features of related art, and input features of the present embodiment. Note that, for the sake of simplicity, a condition for combining features herein is based on the part-of-speech narrowed down to three of &#x201c;postpositional particle&#x201d;, &#x201c;topic-indicating particle&#x201d;, and &#x201c;verbal suffix&#x201d; that are likely to affect the pause estimation. Specifically, when a part-of-speech of a morpheme is any one of these three, the &#x201c;part-of-speech&#x201d; is combined with &#x201c;writing&#x201d;, and for a morpheme with a part-of-speech other than these, the part-of-speech is used directly as the input feature.</p><p id="p-0054" num="0051"><figref idref="DRAWINGS">FIGS. <b>8</b> to <b>11</b></figref> illustrate examples of combinations between features.</p><p id="p-0055" num="0052">In <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the condition for combining features is a &#x201c;part-of-speech&#x201d; other than a &#x201c;noun&#x201d;. Specifically, when the &#x201c;part-of-speech&#x201d; of a morpheme is other than a &#x201c;noun&#x201d;, the &#x201c;part-of-speech&#x201d; and the &#x201c;writing&#x201d; are combined. In other cases (when the &#x201c;part-of-speech&#x201d; of a morpheme is a &#x201c;noun&#x201d;), the &#x201c;writing&#x201d; is directly used as the input feature.</p><p id="p-0056" num="0053">In <figref idref="DRAWINGS">FIG. <b>9</b></figref>, the condition for combining features is the same as that in <figref idref="DRAWINGS">FIG. <b>8</b></figref>. When the condition is satisfied, the &#x201c;part-of-speech&#x201d; and the &#x201c;conjugation&#x201d; are combined. When the condition is not satisfied, the &#x201c;part-of-speech (noun)&#x201d; is directly uses as the input feature.</p><p id="p-0057" num="0054">In <figref idref="DRAWINGS">FIG. <b>10</b></figref>, the condition for combining features is &#x201c;conjugation&#x201d; being &#x201c;topic indicating&#x201d;. Specifically, when the &#x201c;conjugation&#x201d; of a morpheme is &#x201c;topic indicating&#x201d;, the &#x201c;conjugation&#x201d; and the &#x201c;writing&#x201d; are combined. In other cases, the &#x201c;conjugation&#x201d; is directly used as the input feature.</p><p id="p-0058" num="0055">In <figref idref="DRAWINGS">FIG. <b>11</b></figref>, the condition for combining features is the same as that in <figref idref="DRAWINGS">FIG. <b>10</b></figref>. When the condition is satisfied, the &#x201c;conjugation&#x201d; and the &#x201c;reading&#x201d; are combined. When the condition is not satisfied, &#x201c;conjugation&#x201d; is directly used as the input feature.</p><p id="p-0059" num="0056">Learning Unit <b>113</b></p><p id="p-0060" num="0000">The learning unit <b>113</b> receives the input feature and the pause correct label as inputs, uses the input feature and the pause correct label to learn the pause estimation model by machine learning (S<b>113</b>), and outputs the learned pause estimation model.</p><p id="p-0061" num="0057">CRF used in NPL 1, the DNN (BLSTM) used in Reference Document <b>1</b>, or the like can be used for this learning processing. Thus, the learning can be implemented in a procedure that is the same as those in these documents.</p><p id="p-0062" num="0000">(Reference 1) Viacheslav Klimkov, Adam Nadolski, Alexis Moinet, Bartosz Putrycz, Roberto Barra-Chicote, Thomas Merritt, Thomas Drugman, &#x201c;Phrase break prediction for long-form reading TTS: exploiting text structure information&#x201d;, INTERSPEECH 2017, pp. 1064 to 1068, 2017</p><p id="p-0063" num="0058">Note that the pause estimation model is a model that uses the input features as inputs and outputs a pause label. Note that the pause label is information indicating the position of the pause in the target text data.</p><p id="p-0064" num="0059">Effects</p><p id="p-0065" num="0000">With the above configuration, with the learned pause estimation model, the model size of the pause estimation model can be reduced from that in the related art, without compromising the estimation accuracy of the pause in the sentence. Furthermore, with the pause estimation model learning apparatus of the present embodiment, the total number of features can be reduced, whereby the learning processing speed can be improved from that with the related-art. Reducing the total number of features also enables the amount of training data to be reduced, whereby a cost for creating the training data can be reduced.</p><p id="p-0066" num="0060">Results of experiment of performing speech synthesis using the learned pause estimation model output from the pause estimation model learning apparatus according to the present embodiment will be described below.</p><p id="p-0067" num="0061">Experiment Details</p><p id="p-0068" num="0000">To demonstrate the effectiveness of the present embodiment, a pause estimation experiment was performed using methods according to the related-art method and according to the present embodiment, and the experimental results were compared.</p><p id="p-0069" num="0062">Data Used for Experiment.</p><p id="p-0070" num="0000">For the experiment, the Japanese text data comprising 5143 sentences and data as a result of manually attaching the correct labels of the pause position to the data were used. Then, morphological analysis was performed manually on this text data, whereby features &#x201c;writing&#x201d;, &#x201c;part-of-speech&#x201d;, &#x201c;conjugation&#x201d; and &#x201c;reading&#x201d; were provided. Of the sentences, 3962 sentences were used as training data, and 1181 sentences were used as test data.</p><p id="p-0071" num="0063">Condition of Experiment</p><p id="p-0072" num="0000">Of the features obtained by morphological analysis, &#x201c;part-of-speech&#x201d; and &#x201c;writing&#x201d; were used as the input features in the related-art method. In the method according to the present embodiment, &#x201c;part-of-speech&#x201d; was selected as the input feature, and a feature as a combination of &#x201c;part-of-speech&#x201d; and &#x201c;writing&#x201d; was used only when the morpheme is &#x201c;postpositional particle&#x201d;, &#x201c;topic-indicating particle&#x201d;, or &#x201c;verbal suffix&#x201d; A pause estimation model was learned using the input feature of each of the above methods, for comparison in accuracy between the methods. CRF was used for the pause estimation model.</p><p id="p-0073" num="0064"><figref idref="DRAWINGS">FIG. <b>12</b></figref> illustrates features used in CRF. In the experiment, in addition to the feature of a morpheme (hereinafter, denoted by [0]), features of two preceding and two subsequent morpheme (respectively denoted by [&#x2212;2], [&#x2212;1], [+1], and [+2]) were also used as the input features for the morpheme.</p><p id="p-0074" num="0065"><figref idref="DRAWINGS">FIG. <b>13</b></figref> illustrates sizes of models according to a related-art method and according to the present embodiment, and accuracy of pause estimation relative to the same test data. As can be seen in the table, with the model learned in the method of the present embodiment, the learning processing time and the size were respectively reduced to &#x2153; or shorter and &#xbc; or smaller from those with the related-art method, with the pause estimation accuracy almost not compromised at all.</p><heading id="h-0014" level="1">Second Embodiment</heading><p id="p-0075" num="0066">Parts different from the first embodiment will be mainly described.</p><p id="p-0076" num="0067">In the first embodiment, a case is described where &#x201c;part-of-speech&#x201d; is combined with &#x201c;writing&#x201d; under a certain condition, as an example of the input feature of the method of the present embodiment. However, as can be seen in <figref idref="DRAWINGS">FIGS. <b>2</b> and <b>8</b> to <b>11</b></figref>, there are a variety of combinations between features, and the input feature that is a combination between &#x201c;part-of-speech&#x201d; and &#x201c;writing&#x201d; may not necessarily be effective for the pause estimation. In view of this, in the present embodiment, the accuracies of the estimation models learned by the respective input features are compared with each other, and only a model with the most accurate estimation is selected.</p><p id="p-0077" num="0068">In the present embodiment, a feature selection unit combines features, obtained by the morphological analysis, into various combinations to be used as the input features. Here, various conditions for combining features, such as the condition in the first embodiment that is satisfied when &#x201c;part-of-speech&#x201d; is &#x201c;postpositional particle&#x201d;, &#x201c;topic-indicating particle&#x201d;, or &#x201c;verbal suffix&#x201d;, are automatically taken into consideration. The learning unit then learns the pause estimation model independently for each input feature, whereby a plurality of pause estimation models are created. The method for learning and the like are the same as those in the first embodiment.</p><p id="p-0078" num="0069"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a functional block diagram of the pause estimation model learning apparatus according to the second embodiment, and <figref idref="DRAWINGS">FIG. <b>15</b></figref> illustrates an example of the processing flow.</p><p id="p-0079" num="0070">The pause estimation model learning apparatus includes a morphological analysis unit <b>111</b>, a feature selection unit <b>212</b>, and a learning unit <b>213</b>, and a best model selection unit <b>214</b>.</p><p id="p-0080" num="0071">A difference from the first embodiment is that a plurality of input features are output by the feature selection unit <b>212</b>, and the best model selection unit <b>214</b> selects a model using verification data, after the pause estimation learning using each of the input features.</p><p id="p-0081" num="0072">Feature Selection Unit <b>212</b></p><p id="p-0082" num="0000">The feature selection unit <b>212</b> receives the morpheme that has been provided with the M types of information as an input, and outputs an input feature. The input feature is an input feature x<sub>q </sub>as a result of combining N pieces of information among the M pieces of information when a predetermined certain condition is satisfied, or the input feature x<sub>q </sub>is selected as predetermined one of the N pieces of information when the certain condition is not satisfied (S<b>212</b>). Note that q=1, Q holds, where Q is an integer that is equal to or larger than 2, representing the number of types of combinations. For example, M=4 holds and the information provided to the morpheme includes &#x201c;writing&#x201d;, &#x201c;part-of-speech&#x201d;, &#x201c;conjugation&#x201d;, and &#x201c;reading&#x201d;. Furthermore, N=2 and Q=4 hold, with the combinations including &#x201c;part-of-speech&#x201d;+&#x201c;writing&#x201d;, &#x201c;conjugation&#x201d;+&#x201c;writing&#x201d;, &#x201c;part-of-speech&#x201d;+&#x201c;conjugation&#x201d;, and &#x201c;conjugation&#x201d;+&#x201c;reading&#x201d;. Predetermined certain conditions include (i) &#x201c;part-of-speech&#x201d; is not a &#x201c;noun&#x201d;, (ii) &#x201c;conjugation&#x201d; is &#x201c;topic indicating&#x201d;, and the like. <figref idref="DRAWINGS">FIGS. <b>8</b> to <b>11</b></figref> illustrate examples of combination. The combination and condition for combining features are as described in the first embodiment.</p><p id="p-0083" num="0073">Learning Unit <b>213</b></p><p id="p-0084" num="0000">The learning unit <b>213</b> receives Q types of input features x<sub>q </sub>and the pause correct label as inputs, uses these pieces of information to learn Q pause estimation models (S<b>213</b>) corresponding to the Q types of input features, and outputs the Q learned pause estimation models.</p><p id="p-0085" num="0074">Best Model Selection Unit <b>214</b></p><p id="p-0086" num="0000">The best model selection unit <b>214</b> receives the Q learned pause estimation models, verification text data, and verification pause correct label as inputs, uses the verification text data and the verification pause correct label to evaluate the Q learned pause estimation models, selects the model most highly evaluated (S<b>214</b>), and outputs the model as the output value of the pause estimation model learning apparatus.</p><p id="p-0087" num="0075">For example, the verification text data is used for comparison between the models learned by the respective input features, in terms of accuracy and size. For example, as described in the section related to the experiment of the first embodiment, the best model is selected for any of the calculated items including an accuracy, precision, recall, F-measure, and model size. Here, the administrator of the pause estimation model learning apparatus designates in advance, a parameter indicating the weight of each item.</p><p id="p-0088" num="0076">Effects</p><p id="p-0089" num="0000">Such configuration can achieve the identical effects as those in the first embodiment. Furthermore, the pause estimation model using the most effective input feature can be output.</p><heading id="h-0015" level="1">Third Embodiment</heading><p id="p-0090" num="0077">In the present embodiment, a pause estimation apparatus for estimating a pause using the pause estimation model learned in the first embodiment and the second embodiment will be described. The pause estimation apparatus is, for example, incorporated in the language processing unit <b>110</b> in <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0091" num="0078"><figref idref="DRAWINGS">FIG. <b>16</b></figref> is a functional block diagram of the pause estimation apparatus according to the present embodiment, and <figref idref="DRAWINGS">FIG. <b>17</b></figref> illustrates an example of the processing flow.</p><p id="p-0092" num="0079">The pause estimation apparatus includes a morphological analysis unit <b>311</b>, a feature selection unit <b>312</b>, and an estimation unit <b>313</b>.</p><p id="p-0093" num="0080">The pause estimation apparatus receives the text data of interest (hereinafter, also referred to as &#x201c;target text data&#x201d;) as an input, estimates the position of the pause, and outputs the estimated position as a pause label.</p><p id="p-0094" num="0081">Processes S<b>311</b> and S<b>312</b> executed by the morphological analysis unit <b>311</b> and the feature selection unit <b>312</b> are similar to the processes S<b>111</b> and S<b>112</b> executed by the morphological analysis unit <b>111</b> and the feature selection unit <b>112</b> of the first embodiment. However, the processes are executed with the target text data and information obtained from the target text data input, instead of the training text data and the information obtained from the training text data.</p><p id="p-0095" num="0082">Next, details of the processing executed by the estimation unit <b>313</b> will be described.</p><p id="p-0096" num="0083">Estimation unit <b>313</b></p><p id="p-0097" num="0000">The estimation unit <b>313</b> receives the learned pause estimation model before executing the estimation processing.</p><p id="p-0098" num="0084">The estimation unit <b>313</b> receives the input feature as an input, estimates the position of the pause using the pause estimation model (S<b>313</b>), and outputs the position as a pause label. As described in the first embodiment and the second embodiment, the input feature is a combination of N pieces of information, among M pieces of information, when a predetermined certain condition is satisfied, and the input feature is predetermined one of the N pieces of information when the certain condition is not satisfied.</p><p id="p-0099" num="0085">Effects</p><p id="p-0100" num="0000">With a pause estimation model of a model size smaller than that in the related art, the estimation accuracy for a pause in a sentence can be maintained.</p><p id="p-0101" num="0086">Other Modifications</p><p id="p-0102" num="0000">The present disclosure is not limited to the above embodiments and modifications. For example, the various processes described above may be executed not only in chronological order as described but also in parallel or on an individual basis as necessary or depending on the processing capabilities of the apparatuses that execute the processing. In addition, appropriate changes can be made without departing from the spirit of the present disclosure.</p><p id="p-0103" num="0087">Program and Recording Medium</p><p id="p-0104" num="0000">The various types of processing described above can be performed by causing a recording unit <b>2020</b> of the computer illustrated in <figref idref="DRAWINGS">FIG. <b>18</b></figref> to read a program for executing each of steps of the above method and causing a control unit <b>2010</b>, an input unit <b>2030</b>, an output unit <b>2040</b>, and the like to execute the program.</p><p id="p-0105" num="0088">The program in which the processing details are described can be recorded on a computer-readable recording medium. The computer-readable recording medium, for example, may be any type of medium such as a magnetic recording device, an optical disc, a magneto-optical recording medium, or a semiconductor memory.</p><p id="p-0106" num="0089">In addition, the program is distributed, for example, by selling, transferring, or lending a portable recording medium such as a DVD or a CD-ROM with the program recorded on it. Further, the program may be stored in a storage device of a server computer and transmitted from the server computer to another computer via a network, so that the program is distributed.</p><p id="p-0107" num="0090">For example, a computer executing the program first temporarily stores the program recorded on the portable recording medium or the program transmitted from the server computer in its own storage device. When the computer executes the processing, the computer reads the program stored in the recording medium of the computer and executes a process according to the read program. As another execution form of the program, the computer may directly read the program from the portable recording medium and execute processing in accordance with the program. Further, each time the program is transmitted from the server computer to the computer, the computer executes processing in order in accordance with the received program. In another configuration, the processing may be executed through a so-called application service provider (ASP) service in which functions of the processing are implemented just by issuing an instruction to execute the program and obtaining results without transmission of the program from the server computer to the computer. Further, the program in this mode is assumed to include information which is provided for processing of a computer and is equivalent to a program (data or the like that has characteristics of regulating processing of the computer rather than being a direct instruction to the computer).</p><p id="p-0108" num="0091">In addition, although the device is configured by executing a predetermined program on a computer in this mode, at least a part of the processing details may be implemented by hardware.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A pause estimation model learning apparatus comprising a processor configured to execute a method comprising:<claim-text>performing morphological analysis on training text data to provide M types of information, M being an integer that is equal to or larger than 2;</claim-text><claim-text>combining N pieces of information, among the M pieces of information, to be an input feature when a predetermined condition is satisfied;</claim-text><claim-text>selecting predetermined one of the N pieces of information to be the input feature when the predetermined condition is not satisfied, N being an integer that is equal to or larger than 2 and equal to or smaller than M; and</claim-text><claim-text>learning a pause estimation model by using the input feature and a pause correct label including a pause position.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The pause estimation model learning apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the performing further includes providing information &#x201c;writing&#x201d; and information &#x201c;part-of-speech&#x201d;, and</claim-text><claim-text>the combining combines the information &#x201c;writing&#x201d; and the information &#x201c;part-of-speech&#x201d; when &#x201c;part-of-speech&#x201d; provided to a morpheme is any one of &#x201c;postpositional particle&#x201d;, &#x201c;topic-indicating particle&#x201d;, and &#x201c;verbal suffix&#x201d;, to be the input feature, and selects the information &#x201c;part-of-speech&#x201d; as the input feature when &#x201c;part-of-speech&#x201d; provided to a morpheme is none of &#x201c;postpositional particle&#x201d;, &#x201c;topic-indicating particle&#x201d;, and &#x201c;verbal suffix&#x201d;, where N=2 holds.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The pause estimation model learning apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the combining combines N pieces of information, among the M pieces of information, to be an input feature xq when the predetermined certain condition is satisfied, and selects predetermined one of the N pieces of information as an input feature xq when the predetermined condition is not satisfied, where q=1, Q holds, Q being an integer that is equal to or larger than 2,</claim-text><claim-text>the learning learns Q pieces of pause estimation models by using Q types of input features xq and a pause correct label, and</claim-text><claim-text>the processor further configured to execute a method comprising: comprises</claim-text><claim-text>evaluating the Q pieces of pause estimation models by using a verification text data and a verification pause correct label; and</claim-text><claim-text>selecting a model that is most highly evaluated.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. A pause estimation apparatus comprising a processor configured to execute a method for estimating a pause position in text data by using the pause estimation model comprising:<claim-text>performing morphological analysis on the text data, to provide M types of information;</claim-text><claim-text>combining N pieces of information, among the M pieces of information, to be an input feature when a predetermined condition is satisfied, and select predetermined one of the N pieces of information to be the input feature when the predetermined condition is not satisfied;</claim-text><claim-text>receiving the input feature; and</claim-text><claim-text>estimating the pause position, by using the pause estimation model.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. A pause estimation model learning method comprising:<claim-text>performing morphological analysis on training text data to provide M types of information, M being an integer that is equal to or larger than 2;</claim-text><claim-text>combining N pieces of information, among the M pieces of information, to be an input feature when a predetermined condition is satisfied, and selecting predetermined one of the N pieces of information to be the input feature when the predetermined condition is not satisfied, N being an integer that is equal to or larger than 2 and equal to or smaller than M; and</claim-text><claim-text>learning a pause estimation model by using the input feature and a pause correct label including a pause position.</claim-text></claim-text></claim><claim id="CLM-006-7" num="006-7"><claim-text><b>6</b>-<b>7</b>. (canceled)</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The pause estimation model learning apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the pause position corresponds to a position for putting an intermission in synthesizing speech.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The pause estimation model learning apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the pause estimation model estimates a timing of putting an intermission for implementing speech synthesis.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The pause estimation model learning apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the pause estimation model receives the input features as input and outputs a pause label.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The pause estimation model learning apparatus according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the evaluating is based at least one of an accuracy of estimation or a model size.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The pause estimation apparatus according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein<claim-text>the performing further includes providing information &#x201c;writing&#x201d; and information &#x201c;part-of-speech&#x201d;, and</claim-text><claim-text>the combining combines the information &#x201c;writing&#x201d; and the information &#x201c;part-of-speech&#x201d; when &#x201c;part-of-speech&#x201d; provided to a morpheme is any one of &#x201c;postpositional particle&#x201d;, &#x201c;topic-indicating particle&#x201d;, and &#x201c;verbal suffix&#x201d;, to be the input feature, and selects the information &#x201c;part-of-speech&#x201d; as the input feature when &#x201c;part-of-speech&#x201d; provided to a morpheme is none of &#x201c;postpositional particle&#x201d;, &#x201c;topic-indicating particle&#x201d;, and &#x201c;verbal suffix&#x201d;, where N=2 holds.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The pause estimation apparatus according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein<claim-text>the combining combines N pieces of information, among the M pieces of information, to be an input feature xq when the predetermined condition is satisfied, and selects predetermined one of the N pieces of information as an input feature xq when the predetermined condition is not satisfied, where q=1,2, . . . , Q holds, Q being an integer that is equal to or larger than 2,</claim-text><claim-text>the learning learns Q pieces of pause estimation models by using Q types of input features xq and a pause correct label, and</claim-text><claim-text>the processor further configured to execute a method comprising:</claim-text><claim-text>evaluating the Q pieces of pause estimation models by using a verification text data and a verification pause correct label; and</claim-text><claim-text>selecting a model that is most highly evaluated.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The pause estimation apparatus according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the pause position corresponds to a position for putting an intermission in synthesizing speech.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The pause estimation apparatus according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the pause estimation model estimates a timing of putting an intermission for implementing speech synthesis.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The pause estimation apparatus according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the pause estimation model receives the input features as input and outputs a pause label.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The pause estimation apparatus according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the evaluating is based at least one of an accuracy of estimation or a model size.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The pause estimation model learning method according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein<claim-text>the performing further includes providing information &#x201c;writing&#x201d; and information &#x201c;part-of-speech&#x201d;, and</claim-text><claim-text>the combining combines the information &#x201c;writing&#x201d; and the information &#x201c;part-of-speech&#x201d; when &#x201c;part-of-speech&#x201d; provided to a morpheme is any one of &#x201c;postpositional particle&#x201d;, &#x201c;topic-indicating particle&#x201d;, and &#x201c;verbal suffix&#x201d;, to be the input feature, and selects the information &#x201c;part-of-speech&#x201d; as the input feature when &#x201c;part-of-speech&#x201d; provided to a morpheme is none of &#x201c;postpositional particle&#x201d;, &#x201c;topic-indicating particle&#x201d;, and &#x201c;verbal suffix&#x201d;, where N=2 holds.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The pause estimation model learning method according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein<claim-text>the combining combines N pieces of information, among the M pieces of information, to be an input feature xq when the predetermined condition is satisfied, and selects predetermined one of the N pieces of information as an input feature xq when the predetermined condition is not satisfied, where q=1,2, . . . , Q holds, Q being an integer that is equal to or larger than 2,</claim-text><claim-text>the learning learns Q pieces of pause estimation models by using Q types of input features xq and a pause correct label, and</claim-text><claim-text>the method further comprising:</claim-text><claim-text>evaluating the Q pieces of pause estimation models by using a verification text data and a verification pause correct label; and</claim-text><claim-text>selecting a model that is most highly evaluated.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The pause estimation model learning method according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the pause position corresponds to a position for putting an intermission in synthesizing speech.</claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. The pause estimation model learning method according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the pause estimation model estimates a timing of putting an intermission for implementing speech synthesis.</claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. The pause estimation model learning method according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the pause estimation model receives the input features as input and outputs a pause label.</claim-text></claim></claims></us-patent-application>