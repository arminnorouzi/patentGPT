<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004452A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004452</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17852852</doc-number><date>20220629</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>KR</country><doc-number>10-2021-0084961</doc-number><date>20210629</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>54</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>11</main-group><subgroup>34</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>542</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>11</main-group><subgroup>3438</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20200101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">METHOD AND DEVICE FOR ANALYZING FEATURE-LEVEL USAGE OF APP</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Korea Advanced Institute of Science and Technology</orgname><address><city>Daejeon</city><country>KR</country></address></addressbook><residence><country>KR</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Lee</last-name><first-name>Sung-Ju</first-name><address><city>Daejeon</city><country>KR</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Cho</last-name><first-name>Hyunsung</first-name><address><city>Daejeon</city><country>KR</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Kim</last-name><first-name>Donghwi</first-name><address><city>Daejeon</city><country>KR</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Choi</last-name><first-name>Daeun</first-name><address><city>Daejeon</city><country>KR</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Provided are a method and a device for analyzing feature-level usage of an app.</p><p id="p-0002" num="0000">The method may include: detecting an event generated in a user terminal; extracting a user interface component for the detected event as a layout element; detecting a feature of the app based on the extracted layout element; and analyzing a usage at the detected feature-level.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="76.71mm" wi="158.75mm" file="US20230004452A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="175.01mm" wi="95.50mm" orientation="landscape" file="US20230004452A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="190.92mm" wi="111.76mm" orientation="landscape" file="US20230004452A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="154.94mm" wi="109.30mm" file="US20230004452A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="199.05mm" wi="104.06mm" orientation="landscape" file="US20230004452A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="183.90mm" wi="128.95mm" orientation="landscape" file="US20230004452A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="197.70mm" wi="104.22mm" orientation="landscape" file="US20230004452A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="153.67mm" wi="129.29mm" orientation="landscape" file="US20230004452A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="177.72mm" wi="126.49mm" orientation="landscape" file="US20230004452A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="122.26mm" wi="124.04mm" file="US20230004452A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION</heading><p id="p-0003" num="0001">This application claims priority to and the benefit of Korean Patent Application No. 10-2021-0084961 filed in the Korean Intellectual Property Office on Jun. 29, 2021, the entire contents of which are incorporated herein by reference.</p><heading id="h-0002" level="1">BACKGROUND OF THE DISCLOSURE</heading><heading id="h-0003" level="1">(a) Field of the Disclosure</heading><p id="p-0004" num="0002">The present disclosure relates to a method and a device for analyzing a feature-level usage of apps.</p><heading id="h-0004" level="1">(b) Description of the Related Art</heading><p id="p-0005" num="0003">Smart phones are capable of additionally installing various applications to flexibly provide various features of the user's choice. A smart phone user may access software markets, for example, app stores to download an app of the user's choice. As a distribution rate of the smart phones is increased and a time spent to use smart phones that provide various features utilized throughout the day significantly increases, problems of smart phone overuse or smart phone addiction have emerged.</p><p id="p-0006" num="0004">Excessive use of smart phones not only causes symptoms such as cessation and tolerance for smart phone use, disturbances in daily life, and orientation to virtual words, but also affects physical health such as sleep disorder, deteriorated vision function, and turtle neck syndrome. The problems are so recognizable that studies for measuring healthy smart phone use are being actively conducted.</p><p id="p-0007" num="0005">The above information disclosed in this Background section is only for enhancement of understanding of the background of the disclosure, and therefore it may contain information that does not form the prior art that is already known in this country to a person of ordinary skill in the art.</p><heading id="h-0005" level="1">SUMMARY OF THE DISCLOSURE</heading><p id="p-0008" num="0006">The present disclosure has been made in an effort to provide a method and a device for analyzing a usage amount, a use form, and a use pattern of the smart phone in the feature-level of the app which is more subdivided than in the units of apps.</p><p id="p-0009" num="0007">A method for analyzing feature-level usage of an app according to an example embodiment of the present disclosure may include: detecting an event generated in a user terminal; extracting a user interface component for the detected event as a layout element; detecting a feature of the app based on the extracted layout element; and analyzing a usage at the detected feature-level.</p><p id="p-0010" num="0008">In some example embodiments of the present disclosure, the method further includes selecting a feature detector for an app in use in the user terminal, and detecting the feature of the app includes detecting the feature of the app using the selected feature detector.</p><p id="p-0011" num="0009">In some example embodiments of the present disclosure, the app includes a first app and a second app, and selecting a feature detector includes: ending the use of a feature detector for the first app and starting the use of a feature detector for the second app when a session of the first app ends and a session of the second app starts.</p><p id="p-0012" num="0010">In some example embodiments of the present disclosure, extracting a user interface component for the detected event as a layout element includes: storing information about the user interface component as the layout element with a tree structure.</p><p id="p-0013" num="0011">In some example embodiments of the present disclosure, detecting the feature of the app includes: detecting an element selected while traversing the layout element with the tree structure; determining view information by analyzing the element; determining content information by analyzing a text associated with view information; and detecting the feature based on a combination of the view information and the content information.</p><p id="p-0014" num="0012">In some example embodiments of the present disclosure, determining view information includes: determining the view information according to a location, a size or index information of the selected element.</p><p id="p-0015" num="0013">In some example embodiments of the present disclosure, determining view information includes: determining view information according to a class name of a root view for the selected element.</p><p id="p-0016" num="0014">In some example embodiments of the present disclosure, determining view information includes: determining the view information according to whether there are content description and a text for the selected element.</p><p id="p-0017" num="0015">In some example embodiments of the present disclosure, determining view information includes: determining the view information according to viewIdResourceName information for the selected element.</p><p id="p-0018" num="0016">In some example embodiments of the present disclosure, detecting an event includes: detecting at least one of a scroll event, a click event, a focus event, a window transition event, and a window state transition event.</p><p id="p-0019" num="0017">A device for analyzing feature-level usage of an app according to an example embodiment of the present disclosure includes: an event detection module which detects an event generated in a user terminal; a layout element extraction module which extracts a user interface component for the detected event as a layout element; a layout element based feature detection module which detects a feature of the app based on the extracted layout element; and a feature-level usage analysis module which analyzes a usage at the detected feature level.</p><p id="p-0020" num="0018">In some example embodiments of the present disclosure, the device may further include: a feature detector selection module which selects a feature detector for an app in use in the user terminal and the layout element based feature detection module may detect the feature of the app using the selected feature detector.</p><p id="p-0021" num="0019">In some example embodiments of the present disclosure, the app includes a first app and a second app, and the feature detector selection module ends the use of a feature detector for the first app and starts the use of a feature detector for the second app when a session of the first app ends and a session of the second app starts.</p><p id="p-0022" num="0020">In some example embodiments of the present disclosure, the layout element extraction module stores information about the user interface component as the layout element with a tree structure.</p><p id="p-0023" num="0021">In some example embodiments of the present disclosure, the layout element based feature detection module detects an element selected while traversing the layout element with the tree structure, determines view information by analyzing the element, determines content information by analyzing a text associated with view information, and detects the feature based on a combination of the view information and the content information.</p><p id="p-0024" num="0022">In some example embodiments of the present disclosure, the layout element based feature detection module determines the view information according to a location, a size or index information of the selected element.</p><p id="p-0025" num="0023">In some example embodiments of the present disclosure, the layout element based feature detection module determines view information according to a class name of a root view for the selected element.</p><p id="p-0026" num="0024">In some example embodiments of the present disclosure, the layout element based feature detection module determines the view information according to whether there is content description and a text for the selected element.</p><p id="p-0027" num="0025">In some example embodiments of the present disclosure, the layout element based feature detection module determines the view information according to viewIdResourceName information for the selected element.</p><p id="p-0028" num="0026">In some example embodiments of the present disclosure, the event detection module detects at least one of a scroll event, a click event, a focus event, a window transition event, and a window state transition event.</p><p id="p-0029" num="0027">According to an example embodiment of the present disclosure, a usage, use form, or use pattern of the smart phone is analyzed at a feature level of the app to analyze which app is used by the user for a long time, which app is frequently used in the app-unit analysis of the related art. Further, even in one app, which feature is spent a lot of time by the user and which feature is frequently used are also specifically analyzed. Simultaneously, when the data for the analysis is collected, the personal information protection issue may not be caused.</p><p id="p-0030" num="0028">Further, according to the example embodiment of the present disclosure, such detailed analysis is executed as a background service without going through an artificial analysis process by experts to automatically collect a usage, a use form, or a use pattern of a smart phone to ensure the reliable analysis data in a short time.</p><p id="p-0031" num="0029">Further, according to the example embodiment of the present disclosure, since an interface which is capable of customizing a feature-level of an app to be analyzed is provided to a developer or an analyzer, when an app which is newly used is analyzed in the feature level, the method and the device for analyzing a feature-level of an app may easily set the analysis policy without going through complicated coding tasks so that the usability and the usage convenience may be ensured.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0006" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0032" num="0030"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram for explaining a device for analyzing a feature-level usage of an app according to an example embodiment of the present disclosure.</p><p id="p-0033" num="0031"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a view for explaining an example implementation example for a device for analyzing a feature-level usage of an app according to an example embodiment of the present disclosure.</p><p id="p-0034" num="0032"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flowchart for explaining a method for analyzing a feature-level usage of an app according to an example embodiment of the present disclosure.</p><p id="p-0035" num="0033"><figref idref="DRAWINGS">FIGS. <b>4</b> to <b>7</b></figref> are views for explaining some example implementation examples of a device and a method for analyzing a feature-level usage of an app according to an example embodiment of the present disclosure.</p><p id="p-0036" num="0034"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a view for explaining an example applied example of a device and a method for analyzing a feature-level usage of an app according to an example embodiment of the present disclosure.</p><p id="p-0037" num="0035"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a block diagram for explaining a computing device for implementing a method and a device for analyzing a feature-level usage of an app according to an example embodiment of the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0007" level="1">DETAILED DESCRIPTION OF THE EMBODIMENTS</heading><p id="p-0038" num="0036">Hereinafter, the present disclosure will be described more fully hereinafter with reference to the accompanying drawings, in which example embodiments of the disclosure are shown. As those skilled in the art would realize, the described embodiments may be modified in various different ways, all without departing from the spirit or scope of the present disclosure. Accordingly, the drawings and description are to be regarded as illustrative in nature and not restrictive. Like reference numerals designate like elements throughout the specification.</p><p id="p-0039" num="0037">In the specification and the claims, unless explicitly described to the contrary, the word &#x201c;comprise&#x201d; and variations such as &#x201c;comprises&#x201d; or &#x201c;comprising&#x201d;, will be understood to imply the inclusion of stated elements but not the exclusion of any other elements.</p><p id="p-0040" num="0038">In addition, the terms &#x201c;-er&#x201d;, &#x201c;-or&#x201d; and &#x201c;module&#x201d; described in the specification mean units for processing at least one function and operation and can be implemented by hardware components or software components and combinations thereof.</p><p id="p-0041" num="0039"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram for explaining a device for analyzing a feature-level usage of an app according to an example embodiment of the present disclosure and <figref idref="DRAWINGS">FIG. <b>2</b></figref> is a view for explaining an example implementation example for a device for analyzing a feature-level usage of an app according to an example embodiment of the present disclosure.</p><p id="p-0042" num="0040">In the related art, in order to identify a user's smart phone usage state, a usage time and a using frequency were tracked in the unit of apps. However, according to the app-level tracking method, it is difficult to analyze how long and which sub features are used by the user in the app. In order to provide a healthy smart phone usage environment, it is necessary to identify the usage or the usage pattern of the user in the unit which is subdivided more than the app level. However, the app does not provide a feature for tracking the user's usage and usage pattern for every feature.</p><p id="p-0043" num="0041">In order to solve this problem, a device <b>10</b> for analyzing a feature-level usage of an app <b>20</b> according to an example embodiment of the present disclosure detects various features of the app using layout information of the app <b>20</b> and tracks the user's feature level usage or feature level use pattern at the detected feature level to provide fine-grained analysis for the smart phone usage. Further, when the data is collected for the analysis, the device <b>10</b> for analyzing a feature-level usage of an app <b>20</b> may provide a feature for preventing a personal information protection issue (privacy issue).</p><p id="p-0044" num="0042">Referring to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the device <b>10</b> for analyzing a feature-level usage of an app <b>20</b> may be included in a computing device <b>1</b>. Here, the device <b>10</b> for analyzing a feature-level usage of an app <b>20</b> may be implemented by a hardware device, a combination of a hardware device and software, or software. The device <b>10</b> for analyzing a feature-level usage of an app <b>20</b> may be driven or executed together with the app <b>20</b> executed in the computing device <b>1</b> to analyze the app usage.</p><p id="p-0045" num="0043">In the present example embodiment, the device <b>10</b> for analyzing a feature-level usage of an app <b>20</b> includes an event detection module <b>110</b>, a layout element extraction module <b>120</b>, a feature detector selection module <b>130</b>, a layout element based feature detection module <b>140</b> and a feature-level usage analysis module <b>150</b>. Referring to <figref idref="DRAWINGS">FIG. <b>2</b></figref> together, the event detection module <b>110</b>, the layout element extraction module <b>120</b>, the feature detector selection module <b>130</b> and the layout element based feature detection module <b>140</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> may correspond to android accessibility API (Application Programming Interface), &#x201c;LayoutLogger&#x201d;, &#x201c;FeatureDetectManager&#x201d; and &#x201c;AppFeatureDetector&#x201d; of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, respectively.</p><p id="p-0046" num="0044">The event detection module <b>110</b> may detect an event generated in a user terminal. Here, the user terminal may be a smart phone. The scope of the present disclosure is not limited thereto so that the user terminal may include an arbitrary computing device which installs and executes an app or an application to implement functions, such as a tablet computer, a laptop computer, a desktop computer, and a smart watch.</p><p id="p-0047" num="0045">The event generated in the user terminal may be various types of events which are generated while the user interacts with a smart phone or generated while the app is running on the smart phone, such as a scroll event, a click event, a focus event, a window transition event, and a window state transition event.</p><p id="p-0048" num="0046">The event detection module <b>110</b> may be implemented by an accessibility service which is provided through an android accessibility API to detect the event generated in the user terminal. The event generated in the user terminal, for example, may be analyzed by means of &#x201c;OnAccessibilityEvent&#x201d; method of &#x201c;AccessibilityService&#x201d; class of the accessibility service.</p><p id="p-0049" num="0047">The layout element extraction module <b>120</b> extracts a user interface component for the event detected by the event detection module <b>110</b> as a layout element.</p><p id="p-0050" num="0048">Examples of the user interface component for the detected event may include an element selected from a menu bar, a layout of a screen, a class name of activity/components, a window ID, previous features, a text (preferably, hashed text), content description (preferably, hashed content description), rectangular bounds (Rect bounds), selected element (isSelected), editable elements (isEditable), android view resource ID and name (viewIdResourceName). Of course, the user interface component for the detected event is not limited to those mentioned above and may include an arbitrary element which is displayed on the user terminal to interact with the user and generate an event.</p><p id="p-0051" num="0049">The layout element extraction module <b>120</b> may store information about a user interface component for the detected event as described above as a layout element having a tree structure. Such a task may be performed by &#x201c;CreateLayoutElement&#x201d; method which is implemented in &#x201c;LayoutLogger&#x201d; of <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0052" num="0050">One of the reasons for adopting the tree structure to configure the layout element is to increase the efficiency when the layout element based feature detection module <b>140</b> performs a task of searching and analyzing the layout element. However, the scope of the present disclosure is not necessarily limited thereto and for example, the layout element may be stored as another arbitrary data structure such as a linked list.</p><p id="p-0053" num="0051">Particularly, in consideration of a privacy protection issue, the layout element extraction module <b>120</b> hashes a text displayed on the screen and data for content description or performs a process for minimizing collected data by extracting only a keyword which is absolutely necessary for the analysis.</p><p id="p-0054" num="0052">In the meantime, even though the type or a screen size of the user terminal is different, in order to ensure the analysis reliability, when the feature is detected, the layout element extraction module <b>120</b> may not use information about the size of the user interface component or minimize the usage proportion thereof. For example, instead of information about the terminal type or the screen size which may vary depending on the user, information about the class name determined during the development process is mainly used to ensure the operation reliability in spite of the deviation of the usage environment between users.</p><p id="p-0055" num="0053">Further, the layout element extraction module <b>120</b> may use a hash map which matches the detected feature and a window ID to prevent interruption of tracking one feature session due to the unexpected layout detection. Accordingly, even though the unexpected layout appears for reasons such as updating of the app interface, if the window ID is the same, it is determined that the detected feature is also the same so that the operation stability may be ensured.</p><p id="p-0056" num="0054">In some example embodiments of the present disclosure, &#x201c;SaveToLbs&#x201d; of <figref idref="DRAWINGS">FIG. <b>2</b></figref> may be implemented in the layout element extraction module <b>120</b> and this may produce a log file which may be used for the analysis in the future.</p><p id="p-0057" num="0055">The feature detector selection module <b>130</b> selects a feature detector for an app which is being used in the user terminal.</p><p id="p-0058" num="0056">The feature detector selection module <b>130</b> may be performed by &#x201c;MapProperDetector&#x201d; method implemented in &#x201c;FeatureDetectManager&#x201d; of <figref idref="DRAWINGS">FIG. <b>2</b></figref> and the feature provided to the user is different for every app so that a policy for how to analyze the layout element provided from the event detection module <b>110</b> and the layout element extraction module <b>120</b> is defined.</p><p id="p-0059" num="0057">For example, when an app which is being used in the user terminal is Instagram, the &#x201c;MapProperDetector&#x201d; method may provide &#x201c;InstagramFeatureDetector&#x201d; including a policy optimized for a feature provided from the Instagram. Similarly, when the app which is being used in the user terminal is Kakaotalk, the &#x201c;MapProperDetector&#x201d; method provides &#x201c;KakaoFeatureDetector&#x201d; including a policy optimized for a feature provided from the Kakaotalk, when the app which is being used in the user terminal is Facebook, provides &#x201c;FacebookFeatureDetector&#x201d; including a policy optimized for a feature provided from the Facebook, and when the app which is being used in the user terminal is Youtube, provides &#x201c;YouTubeFeatureDetector&#x201d; including a policy optimized for a feature provided from YouTube. As described above, the policy for every app provided from the feature detector selection module <b>130</b> may be utilized as a reference for analysis in the layout element based feature detection module <b>140</b> and the feature-level usage analysis module <b>150</b>.</p><p id="p-0060" num="0058">Further, when an app which is being used in the user terminal is switched from a first app to a second app, that is, a session of the first app ends and a session of the second app starts, the feature detector selection module <b>130</b> ends the usage of the feature detector for the first app and starts the usage of the feature detector for the second app. For example, when the session of the Instagram app ends and the session of the Facebook starts in the user terminal, the feature detector selection module <b>130</b> ends the usage of the feature detector for the Instagram app &#x201c;InstagramFeatureDetector&#x201d; and starts the usage of the feature detector for the Facebook app &#x201c;FacebookFeatureDetector&#x201d;.</p><p id="p-0061" num="0059">In some example embodiments of the present disclosure, &#x201c;ManageSession&#x201d; of <figref idref="DRAWINGS">FIG. <b>2</b></figref> may be implemented in the feature detector selection module <b>130</b>, which may detect the application switch.</p><p id="p-0062" num="0060">The layout element based feature detection module <b>140</b> may detect the feature of the application based on the layout element extracted by the layout element extraction module <b>120</b>. Particularly, the layout element based feature detection module <b>140</b> may detect what is the feature of the app which is currently in use. Such a task may be performed by &#x201c;DetectFeature&#x201d; method implemented in &#x201c;AppFeatureDetector&#x201d; of <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0063" num="0061">For example, when the app which is being used in the user terminal is Instagram, a selected element, among layout elements extracted by the layout feature extraction module <b>120</b>, a lastly used feature, and a layout &#x26; content are analyzed using a feature detector &#x201c;InstagramFeatureDetector&#x201d; including a policy optimized for a feature provided from the Instagram to detect a detailed feature of the Instagram app which is being used by the user.</p><p id="p-0064" num="0062">Particularly, the layout element based feature detection module <b>140</b> detects an element selected while traversing the layout element having a tree structure, analyzes the detected element to determine view information, analyzes a text associated with the view information to determine content information, and detects a currently in use feature based on a combination of the view information and the content information.</p><p id="p-0065" num="0063">In the example embodiment of the present disclosure, the layout element based feature detection module <b>140</b> may determine the view information according to a position, a size, or index information of the selected element.</p><p id="p-0066" num="0064">In the example embodiment of the present disclosure, the layout element based feature detection module <b>140</b> may determine the view information according to the class name of a root view for the selected element.</p><p id="p-0067" num="0065">In the example embodiment of the present disclosure, the layout element based feature detection module <b>140</b> determines the view information according to the content description for the selected element and presence of the text.</p><p id="p-0068" num="0066">In the example embodiment of the present disclosure, the layout element based feature detection module <b>140</b> may determine the view information according to viewIdResourceName information for the selected element.</p><p id="p-0069" num="0067">The feature-level usage analysis module <b>150</b> analyzes the usage at the feature-level detected by the layout element based feature detection module <b>150</b> and displays the analyzed result to the user and induces a survey to the user.</p><p id="p-0070" num="0068"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flowchart for explaining a method for analyzing a feature-level usage of an app according to an example embodiment of the present disclosure.</p><p id="p-0071" num="0069">Referring to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the method for analyzing a feature-level usage of an app according to an example embodiment of the present disclosure includes a step S<b>310</b> of detecting an event generated in a user terminal, a step S<b>320</b> of extracting a user interface component for the detected event as a layout element, a step S<b>330</b> of selecting a feature detector for an app which is being used in the user terminal, a step S<b>340</b> of detecting a feature of an app based on the extracted layout element using the selected feature detector, and a step S<b>350</b> of analyzing the detected feature-level usage.</p><p id="p-0072" num="0070">The method for analyzing a feature-level usage of an app may be described in more detail with reference to the description of the event detection module <b>110</b>, the layout element extraction module <b>120</b>, the feature detector selection module <b>130</b>, the layout element based feature detection module <b>140</b>, and the feature-level usage analysis module <b>150</b> which have been described above with reference to <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>2</b></figref> and a redundant description will be omitted.</p><p id="p-0073" num="0071"><figref idref="DRAWINGS">FIGS. <b>4</b> to <b>7</b></figref> are views for explaining some example implementation examples of a device and a method for analyzing a feature-level usage of an app according to an example embodiment of the present disclosure.</p><p id="p-0074" num="0072">Referring to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the layout element extraction module <b>120</b> extracts a layout of a screen including &#x201c;graphicdesign&#x201d;, popular posts, and recent posts&#x201d; as texts, as a user interface component for the detected event and the selected element as a layout element. The feature detector selection module <b>130</b> selects &#x201c;InstagramFeatureDetector&#x201d; including a policy for the Instagram app and the layout element based feature detection module <b>140</b> may detect a feature which is current in use, for example, detect that the user is currently using a following feed feature based thereon.</p><p id="p-0075" num="0073">Further, the layout element extraction module <b>120</b> extracts &#x201c;ladder game&#x201d; as a text, &#x201c;com.kakao.talk:id/search_card_sharp&#x201d; as viewIdResourceName, and &#x201c;com.kakao.talk.activity.search.card.SharpCardActivity&#x201d; which is a class name of the activity as a layout element, as a user interface component for the detected event. The feature detector selection module <b>130</b> selects &#x201c;KakaoFeatureDetector&#x201d; including a policy for the Kakao app and the layout element based feature detection module <b>140</b> may detect the feature which is currently in use, for example, detect that the user is currently using the game (ladder game) feature, based thereon.</p><p id="p-0076" num="0074">In the meantime, referring to <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the layout element extraction module <b>120</b> extracts infinite scrolling as a layout element, as a user interface component for the detected event, the feature detector selection module <b>130</b> selects &#x201c;FacebookFeatureDetector&#x201d; including a policy for the Facebook app, and the layout element based feature detection module <b>140</b> may detect a currently in use feature, for example, detect that the user is using a feature of scrolling a newsfeed, based thereon.</p><p id="p-0077" num="0075">In the meantime, referring to <figref idref="DRAWINGS">FIG. <b>6</b></figref>, in order to prevent the interruption of the feature session due to the unexpected layout, an example that if the window ID is the same, determinest that the detected feature is also the same using a hash map of matching the detected feature and the window ID is illustrated.</p><p id="p-0078" num="0076">In the meantime, referring to <figref idref="DRAWINGS">FIG. <b>7</b></figref>, when the layout element based feature detection module <b>140</b> detects a following feed feature, a suggested post feature, and other's post feature, an example that induces a survey to the user while displaying a usage and a use pattern of the features on a graph is illustrated.</p><p id="p-0079" num="0077"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a view for explaining an example applied example of a device and a method for analyzing a feature-level usage of an app according to an example embodiment of the present disclosure.</p><p id="p-0080" num="0078">Referring to <figref idref="DRAWINGS">FIG. <b>8</b></figref>, data which analyzes the feature-level usage may be used as various applications including prediction of feature use probability.</p><p id="p-0081" num="0079">That is, the method and apparatus for analyzing a feature-level usage of an app according to the example embodiments of the present disclosure analyze the use of the smart phone at every feature-level, and the app usage or the use pattern may be decomposed into a &#x201c;feature&#x201d; usage or a sequence of a use pattern. For example, the Instagram session may be divided into sub features such as browsing the feed of following user's posts, browsing the feed of system-recommended posts, direct messaging, and viewing stories. Further, use of various app features is tracked using app layout information and the tracking result (usage or use pattern) is provided to the user or will be used for analysis or application later.</p><p id="p-0082" num="0080">The method and apparatus for analyzing a feature-level usage of an app according to the example embodiments of the present disclosure have the following characteristics.<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0000">    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0081">Criteria for features: a feature of a target application is extracted by a combination of two criteria of (1) a user action and (2) a nature of content. For example, the features are grouped based on user actions such as viewing, searching, uploading, and chatting. A nature of content includes a form of the content and a source of the content and examples of the form of the content include videos, feeds, notices, and posts and examples of the source of the content include user's posts, posts that the user follows, and the other user's posts that the user does not follow. The following example features may be defined in consideration of two criteria.</li>    </ul>    </li></ul></p><p id="p-0083" num="0000"><tables id="TABLE-US-00001" num="00001"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="center"/><tbody valign="top"><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>1) In case of KakaoTalk app:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="77pt" align="left"/><colspec colname="2" colwidth="140pt" align="left"/><tbody valign="top"><row><entry>Feature name</entry><entry>Description</entry></row><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row><row><entry>CONTACTS</entry><entry>viewing others' profile</entry></row><row><entry>CHAT</entry><entry>chatting with others</entry></row><row><entry>NEWS</entry><entry>reading news recommended by Kakaotalk</entry></row><row><entry>ETC</entry><entry>viewing a tab with various sub-features such as</entry></row><row><entry/><entry>payment, emoticon shops, etc.</entry></row><row><entry>IN_CHAT_SEARCH</entry><entry>doing web search inside a chat room</entry></row><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0084" num="0000"><tables id="TABLE-US-00002" num="00002"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="center"/><tbody valign="top"><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>2) In case of YouTube app:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="63pt" align="left"/><colspec colname="2" colwidth="154pt" align="left"/><tbody valign="top"><row><entry>Feature name</entry><entry>Description</entry></row><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row><row><entry>BROWSE_HOME</entry><entry>viewing home tab with recommended videos</entry></row><row><entry>EXPLORE</entry><entry>viewing explore tab with trending videos</entry></row><row><entry>SUBSCRIPTIONS</entry><entry>viewing subscriptions tab with subscribing channels'</entry></row><row><entry/><entry>videos</entry></row><row><entry>NOTIFICATIONS</entry><entry>checking notification</entry></row><row><entry>MY_LIBRARY</entry><entry>viewing library tab with recent, history, etc.</entry></row><row><entry>SEARCH</entry><entry>searching videos</entry></row><row><entry>CHANNEL_PAGE</entry><entry>viewing a channel's page</entry></row><row><entry>WATCH_VIDEO</entry><entry>watching videos</entry></row><row><entry>COMMENTS</entry><entry>reading comments</entry></row><row><entry>PLAYLISTS</entry><entry>exploring playlists</entry></row><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0085" num="0000"><tables id="TABLE-US-00003" num="00003"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="center"/><tbody valign="top"><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>3) In case of Instagram app:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="77pt" align="left"/><colspec colname="2" colwidth="140pt" align="left"/><tbody valign="top"><row><entry>Feature name</entry><entry>Description</entry></row><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row><row><entry>FOLLOWING_POSTS</entry><entry>viewing the feed with posts of the following</entry></row><row><entry/><entry>users</entry></row><row><entry>SUGGESTED_POSTS</entry><entry>viewing the feed with posts recommended by</entry></row><row><entry/><entry>Instagram</entry></row><row><entry>SEARCH</entry><entry>searching accounts, tags, places, etc.</entry></row><row><entry>NOTIFICATIONS</entry><entry>checking notification</entry></row><row><entry>MY_POSTS</entry><entry>viewing profile or posts of the user</entry></row><row><entry>OTHER'S_POSTS</entry><entry>viewing profile or posts of other users</entry></row><row><entry>DIRECT_MESSAGE</entry><entry>chatting with other users using direct message</entry></row><row><entry>VIEW_STORY</entry><entry>viewing Instagram story</entry></row><row><entry>UPLOAD_POST</entry><entry>uploading Instagram post</entry></row><row><entry>UPLOAD_STORY</entry><entry>uploading Instagram story</entry></row><row><entry>VIEW_BY_HASHTAG</entry><entry>viewing posts by hashtags</entry></row><row><entry>IN_APP_WEB</entry><entry>using Instagram web browser</entry></row><row><entry>WATCH_VIDEO</entry><entry>viewing videos</entry></row><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0086" num="0000"><tables id="TABLE-US-00004" num="00004"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="center"/><tbody valign="top"><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>4) In case of Facebook app:</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="84pt" align="left"/><colspec colname="2" colwidth="133pt" align="left"/><tbody valign="top"><row><entry>Feature name</entry><entry>Description</entry></row><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row><row><entry>NEWS_FEED</entry><entry>viewing the Facebook news feed</entry></row><row><entry>GROUPS</entry><entry>viewing the posts from a group</entry></row><row><entry>WATCH_VIDEO</entry><entry>watching videos</entry></row><row><entry>MY_TIMELINE</entry><entry>viewing profile or posts of the user</entry></row><row><entry>OTHER'S_TIMELINE</entry><entry>viewing profile or posts of other users</entry></row><row><entry>NOTIFICATIONS</entry><entry>checking notification</entry></row><row><entry>MENU</entry><entry>exploring Facebook menu</entry></row><row><entry>PAGE'S_TIMELINE</entry><entry>viewing the posts from a page</entry></row><row><entry>MESSENGER</entry><entry>chatting with other users using Facebook</entry></row><row><entry/><entry>messenger</entry></row><row><entry>SEARCH</entry><entry>searching accounts, tags, posts, etc</entry></row><row><entry>IN_APP_WEB</entry><entry>using Facebook web browser</entry></row><row><entry>VIEW_POST</entry><entry>viewing a post</entry></row><row><entry>COMMENTS</entry><entry>viewing comments of a post</entry></row><row><entry>VIEW_STORY</entry><entry>viewing Facebook story</entry></row><row><entry>UPLOAD_POST</entry><entry>uploading Facebook post</entry></row><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup></table></tables><ul id="ul0003" list-style="none">    <li id="ul0003-0001" num="0000">    <ul id="ul0004" list-style="none">        <li id="ul0004-0001" num="0082">Design and Implementation: a currently in use feature is detected using Android Accessibility API. UI constituent element information having a tree structure for all scroll, click, and focus events is searched. In order to detect the feature use, some of the information is selected and the element selected from the menu bar becomes a good indicator to represent the feature in use. Further, the layout information of each component is also used. Class names of the activity and the component provided by a developer are used to detect more complicated features and a window ID and a previously detected feature are used to analyze the use of various features in the session. Finally, hashed text data such as content description may be used for minimum use.</li>        <li id="ul0004-0002" num="0083">Personal information protection (Privacy): all screen information is monitored to detect the feature use so that a personal information protection issue needs to be considered. In the collected data, information about the contents of the screen or information exposing a subject should not be included, excluding the text information. Two types of text information including a text of the content and content description (a developer for a purpose of the UI element is added) may be used and in order to minimize the risk for the personal information, all text information is used with a hash format. Further, such information is used only to understand whether specific words or phrases are displayed on the screen. For example, whether the current feature is related to the viewing of posts is identified by identifying only whether the word &#x201c;post&#x201d; is present in the head using the hashed text.</li>        <li id="ul0004-0003" num="0084">Compatibility: the application is configured to consistently operate regardless of various screen sizes. For example, in order to detect the feature use, size information of the UI component may vary depending on the device so that the size information may not be utilized. Instead of that, a class name of the component which is set to be the same in all the devices may be mainly used. Then, pre-determined indicators are captured from the screen to detect the use of the feature. However, indicators for all combinations of the screen contents may not be defined in advance so that unspecified cases are labeled with undefined features. In this case, an undefined feature U is detected while using some pre-defined feature A, so that the pre-defined feature A may be falsely notified as closed. In order to prevent the unexpected interruption, a hash map matching the window ID having the detected feature A is used so that when the user views the screen with the same window ID, it is considered that the same feature A is still in use.</li>    </ul>    </li></ul></p><p id="p-0087" num="0085"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a block diagram for explaining a computing device for implementing a method and a device for analyzing a feature-level usage of an app according to an example embodiment of the present disclosure.</p><p id="p-0088" num="0086">Referring to <figref idref="DRAWINGS">FIG. <b>9</b></figref>, a method and a device for analyzing a feature-level usage of an app according to an example embodiment of the present disclosure may be implemented using a computing device <b>50</b>.</p><p id="p-0089" num="0087">The computing device <b>50</b> may include at least one of a processor <b>510</b>, a memory <b>530</b>, a user interface input device <b>540</b>, a user interface output device <b>550</b>, and a storage device <b>560</b> which communicate with each other via a bus <b>520</b>. The computing device <b>50</b> may include a network interface <b>570</b> which is electrically connected to the network <b>40</b>, for example, a wireless network. The network interface <b>570</b> may transmit or receive a signal with the other entity via the network <b>40</b>.</p><p id="p-0090" num="0088">The processor <b>510</b> may be an application processor (AP) or a central processing unit (CPU) or an arbitrary semiconductor device which executes instructions stored in the memory <b>530</b> or the storage device <b>560</b>. The processor <b>510</b> may be configured to implement the features and methods described above with regard to <figref idref="DRAWINGS">FIGS. <b>1</b> to <b>8</b></figref>.</p><p id="p-0091" num="0089">The memory <b>530</b> and the storage device <b>560</b> may include various types of volatile or non-volatile storage media. For example, the memory includes a read-only memory (ROM) <b>531</b> and a random access memory (RAM) <b>532</b>. In the example embodiment of the present disclosure, the memory <b>530</b> may be located inside or outside the processor <b>510</b> and the memory <b>530</b> may be connected to the processor <b>510</b> by means of various known means.</p><p id="p-0092" num="0090">Further, at least some of the features of the device for analyzing a feature-level usage of an app according to the example embodiment of the present disclosure may be implemented as programs or software which is executed in the computing device <b>50</b> and the programs or software is stored in a computer readable medium.</p><p id="p-0093" num="0091">Further, at least some of the features of the device for analyzing a feature-level usage of an app according to the example embodiment of the present disclosure may be implemented as hardware which is electrically connected to the computing device <b>50</b>.</p><p id="p-0094" num="0092">According to the example embodiment of the present disclosure which has been described so far, a usage, a use form, or a use pattern of the smart phone is analyzed at a feature level of the app to analyze which app is used by the user for a long time and which app is frequently used in the app-unit analysis of the related art. Further, even in one app, which feature is spent a lot of time by the user and which feature is frequently used are also specifically analyzed. Simultaneously, when the data for the analysis is collected, the personal information protection issue may not be caused.</p><p id="p-0095" num="0093">Further, according to the example embodiment of the present disclosure, such detailed analysis is executed as a background service without going through an artificial analysis process by experts to automatically collect a usage, a use form, or a use pattern of a smart phone to ensure the reliable analysis data in a short time.</p><p id="p-0096" num="0094">Further, according to the example embodiment of the present disclosure, since an interface which is capable of customizing a feature-level of an app to be analyzed is provided to a developer or an analyzer, when an app which is newly used is analyzed in the feature level, the method and the device for analyzing a feature-level of an app may easily set the analysis policy without going through complicated coding tasks so that the usability may be increased and the usage convenience may be ensured.</p><p id="p-0097" num="0095">While this disclosure has been described in connection with what is presently considered to be practical example embodiments, it is to be understood that the disclosure is not limited to the disclosed embodiments. On the contrary, it is intended to cover various modifications and equivalent arrangements included within the spirit and scope of the appended claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method for analyzing feature-level usage of an app, comprising:<claim-text>detecting an event generated in a user terminal;</claim-text><claim-text>extracting a user interface component for the detected event as a layout element;</claim-text><claim-text>detecting a feature of the app based on the extracted layout element; and</claim-text><claim-text>analyzing a usage at the detected feature-level.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>selecting a feature detector for an app in use in the user terminal,</claim-text><claim-text>wherein detecting the feature of the app includes:</claim-text><claim-text>detecting the feature of the app using the selected feature detector.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein:<claim-text>the app includes a first app and a second app, and</claim-text><claim-text>selecting a feature detector includes:</claim-text><claim-text>ending the use of a feature detector for the first app and starting the use of a feature detector for the second app when a session of the first app ends and a session of the second app starts.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein extracting a user interface component for the detected event as a layout element includes:<claim-text>storing information about the user interface component as the layout element with a tree structure.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein detecting the feature of the app includes:<claim-text>detecting an element selected while traversing the layout element with the tree structure;</claim-text><claim-text>determining view information by analyzing the element;</claim-text><claim-text>determining content information by analyzing a text associated with view information; and</claim-text><claim-text>detecting the feature based on a combination of the view information and the content information.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein determining view information includes:<claim-text>determining the view information according to a location, a size or index information of the selected element.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein: determining view information includes:<claim-text>determining view information according to a class name of a root view for the selected element.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein determining view information includes:<claim-text>determining the view information according to whether there are content description and a text for the selected element.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein determining view information includes:<claim-text>determining the view information according to viewIdResourceName information for the selected element.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein detecting an event includes:<claim-text>detecting at least one of a scroll event, a click event, a focus event, a window transition event, and a window state transition event.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. A device for analyzing feature-level usage of an app, comprising:<claim-text>an event detection module which detects an event generated in a user terminal;</claim-text><claim-text>a layout element extraction module which extracts a user interface component for the detected event as a layout element;</claim-text><claim-text>a layout element based feature detection module which detects a feature of the app based on the extracted layout element; and</claim-text><claim-text>a feature-level usage analysis module which analyzes a usage at the detected feature level.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The device of <claim-ref idref="CLM-00011">claim 11</claim-ref>, further comprising:<claim-text>a feature detector selection module which selects a feature detector for an app in use in the user terminal,</claim-text><claim-text>wherein the layout element based feature detection module detects the feature of the app using the selected feature detector.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The device of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the app includes a first app and a second app, and<claim-text>the feature detector selection module ends the use of a feature detector for the first app and starts the use of a feature detector for the second app when a session of the first app ends and a session of the second app starts.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The device of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the layout element extraction module stores information about the user interface component as the layout element with a tree structure.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The device of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the layout element based feature detection module detects an element selected while traversing the layout element with the tree structure, determines view information by analyzing the element, determines content information by analyzing a text associated with view information, and detects the feature based on a combination of the view information and the content information.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The device of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the layout element based feature detection module determines the view information according to a location, a size or index information of the selected element.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The device of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the layout element based feature detection module determines view information according to a class name of a root view for the selected element.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The device of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the layout element based feature detection module determines the view information according to whether there are content description and a text for the selected element.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The device of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the layout element based feature detection module determines the view information according to viewIdResourceName information for the selected element.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The device of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the event detection module detects at least one of a scroll event, a click event, a focus event, a window transition event, and a window state transition event.</claim-text></claim></claims></us-patent-application>