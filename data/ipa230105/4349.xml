<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004350A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004350</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17367151</doc-number><date>20210702</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>7</main-group><subgroup>523</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>06</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>7</main-group><subgroup>50</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>7</main-group><subgroup>523</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>0626</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>0659</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>0673</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>7</main-group><subgroup>50</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">COMPUTE IN MEMORY ARCHITECTURE AND DATAFLOWS FOR DEPTH-WISE SEPARABLE CONVOLUTION</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>QUALCOMM Incorporated</orgname><address><city>San Diego</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>LI</last-name><first-name>Ren</first-name><address><city>San Diego</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Certain aspects of the present disclosure provide a method, including: storing a depthwise convolution kernel in a first one or more columns of a CIM array; storing a fused convolution kernel in a second one or more columns of the CIM array; storing pre-activations in one or more input data buffers associated with a plurality of rows of the CIM array; processing the pre-activations with the depthwise convolution kernel in order to generate depthwise output; modifying one or more of the pre-activations based on the depthwise output to generate modified pre-activations; and processing the modified pre-activations with the fused convolution kernel to generate fused output.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="74.68mm" wi="152.74mm" file="US20230004350A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="220.30mm" wi="140.29mm" file="US20230004350A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="146.98mm" wi="160.36mm" file="US20230004350A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="112.69mm" wi="154.77mm" file="US20230004350A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="205.32mm" wi="165.02mm" file="US20230004350A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="259.93mm" wi="169.33mm" file="US20230004350A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="253.07mm" wi="136.57mm" file="US20230004350A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="181.95mm" wi="146.47mm" file="US20230004350A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="228.18mm" wi="128.95mm" orientation="landscape" file="US20230004350A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="162.56mm" wi="147.91mm" orientation="landscape" file="US20230004350A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="251.12mm" wi="169.16mm" orientation="landscape" file="US20230004350A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="260.69mm" wi="160.27mm" file="US20230004350A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="243.92mm" wi="137.24mm" file="US20230004350A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="259.25mm" wi="155.28mm" file="US20230004350A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">INTRODUCTION</heading><p id="p-0002" num="0001">Aspects of the present disclosure relate to performing machine learning tasks and in particular to compute in memory architectures and data flows for performing depthwise separable convolutional in memory without the need for additional multiply-and-accumulate circuits.</p><p id="p-0003" num="0002">Machine learning is generally the process of producing a trained model (e.g., an artificial neural network, a tree, or other structures), which represents a generalize fit to a set of training data that is known a priori. Applying the trained model to new data produces inferences, which may be used to gain insights into the new data. In some cases, applying the model to the new data is described as &#x201c;running an inference&#x201d; on the new data.</p><p id="p-0004" num="0003">As the use of machine learning has proliferated for enabling various machine learning (or artificial intelligence) tasks, the need for more efficient processing of machine learning model data has arisen. In some cases, dedicated hardware, such as machine learning accelerators, may be used to enhance a processing system's capacity to process machine learning model data. However, such hardware requires space and power, which is not always available on the processing device. For example, &#x201c;edge processing&#x201d; devices, such as mobile devices, always on devices, internet of things (IoT) devices, and the like, have to balance processing capabilities with power and packaging constraints. Further, accelerators may need to move data across common data busses, which can cause significant power usage and introduce latency into other processes sharing the data bus. Consequently, other aspects of a processing system are being considered for processing machine learning model data.</p><p id="p-0005" num="0004">Memory devices are one example of another aspect of a processing system that may be leveraged for performing processing of machine learning model data through so-called compute in memory (CIM) processes. Unfortunately, CIM processes may not be able to perform processing of complex model architectures, such as depthwise separable convolutional neural networks, without additional hardware elements, such as digital multiply-and-accumulate circuits (DMACs) and related peripherals. These additional hardware elements require additional space, power, and complexity in their implementation, which tend to reduce the advantages of leveraging the memory device as an additional compute resource. Even where ancillary aspects of a processing system have DMACs available to perform processing that cannot be directly performed using CIM processes, moving the data to and from those ancillary aspects requires time and power and therefore mitigate the benefits of the CIM process.</p><p id="p-0006" num="0005">Accordingly, systems and methods are need for performing computation in memory of a wider variety of machine learning model architectures, such as depthwise separable convolutional neural networks.</p><heading id="h-0002" level="1">BRIEF SUMMARY</heading><p id="p-0007" num="0006">Certain aspects provide an apparatus, comprising: a compute-in-memory (CIM) array configured to: store a depthwise convolution kernel in a first one or more columns of the CIM array; store a fused convolution kernel in a second one or more columns of the CIM array; store pre-activations in a plurality of rows of the CIM array; process the pre-activations with the depthwise convolution kernel in order to generate depthwise output; generate modified pre-activations based on the depthwise output; and process the modified pre-activations with the fused convolution kernel to generate fused output.</p><p id="p-0008" num="0007">Further aspects provide a method, comprising: storing a depthwise convolution kernel in a first one or more columns of a CIM array; storing a fused convolution kernel in a second one or more columns of the CIM array; storing pre-activations in one or more input data buffers associated with a plurality of rows of the CIM array; processing the pre-activations with the depthwise convolution kernel in order to generate depthwise output; modifying one or more of the pre-activations based on the depthwise output to generate modified pre-activations; and processing the modified pre-activations with the fused convolution kernel to generate fused output.</p><p id="p-0009" num="0008">Other aspects provide processing systems configured to perform the aforementioned methods as well as those described herein; non-transitory, computer-readable media comprising instructions that, when executed by one or more processors of a processing system, cause the processing system to perform the aforementioned methods as well as those described herein; a computer program product embodied on a computer readable storage medium comprising code for performing the aforementioned methods as well as those further described herein; and a processing system comprising means for performing the aforementioned methods as well as those further described herein.</p><p id="p-0010" num="0009">The following description and the related drawings set forth in detail certain illustrative features of one or more aspects.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0011" num="0010">The appended figures depict certain aspects of the one or more aspects and are therefore not to be considered limiting of the scope of this disclosure.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIGS. <b>1</b>A-<b>1</b>D</figref> depict examples of various types of neural networks.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>2</b></figref> depicts an example of a conventional convolution operation.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIGS. <b>3</b>A and <b>3</b>B</figref> depicts examples of depthwise separable convolution operations.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates an example compute-in-memory (CIM) array configured for performing machine learning model computations.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIGS. <b>5</b>A and <b>5</b>B</figref> depict example bitcells, which may be representative of the bitcells in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, and which may generally be used for CIM operations as described herein.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>6</b></figref> depicts an example timing diagram of various signals during a compute-in-memory (CIM) array operation.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>7</b></figref> depicts an exemplary convolutional layer architecture implemented by a compute-in-memory (CIM) array.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>8</b></figref> depicts an example process for applying fused weights to a depthwise separable convolution operation</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>9</b></figref> depicts an example CIM architecture configured to use fused weights to perform depthwise separable convolution without a DMAC.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>10</b></figref> depicts an example processing flow for performing depthwise separable convolution in a CIM without a DMAC.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>11</b></figref> depicts an example method for performing depthwise separable convolution in a CIM.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>12</b></figref> depicts an example processing system for performing convolutional neural network processing.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0024" num="0023">To facilitate understanding, identical reference numerals have been used, where possible, to designate identical elements that are common to the drawings. It is contemplated that elements and features of one aspect may be beneficially incorporated in other aspects without further recitation.</p><heading id="h-0004" level="1">DETAILED DESCRIPTION</heading><p id="p-0025" num="0024">Aspects of the present disclosure provide apparatuses, methods, processing systems, and computer-readable mediums for performing computation-in-memory (CIM) of machine learning models, including depthwise separable convolutional neural network models.</p><p id="p-0026" num="0025">CIM-based machine learning (ML) and artificial intelligence (AI) task accelerators may be used for a wide variety of tasks, including image and audio processing, user verification, translation, and others. Generally, CIM may be based on various types of memory architecture, including dynamic random access memory (DRAM), static random access memory (SRAM) (e.g., based on an SRAM cell as in <figref idref="DRAWINGS">FIG. <b>5</b>A</figref>), magnetic random access memory (MRAM), and resistive random access memory (ReRAM), to name a few. Beneficially, CIM may complement other existing processing devices in an electronic device, such as central processing units (CPUs), digital signal processors (DSPs), graphics processing units (GPUs), field programmable gate arrays (FPGAs), and others. Further, CIM may beneficially reduce the &#x201c;memory wall&#x201d; problem, which is where the movement of data in and out of memory consumes more power than the computation of the data. Thus, by performing the computation in memory, significant power savings may be realized. This is particularly useful for various types of electronic devices, such as lower power edge processing devices, mobile devices, and the like.</p><p id="p-0027" num="0026">For example, a mobile device may include one or more CIM arrays configured to perform an ML or AI task, such as face or voice recognition, based on data generated by the mobile device, such as image data generated by a camera sensor of the mobile device or audio data generated by a microphone of the mobile device. A memory controller unit (MCU) of the mobile device may be configured to load machine learning model parameters (e.g., weights and biases) from another on-board memory (e.g., flash or RAM) into the one or more CIM arrays of the memory device. The processing device may then commence processing of the image data (in this example) processing data for one or more layers of the machine learning model to generate a model output (e.g., an inference).</p><p id="p-0028" num="0027">CIM arrays are not inherently capable of processing all types of machine learning models without supporting hardware. For example, conventionally, CIM arrays could not process depthwise separable convolution architectures without supporting external hardware, such as a digital multiplication and accumulation (DMAC) block. Having to process model data in external hardware reintroduces the memory wall problem and thus mitigates the advantages of CIM array processing.</p><p id="p-0029" num="0028">To overcome the shortcomings of conventional CIM array processing of depthwise separable convolutional neural network models, aspects described herein utilize fused weight kernels, which eliminate the need to move data outside of the CIM array for processing by a DMAC. Further, aspects described herein may emulate a nonlinear operation with CIM array processing in order to reduce the amount of input pre-activation data that needs to be processed by the CIM using the fused kernel. Accordingly, the methods described herein improve upon processing efficiency and latency as compared to conventional methods.</p><heading id="h-0005" level="1">Brief Background on Neural Networks, Deep Neural Networks, and Deep Learning</heading><p id="p-0030" num="0029">Neural networks are organized into layers of interconnected nodes. Generally, a node (or neuron) is where computation happens. For example, a node may combine input data with a set of weights (or coefficients) that either amplifies or dampens the input data. The amplification or dampening of the input signals may thus be considered an assignment of relative significances to various inputs with regard to a task the network is trying to learn. Generally, input-weight products are summed (or accumulated) and then the sum is passed through a node's activation function to determine whether and to what extent that signal should progress further through the network.</p><p id="p-0031" num="0030">In a most basic implementation, a neural network may have an input layer, a hidden layer, and an output layer. &#x201c;Deep&#x201d; neural networks generally have more than one hidden layer.</p><p id="p-0032" num="0031">Deep learning is a method of training deep neural networks. Generally, deep learning maps inputs to the network to outputs from the network and is thus sometimes referred to as a &#x201c;universal approximator&#x201d; because it can learn to approximate an unknown function &#x192;(x)=y between any input x and any output y. In other words, deep learning finds the right &#x192;to transform x into y.</p><p id="p-0033" num="0032">More particularly, deep learning trains each layer of nodes based on a distinct set of features, which is the output from the previous layer. Thus, with each successive layer of a deep neural network, features become more complex. Deep learning is thus powerful because it can progressively extract higher level features from input data and perform complex tasks, such as object recognition, by learning to represent inputs at successively higher levels of abstraction in each layer, thereby building up a useful feature representation of the input data.</p><p id="p-0034" num="0033">For example, if presented with visual data, a first layer of a deep neural network may learn to recognize relatively simple features, such as edges, in the input data. In another example, if presented with auditory data, the first layer of a deep neural network may learn to recognize spectral power in specific frequencies in the input data. The second layer of the deep neural network may then learn to recognize combinations of features, such as simple shapes for visual data or combinations of sounds for auditory data, based on the output of the first layer. Higher layers may then learn to recognize complex shapes in visual data or words in auditory data. Still higher layers may learn to recognize common visual objects or spoken phrases. Thus, deep learning architectures may perform especially well when applied to problems that have a natural hierarchical structure.</p><heading id="h-0006" level="1">Layer Connectivity in Neural Networks</heading><p id="p-0035" num="0034">Neural networks, such as deep neural networks, may be designed with a variety of connectivity patterns between layers.</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>1</b>A</figref> illustrates an example of a fully connected neural network <b>102</b>. In a fully connected neural network <b>102</b>, a node in a first layer communicate its output to every node in a second layer, so that each node in the second layer will receive input from every node in the first layer.</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>1</b>B</figref> illustrates an example of a locally connected neural network <b>104</b>. In a locally connected neural network <b>104</b>, a node in a first layer may be connected to a limited number of nodes in the second layer. More generally, a locally connected layer of the locally connected neural network <b>104</b> may be configured so that each node in a layer will have the same or a similar connectivity pattern, but with connections strengths (or weights) that may have different values (e.g., <b>110</b>, <b>112</b>, <b>114</b>, and <b>116</b>). The locally connected connectivity pattern may give rise to spatially distinct receptive fields in a higher layer, because the higher layer nodes in a given region may receive inputs that are tuned through training to the properties of a restricted portion of the total input to the network.</p><p id="p-0038" num="0037">One type of locally connected neural network is a convolutional neural network. <figref idref="DRAWINGS">FIG. <b>1</b>C</figref> illustrates an example of a convolutional neural network <b>106</b>. Convolutional neural network <b>106</b> may be configured such that the connection strengths associated with the inputs for each node in the second layer are shared (e.g., <b>108</b>). Convolutional neural networks are well-suited to problems in which the spatial location of inputs is meaningful.</p><p id="p-0039" num="0038">One type of convolutional neural network is a deep convolutional network (DCN). Deep convolutional networks are networks of multiple convolutional layers, which may further be configured with, for example, pooling and normalization layers.</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>1</b>D</figref> illustrates an example of a DCN <b>100</b> designed to recognize visual features in an image <b>126</b> generated by an image capturing device <b>130</b>. For example, if the image capturing device <b>130</b> was a camera mounted in a vehicle, then DCN <b>100</b> may be trained with various supervised learning techniques to identify a traffic sign and even a number on the traffic sign. DCN <b>100</b> may likewise be trained for other tasks, such as identifying lane markings or identifying traffic lights. These are just some example tasks, and many others are possible.</p><p id="p-0041" num="0040">In this example, DCN <b>100</b> includes a feature extraction section and a classification section. Upon receiving the image <b>126</b>, a convolutional layer <b>132</b> applies convolutional kernels (for example, as depicted and described in <figref idref="DRAWINGS">FIG. <b>2</b></figref>) to the image <b>126</b> to generate a first set of feature maps (or intermediate activations) <b>118</b>. Generally, &#x201c;kernels&#x201d; and &#x201c;filters&#x201d; comprise multi-dimensional arrays (e.g., matrices or tensors) of weights designed to emphasize different aspects of an input data channel. Three-dimensional kernels are frequently used in deep learning.</p><p id="p-0042" num="0041">The first set of feature maps <b>118</b> may then be subsampled by a pooling layer (e.g., a max pooling layer, not shown) to generate a second set of feature maps <b>120</b>. The pooling layer may reduce the size of the first set of feature maps <b>118</b> while maintain much of the information in order to improve model performance. For example, the second set of feature maps <b>120</b> may be down-sampled to 14&#xd7;14 from 28&#xd7;28 by the pooling layer.</p><p id="p-0043" num="0042">This process may be repeated through many layers. In other words, the second set of feature maps <b>120</b> may be further convolved via one or more subsequent convolutional layers (not shown) to generate one or more subsequent sets of feature maps (not shown).</p><p id="p-0044" num="0043">In the example of <figref idref="DRAWINGS">FIG. <b>1</b>D</figref>, the second set of feature maps <b>120</b> is provided to a fully-connected layer <b>124</b>, which in turn generates an output feature vector <b>128</b>. Each feature of the output feature vector <b>128</b> may include a number that corresponds to a possible feature of the image <b>126</b>, such as &#x201c;sign,&#x201d; &#x201c;60,&#x201d; and &#x201c;100.&#x201d; In some cases, a softmax function (not shown) may convert the numbers in the output feature vector <b>128</b> to a probability. In such cases, an output <b>122</b> of the DCN <b>100</b> is a probability of the image <b>126</b> including one or more features.</p><p id="p-0045" num="0044">A softmax function (not shown) may convert the numbers in the fourth feature map <b>128</b> into a probability in order that an output <b>122</b> of DCN <b>100</b> is one or more probabilities of the image <b>126</b> including one or more features, such as a sign with the numbers &#x201c;60&#x201d; on it, as in input image <b>126</b>. Thus, in the present example, the probabilities in the output <b>122</b> for &#x201c;sign&#x201d; and &#x201c;60&#x201d; should be higher than the probabilities of the others of the output <b>122</b>, such as &#x201c;30,&#x201d; &#x201c;40,&#x201d; &#x201c;50,&#x201d; &#x201c;70,&#x201d; &#x201c;80,&#x201d; &#x201c;90,&#x201d; and &#x201c;100&#x201d;.</p><p id="p-0046" num="0045">Before training DCN <b>100</b>, the output <b>122</b> produced by DCN <b>100</b> may be incorrect. Thus, an error may be calculated between the output <b>122</b> and a target output known a priori. For example, here the target output is an indication that the image <b>126</b> includes a &#x201c;sign&#x201d; and the number &#x201c;60&#x201d;. Utilizing the known, target output, the weights of DCN <b>100</b> may then be adjusted through training so that subsequent output <b>122</b> of DCN <b>100</b> achieves the target output.</p><p id="p-0047" num="0046">To adjust the weights of DCN <b>100</b>, a learning algorithm may compute a gradient vector for the weights. The gradient may indicate an amount that an error would increase or decrease if a weight were adjusted in a particular way. The weights may then be adjusted to reduce the error. This manner of adjusting the weights may be referred to as &#x201c;back propagation&#x201d; as it involves a &#x201c;backward pass&#x201d; through the layers of DCN <b>100</b>.</p><p id="p-0048" num="0047">In practice, the error gradient of weights may be calculated over a small number of examples, so that the calculated gradient approximates the true error gradient. This approximation method may be referred to as stochastic gradient descent. Stochastic gradient descent may be repeated until the achievable error rate of the entire system has stopped decreasing or until the error rate has reached a target level.</p><p id="p-0049" num="0048">After training, DCN <b>100</b> may be presented with new images and DCN <b>100</b> may generate inferences, such as classifications, or probabilities of various features being in the new image.</p><heading id="h-0007" level="1">Convolution Techniques for Convolutional Neural Networks</heading><p id="p-0050" num="0049">Convolution is generally used to extract useful features from an input data set. For example, in convolutional neural networks, such as described above, convolution enables the extraction of different features using kernels and/or filters whose weights are automatically learned during training. The extracted features are then combined to make inferences.</p><p id="p-0051" num="0050">An activation function may be applied before and/or after each layer of a convolutional neural network. Activation functions are generally mathematical functions (e.g., equations) that determine the output of a node of a neural network. Thus, the activation function determines whether it a node should pass information or not, based on whether the node's input is relevant to the model's prediction. In one example, where y=Conv(x) (i.e., y=a convolution of x), both x and y may be generally considered as &#x201c;activations&#x201d;. However, in terms of a particular convolution operation, x may also be referred to as &#x201c;pre-activations&#x201d; or &#x201c;input activations&#x201d; as it exists before the particular convolution and y may be referred to as output activations or a feature map.</p><p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. <b>2</b></figref> depicts an example of a traditional convolution in which a 12 pixel&#xd7;12 pixel&#xd7;3 channel input image is convolved using a 5&#xd7;5&#xd7;3 convolution kernel <b>204</b> using a stride of 1. The resulting output feature map <b>206</b> is 8 pixels&#xd7;8 pixels&#xd7;1 channel. As seen in this example, the traditional convolution may change the dimensionality of the input data as compared to the output data (here, from 12&#xd7;12 to 8&#xd7;8 pixels), including the channel dimensionality (here, from 3 to 1 channel).</p><p id="p-0053" num="0052">One way to reduce the computational burden (e.g., measured in floating point operations per second (FLOPs)) and the number parameters associated with a neural network comprising convolutional layers is to factorize the convolutional layers. For example, a spatial separable convolution, such as depicted in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, may be factorized into two components: (1) a depthwise convolution, wherein each spatial channel is convolved independently by a depthwise convolution (e.g., a spatial fusion); and (2) a pointwise convolution, wherein all the spatial channels are linearly combined (e.g., a channel fusion). An examples of a depthwise separable convolution is depicted in <figref idref="DRAWINGS">FIGS. <b>3</b>A and <b>3</b>B</figref>. Generally, during spatial fusion, a network learns features from the spatial planes and during channel fusion the network learns relations between these features across channels.</p><p id="p-0054" num="0053">In one example, a separable depthwise convolutions may be implemented using 3&#xd7;3 kernels for spatial fusion, and 1&#xd7;1 kernels for channel fusion. In particular, the channel fusion may use a 1&#xd7;1&#xd7;d kernel that iterates through every single point in an input image of depth d, wherein the depth d of the kernel generally matches the number of channels of the input image. Channel fusion via pointwise convolution is useful for dimensionality reduction for efficient computations. Applying 1&#xd7;1&#xd7;d kernels and adding an activation layer after the kernel may give a network added depth, which may increase its performance.</p><p id="p-0055" num="0054"><figref idref="DRAWINGS">FIGS. <b>3</b>A and <b>3</b>B</figref> depicts an example of a depthwise separable convolution operation.</p><p id="p-0056" num="0055">In particular, in <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>, the 12 pixel&#xd7;12 pixel&#xd7;3 channel input image <b>302</b> is convolved with a filter comprising three separate kernels <b>304</b>A-C, each having a 5&#xd7;5&#xd7;1 dimensionality, to generate an output feature map <b>306</b> of 8 pixels&#xd7;8 pixels&#xd7;3 channels, where each channel is generated by an individual kernel amongst <b>304</b>A-C.</p><p id="p-0057" num="0056">Then output feature map <b>306</b> is further convolved using a pointwise convolution operation in which a filter <b>308</b> having dimensionality 1&#xd7;1&#xd7;3 to generate an output feature map <b>310</b> of 8 pixels&#xd7;8 pixels&#xd7;1 channel. As is depicted in this example, output feature map <b>310</b> has reduced dimensionality (1 channel versus 3), which allows for more efficient computations with output feature map <b>310</b>.</p><p id="p-0058" num="0057">Though the result of the depthwise separable convolution in <figref idref="DRAWINGS">FIGS. <b>3</b>A and <b>3</b>B</figref> is the same as (or substantially similar to) conventional convolution in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the number of computations is significantly reduced, and thus depthwise separable convolution offers a significant efficiency gain where a network design allows it.</p><p id="p-0059" num="0058">Though not depicted in <figref idref="DRAWINGS">FIG. <b>3</b>B</figref>, multiple (e.g., m) pointwise convolution kernels <b>308</b> (e.g., individual components of a filter) can be used to increase the channel dimensionality of the convolution output. So, for example, m=256 1&#xd7;1&#x3e;3 kernels <b>308</b> can be generated, which each output an 8 pixels&#xd7;8 pixels&#xd7;1 channel output image (e.g., <b>310</b>), and these output images can be stacked to get a resulting output image of 8 pixels&#xd7;8 pixels&#xd7;256 channels. The resulting increase in channel dimensionality provides more parameters for training, which may improve a convolutional neural network's ability to identify features (e.g., in input image <b>302</b>).</p><heading id="h-0008" level="1">Example Compute in Memory (CIM) Architecture</heading><p id="p-0060" num="0059"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates an exemplary compute-in-memory (CIM) array <b>400</b> configured for performing machine learning model computations, according to aspects of the present disclosure. In this example, CIM array <b>400</b> is configured to simulate MAC operations using mixed analog/digital operations for an artificial neural network. Accordingly, as used herein, the terms multiplication and addition may refer to such simulated operations. CIM array <b>400</b> can be used to implement aspects of the processing techniques described herein.</p><p id="p-0061" num="0060">In the depicted aspect, CIM array <b>400</b> includes pre-charge word lines (PCWLs) <b>425</b><i>a</i>, <b>425</b><i>b </i>and <b>425</b><i>c </i>(collectively <b>425</b>), read word lines (RWLs) <b>427</b><i>a</i>, <b>427</b><i>b</i>, and <b>427</b><i>c </i>(collectively <b>427</b>), analog-to-digital converters (ADCs) <b>410</b><i>a</i>, <b>410</b><i>b </i>and <b>410</b><i>c </i>(collectively <b>410</b>), a digital processing unit <b>413</b>, bitlines <b>418</b><i>a</i>, <b>418</b><i>b</i>, and <b>418</b><i>c </i>(collectively <b>418</b>), PMOS transitors <b>411</b><i>a</i>-<b>411</b><i>i </i>(collectively <b>411</b>), NMOS transitors <b>413</b><i>a</i>-<b>413</b><i>i </i>(collectively <b>413</b>), and capacitors <b>423</b><i>a</i>-<b>423</b><i>i </i>(collectively <b>423</b>).</p><p id="p-0062" num="0061">Weights associated with a neural network layer may be stored in SRAM cells of CIM array <b>400</b>. In this example, binary weights are shown in the SRAM bitcells <b>405</b><i>a</i>-<b>405</b><i>i </i>of CIM array <b>400</b>. Input activations (e.g., input values that may be an input vector) are provided on the PCWLs <b>425</b><i>a</i>-<i>c. </i></p><p id="p-0063" num="0062">Multiplication occurs in each bitcell <b>405</b><i>a</i>-<b>405</b><i>i </i>of CIM array <b>400</b> associated with a bitline and the accumulation (summation) of all the bitcell multiplication results occurs on the same bitline for one column. The multiplication in each bitcell <b>405</b><i>a</i>-<b>405</b><i>i </i>is in the form of an operation equivalent to an AND operation of the corresponding activation and weight, where the result is stored as a charge on the corresponding capacitor <b>423</b>. For example, a product of 1, and consequently a charge on the capacitor <b>423</b>, is produced only where the activation is one (here, because a PMOS is used, the PCWL is zero for an activation of one) and the weight is one.</p><p id="p-0064" num="0063">For example, in an accumulating stage, RWLs <b>427</b> are switched to high so that any charges on capacitors <b>423</b> (which is based on corresponding bitcell (weight) and PCWL (activation) values) can be accumulated on corresponding bitlines <b>418</b>. The voltage values of the accumulated charges are then converted by ADCs <b>410</b> to digital values (where, for example, the output values may be a binary value indicating whether the total charge is greater than a reference voltage). These digital values (outputs) may be provided as input to another aspect of a machine learning model, such as a following layer.</p><p id="p-0065" num="0064">When activations on pre-charge word lines (PCWLs) <b>425</b><i>a</i>, <b>425</b><i>b </i>and <b>425</b><i>c </i>are, for example, 1, 0, 1, then the sums of bitlines <b>418</b><i>a</i>-<i>c </i>correspond to 0+0+1=1, 1+0+0=1, and 1+0+1=2, respectively. The output of the ADCs <b>410</b><i>a</i>, <b>410</b><i>b </i>and <b>410</b><i>c </i>are passed on to the digital processing unit <b>413</b> for further processing. For example, if CIM <b>400</b> is processing multi-bit weight values, the digital outputs of ADCs <b>410</b> may be summed to generate a final output</p><p id="p-0066" num="0065">The exemplary 3&#xd7;3 CIM circuit <b>400</b> may be used, for example, for performing efficient 3-channel convolution for three-element kernels (or filters), where the weights of each kernel correspond to the elements of each of the three columns, so that for a given three-element receptive field (or input data patch), the outputs for each of the three channels are calculated in parallel.</p><p id="p-0067" num="0066">Notably, while <figref idref="DRAWINGS">FIG. <b>4</b></figref> describes an example of CIM using SRAM cells, other memory types can be used. For example, dynamic random access memory (DRAM), magnetoresistive random-access memory (MRAM), and resistive random-access memory (ReRAM or RRAM) can likewise be used in other embodiments.</p><p id="p-0068" num="0067"><figref idref="DRAWINGS">FIG. <b>5</b>A</figref> depicts additional details of an exemplary bitcell <b>500</b>.</p><p id="p-0069" num="0068">Aspects of <figref idref="DRAWINGS">FIG. <b>5</b>A</figref> may be exemplary of or otherwise relate to aspect of <figref idref="DRAWINGS">FIG. <b>4</b></figref>. In paritucular, bitline <b>521</b> is similar to the bitline <b>418</b><i>a</i>, capacitor <b>523</b> is similar to the capacitor <b>423</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>, read word line <b>527</b> is similar to the read word line <b>427</b><i>a </i>of <figref idref="DRAWINGS">FIG. <b>4</b></figref>, pre-charge word line <b>525</b> is similar to the pre-charge word line <b>425</b><i>a </i>of <figref idref="DRAWINGS">FIG. <b>4</b></figref>, PMOS transitor <b>511</b> is similar to PMOS transitor <b>411</b><i>a </i>of <figref idref="DRAWINGS">FIG. <b>4</b></figref>, and NMOS transitor <b>513</b> is similar to NMOS transitor <b>413</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0070" num="0069">The bitcell <b>500</b> includes a static random access memory (SRAM) cell <b>501</b> which may be representative of SRAM bitcells <b>405</b>a of <figref idref="DRAWINGS">FIG. <b>4</b></figref>, as well as transistors <b>511</b> (e.g., a PMOS transistor), transitor <b>513</b> (e.g., an NMOS transistor), and capacitor <b>523</b> coupled to ground. Although a PMOS transistor is used for the transistor <b>511</b>, other transistors (e.g., an NMOS transistor) can be used in place of the PMOS transistor, with corresponding adjustment (e.g., inversion) of their respective control signals. The same applies to the other transistores described herein. The additional transistors <b>511</b> and <b>513</b> are included to implement the compute-in-memory array, according to aspects of the present disclosure. In one aspect, the SRAM cell <b>501</b> is a conventional six transistor (6T) SRAM cell.</p><p id="p-0071" num="0070">Programming of weights in the bitcell may be performed once for a multitude of activations. For example, in operation, the SRAM cell <b>501</b> receives only one bit of information at nodes <b>517</b> and <b>519</b> via a write word line (WWL) <b>516</b>. For example, during write (when WWL <b>516</b> is high), if write bit line (WBL) <b>529</b> is high (e.g., &#x201c;1&#x201d;), then node <b>517</b> sets to high and node <b>519</b> sets to low (e.g., &#x201c;0&#x201d;); or if WBL <b>529</b> is low, then node <b>517</b> sets to low and node <b>519</b> sets to high. Conversely, during write (when WWL <b>516</b> is high), if write bit bar line (WBBL) <b>531</b> is high, then node <b>517</b> sets to low and node <b>519</b> sets to high; or if WBBL <b>529</b> is low, then node <b>517</b> sets to high and node <b>519</b> sets to low.</p><p id="p-0072" num="0071">The programming of weights may be followed by an an activation input and multiplication step to charge the capacitors in accordance with the corresponding products. For example, the transistor <b>511</b> is activated by an activation signal through a pre-charge word line (PCWL) <b>525</b> of the compute-in-memory array to perform the multiplication step. Then, the transistor <b>513</b> is activated by a through another word line (e.g., a read word line (RWL) <b>527</b>) of the compute-in-memory array to perfom the accumulation of the multiplication value from bitcell <b>500</b> with other bitcells of an array, such as described above with respect to <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0073" num="0072">If node <b>517</b> is a &#x201c;0,&#x201d; (e.g., when the the stored weight value is &#x201c;0&#x201d;) the capacitor <b>523</b> will not be charged if a low PCWL indicates an activation of &#x201c;1&#x201d; at the gate of the transistor <b>511</b>. Accordingly, no charge is provided to a bitline <b>521</b>. However, if node <b>517</b>, which corresponds to the weight value, is a &#x201c;1&#x201d;, and PCWL is set to low (e.g., when the activation input is high), which turns on PMOS transistor <b>511</b>, which acts as a short, allowing capacitor <b>523</b> to be charged. After the capacitor <b>523</b> is charged, the transistor <b>511</b> is turned off so the charge is stored in the capacitor <b>523</b>. To move the charge from the capacitor <b>523</b> to the bitline <b>521</b>, the NMOS transistor <b>513</b> is turned on by RWL <b>527</b> causing the the NMOS transistor <b>513</b> to act as a short.</p><p id="p-0074" num="0073">Table 1 illustrates an example of compute-in-memory array operations according to an AND operational setting, such as may be implemented by bitcell <b>500</b> in <figref idref="DRAWINGS">FIG. <b>5</b>A</figref>.</p><p id="p-0075" num="0000"><tables id="TABLE-US-00001" num="00001"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="center"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE 1</entry></row></thead><tbody valign="top"><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>AND Operation</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="4"><colspec colname="1" colwidth="42pt" align="center"/><colspec colname="2" colwidth="35pt" align="center"/><colspec colname="3" colwidth="77pt" align="center"/><colspec colname="4" colwidth="63pt" align="center"/><tbody valign="top"><row><entry>Activation</entry><entry>PCWL</entry><entry>Cell Node (Weight)</entry><entry>Capacitor Node</entry></row><row><entry namest="1" nameend="4" align="center" rowsep="1"/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="4"><colspec colname="1" colwidth="42pt" align="char" char="."/><colspec colname="2" colwidth="35pt" align="char" char="."/><colspec colname="3" colwidth="77pt" align="char" char="."/><colspec colname="4" colwidth="63pt" align="char" char="."/><tbody valign="top"><row><entry>1</entry><entry>0</entry><entry>1</entry><entry>1</entry></row><row><entry>1</entry><entry>0</entry><entry>0</entry><entry>0</entry></row><row><entry>0</entry><entry>1</entry><entry>1</entry><entry>0</entry></row><row><entry>0</entry><entry>1</entry><entry>0</entry><entry>0</entry></row><row><entry namest="1" nameend="4" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0076" num="0074">A first column (Activation) of Table 1 includes possible values of an incoming activation signal.</p><p id="p-0077" num="0075">A second column (PCWL) of Table 1 includes PCWL values that activate transistors designed to implement compute-in-memory functions according to aspects of the present disclosure. Because the transistor <b>511</b> in this example is a PMOS transistor, the PCWL values are inverses of the activation values. For example, the compute-in-memory array includes the transistor <b>511</b> that is activated by an activation signal (PCWL signal) through the pre-charge word line (PCWL) <b>525</b>.</p><p id="p-0078" num="0076">A third column (Cell Node) of Table 1 includes weight values stored in the SRAM cell node, for example, corresponding to weights in a weight tensor, such as a may be used in convolution operations.</p><p id="p-0079" num="0077">A fourth column (Capacitor Node) of Table 1 shows the resultant products that will be stored as charge on a capacitor. For example, the charge may be stored at a node of the capacitor <b>523</b> or a node of one of the capacitors <b>423</b><i>a</i>-<b>423</b><i>i</i>. The charge from the capacitor <b>523</b> is moved to the bitline <b>521</b> when the transistor <b>513</b> is activated. For example, referring to the transistor <b>511</b>, when the weight at the cell node <b>517</b> is a &#x201c;1&#x201d; (e.g., high voltage) and the input activation is a &#x201c;1&#x201d; (so PCWL is &#x201c;0&#x201d;), the capacitor <b>523</b> is charged (e.g., the node of the capacitor is a &#x201c;1&#x201d;). For all other combinations, the capacitor node will have a value of 0.</p><p id="p-0080" num="0078"><figref idref="DRAWINGS">FIG. <b>5</b>B</figref> depicts additional details of another exemplary bitcell <b>550</b>, which may likewise be representative of any of bitccells <b>405</b><i>a</i>-<i>i </i>in <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0081" num="0079">Bitcell <b>550</b> differs from bitcell <b>500</b> in <figref idref="DRAWINGS">FIG. <b>5</b>A</figref> primarily based on the inclusion of an additional pre-charge word line <b>552</b> coupled to an additional transistor <b>554</b>. Pre-charge word line <b>552</b> allows for bitcell <b>550</b> to be placed in an AND operating mode or an XNOR operating mode based on its state. For example, when pre-charge word line <b>552</b> is tied high, bitcell <b>550</b> operates in an AND mode, and otherwise it acts in an XNOR mode.</p><p id="p-0082" num="0080">Table 2 illustrates an example of compute-in-memory array operations similar to Table 1, except according to an XNOR operational setting, such as may be implemented by bitcell <b>550</b> in <figref idref="DRAWINGS">FIG. <b>5</b>B</figref>.</p><p id="p-0083" num="0000"><tables id="TABLE-US-00002" num="00002"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="center"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE 2</entry></row></thead><tbody valign="top"><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>XNOR Operation</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="5"><colspec colname="1" colwidth="42pt" align="center"/><colspec colname="2" colwidth="35pt" align="center"/><colspec colname="3" colwidth="35pt" align="center"/><colspec colname="4" colwidth="42pt" align="center"/><colspec colname="5" colwidth="63pt" align="center"/><tbody valign="top"><row><entry/><entry/><entry/><entry>Cell Node</entry><entry/></row><row><entry>Activation</entry><entry>PCWL1</entry><entry>PCWL2</entry><entry>(Weight)</entry><entry>Capacitor Node</entry></row><row><entry namest="1" nameend="5" align="center" rowsep="1"/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="5"><colspec colname="1" colwidth="42pt" align="char" char="."/><colspec colname="2" colwidth="35pt" align="char" char="."/><colspec colname="3" colwidth="35pt" align="char" char="."/><colspec colname="4" colwidth="42pt" align="char" char="."/><colspec colname="5" colwidth="63pt" align="char" char="."/><tbody valign="top"><row><entry>1</entry><entry>0</entry><entry>1</entry><entry>1</entry><entry>1</entry></row><row><entry>1</entry><entry>0</entry><entry>1</entry><entry>0</entry><entry>0</entry></row><row><entry>0</entry><entry>1</entry><entry>0</entry><entry>1</entry><entry>0</entry></row><row><entry>0</entry><entry>1</entry><entry>0</entry><entry>0</entry><entry>1</entry></row><row><entry namest="1" nameend="5" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0084" num="0081">A first column (Activation) of Table 2 includes possible values of an incoming activation signal.</p><p id="p-0085" num="0082">A second column (PCWL1) of Table 2 includes PCWL1 values that activate transistors designed to implement compute-in-memory functions according to aspects of the present disclosure. Here again, the transistor <b>511</b> is a PMOS transistor, the PCWL1 values are inverses of the activation values.</p><p id="p-0086" num="0083">A third column (PCWL2) of Table 2 includes PCWL2 values that activate further transistors designed to implement compute-in-memory functions according to aspects of the present disclosure.</p><p id="p-0087" num="0084">A fourth column (Cell Node) of Table 2 includes weight values stored in the SRAM cell node, for example, corresponding to weights in a weight tensor, such as a may be used in convolution operations.</p><p id="p-0088" num="0085">A fifth column (Capacitor Node) of Table 2 shows the resultant products that will be stored as charge on a capacitor, such as capacitor <b>523</b>.</p><p id="p-0089" num="0086"><figref idref="DRAWINGS">FIG. <b>6</b></figref> depicts an example timing diagram <b>600</b> of various signals during a compute-in-memory (CIM) array operation.</p><p id="p-0090" num="0087">In the depicted example, a first row of the timing diagram <b>600</b> shows a pre-charge word line PCWL (e.g., <b>425</b><i>a </i>of <figref idref="DRAWINGS">FIG. <b>4</b> or <b>525</b></figref> of <figref idref="DRAWINGS">FIG. <b>5</b>A</figref>), going low. In this example, a lowPCWL indicates an activation of &#x201c;1.&#x201d; The PMOS transistor turns on when PCWL is low, which allows charging of the capacitor (if the weight is &#x201c;1&#x201d;). A second row shows a read word line RWL (e.g., read word line <b>427</b><i>a </i>of <figref idref="DRAWINGS">FIG. <b>4</b> or <b>527</b></figref> of <figref idref="DRAWINGS">FIG. <b>5</b>A</figref>.) A third row shows a read bitline RBL (e.g. <b>418</b> of <figref idref="DRAWINGS">FIG. <b>4</b> or <b>521</b></figref> of <figref idref="DRAWINGS">FIG. <b>5</b>A</figref>), a fourth row shows an analog-to-digital converter (ADC) readout signal and a fifth row shows a reset signal.</p><p id="p-0091" num="0088">For example, referring to the transistor <b>511</b> of <figref idref="DRAWINGS">FIG. <b>5</b>A</figref>, a charge from the capacitor <b>523</b> is gradually passed on to the read bitline RBL when the read word line RWL is high.</p><p id="p-0092" num="0089">A summed charge/current/voltage (e.g., <b>403</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref> or charges summed from the the bitline <b>521</b> of <figref idref="DRAWINGS">FIG. <b>5</b>A</figref>) is passed on to a comparator or ADC (e.g., the ADC <b>410</b><i>a </i>of <figref idref="DRAWINGS">FIG. <b>4</b></figref>) where the summed charge is converted to a digital output (e.g., digital signal/number). The summing of the charge may occur in an accumulation region of the timing diagram <b>600</b> and a readout from the ADC may be associated with the ADC readout region of the timing diagram <b>600</b>. After the ADC readout is obtained, the reset signal discharges all of the capacitors (e.g., capacitors <b>423</b><i>a</i>-<b>423</b><i>i</i>) in preparation for processing the next set of activation inputs.</p><heading id="h-0009" level="1">Example of Convolution Processing in Memory</heading><p id="p-0093" num="0090"><figref idref="DRAWINGS">FIG. <b>7</b></figref> depicts an exemplary convolutional layer architecture <b>700</b> implemented by a compute-in-memory (CIM) array <b>708</b>. The convolutional layer architecture <b>700</b> may be a part of a convolutional neural network (e.g., as described above with respect to <figref idref="DRAWINGS">FIG. <b>1</b>D</figref>) and designed to process multidemensional data, such as tensor data.</p><p id="p-0094" num="0091">In the depicted example, input <b>702</b> to the convolutional layer architecture <b>700</b> has dimensions of 38 (height)&#xd7;11 (width)&#xd7;1 (depth). The output <b>704</b> of the convolutional layer has dimensions 34&#xd7;10&#xd7;64, which includes 64 output channels corresponding to the 64 kernels of kernel tensor <b>714</b> applied as part of the convolution process. Further in this example, each kernel (e.g., exemplary kernel <b>712</b>) of the 64 kernels of kernel tensor <b>714</b> has dimensions of 5&#xd7;2&#xd7;1 (all together, the kernels of filter tensor <b>714</b> are equivalent to one 5&#xd7;2&#xd7;64 kernel).</p><p id="p-0095" num="0092">During the convolution process, each 5&#xd7;2&#xd7;1 kernel is convolved with the input <b>702</b> to generate one 34&#xd7;10&#xd7;1 layer of output <b>704</b>. During the convolution, the <b>640</b> weights of kernel tensor <b>714</b> (5&#xd7;2&#xd7;64) may be stored in the compute-in-memory (CIM) array <b>708</b>, which in this example includes a column for each kernel (i.e., 64 columns). Then activations of each of the 5&#xd7;2 receptive fields (e.g., receptive field input <b>706</b>) are input to the CIM array <b>708</b> using the word lines, e.g., <b>716</b>, and multiplied by the corresponding weights to produce a 1&#xd7;1&#xd7;64 output tensor (e.g., an output tensor <b>710</b>). Output tensors <b>704</b> represent an accumulation of the 1&#xd7;1&#xd7;64 individual output tensors for all of the receptive fields (e.g., the receptive field input <b>706</b>) of the input <b>702</b>. For simplicity, the compute-in-memory array <b>708</b> of <figref idref="DRAWINGS">FIG. <b>7</b></figref> only shows a few illustrative lines for the input and the output of the compute-in-memory array <b>708</b>.</p><p id="p-0096" num="0093">In the depicted example, CIM array <b>708</b> includes wordlines <b>716</b> through which the CIM array <b>708</b> receives the receptive fields (e.g., receptive field input <b>706</b>), as well as bitlines <b>718</b> (corresponding to the columns of the CIM array <b>708</b>). Though not depicted, CIM array <b>708</b> may also include pre-charge wordlines (PCWL) and read word lines RWL (as described above with respect to <figref idref="DRAWINGS">FIGS. <b>4</b> and <b>5</b></figref>).</p><p id="p-0097" num="0094">In this example, wordlines <b>716</b> are used for initial weight definition. However, once the initial weight definition occurs, the activation input activates a specially designed line in a CIM bitcell to perform a MAC operation. Thus, each intersection of a bitline <b>718</b> and a wordline <b>716</b> represents a kernel weight value, which is multiplied by the input activation on the wordline <b>716</b> to generate a product. The individual products along each bitline <b>718</b> are then summed to generate corresponding output values of the output tensor <b>710</b>. The summed value may be charge, current, or voltage. In this example, the dimensions of the output tensor <b>704</b>, after processing the entire input <b>702</b> of the convolutional layer, are 34&#xd7;10&#xd7;64, though only 64 kernel outputs are generated at a tme by the CIM array <b>708</b>. Thus, the processing of the entire input <b>702</b> may be completed in 34&#xd7;10 or 340 cycles.</p><heading id="h-0010" level="1">CIM Architectures for Depthwise Separable Convolution</heading><p id="p-0098" num="0095">Vector-matrix multiplication blocks implemented in memory for CIM architectures can perform conventional convolutional neural network processing generally well, but they are not efficient for supporting depthwise separable convolutional neural networks, which are found in many state of the art machine learning architectures. For example, existing CIM architectures generally cannot perform depthwise separable convolutional neural networks processing in one phase because each multidimensional kernel needs different input channels. Thus, the kernel weights in the same row cannot share the same activation input for different channels. Consequently, matrix-matrix multiplication (M&#xd7;M) architectures are generally needed to support depthwise separable convolution processing in a one phase cycle.</p><p id="p-0099" num="0096">Conventional solutions for addressing this shortcoming include adding a separate digital multiplication and accumulation (DMAC) block to handle processing for the depthwise portion of a separable convolution while a CIM array can handle the pointwise portion of the separable convolution. However, this hybrid approach results in increased data movement, which can offset the memory efficient advantage of the CIM architecture. Further, the hybrid approach generally requires additional hardware (e.g., the DMAC element, DMA, buffer, and others), which increases space and power needs, and increased processing latency. Moreover, the use of DMACs may impact timing of processing operations and cause model output timing constraints (or other dependencies) to be overrun. In order to resolve that issue, various compromises may be necessary, such as reducing the frame rate of incoming data, increasing the clock rate of processing system elements (including a CIM array), reducing input feature size, and others.</p><p id="p-0100" num="0097">Accordingly, described herein are CIM architectures that support both conventional convolutional neural networks (CNNs) and depthwise separable CNNs (DW-CNNs) with minimal additional hardware. In particular, the CIM architectures described herein fuse the weights for the conventionally separate phases of depthwise and pointwise convolutions so that they may be performed effectively simultaneously using a CIM array without the need for additional hardware, like a DMAC.</p><p id="p-0101" num="0098">The CIM architectures described herein improve the power consumption and timing performance of processing operations for depthwise separable convolutions. These improvements beneficially result in less cycle time for depthwise separable convolution operations and achieve higher total operations per second (TOPS) per watt of processing power, i.e., TOPS/W, compared to conventional architectures that require more hardware (e.g., DMACs) and/or more data movement.</p><p id="p-0102" num="0099">Further, the CIM architectures described herein may also implement input range compression to improve CIM analog to digital conversion accuracy.</p><heading id="h-0011" level="1">DMAC-Free Depthwise Separable Convolution Processing with Fused Weights</heading><p id="p-0103" num="0100">Fusing weights for the two conventional phases of a depthwise separable convolution (i.e., the depthwise phase and the pointwise phase) beneficially allows for processing an equivalent, or near equivalent (for example, bitwise, quantization may cause slight variances in convolution output), depthwise separable convolution without need for a DMAC. Further, fusing the weights allows both phases of a depthwise separable convolution (e.g., the depthwise and pointwise phases) to be processed without the need to generate and store the depthwise output explicitly, thus avoiding any data movement, reducing power use, and reducing processing time and latency.</p><p id="p-0104" num="0101"><figref idref="DRAWINGS">FIG. <b>8</b></figref> depicts an example process <b>800</b> for applying fused weights to a depthwise separable convolution operation (such as described generally with respect to <figref idref="DRAWINGS">FIGS. <b>3</b>A and <b>3</b>B</figref>, above).</p><p id="p-0105" num="0102">Generally, fusing the weights of a depthwise convolution layer and the weights of a pointwise convolution layer may be performed offline (e.g., at step <b>808</b>) by convolving the weights of each layer, where the weights for each layer have the same channel dimensionality. By way of example, with reference to <figref idref="DRAWINGS">FIGS. <b>3</b>A-<b>3</b>B</figref>, the depthwise kernel size is 5&#xd7;5&#xd7;3, the pointwise kernel size is 1&#xd7;1&#xd7;3&#xd7;N, where N is the number of 1 x 1 kernels. The resulting fused weights size is 5&#xd7;5&#xd7;3&#xd7;N.</p><p id="p-0106" num="0103">The fused weights may need to be requantized to a preferred bit width prior to being used in a CIM array to process input data. Thus, fused weights (W<sub>&#x192;used</sub>) may be obtained by the following expression:</p><p id="p-0107" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>W</i><sub>&#x192;used</sub>=requantize(convolve(<i>W</i><sup>depthwise</sup><i>,W</i><sup>pointwise</sup>).<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0108" num="0104">Though not depicted in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, requantizing the fused weights, W<sub>&#x192;used</sub>, may be performed prior to applying the fused weights in step <b>810</b>, such as by a fusion processing block that is part of or otherwise in data communication with a host processing system in which the CIM array is implemented.</p><p id="p-0109" num="0105">Using fused weights may require two operations per patch of input data.</p><p id="p-0110" num="0106">First, a depthwise convolution may be run in a CIM array (e.g., at step <b>804</b>) on input data <b>802</b> to determine a gain for input data to be processed by the CIM array with fused weights in step <b>810</b>. For example, the gain may be used to set zero and negative output values from the convolution at step <b>804</b> to zero prior to being processed with the fused weights at step <b>810</b>. Thus, the gain may act as an equivalent to applying a nonlinear operation, such as a ReLU operation, after the convolution operation at step <b>804</b>, without actually needing to run the output through a nonlinear operation block.</p><p id="p-0111" num="0107">In some scenarios, when the threshold comparison indicates that the gain can be set to zero, an input row in the CIM array may instead be deactivated (e.g., tristated), which saves power during processing. For example, when the output of depthwise operation at <b>804</b> is below the threshold, then the row(s) associated with the input that generated the output may be tristated if a ReLU nonlinear operation follows the depthwise layer in the model architecture. For nonlinear functions other than ReLU following the depthwise layer in the model architecture, a gain may generally be set based on the shape of the non-linear function. The gain may be read in from registers, retrieved from a look-up table, or the like. So, for example, where a depthwise separable convolution architecture implements a different nonlinear operation, such as Leaky ReLU, then a non-zero gain may be set.</p><p id="p-0112" num="0108">Second, the determined gain may be applied to the input data at step <b>806</b>, which may zero out some of the input data elements and thereby form modified input data, and then fused weights may be applied to the modified input data at step <b>810</b> in a CIM array. As above, the fused weights may be generated offline by convolving the depthwise and pointwise weights at step <b>808</b>.</p><heading id="h-0012" level="1">Example CIM Architecture for Performing Depthwise Separable Convolution without a DMAC</heading><p id="p-0113" num="0109"><figref idref="DRAWINGS">FIG. <b>9</b></figref> depicts an example CIM architecture <b>900</b> configured to use fused weights to perform depthwise separable convolution without the need for a DMAC.</p><p id="p-0114" num="0110">In the example of <figref idref="DRAWINGS">FIG. <b>9</b></figref>, there are N bits of input pre-activation data stored, for example, in input data buffers <b>902</b>, corresponding to the number of input bits for a patch (or receptive field) of input data. For example, for a 3&#xd7;3 patch of binary input data (i.e., 1 bit), N=9. The pre-activation data may be received from an input data buffer, such as <b>802</b> in <figref idref="DRAWINGS">FIG. <b>8</b></figref>. For multibit pre-activation data, the input data may be sequentially processed in 1-bit patches. For example, each individual input data buffer (e.g., associated with one row of the CIM, may have pre-activation input buffer registers that hold 8 bits of input data for each CIM input line. The input data buffer can retrieve or receive data from system memory.</p><p id="p-0115" num="0111">Multi-bit depthwise kernel weights are stored in a plurality of bitcells <b>906</b> of a CIM array, wherein each individual bitcell (e.g., <b>908</b>) is addressable by a wordline (WL) and a bitline (BL) for performing CIM processing. In this example, the depthwise kernel weights are two-bit and thus are written to two bitlines (e.g., B<b>0</b> and B<b>1</b> in this example). For simplicity, only N wordlines (rows) of the CIM array are depicted in <figref idref="DRAWINGS">FIG. <b>9</b></figref>, but the CIM array may comprise additional wordlines, bitlines, bitcells, and the like. Similarly, the input activation size and kernel sizes may be different in other examples.</p><p id="p-0116" num="0112">Sequential accumulators <b>912</b> are connected to the outputs of bitcells <b>906</b> and configured to perform sequential accumulation (e.g., voltage accumulation) on the output of each of the bitcells <b>906</b> during CIM processing. In some aspects, there is one sequential accumulator connected to each bitline, and the sequential accumulators can be connected together in order to accumulate across multiple bitlines (e.g., voltage across multiple bitlines). In some aspects, every set number of serially connected accumulators is connected to an output to another processing block. For example, in an 8-bit example, every eighth serial accumulator may be connected to an output, such as comparator <b>914</b> or ADC <b>918</b> in the example of <figref idref="DRAWINGS">FIG. <b>9</b></figref>. Note that this set number may vary in different aspects. In some aspects, comparator <b>914</b> can be considered as a simplified ADC with a coarse grain output.</p><p id="p-0117" num="0113">In this example, the output of sequential accumulators <b>912</b> is provided to comparator <b>914</b>, which is configured to compare the output to a threshold, such as a voltage threshold. In one aspect, comparator <b>914</b> comprises an array of M comparators, where M is the number of depthwise channels, and each comparator output controls a group of 9 input gain blocks (e.g., for a 3&#xd7;3 depthwise kernel size case) at a time.</p><p id="p-0118" num="0114">Based on the comparison by comparator <b>914</b>, a gain is determined and set by gain blocks <b>904</b>. In one example, when the output of sequential accumulators <b>912</b> is below a threshold, as determined by comparator <b>914</b>, the gain blocks <b>904</b> are set to zero for activations 1 . . . N when processed by the fused weights in bitcells <b>910</b>.</p><p id="p-0119" num="0115">The processing of bitcells <b>906</b> by the CIM array and the subsequent gain setting through sequential accumulators <b>912</b>, comparator <b>914</b>, and gain blocks <b>904</b> corresponds to the first operation described above with respect to <figref idref="DRAWINGS">FIG. <b>8</b></figref>.</p><p id="p-0120" num="0116">Fused weights are stored in a plurality of bitcells <b>910</b> of the CIM array. In this example, the fused weights stored in bitcell <b>910</b> are likewise two-bit, and thus are written to two bitlines (e.g., B<b>2</b> and B<b>3</b> in this example). More generally, a fused weight kernel with stride=1 may generally have a dimensionality of W<sub>DW</sub>&#xd7;H<sub>DW</sub>&#xd7;C<sub>DW</sub>&#xd7;C<sub>PW</sub>, where W<sub>DW </sub>is the width of the depthwise kernel, H<sub>DW </sub>is the height of the depthwise kernel, C<sub>DW </sub>is number of the channels in the depthwise kernel, and C<sub>PW </sub>is the number of channels in the pointwise kernel. Thus, the CIM dimensionality of the fused weight kernel may be (W<sub>DW</sub>&#xd7;H<sub>PW</sub>&#xd7;C<sub>DW</sub>) rows (wordlines) by (BW<sub>DW</sub>+(BW<sub>Fused</sub>&#xd7;C<sub>DW</sub>)) columns (bitlines).</p><p id="p-0121" num="0117">As described above with respect to <figref idref="DRAWINGS">FIG. <b>8</b></figref>, fused weights may be generated offline by convolving depthwise convolution layer weights with pointwise convolution layer weights. In some examples, the resulting fused weights may be scaled to match the bit width of the CIM array in which they will be processed. Note that it is possible for the depthwise and pointwise weights to have different bit widths. If the fused weights have a bit width that is larger than the bit width of the CIM array, then the fused weights can be scaled using a scaling factor and the final output can be scaled back after the analog to digital conversion.</p><p id="p-0122" num="0118">Sequential accumulators <b>916</b> are connected to the outputs of bitcells <b>910</b> and configured to perform sequential accumulation (e.g., voltage accumulation) on the output of each of the bitcells <b>910</b> during CIM processing. Note that in this example a separate set of sequential accumulators <b>916</b> is depicted, but this may be the same sequential accumulators as <b>912</b> multiplexed between various sets of columns in some aspects.</p><p id="p-0123" num="0119">Analog-to-digital converter (ADC) <b>918</b> is connected to sequential accumulators <b>916</b> and configured to convert the analog domain output to digital domain output. As above, ADC <b>918</b> may be connected to one sequential accumulator at the end of a string of a set number of sequential accumulators. That is, ADC <b>918</b> need not have a direct connection to each and every sequential accumulator.</p><p id="p-0124" num="0120">In some aspects, such as for an 8-bit weight case, each group of 8 columns in a CIM array may be connected to one ADC (e.g., an 8-bit ADC), such as ADC <b>918</b>. In other aspects, for example for certain low power implementation, an ADC may be time shared by multiple 8 column groups to save area and power. In other aspects, the number of columns connected to each ADC may be based on the bit-width of the weights.</p><p id="p-0125" num="0121">Scaling block <b>920</b> is connected to the output of ADC <b>918</b> and configured to optionally scale the output prior to performing a nonlinear activation operation on the output. For example, if the bit width of the activation was scaled for the size of the CIM array, it can be rescaled to an original bit width prior to downstream digital domain processing, such as in nonlinear activation block <b>922</b>.</p><p id="p-0126" num="0122">Accordingly, <figref idref="DRAWINGS">FIG. <b>9</b></figref> depicts an example of a single CIM array performing depthwise separable convolutional neural network processing with fused weights without the need for a DMAC block.</p><p id="p-0127" num="0123">Note that <figref idref="DRAWINGS">FIG. <b>9</b></figref> only depicts a subset of bitcells of a CIM array for simplicity and clarity. For example, pre-activation data stored in input buffers <b>902</b> may represent a single channel-wise group of data for processing, as described further below with respect to <figref idref="DRAWINGS">FIG. <b>10</b></figref>. However, the CIM array may have many additional wordlines (rows) and bitlines (columns) such that additional data may be loaded and processed concurrently, such as multiple channel-wise groups of pre-activation data.</p><p id="p-0128" num="0124">Further, <figref idref="DRAWINGS">FIG. <b>9</b></figref> depicts one arrangement of depthwise weights and fused weights, wherein the depthwise weights are written to the first set of columns, and the fused weights are written to a second set of columns after the depthwise columns, but in other aspects, different arrangements could be configured. <figref idref="DRAWINGS">FIG. <b>9</b></figref> depicts just one example.</p><heading id="h-0013" level="1">Example Processing Flow for Performing Depthwise Separable Convolution in a CIM without a DMAC</heading><p id="p-0129" num="0125"><figref idref="DRAWINGS">FIG. <b>10</b></figref> depicts an example processing flow <b>1000</b> for performing depthwise separable convolution in a CIM without a DMAC.</p><p id="p-0130" num="0126">Process <b>1000</b> begins at step <b>1002</b> where, for example, pre-activation data (e.g., for a convolutional neural network layer) is received for processing by a CIM array.</p><p id="p-0131" num="0127">Process <b>1000</b> then proceeds to step <b>1004</b> where weights for a depthwise portion of a depthwise separable convolution (e.g., for a convolutional neural network layer) are loaded into CIM columns, such as the weights loaded into bitcells <b>906</b> in FIG. <b>9</b>. In some aspects, as depicted in <figref idref="DRAWINGS">FIG. <b>9</b></figref>, the depthwise weights are loaded into the first BW columns of a CIM array, where BW is the bit width of the weights. For example, in <figref idref="DRAWINGS">FIG. <b>9</b></figref>, the weights are 2-bit and thus the first BW=2 columns of the CIM array are loaded with the depthwise weights.</p><p id="p-0132" num="0128">Process <b>1000</b> then proceeds to step <b>1006</b> where fused weights are loaded into CIM columns, such as the weights loaded into bitcells <b>910</b> in <figref idref="DRAWINGS">FIG. <b>9</b></figref>. As above, the fusing of the weights may be performed offline, and thus is not depicted as a step in process <b>1000</b>.</p><p id="p-0133" num="0129">Process <b>1000</b> then proceeds to step <b>1008</b> where pre-activations are loaded into wordlines (rows) of the CIM array in channel-wise groups. For example, assume a 5&#xd7;5&#xd7;3 depthwise kernel, as depicted in <figref idref="DRAWINGS">FIG. <b>3</b>A</figref> at <b>304</b>A-C, then three channel-wise groups of input pre-activations comprising 5&#xd7;5=25 bits are loaded into rows of the CIM array. So, for example, in <figref idref="DRAWINGS">FIG. <b>9</b></figref>, N=25 for each channel-wise group of pre-activations. Where the pre-activation data is multibit, then each wordline is loaded with one bit of the multibit pre-activation data in a time serial fashion.</p><p id="p-0134" num="0130">Process <b>1000</b> then proceeds to step <b>1010</b> where a channel-wise group of pre-activation data is processed by the CIM array to generate depthwise output. For example, as depicted in <figref idref="DRAWINGS">FIG. <b>9</b></figref>, the output of bitcells <b>906</b> corresponding to the depthwise kernel may be accumulated by sequential accumulators <b>912</b>. In some aspects, the accumulation is a voltage-type accumulation.</p><p id="p-0135" num="0131">Process <b>1000</b> then proceeds to step <b>1012</b> where the depthwise output is compared to a threshold. For example, the accumulated voltage may be compared to a voltage threshold.</p><p id="p-0136" num="0132">If at step <b>1012</b>, the depthwise output is below the threshold, then the pre-activation values for the channel-wise group can modified. For example, the pre-activations can be set to zero and/or any buffer associated with the pre-activations can be cleared at step <b>1014</b>.</p><p id="p-0137" num="0133">For example, in <figref idref="DRAWINGS">FIG. <b>9</b></figref>, the pre-activations in input buffers <b>902</b> may be zeroed out by setting the associated gain blocks <b>904</b> to zero. In other words, the gain acts like a multiplication by zero in order to zero out the pre-activation values. Further, the wordlines (rows) associated with the current channel-wise group of wordlines may be disabled (e.g., &#x201c;tristated&#x201d;) so that they do not contribute to subsequent processing with the fused weights, which beneficially saves processing power. This step has the equivalent effect of performing a nonlinear operation, such as a ReLU operation, on the depthwise output without having to actually perform the nonlinear operation, which may require data movement and further processing blocks, such as a DMAC. Thus, processing and memory efficiency is improved using the thresholding at step <b>1012</b> and pre-activation data adaptation at step <b>1014</b>.</p><p id="p-0138" num="0134">If at step <b>1012</b>, the output is above the threshold, then process <b>1000</b> proceeds to step <b>1016</b> where it is determined if the currently processed channel-wise group is the last channel wise group. If not, then process <b>1000</b> returns to step <b>1010</b> and processes the next channel-wise group according to the same process as described above. If so, then process <b>1000</b> proceeds to step <b>1018</b>.</p><p id="p-0139" num="0135">At step <b>1018</b>, the pre-activation data is processed by the fused weights stored in the CIM array. For example, the pre-activation data in input buffers <b>902</b> of <figref idref="DRAWINGS">FIG. <b>9</b></figref> may be processed with the fused weights stored in bitcells <b>910</b> to generate fused weight output that is accumulated by sequential accumulators <b>916</b>.</p><p id="p-0140" num="0136">Note that the set of pre-activation data in this processing step may differ from that originally loaded in step <b>1008</b> because of the thresholding and clearing in steps <b>1012</b> and <b>1014</b>. For example, certain pre-activations may be zeroed out as compared to their original values. Further, as above, certain rows may be disabled for processing at step <b>1018</b> based on the thresholding at step <b>1012</b>.</p><p id="p-0141" num="0137">Further, in some cases the pre-activation data and weight data loaded to a CIM array may not use all of the bitcells in the CIM array. In such cases, tiling may be used to activate or deactivate certain portions of the CIM array to save power. However, depending on the granularity of the tiling control available, there may be certain bitcells that are still active even after tiling control, but which do not have weight data for the current processing. In such cases, dummy weight data may be added to the bitcells (e.g., zero weights) so that any pre-activation data processed by the dummy weights does not affect the accumulations, and therefore the output.</p><p id="p-0142" num="0138">The output of step <b>1018</b> is processed by a nonlinear operation at step <b>1020</b> to generate layer output (e.g., depthwise convolutional layer output). Note that various intermediate operations may be performed between the processing by the CIM in step <b>1018</b> and the nonlinear operation at step <b>1020</b>, such as analog-to-digital conversion (e.g., via ADC <b>918</b> in <figref idref="DRAWINGS">FIG. <b>9</b></figref>), pooling, biasing, shifting, scaling (e.g., scaling block <b>920</b> in <figref idref="DRAWINGS">FIG. <b>9</b></figref>), and the like. The output (not depicted) of the nonlinear operation at step <b>1020</b> may be provided, for example, to an activation buffer for further processing (e.g., in another layer of a model) or for use by a host processing system.</p><p id="p-0143" num="0139">Process <b>1000</b> then proceeds to step <b>1022</b> where it is determined whether the last patch of data within a convolution sliding window has been processed. If not, then process <b>1000</b> returns to step <b>1008</b> where the next patch of pre-activations in the convolution window are loaded. If so, then process proceeds to step <b>1024</b>.</p><p id="p-0144" num="0140">At step <b>1024</b>, it is determined whether the last layer (e.g., the last convolutional layer of a neural network model) has been processed. If not, then process <b>1000</b> returns to step <b>1004</b> where a new layer's data may be processed as described above. If so, then the process ends at step <b>1026</b>.</p><p id="p-0145" num="0141">Note that <figref idref="DRAWINGS">FIG. <b>10</b></figref> is just one example method, and other methods are possible with fewer, more, or alternative steps in accordance with the various aspects described herein. For example, certain steps may be performed in a different order in other aspects, such as the loading of weights and pre-activations in steps <b>1004</b>-<b>1008</b>.</p><heading id="h-0014" level="1">Reducing Range Compression to Improve CIM Accuracy</heading><p id="p-0146" num="0142">As described with respect to steps <b>1012</b> and <b>1014</b> in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, the initial processing of pre-activation data with depthwise weights may result in various pre-activations being zeroed out and/or various wordlines of a CIM array being disabled for subsequent processing with fused weights. While this leads to improved energy efficiency during processing, it can also cause range compression for downstream ADCs, which can affect the range of the ADC output (e.g., the ADC quantization dynamic range is reduced).</p><p id="p-0147" num="0143">One method for addressing ADC range compression is through offline pre-processing. For example, signal range can be calculated and scaled for quantization optimization (min and max and scale factor). In one aspect, an input gain (or shift) control block may be implemented, for example in a CIM glue logic block, and the input gain block can be used to adjust the input signal level based on comparator output. For example, an input gain control block could be added after comparator <b>914</b> in <figref idref="DRAWINGS">FIG. <b>9</b></figref> in order to perform this control.</p><p id="p-0148" num="0144">For, example, for a conventional convolutional neural network, a first column of weight data may be activated and the accumulated output may be compared to a threshold to determine an input gain shift number.</p><p id="p-0149" num="0145">For a depthwise convolutional neural network, as described above, when the accumulated output for a channel-wise group of rows is below a threshold, then the pre-activation data may be zeroed out. When it is above the threshold, then an input gain shift number can be determined as in the case of a conventional convolutional neural network.</p><p id="p-0150" num="0146">Another method for addressing ADC range compression is to arrange weights within the CIM such that the first S columns of weights represent the array statistics of the whole array. Then, the output of the first S columns can be used to determine the input gain shift value, which as above, may be implemented by a block after comparator <b>914</b> in <figref idref="DRAWINGS">FIG. <b>9</b></figref> (as one example).</p><p id="p-0151" num="0147">In some aspects, ADC dynamic range gain updates can be performed for every input patch of data, for every set number of input patches, or based on another delimiter depending on input signal dynamics.</p><heading id="h-0015" level="1">Example Method for Performing Depthwise Separable Convolution in a CIM without a DMAC</heading><p id="p-0152" num="0148"><figref idref="DRAWINGS">FIG. <b>11</b></figref> depicts an example method <b>1100</b> for performing depthwise separable convolution in a CIM without use of a DMAC.</p><p id="p-0153" num="0149">Method <b>1100</b> begins at <b>1102</b> with storing a depthwise convolution kernel in a first one or more columns of a CIM array, such as depicted for example in bitcells <b>906</b> of <figref idref="DRAWINGS">FIG. <b>9</b></figref>.</p><p id="p-0154" num="0150">Method <b>1100</b> then proceeds to step <b>1104</b> with storing a fused convolution kernel in a second one or more columns of the CIM array, such as depicted for example in bitcells <b>910</b> of <figref idref="DRAWINGS">FIG. <b>9</b></figref>.</p><p id="p-0155" num="0151">Method <b>1100</b> then proceeds to step <b>1106</b> with storing pre-activations in input data buffers associated with a plurality of rows of the CIM array, such as in blocks <b>902</b> in <figref idref="DRAWINGS">FIG. <b>9</b></figref>.</p><p id="p-0156" num="0152">In some aspects, the pre-activations may be loaded from an activation buffer, or a host processing system memory, in the host processing machine in which the CIM array is integrated.</p><p id="p-0157" num="0153">Method <b>1100</b> then proceeds to step <b>1108</b> with processing the pre-activations with the depthwise convolution kernel in order to generate depthwise output, such as described above with respect to step <b>1010</b> of <figref idref="DRAWINGS">FIG. <b>10</b></figref>.</p><p id="p-0158" num="0154">Method <b>1100</b> then proceeds to step <b>1110</b> with generating modified pre-activations based on the depthwise output, such as described above with respect to step <b>1014</b> of <figref idref="DRAWINGS">FIG. <b>10</b></figref>.</p><p id="p-0159" num="0155">In some aspects, of method <b>1110</b>, modifying the one or more of the pre-activations comprises setting a gain for the one or more pre-activations to zero, such as by action of the gain blocks <b>904</b> in <figref idref="DRAWINGS">FIG. <b>9</b></figref>.</p><p id="p-0160" num="0156">Method <b>1100</b> then proceeds to step <b>1112</b> with processing the modified pre-activations with the fused convolution kernel to generate fused output, such as described above with respect to step <b>1018</b> of <figref idref="DRAWINGS">FIG. <b>10</b></figref>.</p><p id="p-0161" num="0157">In some aspects, method <b>1100</b> further includes comparing the depthwise output to a threshold, such as described above in <figref idref="DRAWINGS">FIG. <b>10</b></figref> with respect to step <b>1012</b>.</p><p id="p-0162" num="0158">In some aspects of method <b>1100</b>, modifying the one or more of the pre-activations comprises disabling the rows of the CIM array (e.g., tristating the rows) associated with the one or more of the pre-activations.</p><p id="p-0163" num="0159">In some aspects of method <b>1100</b>, the pre-activations comprise one channel-wise group of pre-activations of a plurality of channel-wise groups of pre-activations. More generally, method <b>1100</b> may iterate through a plurality of channel-wise groups of pre-activation data until all depthwise channels are processed, such as described with respect to <figref idref="DRAWINGS">FIG. <b>10</b></figref>. This channel-wise grouping allows for modifying pre-activations based on channel-specific data, which may beneficially lead to increased sparsity in the modified pre-activation data and thereby more power savings during processing of an entire layer's pre-activation data.</p><p id="p-0164" num="0160">In some aspects of method <b>1100</b>, processing the modified pre-activations with the fused convolution kernel to generate fused output is performed after each channel-wise group of pre-activations of the plurality of channel-wise groups of pre-activations has been processed by the depthwise kernel.</p><p id="p-0165" num="0161">In some aspects, method <b>1100</b> further includes converting the fused output to digital output data via an analog-to-digital converter (ADC), such as described above with respect to ADC <b>918</b> in <figref idref="DRAWINGS">FIG. <b>9</b></figref>.</p><p id="p-0166" num="0162">In some aspects, method <b>1100</b> further includes processing the digital output data with a nonlinear operation to generate activation data.</p><p id="p-0167" num="0163">In some aspects, method <b>1100</b> further includes providing the activation data to a host processing system, such as the processing system described with respect to <figref idref="DRAWINGS">FIG. <b>12</b></figref>.</p><p id="p-0168" num="0164">In some aspects, method <b>1100</b> further includes providing the activation data to an activation buffer for processing another layer of a model using the CIM array. For example, the activation data may be used in the next layer after the &#x201c;NO&#x201d; branch of step <b>1022</b> in <figref idref="DRAWINGS">FIG. <b>10</b></figref>.</p><p id="p-0169" num="0165">In some aspects, method <b>1100</b> further includes scaling the digital output data prior to processing the digital output data with the nonlinear operation, such as described above with respect to scaling block <b>920</b> in <figref idref="DRAWINGS">FIG. <b>9</b></figref>.</p><p id="p-0170" num="0166">In some aspects, method <b>1100</b> further includes fusing the depthwise convolution kernel and a pointwise convolution kernel in order to generate the fused convolution kernel.</p><p id="p-0171" num="0167">In some aspects of method <b>1100</b>, the CIM array comprises a plurality of static random access memory (SRAM) bitcells. In some aspects, one or more of the SRAM bitcells may be configurable between and AND operational logic and an XNOR operational logic, such as described above with respect to <figref idref="DRAWINGS">FIGS. <b>5</b>A and <b>5</b>B</figref>.</p><p id="p-0172" num="0168">In some aspects, method <b>1100</b> further includes scaling the one or more of the pre-activations to reduce range compression of the fused output.</p><p id="p-0173" num="0169">In some aspects of method <b>1100</b>, scaling is performed only on the one or more of the pre-activations having a depthwise output greater than the threshold.</p><p id="p-0174" num="0170">Note that <figref idref="DRAWINGS">FIG. <b>11</b></figref> is just one example method, and other methods are possible with fewer, more, or alternative steps in accordance with the various aspects described herein. For example, certain steps may be performed in a different order in other aspects, such as the storing of kernels and pre-activations in steps <b>1102</b>-<b>1106</b>.</p><heading id="h-0016" level="1">Example Processing System for Performing Convolutional Neural Network Processing</heading><p id="p-0175" num="0171"><figref idref="DRAWINGS">FIG. <b>12</b></figref> depicts an example processing system <b>1200</b> for performing convolutional neural network processing, such as described herein for example with respect to <figref idref="DRAWINGS">FIGS. <b>8</b>-<b>11</b></figref>.</p><p id="p-0176" num="0172">Processing system <b>1200</b> includes a central processing unit (CPU) <b>1202</b>, which in some examples may be a multi-core CPU. Instructions executed at the CPU <b>1202</b> may be loaded, for example, from a program memory associated with the CPU <b>1202</b> or may be loaded from a memory partition <b>1224</b>.</p><p id="p-0177" num="0173">Processing system <b>1200</b> also includes additional processing components tailored to specific functions, such as a graphics processing unit (GPU) <b>1204</b>, a digital signal processor (DSP) <b>1206</b>, a neural processing unit (NPU) <b>1208</b>, a multimedia processing unit <b>1210</b>, a multimedia processing unit <b>1210</b>, and a wireless connectivity component <b>1212</b>.</p><p id="p-0178" num="0174">An NPU, such as <b>1208</b>, is generally a specialized circuit configured for implementing all the necessary control and arithmetic logic for executing machine learning algorithms, such as algorithms for processing artificial neural networks (ANNs), deep neural networks (DNNs), random forests (RFs), and the like. An NPU may sometimes alternatively be referred to as a neural signal processor (NSP), tensor processing units (TPU), neural network processor (NNP), intelligence processing unit (IPU), vision processing unit (VPU), or graph processing unit.</p><p id="p-0179" num="0175">NPUs, such as <b>1208</b>, are configured to accelerate the performance of common machine learning tasks, such as image classification, machine translation, object detection, and various other predictive models. In some examples, a plurality of NPUs may be instantiated on a single chip, such as a system on a chip (SoC), while in other examples they may be part of a dedicated neural-network accelerator.</p><p id="p-0180" num="0176">NPUs may be optimized for training or inference, or in some cases configured to balance performance between both. For NPUs that are capable of performing both training and inference, the two tasks may still generally be performed independently.</p><p id="p-0181" num="0177">NPUs designed to accelerate training are generally configured to accelerate the optimization of new models, which is a highly compute-intensive operation that involves inputting an existing dataset (often labeled or tagged), iterating over the dataset, and then adjusting model parameters, such as weights and biases, in order to improve model performance. Generally, optimizing based on a wrong prediction involves propagating back through the layers of the model and determining gradients to reduce the prediction error.</p><p id="p-0182" num="0178">NPUs designed to accelerate inference are generally configured to operate on complete models. Such NPUs may thus be configured to input a new piece of data and rapidly process it through an already trained model to generate a model output (e.g., an inference).</p><p id="p-0183" num="0179">In one implementation, NPU <b>1208</b> is a part of one or more of CPU <b>1202</b>, GPU <b>1204</b>, and/or DSP <b>1206</b>.</p><p id="p-0184" num="0180">In some examples, wireless connectivity component <b>1212</b> may include subcomponents, for example, for third generation (3G) connectivity, fourth generation (4G) connectivity (e.g., 4G LTE), fifth generation connectivity (e.g., 5G or NR), Wi-Fi connectivity, Bluetooth connectivity, and other wireless data transmission standards. Wireless connectivity processing component <b>1212</b> is further connected to one or more antennas <b>1214</b>.</p><p id="p-0185" num="0181">Processing system <b>1200</b> may also include one or more sensor processing units <b>1216</b> associated with any manner of sensor, one or more image signal processors (ISPs) <b>1218</b> associated with any manner of image sensor, and/or a navigation processor <b>1220</b>, which may include satellite-based positioning system components (e.g., GPS or GLONASS) as well as inertial positioning system components.</p><p id="p-0186" num="0182">Processing system <b>1200</b> may also include one or more input and/or output devices <b>1222</b>, such as screens, touch-sensitive surfaces (including touch-sensitive displays), physical buttons, speakers, microphones, and the like.</p><p id="p-0187" num="0183">In some examples, one or more of the processors of processing system <b>1200</b> may be based on an ARM or RISC-V instruction set.</p><p id="p-0188" num="0184">Processing system <b>1200</b> also includes memory <b>1224</b>, which is representative of one or more static and/or dynamic memories, such as a dynamic random access memory, a flash-based static memory, and the like. In this example, memory <b>1224</b> includes computer-executable components, which may be executed by one or more of the aforementioned processors of processing system <b>1200</b>.</p><p id="p-0189" num="0185">In particular, in this example, memory <b>1224</b> includes CIM loading component <b>1224</b>A, CIM processing component <b>1224</b>B, scaling component <b>1224</b>C, comparing component <b>1224</b>D, nonlinear operation component <b>1224</b>E, and model parameters <b>1224</b>F. The depicted components, and others not depicted, may be configured to perform various aspects of the methods described herein.</p><p id="p-0190" num="0186">Processing system <b>1200</b> further comprises compute-in-memory (CIM) circuit <b>1226</b>, such as described above, for example, with respect to <figref idref="DRAWINGS">FIGS. <b>9</b></figref>. Further, the bitcells depicted in <figref idref="DRAWINGS">FIG. <b>9</b></figref> may be as described in <figref idref="DRAWINGS">FIGS. <b>5</b>A and <b>5</b>B</figref> in various aspects.</p><p id="p-0191" num="0187">Processing system <b>1200</b> further comprises comparator circuit <b>1228</b>, such as described above with respect to <figref idref="DRAWINGS">FIG. <b>9</b></figref>.</p><p id="p-0192" num="0188">Processing system <b>1200</b> further comprises analog-to-digital converter (ADC) circuit <b>1230</b>, such as described above, for example, with respect to <figref idref="DRAWINGS">FIGS. <b>9</b></figref>.</p><p id="p-0193" num="0189">Processing system <b>1200</b> further comprises scaling circuit <b>1232</b>, such as described above, for example, with respect to <figref idref="DRAWINGS">FIG. <b>9</b></figref>.</p><p id="p-0194" num="0190">Processing system <b>1200</b> further comprises nonlinear operation circuit <b>1234</b>, such as described above, for example, with respect to <figref idref="DRAWINGS">FIG. <b>9</b></figref>.</p><p id="p-0195" num="0191">Processing system <b>1200</b> further comprises buffer circuit <b>1234</b>, such as an activation buffer described above, for example, with respect to <figref idref="DRAWINGS">FIG. <b>10</b></figref>.</p><p id="p-0196" num="0192">Generally, processing system <b>1200</b> and/or components thereof may be configured to perform the methods described herein.</p><p id="p-0197" num="0193">Notably, in other aspects, aspects of processing system <b>1200</b> may be omitted, such as where processing system <b>1200</b> is a server computer or the like. For example, multimedia component <b>1210</b>, wireless connectivity <b>1212</b>, sensors <b>1216</b>, ISPs <b>1218</b>, and/or navigation component <b>1220</b> may be omitted in other aspects. Further, aspects of processing system <b>1200</b> maybe distributed, such as training a model and using the model to generate inferences, such as user verification predictions.</p><heading id="h-0017" level="1">Example Clauses</heading><p id="p-0198" num="0194">Implementation examples are described in the following numbered clauses:</p><p id="p-0199" num="0195">Clause 1: An apparatus, comprising: a compute-in-memory (CIM) array configured to: store a depthwise convolution kernel in a first one or more columns of the CIM array; store a fused convolution kernel in a second one or more columns of the CIM array; store pre-activations in a plurality of rows of the CIM array; process the pre-activations with the depthwise convolution kernel in order to generate depthwise output; generate modified pre-activations based on the depthwise output; and process the modified pre-activations with the fused convolution kernel to generate fused output.</p><p id="p-0200" num="0196">Clause 2: The apparatus of Clause 1, further comprising: a comparator configured to compare the depthwise output to a threshold; and a gain block connected to each respective row of the plurality of rows of the CIM array and configured to modify a pre-activation stored in its respective row if the depthwise output is less than the threshold.</p><p id="p-0201" num="0197">Clause 3: The apparatus of any one of Clauses 1-2, wherein: the pre-activations correspond to one channel-wise group of pre-activation data, and the depthwise output corresponds to one channel of the depthwise convolution kernel.</p><p id="p-0202" num="0198">Clause 4: The apparatus of any one of Clauses 1-3, wherein in order to modify one or more of the pre-activations based on the depthwise output, the CIM array is further configured to set the one or more pre-activations values to zero.</p><p id="p-0203" num="0199">Clause 5: The apparatus of any one of Clauses 1-4, wherein the CIM array is further configured to disable one or more rows of the CIM array associated with the modified pre-activations.</p><p id="p-0204" num="0200">Clause 6: The apparatus of any one of Clauses 1-5, further comprising a fusion block configured to fuse the depthwise convolution kernel and a pointwise convolution kernel in order to generate the fused convolution kernel.</p><p id="p-0205" num="0201">Clause 7: The apparatus of any one of Clauses 1-6, further comprising: a sequential accumulator connected to each column of the first one or more columns of the CIM array in which the depthwise convolution kernel is stored; and a comparator configured to receive accumulated output from at least one sequential accumulator connected to the first one or more columns of the CIM array.</p><p id="p-0206" num="0202">Clause 8: The apparatus of any one of Clauses 1-7, further comprising: a sequential accumulator connected to each column of the second one or more columns of the CIM array in which the fused convolution kernel is stored; an analog-to-digital converter (ADC) configured to receive accumulated output from at least one sequential accumulator connected to the second one or more columns of the CIM array and output digital data; and a nonlinear operation block configured to process the digital data and generate activation data.</p><p id="p-0207" num="0203">Clause 9: The apparatus of Clause 8, further comprising a scaling block configured to scale the digital data output from the ADC from a first bit width to a second bit width prior to providing the digital data to the nonlinear operation block.</p><p id="p-0208" num="0204">Clause 10: The apparatus of any one of Clauses 1-9, wherein the CIM array comprises a plurality of static random access memory (SRAM) bitcells.</p><p id="p-0209" num="0205">Clause 11: A method, comprising: storing a depthwise convolution kernel in a first one or more columns of a CIM array; storing a fused convolution kernel in a second one or more columns of the CIM array; storing pre-activations in one or more input data buffers associated with a plurality of rows of the CIM array; processing the pre-activations with the depthwise convolution kernel in order to generate depthwise output; modifying one or more of the pre-activations based on the depthwise output to generate modified pre-activations; and processing the modified pre-activations with the fused convolution kernel to generate fused output.</p><p id="p-0210" num="0206">Clause 12: The method of Clause 11, further comprising: comparing the depthwise output to a threshold, wherein modifying the one or more of the pre-activations comprises setting a gain for the one or more pre-activations to zero.</p><p id="p-0211" num="0207">Clause 13: The method of any one of Clauses 11-12, wherein modifying the one or more of the pre-activations comprises disabling the rows of the CIM array associated with the one or more of the pre-activations.</p><p id="p-0212" num="0208">Clause 14: The method of any one of Clauses 11-13, wherein the pre-activations comprise one channel-wise group of pre-activations of a plurality of channel-wise groups of pre-activations.</p><p id="p-0213" num="0209">Clause 15: The method of Clause 14, wherein processing the modified pre-activations with the fused convolution kernel to generate fused output is performed after each channel-wise group of pre-activations of the plurality of channel-wise groups of pre-activations has been processed by the depthwise convolution kernel.</p><p id="p-0214" num="0210">Clause 16: The method of any one of Clauses 11-15, further comprising converting the fused output to digital output data via an analog-to-digital converter (ADC).</p><p id="p-0215" num="0211">Clause 17: The method of Clause 16, further comprising processing the digital output data with a nonlinear operation to generate activation data.</p><p id="p-0216" num="0212">Clause 18: The method of Clause 17, further comprising providing the activation data to a host processing system.</p><p id="p-0217" num="0213">Clause 19: The method of Clause 17, further comprising providing the activation data to an activation buffer for processing another layer of a model using the CIM array.</p><p id="p-0218" num="0214">Clause 20: The method of any one of Clauses 17-19, further comprising scaling the digital output data prior to processing the digital output data with the nonlinear operation.</p><p id="p-0219" num="0215">Clause 21: The method of any one of Clauses 11-20, further comprising: fusing the depthwise convolution kernel and a pointwise convolution kernel in order to generate the fused convolution kernel.</p><p id="p-0220" num="0216">Clause 22: The method of any one of Clauses 11-21, wherein the CIM array comprises a plurality of static random access memory (SRAM) bitcells.</p><p id="p-0221" num="0217">Clause 23: The method of any one of Clauses 12-22, further comprising scaling the one or more of the pre-activations to reduce range compression of the fused output.</p><p id="p-0222" num="0218">Clause 24: The method of Clause 23, wherein scaling is performed only on the one or more of the pre-activations having a depthwise output greater than the threshold.</p><p id="p-0223" num="0219">Clause 25: A processing system, comprising: a memory comprising computer-executable instructions; one or more processors configured to execute the computer-executable instructions and cause the processing system to perform a method in accordance with any one of Clauses 11-24.</p><p id="p-0224" num="0220">Clause 26: A processing system, comprising means for performing a method in accordance with any one of Clauses 11-24.</p><p id="p-0225" num="0221">Clause 27: A non-transitory computer-readable medium comprising computer-executable instructions that, when executed by one or more processors of a processing system, cause the processing system to perform a method in accordance with any one of Clauses 11-24.</p><p id="p-0226" num="0222">Clause 28: A computer program product embodied on a computer-readable storage medium comprising code for performing a method in accordance with any one of Clauses 11-24.</p><heading id="h-0018" level="1">Additional Considerations</heading><p id="p-0227" num="0223">The preceding description is provided to enable any person skilled in the art to practice the various aspects described herein. The examples discussed herein are not limiting of the scope, applicability, or aspects set forth in the claims. Various modifications to these aspects will be readily apparent to those skilled in the art, and the generic principles defined herein may be applied to other aspects. For example, changes may be made in the function and arrangement of elements discussed without departing from the scope of the disclosure. Various examples may omit, substitute, or add various procedures or components as appropriate. For instance, the methods described may be performed in an order different from that described, and various steps may be added, omitted, or combined. Also, features described with respect to some examples may be combined in some other examples. For example, an apparatus may be implemented or a method may be practiced using any number of the aspects set forth herein. In addition, the scope of the disclosure is intended to cover such an apparatus or method that is practiced using other structure, functionality, or structure and functionality in addition to, or other than, the various aspects of the disclosure set forth herein. It should be understood that any aspect of the disclosure disclosed herein may be embodied by one or more elements of a claim.</p><p id="p-0228" num="0224">As used herein, the word &#x201c;exemplary&#x201d; means &#x201c;serving as an example, instance, or illustration.&#x201d; Any aspect described herein as &#x201c;exemplary&#x201d; is not necessarily to be construed as preferred or advantageous over other aspects.</p><p id="p-0229" num="0225">As used herein, a phrase referring to &#x201c;at least one of&#x201d; a list of items refers to any combination of those items, including single members. As an example, &#x201c;at least one of: a, b, or c&#x201d; is intended to cover a, b, c, a-b, a-c, b-c, and a-b-c, as well as any combination with multiples of the same element (e.g., a-a, a-a-a, a-a-b, a-a-c, a-b-b, a-c-c, b-b, b-b-b, b-b-c, c-c, and c-c-c or any other ordering of a, b, and c).</p><p id="p-0230" num="0226">As used herein, the term &#x201c;determining&#x201d; encompasses a wide variety of actions. For example, &#x201c;determining&#x201d; may include calculating, computing, processing, deriving, investigating, looking up (e.g., looking up in a table, a database or another data structure), ascertaining and the like. Also, &#x201c;determining&#x201d; may include receiving (e.g., receiving information), accessing (e.g., accessing data in a memory) and the like. Also, &#x201c;determining&#x201d; may include resolving, selecting, choosing, establishing and the like.</p><p id="p-0231" num="0227">The methods disclosed herein comprise one or more steps or actions for achieving the methods. The method steps and/or actions may be interchanged with one another without departing from the scope of the claims. In other words, unless a specific order of steps or actions is specified, the order and/or use of specific steps and/or actions may be modified without departing from the scope of the claims. Further, the various operations of methods described above may be performed by any suitable means capable of performing the corresponding functions. The means may include various hardware and/or software component(s) and/or module(s), including, but not limited to a circuit, an application specific integrated circuit (ASIC), or processor. Generally, where there are operations illustrated in figures, those operations may have corresponding counterpart means-plus-function components with similar numbering.</p><p id="p-0232" num="0228">The following claims are not intended to be limited to the aspects shown herein, but are to be accorded the full scope consistent with the language of the claims. Within a claim, reference to an element in the singular is not intended to mean &#x201c;one and only one&#x201d; unless specifically so stated, but rather &#x201c;one or more.&#x201d; Unless specifically stated otherwise, the term &#x201c;some&#x201d; refers to one or more. No claim element is to be construed under the provisions of 35 U.S.C. &#xa7;112(f) unless the element is expressly recited using the phrase &#x201c;means for&#x201d; or, in the case of a method claim, the element is recited using the phrase &#x201c;step for.&#x201d; All structural and functional equivalents to the elements of the various aspects described throughout this disclosure that are known or later come to be known to those of ordinary skill in the art are expressly incorporated herein by reference and are intended to be encompassed by the claims. Moreover, nothing disclosed herein is intended to be dedicated to the public regardless of whether such disclosure is explicitly recited in the claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An apparatus, comprising:<claim-text>a compute-in-memory (CIM) array configured to:<claim-text>store a depthwise convolution kernel in a first one or more columns of the CIM array;</claim-text><claim-text>store a fused convolution kernel in a second one or more columns of the CIM array;</claim-text><claim-text>store pre-activations in a plurality of rows of the CIM array;</claim-text><claim-text>process the pre-activations with the depthwise convolution kernel in order to generate depthwise output;</claim-text><claim-text>generate modified pre-activations based on the depthwise output; and</claim-text><claim-text>process the modified pre-activations with the fused convolution kernel to generate fused output.</claim-text></claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>a comparator configured to compare the depthwise output to a threshold; and</claim-text><claim-text>a gain block connected to each respective row of the plurality of rows of the CIM array and configured to modify a pre-activation stored in its respective row if the depthwise output is less than the threshold.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>the pre-activations correspond to one channel-wise group of pre-activation data, and</claim-text><claim-text>the depthwise output corresponds to one channel of the depthwise convolution kernel.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein in order to modify one or more of the pre-activations based on the depthwise output, the CIM array is further configured to set the one or more pre-activations values to zero.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the CIM array is further configured to disable one or more rows of the CIM array associated with the modified pre-activations.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising a fusion block configured to fuse the depthwise convolution kernel and a pointwise convolution kernel in order to generate the fused convolution kernel.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>a sequential accumulator connected to each column of the first one or more columns of the CIM array in which the depthwise convolution kernel is stored; and</claim-text><claim-text>a comparator configured to receive accumulated output from at least one sequential accumulator connected to the first one or more columns of the CIM array.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>a sequential accumulator connected to each column of the second one or more columns of the CIM array in which the fused convolution kernel is stored;</claim-text><claim-text>an analog-to-digital converter (ADC) configured to receive accumulated output from at least one sequential accumulator connected to the second one or more columns of the CIM array and output digital data; and</claim-text><claim-text>a nonlinear operation block configured to process the digital data and generate activation data.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The apparatus of <claim-ref idref="CLM-00008">claim 8</claim-ref>, further comprising a scaling block configured to scale the digital data output from the ADC from a first bit width to a second bit width prior to providing the digital data to the nonlinear operation block.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the CIM array comprises a plurality of static random access memory (SRAM) bitcells.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. A method, comprising:<claim-text>storing a depthwise convolution kernel in a first one or more columns of a CIM array;</claim-text><claim-text>storing a fused convolution kernel in a second one or more columns of the CIM array;</claim-text><claim-text>storing pre-activations in one or more input data buffers associated with a plurality of rows of the CIM array;</claim-text><claim-text>processing the pre-activations with the depthwise convolution kernel in order to generate depthwise output;</claim-text><claim-text>modifying one or more of the pre-activations based on the depthwise output to generate modified pre-activations; and</claim-text><claim-text>processing the modified pre-activations with the fused convolution kernel to generate fused output.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, further comprising:<claim-text>comparing the depthwise output to a threshold,</claim-text><claim-text>wherein modifying the one or more of the pre-activations comprises setting a gain for the one or more pre-activations to zero.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein modifying the one or more of the pre-activations comprises disabling the rows of the CIM array associated with the one or more of the pre-activations.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the pre-activations comprise one channel-wise group of pre-activations of a plurality of channel-wise groups of pre-activations.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein processing the modified pre-activations with the fused convolution kernel to generate fused output is performed after each channel-wise group of pre-activations of the plurality of channel-wise groups of pre-activations has been processed by the depthwise convolution kernel.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, further comprising converting the fused output to digital output data via an analog-to-digital converter (ADC).</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref>, further comprising processing the digital output data with a nonlinear operation to generate activation data.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The method of <claim-ref idref="CLM-00017">claim 17</claim-ref>, further comprising providing the activation data to a host processing system.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The method of <claim-ref idref="CLM-00017">claim 17</claim-ref>, further comprising providing the activation data to an activation buffer for processing another layer of a model using the CIM array.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The method of <claim-ref idref="CLM-00017">claim 17</claim-ref>, further comprising scaling the digital output data prior to processing the digital output data with the nonlinear operation.</claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, further comprising: fusing the depthwise convolution kernel and a pointwise convolution kernel in order to generate the fused convolution kernel.</claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the CIM array comprises a plurality of static random access memory (SRAM) bitcells.</claim-text></claim><claim id="CLM-00023" num="00023"><claim-text><b>23</b>. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, further comprising scaling the one or more of the pre-activations to reduce range compression of the fused output.</claim-text></claim><claim id="CLM-00024" num="00024"><claim-text><b>24</b>. The method of <claim-ref idref="CLM-00023">claim 23</claim-ref>, wherein scaling is performed only on the one or more of the pre-activations having a depthwise output greater than the threshold.</claim-text></claim><claim id="CLM-00025" num="00025"><claim-text><b>25</b>. A non-transitory computer-readable medium comprising instructions that, when executed by a processor of a processing system, cause the processing system to perform a method, the method comprising:<claim-text>storing a depthwise convolution kernel in a first one or more columns of a CIM array;</claim-text><claim-text>storing a fused convolution kernel in a second one or more columns of the CIM array;</claim-text><claim-text>storing pre-activations in one or more input data buffers associated with a plurality of rows of the CIM array;</claim-text><claim-text>processing the pre-activations with the depthwise convolution kernel in order to generate depthwise output;</claim-text><claim-text>modifying one or more of the pre-activations based on the depthwise output to generate modified pre-activations; and</claim-text><claim-text>processing the modified pre-activations with the fused convolution kernel to generate fused output.</claim-text></claim-text></claim><claim id="CLM-00026" num="00026"><claim-text><b>26</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00025">claim 25</claim-ref>, wherein the method further comprises:<claim-text>comparing the depthwise output to a threshold,</claim-text><claim-text>wherein modifying the one or more of the pre-activations comprises setting a gain for the one or more pre-activations to zero.</claim-text></claim-text></claim><claim id="CLM-00027" num="00027"><claim-text><b>27</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00025">claim 25</claim-ref>, wherein modifying the one or more of the pre-activations comprises disabling the rows of the CIM array associated with the one or more of the pre-activations.</claim-text></claim><claim id="CLM-00028" num="00028"><claim-text><b>28</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00025">claim 25</claim-ref>, wherein the pre-activations comprise one channel-wise group of pre-activations of a plurality of channel-wise groups of pre-activations.</claim-text></claim><claim id="CLM-00029" num="00029"><claim-text><b>29</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00028">claim 28</claim-ref>, wherein processing the modified pre-activations with the fused convolution kernel to generate fused output is performed after each channel-wise group of pre-activations of the plurality of channel-wise groups of pre-activations has been processed by the depthwise convolution kernel.</claim-text></claim><claim id="CLM-00030" num="00030"><claim-text><b>30</b>. A processing system, comprising:<claim-text>means for storing a depthwise convolution kernel in a first one or more columns of a CIM array;</claim-text><claim-text>means for storing a fused convolution kernel in a second one or more columns of the CIM array;</claim-text><claim-text>means for storing pre-activations in one or more input data buffers associated with a plurality of rows of the CIM array;</claim-text><claim-text>means for processing the pre-activations with the depthwise convolution kernel in order to generate depthwise output;</claim-text><claim-text>means for modifying one or more of the pre-activations based on the depthwise output to generate modified pre-activations; and</claim-text><claim-text>means for processing the modified pre-activations with the fused convolution kernel to generate fused output.</claim-text></claim-text></claim></claims></us-patent-application>