<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005478A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005478</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17850355</doc-number><date>20220627</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>JP</country><doc-number>2021-108897</doc-number><date>20210630</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>22</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>30</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>22</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>005</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>30</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>C</subclass><main-group>21</main-group><subgroup>3629</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">INFORMATION PROCESSING APPARATUS, INFORMATION PROCESSING SYSTEM, AND INFORMATION PROCESSING METHOD</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>TOYOTA JIDOSHA KABUSHIKI KAISHA</orgname><address><city>Toyota-shi</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>MORI</last-name><first-name>Koki</first-name><address><city>Inuyama-shi</city><country>JP</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>TOYOTA JIDOSHA KABUSHIKI KAISHA</orgname><role>03</role><address><city>Toyota-shi</city><country>JP</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">An information processing apparatus comprises a controller configured to: give a speech guidance to a user using first speech data corresponding to a first language;<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0000">    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0000">determine that the user utilizes a language different from the first language; and acquire second speech data corresponding to the language to be utilized by the user on a basis of a result of the determination.</li>    </ul>    </li></ul></p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="63.42mm" wi="95.67mm" file="US20230005478A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="78.15mm" wi="97.71mm" file="US20230005478A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="203.37mm" wi="120.99mm" orientation="landscape" file="US20230005478A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="147.49mm" wi="111.51mm" orientation="landscape" file="US20230005478A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="202.95mm" wi="54.69mm" orientation="landscape" file="US20230005478A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="68.92mm" wi="87.55mm" file="US20230005478A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="199.64mm" wi="101.01mm" orientation="landscape" file="US20230005478A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="142.24mm" wi="134.62mm" orientation="landscape" file="US20230005478A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="129.71mm" wi="85.85mm" file="US20230005478A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="136.57mm" wi="134.62mm" file="US20230005478A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="127.51mm" wi="85.85mm" file="US20230005478A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="183.22mm" wi="54.44mm" orientation="landscape" file="US20230005478A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">CROSS REFERENCE TO THE RELATED APPLICATION</heading><p id="p-0002" num="0001">This application claims the benefit of Japanese Patent Application No. 2021-108897, filed on Jun. 30, 2021, which is hereby incorporated by reference herein in its entirety.</p><heading id="h-0002" level="1">BACKGROUND</heading><heading id="h-0003" level="1">Technical Field</heading><p id="p-0003" num="0002">The present disclosure relates to an apparatus that provides information.</p><heading id="h-0004" level="1">Description of the Related Art</heading><p id="p-0004" num="0003">A system that provides information by an in-vehicle computer is in widespread use.</p><p id="p-0005" num="0004">Concerning this, for example, Japanese Patent Laid-Open No. 2008-026653 discloses an invention related to a navigation apparatus that outputs a guidance through a speech and visual information.</p><heading id="h-0005" level="1">SUMMARY</heading><p id="p-0006" num="0005">The present disclosure is directed to reducing cost of an apparatus that gives a speech guidance.</p><p id="p-0007" num="0006">The present disclosure in its one aspect provides an information processing apparatus comprising a controller configured to: give a speech guidance to a user using first speech data corresponding to a first language; determine that the user utilizes a language different from the first language; and acquire second speech data corresponding to the language to be utilized by the user on a basis of a result of the determination.</p><p id="p-0008" num="0007">The present disclosure in its another aspect provides an information processing system comprising a first apparatus that gives a speech guidance to a user and a second apparatus that provides speech data to be used for the speech guidance, wherein the first apparatus comprises a controller configured to: give a speech guidance to the user using first speech data corresponding to a first language; determine that the user utilizes a language different from the first language; and acquire second speech data corresponding to the language to be utilized by the user from the second apparatus on a basis of a result of the determination.</p><p id="p-0009" num="0008">The present disclosure in its another aspect provides an information processing method comprising: a step of giving a speech guidance to a user using first speech data corresponding to a first language; a step of determining that the user utilizes a language different from the first language; and a step of acquiring second speech data corresponding to the language to be utilized by the user on a basis of a result of the determination.</p><p id="p-0010" num="0009">According to the present disclosure, it is possible to reduce cost of an apparatus that gives a speech guidance.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0006" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic view of a vehicle system according to a first embodiment;</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a view for explaining components of a vehicle according to the first embodiment;</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a schematic view for explaining functional modules of a controller and data stored in a storage;</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is an example of a data set stored in the storage;</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a schematic view of a server apparatus in the first embodiment;</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is an example of a speech database stored in the server apparatus;</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart of processing in the first embodiment;</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is an example of a screen on which language setting is performed;</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a flowchart of processing in a second embodiment;</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is an example of a screen on which language setting is suggested; and</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is an example of speech data to be generated by a server apparatus in a third embodiment.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0007" level="1">DESCRIPTION OF THE EMBODIMENTS</heading><p id="p-0022" num="0021">In recent years, an automobile that can be connected to a network has been in widespread use. By an in-vehicle device providing network connection, a service that performs support in an emergency and a service related to security can be provided. Such a device is also called a data communication module (DCM). Further, a DCM that can give a speech guidance using speech data set in advance is known.</p><p id="p-0023" num="0022">However, in a case where vehicles are sold across a plurality of countries and regions, a problem can arise that a language that a user desires to use does not match a language of speech data set in advance in the apparatus. While it is also possible to hold speech data corresponding to all languages in advance and make the speech data selectable, another problem arises that manufacturing cost of the apparatus increases.</p><p id="p-0024" num="0023">The information processing apparatus according to the present disclosure solves such a problem.</p><p id="p-0025" num="0024">An information processing apparatus according to one aspect of the present disclosure includes a controller configured to give a speech guidance to a user using first speech data corresponding to a first language, determine that the user utilizes a language different from the first language and acquire second speech data corresponding to the language to be utilized by the user on the basis of a result of the determination.</p><p id="p-0026" num="0025">While the information processing apparatus is typically an apparatus mounted on a vehicle, the information processing apparatus is not limited to this. The first speech data can be made, for example, a set of a plurality of pieces of speech data stored in advance in the apparatus. The controller gives a speech guidance to the user using the first speech data and determines that the user utilizes a language different from the first language.</p><p id="p-0027" num="0026">The language to be utilized by the user may be acquired, for example, via another device mounted on the vehicle. For example, in a case where another device (such as, for example, a car navigation device) that can be utilized as a user interface is mounted on the vehicle, designation of the language may be accepted via the device. Further, in a case where a speech input device is mounted on the vehicle, the language to be utilized by the user may be determined on the basis of a speech made by the user. In this manner, the information processing apparatus according to the present disclosure does not necessarily have to include an input device for designating the language.</p><p id="p-0028" num="0027">The controller acquires the second speech data corresponding to the language to be utilized by the user on the basis of the result of the determination. The second speech data may be acquired, for example, from an external apparatus that provides speech data via a network.</p><p id="p-0029" num="0028">The controller switches the language, for example, by overwriting the first speech data with the acquired second speech data.</p><p id="p-0030" num="0029">Specific embodiments of the present disclosure will be described below on the basis of the drawings. Hardware configurations, module configurations, functional configurations, and the like, described in the embodiments are not intended to limit the technical scope of the disclosure unless otherwise described.</p><heading id="h-0008" level="1">First Embodiment</heading><p id="p-0031" num="0030">Outline of a vehicle system according to a first embodiment will be described with reference to <figref idref="DRAWINGS">FIG. <b>1</b></figref>. The vehicle system according to the present embodiment includes a vehicle <b>10</b> and a server apparatus <b>20</b>.</p><p id="p-0032" num="0031">The vehicle <b>10</b> is a connected car that has a function of communicating with an external network. The vehicle <b>10</b> includes a data communication module (DCM) <b>100</b>, an in-vehicle device <b>200</b> and an electronic control unit (ECU) <b>300</b>.</p><p id="p-0033" num="0032">Note that while <figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a single ECU <b>300</b>, the vehicle <b>10</b> may include a plurality of ECUs <b>300</b>.</p><p id="p-0034" num="0033">The DCM <b>100</b> is a device that performs wireless communication with the external network. The DCM <b>100</b> functions as a gateway for connecting components (hereinafter, vehicle components) provided at the vehicle <b>10</b> to the external network. For example, the DCM <b>100</b> provides access to the external network, to the in-vehicle device <b>200</b> and the ECUs <b>300</b> provided at the vehicle <b>10</b>. This enables the in-vehicle device <b>200</b> and the ECUs <b>300</b> to communicate with an external apparatus connected to the network via the DCM <b>100</b>.</p><p id="p-0035" num="0034">The in-vehicle device <b>200</b> is a device (for example, a car navigation device) that provides information to passengers of the vehicle. The in-vehicle device <b>200</b> is also called a car navigation device, an infotainment device or a head unit. The in-vehicle device <b>200</b> enables navigation and entertainment to be provided to passengers of the vehicle. The in-vehicle device <b>200</b> may download traffic information, road map data, music, a moving image, and the like, via the DCM <b>100</b>.</p><p id="p-0036" num="0035">The server apparatus <b>20</b> is an apparatus that provides information to the vehicle <b>10</b>. In the present embodiment, the server apparatus <b>20</b> provides to the vehicle <b>10</b>, speech data to be utilized when the DCM <b>100</b> gives a speech guidance. Note that the server apparatus <b>20</b> may also serve as an apparatus that provides other information (such as, for example, traffic information and information related to infotainment) to the vehicle <b>10</b>.</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a view for explaining components of the vehicle <b>10</b> according to the present embodiment. The vehicle <b>10</b> according to the present embodiment includes the DCM <b>100</b>, the in-vehicle device <b>200</b> and a plurality of ECUs <b>300</b>A, <b>300</b>B, . . . (hereinafter, collectively referred to as an ECU <b>300</b>).</p><p id="p-0038" num="0037">The ECU <b>300</b> may include a plurality of ECUs that control different vehicle components. Examples of the plurality of ECUs can include, for example, a body ECU, an engine ECU, a hybrid ECU, a power train ECU, and the like. Further, the ECU <b>300</b> may be divided on a function basis. For example, the ECU <b>300</b> may be divided into an ECU that executes a security function, an ECU that executes an autonomous parking function, and an ECU that executes a remote control function.</p><p id="p-0039" num="0038">The DCM <b>100</b> includes an antenna <b>110</b>, a communication module <b>120</b>, a GPS antenna <b>130</b>, a GPS module <b>140</b>, a controller <b>101</b>, a storage <b>102</b>, a communication unit <b>103</b> and an input/output unit <b>104</b>.</p><p id="p-0040" num="0039">The antenna <b>110</b> is an antenna element that inputs/outputs a wireless signal. In the present embodiment, the antenna <b>110</b> complies with mobile communication (for example, mobile communication such as 3G, LTE and 5G). Note that the antenna <b>110</b> may include a plurality of physical antennas. For example, in a case where mobile communication utilizing a high-frequency radio wave such as a microwave and a millimeter wave is performed, a plurality of antennas may be arranged in a dispersed manner to achieve stable communication.</p><p id="p-0041" num="0040">The communication module <b>120</b> is a communication module for performing mobile communication.</p><p id="p-0042" num="0041">The GPS antenna <b>130</b> is an antenna that receives a positioning signal transmitted from a navigation satellite (also referred to as a GNSS satellite).</p><p id="p-0043" num="0042">The GPS module <b>140</b> is a module that calculates position information on the basis of a signal received by the GPS antenna <b>130</b>.</p><p id="p-0044" num="0043">The controller <b>101</b> is an arithmetic unit that implements various kinds of functions of the DCM <b>100</b> by executing a predetermined program. The controller <b>101</b> may be, for example, implemented by a CPU, or the like.</p><p id="p-0045" num="0044">The controller <b>101</b> executes functions of mediating communication to be performed between the external network and components (vehicle components) provided at the vehicle <b>10</b>. For example, in a case where a certain vehicle component requires to communicate with the external network, the controller <b>101</b> executes a function of relaying data transmitted from the vehicle component to the external network. Further, the controller <b>101</b> executes a function of receiving data transmitted from the external network and transferring the data to an appropriate vehicle component.</p><p id="p-0046" num="0045">Still further, the controller <b>101</b> can execute functions specific to the own apparatus. For example, the controller <b>101</b> is configured to be able to execute a monitoring function and a call function of a security system and can make a security notification, an emergency notification, or the like, on the basis of a trigger occurring inside the vehicle.</p><p id="p-0047" num="0046">The storage <b>102</b> is a memory device including a main memory and an auxiliary memory. In the auxiliary memory, an operating system (OS), various kinds of programs, various kinds of tables, and the like, are stored, and each function that matches a predetermined purpose as will be described later can be achieved by the programs stored therein being loaded to the main memory and executed.</p><p id="p-0048" num="0047">The communication unit <b>103</b> is an interface unit for connecting the DCM <b>100</b> to an in-vehicle network. In the present embodiment, a plurality of vehicle components including the in-vehicle device <b>200</b> and the ECU <b>300</b> are connected to each other via a bus <b>400</b> of the in-vehicle network. Examples of standards of the in-vehicle network can include, for example, a controller area network (CAN). Note that in a case where the in-vehicle network utilizes a plurality of standards, the communication unit <b>103</b> may include a plurality of interface devices in accordance with standards of communication destinations. Examples of the communication standards can also include, for example, Ethernet (registered trademark) as well as the CAN.</p><p id="p-0049" num="0048">Functions to be executed by the controller <b>101</b> will be described next. <figref idref="DRAWINGS">FIG. <b>3</b></figref> is a schematic view for explaining functional modules of the controller <b>101</b> and data stored in the storage <b>102</b>. The functional modules of the controller <b>101</b> can be implemented by the controller <b>101</b> executing programs stored in a storage unit such as a ROM.</p><p id="p-0050" num="0049">A data relay unit <b>1011</b> relays data to be transmitted/received between vehicle components. For example, the data relay unit <b>1011</b> performs processing of receiving a message sent by a first apparatus connected to the in-vehicle network and transferring the message to a second apparatus connected to the in-vehicle network as necessary. The first apparatus and the second apparatus may be ECUs <b>300</b> or may be other vehicle components.</p><p id="p-0051" num="0050">Further, in a case where the data relay unit <b>1011</b> receives a message addressed to the external network from a vehicle component, the data relay unit <b>1011</b> relays the message to the external network. Still further, the data relay unit <b>1011</b> receives data transmitted from the external network and transfers the data to an appropriate vehicle component.</p><p id="p-0052" num="0051">An emergency notification unit <b>1012</b> makes an emergency notification to an operator outside the vehicle in a case where an abnormal situation occurs in the vehicle <b>10</b>. Examples of the abnormal situation can include occurrence of a traffic accident and a vehicle failure. The emergency notification unit <b>1012</b>, for example, starts connection to the operator so that passengers of the vehicle can talk with the operator in a case where a predetermined trigger such as depression of a call button provided inside the vehicle and expansion of an air bag occurs. Note that upon emergency notification, the emergency notification unit <b>1012</b> may transmit the position information on the vehicle to the operator. In this case, the emergency notification unit <b>1012</b> may acquire the position information from the GPS module <b>140</b>. The emergency notification unit <b>1012</b> can output a speech guidance by utilizing speech data which will be described later.</p><p id="p-0053" num="0052">A security management unit <b>1013</b> performs security monitoring processing. The security management unit <b>1013</b>, for example, detects that the vehicle is unlocked through a procedure that is not a normal procedure, on the basis of data received from the ECU <b>300</b> that controls an electronic lock of the vehicle and transmits a security notification to a predetermined apparatus. Note that the security notification may include the position information on the vehicle. In this case, the security management unit <b>1013</b> may acquire the position information from the GPS module <b>140</b>. In a case where the security management unit <b>1013</b> determines that a problem occurs in security of the own vehicle, the security management unit <b>1013</b> may acquire the position information and may periodically transmit the acquired position information to an external apparatus designated in advance. The security management unit <b>1013</b> can also output a speech guidance in a similar manner to the emergency notification unit <b>1012</b>.</p><p id="p-0054" num="0053">A language management unit <b>1014</b> manages speech data to be utilized by the DCM <b>100</b>. Each functional module provided at the DCM <b>100</b> can give a speech guidance using the speech data stored in the storage <b>102</b>.</p><p id="p-0055" num="0054">Meanwhile, in a case where the vehicle <b>10</b> is sold (resold) across countries and regions, a case occurs where the language to be utilized by the user does not match the language of a speech to be provided by the DCM <b>100</b>. Thus, in a case where the language to be utilized by the user differs from the language of a speech to be provided by the DCM <b>100</b>, the language management unit <b>1014</b> acquires speech data corresponding to an appropriate language from the server apparatus <b>20</b> and resets the language of the speech guidance. This enables an appropriate speech guidance to be provided even in a case where the vehicle <b>10</b> moves to a country or a region that are not planned at the beginning.</p><p id="p-0056" num="0055">The storage <b>102</b> stores a data set <b>102</b>A.</p><p id="p-0057" num="0056">The data set <b>102</b>A is an aggregate of speech data to be utilized when the DCM <b>100</b> gives a speech guidance. <figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates an example of the data set <b>102</b>A. The data set <b>102</b>A includes a plurality of records including a language ID, a speech ID and binary data. The language ID is an identifier of a language (for example, Japanese). The storage <b>102</b> stores only the data set <b>102</b>A corresponding to a single language. The speech ID is an identifier allocated to each speech. The binary data is a body of the speech data. As illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, a plurality of pieces of speech data corresponding to a plurality of guidances to be provided by the DCM <b>100</b> are stored in the data set <b>102</b>A.</p><p id="p-0058" num="0057">The communication unit <b>103</b> is an interface unit for connecting the DCM <b>100</b> to the in-vehicle network. In the present embodiment, a plurality of vehicle components including the ECU <b>200</b> are connected to each other via a bus <b>400</b> of the in-vehicle network. Examples of standards of the in-vehicle network can include, for example, a controller area network (CAN). Note that in a case where the in-vehicle network utilizes a plurality of standards, the communication unit <b>103</b> may include a plurality of interface devices in accordance with standards of communication destinations. Examples of the standards can also include, for example, Ethernet (registered trademark) as well as the CAN.</p><p id="p-0059" num="0058">The input/output unit <b>104</b> is a unit that inputs/outputs information. More specifically, the input/output unit <b>104</b> includes a help button to be depressed in emergency circumstances, a microphone, a speaker, and the like. In the present embodiment, the input/output unit <b>104</b> does not have a screen.</p><p id="p-0060" num="0059">Note that the DCM <b>100</b> may be able to operate independently of other components provided at the vehicle <b>10</b>. For example, an auxiliary battery may be incorporated into the DCM <b>100</b>, and the DCM <b>100</b> may be able to independently operate without an external power supply. Such a configuration enables an emergency notification, or the like, to be made even in a case where an operation failure (such as, for example, a failure in power feeding) occurs in other components of the vehicle <b>10</b> due to a traffic accident, or the like.</p><p id="p-0061" num="0060">Returning to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the in-vehicle device <b>200</b> will be described.</p><p id="p-0062" num="0061">The in-vehicle device <b>200</b> is a device that provides information to passengers of the vehicle <b>10</b> and is also called a car navigation system, an infotainment system or a head unit. The in-vehicle device <b>200</b> can provide navigation or entertainment to passengers of the vehicle. Further, the in-vehicle device <b>200</b> may have a function of downloading traffic information, road map data, music, a moving image, and the like, by communication with an external network of the vehicle <b>10</b>. Still further, the in-vehicle device <b>200</b> may be a device that coordinates with a smartphone, or the like.</p><p id="p-0063" num="0062">Further, the in-vehicle device <b>200</b> also functions as a front end of the DCM <b>100</b>. For example, when the DCM <b>100</b> executes predetermined processing (for example, emergency notification), the in-vehicle device <b>200</b> inputs/outputs information related to the processing (for example, displays a calling status of the operator). Further, the in-vehicle device <b>200</b> acquires designation of the language, or the like, when the DCM <b>100</b> changes the language of the speech guidance.</p><p id="p-0064" num="0063">The in-vehicle device <b>200</b> can be constituted with a computer. In other words, the in-vehicle device <b>200</b> can be constituted as a computer including a processor such as a CPU and a GPU, a main memory such as a RAM and a ROM, and an auxiliary memory such as an EPROM, a hard disk drive and a removable medium. An operating system (OS), various kinds of programs, various kinds of tables, and the like, are stored in the auxiliary memory, and each function that matches a predetermined purpose as will be described later can be implemented by the programs stored in the auxiliary memory being executed. However, some or all of the functions may be implemented by a hardware circuit such as an ASIC and an FPGA.</p><p id="p-0065" num="0064">The in-vehicle device <b>200</b> includes a controller <b>201</b>, a storage <b>202</b>, a communication unit <b>203</b>, and an input/output unit <b>204</b>.</p><p id="p-0066" num="0065">The controller <b>201</b> is a unit configured to manage control of the in-vehicle device <b>200</b>. The controller <b>201</b> is constituted with, for example, information processing units such as a central processing unit (CPU) and a graphics processing unit (GPU).</p><p id="p-0067" num="0066">The controller <b>201</b> provides information to passengers of the vehicle. Examples of the information to be provided include, for example, traffic information, navigation information, music, a video, radio broadcasting, digital TV broadcasting, and the like. The controller <b>201</b> outputs information via the input/output unit <b>204</b>.</p><p id="p-0068" num="0067">The storage <b>202</b> is a unit configured to store information and is constituted with storage media such as a RAM, a magnetic disk, a flash memory, and the like. In the storage <b>202</b>, various kinds of programs to be executed at the controller <b>201</b>, data to be utilized by the programs, and the like, are stored.</p><p id="p-0069" num="0068">The communication unit <b>203</b> is a communication interface that connects the in-vehicle device <b>200</b> to the bus <b>400</b> of the in-vehicle network.</p><p id="p-0070" num="0069">The input/output unit <b>204</b> is a unit configured to accept input operation performed by the user and present information to the user. More specifically, the input/output unit <b>204</b> is constituted with a touch panel and a control unit thereof, and a liquid crystal display and a control unit thereof. In the present embodiment, the touch panel and the liquid crystal display are constituted as one touch panel display. Further, the input/output unit <b>204</b> may include a speaker, and the like, for outputting a speech.</p><p id="p-0071" num="0070">The ECU <b>300</b> will be described next.</p><p id="p-0072" num="0071">The ECU <b>300</b> is an electronic control unit that controls components provided at the vehicle <b>10</b>. The vehicle <b>10</b> may include a plurality of ECUs <b>300</b>. The plurality of ECUs <b>300</b>, for example, control components of different systems such as an engine system, an electronic equipment system and a power train system. The ECU <b>300</b> has a function of generating a specified message and periodically transmitting/receiving the message via the in-vehicle network.</p><p id="p-0073" num="0072">Further, the ECU <b>300</b> can provide a predetermined service by communicating with the external network via the DCM <b>100</b>. Examples of the predetermined service can include, for example, a remote service (for example, a remote air conditioning service), a security monitoring service, a service of coordinating with a smart home, an autonomous parking service (a service of autonomously traveling between a parking slot and an entrance of a building), and the like.</p><p id="p-0074" num="0073">The ECU <b>300</b> can be constituted as a computer including a processor such as a CPU and a GPU, a main memory such as a RAM and a ROM, and an auxiliary memory such as an EPROM, a disk drive and a removable medium in a similar manner to the DCM <b>100</b>.</p><p id="p-0075" num="0074">The network <b>400</b> is a communication bus that constitutes the in-vehicle network. Note that while one bus is illustrated in the present example, the vehicle <b>10</b> may include two or more communication buses. A plurality of communication buses may be connected to each other by the DCM <b>100</b> or a gateway that puts the plurality of communication buses together.</p><p id="p-0076" num="0075">The server apparatus <b>20</b> will be described next. <figref idref="DRAWINGS">FIG. <b>5</b></figref> is a schematic view of the server apparatus <b>20</b> in the first embodiment.</p><p id="p-0077" num="0076">In the present embodiment, the server apparatus <b>20</b> provides a set of speech data corresponding to a predetermined language in response to a request from the DCM <b>100</b> (language management unit <b>1014</b>). The server apparatus <b>20</b> may also serve as an apparatus that provides other information (such as, for example, traffic information and information related to navigation) to the DCM <b>100</b> and the in-vehicle device <b>200</b>.</p><p id="p-0078" num="0077">The server apparatus <b>20</b> can be constituted with a computer. In other words, the server apparatus <b>20</b> can be constituted as a computer including a processor such as a CPU and a GPU, a main memory such as a RAM and a ROM, and an auxiliary memory such as an EPROM, a hard disk drive and a removable medium.</p><p id="p-0079" num="0078">The server apparatus <b>20</b> includes a controller <b>21</b>, a storage <b>22</b> and a communication unit <b>23</b>.</p><p id="p-0080" num="0079">The controller <b>21</b> is an arithmetic device that manages control to be performed by the server apparatus <b>20</b>. The controller <b>21</b> can be implemented by an arithmetic processing device such as a CPU.</p><p id="p-0081" num="0080">The controller <b>21</b> includes a data provision unit <b>211</b> as a functional module. The functional module may be implemented by a stored program being executed by the CPU.</p><p id="p-0082" num="0081">The data provision unit <b>211</b> acquires a set of speech data corresponding to a predetermined language from a speech database <b>22</b>A which will be described later in response to a request from the DCM <b>100</b> (language management unit <b>1014</b>) and provides the set of speech data to the DCM <b>100</b>.</p><p id="p-0083" num="0082">The storage <b>22</b> includes a main memory and an auxiliary memory. The main memory is a memory in which programs to be executed by the controller <b>21</b> and data to be utilized by the control program are expanded. The auxiliary memory is a device in which programs to be executed at the controller <b>21</b> and data to be utilized by the control program are stored.</p><p id="p-0084" num="0083">The speech database <b>22</b>A is stored in the storage <b>22</b>. The speech database <b>22</b>A is a database that manages speech data to be utilized by the DCM <b>100</b> for each of a plurality of languages. <figref idref="DRAWINGS">FIG. <b>6</b></figref> is a view for explaining a configuration of the speech database <b>22</b>A. As illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, a plurality of pieces of speech data corresponding to a plurality of language IDs (L001, L002, . . . ) are stored in the speech database <b>22</b>A. The data provision unit <b>211</b> acquires a set of speech data corresponding to the language designated by the DCM <b>100</b> from the speech database <b>22</b>A and transmits the set of speech data to the DCM <b>100</b>.</p><p id="p-0085" num="0084">The communication unit <b>23</b> is a communication interface for connecting the server apparatus <b>20</b> to a network. The communication unit <b>23</b> includes, for example, a network interface board and a wireless communication interface for wireless communication.</p><p id="p-0086" num="0085">Processing of the DCM <b>100</b> changing the language of a speech guidance will be described next. <figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart of processing to be executed by components included in a vehicle system according to the present embodiment. The illustrated processing is started in a case where the user of the vehicle performs operation of changing the language of the speech guidance.</p><p id="p-0087" num="0086">First, in step S<b>11</b>, the in-vehicle device <b>200</b> accepts operation of changing language setting. The in-vehicle device <b>200</b>, for example, accepts operation of changing language setting via the screen as illustrated in <figref idref="DRAWINGS">FIG. <b>8</b></figref>. Note that while the illustrated screen is an example of a case where the in-vehicle device <b>200</b> functions as a front end of the DCM <b>100</b>, the in-vehicle device <b>200</b> may be a device that is independent of the DCM <b>100</b>, such as a car navigation device. In this case, language setting of the DCM <b>100</b> may be started by being triggered by change of language setting at the navigation device. In this case, the language of the speech guidance to be utilized by the DCM <b>100</b> is the same as the language set at the navigation device.</p><p id="p-0088" num="0087">Note that the present step may be executed at a timing of initial setting of the vehicle <b>10</b> (setting to be performed by the user after purchase of the vehicle).</p><p id="p-0089" num="0088">Information regarding the language after change is transmitted to the DCM <b>100</b>.</p><p id="p-0090" num="0089">In step S<b>12</b>, the DCM <b>100</b> (language management unit <b>1014</b>) acquires the language that is currently being used at the own apparatus.</p><p id="p-0091" num="0090">Then, in step S<b>13</b>, it is determined whether or not the language after change is the same as the language being used. In a case where a positive determination result is obtained in the present step, the processing is finished. In a case where a negative determination result is obtained in the present step, the processing transitions to step S<b>14</b>.</p><p id="p-0092" num="0091">In step S<b>14</b>, the language management unit <b>1014</b> generates an acquisition request of speech data and transmits the acquisition request to the server apparatus <b>20</b> (data provision unit <b>211</b>). The request includes information for identifying the language after change.</p><p id="p-0093" num="0092">Then, in step S<b>15</b>, the data provision unit <b>211</b> acquires a set of speech data corresponding to the requested language from the speech database <b>22</b>A stored in the storage <b>22</b> and transmits the acquired speech data to the DCM <b>100</b>.</p><p id="p-0094" num="0093">In step S<b>16</b>, the language management unit <b>1014</b> updates speech data included in the data set <b>102</b>A with the acquired speech data. As a result of this, the language of the speech guidance is changed.</p><p id="p-0095" num="0094">As described above, the DCM <b>100</b> according to the first embodiment acquires speech data for giving a guidance from the server apparatus on the basis of the request from the user. This enables the language of the speech guidance to be switched to an arbitrary language.</p><p id="p-0096" num="0095">In the present embodiment, instead of storing speech data corresponding to all languages in the DCM <b>100</b>, only in a case where the language of the speech guidance differs from the language to be utilized by the user, necessary speech data is externally acquired. This can reduce capacity of the memory provided at the DCM <b>100</b>, so that it is possible to contribute to reduction in manufacturing cost.</p><heading id="h-0009" level="1">Second Embodiment</heading><p id="p-0097" num="0096">In the first embodiment, the language after switching is designated by the user. In contrast, a second embodiment is an embodiment in which the language being utilized by the user is automatically determined and the language after switching is determined on the basis of a result of the determination.</p><p id="p-0098" num="0097">In the second embodiment, the language management unit <b>1014</b> actively determines that the language being utilized by the user differs from the language set at the DCM <b>100</b> and suggests the user change of the language of the speech guidance.</p><p id="p-0099" num="0098"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a flowchart of processing to be executed by components included in a vehicle system according to the second embodiment. Step similar to the step in the first embodiment is indicated with a dotted line, and description thereof will be omitted.</p><p id="p-0100" num="0099">First, in step S<b>11</b>A, the DCM <b>100</b> (language management unit <b>1014</b>) acquires a speech inside the vehicle <b>10</b>. The speech inside the vehicle can be acquired using a microphone, or the like, included in the input/output unit <b>104</b>.</p><p id="p-0101" num="0100">Then, in step S<b>12</b>A, the language management unit <b>1014</b> analyzes the acquired speech to determine the language. For example, the language management unit <b>1014</b> determines that a conversation is held in English inside the vehicle. The language obtained as a result of the analysis becomes a candidate for the language after change.</p><p id="p-0102" num="0101">In a case where the candidate language differs from the language of the speech guidance of the DCM <b>100</b>, the processing transitions to step S<b>13</b>A.</p><p id="p-0103" num="0102">In step S<b>13</b>A, it is determined whether or not to execute switching of the language. In the present step, for example, switching of the language is suggested to the user via the input/output unit <b>104</b> or the in-vehicle device <b>200</b>. For example, the screen as illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref> is output, and a response from the user is acquired. In a case where permission of the user can be obtained as a result of this, the processing transitions to step S<b>14</b>. In a case where permission cannot be obtained, the processing is finished.</p><p id="p-0104" num="0103">Processing in step S<b>14</b> and subsequent step is similar to that in the first embodiment.</p><p id="p-0105" num="0104">As described above, in the second embodiment, change of the language of the speech guidance at the DCM <b>100</b> is suggested on the basis of the language of the speech detected inside the vehicle. This enables change of the language of the speech guidance without the user actively performing operation.</p><p id="p-0106" num="0105">Note that the language detected from the speech inside the vehicle is not necessarily a native language of passengers of the vehicle <b>10</b>. Thus, in a case where the proposal is not accepted in step S<b>13</b>A, the subsequent proposal (or a proposal to switch to the same language) may be stopped.</p><p id="p-0107" num="0106">Further, the processing illustrated in <figref idref="DRAWINGS">FIG. <b>9</b></figref> may be performed only upon initial setting of the vehicle <b>10</b>. For example, the DCM <b>100</b> may encourage the user to make an utterance in a language that the user desires to set and may determine the language after change on the basis of the utterance.</p><heading id="h-0010" level="1">Third Embodiment</heading><p id="p-0108" num="0107">In the first and the second embodiments, a single language is set as the language after change. However, there is a case where it is not preferable to utilize a single language depending on a region in which the vehicle <b>10</b> travels, races or nationality of passengers of the vehicle <b>10</b>. For example, such a case includes a case where a native language of the driver is different from a native language of a fellow passenger. In particular, in a case where the DCM <b>100</b> provides an emergency notification service, or the like, it is preferable to provide a speech guidance in a plurality of languages so that all the passengers can understand the guidance.</p><p id="p-0109" num="0108">A third embodiment is an embodiment in which the server apparatus <b>20</b> generates speech data for giving a guidance in a plurality of languages to address this.</p><p id="p-0110" num="0109">The third embodiment differs from the embodiments described above in that there are two or more types of languages to be determined by the language management unit <b>1014</b>.</p><p id="p-0111" num="0110">For example, in step S<b>11</b> in the first embodiment, two or more types of languages may be designated. Further, the speech acquired in step S<b>11</b>A in the second embodiment may include two or more types of languages.</p><p id="p-0112" num="0111">For example, the language management unit <b>1014</b> determines that &#x201c;the user designates two languages of Japanese and English as the languages after change&#x201d; or that &#x201c;Japanese and English are detected from the speech inside the vehicle&#x201d;.</p><p id="p-0113" num="0112">An acquisition request to be transmitted from the DCM <b>100</b> to the server apparatus <b>20</b> includes designation of two or more languages (for example, Japanese and English).</p><p id="p-0114" num="0113">Further, in the present embodiment, in step S<b>15</b>, the server apparatus <b>20</b> (data provision unit <b>211</b>) generates speech data that gives a speech guidance in the designated two or more languages.</p><p id="p-0115" num="0114">In step S<b>15</b>, the data provision unit <b>211</b> generates a set of speech data that gives a guidance in a plurality of languages by combining speech data included in the speech database <b>22</b>A. For example, Japanese speech data indicated with reference numeral <b>601</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref> and English speech data indicated with reference numeral <b>602</b> are combined to generate speech data that gives a guidance both in Japanese and English as indicated with reference numeral <b>1101</b> in <figref idref="DRAWINGS">FIG. <b>11</b></figref>. This processing is executed for each of a plurality of speech IDs. The generated speech data is transmitted to the DCM <b>100</b>.</p><p id="p-0116" num="0115">The DCM <b>100</b> updates the data set <b>102</b>A stored in the storage <b>102</b> with such speech data. By this configuration, it is possible to constitute the DCM <b>100</b> that gives a speech guidance in a plurality of languages.</p><p id="p-0117" num="0116">Note that while in the present embodiment, an example of two types of languages has been described, there may be three or more types of languages. Also in this case, target speech data can be obtained by the server apparatus <b>20</b> combining speech data corresponding to a plurality of languages.</p><heading id="h-0011" level="1">MODIFIED EXAMPLES</heading><p id="p-0118" num="0117">The above-described embodiments are merely examples, and the present disclosure can be changed and implemented as appropriate within a scope not deviating from the gist of the present disclosure.</p><p id="p-0119" num="0118">For example, the processing and the units described in the present disclosure can be freely combined and implemented unless technical inconsistency occurs.</p><p id="p-0120" num="0119">Further, while in the description of the embodiments, processing of changing language setting is started on the basis of the request from the user or the utterance of the user, the processing may be triggered by other events. For example, in a case where the DCM <b>100</b> gives a speech guidance in a first language, and there is no response to this from the user, processing of proposing to change the language setting to the user may be executed. Alternatively, processing of changing language setting to the default (such as English) may be executed.</p><p id="p-0121" num="0120">In addition, the processing described as being performed by one device may be shared and executed by a plurality of devices. Alternatively, the processing described as being performed by different devices may be executed by one device. In a computer system, what hardware configuration (server configuration) realizes each function can be flexibly changed.</p><p id="p-0122" num="0121">The present disclosure can also be realized by supplying a computer program including the functions described in the above embodiments to a computer and causing one or more processors included in the computer to read and execute the program. Such a computer program may be provided to the computer by a non-transitory computer-readable storage medium connectable to a system bus of the computer, or may be provided to the computer via a network. Examples of non-transitory computer readable storage media include: any type of disk such as a magnetic disk (floppy (registered trademark) disk, hard disk drive (HDD), etc.), an optical disk (CD-ROM, DVD disk, Blu-ray disk, etc.); and any type of medium suitable for storing electronic instructions, such as read-only memory (ROM), random access memory (RAM), EPROM, EEPROM, magnetic cards, flash memory, and optical cards.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An information processing apparatus comprising a controller configured to:<claim-text>give a speech guidance to a user using first speech data corresponding to a first language;</claim-text><claim-text>determine that the user utilizes a language different from the first language; and</claim-text><claim-text>acquire second speech data corresponding to the language to be utilized by the user on a basis of a result of the determination.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The information processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the information processing apparatus is mounted on a vehicle,</claim-text><claim-text>the information processing apparatus further comprising a communication module, and</claim-text><claim-text>the controller acquires the second speech data from an external apparatus via the communication module.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The information processing apparatus according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein<claim-text>in a case where operation of changing language setting is performed on a second information processing apparatus mounted on the vehicle, the controller acquires the second speech data corresponding to the language after change.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The information processing apparatus according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein<claim-text>the second information processing apparatus is a car navigation apparatus or a head unit apparatus including a display device.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The information processing apparatus according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein<claim-text>the controller further acquires a speech inside the vehicle and determines the language to be utilized by the user on a basis of the acquired speech.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The information processing apparatus according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein<claim-text>in a case where a language other than the first language is detected from the acquired speech, the controller suggests performing processing of switching the language of the speech guidance.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The information processing apparatus according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein<claim-text>in a case where there is no response to the speech guidance in the first language from the user, the controller suggests performing processing of switching the language of the speech guidance.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The information processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the controller switches speech data to be used for the speech guidance from the first speech data to the second speech data after acquiring the second speech data.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The information processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the user utilizes two or more languages, and</claim-text><claim-text>the controller acquires the second speech data for giving the speech guidance in the two or more languages.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The information processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the information processing apparatus is mounted on a vehicle and is capable of providing a connected service independently of other components provided at the vehicle.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. An information processing system comprising a first apparatus that gives a speech guidance to a user and a second apparatus that provides speech data to be used for the speech guidance,<claim-text>wherein the first apparatus comprises a controller configured to:</claim-text><claim-text>give a speech guidance to the user using first speech data corresponding to a first language;</claim-text><claim-text>determine that the user utilizes a language different from the first language; and</claim-text><claim-text>acquire second speech data corresponding to the language to be utilized by the user from the second apparatus on a basis of a result of the determination.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The information processing system according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein<claim-text>the first apparatus is an apparatus mounted on a vehicle, and</claim-text><claim-text>the second apparatus is a server apparatus that manages the vehicle.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The information processing system according to <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein<claim-text>in a case where operation of changing language setting is performed on a third apparatus mounted on the vehicle, the controller acquires the second speech data corresponding to the language after change.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The information processing system according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein<claim-text>the third apparatus is a car navigation apparatus or a head unit apparatus including a display device.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The information processing system according to <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein<claim-text>the controller further acquires a speech inside the vehicle and determines the language to be utilized by the user on a basis of the acquired speech.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The information processing system according to <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein<claim-text>in a case where a language other than the first language is detected from the acquired speech, the controller suggests performing processing of switching a language of the speech guidance.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The information processing system according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein<claim-text>the controller switches speech data to be used for the speech guidance from the first speech data to the second speech data after acquiring the second speech data.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The information processing system according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein<claim-text>the user utilizes two or more languages, and</claim-text><claim-text>the second apparatus generates the second speech data for giving the speech guidance in the two or more languages.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The information processing system according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein<claim-text>the first apparatus is an apparatus that is mounted on a vehicle and is capable of providing a connected service independently of other components provided at the vehicle.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. An information processing method comprising:<claim-text>a step of giving a speech guidance to a user using first speech data corresponding to a first language;</claim-text><claim-text>a step of determining that the user utilizes a language different from the first language; and</claim-text><claim-text>a step of acquiring second speech data corresponding to the language to be utilized by the user on a basis of a result of the determination.</claim-text></claim-text></claim></claims></us-patent-application>