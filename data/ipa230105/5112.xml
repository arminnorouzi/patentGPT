<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005113A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005113</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17741098</doc-number><date>20220510</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>CN</country><doc-number>2021107547211</doc-number><date>20210705</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>5</main-group><subgroup>50</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>73</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>174</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>5</main-group><subgroup>50</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>0012</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>74</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>174</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30096</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20221</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">METHOD AND SYSTEM FOR MEDICAL IMAGE DATA ENHANCEMENT</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="obligated-assignee"><addressbook><orgname>SHENZHEN KEYA MEDICAL TECHNOLOGY CORPORATION</orgname><address><city>Shenzhen</city><country>CN</country></address></addressbook><residence><country>CN</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>LI</last-name><first-name>Guang</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>LI</last-name><first-name>Jinchen</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>SUN</last-name><first-name>Chengwei</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>CHEN</last-name><first-name>Cong</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>CAO</last-name><first-name>Kunlin</first-name><address><city>Kenmore</city><state>WA</state><country>US</country></address></addressbook></inventor><inventor sequence="05" designation="us-only"><addressbook><last-name>SONG</last-name><first-name>Qi</first-name><address><city>Seattle</city><state>WA</state><country>US</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>SHENZHEN KEYA MEDICAL TECHNOLOGY CORPORATION</orgname><role>03</role><address><city>Shenzhen</city><country>CN</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A method for medical image data enhancement is provided. The method includes: receiving a medical image sample set related to an object to be detected; based on an attribute of the object lacking in the medical image sample set, selecting a first medical image and a second medical image from the medical image sample set, where the first medical image contains the object lacking the attribute, and the second medical image does not contain the object lacking the attribute; determining a first area image block containing the lacking attribute; determining a second area image block not containing the lacking attribute; generating a composite area image block by fusing the first area image block and the second area image block based on a mask including an object part and a peripheral part around the object part; embedding the composite area image block back into the second medical image to obtain a third medical image; including the third medical image in the medical image sample set to obtain a data-enhanced medical image sample set.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="74.59mm" wi="158.75mm" file="US20230005113A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="246.55mm" wi="160.36mm" file="US20230005113A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="167.72mm" wi="161.54mm" orientation="landscape" file="US20230005113A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="236.05mm" wi="171.53mm" orientation="landscape" file="US20230005113A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="260.10mm" wi="166.03mm" orientation="landscape" file="US20230005113A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="239.27mm" wi="162.39mm" file="US20230005113A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">The application is based upon and claims the benefit of priority to Chinese Patent Application No. 2021107547211, filed Jul. 5, 2021, the entire content of which is incorporated herein by reference.</p><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">The present disclosure relates to medical image processing, more specifically, to a method and system for medical image data enhancement for improving detection results.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">The automatic detection of lesions in a medical image can not only improve the reading efficiency of the doctors, but also provide quantitative information of lesions. At present, the lesion detection method based on deep learning has high robustness and accuracy, but lesions in the medical image are very widely distributed, and the training data cannot cover all the distribution of the lesions. Moreover, the imbalance between the proportions of positive and negative samples of a lesion type and anatomical position or the combination of the above factors will also have a great impact on the accuracy and generalization ability of deep learning. Therefore, data enhancement is the key to improve the lesion detection accuracy and model generalization ability.</p><p id="p-0005" num="0004">At present, the more popular methods for image data enhancement include such as an overall transformation of the whole image by means of the image rotation, translation, zooming and flipping etc., Mix-up and Copy-Move. However, image overall transformation cannot add positive samples at different anatomical structure positions. Coping-Moving is to simply copy a lesion area in a positive sample image containing a certain type of lesion and move it into other negative sample images, which are then added to the medical image sample set as new positive samples. This method will leave obvious traces of image composition on the enhanced image. Compared with a real medical image sample containing the lesion, there is a large gap in the authenticity of the sample. Even when it is used in machine learning, it may cause additional adverse effects. Therefore, there is currently no sufficiently effective method for data enhancement for the imbalance between positive and negative samples in terms of lesion type and anatomical position or the like in the medical image sample set.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0006" num="0005">In view of the shortcomings of the prior arts, the present disclosure is provided to solve the above problems in the prior art.</p><p id="p-0007" num="0006">There is a need for a method, a device for data enhancement for medical image detection, and a storage medium. The lack of the amount of samples of the object to be detected in terms of lesion type, position and size, etc., or the imbalance between positive and negative samples in the medical image sample set can be identified. And based on the identified lacking attribute of the object, a positive sample of the medical image that contains the object lacking the attribute and a negative sample of the medical image that does not contain the object lacking the attribute are selected in the medical image sample set respectively. And by using a masking based image block fusing algorithm, a composite area image block containing the object lacking the attribute after fusing process is embedded back into the negative sample of the medical image, which is added as a positive sample into the medical image sample set to achieve data enhancement for the medical image sample set. Therefore, the accuracy of detecting the object to be detected by using the medical image sample set after data enhancement, and the generalization ability of a deep learning model trained with this medical image sample set are improved.</p><p id="p-0008" num="0007">According to a first aspect of the present disclosure, a method for medical image data enhancement is provided. The method may include receiving a medical image sample set related to an object to be detected. The method may further include, based on an attribute of the object lacking in the medical image sample set, selecting a first medical image and a second medical image from the medical image sample set, where the first medical image contains the object lacking the attribute, and the second medical image does not contain the object lacking the attribute. The method may further include determining a first area image block encompassing an area containing the object lacking the attribute in the first medical image. The method may further include determining a second area image block encompassing an area not containing the object lacking the attribute in the second medical image. The method may further include generating a composite area image block by fusing the first area image block and the second area image block based on a masking including an object part and a peripheral part around the object part. The method may further include embedding the composite area image block back into the second medical image to obtain a third medical image. The method may further include including the third medical image in the medical image sample set to obtain a data-enhanced medical image sample set.</p><p id="p-0009" num="0008">According to a second aspect of the present disclosure, a device for medical image data enhancement is provided. The device may include a communication interface configured to receive a medical image sample set related to an object to be detected, and a processor. The processor is configured to, based on an attribute of the object lacking in the medical image sample set, select a first medical image and a second medical image from the medical image sample set, where the first medical image contains the object lacking the attribute, and the second medical image does not contain the object lacking the attribute. The processor is further configured to determine a first area image block encompassing an area containing the object lacking the attribute in the first medical image; and a determine a second area image block encompassing an area not containing the object lacking the attribute in the second medical image; and generate a composite area image block by fusing the first area image block and the second area image block based on a masking including an object part and a peripheral part around the object part. The processor is also configured to embed the composite area image block back into the second medical image to obtain a third medical image; and including the third medical image in the medical image sample set through the communication interface to obtain a data-enhanced medical image sample set.</p><p id="p-0010" num="0009">According to a third aspect of the present disclosure, a non-transitory computer readable storage medium having computer executable instructions stored thereon is provided. The computer executable instructions, when executed by a processor, implement a method for medical image data enhancement. The method may include receiving a medical image sample set related to an object to be detected. The method may further include, based on an attribute of the object lacking in the medical image sample set, selecting a first medical image and a second medical image from the medical image sample set, where the first medical image contains the object lacking the attribute, and the second medical image does not contain the object lacking the attribute. The method may further include determining a first area image block encompassing an area containing the object lacking the attribute in the first medical image. The method may further include determining a second area image block encompassing an area not containing the object lacking the attribute in the second medical image. The method may further include generating a composite area image block by fusing the first area image block and the second area image block based on a masking including an object part and a peripheral part around the object part. The method may further include embedding the composite area image block back into the second medical image to obtain a third medical image. The method may further include including the third medical image in the medical image sample set to obtain a data-enhanced medical image sample set.</p><p id="p-0011" num="0010">With the method, the device for medical image data enhancement, and the storage medium according to embodiments of the present disclosure, the imbalance of the attributes of the object to be detected in the medical image sample set can be identified. And based on the identified lacking attribute of the object, by using a masking based image block fusing algorithm, a composite medical image containing the object lacking the attribute is generated. The composite medical image generated by using the method described above is very close to the real medical image containing the object lacking the attribute, since the image fusing process was performed. The above-mentioned composite medical image, as a positive sample, is added into the medical image sample set, so that the medical image sample set with respect to the lacking attribute of the object can be comprehensively enhanced in terms of the amount of sample, the quality of sample, and adaptability of the sample to various scenarios, etc. Therefore, the accuracy of detecting the object to be detected by using the medical image sample set after data enhancement, and the generalization ability of a deep learning model trained with this medical image sample set are improved.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0012" num="0011">In the drawings, which are not necessarily drawn to scale, like numerals may describe similar components in different views. The drawings illustrate generally, by way of example instead of limitation, various embodiments, and together with the description and claims, serve to explain the disclosed embodiments. The same reference signs in all of the drawings are used to refer to the same or similar portions when appropriate. Such embodiments are demonstrative and not intended to be exhaustive or exclusive embodiments of the present device or method.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows a flow of a method for medical image data enhancement according to an embodiment of the present disclosure;</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates a calculation method for a first fusing coefficient and a second fusing coefficient according to an example of an embodiment of the present disclosure;</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>3</b></figref> shows a schematic diagram of an example of fusing image blocks based on a masking according to an embodiment of the present disclosure;</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows a comparison of the effects between a new nodule image generated by fusing the image blocks based on a masking according to an embodiment of the present disclosure and a new nodule image generated by a conventional manner of copy-move; and</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>5</b></figref> shows a schematic diagram of a device for medical image data enhancement according to an embodiment of the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0018" num="0017">In order to enable those skilled in the art to better understand the technical solutions of the present disclosure, the present disclosure will be described in detail below in conjunction with the drawings and specific implementations. The present disclosure will be further described below in conjunction with the drawings and specific embodiments, but not as a limitation to the present disclosure. The order of the individual steps described herein as examples should not be viewed as a limitation if there is no need for a front-back relationship with each other, to which those skilled in the art should know that the order can be adjusted, as long as the logic therebetween may not be disrupted so that the entire flow can be achieved.</p><p id="p-0019" num="0018">Furthermore, those skilled in the art should understand that the drawings provided herein are all for illustrative purposes and that the drawings are not necessarily drawn to scale.</p><p id="p-0020" num="0019">Unless definitely required by the context, similar words such as &#x201c;comprises&#x201d; or &#x201c;including&#x201d; throughout the specification and claims should be construed as meaning inclusive rather than exclusive or exhaustive, that is, be the meaning &#x201c;including but not limited to&#x201d;.</p><p id="p-0021" num="0020">In the description of the present disclosure, it is understood that the terms &#x201c;first&#x201d;, &#x201c;second&#x201d;, etc. are intended for descriptive purposes only and cannot be understood as indicating or suggesting relative importance. In addition, in the description of the present disclosure, unless otherwise stated, &#x201c;multiple&#x201d; means two or more.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows a flow of a method for medical image data enhancement according to an embodiment of the present disclosure. As shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the flow may begin at step S<b>101</b>. At step S<b>101</b>, a medical image sample set related to an object to be detected is obtained at first. The medical image in the sample set may be either a three-dimensional medical image or a two-dimensional medical image, and no restrictions are made here. In some embodiments, the object to be detected may be any lesion that is applicable to the medical image, such as including but not limited to nodule, tumor, cyst, and hemorrhage lesion, etc.</p><p id="p-0023" num="0022">At step S<b>102</b>, based on an attribute of the object lacking in the medical image sample set, a first medical image and a second medical image are selected from the medical image sample set, so that the first medical image contains the object lacking the attribute, while the second medical image does not contain the object lacking the attribute.</p><p id="p-0024" num="0023">In some embodiments, the first medical image and the second medical image may be randomly selected from the medical image sample set. Specifically, the first medical image containing the object lacking the attribute and the second medical image not containing the object lacking the attribute may be selected from all sample images in the medical image sample set according to a certain random algorithm for subsequent data enhancement operations.</p><p id="p-0025" num="0024">In some other embodiments, the first medical image and the second medical image may also be randomly selected from a subset of the medical image sample set. In some embodiments, for example, medical images that contain the lacking attribute of the object and are determined to have high image quality may be selected as a subset of the medical image samples. As another example, original, unprocessed images containing the lacking attribute of the object may also be selected as a subset of medical image samples, so as to suppress the cumulative deviation between the composite image and the real medical image that may be caused by multiple image fusing processes.</p><p id="p-0026" num="0025">In some embodiments, the attribute of the object may be any attribute with diagnostic significance in clinical application, such as at least one or a combination of a subtype, a position, and a size of the object. In some embodiments, when the object is a pulmonary lesion, the subtype of the object may include a solid lesion, a ground-glass lesion or a semi-solid lesion, etc., and the position of the object may include the inside of the lung lobes or the inside of the thorax, etc. In some embodiments, the size of the object may be divided into large size, medium size and small size, etc., according to a clinical definition of different types of lesion.</p><p id="p-0027" num="0026">In some embodiments, the attribute of the object lacking in the medical image sample set may be determined through a comparison with the actual distribution of the attributes of the object. For example, a lung solid lesion located in the thorax may be detected in clinical practice, while the medical image sample set does not have or only has a small number of images of the object containing such attribute, and it may be determined that the medical sample set lacks the attribute of being located in the thorax of the lung solid lesion object. In other embodiments, a determination of lack may also be made through a comparison with the proportions of the attributes of the object in clinical practice.</p><p id="p-0028" num="0027">In some other embodiments, the attribute of the object lacking in the medical image sample set may also be determined by the following machine learning method. That is, the medical image is detected by using a model trained based on the medical image sample set to obtain a detection distribution of the attributes of the object, and then the detection distribution of the attributes of the object is compared with the actual distribution of the attributes of the object. When the detection of a certain type of attribute of the object is not ideal, it usually implies a lack of medical image samples containing the related attribute of the object during model training. Or, even if the poor detection effect is not caused by the lack of related attribute of the object in the sample set, from the perspective of result orientation, by increasing the number of image samples of the corresponding attribute of the object, the training effect of the model may also be improved.</p><p id="p-0029" num="0028">Next, after determining the attribute of the object lacking in the medical image sample set, steps S<b>103</b> to S<b>107</b> may be performed for targeted data enhancement of the medical image sample set.</p><p id="p-0030" num="0029">At step S<b>103</b>, an area containing the object lacking the attribute is identified in the selected first medical image as a first area image block.</p><p id="p-0031" num="0030">At step S<b>104</b>, an area not containing the object lacking the attribute is identified in the second medical image as a second area image block.</p><p id="p-0032" num="0031">In some embodiments, the first area image block may be selected as an area that centers on the center of the object to be detected and covers most area of the object to be detected, such as a square or a circle, to facilitate the subsequent image fusing process.</p><p id="p-0033" num="0032">In some embodiments, when selecting the second area image block, the same size as the first area image block may be adopted to facilitate the subsequent image fusing process.</p><p id="p-0034" num="0033">Next, at step S<b>105</b>, the first area image block and the second area image block are fused based on a masking including an object part and a peripheral part around the object part so as to obtain a composite area image block.</p><p id="p-0035" num="0034">In some embodiments, a masking is defined to be used to perform a gradual fusing process of the first area image block and the second area image block, wherein the masking may include two parts, that is, an object part of the masking and a peripheral part around the object part.</p><p id="p-0036" num="0035">In some embodiments, the object part of the masking may be set according to the shape and range of the object to be detected in the first area image block. In some other embodiments, for ease of calculation, a regular shape, such as a circle, that centers on the center of the object to be detected in the first area image block and covers main features of the object to be detected may be set as the object part of the masking.</p><p id="p-0037" num="0036">In some embodiments, when setting the range of the peripheral part around the object part of the masking, the size thereof may be set to match the size of the first area image block. As an example, when the shape of the object part of the masking is the same as that of the first area image block, the boundary of the peripheral part around the object part may be set to overlap the boundary of the first area image block. In other embodiments, the peripheral part around the object part may be set as an area of which boundary shape is the same as that of the object part and the boundary has a constant distance from the boundary of the object part.</p><p id="p-0038" num="0037">In some embodiments, the shape and size of the object part and the peripheral part around the object part of the masking may be determined according to the specific object to be detected. In some other representations, the shape and size of the masking may be adjusted according to the detection effect of the object to be detected after data enhancement.</p><p id="p-0039" num="0038">In some embodiments, when fusing the first area image block and the second area image block based on a masking including the object part and the peripheral part around the object part, for each position in each part of the masking, the image information of the corresponding position in the first area image block and the second area image block is fused based on a first fusing coefficient and a second fusing coefficient respectively. Various applicable calculation methods may be adopted for the above first fusing coefficient and the second fusing coefficient, which will be described in detail below in combination with the illustrations in <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0040" num="0039">Next, at step S<b>106</b>, the composite area image block generated based on the masking, the first area image block and the second area image block are embedded back into the second medical image to obtain a third medical image.</p><p id="p-0041" num="0040">In some embodiments, multiple different composite area image blocks may be generated by using the same first area image block in the same first medical image and different second area image blocks in the same/different second medical images, and are then embedded back into the corresponding second medical image to obtain multiple third medical images.</p><p id="p-0042" num="0041">At step S<b>107</b>, the third medical image is added into the medical image sample set to obtain a data-enhanced medical image sample set.</p><p id="p-0043" num="0042">In some embodiments, the required multiple third medical images may be generated at a time in batch processing. After a complete round of training on the deep learning model with the medical image sample set, the multiple third medical images may be added into the medical image sample set in an offline manner, and the data-enhanced medical image sample set is used for the next round of training.</p><p id="p-0044" num="0043">In some other embodiments, it is also possible to use, in an online manner at any time, the generated third medical image having the lacking attribute of the object for the update of the medical image sample set that is under training.</p><p id="p-0045" num="0044">Through the above steps S<b>101</b> to S<b>107</b>, the amount and diversity of samples having the lacking attribute of the object in the medical image sample set are increased. Therefore, the balance of the medical image sample set in a specific attribute of the object is enhanced, and the enhanced medical image sample set has higher detection accuracy and adaptability to different scenarios. The trained deep learning model also has stronger generalization ability.</p><p id="p-0046" num="0045">In order to more clearly describe the specific algorithm of fusing the first area image block and the second area image block based on the masking, the calculation method of the first fusing coefficient and the second fusing coefficient will be further described in more detail in combination with <figref idref="DRAWINGS">FIG. <b>2</b></figref> below.</p><p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates the calculation method of the first fusing coefficient and the second fusing coefficient according to an example of an embodiment of the present disclosure. For ease of description, in the example of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, it is assumed that the selected first area image block and the second area image block have exactly the same shape and size, corresponding to an area <b>20</b> in <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0048" num="0047">In some embodiments, the first area image block contains an object to be detected, such as a nodule, tumor, cyst, and hemorrhage lesion. In <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the object part <b>201</b> of the masking covers the most area of the object to be detected. It should be noted that the object part <b>201</b> of the masking is not necessarily a circle as shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref> or other regular shape, and it may also be any other shape determined according to the actual situation of the object to be detected in other embodiments.</p><p id="p-0049" num="0048">In some embodiments, the peripheral part <b>202</b> around the object part of the masking may be defined based on a parameter d_Q. d_Q is the minimum distance from all points on the boundary of the peripheral part <b>202</b> around the object part of the masking to the boundary of the object part <b>201</b> of the masking. In some embodiments, the parameter d_Q may be defined as the distance from a point, among all points on the boundary of the area <b>20</b>, having the smallest distance to the boundary of the object part of the masking, namely point Q in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, to the boundary of the object part of the masking. For example, in the example of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, in a case where the area <b>20</b> is a square and the object part <b>201</b> of the masking is a circle, the peripheral part <b>202</b> around the object part of the masking is the circular area with a ring width of d_Q outside of the object part <b>201</b> of the masking. Similarly, in some other embodiments, the shape of the peripheral part <b>202</b> around the object part of the masking may be determined based on the shape and the parameter d_Q of the object part <b>201</b> of the masking, and therefore is not limited to a ring or other regular shapes. The area <b>203</b> in <figref idref="DRAWINGS">FIG. <b>2</b></figref> refers to other areas in the area <b>20</b> other than the object part <b>201</b> of the masking and the peripheral part <b>202</b> around the object part of the masking.</p><p id="p-0050" num="0049">Based on the above definition of each area, a description is given to a calculation method of a first fusing coefficient value S<sub>1</sub>(i,j) in the first fusing coefficient field <b>21</b> and a second fusing coefficient value S<sub>2</sub>(i,j) in a second fusing coefficient field <b>22</b> at each position (i,j).</p><p id="p-0051" num="0050">In the first fusing coefficient field <b>21</b> and the second fusing coefficient field <b>22</b> as shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the values of fusing coefficients for each position (i,j) are in a range of [0,1], wherein white represents a value of 1, black represents a value of 0, and other gray values represent values within (0,1). In some embodiments, S<sub>1</sub>(i,j) for each position(i,j) in the area <b>211</b> of the first fusing coefficient field <b>21</b> corresponding to the object part <b>201</b> of the masking may be set as 1, while S<sub>2</sub>(i,j) for each position(i,j) in the respective area <b>221</b> in the second fusing coefficient field <b>22</b> may be set as 0. With such settings, the area corresponding to the object part of the masking upon the image fusing may fully use the image value in the first area image block, that is, the image value of the object to be detected in the medical image indicated by the object part of the masking, regardless of the image value in the second area image block that does not contain the object to be detected.</p><p id="p-0052" num="0051">In some other embodiments, especially when the size of the masking is not exactly the same as that of the area <b>20</b>, for example, when the area <b>203</b> other than the object part <b>201</b> of the masking and the peripheral part <b>202</b> around the object part of the masking exists as shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, S<sub>1</sub>(i,j) for each position(i,j) in the area <b>213</b> outside the area corresponding to the masking in the first fusing coefficient field <b>21</b> may be set as 0. Correspondingly, S<sub>2</sub>(i,j) for each position(i,j) in the area <b>223</b> outside the area corresponding to the masking in the second fusing coefficient field <b>22</b> may be set as 1. With such settings, the area outside the masking upon the image fusing may fully use the original image values in the second area image block, regardless of the image values in the first area image block. In other words, the image fusing based on the first area image block and the second area image block is performed only in the area corresponding to the masking, and areas other than the masking retain the original image values of the second area image block.</p><p id="p-0053" num="0052">In some other embodiments, S<sub>1</sub>(i,j) and S<sub>2</sub>(i,j) for each position(i,j) in the peripheral part <b>202</b> around the object part of the masking may be determined by a distance d(i,j) to the boundary of the object part of the masking. The closer to the boundary of the object part of the masking, i.e., the lower d(i,j) is, the higher S<sub>1</sub>(i,j) is, and correspondingly, the lower S<sub>2</sub>(i,j) is. That is, the change of S<sub>1</sub>(i,j) and S<sub>2</sub>(i,j) shows opposite trend. In some embodiments, for example, in the case of normalization, for any position (i,j) in the peripheral part around the object part of the masking, the sum of S<sub>1</sub>(i,j) and S<sub>2</sub>(i,j) is 1. With such a setting, when data fusing is performed in the peripheral part <b>202</b> around the object part of the masking, the closer to the object part <b>201</b> of the masking, the greater the influence of the first area image block. Otherwise, the influence of the second area image block gradually increases with the increase of the distance to the object part <b>201</b> of the masking.</p><p id="p-0054" num="0053">In a case where S<sub>1</sub>(i,j), S<sub>2</sub>(i,j), d(i,j) and d_Q are defined as above, for example, S<sub>1</sub>(i,j) and S<sub>2</sub>(i,j) may have a relationship as described in Formula (1) below:</p><p id="p-0055" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <msub>       <mi>S</mi>       <mn>1</mn>      </msub>      <mo>(</mo>      <mrow>       <mi>i</mi>       <mo>,</mo>       <mi>j</mi>      </mrow>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mrow>      <mo>{</mo>      <mrow>       <mtable>        <mtr>         <mtd>          <mn>0</mn>         </mtd>         <mtd>          <mrow>           <mrow>            <mi>d</mi>            <mo>&#x2061;</mo>            <mo>(</mo>            <mrow>             <mi>i</mi>             <mo>,</mo>             <mi>j</mi>            </mrow>            <mo>)</mo>           </mrow>           <mo>&#x3e;</mo>           <mi>d_Q</mi>          </mrow>         </mtd>        </mtr>        <mtr>         <mtd>          <mfrac>           <mrow>            <mi>d_Q</mi>            <mo>-</mo>            <mrow>             <mi>d</mi>             <mo>&#x2061;</mo>             <mo>(</mo>             <mrow>              <mi>i</mi>              <mo>,</mo>              <mi>j</mi>             </mrow>             <mo>)</mo>            </mrow>           </mrow>           <mi>d_Q</mi>          </mfrac>         </mtd>         <mtd>          <mrow>           <mrow>            <mi>d</mi>            <mo>&#x2062;</mo>            <mrow>             <mo>(</mo>             <mrow>              <mi>i</mi>              <mo>,</mo>              <mi>j</mi>             </mrow>             <mo>)</mo>            </mrow>           </mrow>           <mo>&#x3c;</mo>           <mi>d_Q</mi>          </mrow>         </mtd>        </mtr>       </mtable>       <mo>,</mo>       <mrow>        <mrow>         <msub>          <mi>S</mi>          <mn>2</mn>         </msub>         <mo>(</mo>         <mrow>          <mi>i</mi>          <mo>,</mo>          <mi>j</mi>         </mrow>         <mo>)</mo>        </mrow>        <mo>=</mo>        <mrow>         <mn>1</mn>         <mo>-</mo>         <mrow>          <msub>           <mi>S</mi>           <mn>1</mn>          </msub>          <mo>(</mo>          <mrow>           <mi>i</mi>           <mo>,</mo>           <mi>j</mi>          </mrow>          <mo>)</mo>         </mrow>        </mrow>       </mrow>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mi>Formula</mi>     <mo>&#x2062;</mo>     <mtext>   </mtext>     <mrow>      <mo>(</mo>      <mn>1</mn>      <mo>)</mo>     </mrow>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0056" num="0054">In some embodiments, after S<sub>1</sub>(i,j) and S<sub>2</sub>(i,j) for each position(i,j) in the area <b>20</b> are determined, the image value I<sub>new</sub>(i,j) for each position(i,j) in the composite area image block may be calculated in accordance with any applicable rules by performing the image fusing based on a masking including an object part and a peripheral part around the object part. As an example, weight fusion of I<sub>1</sub>(i,j) and I<sub>2</sub>(i,j) may be made according to Formula (2):</p><p id="p-0057" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>I</i><sub>new</sub>(<i>i,j</i>)=<i>I</i><sub>1</sub>(<i>i,j</i>)*<i>S</i><sub>1</sub>(<i>i,j</i>)+<i>I</i><sub>2</sub>(<i>i,j</i>)*<i>S</i><sub>2</sub>(<i>i,j</i>)&#x2003;&#x2003;Formula (2)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0058" num="0055">wherein I<sub>new</sub>(i,j) is an image value for position (i,j) in the composite area image block, and I<sub>1</sub>(i,j) and I<sub>2</sub>(i,j) are respectively the image value for position (i,j) in the first area image block and the second area image block.</p><p id="p-0059" num="0056">The above Formula (1) and Formula (2) are only examples instead of limitations. In some other embodiments, other applicable calculation methods may also be used for S<sub>1</sub>(i,j) and S<sub>2</sub>(i,j). For example, when it may be determined by other means that the boundary of the object to be detected in the first area image block is clear, and the object part <b>201</b> of the defined masking can accurately represent the boundary of the object to be detected, S<sub>1</sub>(i,j) may be set such that it approaches 0 faster with the increase of d(i,j), so that the characteristics of the object to be detected in the fused composite area image block are closer to those of the actual object to be detected. In some other embodiments, other image fusing methods may also be used to utilize S<sub>1</sub>(i,j) and S<sub>2</sub>(i,j) to calculate I<sub>new</sub>(i,j), for example, an image fusing method with nonlinear weight, so that the fused composite area image block has a more realistic effect.</p><p id="p-0060" num="0057">Next, an example of fusing the image blocks based on the masking according to an embodiment of the present disclosure will be described with reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0061" num="0058">In this example, it is assumed that by comparing with the actual distribution of pulmonary lesion, it is found that the amount of the image samples, in which the attribute of the object is the pulmonary lesion distributed in the lung lobe (for example, pulmonary nodule), contained in the medical image sample set is small. Therefore, in this embodiment, taking a pulmonary nodule as a pulmonary lesion as an example, an image <b>31</b> containing a pulmonary nodule and an image <b>32</b> not containing a pulmonary nodule as shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref> are randomly selected from the medical image sample set. However, it should be noted that the pulmonary lesion is not necessarily the pulmonary nodule, and in other embodiments, it may also be any other type of pulmonary lesions.</p><p id="p-0062" num="0059">Then, an image block <b>31</b><i>a </i>containing a pulmonary nodule lesion is intercepted from the image <b>31</b> containing the pulmonary nodule for the subsequent image fusing. The two image blocks <b>32</b><i>a </i>marked with boxes in the image are the image blocks for composing a pulmonary nodule in the lung lobe area selected from the image <b>32</b> not containing the pulmonary nodule. The black dots in the image blocks are the center points for the composing of a pulmonary nodule. The masking <b>33</b> in <figref idref="DRAWINGS">FIG. <b>3</b></figref> is defined according to the pulmonary nodule lesion contained in the image block <b>31</b><i>a</i>, and includes the object part of the masking and the peripheral part around the object part, as shown in the circular area <b>33</b><i>a </i>in the central section of the masking <b>33</b> and the surrounding annular area <b>33</b><i>b </i>respectively.</p><p id="p-0063" num="0060">Next, by using the masking <b>33</b>, the image block <b>31</b><i>a </i>containing the pulmonary nodule lesion as the first area image block and the image block <b>32</b><i>a </i>in the left lung lobe as the second area image block are fused to generate a composite area image block (not shown) of the left lung lobe. The composite area image block is then embedded back into the image <b>32</b> as the second medical image to generate a new image <b>34</b><i>a </i>with a nodule lesion contained in the new left lung lobe as shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>. Similarly, by using the masking <b>33</b>, the image block <b>31</b><i>a </i>containing the pulmonary nodule lesion as the first area image block and the image block <b>32</b><i>b </i>in the right lung lobe as the second area image block are fused to generate a composite area image block (not shown) of the right lung lobe. The composite area image block is then embedded back into the image <b>32</b> as the second medical image to generate a new image <b>34</b><i>b </i>with a nodule lesion contained in the right left lung lobe. The generated third medical image containing the nodule lesion may be added to the medical image sample set as a new sample to achieve the date enhancement of the original medical image sample set.</p><p id="p-0064" num="0061">In the above example, by using one medical image containing the pulmonary nodule in the lung lobe in the sample set, two new medical images with pulmonary nodule in different positions of the left lung lobe and the right lung lobe are generated, which effectively makes up the attribute of the object of pulmonary nodule in the lung lobe lacking in the medical image sample set. The same attribute is added in a convenient and efficient way. However, due to the different composition positions of the lesion having the attribute of this object, the new medical image samples finally generated have sufficient difference. Therefore, the enhancement of the medical image sample set can be effectively realized. In some embodiments, it is possible that, according to the actual lack of the attributes of the object and the actual distribution of positive and negative samples in the sample set, one first medical image is selected to correspond to one second medical image, and one or more third medical images are generated at different positions that meet the requirement of the attributes of the object. For example, the identified lacking attribute of the object in the above example is pulmonary nodule in the lung lobe, and when selecting the image block to be composed in the second medical image, all lung lobe areas are candidate areas that meet the requirement. In some other embodiments, it is also possible to select one first medical image corresponding to multiple second medical images to generate multiple third medical images with diversified characteristics.</p><p id="p-0065" num="0062">In some embodiments, the number of the generated third medical images having the lacking attribute of the object may be determined based on whether the data distribution of the attribute of the object in real scenario is closed to or reached, or may be based on whether the detection accuracy of the attribute of the object by the data-enhanced medical image sample set reaches an expected value.</p><p id="p-0066" num="0063">In some other embodiments, it is necessary to consider that when machine learning is performed based on the enhanced medical image sample set, there will be no adverse effects, such as over-fitting, on the learning of the attribute of the object.</p><p id="p-0067" num="0064">The medical image sample set enhanced in the above way can not only enhance specific object attributes, such as the amount of pulmonary nodule lesions in the lung lobe, but also improve the diversity of samples. Therefore, in practical applications, when detecting the object to be detected by using a machine learning network, such as a neural network, trained with the sample set enhanced in this way, higher detection accuracy will be obtained, and the deep learning model trained with the data-enhanced medical image sample set will also have better generalization ability.</p><p id="p-0068" num="0065">In some embodiments, the generated third medical image having the lacking attribute of the object may data-enhance the medical image sample set in the way of offline expansion. In some other embodiments, the medical image sample set may also be expanded online in the way of online enhancement. The specific enhancement manner may be determined according to the capacity of the image sample set or the like.</p><p id="p-0069" num="0066"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows a comparison of the effects between a new nodule image generated by fusing the image blocks based on a masking according to an embodiment of the present disclosure and a new nodule image generated by a conventional manner of copy-move method. In <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the image <b>40</b> is an image in which the pulmonary nodule is contained in the lung lobe selected from the medical image sample set, and the image <b>41</b> is an image in which the pulmonary nodule is not contained in the lung lobe selected from the medical image sample set. The image <b>40</b><i>a </i>and the image <b>41</b><i>a </i>are respectively two new medical images containing the pulmonary nodules, which are generated through a traditional copy-move manner respectively, i.e., by copying the image block containing the pulmonary nodule in the image <b>40</b>, and moving into the right lung lobe and the left lung lobe of the image <b>41</b>. It can be seen from the image <b>40</b><i>a </i>and the image <b>41</b><i>a </i>that due to the simple copy-move method, there are obvious boundary traces around the pulmonary nodules, leading to a big gap between them and the medical image taken naturally. On the contrary, in the new medical images <b>40</b><i>b </i>and <b>41</b><i>b </i>containing the pulmonary nodules generated by the method of fusing the image blocks based on the masking according to the embodiment of the present disclosure, there is no obvious boundary trace around the pulmonary nodules, which is well fused with the original medical image and is very close to the medical image taken naturally. It is easy to conceive of that when the image <b>40</b><i>a </i>and the image <b>41</b><i>a </i>are added to the original medical image sample set for the training of the machine learning network such as the neural network, the above boundary trace can be considered as an undesired noise introduced artificially, which may lead to false recognition, and the effect of enhancing the medical image sample set cannot be obtained. The new nodule image generated by fusing the image blocks based on the masking according to the embodiment of the present disclosure will not bring the above adverse effects that lead to false recognition during machine learning training, and can effectively enhance the original medical image sample set.</p><p id="p-0070" num="0067">The following is the embodiments of a device of the present disclosure, which may be used to implement the method embodiment of the present disclosure. For details not disclosed in the device embodiment of the present disclosure, please refer to the method embodiment of the present disclosure.</p><p id="p-0071" num="0068"><figref idref="DRAWINGS">FIG. <b>5</b></figref> shows a schematic diagram of a device for medical image data enhancement according to an embodiment of the present disclosure. In some embodiments, a device <b>500</b> for data enhancement for medical image detection may be a dedicated intelligent device or a general intelligent device. For example, the device <b>500</b> may be a computer customized for image data processing tasks, or a cloud server. For example, the device <b>500</b> may be integrated into an image processing device.</p><p id="p-0072" num="0069">As an example, the device <b>500</b> for data enhancement for medical image detection may at least include a communication interface <b>501</b>, a memory <b>502</b> and a processor <b>503</b>.</p><p id="p-0073" num="0070">In some embodiments, the communication interface <b>501</b> is used to obtain a medical image sample set related to an object to be detected. For example, the communication interface <b>501</b> may receive data about the medical image sample set via a communication cable, wireless local area network (WLAN), wide area network (WAN), wireless network (such as radio wave, cellular or telecommunication network, and/or local or short-range wireless network, e.g., Bluetooth&#x2122;) or other communication manners.</p><p id="p-0074" num="0071">In some embodiments, the communication interface <b>501</b> may include an integrated service digital network (ISDN) card, a cable modem, a satellite modem or a modem to provide data communication connection. In such an implementation, the communication interface <b>501</b> may send and receive electrical, electromagnetic and/or optical signals via direct communication link, which carries analog/digital data streams representing various types of information. In some other embodiments, the communication interface <b>501</b> may also include a local area network (LAN) card (e.g., an Ethernet adapter) to provide a data communication connection to a compatible LAN. As an example, the communication interface <b>501</b> may also include a network interface <b>5011</b> via which the device <b>500</b> may be connected to a network (not shown), such as, but not limited to, a local area network or the Internet in a hospital. The network may connect the device <b>500</b> for data enhancement of medical image detection with an external device such as an image acquisition device (not shown), a medical image database <b>504</b> and an image data storage device <b>505</b>. The image acquisition device may be any device that acquires an image of an object, such as a DSA imaging apparatus, an MRI imaging apparatus, a CT imaging apparatus, a PET imaging apparatus, an ultrasound apparatus, a fluoroscopy apparatus, a SPECT imaging apparatus, or other medical imaging apparatus used to obtain a medical image of a patient. For example, the imaging device may be a lung CT imaging device, etc.</p><p id="p-0075" num="0072">In some embodiments, the communication interface <b>501</b> may be used to add the third medical image generated by the processor <b>503</b> to the medical image sample set to obtain a data-enhanced medical image sample set.</p><p id="p-0076" num="0073">In some embodiments, the device <b>500</b> for data enhancement for medical image detection may additionally include at least one of an input/output <b>506</b> and an image display <b>507</b>.</p><p id="p-0077" num="0074">The processor <b>503</b> may be a processing device that includes more than one general processing device, such as a micro-processor, a central processing unit (CPU) and a graphics processing unit (GPU), etc. More specifically, the processor <b>503</b> may be a complex instruction set computing (CISC) micro-processor, a reduced instruction set computing (RISC) micro-processor, a very long instruction word (VLIW) micro-processor, a processor that runs other instruction sets or a processor that runs a combination of instruction sets. The processor <b>503</b> may also be more than one dedicated processing device, such as dedicated Integrated circuit (ASIC), field programmable gate array (FPGA), digital signal processor (DSP) and system on chip (SoC). As those skilled in the art will understand, in some embodiments, the processor <b>503</b> may be a dedicated processor instead of a general-purpose processor. The processor <b>503</b> may include one or more known processing devices, such as any of microprocessors of Pentium&#x2122;, Core&#x2122;, Xeon&#x2122; or Itanium&#x2122; series manufactured by Intel, microprocessors of Turion&#x2122; Athlon&#x2122;, Sempron&#x2122;, Opteron&#x2122;, FX&#x2122;, Phenom&#x2122; series manufactured by AMD, or microprocessors manufactured by Sun Microsystems. The processor <b>503</b> may further include graphics processing units, such as GPU from GeForce&#xae;, Quadro&#xae;, and Tesla&#xae; series manufactured by Nvidia, and GPU of GMA, Iris&#x2122; series manufactured by Intel&#x2122;, or GPU of Radeon&#x2122; series manufactured by AMD. The processor <b>503</b> may further include accelerated processing units, such as desktop A-4 (6, 8) series manufactured by AMD and Xeon Phi&#x2122; series manufactured by Intel. The disclosed embodiments are not limited to any type of processor or processor circuit, which are configured in other ways to meet the following computational requirements: performing the method for data enhancement for medical image detection according to various embodiments of the present disclosure, for example. In addition, the term &#x201c;processor&#x201d; or &#x201c;image processor&#x201d; may include more than one processor, for example, a multi-core design or a plurality of processors, each of which has a multi-core design. The processor <b>503</b> can execute a sequence of computer program instructions stored in the memory <b>502</b> to execute various operations, processes, methods disclosed by this document.</p><p id="p-0078" num="0075">The processor <b>503</b> may be communicatively coupled to the memory <b>502</b> and configured to perform computer-executable instructions stored therein. The memory <b>502</b> may include read-only memory (ROM), flash memory, random access memory (RAM), such as synchronous DRAM (SDRAM) or Rambus DRAM dynamic random access memory (DRAM), static memory (e.g., flash memory, static random access memory), etc., on which computer-executable instructions are stored in any format. In some embodiments, the memory <b>502</b> may store computer-executable instructions of one or more programs <b>5021</b> for medical image data enhancement. The computer program instructions may be accessed by the processor <b>503</b>, read from ROM or any other suitable storage location, and loaded into the RAM for execution by the processor <b>503</b>. For example, the memory <b>502</b> may store one or more of software application programs. The software application programs stored in the memory <b>502</b> may include, for example, an operating system (not shown) and a soft control device for a general-purpose computer system. In addition, the memory <b>502</b> may store the entire software application program or only a part of the software application program (for example, the programs <b>5021</b> for data enhancement of medical image detection) to be executable by the processor <b>503</b>. In addition, the memory <b>502</b> may store a plurality of software modules for achieving the method for data enhancement for medical image detection, or individual steps in the process of training the learning network for medical image detection, consistency with the present disclosure. In addition, the memory <b>502</b> may store data generated/cached when performing the computer program, such as the medical image data <b>5022</b> that includes medical images sent from an image acquisition device, a medical image database <b>504</b>, an image data storage device <b>505</b>, etc. Such medical image data <b>5022</b> may include a received medical image sample set to be detected to which the detecting of the lacking attribute of the object is to be performed.</p><p id="p-0079" num="0076">In addition, the medical image data <b>5022</b> may also include a medical image selected from the medical image sample set to be fused so as to compose a new medical image having the lacking attribute of the object, and each new composed medical image.</p><p id="p-0080" num="0077">The processor <b>503</b> may execute the program <b>5021</b> for data enhancement for medical image detection to implement the method for data enhancement for medical image detection. In some embodiments, when executing the program <b>5021</b> for data enhancement for medical image detection, the processor <b>503</b> may associate the data on the detected lacking attribute of the object with the first medical image and the second medical image selected from the medical image sample set to be fused to compose a new image having the lacking attribute of the object. Each composed third medical image is stored into the memory <b>502</b>. In some embodiments, the memory <b>502</b> may communicate with the medical image database <b>504</b> via the communication interface <b>501</b>, and add each third medical image generated by the processor <b>503</b> into the medical image sample set in the medical image database <b>504</b> to obtain a data-enhanced medical image sample set.</p><p id="p-0081" num="0078">In some embodiments, the learning network for medical image detection may be stored in the memory <b>502</b>. In some other embodiments, the learning network for medical image detection may be stored in a remote apparatus, a discrete database (such as the medical image database <b>504</b>), a distributed apparatus, and may be used by a program <b>5021</b> for data enhancement for medical image detection.</p><p id="p-0082" num="0079">The input/output <b>506</b> may be configured to allow the device <b>500</b> for data enhancement for medical image detection to receive and/or send data. The input/output <b>506</b> may include one or more digital and/or analog communication apparatus that allow the device <b>500</b> to communicate with users or other machines and devices. For example, the input/output <b>506</b> may include a keyboard and mouse that allow the user to input.</p><p id="p-0083" num="0080">The network interface <b>5011</b> may include a network adaptor, a cable connector, a serial connector, an USB connector, a parallel connector, a high-speed data transmission adaptor such as optical fiber, an USB 3.0, a lightning, a wireless network adaptor such as wiki adaptor, a Telecommunication (3G, 4G/LTE, etc.) adaptor. The device <b>500</b> may connect to the network through the network interface <b>5011</b>. The network may provide functions of local area network (LAN), wireless network, cloud computing environment (e.g., software as a service, platform as a service, infrastructure as a service, etc.), client server, wide area network (WAN), etc.</p><p id="p-0084" num="0081">In addition to display the medical image, the image display <b>507</b> may display other information, such as image and/or text information about the attribute of the object lacking in the medical image sample set, a bounding box indicating the masking range, etc. For example, the image display <b>507</b> may be an LCD, CRT, or LED display.</p><p id="p-0085" num="0082">Moreover, while illustrative embodiments have been described herein, the scope includes any and all embodiments having equivalent elements, modifications, omissions, combinations (e.g., of aspects across various embodiments), adaptations or alterations based on the present disclosure. The elements in the claims are to be interpreted broadly based on the language employed in the claims and not limited to examples described in the present specification or during the prosecution of the application, which examples are to be construed as non-exclusive. It is intended, therefore, that the descriptions and examples be considered as examples only, with a true scope and spirit being indicated by the following claims and their full scope of equivalents.</p><p id="p-0086" num="0083">The above description is intended to be illustrative, and not restrictive. For example, the above-described examples (or one or more aspects thereof) may be used in combination with each other. Other embodiments can be used by one of ordinary skill in the art upon reviewing the above description. Also, in the above Detailed Description, various features may be grouped together to streamline the disclosure. This should not be interpreted as intending that an unclaimed disclosed feature is essential to any claim. Rather, inventive subject matter may lie in less than all features of a disclosed embodiment. Thus, the following claims are hereby incorporated into the Detailed Description as examples or embodiments, with each claim standing on its own as a separate embodiment, and it is contemplated that such embodiments can be combined with each other in various combinations or permutations. The scope of the invention should be determined with reference to the appended claims, along with the full scope of equivalents to which such claims are entitled.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230005113A1-20230105-M00001.NB"><img id="EMI-M00001" he="9.14mm" wi="76.20mm" file="US20230005113A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002" nb-file="US20230005113A1-20230105-M00002.NB"><img id="EMI-M00002" he="9.14mm" wi="76.20mm" file="US20230005113A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method for medical image data enhancement, comprising:<claim-text>receiving, by a communication interface, a medical image sample set related to an object to be detected;</claim-text><claim-text>based on an attribute of the object lacking in the medical image sample set, selecting, by a processor, a first medical image and a second medical image from the medical image sample set, wherein the first medical image contains the object lacking the attribute, and the second medical image does not contain the object lacking the attribute;</claim-text><claim-text>determining a first area image block encompassing an area containing the object lacking the attribute in the first medical image;</claim-text><claim-text>determining a second area image block encompassing an area not containing the object lacking the attribute in the second medical image;</claim-text><claim-text>generating a composite area image block, by the processor, by fusing the first area image block and the second area image block based on a masking including an object part and a peripheral part around the object part;</claim-text><claim-text>embedding, by the processor, the composite area image block back into the second medical image to obtain a third medical image; and</claim-text><claim-text>including the third medical image in the medical image sample set to obtain a data-enhanced medical image sample set.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said fusing the first area image block and the second area image block further comprises:<claim-text>for each position in each part of the masking, fusing image information at the position of the first area image block and image information at the position of the second area image block respectively based on a first fusing coefficient and a second fusing coefficient.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein for each position in the peripheral part of the masking,<claim-text>the closer a position to the object part, the larger the first fusing coefficient and the smaller the second fusing coefficient.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein a size of the first area image block and a size of the second area image block are the same, and a size of the peripheral part of the masking matches the size of the first area image block.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>determining the attribute of the object lacking in the medical image sample set using a trained model.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein determining the attribute of the object lacking in the medical image sample set using a trained model further comprises:<claim-text>detecting a distribution of the attributes of the object by applying the trained model to the medical image sample set; and</claim-text><claim-text>comparing the detected distribution of the attributes of the object with the actual distribution of the attributes of the object.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, further comprising:<claim-text>training the model based on the data-enhanced medical image sample set.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first medical image and the second medical image are randomly selected.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the attribute includes at least one of a predetermined subtype of the object, a predetermined position of the object, or a predetermined size of the object.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein, when the object is a pulmonary lesion, the predetermined subtype of the object includes a solid lesion, a ground-glass lesion or a semi-solid lesion.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein, when the object is a pulmonary lesion, the predetermined position of the object is inside lung lobes or inside a thorax.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein for each position in the object part of the masking, the first fusing coefficient is 1, and the second fusing coefficient is 0.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein in the first area image block, the first fusing coefficient for each position outside of an area corresponding to the masking is 0, and in the second area image block, the second fusing coefficient for each position outside of the area corresponding to the masking is 1.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein for each position in the peripheral part of the masking, a sum of the first fusing coefficient and the second fusing coefficient is 1.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein an image value for each position in the composite area image block is:<claim-text><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>I</i><sub>new</sub>(<i>i,j</i>)=<i>I</i><sub>1</sub>(<i>i,j</i>)*<i>S</i><sub>1</sub>(<i>i,j</i>)+<i>I</i><sub>2</sub>(<i>i,j</i>)*<i>S</i><sub>2</sub>(<i>i,j</i>)<?in-line-formulae description="In-line Formulae" end="tail"?></claim-text><claim-text>wherein I<sub>new</sub>(i,j) is image value for position (i,j) in the composite area image block, I<sub>1</sub>(i,j) and I<sub>2</sub>(i,j) are respectively image values of the first area image block and the second area image block for position (i,j), and S<sub>1</sub>(i,j) and S<sub>2</sub>(i,j) are respectively the first fusing coefficient and the second fusing coefficient for position (i,j).</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the first fusing coefficient and the second fusing coefficient for each position in the peripheral part of the masking are calculated as follows:</claim-text><claim-text><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mrow>  <mrow>   <msub>    <mi>S</mi>    <mn>1</mn>   </msub>   <mo>(</mo>   <mrow>    <mi>i</mi>    <mo>,</mo>    <mi>j</mi>   </mrow>   <mo>)</mo>  </mrow>  <mo>=</mo>  <mrow>   <mo>{</mo>   <mrow>    <mtable>     <mtr>      <mtd>       <mn>0</mn>      </mtd>      <mtd>       <mrow>        <mrow>         <mi>d</mi>         <mo>&#x2061;</mo>         <mo>(</mo>         <mrow>          <mi>i</mi>          <mo>,</mo>          <mi>j</mi>         </mrow>         <mo>)</mo>        </mrow>        <mo>&#x3e;</mo>        <mi>d_Q</mi>       </mrow>      </mtd>     </mtr>     <mtr>      <mtd>       <mfrac>        <mrow>         <mi>d_Q</mi>         <mo>-</mo>         <mrow>          <mi>d</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <mrow>           <mi>i</mi>           <mo>,</mo>           <mi>j</mi>          </mrow>          <mo>)</mo>         </mrow>        </mrow>        <mi>d_Q</mi>       </mfrac>      </mtd>      <mtd>       <mrow>        <mrow>         <mi>d</mi>         <mo>&#x2062;</mo>         <mrow>          <mo>(</mo>          <mrow>           <mi>i</mi>           <mo>,</mo>           <mi>j</mi>          </mrow>          <mo>)</mo>         </mrow>        </mrow>        <mo>&#x3c;</mo>        <mi>d_Q</mi>       </mrow>      </mtd>     </mtr>    </mtable>    <mo>,</mo>    <mrow>     <mrow>      <msub>       <mi>S</mi>       <mn>2</mn>      </msub>      <mo>(</mo>      <mrow>       <mi>i</mi>       <mo>,</mo>       <mi>j</mi>      </mrow>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mrow>      <mn>1</mn>      <mo>-</mo>      <mrow>       <msub>        <mi>S</mi>        <mn>1</mn>       </msub>       <mo>(</mo>       <mrow>        <mi>i</mi>        <mo>,</mo>        <mi>j</mi>       </mrow>       <mo>)</mo>      </mrow>     </mrow>    </mrow>   </mrow>  </mrow> </mrow></math></maths><claim-text>wherein S<sub>1</sub>(i,j) and S<sub>2</sub>(i,j) are respectively the first fusing coefficient and the second fusing coefficient for position(i,j), d(i,j) is the minimum distance between the position (i,j) and the boundary of the object part of the masking, and d_Q is the minimum distance from all points on the boundary of the peripheral part of the masking to the boundary of the object part of the masking.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. A device for medical image data enhancement, comprising:<claim-text>a communication interface configured to receive a medical image sample set related to an object to be detected; and</claim-text><claim-text>a processor configured to:</claim-text><claim-text>based on an attribute of the object lacking in the medical image sample set, selecting a first medical image and a second medical image from the medical image sample set, wherein the first medical image contains the object lacking the attribute, and the second medical image does not contain the object lacking the attribute;</claim-text><claim-text>determine a first area image block encompassing an area containing the object lacking the attribute in the first medical image;</claim-text><claim-text>determine a second area image block encompassing an area not containing the object lacking the attribute in the second medical image;</claim-text><claim-text>generating a composite area image block by fusing the first area image block and the second area image block based on a masking including an object part and a peripheral part around the object part;</claim-text><claim-text>embedding the composite area image block back into the second medical image to obtain a third medical image; and</claim-text><claim-text>including the third medical image in the medical image sample set through the communication interface to obtain a data-enhanced medical image sample set.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The device according to <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein fusing the first area image block and the second area image block further comprises:<claim-text>for each position in each part of the masking, fusing image information at the position of the first area image block and image information at the position of the second area image block respectively based on a first fusing coefficient and a second fusing coefficient.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The device according to <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein, for each position in the peripheral part of the masking,<claim-text>the closer a position to the object part, the larger the first fusing coefficient and the smaller the second fusing coefficient.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. A non-transitory computer readable storage medium having computer executable instructions stored thereon, wherein the computer executable instructions, when executed by a processor, implement a method for medical image data enhancement, wherein the method comprises:<claim-text>receiving a medical image sample set related to an object to be detected;</claim-text><claim-text>based on an attribute of the object lacking in the medical image sample set, selecting a first medical image and a second medical image from the medical image sample set, wherein the first medical image contains the object lacking the attribute, and the second medical image does not contain the object lacking the attribute;</claim-text><claim-text>determining a first area image block encompassing an area containing the object lacking the attribute in the first medical image;</claim-text><claim-text>determining a second area image block encompassing intercepting an area not containing the object lacking the attribute in the second medical image;</claim-text><claim-text>generating a composite area image block by fusing the first area image block and the second area image block based on a masking including an object part and a peripheral part around the object part;</claim-text><claim-text>embedding the composite area image block back into the second medical image to obtain a third medical image; and</claim-text><claim-text>including the third medical image in the medical image sample set to obtain a data-enhanced medical image sample set.</claim-text></claim-text></claim></claims></us-patent-application>