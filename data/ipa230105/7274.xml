<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230007275A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230007275</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17941083</doc-number><date>20220909</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>159</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>126</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>186</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>94</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>105</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>176</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>70</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>172</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>136</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>463</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20141101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>159</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20141101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>126</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20141101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>186</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20141101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>94</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20141101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>105</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20141101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>176</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20141101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>70</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20141101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>172</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20141101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>136</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20141101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>463</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">ENCODING DEVICE, DECODING DEVICE, ENCODING METHOD, AND DECODING METHOD</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16858002</doc-number><date>20200424</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11483570</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17941083</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16521145</doc-number><date>20190724</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>10674161</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>16858002</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16113330</doc-number><date>20180827</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>10412394</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>16521145</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>15679520</doc-number><date>20170817</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>10110904</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>16113330</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>15590876</doc-number><date>20170509</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>9872027</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>15679520</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>14799253</doc-number><date>20150714</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>9681138</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>15590876</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>14028024</doc-number><date>20130916</date></document-id><parent-status>ABANDONED</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>14799253</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>PCT/JP2011/073851</doc-number><date>20111017</date></document-id><parent-status>PENDING</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>14028024</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>KABUSHIKI KAISHA TOSHIBA</orgname><address><city>Tokyo</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>TANIZAWA</last-name><first-name>Akiyuki</first-name><address><city>Kanagawa</city><country>JP</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>CHUJOH</last-name><first-name>Takeshi</first-name><address><city>Kanagawa</city><country>JP</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>SHIODERA</last-name><first-name>Taichiro</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>KABUSHIKI KAISHA TOSHIBA</orgname><role>03</role><address><city>Tokyo</city><country>JP</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">According to an embodiment, an encoding device includes an index setting unit and an encoding unit. The index setting unit generates a common index in which reference indices of one or more reference images included in a first index and a second index are sorted in a combination so as not to include a same reference image in accordance with a predetermined scanning order. The first index representing a combination of the one or more reference images referred to by a first reference image. The second index representing a combination of the one or more reference images referred to by a second reference image. The encoding unit encodes the common index.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="156.13mm" wi="158.75mm" file="US20230007275A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="174.92mm" wi="161.63mm" file="US20230007275A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="96.27mm" wi="94.15mm" file="US20230007275A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="165.10mm" wi="73.83mm" file="US20230007275A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="191.43mm" wi="67.82mm" file="US20230007275A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="226.65mm" wi="122.51mm" orientation="landscape" file="US20230007275A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="107.36mm" wi="112.10mm" file="US20230007275A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="204.47mm" wi="111.42mm" orientation="landscape" file="US20230007275A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="183.56mm" wi="159.17mm" file="US20230007275A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="158.07mm" wi="158.16mm" file="US20230007275A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="149.01mm" wi="150.62mm" file="US20230007275A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="149.35mm" wi="150.71mm" file="US20230007275A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="112.10mm" wi="156.38mm" file="US20230007275A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="43.26mm" wi="116.76mm" file="US20230007275A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="233.51mm" wi="163.41mm" file="US20230007275A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="205.15mm" wi="154.69mm" file="US20230007275A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="211.84mm" wi="154.35mm" file="US20230007275A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="223.18mm" wi="154.35mm" file="US20230007275A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="220.13mm" wi="154.35mm" file="US20230007275A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00019" num="00019"><img id="EMI-D00019" he="77.81mm" wi="156.72mm" file="US20230007275A1-20230105-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00020" num="00020"><img id="EMI-D00020" he="234.61mm" wi="158.33mm" file="US20230007275A1-20230105-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00021" num="00021"><img id="EMI-D00021" he="217.76mm" wi="154.35mm" file="US20230007275A1-20230105-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00022" num="00022"><img id="EMI-D00022" he="212.68mm" wi="161.71mm" file="US20230007275A1-20230105-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00023" num="00023"><img id="EMI-D00023" he="171.87mm" wi="159.85mm" file="US20230007275A1-20230105-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00024" num="00024"><img id="EMI-D00024" he="172.72mm" wi="60.37mm" file="US20230007275A1-20230105-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00025" num="00025"><img id="EMI-D00025" he="77.47mm" wi="144.95mm" file="US20230007275A1-20230105-D00025.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00026" num="00026"><img id="EMI-D00026" he="212.34mm" wi="136.65mm" file="US20230007275A1-20230105-D00026.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00027" num="00027"><img id="EMI-D00027" he="73.91mm" wi="144.95mm" file="US20230007275A1-20230105-D00027.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00028" num="00028"><img id="EMI-D00028" he="221.23mm" wi="145.29mm" file="US20230007275A1-20230105-D00028.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00029" num="00029"><img id="EMI-D00029" he="158.41mm" wi="144.95mm" file="US20230007275A1-20230105-D00029.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00030" num="00030"><img id="EMI-D00030" he="238.00mm" wi="145.03mm" file="US20230007275A1-20230105-D00030.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00031" num="00031"><img id="EMI-D00031" he="213.53mm" wi="128.69mm" orientation="landscape" file="US20230007275A1-20230105-D00031.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00032" num="00032"><img id="EMI-D00032" he="155.70mm" wi="156.72mm" file="US20230007275A1-20230105-D00032.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00033" num="00033"><img id="EMI-D00033" he="207.52mm" wi="157.90mm" orientation="landscape" file="US20230007275A1-20230105-D00033.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00034" num="00034"><img id="EMI-D00034" he="76.20mm" wi="116.33mm" file="US20230007275A1-20230105-D00034.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application is a continuation application of U.S. application Ser. No. 16/858,002, filed on Apr. 24, 2020, which is a continuation application of U.S. application Ser. No. 16/521,145, filed Jul. 24, 2019 (now U.S. Pat. No. 10,674,161), which is a continuation application of U.S. application Ser. No. 16/113,330, filed Aug. 27, 2018 (now U.S. Pat. No. 10,412,394), which is a continuation application of U.S. application Ser. No. 15/679,520, filed Aug. 17, 2017 (now U.S. Pat. No. 10,110,904), which is a continuation application of U.S. application Ser. No. 15/590,876, filed May 9, 2017 (now U.S. Pat. No. 9,872,027), which is a continuation of U.S. application Ser. No. 14/799,253 filed Jul. 14, 2015 (now U.S. Pat. No. 9,681,138), which is a continuation application of U.S. application Ser. No. 14/028,024 filed Sep. 16, 2013, which is a continuation of PCT international Application Ser. No. PCT/JP2011/073851, filed on Oct. 17, 2011, which designates the United States; the entire contents of each of which are incorporated herein by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">FIELD</heading><p id="p-0003" num="0002">Embodiments described herein relate generally to an encoding method and a decoding method.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">In recent years, a method of encoding an image with markedly improved coding efficiency is recommended as ITU-T REC. H.264 and ISO/IEC 14496-10 (hereinafter, referred to as &#x201c;H.264&#x201d;) in cooperation of ITU-T (International Telecommunication Union Telecommunication Standardization Sector) and ISO (International Organization for Standardization)/IEC (International Electrotechnical Commission).</p><p id="p-0005" num="0004">In H.264, an inter-prediction coding system is disclosed in which redundancy in the time direction is eliminated to achieve high coding efficiency by making a motion compensation prediction of fractional precision using a coded image as a reference image.</p><p id="p-0006" num="0005">In addition, a system is proposed in which a moving image including a fading or dissolving effect is encoded with efficiency higher than that of an inter-prediction coding system according to ISO/IEC MPEG (Moving Picture Experts Group)-1, 2, 4. In this system, a motion compensation prediction of fractional precision is made for an input moving image having luminance and two color differences as frames for predicting a change in the brightness in the time direction. Then, by using an index representing a combination of a reference image, a weighting factor for each luminance and two color differences, and an offset for each luminance and two color differences, a predicted image is multiplied by the weighting factor, and the offset is added thereto.</p><p id="p-0007" num="0006">However, in the technology of the related art as described above, in a bidirectional prediction slice in which bi-directional predictions can be selected, when weighted motion compensation is performed using two indices having the same reference image but having different reference image numbers different from each other, there are cases where an index having the same value is encoded twice, and accordingly, there are cases where the coding efficiency decreases. An object of the present invention is to provide an encoding method and a decoding method capable of improving the coding efficiency.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram that illustrates an example of an encoding device according to a first embodiment;</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is an explanatory diagram that illustrates an example of a predicted coding sequence for a pixel block according to the first embodiment;</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>3</b>A</figref> is a diagram that illustrates an example of the size of a coding tree block according to the first embodiment;</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>3</b>B</figref> is a diagram that illustrates a specific example of the coding tree block according to the first embodiment;</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>3</b>C</figref> is a diagram that illustrates another specific example of the coding tree block according to the first embodiment;</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>3</b>D</figref> is a diagram that illustrates another specific example of the coding tree block according to the first embodiment;</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a block diagram that illustrates an example of a predicted image generating unit according to the first embodiment;</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram that illustrates an example of the relation between motion vectors for a motion-compensated prediction in a bidirectional prediction according to the first embodiment;</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a block diagram that illustrates an example of a multi-frame motion compensation unit according to the first embodiment;</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is an explanatory diagram that illustrates an example of fixed point precision of a weighting factor according to the first embodiment;</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a block diagram that illustrates an example of an index setting unit according to the first embodiment;</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>9</b>A</figref> is a diagram that illustrates an example of WP parameter information according to the first embodiment;</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>9</b>B</figref> is a diagram that illustrates an example of WP parameter information according to the first embodiment;</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a diagram that illustrates an example of an encoding order and a display order in a low-delay encoding structure according to the first embodiment;</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a diagram that illustrates an example of the relation between a reference image and a reference number in the low-delay encoding structure according to the first embodiment;</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a diagram that illustrates an example of an encoding order and a display order in a random access encoding structure according to the first embodiment;</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a diagram that illustrates an example of the relation between a reference image and a reference number in the random access encoding structure according to the first embodiment;</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a diagram that illustrates an example of the scanning order of a list number and a reference number of a reference image according to the first embodiment;</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a diagram that illustrates an example of the WP parameter information after a common list conversion according to the first embodiment in a simplified manner;</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>16</b></figref> is diagram that illustrates an example of the WP parameter information after a common list conversion according to the first embodiment in a simplified manner;</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is a flowchart that illustrates an example of the process of generating index information according to the first embodiment;</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>18</b></figref> is a diagram that illustrates an example of syntax according to the first embodiment;</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>19</b></figref> is a diagram that illustrates an example of picture parameter set syntax according to the first embodiment;</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>20</b></figref> is a diagram that illustrates slice header syntax according to the first embodiment;</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>21</b></figref> is a diagram that illustrates an example of pred weight table syntax according to the first embodiment.</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>22</b></figref> is a diagram that illustrates an example of sequence parameter set syntax according to a modification;</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>23</b></figref> is a diagram that illustrates an example of adaptation parameter set syntax according to a modification;</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>24</b></figref> is a diagram that illustrates an example of a pred weight table syntax according to a modification;</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>25</b></figref> is a block diagram that illustrates an example of the configuration of an index setting unit according to a second embodiment;</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>26</b></figref> is a flowchart that illustrates an example of the process of generating index information according to the second embodiment;</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>27</b></figref> is a diagram that illustrates an example of pred weight table syntax according to the second embodiment;</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>28</b></figref> is a block diagram that illustrates an example of the configuration of an encoding device according to a third embodiment;</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>29</b></figref> is a diagram that illustrates a detailed example of motion information memory according to the third embodiment;</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>30</b>A</figref> is a diagram that illustrates an example of block positions at which motion information candidates are derived for an encoding pixel block according to the third embodiment;</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>30</b>B</figref> is a diagram that illustrates an example of a block position at which a motion information candidate is derived for an encoding pixel block according to the third embodiment;</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. <b>31</b></figref> is a diagram that illustrates an example of the relation between pixel block positions of a plurality of motion information candidates and pixel block position indices according to the third embodiment;</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. <b>32</b></figref> is a flowchart that illustrates an example of a storage process for MergeCandList according to the third embodiment;</p><p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. <b>33</b></figref> is a diagram that illustrates an example of a storage list of motion information according to the third embodiment;</p><p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. <b>34</b></figref> is a flowchart that illustrates an example of a method of storing motion information in the storage list according to the third embodiment;</p><p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. <b>35</b></figref> is a diagram that illustrates an example of a combination of a bidirectional prediction according to the third embodiment;</p><p id="p-0048" num="0047"><figref idref="DRAWINGS">FIG. <b>36</b></figref> is a diagram that illustrates an example of pred unit syntax according to the third embodiment;</p><p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. <b>37</b></figref> is a flowchart that illustrates another example of the storage process for MergeCandList according to the third embodiment;</p><p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. <b>38</b></figref> is a block diagram that illustrates an example of the configuration of a decoding device according to a fourth embodiment;</p><p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. <b>39</b></figref> is a block diagram that illustrates an example of the configuration of an index setting unit according to the fourth embodiment;</p><p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. <b>40</b></figref> is a block diagram that illustrates an example of the configuration of an index setting unit according to a fifth embodiment;</p><p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. <b>41</b></figref> is a block diagram that illustrates an example of the configuration of a decoding device according to a sixth embodiment; and</p><p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. <b>42</b></figref> is a view illustrating a hardware configuration of the device according to each embodiment.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0055" num="0054">According to an embodiment, an encoding device includes an index setting unit and an encoding unit. The index setting unit generates a common index in which reference indices of one or more reference images included in a first index and a second index are sorted in a combination so as not to include a same reference image in accordance with a predetermined scanning order. The first index representing a combination of the one or more reference images referred to by a first reference image. The second index representing a combination of the one or more reference images referred to by a second reference image. The encoding unit encodes the common index.</p><p id="p-0056" num="0055">Hereinafter, embodiments will be described in detail with reference to the accompanying drawings. An encoding device and a decoding device according to each embodiment presented below may be implemented by hardware such as an LSI (Large-Scale Integration) chip, a DSP (Digital Signal Processor), or an FPGA (Field Programmable Gate Array). In addition, an encoding device and a decoding device according to each embodiment presented below may be implemented by causing a computer to execute a program, in other words, by software. In description presented below, a term &#x201c;image&#x201d; may be appropriately replaced by a term such as a &#x201c;video&#x201d;, a &#x201c;pixel&#x201d;, an &#x201c;image signal&#x201d;, a &#x201c;picture&#x201d;, or &#x201c;image data&#x201d;.</p><heading id="h-0006" level="1">First Embodiment</heading><p id="p-0057" num="0056">In a first embodiment, an encoding device encoding a moving image will be described.</p><p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram that illustrates an example of the configuration of an encoding device <b>100</b> according to a first embodiment.</p><p id="p-0059" num="0058">The encoding device <b>100</b> divides each frame or each field configuring an input image into a plurality of pixel blocks and performs predicted encoding of the divided pixel blocks using encoding parameters input from an encoding control unit <b>111</b>, thereby generating a predicted image. Then, the encoding device <b>100</b> generates a prediction error by subtracting the predicted image from the input image divided into the plurality of pixel blocks, generates encoded data by performing orthogonal transformation, and quantization, and then entropy encoding for the generated prediction error, and outputs the generated encoded data.</p><p id="p-0060" num="0059">The encoding device <b>100</b> performs predicted encoding by selectively applying a plurality of prediction modes that are different from each other in at least one of the block size of the pixel block and the method of generating a predicted image. The method of generating a predicted image can be largely divided into two types including an intra-prediction in which a prediction is made within an encoding target frame and an inter-prediction in which a motion-compensated prediction is made using one or more reference frames of different time points. The intra-prediction is also called an internal-screen prediction, an internal-frame prediction, or the like, and the inter-prediction is also called an inter-screen prediction, an inter-frame prediction, a motion-compensated prediction, or the like.</p><p id="p-0061" num="0060"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is an explanatory diagram that illustrates an example of a predicted coding sequence for a pixel block according to the first embodiment. In the example illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the encoding device <b>100</b> performs prediction encoding from the upper left side toward the lower right side in the pixel block. Thus, in an encoding processing target frame f, on the left side and the upper side of the encoding target pixel block c, pixel blocks p that have been completed to be encoded are located. Hereinafter, for the simplification of description, while it is assumed that the encoding device <b>100</b> performs prediction encoding in order illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the order in the predicted encoding is not limited thereto.</p><p id="p-0062" num="0061">The pixel block represents a unit for processing an image and, for example, a block having an M&#xd7;N size (here, M and N are natural numbers), a coding tree block, a macro block, a sub-block, one pixel, or the like corresponds thereto. In description presented below, basically, the pixel block is used as the meaning of a coding tree block but may be used as a different meaning. For example, in description of a prediction unit, a pixel block is used as the meaning of a pixel block of the prediction unit. A block may be referred to as a unit or the like. For example, a coding block may be referred to as a coding unit.</p><p id="p-0063" num="0062"><figref idref="DRAWINGS">FIG. <b>3</b>A</figref> is a diagram that illustrates an example of the size of a coding tree block according to the first embodiment. The coding tree block, typically, is a pixel block of 64&#xd7;64 as illustrated in <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>. However, the coding tree block is not limited thereto but may be a pixel block of 32&#xd7;32, a pixel block of 16&#xd7;16, a pixel block of 8&#xd7;8, a pixel block of 4&#xd7;4, or the like. Here, the coding tree block may not be a square but, for example, may be a pixel block of an M&#xd7;N size (here, M&#x2260;N).</p><p id="p-0064" num="0063"><figref idref="DRAWINGS">FIGS. <b>3</b>B to <b>3</b>D</figref> are diagrams representing specific examples of the coding tree block according to the first embodiment. <figref idref="DRAWINGS">FIG. <b>3</b>B</figref> represents a coding tree block having a size of 64&#xd7;64 (N=32). Here, N represents the size of a reference coding tree block. The size of a case where the coding tree block is divided is defined as N, and the size of a case where the coding tree block is not divided is defined as 2N. <figref idref="DRAWINGS">FIG. <b>3</b>C</figref> represents a coding tree block acquired by dividing the coding tree block illustrated in <figref idref="DRAWINGS">FIG. <b>3</b>B</figref> into a quadtree. The coding tree block, as illustrated in <figref idref="DRAWINGS">FIG. <b>3</b>C</figref>, has a quadtree structure. In a case where the coding tree block is divided, as illustrated in <figref idref="DRAWINGS">FIG. <b>3</b>C</figref>, numbers are attached to four pixel blocks after division in the Z scanning order.</p><p id="p-0065" num="0064">In addition, within each number of the quadtree, the coding tree block may be further divided into a quadtree. Accordingly, the coding tree block may be divided in a hierarchical manner. In such a case, the depth of the division is defined as Depth. <figref idref="DRAWINGS">FIG. <b>3</b>D</figref> represents one of the coding tree blocks acquired by dividing the coding tree block illustrated in <figref idref="DRAWINGS">FIG. <b>3</b>B</figref> into a quadtree, and the block size thereof is 32&#xd7;32 (N=16). The depth of the coding tree block illustrated in <figref idref="DRAWINGS">FIG. <b>3</b>B</figref> is &#x201c;0&#x201d;, and the depth of the coding tree block illustrated in <figref idref="DRAWINGS">FIG. <b>3</b>D</figref> is &#x201c;1&#x201d;. In addition, a coding tree block having a largest unit is called a large coding tree block, and an input image signal is encoded in such a unit in the raster scanning order.</p><p id="p-0066" num="0065">In the description presented below, the encoded target block or the coding tree block of an input image may be referred to as a prediction target block or a prediction pixel block. In addition, the encoding unit is not limited to the pixel block, but at least one of a frame, a field, a slice, a line, and a pixel may be used as the encoding unit.</p><p id="p-0067" num="0066">The encoding device <b>100</b>, as illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, includes: a subtraction unit <b>101</b>; an orthogonal transformation unit <b>102</b>; a quantization unit <b>103</b>; an inverse quantization unit <b>104</b>; an inverse orthogonal transformation unit <b>105</b>; an addition unit <b>106</b>; a predicted image generating unit <b>107</b>; an index setting unit <b>108</b>; a motion evaluating unit <b>109</b>; and an encoding unit <b>110</b>. In addition, the encoding control unit <b>111</b> illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref> controls the encoding device <b>100</b> and, for example, may be implemented by using a CPU (Central Processing Unit) or the like.</p><p id="p-0068" num="0067">The subtraction unit <b>101</b> acquires a prediction error by subtracting a corresponding predicted image from an input image divided into pixel blocks. The subtraction unit <b>101</b> outputs the prediction error so as to be input to the orthogonal transformation unit <b>102</b>.</p><p id="p-0069" num="0068">The orthogonal transformation unit <b>102</b> performs an orthogonal transformation such as a discrete cosine transform (DCT) or a discrete sine transform (DST) for the prediction error input from the subtraction unit <b>101</b>, thereby acquiring a transformation coefficient. The orthogonal transformation unit <b>102</b> outputs the transformation coefficient so as to be input to the quantization unit <b>103</b>.</p><p id="p-0070" num="0069">The quantization unit <b>103</b> performs a quantization process for the transformation coefficient input from the orthogonal transformation unit <b>102</b>, thereby acquiring a quantization transformation coefficient. More specifically, the quantization unit <b>103</b> performs quantization based on a quantization parameter designated by the encoding control unit <b>111</b> and quantization information such as a quantization matrix. Described in more detail, the quantization unit <b>103</b> acquires the quantization transformation coefficient by dividing the transformation coefficient by a quantization step size derived based on the quantization information. The quantization parameter represents the fineness of the quantization. The quantization matrix is used for weighting the fineness of the quantization for each component of the transformation coefficient. The quantization unit <b>103</b> outputs the quantization transformation coefficient so as to be input to the inverse quantization unit <b>104</b> and the encoding unit <b>110</b>.</p><p id="p-0071" num="0070">The inverse quantization unit <b>104</b> performs an inverse quantization process for the quantization transformation coefficient input from the quantization unit <b>103</b>, thereby acquiring a restoration transformation coefficient. More specifically, the inverse quantization unit <b>104</b> performs inverse quantization based on the quantization information used by the quantization unit <b>103</b>. Described in detail, the inverse quantization unit <b>104</b> acquires a restoration transformation coefficient by multiplying the quantization transformation coefficient by the quantization step size derived based on the quantization information. In addition, the quantization information used by the quantization unit <b>103</b> is loaded from internal memory, which is not illustrated in the figure, of the encoding control unit <b>111</b> and is used. The inverse quantization unit <b>104</b> outputs the restoration transformation coefficient so as to be input to the inverse orthogonal transformation unit <b>105</b>.</p><p id="p-0072" num="0071">The inverse orthogonal transformation unit <b>105</b> performs an inverse orthogonal transformation such as an inverse discrete cosine transform (IDCT) or an inverse discrete sine transform (IDST) for the restoration transformation coefficient input from the inverse quantization unit <b>104</b>, thereby acquiring a restoration prediction error. Here, the inverse orthogonal transformation performed by the inverse orthogonal transformation unit <b>105</b> corresponds to an orthogonal transformation performed by the orthogonal transformation unit <b>102</b>. The inverse orthogonal transformation unit <b>105</b> outputs the restoration prediction error so as to be input to the addition unit <b>106</b>.</p><p id="p-0073" num="0072">The addition unit <b>106</b> adds the restoration prediction error input from the inverse orthogonal transformation unit <b>105</b> and a corresponding predicted image, thereby generating a local decoded image. The addition unit <b>106</b> outputs the local decoded image so as to be input to the predicted image generating unit <b>107</b>.</p><p id="p-0074" num="0073">The predicted image generating unit <b>107</b> stores the local decoded image input from the addition unit <b>106</b> in memory (not illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>) as a reference image and outputs the reference image stored in the memory so as to be input to the motion evaluating unit <b>109</b>. In addition, the predicted image generating unit <b>107</b> generates a predicted image by performing a weighted motion-compensated prediction based on the motion information and WP parameter information input from the motion evaluating unit <b>109</b>. The predicted image generating unit <b>107</b> outputs the predicted image so as to be input to the subtraction unit <b>101</b> and the addition unit <b>106</b>.</p><p id="p-0075" num="0074"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a block diagram that illustrates an example of the configuration of the predicted image generating unit <b>107</b> according to the first embodiment. The predicted image generating unit <b>107</b>, as illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, includes: a multi-frame motion compensation unit <b>201</b>; a memory <b>202</b>; a single-directional motion compensation unit <b>203</b>; a prediction parameter control unit <b>204</b>; a reference image selector <b>205</b>; a frame memory <b>206</b>; and a reference image control unit <b>207</b>.</p><p id="p-0076" num="0075">The frame memory <b>206</b> stores the local decoded image input from the addition unit <b>106</b> as a reference image under the control of the reference image control unit <b>207</b>. The frame memory <b>206</b> includes a plurality of memory sets FM<b>1</b> to FMN (here, N&#x2265;2) used for temporarily storing the reference image.</p><p id="p-0077" num="0076">The prediction parameter control unit <b>204</b> prepares a plurality of combinations a reference image number and a prediction parameter as a table based on the motion information input from the motion evaluating unit <b>109</b>. Here, the motion information represents information of a motion vector representing the deviation of a motion that is used for the motion-compensated prediction, the reference image number, and a prediction mode such as a single-directional/bidirectional prediction. The prediction parameter represents information relating to the motion vector and the prediction mode. Then, the prediction parameter control unit <b>204</b> selects a combination of a reference number and a prediction parameter used for generating a predicted image based on the input image and outputs the selected combination so as to allow the reference image number to be input to the reference image selector <b>205</b> and allow the prediction parameter to be input to the single-directional motion compensation unit <b>203</b>.</p><p id="p-0078" num="0077">The reference image selector <b>205</b> is a switch that changes one of output terminals of the frame memories FM<b>1</b> to FMN, which are included in the frame memory <b>206</b>, to be switched to based on a reference image number input from the prediction parameter control unit <b>204</b>. For example, when the reference image number is &#x201c;0&#x201d;, the reference image selector <b>205</b> connects the output terminal of the frame memory FM<b>1</b> to the output terminal of the reference image selector <b>205</b>, and, when the reference image number is N&#x2212;1, the reference image selector <b>205</b> connects the output terminal of the frame memory FMN to the output terminal of the reference image selector <b>205</b>. The reference image selector <b>205</b> outputs a reference image stored in the frame memory of which the output terminal is connected thereto from among the frame memories FM<b>1</b> to FMN included in the frame memory <b>206</b> so as to be input to the single-directional motion compensation unit <b>203</b> and the motion evaluating unit <b>109</b>.</p><p id="p-0079" num="0078">The single-directional predicted motion compensation unit <b>203</b> performs a motion-compensated prediction process based on the prediction parameter input from the prediction parameter control unit <b>204</b> and the reference image input from the reference image selector <b>205</b>, thereby generating a single-directional predicted image.</p><p id="p-0080" num="0079"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram that illustrates an example of the relation between motion vectors for a motion-compensated prediction in a bidirectional prediction according to the first embodiment. In the motion-compensated prediction, an interpolation process is performed using the reference image, and a single-directional predicted image is generated based on deviations of motions of the generated interpolated image and the input image from the pixel block located at the encoding target position. Here, the deviation is a motion vector. As illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, in the bidirectional prediction slice (B-slice), a predicted image is generated by using two types of reference images and a motion vector set. As the interpolation process, an interpolation process of &#xbd;-pixel precision, an interpolation process of &#xbc;-pixel precision, or the like is used, and, by performing a filtering process for the reference image, a value of the interpolated image is generated. For example, in H.264 in which an interpolation up to &#xbc;-pixel precision can be performed for a luminance signal, the deviation is represented as four times integer pixel precision.</p><p id="p-0081" num="0080">The single-directional predicted motion compensation unit <b>203</b> outputs a single-directional predicted image and temporarily stores the single-directional predicted image in the memory <b>202</b>. Here, in a case where the motion information (prediction parameter) represents a bi-directional prediction, the multi-frame motion compensation unit <b>201</b> makes a weighted prediction using two types of single-directional predicted images. Accordingly, the single-directional predicted motion compensation unit <b>203</b> stores a single-directional predicted image corresponding to the first type in the single-directional predicted image in the memory <b>202</b> and directly outputs a single-directional predicted image corresponding to the second type to the multi-frame motion compensation unit <b>201</b>. Here, the single-directional predicted image corresponding to the first type will be referred to as a first predicted image, and the single-directional predicted image corresponding to the second type will be referred to as a second predicted image.</p><p id="p-0082" num="0081">In addition, two single-directional motion compensation units <b>203</b> may be prepared and generate two single-directional predicted images. In such a case, when the motion information (prediction parameter) represents a single-directional prediction, the single-directional motion compensation unit <b>203</b> may directly output the first single-directional predicted image to the multi-frame motion compensation unit <b>201</b> as a first predicted image.</p><p id="p-0083" num="0082">The multi-frame motion compensation unit <b>201</b> makes a weighted prediction by using the first predicted image input from the memory <b>202</b>, the second predicted image input from the single-directional predicted motion compensation unit <b>203</b>, and the WP parameter information input from the motion evaluating unit <b>109</b>, thereby generating a predicted image. The multi-frame motion compensation unit <b>201</b> outputs the predicted image so as to be input to the subtraction unit <b>101</b> and the addition unit <b>106</b>.</p><p id="p-0084" num="0083"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a block diagram that illustrates an example of the configuration of the multi-frame motion compensation unit <b>201</b> according to the first embodiment. As illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the multi-frame motion compensation unit <b>201</b> includes: a default motion compensation unit <b>301</b>; a weighted motion compensation unit <b>302</b>; a WP parameter control unit <b>303</b>; and WP selectors <b>304</b> and <b>305</b>.</p><p id="p-0085" num="0084">The WP parameter control unit <b>303</b> outputs a WP application flag and weighting information based on the WP parameter information input from the motion evaluating unit <b>109</b> so as to input the WP application flag to the WP selectors <b>304</b> and <b>305</b> and input the weighting information to the weighted motion compensation unit <b>302</b>.</p><p id="p-0086" num="0085">Here, the WP parameter information includes information of the fixed point precision of the weighting factor, a first WP application flag, a first weighting factor, and a first offset corresponding to the first predicted image, and a second WP application flag, a second weighting factor, and a second offset corresponding to the second predicted image. The WP application flag is a parameter that can be set for each corresponding reference image and signal component and represents whether or not a weighted motion compensation prediction is made. The weighting information includes information of the fixed point precision of the weighting factor, the first weighting factor, the first offset, the second weighting factor, and the second offset.</p><p id="p-0087" num="0086">Described in detail, when the WP parameter information is input from the motion evaluating unit <b>109</b>, the WP parameter control unit <b>303</b> outputs the WP parameter information with being divided into the first WP application flag, the second WP application flag, and the weighting information, thereby inputting the first WP application flag to the WP selector <b>304</b>, inputting the second WP application flag to the WP selector <b>305</b>, and inputting the weighting information to the weighted motion compensation unit <b>302</b>.</p><p id="p-0088" num="0087">The WP selectors <b>304</b> and <b>305</b> change the connection ends of the predicted images based on the WP application flags input from the WP parameter control unit <b>303</b>. In a case where the corresponding WP application flag is &#x201c;0&#x201d;, each one of the WP selectors <b>304</b> and <b>305</b> connects the output end thereof to the default motion compensation unit <b>301</b>. Then, the WP selectors <b>304</b> and <b>305</b> output the first and second predicted images so as to be input to the default motion compensation unit <b>301</b>. On the other hand, in a case where the corresponding WP application flag is &#x201c;1&#x201d;, each one of the WP selectors <b>304</b> and <b>305</b> connects the output end thereof to the weighted motion compensation unit <b>302</b>. Then, the WP selectors <b>304</b> and <b>305</b> output the first and second predicted images so as to be input to the weighted motion compensation unit <b>302</b>.</p><p id="p-0089" num="0088">The default motion compensation unit <b>301</b> performs average processing based on the two single-directional predicted images (the first and second predicted images) input from the WP selectors <b>304</b> and <b>305</b>, thereby generating a predicted image. More specifically, in a case where the first and second WP application flags are &#x201c;0&#x201d;, the default motion compensation unit <b>301</b> performs average processing based on Numerical Expression (1).</p><p id="p-0090" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>P</i>[<i>x,y</i>]=Clip1((<i>PL</i>0[<i>x,y</i>]+<i>PL</i>1[<i>x,y</i>]+offset2)&#x3e;&#x3e;(shift2))&#x2003;&#x2003;(1)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0091" num="0089">Here, P[x, y] is a predicted image, PL0[x, y] is a first predicted image, and PL1[x, y] is a second predicted image. In addition, offset2 and shift2 are parameters of a rounding process in the average processing and are determined based on the internal calculation precision of the first and second predicted images. When the bit precision of the predicted image is L, and the bit precision of the first and second predicted images is M (L M), shift2 is formulated by Numerical Expression (2), and offset2 is formulated by Numerical Expression (3).</p><p id="p-0092" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>shift2=(<i>M&#x2212;L+</i>1)&#x2003;&#x2003;(2)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0093" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>offset2=(1&#x3c;&#x3c;(shift2&#x2212;1)&#x2003;&#x2003;(3)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0094" num="0090">For example, the bit precision of the predicted image is &#x201c;8&#x201d;, and the bit precision of the first and second predicted images is &#x201c;14&#x201d;, shift2=7 based on Numerical Expression (2), and offset2=(1&#x3c;&#x3c;6) based on Numerical Expression (3).</p><p id="p-0095" num="0091">In addition, in a case where the prediction mode represented by the motion information (prediction parameter) is the single-directional prediction, the default motion compensation unit <b>301</b> calculates a final predicted image using only the first predicted image based on Numerical Expression (4).</p><p id="p-0096" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>P</i>[<i>x,y</i>]=Clip1((<i>PLX</i>[<i>x,y</i>]+offset1)&#x3e;&#x3e;(shift1))&#x2003;&#x2003;(4)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0097" num="0092">Here, PLX[x, y] represents a single-directional predicted image (first predicted image), and X is an identifier representing either &#x201c;0&#x201d; or &#x201c;1&#x201d; as a reference list. For example, PLX[x, y] is PL0[x, y] in a case where the reference list is &#x201c;0&#x201d; and is PL1[x, y] in a case where the reference list is &#x201c;1&#x201d;. In addition, offset1 and shift1 are parameters for a rounding process and are determined based on the internal calculation precision of the first predicted image. When the bit precision of the predicted image is L, and the bit precision of the first predicted image is M, shift1 is formulated by Numerical Expression (5), and offset1 is formulated by Numerical Expression (6).</p><p id="p-0098" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>shift1=(<i>M&#x2212;L</i>)&#x2003;&#x2003;(5)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0099" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>offset1=(1&#x3c;&#x3c;(shift1&#x2212;1)&#x2003;&#x2003;(6)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0100" num="0093">For example, in a case where the bit precision of the predicted image is &#x201c;8&#x201d;, and the bit precision of the first predicted image is &#x201c;14&#x201d;, shift1=6 based on Numerical Expression (5), and offset1=(1&#x3c;&#x3c;5) based on Numerical Expression (6).</p><p id="p-0101" num="0094">The weighted motion compensation unit <b>302</b> performs weighted motion compensation based on the two single-directional predicted images (the first and second predicted images) input from the WP selectors <b>304</b> and <b>305</b> and the weighting information input from the WP parameter control unit <b>303</b>. More specifically, the weighted motion compensation unit <b>302</b> performs the weighting process based on Numerical Expression (7) in a case where the first and second WP application flags are &#x201c;1&#x201d;.</p><p id="p-0102" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>P</i>[<i>x,y</i>]=Clip1(((<i>PL</i>0[<i>x,y</i>]*<i>w</i><sub>0C</sub><i>+PL</i>1[<i>x,y</i>]*<i>w</i><sub>1c</sub>+(1&#x3c;&#x3c;log <i>WD</i><sub>C</sub>))&#x3e;&#x3e;(log <i>WD</i><sub>C</sub>+1))+((<i>o</i><sub>0c</sub><i>+o</i><sub>1c</sub>+1)&#x3c;&#x3c;1))&#x2003;&#x2003;(7)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0103" num="0095">Here, w<sub>0C </sub>represents a weighting factor corresponding to the first predicted image, w<sub>1C </sub>represents a weighting factor corresponding to the second predicted image, o<sub>0C </sub>represents an offset corresponding to the first predicted image, and o<sub>1C </sub>represents an offset corresponding to the second predicted image. Thereafter, they will be referred to as a first weighting factor, a second weighting factor, a first offset, and a second offset. log WD<sub>C </sub>is a parameter representing fixed point precision of each weighting factor. In addition, a variable C represents a signal component. For example, in the case of a YUV spatial signal, a luminance signal is represented by C=Y, a Cr color difference signal is represented by C=Cr, and a Cb color difference component is represented by C=Cb.</p><p id="p-0104" num="0096">In addition, in a case where the calculation precision of the first and second predicted images and the calculation precision of the predicted image are different from each other, the weighted motion compensation unit <b>302</b> realizes a rounding process by controlling log WD<sub>C</sub>, which is fixed point precision, as in Numerical Expression (8).</p><p id="p-0105" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>log <i>WD&#x2032;</i><sub>C</sub>=log <i>WD</i><sub>C</sub>+offset1&#x2003;&#x2003;(8)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0106" num="0097">The rounding process can be realized by replacing log WD<sub>C </sub>represented in Numerical Expression (7) with log WD&#x2032;<sub>C </sub>represented in Numerical Expression (8). For example, in a case where the bit precision of the predicted image is &#x201c;8&#x201d;, and the bit precision of the first and second predicted images is &#x201c;14&#x201d;, by resetting log WD<sub>C</sub>, it is possible to realize a batch rounding process for the calculation precision similar to that of shift2 represented in Numerical Expression (1).</p><p id="p-0107" num="0098">In addition, in a case where the prediction mode represented by the motion information (prediction parameter) is a single directional prediction, the weighted motion compensation unit <b>302</b> calculates a final predicted image using only the first predicted image based on Numerical Expression (9).</p><p id="p-0108" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>P</i>[<i>x,y</i>]=Clip1((<i>PLX</i>[<i>x,y</i>]*<i>w</i><sub>XC</sub>+(1&#x3c;&#x3c;log <i>WD</i><sub>C</sub>&#x2212;1))&#x3e;&#x3e;(log <i>WD</i><sub>C</sub>))&#x2003;&#x2003;(9)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0109" num="0099">Here, PLX[x, y] represents a single-directional predicted image (first predicted image), w<sub>XC </sub>represents a weighting factor corresponding to the single directional prediction, and X is an identifier representing either &#x201c;0&#x201d; or &#x201c;1&#x201d; as a reference list. For example, PLX[x, y] and w<sub>XC </sub>are PL0[x, y] and w<sub>0C </sub>in a case where the reference list is &#x201c;0&#x201d; and are PL1[x, y] and w<sub>1C </sub>in a case where the reference list is &#x201c;1&#x201d;.</p><p id="p-0110" num="0100">In addition, in a case where the calculation precision of the first and second predicted images and the calculation precision of the predicted image are different from each other, the weighted motion compensation unit <b>302</b> realizes a rounding process by controlling log WD<sub>C</sub>, which is fixed point precision, as in Numerical Expression (8), similarly to the case of the bi-directional prediction.</p><p id="p-0111" num="0101">The rounding process can be realized by replacing log WD<sub>C </sub>represented in Numerical Expression (7) with log WD&#x2032;<sub>C </sub>represented in Numerical Expression (8). For example, in a case where the bit precision of the predicted image is &#x201c;8&#x201d;, and the bit precision of the first and second predicted images is &#x201c;14&#x201d;, by resetting log WD<sub>C</sub>, it is possible to realize a batch rounding process for the calculation precision similar to that of shift1 represented in Numerical Expression (4).</p><p id="p-0112" num="0102"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is an explanatory diagram that illustrates an example of fixed point precision of a weighting factor according to the first embodiment and is a diagram that illustrates an example of changes in a moving image having a brightness change in the time direction and a gray scale value. In the example illustrated in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, an encoding target frame is Frame(t), a frame that is one frame before the encoding target frame in time is Frame(t&#x2212;1), and a frame that is one frame after the encoding target frame in time is Frame(t+1). As illustrated in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, in a fading image changing from white to black, the brightness (gray scale value) of the image decreases in accordance with elapse of time. The weighting factor represents the degree of change in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, and, as is apparent from Numerical Expressions (7) and (9), takes a value of &#x201c;1.0&#x201d; in a case where there is no change in the brightness. The fixed point precision is a parameter controlling an interval width corresponding to a decimal point of the weighing factor, and the weighting factor is 1&#x3c;&#x3c;log WD<sub>C </sub>in a case where there is no change in brightness.</p><p id="p-0113" num="0103">In addition, in the case of a single directional prediction, various parameters (the second WP application flag, the second weighting factor, and the second offset information) corresponding to the second predicted image are not used and may be set to initial values determined in advance.</p><p id="p-0114" num="0104">Referring back to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the motion evaluating unit <b>109</b> performs a motion evaluation between a plurality of frames based on an input image and a reference image input from the predicted image generating unit <b>107</b> and outputs the motion information and the WP parameter information, thereby inputting the motion information to the predicted image generating unit <b>107</b> and the encoding unit <b>110</b> and inputting the WP parameter information to the predicted image generating unit <b>107</b> and the index setting unit <b>108</b>.</p><p id="p-0115" num="0105">The motion evaluating unit <b>109</b> calculates an error, for example, by calculating differences between an input image of a prediction target pixel block and a plurality of reference images corresponding to the same position as a starting point, shifts the position with fractional precision, and calculates optimal motion information using a technique such as block matching for finding a block of a minimal error or the like. In the case of a bi-directional prediction, the motion evaluating unit <b>109</b> performs block matching including a default motion compensation prediction as represented in Numerical Expressions (1) and (4) using the motion information derived from the single-directional prediction, thereby calculating motion information of the bidirectional prediction.</p><p id="p-0116" num="0106">At this time, the motion evaluating unit <b>109</b> can calculate the WP parameter information by performing block matching including a weighted motion compensation prediction as represented in Numerical Expressions (7) and (9). In addition, for the calculation of the WP parameter information, a method of calculating a weighting factor or an offset using a brightness gradient of the input image, a method of calculating a weighting factor or an offset in accordance with the accumulation of a prediction error at the time of encoding, or the like may be used. Furthermore, as the WP parameter information, a fixed value determined in advance for each encoding device may be used.</p><p id="p-0117" num="0107">Here, a method of calculating a weighting factor, the fixed point precision of the weighting factor, and an offset from a moving image having a brightness change in time will be described with reference to <figref idref="DRAWINGS">FIG. <b>7</b></figref>. As described above, in the fading image changing from white to black as illustrated in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the brightness (gray scale value) of the image decreases in accordance with the elapse of time. The motion evaluating unit <b>109</b> can calculate the weighting factor by calculating the slope thereof.</p><p id="p-0118" num="0108">The fixed point precision of the weighting factor is information representing the precision of the slope, and the motion evaluating unit <b>109</b> can calculate an optimal value based on a distance to the reference image in time and the degree of change of the image brightness. For example, in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, in a case where the weighting factor between Frame(t&#x2212;1) and Frame(t+1) is 0.75 with fractional precision, &#xbe; can be represented in the case of &#xbc; precision, and accordingly, the motion evaluating unit <b>109</b> sets the fixed point precision to 2 (1&#x3c;&#x3c;2). Since the value of the fixed point precision influences on the code amount of a case where the weighting factor is encoded, as the value of the fixed point precision, an optimal value may be selected in consideration of the code amount and the prediction precision. In addition, the value of the fixed point precision may be a fixed value determined in advance.</p><p id="p-0119" num="0109">In addition, in a case where the slope is not matched, the motion evaluating unit <b>109</b> can calculate the value of the offset by acquiring a correction value (deviation amount) corresponding to the intercept of the linear function. For example, in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, in a case where a weighing factor between Frame(t&#x2212;1) and Frame(t+1) is 0.60 with decimal point precision, and the fixed point precision is &#x201c;1&#x201d; (1&#x3c;&#x3c;1), there is a high possibility that the weighting factor is set to &#x201c;1&#x201d; (corresponding to decimal point precision of 0.50 of the weighting factor). In such a case, since the decimal point precision of the weighting factor deviates from 0.60, which is an optimal value, by 0.10, the motion evaluating unit <b>109</b> calculates a correction value corresponding thereto based on a maximum value of the pixel and is set as the value of the offset. In a case where the maximum value of the pixel is 255, the motion evaluating unit <b>109</b> may set a value such as 25 (255&#xd7;0.1).</p><p id="p-0120" num="0110">In the first embodiment, although the motion evaluating unit <b>109</b> is represented as one function of the encoding device <b>100</b> as an example, the motion evaluating unit <b>109</b> is not an essential configuration of the encoding device <b>100</b>, and, for example, the motion evaluating unit <b>109</b> may be a device other than the encoding device <b>100</b>. In such a case, the motion information and the WP parameter information calculated by the motion evaluating unit <b>109</b> may be loaded into the encoding device <b>100</b>.</p><p id="p-0121" num="0111">The index setting unit <b>108</b> receives the WP parameter information input from the motion evaluating unit <b>109</b>, checks a reference list (list number) and a reference image (reference number), and outputs index information so as to be input to the index setting unit <b>108</b>.</p><p id="p-0122" num="0112"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a block diagram that illustrates an example of the configuration of the index setting unit <b>108</b> according to the first embodiment. The index setting unit <b>108</b>, as illustrated in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, includes a reference image checking unit <b>401</b> and an index generating unit <b>402</b>.</p><p id="p-0123" num="0113">The reference image checking unit <b>401</b> receives the WP parameter information input from the motion evaluating unit <b>109</b> and checks whether or not there are WP parameters representing a reference image having the same reference number included in two reference lists. Then, the reference image checking unit <b>401</b> removes the WP parameter representing the same reference image included in the WP parameter information and outputs the WP parameter information after removal so as to be input to the index generating unit <b>402</b>.</p><p id="p-0124" num="0114">The index generating unit <b>402</b> receives the WP parameter information in which the redundant WP parameter has been removed from the reference image checking unit <b>401</b> and generates index information by mapping the WP parameter information into a syntax element to be described later. The index generating unit <b>402</b> outputs the index information so as to be input to the encoding unit <b>110</b>.</p><p id="p-0125" num="0115"><figref idref="DRAWINGS">FIGS. <b>9</b>A and <b>9</b>B</figref> are diagrams illustrating examples of the WP parameter information according to the first embodiment. An example of the WP parameter information at the time of P-slice is as illustrated in <figref idref="DRAWINGS">FIG. <b>9</b>A</figref>, and an example of the WP parameter information at the time of B-slice is as illustrated in <figref idref="DRAWINGS">FIGS. <b>9</b>A and <b>9</b>B</figref>. A list number is an identifier representing a prediction direction. The list number has a value of &#x201c;0&#x201d; in the case of a single-directional prediction. On the other hand, in the case of a bi-directional prediction, two types of prediction can be used, and accordingly, the list number has two values of &#x201c;0&#x201d; and &#x201c;1&#x201d;. A reference number is a value corresponding to any one of 1 to N represented in the frame memory <b>206</b>. Since the WP parameter information is maintained for each reference list and reference image, in a case where there are N reference images, 2N pieces of information are necessary at the time of B-slice. The reference image checking unit <b>401</b> reconverts the WP parameter information and removes the redundant WP parameter.</p><p id="p-0126" num="0116"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a diagram that illustrates an example of an encoding order and a display order (POC: picture order count) in a low-delay encoding structure according to the first embodiment and illustrates an example of an encoding structure in which a B-slice that can be a reference image is used as an rB-slice. In the low-delay encoding structure, an image cannot be predicted, and the encoding order and the display order are the same. Here, a case will be considered in which frames 0 to 3 have been encoded in the display order, and a frame 4 is encoded.</p><p id="p-0127" num="0117"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a diagram that illustrates an example of the relation between a reference image and a reference number in the low-delay encoding structure according to the first embodiment and illustrates the relation between a reference image and a reference number of a case where the frame 4 is encoded using simplified WP parameter information. In the example illustrated in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, the order of reference numbers within a reference list is the same in Lists 0 and 1, which represents that the reference images of Lists 0 and 1 are the same.</p><p id="p-0128" num="0118"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a diagram that illustrates an example of an encoding order and a display order in a random access encoding structure according to the first embodiment. Here, a case will be considered in which frames 0, 2, 4, and 8 have been encoded in the display order, and a frame 1 is encoded.</p><p id="p-0129" num="0119"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a diagram that illustrates an example of the relation between a reference image and a reference number in the random access encoding structure according to the first embodiment and illustrates the relation between a reference image and a reference number of a case where a frame 1 is encoded using simplified WP parameter information. In the example illustrated in <figref idref="DRAWINGS">FIG. <b>13</b></figref>, while the orders of reference numbers for the reference lists are different from each other, common four reference images are included in Lists 0 and 1, which represents the reference images of Lists 0 and 1 are the same.</p><p id="p-0130" num="0120">In order to remove redundant WP parameters, in other words, the WP parameters in which reference numbers included in two reference lists represent the same reference images, the reference image checking unit <b>401</b> sorts the reference numbers so as to be converted into a common new index.</p><p id="p-0131" num="0121"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a diagram that illustrates an example of the scanning order of a list number and a reference number of a reference image according to the first embodiment. The reference image checking unit <b>401</b>, in the scanning order illustrated in <figref idref="DRAWINGS">FIG. <b>14</b></figref>, a two-dimensional reference list (two reference lists) is converted into a one-dimensional common list (one common list). More specifically, the reference image checking unit <b>401</b> reads the WP parameter information represented in <figref idref="DRAWINGS">FIGS. <b>9</b>A and <b>9</b>B</figref> in the scanning order illustrated in <figref idref="DRAWINGS">FIG. <b>14</b></figref>, sorts the two reference lists into a common list, and removes redundant WP parameters. Here, the reference image checking unit <b>401</b> reads the WP parameter information in accordance with a pseudo code represented in Numerical Expression (10).</p><p id="p-0132" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mi>for</mi>     <mo>&#x2062;</mo>     <mtext>   </mtext>     <mrow>      <mo>(</mo>      <mrow>       <mrow>        <mi>scan</mi>        <mo>=</mo>        <mn>0</mn>       </mrow>       <mo>;</mo>       <mrow>        <mi>scan</mi>        <mo>&#x3c;=</mo>        <mrow>         <mi>num_of</mi>         <mo>&#x2062;</mo>         <mi>_common</mi>         <mo>&#x2062;</mo>         <mi>_active</mi>         <mo>&#x2062;</mo>         <mi>_ref</mi>         <mo>&#x2062;</mo>         <mi>_minus1</mi>        </mrow>       </mrow>       <mo>;</mo>       <mtext></mtext>       <mrow>        <mi>scan</mi>        <mo>&#x2062;</mo>        <mrow>         <mo>+</mo>         <mo>+</mo>        </mrow>       </mrow>      </mrow>      <mo>)</mo>     </mrow>     <mo>&#x2062;</mo>     <mtext>   </mtext>     <mrow>      <mo>{</mo>      <mtext></mtext>      <mrow>       <mrow>        <mi>list</mi>        <mo>=</mo>        <mrow>         <mi>common_scan</mi>         <mo>&#x2062;</mo>         <mi>_list</mi>         <mo>&#x2062;</mo>         <mrow>          <mo>(</mo>          <mi>scan</mi>          <mo>)</mo>         </mrow>        </mrow>       </mrow>       <mo>&#x2062;</mo>       <mtext></mtext>       <mrow>        <mi>ref_idx</mi>        <mo>=</mo>        <mrow>         <mi>common_scan</mi>         <mo>&#x2062;</mo>         <mi>_ref</mi>         <mo>&#x2062;</mo>         <mi>_idx</mi>         <mo>&#x2062;</mo>         <mrow>          <mo>(</mo>          <mi>scan</mi>          <mo>)</mo>         </mrow>        </mrow>       </mrow>       <mo>&#x2062;</mo>       <mtext></mtext>       <mrow>        <mi>refPOC</mi>        <mo>=</mo>        <mrow>         <mi>RefPicOrderCount</mi>         <mo>&#x2061;</mo>         <mo>(</mo>         <mrow>          <mi>list</mi>          <mo>,</mo>          <mi>ref_idx</mi>         </mrow>         <mo>)</mo>        </mrow>       </mrow>       <mo>&#x2062;</mo>       <mtext></mtext>       <mrow>        <mrow>         <mrow>          <mi>identical_ref</mi>          <mo>&#x2062;</mo>          <mi>_flag</mi>         </mrow>         <mo>=</mo>         <mi>false</mi>        </mrow>        <mo>;</mo>       </mrow>       <mo>&#x2062;</mo>       <mtext></mtext>       <mrow>        <mi>for</mi>        <mo>&#x2062;</mo>        <mtext>   </mtext>        <mrow>         <mo>(</mo>         <mrow>          <mrow>           <mi>currIdx</mi>           <mo>=</mo>           <mn>0</mn>          </mrow>          <mo>;</mo>          <mrow>           <mi>currIdx</mi>           <mo>&#x3c;=</mo>           <mrow>            <mi>num_of</mi>            <mo>&#x2062;</mo>            <mi>_common</mi>            <mo>&#x2062;</mo>            <mi>_active</mi>            <mo>&#x2062;</mo>            <mi>_ref</mi>            <mo>&#x2062;</mo>            <mi>_minus1</mi>           </mrow>          </mrow>          <mo>;</mo>          <mtext></mtext>          <mrow>           <mi>currIdx</mi>           <mo>++</mo>          </mrow>         </mrow>         <mo>)</mo>        </mrow>        <mo>&#x2062;</mo>        <mtext>   </mtext>        <mrow>         <mo>{</mo>         <mtext></mtext>         <mtext> </mtext>         <mrow>          <mrow>           <mi>if</mi>           <mo>&#x2061;</mo>           <mo>(</mo>           <mrow>            <mi>refPOC</mi>            <mo>==</mo>            <mrow>             <mi>CommonRefPicOrderCount</mi>             <mo>&#x2061;</mo>             <mo>(</mo>             <mi>currIdx</mi>             <mo>)</mo>            </mrow>           </mrow>           <mo>)</mo>          </mrow>          <mo>&#x2062;</mo>          <mtext>   </mtext>          <mrow>           <mo>{</mo>           <mtext></mtext>           <mrow>            <mrow>             <mrow>              <mi>identical_ref</mi>              <mo>&#x2062;</mo>              <mi>_flag</mi>             </mrow>             <mo>=</mo>             <mi>true</mi>            </mrow>            <mo>;</mo>           </mrow>           <mtext></mtext>           <mo>}</mo>          </mrow>         </mrow>         <mtext></mtext>         <mo>}</mo>        </mrow>       </mrow>       <mo>&#x2062;</mo>       <mtext></mtext>       <mrow>        <mi>if</mi>        <mo>&#x2061;</mo>        <mo>(</mo>        <mrow>         <mo>!</mo>         <mrow>          <mi>identical_ref</mi>          <mo>&#x2062;</mo>          <mi>_flag</mi>         </mrow>        </mrow>        <mo>)</mo>       </mrow>       <mo>&#x2062;</mo>       <mtext></mtext>       <mrow>        <mrow>         <mi>InsertCommonRefPicOrderCount</mi>         <mo>&#x2061;</mo>         <mo>(</mo>         <mrow>          <mi>scan</mi>          <mo>,</mo>          <mi>refPOC</mi>         </mrow>         <mo>)</mo>        </mrow>        <mo>;</mo>       </mrow>      </mrow>      <mtext></mtext>      <mo>}</mo>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>10</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0133" num="0122">Here, common_scan_list( ) is a function for returning a list number corresponding to a scanning number illustrated in <figref idref="DRAWINGS">FIG. <b>14</b></figref>. In addition, common_scan_ref_idx( ) is a function for returning a reference number corresponding to a scanning number illustrated in <figref idref="DRAWINGS">FIG. <b>14</b></figref>. RefPicOrderCnt( ) is a function for returning a POC number corresponding to a list number and a reference number. Here, the POC number is a number that represents the display order of reference images. When exceeding a maximum number determined in advance, the POC number is mapped into an initial value. For example, in a case where the maximum value of the POC number is 255, a POC number corresponding to 256 is 0. CommonRefPicOrderCount( ) is a function for returning a POC number corresponding to a reference number of a common list (CommonList). InsertCommonRefPicOrderCount( ) is a function for inserting a POC number corresponding to a scanning number into a common list (CommonList).</p><p id="p-0134" num="0123">Here, the scanning order illustrated in <figref idref="DRAWINGS">FIG. <b>14</b></figref> is an example, any other scanning order may be used as long as it is a scanning order determined in advance. In addition, the pseudo code represented in Numerical Expression (10) is an example, and addition of a process or removal of a redundant process can be performed as long as the purpose of this process can be realized.</p><p id="p-0135" num="0124"><figref idref="DRAWINGS">FIGS. <b>15</b> and <b>16</b></figref> are diagrams that illustrate an example of the WP parameter information after a common list conversion according to the first embodiment in a simplified manner. <figref idref="DRAWINGS">FIG. <b>15</b></figref> illustrates the WP parameter information of a common list that is acquired by converting the WP parameter information of the reference list illustrated in <figref idref="DRAWINGS">FIG. <b>11</b></figref> in a simplified manner, and <figref idref="DRAWINGS">FIG. <b>16</b></figref> illustrates the WP parameter information of a common list that is acquired by converting the WP parameter information of the reference list illustrated in <figref idref="DRAWINGS">FIG. <b>13</b></figref> in a simplified manner.</p><p id="p-0136" num="0125"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is a flowchart that illustrates an example of the process of generating index information that is performed by the index setting unit <b>108</b> according to the first embodiment.</p><p id="p-0137" num="0126">When WP parameter information is input, the reference image checking unit <b>401</b> branches the process in accordance with the type of a slice (Step S<b>101</b>).</p><p id="p-0138" num="0127">In a case where the slice type is a single-directional prediction slice (P-slice) using only one reference list (No in Step S<b>101</b>), the reference image checking unit <b>401</b> directly outputs the WP parameter information illustrated in <figref idref="DRAWINGS">FIG. <b>9</b>A</figref> to the index generating unit <b>402</b>. The index generating unit <b>402</b> generates index information by mapping the WP parameter information input from the reference image checking unit <b>401</b> into a predetermined syntax element to be described later and outputs the generated index information.</p><p id="p-0139" num="0128">On the other hand, in a case where the slice type is a bi-directional prediction slice (B-slice) using two reference lists (Yes in Step S<b>101</b>), the reference image checking unit <b>401</b> initializes a variable scan to zero (Step S<b>102</b>). The variable scan represents a scanning number illustrated in <figref idref="DRAWINGS">FIG. <b>14</b></figref>.</p><p id="p-0140" num="0129">Subsequently, the reference image checking unit <b>401</b> acquires a variable list representing a list number corresponding to a scanning number using the variable scan (Step S<b>103</b>) and acquires a variable refIdx representing a reference number corresponding to the scanning number (Step S<b>104</b>).</p><p id="p-0141" num="0130">Subsequently, the reference image checking unit <b>401</b> derives a variable refPOC representing a POC number of the reference image corresponding to a list number represented by the variable list and the reference number represented by the variable refIdx (Step S<b>105</b>).</p><p id="p-0142" num="0131">Subsequently, the reference image checking unit <b>401</b> sets a flag identical_ref_flag to false (Step S<b>106</b>) and sets a variable currIdx to &#x201c;0&#x201d; (Step S<b>107</b>). The flag identical_ref_flag represents whether or not there is the same reference image in the common list. The variable currIdx represents a reference number of the common list.</p><p id="p-0143" num="0132">Subsequently, the reference image checking unit <b>401</b> determines whether or not the POC number represented by the derived variable refPOC and the POC number of the reference image corresponding to the reference number represented by the variable currIdx are the same (Step S<b>108</b>).</p><p id="p-0144" num="0133">In a case where both the POC numbers are the same (Yes in Step S<b>108</b>), the reference image checking unit <b>401</b> sets the flag identical_ref_flag to true (Step S<b>109</b>). On the other hand, in a case where both the POC numbers are not the same (No in Step S<b>108</b>), the process of Step S<b>109</b> is not performed.</p><p id="p-0145" num="0134">Subsequently, the reference image checking unit <b>401</b> increments the variable currIdx (Step S<b>110</b>).</p><p id="p-0146" num="0135">Subsequently, the reference image checking unit <b>401</b> determines whether the value of the variable currIdx is larger than num_of_common_active_ref_minus1 that is a value acquired by subtracting one from a maximum number of the common list (Step S<b>111</b>). Then, when the value is num_of_common_active_ref_minus1 or less (No in Step S<b>111</b>), the process of Steps S<b>108</b> to S<b>110</b> is repeated.</p><p id="p-0147" num="0136">When the value is larger than num_of_common_active_ref_minus1 (Yes in Step S<b>111</b>), the reference image checking unit <b>401</b> completes the checking of the common list and further checks whether or not the flag identical_ref_flag is false (Step S<b>112</b>).</p><p id="p-0148" num="0137">In a case where the flag identical_ref_flag is false (Yes in Step S<b>112</b>), the reference image checking unit <b>401</b> determines that a reference image that is the same as the reference image of the POC number represented by the variable refPOC is not included in the common list and adds the POC number represented by the variable refPOC to the common list (Step S<b>113</b>). On the other hand, in a case where the flag identical_ref_flag is true (No in Step S<b>112</b>), the reference image checking unit <b>401</b> determines that a reference image that is the same as the reference image of the POC number represented by the variable refPOC is included in the common list and does not perform the process of Step S<b>113</b>.</p><p id="p-0149" num="0138">Subsequently, the reference image checking unit <b>401</b> increments the variable scan (Step S<b>114</b>).</p><p id="p-0150" num="0139">Subsequently, the reference image checking unit <b>401</b> determines whether the value of the variable scan is num_of_common_active_ref_minus1, which is a value acquired by subtracting one from a maximum number of the common list, or more (Step S<b>115</b>). In a case where the value of the variable scan is num_of_common_active_ref_minus1 or less (No in Step S<b>115</b>), the process is returned to Step S<b>103</b>. On the other hand, in a case where the value of the variable scan is larger than num_of_common_active_ref_minus1 (Yes in Step S<b>115</b>), the reference image checking unit <b>401</b> outputs the WP parameter information after the conversion into the common list to the index generating unit <b>402</b>. The index generating unit <b>402</b> generates index information by mapping the WP parameter information input from the reference image checking unit <b>401</b> into a predetermined syntax element to be described later and outputs the generated index information.</p><p id="p-0151" num="0140">Referring back to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the encoding unit <b>110</b> performs entropy encoding of various encoding parameters such as the quantization transformation coefficient input from the quantization unit <b>103</b>, the motion information input from the motion evaluating unit <b>109</b>, the index information input from the index setting unit <b>108</b>, and the quantization information designated by the encoding control unit <b>111</b>, thereby generating encoded data. As the entropy encoding, for example, there is a Huffman encoding or arithmetic coding.</p><p id="p-0152" num="0141">Here, the encoding parameters are parameters such as prediction information representing a prediction method or the like, information relating to the quantization transformation coefficient, and information relating to quantization that are necessary for a decoding process. For example, it may be configured such that an internal memory not illustrated in the figure is included in the encoding control unit <b>111</b>, the encoding parameters are maintained in the internal memory, and the encoding parameters of an adjacent pixel block, which has been completed to be encoded, is used when a pixel block is encoded. For example, in an intra-prediction of H.264, prediction information of a pixel block may be derived from the prediction information of an adjacent block that has been completed to be encoded.</p><p id="p-0153" num="0142">The encoding unit <b>110</b> outputs the generated encoded data at appropriate output timing managed by the encoding control unit <b>111</b>. Various kinds of information, which is output encoded data, for example, is multiplexed by a multiplexing unit not illustrated in the figure or the like, is temporarily stored in an output buffer not illustrated in the figure or the like, and, then, for example, is output to a storage system (storage medium) or a transmission system (communication line).</p><p id="p-0154" num="0143"><figref idref="DRAWINGS">FIG. <b>18</b></figref> is a diagram that illustrates an example of syntax <b>500</b> used by the encoding device <b>100</b> according to the first embodiment. The syntax <b>500</b> illustrates the structure of encoded data generated by encoding an input image (moving image data) using the encoding device <b>100</b>. When the encoded data is decoded, a decoding device to be described later performs a syntax analysis of a moving image by referring to a syntax structure that is the same as that of the syntax <b>500</b>.</p><p id="p-0155" num="0144">The syntax <b>500</b> includes three parts including a high-level syntax <b>501</b>, a slice-level syntax <b>502</b>, and a coding tree level syntax <b>503</b>. The high-level syntax <b>501</b> includes syntax information of an upper layer that has a level higher than the slice. Here, the slice represents a rectangular area or a continuous area included in a frame or a field. The slice-level syntax <b>502</b> includes information that is necessary for decoding each slice. The coding tree level syntax <b>503</b> includes information that is necessary for decoding each coding tree (in other words, each coding tree block). Each of these parts includes more detailed syntax.</p><p id="p-0156" num="0145">The high-level syntax <b>501</b> includes syntax of a sequence and a picture level such as a sequence parameter set syntax <b>504</b>, a picture parameter set syntax <b>505</b>, and an adaptation parameter set syntax <b>506</b>.</p><p id="p-0157" num="0146">The slice-level syntax <b>502</b> includes a slice header syntax <b>507</b>, a pred weight table syntax <b>508</b>, a slice data syntax <b>509</b>, and the like. The pred weight table syntax <b>508</b> is called from the slice header syntax <b>507</b>.</p><p id="p-0158" num="0147">The coding tree level syntax <b>503</b> includes a coding tree unit syntax <b>510</b>, a transform unit syntax <b>511</b>, a prediction unit syntax <b>512</b>, and the like. The coding tree unit syntax <b>510</b> may have a quadtree structure. More specifically, the coding tree unit syntax <b>510</b> may be recursively further called as a syntax element of the coding tree unit syntax <b>510</b>. In other words, one coding tree block may be subdivided into quadtrees. In addition, the transform unit syntax <b>511</b> is included in the coding tree unit syntax <b>510</b>. The transform unit syntax <b>511</b> is called from each coding tree unit syntax <b>510</b> located at a tail end of the quadtree. In the transform unit syntax <b>511</b>, information relating to inverse orthogonal transformation, quantization, and the like is described. In the syntax, information relating to the weighted motion compensation prediction may be described.</p><p id="p-0159" num="0148"><figref idref="DRAWINGS">FIG. <b>19</b></figref> is a diagram that illustrates an example of the picture parameter set syntax <b>505</b> according to the first embodiment. Here, weighted_pred_flag, for example, is a syntax element representing the validness or invalidness of a weighted compensation prediction according to the first embodiment for a P-slice. In a case where the weighted_pred_flag is &#x201c;0&#x201d;, the weighted motion compensation prediction according to the first embodiment within the P-slice is invalid. Accordingly, the WP application flag included in the WP parameter information is constantly set to &#x201c;0&#x201d;, and the output ends of the WP selectors <b>304</b> and <b>305</b> are connected to the default motion compensation unit <b>301</b>. On the other hand, in a case where the weighted_pred_flag is &#x201c;1&#x201d;, the weighted motion compensation prediction according to the first embodiment within the P-slice is valid.</p><p id="p-0160" num="0149">As another example, in a case where the weighted_pred_flag is &#x201c;1&#x201d;, the validness or invalidness of the weighted motion compensation prediction according to the first embodiment may be defined for each local area within the slice in the syntax of a lower layer (the slice header, the coding tree block, the transform unit, the prediction unit, and the like).</p><p id="p-0161" num="0150">In addition, weighted_bipred_idc, for example, is a syntax element representing the validness or invalidness of a weighted compensation prediction according to the first embodiment for a B-slice. In a case where the weighted_bipred_idc is &#x201c;0&#x201d;, the weighted motion compensation prediction according to the first embodiment within the B-slice is invalid. Accordingly, the WP application flag included in the WP parameter information is constantly set to &#x201c;0&#x201d;, and the output ends of the WP selectors <b>304</b> and <b>305</b> are connected to the default motion compensation unit <b>301</b>. On the other hand, in a case where the weighted_bipred_idc is &#x201c;1&#x201d;, the weighted motion compensation prediction according to the first embodiment within the B-slice is valid.</p><p id="p-0162" num="0151">As another example, in a case where the weighted_bipred_idc is &#x201c;1&#x201d;, the validness or invalidness of the weighted motion compensation prediction according to the first embodiment may be defined for each local area within the slice in the syntax of a lower layer (the slice header, the coding tree block, the transform unit, the prediction unit, and the like).</p><p id="p-0163" num="0152"><figref idref="DRAWINGS">FIG. <b>20</b></figref> is a diagram that illustrates an example of the slice header syntax <b>507</b> according to the first embodiment. Here, slice-type represents the type (an I-slice, a P-slice, a B-slice, or the like) of slice. In addition, pic_parameter_set_id is an identifier representing which picture parameter set syntax <b>505</b> to be referred. num_ref_idx_active_override_flag is a flag representing whether to update the number of valid reference images, and, in a case where this flag is &#x201c;1&#x201d;, num_ref_idx_l0_active_minus1 and num_ref_idx_l1_active_minus1 that define the numbers of reference images of the reference list may be used. In addition, pred weight table( ) is a function representing the pred weight table syntax used for a weighted motion compensation prediction, and this function is called in a case where the weighted_pred_flag is &#x201c;1&#x201d; in the case of a P-slice and a case where weighted_bipred_idc is &#x201c;1&#x201d; in the case of a B-slice.</p><p id="p-0164" num="0153"><figref idref="DRAWINGS">FIG. <b>21</b></figref> is a diagram that illustrates an example of the pred weight table syntax <b>508</b> according to the first embodiment. Here, luma_log2_weight_denom represents the fixed point precision of the weighting factor of the luminance signal in a slice and is a value corresponding to log WD<sub>C </sub>represented in Numerical Expression (7) or (9). In addition, chroma_log2_weight_denom represents the fixed point precision of the weighting factor of a color difference signal in a slice and is a value corresponding to log WD<sub>C </sub>represented in Numerical Expression (7) or (9). chroma_format_idc is an identifier representing a color space, and MONO IDX is a value representing a monochrome video. In addition, num_ref_common_active_minus1 represents a value that is acquired by subtracting one from the number of reference images included in a common list in a slice and is used in Numerical Expression (10). A maximum value of this value is acquired by adding maximum values of the numbers of reference images of two Lists 0 and 1.</p><p id="p-0165" num="0154">luma_weight_common_flag represents a WP application flag of a luminance signal. In a case where this flag is &#x201c;1&#x201d;, a weighted motion compensation prediction of the luminance signal according to the first embodiment is valid for the whole area within the slice. In addition, chroma_weight_common_flag represents a WP application flag of a color difference signal. In a case where this flag is &#x201c;1&#x201d;, a weighted motion compensation prediction of a color difference signal according to the first embodiment is valid for the whole area within the slice. Here, luma_weight_common[i] is a weighting factor of the i-th luminance signal managed in the common list. In addition, luma_offset_common[i] is an offset of the i-th luminance signal managed in the common list. These are values corresponding to w<sub>0C</sub>, w<sub>1C</sub>, o<sub>0C</sub>, and o<sub>1C </sub>represented in Numerical Expression (7) or (9). Here, C=Y.</p><p id="p-0166" num="0155">Here, chroma_weight_common[i] [j] is a weighting factor of the i-th color difference signal managed in the common list. In addition, chroma_offset_common[i] [j] is an offset of the i-th color difference signal managed in the common list. These are values corresponding to w<sub>0C</sub>, w<sub>1C</sub>, o<sub>0C</sub>, o<sub>1C </sub>represented in Numerical Expression (7) or (9). Here, C=Cr or Cb. In addition, j represents a component of the color difference, and, for example, in the case of a signal of YUV 4:2:0, j=0 represents a Cr component, and j=1 represents a Cb component.</p><p id="p-0167" num="0156">As above, in the first embodiment, in a case where there are combinations including the same reference image in two reference lists of Lists 0 and 1 in the WP parameter information, the index setting unit <b>108</b> converts the two reference lists into a common list, thereby removing a redundant WP parameter that is duplicate based on the WP parameter information and generating index information. Therefore, according to the first embodiment, the code amount of the index information can be reduced.</p><p id="p-0168" num="0157">Particularly, since the weighting factor and the offset need to be encoded for each reference image, the amount of information to be signaled to the decoder increases in accordance with an increase in the number of reference images. However, as the number of reference images increases, the number of cases where the same reference image is referred to in the reference list increases, and accordingly, by managing a common list, an advantage of markedly reducing the code amount can be expected.</p><heading id="h-0007" level="1">Modification of First Embodiment</heading><p id="p-0169" num="0158">A modification of the first embodiment will be described. In the modification of the first embodiment, syntax elements used by the index generating unit <b>402</b> are different from those of the first embodiment.</p><p id="p-0170" num="0159"><figref idref="DRAWINGS">FIG. <b>22</b></figref> is a diagram that illustrates an example of a sequence parameter set syntax <b>504</b> according to a modification of the first embodiment. Here, profile_idc is an identifier representing information relating to a profile of encoded data. level_idc is an identifier representing information relating to a level of encoded data. In addition, seq_parameter_set_id is an identifier representing a sequence parameter set syntax <b>504</b> that is referred to. num_ref_frames is a variable representing a maximal number of reference images in a frame. In addition, weighted_prediction_enabled_flag, for example, is a syntax element representing the validness/invalidness of a weighted motion compensation prediction according to the modification for encoded data.</p><p id="p-0171" num="0160">In a case where weighted_prediction_enabled_flag is &#x201c;0&#x201d;, a weighted motion compensation prediction according to the modification in the encoded data is invalid. Accordingly, the WP application flag included in the WP parameter information is constantly set to &#x201c;0&#x201d;, and the WP selectors <b>304</b> and <b>305</b> connect the output ends thereof to the default motion compensation unit <b>301</b>. On the other hand, in a case where weighted_prediction_enabled_flag is &#x201c;1&#x201d;, a weighted motion compensation prediction according to the modification is valid in the whole area of the encoded data.</p><p id="p-0172" num="0161">As another example, in a case where weighted_prediction_enabled_flag is &#x201c;1&#x201d;, the validness/invalidness of a weighted motion compensation prediction according to the modification may be defined for each local area inside the slice in syntax of a further lower layer (a picture parameter set, an adaptation parameter set, a slice header, a coding tree block, a transform unit, a prediction unit, and the like).</p><p id="p-0173" num="0162"><figref idref="DRAWINGS">FIG. <b>23</b></figref> is a diagram that illustrates an example of the adaptation parameter set syntax <b>506</b> according to a modification of the first embodiment. The adaptation parameter set syntax <b>506</b> maintains a parameter influencing the whole encoding frame and is independently encoded as a higher-level unit. For example, in H.264, a syntax element corresponding to a high-level syntax <b>501</b> is encoded as a NAL unit. Accordingly, the encoding unit of the adaptation parameter set syntax <b>506</b> is different from that of syntax of a level lower than the level of the slice-level syntax <b>502</b> that maintains a parameter of lower-level information represented by a slice or a pixel block</p><p id="p-0174" num="0163">Here, aps_id is an identifier representing an adaptation parameter set syntax <b>506</b> that is referred to. By referring to this identifier, a lower-level slice can refer to an arbitrary aps_id that has been encoded. In such a case, for example, by adding the same aps_id to the slice header syntax <b>507</b> represented in <figref idref="DRAWINGS">FIG. <b>20</b></figref>, a parameter influencing the whole frame can be read.</p><p id="p-0175" num="0164">aps_weighted_prediction_flag, for example, is a syntax element representing the validness or invalidness of a weighted motion compensation prediction according to the modification for a P-slice in a frame. In a case where aps_weighted_prediction_flag is &#x201c;0&#x201d;, a weighted motion compensation prediction according to the modification for a P-slice within the frame is invalid. Accordingly, the WP application flag included in the WP parameter information is constantly set to &#x201c;0&#x201d;, and the WP selectors <b>304</b> and <b>305</b> connect the output ends thereof to the default motion compensation unit <b>301</b>. On the other hand, in a case where aps_weighted_prediction_flag is &#x201c;1&#x201d;, a weighted motion compensation prediction according to the modification is valid for the whole area within the frame.</p><p id="p-0176" num="0165">In addition, as another example, in a case where aps_weighted_prediction_flag is &#x201c;1&#x201d;, in syntax of a lower layer (a slice header, a coding tree block, a transform unit, a prediction unit, and the like), the validness or invalidness of a weight motion compensation prediction according to the modification may be defined for each local area inside the slice.</p><p id="p-0177" num="0166">aps_weighted_bipred_idx, for example, is a syntax element representing the validness or invalidness of a weighted motion compensation prediction according to the modification for a B-slice in a frame. In a case where aps_weighted_bipred_idx is &#x201c;0&#x201d;, a weighted motion compensation prediction according to the modification for a P-slice within the frame is invalid. Accordingly, the WP application flag included in the WP parameter information is constantly set to &#x201c;0&#x201d;, and the WP selectors <b>304</b> and <b>305</b> connect the output ends thereof to the default motion compensation unit <b>301</b>. On the other hand, in a case where aps_weighted_bipred_idx is &#x201c;1&#x201d;, a weighted motion compensation prediction according to the modification is valid for the whole area within the frame.</p><p id="p-0178" num="0167">In addition, as another example, in a case where aps_weighted_bipred_idx is &#x201c;1&#x201d;, in syntax of a lower layer (a slice header, a coding tree block, a transform unit, a prediction unit, and the like), the validness or invalidness of a weight motion compensation prediction according to the modification may be defined for each local area inside the slice.</p><p id="p-0179" num="0168">Furthermore, pred weight table( ) is a function representing a pred weight table syntax used for a weighted motion compensation prediction, and this function is called in a case where aps_weighted_prediction_flag described above is &#x201c;1&#x201d; or aps_weighted_bipred_idx is &#x201c;1&#x201d;.</p><p id="p-0180" num="0169"><figref idref="DRAWINGS">FIG. <b>24</b></figref> is a diagram that illustrates an example of the pred weight table syntax <b>508</b> according to a modification of the first embodiment. In the modification of the first embodiment, in the syntax structure illustrated in <figref idref="DRAWINGS">FIG. <b>18</b></figref>, the pred weight table syntax <b>508</b> is called from the adaptation parameter set syntax <b>506</b>. A difference from the pred weight table syntax <b>508</b> illustrated in <figref idref="DRAWINGS">FIG. <b>21</b></figref> is that, num_ref_common_active_minus1 is replaced by MAX_COMMON_REF_MINUS1. Since the number of reference images is described in the slice header syntax <b>507</b>, it cannot be referred to in the adaptation parameter set syntax <b>506</b> that is a higher-level layer. Accordingly, for example, a value acquired by subtracting one from the value of num_ref_frames described in the sequence parameter set syntax <b>504</b> is set to MAX_COMMON_REF_MINUS1. In addition, a value taken as a maximum number of reference images may be set in accordance with a predetermined profile or level. The other syntax elements are the same as those illustrated in <figref idref="DRAWINGS">FIG. <b>21</b></figref>.</p><p id="p-0181" num="0170">As above, according to the modification of the first embodiment, by employing a structure in which the pred weight table syntax <b>508</b> is called from the adaptation parameter set syntax <b>506</b>, the code amount of the WP parameter information at the time of dividing one frame into a plurality of slices can be markedly reduced.</p><p id="p-0182" num="0171">For example, by encoding the adaptation parameter set syntax <b>506</b> having mutually-different three types of used WP parameter information first and calling necessary WP parameter information from the slice header syntax <b>507</b> using aps_id depending on the situations, the code amount can be smaller than that of a configuration in which the WP parameter information is constantly encoded by the slice header syntax <b>507</b>.</p><heading id="h-0008" level="1">Second Embodiment</heading><p id="p-0183" num="0172">A second embodiment will be described. In an encoding device <b>600</b> according to the second embodiment, the configuration of an index setting unit <b>608</b> is different from that of the encoding device <b>100</b> according to the first embodiment. Hereinafter, differences from the first embodiment will be mainly described, the same name/reference numeral as those of the first embodiment will be assigned to each constituent element having the same function as that of the first embodiment, and the description thereof will not be presented.</p><p id="p-0184" num="0173"><figref idref="DRAWINGS">FIG. <b>25</b></figref> is a block diagram that illustrates an example of the configuration of the index setting unit <b>608</b> according to the second embodiment. The index setting unit <b>608</b>, as illustrated in <figref idref="DRAWINGS">FIG. <b>25</b></figref>, includes a reuse determining unit <b>601</b> and an index generating unit <b>602</b>.</p><p id="p-0185" num="0174">The reuse determining unit <b>601</b> combines a reference number of List 1 with a reference number of List 0 and determines whether to reuse the WP parameter information of List 0 as the WP parameter information of List 1. In a case where the same reference image is included in the two reference lists, and the values of corresponding WP parameter information are the same, when the same information is encoded not only in List 0 but also in List 1, the code amount increases. Accordingly, the reuse determining unit <b>601</b> reuses the WP parameter information of List 0.</p><p id="p-0186" num="0175">More specifically, in a case where the WP parameter information of List 0 is reused, the reuse determining unit <b>601</b> sets the reuse flag to &#x201c;1&#x201d; and set the reference number of a reference destination (List 0) as reference information. On the other hand, in a case where the WP parameter information of List 0 is not reused, the reuse determining unit <b>601</b> sets the reuse flag to &#x201c;0&#x201d; and sets WP parameter information corresponding to the reference number of List 1 in the syntax.</p><p id="p-0187" num="0176">The reuse determining unit <b>601</b>, for example, in accordance with a pseudo code represented in Numerical Expression (11), determines whether or not to reuse the WP parameter information of List 0 as the WP parameter information of List 1.</p><p id="p-0188" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mi>for</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mrow>       <mrow>        <mi>refIdx</mi>        <mo>=</mo>        <mn>0</mn>       </mrow>       <mo>;</mo>       <mrow>        <mi>refIdx</mi>        <mo>&#x3c;=</mo>        <mrow>         <mi>num_of</mi>         <mo>&#x2062;</mo>         <mi>_active</mi>         <mo>&#x2062;</mo>         <mi>_ref</mi>         <mo>&#x2062;</mo>         <mi>_l1</mi>         <mo>&#x2062;</mo>         <mi>_minus1</mi>        </mrow>       </mrow>       <mo>;</mo>       <mtext></mtext>       <mrow>        <mi>refIdx</mi>        <mo>++</mo>       </mrow>      </mrow>      <mo>)</mo>     </mrow>     <mo>&#x2062;</mo>     <mrow>      <mo>{</mo>      <mtext></mtext>      <mrow>       <mrow>        <mi>refPOC</mi>        <mo>=</mo>        <mrow>         <mrow>          <mrow>           <mi>RefPicOrderCnt</mi>           <mo>&#x2061;</mo>           <mo>(</mo>           <mrow>            <mi>ListL1</mi>            <mo>,</mo>            <mi>refIdx</mi>           </mrow>           <mo>)</mo>          </mrow>          <mo>&#x2062;</mo>          <mtext></mtext>          <mi>refWP</mi>         </mrow>         <mo>=</mo>         <mrow>          <mrow>           <mrow>            <mi>RefWPTable</mi>            <mo>&#x2061;</mo>            <mo>(</mo>            <mrow>             <mi>ListL1</mi>             <mo>,</mo>             <mi>refIdx</mi>            </mrow>            <mo>)</mo>           </mrow>           <mo>&#x2062;</mo>           <mtext></mtext>           <mi>reuse_wp</mi>           <mo>&#x2062;</mo>           <mi>_flag</mi>          </mrow>          <mo>=</mo>          <mi>false</mi>         </mrow>        </mrow>       </mrow>       <mo>;</mo>       <mtext></mtext>       <mrow>        <mrow>         <mi>reuse_ref</mi>         <mo>&#x2062;</mo>         <mi>_idx</mi>        </mrow>        <mo>=</mo>        <mn>0</mn>       </mrow>       <mo>;</mo>       <mtext></mtext>       <mrow>        <mrow>         <mi>for</mi>         <mo>&#x2061;</mo>         <mo>(</mo>         <mrow>          <mrow>           <mi>currId</mi>           <mo>=</mo>           <mn>0</mn>          </mrow>          <mo>;</mo>          <mrow>           <mi>currIdx</mi>           <mo>&#x3c;=</mo>           <mrow>            <mi>num_of</mi>            <mo>&#x2062;</mo>            <mi>_active</mi>            <mo>&#x2062;</mo>            <mi>_ref</mi>            <mo>&#x2062;</mo>            <mi>_l0</mi>            <mo>&#x2062;</mo>            <mi>_minus1</mi>           </mrow>          </mrow>          <mo>;</mo>          <mtext></mtext>          <mrow>           <mi>currIdx</mi>           <mo>++</mo>          </mrow>         </mrow>         <mo>)</mo>        </mrow>        <mo>&#x2062;</mo>        <mrow>         <mo>{</mo>         <mtext></mtext>         <mrow>          <mrow>           <mi>if</mi>           <mo>&#x2061;</mo>           <mo>(</mo>           <mrow>            <mrow>             <mi>refPOC</mi>             <mo>==</mo>             <mrow>              <mi>RefPicOrderCnt</mi>              <mo>&#x2061;</mo>              <mo>(</mo>              <mrow>               <mi>ListL0</mi>               <mo>,</mo>               <mi>currIdx</mi>              </mrow>              <mo>)</mo>             </mrow>            </mrow>            <mtext></mtext>            <mo>&#x26;&#x26;</mo>            <mrow>             <mi>RefWPTable</mi>             <mo>&#x2061;</mo>             <mo>(</mo>             <mrow>              <mi>ListL0</mi>              <mo>,</mo>              <mi>currIdx</mi>              <mo>,</mo>              <mi>refWP</mi>             </mrow>             <mo>)</mo>            </mrow>           </mrow>           <mo>)</mo>          </mrow>          <mo>&#x2062;</mo>          <mrow>           <mo>{</mo>           <mtext></mtext>           <mrow>            <mrow>             <mrow>              <mi>reuse_wp</mi>              <mo>&#x2062;</mo>              <mi>_flag</mi>             </mrow>             <mo>=</mo>             <mi>true</mi>            </mrow>            <mo>;</mo>            <mtext></mtext>            <mrow>             <mrow>              <mi>reuse_ref</mi>              <mo>&#x2062;</mo>              <mi>_idx</mi>             </mrow>             <mo>=</mo>             <mi>currIdx</mi>            </mrow>            <mo>;</mo>           </mrow>           <mtext></mtext>           <mo>}</mo>          </mrow>         </mrow>         <mtext></mtext>         <mo>}</mo>        </mrow>       </mrow>      </mrow>      <mtext></mtext>      <mo>}</mo>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>11</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0189" num="0177">Here, ListL0 represents List 0, and ListL1 represents List 1. RefWPTable( ) is a function for returning whether or not an input reference WP parameter matches a WP parameter corresponding to a list number and a reference number, which have been input, as a flag when the list number, the reference number, and the reference WP parameter are input. In a case where the value of the flag is &#x201c;1&#x201d;, it represents that both the WP parameters match each other. In the pseudo code represented in Numerical Expression (11), in a case where the reference numbers of List 1 are sequentially scanned, and there are reference images having the same POC number within List 0, when the WP parameters are the same, reuse_wp_flag representing a reuse flag is set to true, and a reference number corresponding to List 0 is set to reuse_ref_idx.</p><p id="p-0190" num="0178">The reuse determining unit <b>601</b> outputs the WP parameter information after the reuse determination so as to be input to the index generating unit <b>602</b>.</p><p id="p-0191" num="0179">The index generating unit <b>602</b> receives the WP parameter information after the reuse determination from the reuse determining unit <b>601</b> and maps the WP parameter information after the reuse into a syntax element to be described later, thereby generating index information. The syntax element into which the WP parameter information is mapped by the index generating unit <b>602</b> is different from that of the first embodiment. The index generating unit <b>602</b> outputs the index information so as to be input to the encoding unit <b>110</b>.</p><p id="p-0192" num="0180"><figref idref="DRAWINGS">FIG. <b>26</b></figref> is a flowchart that illustrates an example of the process of generating index information that is performed by the index setting unit <b>608</b> according to the second embodiment.</p><p id="p-0193" num="0181">When the WP parameter information is input, the reuse determining unit <b>601</b> branches the process based on the slice type (Step S<b>201</b>).</p><p id="p-0194" num="0182">In a case where the slice type is a single-directional prediction slice (P-slice) using only one reference list (No in Step S<b>201</b>), the reuse determining unit <b>601</b> directly outputs the WP parameter information illustrated in <figref idref="DRAWINGS">FIG. <b>9</b>A</figref> to the index generating unit <b>602</b>. The index generating unit <b>602</b> generates index information by mapping the WP parameter information input from the reuse determining unit <b>601</b> into a predetermined syntax element to be described later and outputs the generated index information.</p><p id="p-0195" num="0183">On the other hand, in a case where the slice type is a bi-directional prediction slice (B-slice) using two reference lists (Yes in Step S<b>201</b>), the reuse determining unit <b>601</b> initializes a variable refIdc to zero (Step S<b>202</b>). The variable refIdc represents a reference number of List 1.</p><p id="p-0196" num="0184">Subsequently, the reuse determining unit <b>601</b> derives a variable refPOC representing a POC number of a reference image corresponding to the reference number represented by the variable refIdx (Step S<b>203</b>) and derives a WP parameter corresponding to a reference number represented by the variable refIdx (Step S<b>204</b>).</p><p id="p-0197" num="0185">Subsequently, the reuse determining unit <b>601</b> sets a flag reuse_wp_flag to false (Step S<b>205</b>) and sets a variable currIdx to &#x201c;0&#x201d; (Step S<b>206</b>). The flag reuse_wp_flag represents whether the WP parameters of Lists 0 and 1 are the same. In addition, the variable currIdx represents a reference number of List 0.</p><p id="p-0198" num="0186">Subsequently, the reuse determining unit <b>601</b> determines whether a POC number represented by the derived variable refPOC and a POC number of a reference image corresponding to the reference number represented by the variable currIdx are the same, and a WP parameter corresponding to the reference number represented by the variable refIdx and a WP parameter corresponding to the reference number represented by the variable currIdx are the same (Step S<b>207</b>).</p><p id="p-0199" num="0187">In a case where both the POC numbers and both the WP parameters are the same (Yes in Step S<b>207</b>), the reuse determining unit <b>601</b> sets a flag reuse_wp_flag to true (Step S<b>208</b>) and substitutes the value of the variable currIdx into the variable reuse_ref_idx (Step S<b>209</b>). On the other hand, in a case where both POC numbers or both the WP parameters are not the same (No in Step S<b>207</b>), the process of Steps S<b>208</b> and S<b>209</b> is not performed. In addition, a configuration may be employed in which the matching of the POC numbers is not checked.</p><p id="p-0200" num="0188">Subsequently, the reuse determining unit <b>601</b> increments the variable currIdx (Step S<b>210</b>).</p><p id="p-0201" num="0189">Subsequently, the reuse determining unit <b>601</b> determines whether the value of the variable currIdx is num_of_active_ref_l0_minus1, which is a value acquired by subtracting one from a maximum number of List 0, or more (Step S<b>211</b>). In a case where the value of the variable currIdx is num_of_active_ref_l0_minus1 or less (No in Step S<b>211</b>), the process of Steps S<b>207</b> to S<b>210</b> is repeated.</p><p id="p-0202" num="0190">On the other hand, in a case where the value of the variable currIdx is larger than num_of_active_ref_l0_minus1 (Yes in Step S<b>211</b>), the reuse determining unit <b>601</b> completes the checking of List 0 and increments the variable refIdx (Step S<b>212</b>).</p><p id="p-0203" num="0191">Subsequently, the reuse determining unit <b>601</b> determines whether the value of the variable refIdx is num_of_active_ref_l1_minus1, which is a value acquired by subtracting one from a maximum number of List 1, or more (Step S<b>213</b>). In a case where the value of the variable refIdx is num_of_active_ref_l1_minus1 or less (No in Step S<b>213</b>), the process of Steps S<b>203</b> to S<b>212</b> is repeated.</p><p id="p-0204" num="0192">On the other hand, in a case where the value of the variable refIdx is larger than num_of_active_ref_l1_minus1 (Yes in Step S<b>213</b>), the reuse determining unit <b>601</b> completes the checking of List 1 and outputs the WP parameter information after the reuse determination to the index generating unit <b>602</b>. The index generating unit <b>602</b> generates index information by mapping the WP parameter information input from the reuse determining unit <b>601</b> into a predetermined syntax element to be described later and outputs the generated index information.</p><p id="p-0205" num="0193"><figref idref="DRAWINGS">FIG. <b>27</b></figref> is a diagram that illustrates an example of the pred weight table syntax <b>506</b> according to the second embodiment. In syntax elements illustrated in <figref idref="DRAWINGS">FIG. <b>27</b></figref>, symbols of l0 and l1 correspond to Lists 0 and 1, respectively. In addition, a syntax element having the same prefix as that illustrated in <figref idref="DRAWINGS">FIG. <b>21</b></figref> is used as the same variable except for different treatment of a reference list although there is a difference of List 0 or List 1 but not of common list. For example, luma_weight_l0[i] is a syntax element representing a weighting factor of the i-th reference number in the reference list 10.</p><p id="p-0206" num="0194">luma_log2_weight_denom and chroma_log2_weight_denom have the same configuration as that of the first embodiment. First, for List 0 in which a symbol of l0 is included, luma_weight_l0_flag, luma_weight_l0[i], luma_offset_l0[i], chroma_weight_l0_flag, chroma_weight_l0[i] [j], and chroma_offset_l0[i] [j] are defined. Next, syntax elements for List 1 in which a symbol of l1 is included are defined.</p><p id="p-0207" num="0195">Here, reuse_wp_flag is a syntax element representing whether or not a WP parameter of l0 corresponding to List 0 is reused. In a case where reuse_wp_flag is &#x201c;1&#x201d;, the weighing factor and the offset are not encoded, but reuse_ref_idx is encoded. reuse_ref_idx represents a reference number of l0 corresponding to List 0 to which the WP parameter corresponds that is used. For example, in a case where reuse_ref_idx is &#x201c;1&#x201d;, a WP parameter corresponding to a reference number &#x201c;1&#x201d; of List 0 is copied to a WP parameter corresponding to the reference number i of List 1. On the other hand, in a case where reuse_wp_flag is &#x201c;0&#x201d;, similarly to List 0, luma_weight_l1_flag, luma_weight_l1[i], luma_offset_l1[i], chroma_weight_l1_flag, chroma_weight_l1[i] [j], and chroma_offset_l1[i] [j] are encoded.</p><p id="p-0208" num="0196">As above, according to the second embodiment, in a case where there is a combination including the same reference image in two reference lists of Lists 0 and 1 in the WP parameter information, the index setting unit <b>608</b> removes a redundant WP parameter that is duplicate in the WP parameter information and generates index information by reusing the duplicate WP parameter. Therefore, according to the second embodiment, the code amount of the index information can be reduced.</p><p id="p-0209" num="0197">According to the second embodiment, WP parameter information corresponding to List 0 is encoded in a conventional manner, and, in order to encode the WP parameter information corresponding to List 1, it is checked whether the same reference image is to be referred to inside List 0. In addition, the same reference image is referred to, and, in a case where the WP parameters are the same as well, information representing a reference number of List 0 to which the WP parameter corresponds is encoded together with encoding the flag for reusing the WP parameter of List 0. The amount of such information is markedly smaller than that of information relating to the weighting factor and the offset, and accordingly, the code amount of the WP parameter information can be configured to be much smaller than that of a case where Lists 0 and 1 are separately encoded.</p><p id="p-0210" num="0198">In addition, according to the second embodiment, in a case where the reference numbers included in Lists 0 and 1 represent the same reference image, and the WP parameters are different from each other, WP parameters different from each other can be set. In other words, in a case where there is a combination representing the same reference image within reference lists, the same WP parameters are forcibly set in the first embodiment, and, a redundant representation of the same WP parameters is removed only in a case where the WP parameters are the same in the second embodiment.</p><heading id="h-0009" level="1">Third Embodiment</heading><p id="p-0211" num="0199">A third embodiment will be described. <figref idref="DRAWINGS">FIG. <b>28</b></figref> is a block diagram that illustrates an example of the configuration of an encoding device <b>700</b> according to the third embodiment. The encoding device <b>700</b> according to the third embodiment further includes a motion information memory <b>701</b>, a motion information acquiring unit <b>702</b>, and a selector switch <b>703</b>, which is different from the encoding device <b>100</b> according to the first embodiment. Hereinafter, differences from the first embodiment will be mainly described, the same name/reference numeral will be assigned to each constituent element having the same function as that of the first embodiment, and the description thereof will not be presented.</p><p id="p-0212" num="0200">The motion information memory <b>701</b> temporarily stores motion information applied to a pixel block that has been encoded as reference motion information. The motion information memory <b>701</b> may reduce the amount of information by performing a compression process such as sub-sampling for the motion information.</p><p id="p-0213" num="0201"><figref idref="DRAWINGS">FIG. <b>29</b></figref> is a diagram that illustrates a detailed example of the motion information memory <b>701</b> according to the third embodiment. The motion information memory <b>701</b>, as illustrated in <figref idref="DRAWINGS">FIG. <b>29</b></figref>, is maintained in units of frames or slices and further includes a spatial-directional reference motion information memory <b>701</b>A that stores motion information on a same frame as reference motion information <b>710</b> and a time-directional reference motion information memory <b>701</b>B storing motion information of a frame that has been encoded as reference motion information <b>710</b>. A plurality of time-directional reference motion information memories <b>701</b>B may be arranged in accordance with the number of reference frames used by the encoding target frame for a prediction.</p><p id="p-0214" num="0202">The reference motion information <b>710</b> is maintained in the spatial-directional reference motion information memory <b>701</b>A and the time-directional reference motion information memory <b>701</b>B in units of predetermined areas (for example, in units of 4&#xd7;4 pixel blocks). The reference motion information <b>710</b> further includes information representing whether the area is encoded by an inter-prediction to be described later or is encoded by an intra-prediction to be described later. In addition, also in a case where a pixel block (a coding unit or a prediction unit) is inter-predicted using motion information predicted from an encoded area without the value of a motion vector included in the motion information being encoded as in a skip mode or a direct mode defined in H.264 or a merge mode to be described later, the motion information of the pixel block is maintained as the reference motion information <b>710</b>.</p><p id="p-0215" num="0203">When the encoding process for the encoding target frame or the encoding target slice is completed, the spatial-directional reference motion information memory <b>701</b>A of the frame is changed to be treated as the time-directional reference motion information memory <b>701</b>B used for a frame to be encoded next. At this time, in order to reduce the capacity of the time-directional reference motion information memory <b>701</b>B, it may be configured such that the motion information is compressed, and the compressed motion information is stored in the time-directional reference motion information memory <b>701</b>B.</p><p id="p-0216" num="0204">The motion information acquiring unit <b>702</b> receives reference motion information from the motion information memory <b>701</b> as input and outputs motion information B used for an encoding pixel block. Details of the motion information acquiring unit <b>702</b> will be described later.</p><p id="p-0217" num="0205">The selector switch <b>703</b> selects one of the motion information B output from the motion information acquiring unit <b>702</b> and motion information A output from the motion evaluating unit <b>109</b> in accordance with prediction mode information to be described later and outputs the selected motion information to the predicted image generating unit <b>107</b> as motion information. The motion information A output from the motion evaluating unit <b>109</b> is used for encoding information of a difference from a predicted motion vector acquired from a predicted motion vector acquiring unit not illustrated in the figure and predicted motion vector acquiring position information. Hereinafter, such a prediction mode will be referred to as an inter-mode. On the other hand, the motion information B output from the motion information acquiring unit <b>702</b> in accordance with prediction mode information is used for merging motion information from adjacent pixel blocks and is directly applied to an encoding pixel block, and accordingly, other information (for example, motion vector difference information) relating to the motion information does not need to be encoded. Hereinafter, such a prediction mode will be referred to as a merge mode.</p><p id="p-0218" num="0206">The prediction mode information is in accordance with a prediction mode that is controlled by the encoding control unit <b>111</b> and includes switching information of the selector switch <b>703</b>.</p><p id="p-0219" num="0207">Hereinafter, details of the motion information acquiring unit <b>702</b> will be described.</p><p id="p-0220" num="0208">The motion information acquiring unit <b>702</b> receives the reference motion information as an input and outputs the motion information B to the selector switch <b>703</b>. <figref idref="DRAWINGS">FIGS. <b>30</b>A and <b>30</b>B</figref> are diagrams that illustrate an example of block positions at which motion information candidates are derived for an encoding pixel block according to the third embodiment. In <figref idref="DRAWINGS">FIG. <b>30</b>A</figref>, A to E represent pixel blocks, which are spatially adjacent to the encoding pixel block, used for deriving motion information candidates. In addition, in <figref idref="DRAWINGS">FIG. <b>30</b>B</figref>, T represents a pixel block, which is adjacent in time, used for deriving a motion information candidate.</p><p id="p-0221" num="0209"><figref idref="DRAWINGS">FIG. <b>31</b></figref> is a diagram that illustrates an example of the relation between pixel block positions of a plurality of motion information candidates and pixel block position indices (idx) according to the third embodiment. The block positions of the motion information candidates are arranged in order of spatially-adjacent block positions A to E and the pixel block position T that is adjacent in time, and, in a case where there is an available (an inter-prediction is applied) pixel block in the order, indices idx are sequentially assigned thereto until the maximum number (N&#x2212;1) of the motion information stored in the storage list MergeCandList of the merge mode is reached.</p><p id="p-0222" num="0210"><figref idref="DRAWINGS">FIG. <b>32</b></figref> is a flowchart that illustrates an example of a storage process for MergeCandList according to the third embodiment.</p><p id="p-0223" num="0211">First, the motion information acquiring unit <b>702</b> initializes the storage number numMergeCand for MergeCandList to &#x201c;0&#x201d; (Step S<b>301</b>).</p><p id="p-0224" num="0212">Subsequently, the motion information acquiring unit <b>702</b> determines whether or not all the spatially-adjacent blocks (for example, blocks A to E) are available (Steps S<b>302</b> and S<b>303</b>). In a case where all the spatially-adjacent blocks are available (Yes in Step S<b>303</b>), the motion information acquiring unit <b>702</b> stores the motion information of the spatially-adjacent blocks in MergeCandList and increment numMergeCand (Step S<b>304</b>). On the other hand, in a case where all the spatially-adjacent blocks are not available (No in Step S<b>303</b>), the motion information acquiring unit <b>702</b> does not perform the process of Step S<b>304</b>.</p><p id="p-0225" num="0213">When the process of Steps S<b>303</b> and S<b>304</b> is performed for all the spatially-adjacent blocks (Step S<b>305</b>), the motion information acquiring unit <b>702</b> determines whether a block (for example, the block T) that is adjacent in time is available (Step S<b>306</b>).</p><p id="p-0226" num="0214">In a case where the block that is adjacent in time is available (Yes in Step S<b>306</b>), the motion information acquiring unit <b>702</b> stores the motion information of the block T that is adjacent in time in MergeCandList and increment numMergeCand (Step S<b>307</b>). On the other hand, in a case where the block that is adjacent in time is not available (No in Step S<b>306</b>), the motion information acquiring unit <b>702</b> does not perform the process of Step S<b>307</b>.</p><p id="p-0227" num="0215">Subsequently, the motion information acquiring unit <b>702</b> removes duplicate motion information within MergeCandList and decreases numMergeCand by a value corresponding to the number of removals (Step S<b>308</b>). In order to determine whether or not the motion information is duplicate within MergeCandList, the motion information acquiring unit <b>702</b> determines whether or not two types of motion information (a motion vector my and a reference number refIdx) and WP parameter information within MergeCandList match each other and removes one side from MergeCandList[ ] in a case where all the information match each other</p><p id="p-0228" num="0216"><figref idref="DRAWINGS">FIG. <b>33</b></figref> is a diagram that illustrates an example of a storage list of motion information according to the third embodiment and, described in detail, illustrates an example of a storage list MergeCandList[idx] of motion information (a motion vector my and a reference number refIdx) corresponding to each idx in accordance with the table illustrated in <figref idref="DRAWINGS">FIG. <b>31</b></figref> with N=5. In <figref idref="DRAWINGS">FIG. <b>33</b></figref>, in a storage list of idx=0, reference motion information (a motion vector mv0L0, a reference number refIdx0L0) of List 0 is stored, in a storage list of idx=1, reference motion information (a motion vector mv1L1, a reference number refIdx1L1) of List 1 is stored, and, in storage lists of idx=2 to 4, reference motion information (motion vectors mv2L0 and mv2L1, reference numbers refIdx2L0 and refIdx2L1) of both Lists 0 and 1 are stored.</p><p id="p-0229" num="0217">The encoding pixel block selects one of a maximum N kinds of indices idx, derives reference motion information of a pixel block corresponding to the selected index idx from MergeCandList[idx], and outputs the derived reference information as motion information B. In a case where there is no reference motion information, the encoding pixel block outputs motion information having a zero vector as a predicted motion information candidate B. The information (a syntax merge_idx to be described later) of the selected index idx is included in the predicted mode information and is encoded by the encoding unit <b>110</b>.</p><p id="p-0230" num="0218">In addition, blocks that are adjacent in space or time are not limited to those illustrated in <figref idref="DRAWINGS">FIGS. <b>30</b>A and <b>30</b>B</figref>, and the adjacent block may be located at any position within an area that has been encoded. Furthermore, as another example of the above-described list, the maximum storage number (N) for MergeCandList and the storage order for MergeCandList are not limited to the above-described ranges, and arbitrary order or an arbitrary maximum number may be used.</p><p id="p-0231" num="0219">In a case where the encoded slice is a B slice, and the maximum storage number N has not been arrived even in a case where the reference motion information of the pixel blocks that are adjacent in time or space is stored in MergeCandList[ ], the motion information of List 0 and the motion information of List 1 stored in MergeCandList[ ] at that time point are combined so as to newly generate a bi-directional prediction, and the generated bi-directional prediction is stored in MergeCandList[ ].</p><p id="p-0232" num="0220"><figref idref="DRAWINGS">FIG. <b>34</b></figref> is a flowchart that illustrates an example of a method of storing motion information in the storage list according to the third embodiment.</p><p id="p-0233" num="0221">First, the motion information acquiring unit <b>702</b> sets the storage number from pixel blocks adjacent in time or space to MergeCandList to numInputMergeCand, initializes a variable numMergeCand representing the current storage number to numInputMergeCand, and initializes variables combIdx and combCnt to zero (Step S<b>401</b>).</p><p id="p-0234" num="0222">Subsequently, the motion information acquiring unit <b>702</b> derives variables l0CandIdx and l1CandIdx from combIdx using a newly generated combination (see <figref idref="DRAWINGS">FIG. <b>35</b></figref>) of bi-directional predictions (Step S<b>402</b>). In addition, the combinations for deriving the variables l0CandIdx and l1CandIdx from the variable combIdx are not limited to the example illustrated in <figref idref="DRAWINGS">FIG. <b>36</b></figref> but may be arranged in arbitrary order unless they are duplicate.</p><p id="p-0235" num="0223">Subsequently, the motion information acquiring unit <b>702</b> sets the reference motion information (two types of Lists 0 and 1 in the case of a bi-directional prediction) stored in MergeCandList[l0CandIdx] to l0Cand. A reference number at motion information of l0Cand will be referred to as refIdxL0l0Cand, and a motion vector at the motion information of l0Cand will be referred to as mvL0l0Cand. Similarly, the motion information acquiring unit <b>702</b> sets the reference motion information stored in MergeCandList[l1CandIdx] to l1Cand. A reference number at reference motion information of l1Cand will be referred to as refIdxL1l1Cand, and a motion vector at the reference motion information of l1Cand will be referred to as mvL1l1Cand (Step S<b>403</b>). In addition, a combination of motion information for making a bi-directional prediction using 10Cand as the motion information of List 0 and L1Cand as the motion information of List 1 will be referred to as combCand.</p><p id="p-0236" num="0224">Subsequently, the motion information acquiring unit <b>702</b> determines whether or not blocks referred to by l0Cand and l1Cand are the same using Conditional Equations (12) to (15) (Step S<b>404</b>).</p><p id="p-0237" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>Are <i>l</i>0Cand and <i>l</i>1Cand available?&#x2003;&#x2003;(12)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0238" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>RefPicOrderCnt(refIdxL0<i>l</i>0Cand,<i>L</i>0)!=RefPicOrderCnt(refIdx<i>L</i>1<i>l</i>1Cand,<i>L</i>1)&#x2003;&#x2003;(13)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0239" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>mvL</i>0<i>l</i>0Cand !=<i>mvL</i>1<i>l</i>1Cand&#x2003;&#x2003;(14)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0240" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>Wp</i>Param(refIdx<i>L</i>0<i>l</i>0Cand,<i>L</i>0)!=<i>Wp</i>Param(refIdx<i>L</i>1<i>l</i>1Cand,<i>L</i>1)&#x2003;&#x2003;(15)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0241" num="0225">RefPicOrderCnt(refIdx, LX) is a function for deriving a POC (Picture Order Count) of a reference frame corresponding to the reference number refIdx in reference list X (here, X=0 or 1). In addition, WpParam(refIdx, LX) is a function for deriving WP parameter information of a reference frame corresponding to the reference number refIdx in reference list X (here, X=0 or 1). This function returns &#x201c;Yes&#x201d; in a case where reference frames of List 0 and 1 do not have the same WP parameters (presence/no presence of WP WP_flag, a weighting factor Weight, and an offset Offset) and returns &#x201c;No&#x201d; in a case where the reference frames have totally same parameters. As another example of WpParam( ), the determination may be made using only a part of the data of WP parameters as in a case where only the matching of the presence/no-presence of WP WP-flag and the offsets Offset are checked without checking the matching of all the parameters or the like.</p><p id="p-0242" num="0226">In a case where Conditional Equation (12) is satisfied, and one of Conditional Equations (13) to (15) is satisfied (Yes in Step S<b>404</b>), the motion information acquiring unit <b>702</b> determines whether or not there is a bi-directional prediction using the same combination as that of l0Cand and l1Cand within mergeCandList[ ] (Step S<b>405</b>).</p><p id="p-0243" num="0227">In a case where there is the same combination as that of l0Cand and l1Cand within mergeCandList[ ] (Yes in Step S<b>405</b>), the motion information acquiring unit <b>702</b> adds combCand to the rearmost end of mergeCandList (Step S<b>406</b>). More specifically, the motion information acquiring unit <b>702</b> substitutes combCand into mergeCandList[numMergeCand] and increments numMergeCand and combCnt.</p><p id="p-0244" num="0228">Subsequently, the motion information acquiring unit <b>702</b> increments combIdx (Step S<b>407</b>).</p><p id="p-0245" num="0229">Subsequently, the motion information acquiring unit <b>702</b> determines whether to complete storage for mergeCandList[ ] by using Conditional Equations (16) to (18) (Step S<b>408</b>).</p><p id="p-0246" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>combIdx==numInputMergeCand*(numInputMergeCand?1))&#x2003;&#x2003;(16)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0247" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>numMergeCand==maxNumMergeCand&#x2003;&#x2003;(17)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0248" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>combCnt==<i>N</i>&#x2003;&#x2003;(18)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0249" num="0230">In a case where one of Conditional Equations (16) to (18) is satisfied (Yes in Step S<b>408</b>), the process ends. On the other hand, in a case where all the Conditional Equations (16) to (18) are satisfied (No in Step S<b>408</b>), the process is returned to Step S<b>402</b>.</p><p id="p-0250" num="0231">The process of a case where the encoding slice is a B slice, and the maximum storage number N has not been reached even when the reference motion information of pixel block that are adjacent in space or time is stored in MergeCandList[ ] has been described. As above, the motion information B is output from the motion information acquiring unit <b>702</b> to the selector switch <b>703</b>.</p><p id="p-0251" num="0232"><figref idref="DRAWINGS">FIG. <b>36</b></figref> is a diagram that illustrates an example of a pred unit syntax <b>512</b> according to the third embodiment. skip flag is a flag representing whether or not a prediction mode of a coding unit to which a prediction unit syntax belongs is a skip mode. In a case where skip flag is &#x201c;1&#x201d;, it represents that syntaxes (the coding unit syntax, the prediction unit syntax, and the transform unit syntax) other than the prediction mode information are not encoded. On the other hand, in a case where skip flag is &#x201c;0&#x201d;, it represents that the prediction mode of a coding unit to which the prediction unit syntax belongs is not a skip mode.</p><p id="p-0252" num="0233">Next, merge_flag that is a flag representing whether or not the encoding pixel block (prediction unit) is in the merge mode is encoded. In a case where the value of Merge_flag is &#x201c;1&#x201d;, it represents that the prediction unit is in the merge mode. On the other hand, in a case where the value is &#x201c;0&#x201d;, it represents that the prediction unit uses an inter-mode. In the case of Merge_flag, merge_idx that is information used for specifying the pixel block position index (idx) described above is encoded.</p><p id="p-0253" num="0234">In a case where Merge_flag is &#x201c;1&#x201d;, prediction unit syntaxes other than merge_flag and merge_idx do not need to be encoded. On the other hand, in a case where Merge_flag is &#x201c;0&#x201d;, it represents that the prediction unit is the inter-mode.</p><p id="p-0254" num="0235">As above, according to the third embodiment, in simultaneously applying the weighted motion compensation and the merge mode, the encoding device <b>700</b> solves the problem in which reference motion information of pixel blocks adjacent in space or time is removed from MergeCandList even in a case where motion vectors and reference numbers match each other and WP parameter information is different from each other when the motion information that is duplicate within the storage list MergeCandList of the merge mode is removed. In addition, in a case where the maximum storage number N has not been reached even when the reference motion information is stored in MergeCandList, a problem is solved in which two types of motion information used for a bi-directional prediction refer to the same block when motion information of Lists 0 and 1 stored in MergeCandList at that time point is combined together, and a bi-directional prediction is newly generated and is stored in MergeCandList[ ].</p><p id="p-0255" num="0236">In a case where two types of motion information used for a bi-directional prediction refer to the same block, a predicted value according to a bi-directional prediction and a predicted value according to a single-directional prediction match each other. Generally, the efficiency of the bi-directional prediction is higher than the efficiency of the single-directional prediction, and accordingly, it is preferable that the two types of motion information used for a bi-directional prediction do not refer to the same block. When it is determined whether or not the two types of motion information used for a bi-directional prediction refer to the same block, in addition to the reference frame number position (POC) derived from a reference number, a parameter of the weighted motion compensation is introduced to the determination item. Accordingly, even when the reference frame number position (POC) is the same as of the motion vector, the encoding device <b>700</b> determines that two types of motion information having mutually-different parameters of the weighted motion compensation do not refer to the same block, and the prediction efficiency of the motion information used for the bidirectional prediction stored in the storage list of the merge mode can be improved.</p><p id="p-0256" num="0237">In addition, as another example of the storage process for MergeCandList, it may be configured such that it is determined whether or not the WP parameter of the reference frame of a block T that is adjacent in time and the WP parameter of the reference frame of the encoding pixel block match each other when the motion information of the block T that is adjacent in time is stored in MergeCandList, and the motion information is stored in MergeCandList only in a case where the WP parameters match each other. The reason for this that, in a case where the WP parameter of the reference frame of the block T that is adjacent in time and the WP parameter of the reference frame of the encoding pixel block are different from each other, it can be estimated that the correlation between the motion information of the block T that is adjacent in time and the motion information of the encoding pixel block is lowered.</p><p id="p-0257" num="0238"><figref idref="DRAWINGS">FIG. <b>37</b></figref> is a flowchart that illustrates another example of the storage process for MergeCandList according to the third embodiment. The flowchart illustrated in <figref idref="DRAWINGS">FIG. <b>38</b></figref> is the same as that illustrated in <figref idref="DRAWINGS">FIG. <b>32</b></figref> except for Step S<b>507</b>.</p><p id="p-0258" num="0239">In addition, as a further another example, in a case where WP parameter of the reference frame of the block T that is adjacent in time and the WP parameter of the reference frame of the encoding pixel block are different from each other, a block having the same WP parameter in the reference frame of the encoding pixel block out of blocks adjacent to the block T in space may be replaced with the block T. At this time, the correlation between the motion information of the block T that is adjacent in time and the motion information of the encoding pixel block is not lowered.</p><heading id="h-0010" level="1">Fourth Embodiment</heading><p id="p-0259" num="0240">In a fourth embodiment, a decoding device that decodes encoded data encoded by the encoding device according to the first embodiment will be described.</p><p id="p-0260" num="0241"><figref idref="DRAWINGS">FIG. <b>38</b></figref> is a block diagram that illustrates an example of the configuration of the decoding device <b>800</b> according to the fourth embodiment.</p><p id="p-0261" num="0242">The decoding device <b>800</b> decodes encoded data stored in an input buffer not illustrated in the figure or the like into a decoded image and outputs the decoded image to an output buffer not illustrated in the figure as an output image. The encoded data, for example, is output from the encoding device <b>100</b> illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref> or the like and is input to the decoding device <b>800</b> through a storage system, a transmission system, a buffer, or the like not illustrated in the figure.</p><p id="p-0262" num="0243">The decoding device <b>800</b>, as illustrated in <figref idref="DRAWINGS">FIG. <b>38</b></figref>, includes: a decoding unit <b>801</b>, an inverse quantization unit <b>802</b>; an inverse orthogonal transformation unit <b>803</b>; an addition unit <b>804</b>; a predicted image generating unit <b>805</b>; and an index setting unit <b>806</b>. The inverse quantization unit <b>802</b>, the inverse orthogonal transformation unit <b>803</b>, the addition unit <b>804</b>, and the predicted image generating unit <b>805</b> are elements that are substantially the same as or similar to the inverse quantization unit <b>104</b>, the inverse orthogonal transformation unit <b>105</b>, the addition unit <b>106</b>, and the predicted image generating unit <b>107</b> illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, respectively. In addition, a decoding control unit <b>807</b> illustrated in <figref idref="DRAWINGS">FIG. <b>38</b></figref> controls the decoding device <b>800</b> and, for example, is realized by a CPU or the like.</p><p id="p-0263" num="0244">In order to decode encoded data, the decoding unit <b>801</b> performs decoding based on the syntax for each frame or each field. The decoding unit <b>801</b> sequentially performs entropy decoding of a code string of each syntax and regenerates motion information including a prediction mode, a motion vector, and a reference number, index information used for a weighted motion compensated prediction, and encoding parameters of an encoding target block such as a quantization transformation coefficient and the like. Here, the encoding parameters are all the parameters that are necessary for decoding information relating to a transformation coefficient, information relating to quantization, and the like in addition to those described above. The decoding unit <b>801</b> outputs the motion information, the index information, and the quantization transformation coefficient, so as to input the quantization transformation coefficient to the inverse quantization unit <b>802</b>, input the index information to the index setting unit <b>806</b>, and input the motion information to the predicted image generating unit <b>805</b>.</p><p id="p-0264" num="0245">The inverse quantization unit <b>802</b> performs an inverse quantization process for the quantization transformation coefficient input from the decoding unit <b>801</b> and acquires a restoration transformation coefficient. More specifically, the inverse quantization unit <b>802</b> performs inverse quantization based on the quantization information used by the decoding unit <b>801</b>. Described in more detail, the inverse quantization unit <b>802</b> multiplies the quantization transformation coefficient by a quantization step size derived based on the quantization information, thereby acquiring a restored transformation coefficient. The inverse quantization unit <b>802</b> outputs the restored transformation coefficient so as to be input to the inverse orthogonal transformation unit <b>803</b>.</p><p id="p-0265" num="0246">The inverse orthogonal transformation unit <b>803</b> performs an inverse orthogonal transformation corresponding to the orthogonal transformation performed on the encoding side for the restored transformation coefficient input from the inverse quantization unit <b>802</b>, thereby acquiring a restored prediction error. The inverse orthogonal transformation unit <b>803</b> outputs the restored prediction error so as to be input to the addition unit <b>804</b>.</p><p id="p-0266" num="0247">The addition unit <b>804</b> adds the restored prediction error input from the inverse orthogonal transformation unit <b>803</b> and a corresponding predicted image, thereby generating a decoded image. The addition unit <b>804</b> outputs the decoded image so as to be input to the predicted image generating unit <b>805</b>. In addition, the addition unit <b>804</b> outputs the decoded image to the outside as an output image. Thereafter, the output image is temporarily stored in an external output buffer not illustrated in the figure or the like and is output to a display device system such as a display or a monitor not illustrated in the figure or a video device system, for example, at output timing managed by the decoding control unit <b>807</b>.</p><p id="p-0267" num="0248">The index setting unit <b>806</b> receives the index information input from the decoding unit <b>801</b>, checks a reference list (list number) and a reference image (reference number), and outputs the WP parameter information so as to be input to the predicted image generating unit <b>805</b>.</p><p id="p-0268" num="0249"><figref idref="DRAWINGS">FIG. <b>39</b></figref> is a block diagram that illustrates an example of the configuration of the index setting unit <b>806</b> according to the fourth embodiment. The index setting unit <b>806</b>, as illustrated in <figref idref="DRAWINGS">FIG. <b>39</b></figref>, includes a reference image checking unit <b>901</b> and a WP parameter generating unit <b>902</b>.</p><p id="p-0269" num="0250">The reference image checking unit <b>901</b> receives index information from the decoding unit <b>801</b> and checks whether or not reference numbers included in two reference lists represent the same reference image.</p><p id="p-0270" num="0251">Here, the reference number included in the reference list, for example, has already been decoded by using a method defined in H.264 or the like. Accordingly, the reference image checking unit <b>901</b> can check whether there is a combination representing the same reference image using derived reference lists and reference numbers in accordance with the management of a decoded picture buffer (DPB) defined in H.264 or the like. In addition, for the control of the DPB, a method defined in H.264 or the like may be employed, or any other method may be employed. Here, a reference list and a reference number may be determined in advance based on the control of the DPB.</p><p id="p-0271" num="0252">The reference image checking unit <b>901</b> creates a common reference list and checks reference lists and reference numbers in the scanning order illustrated in <figref idref="DRAWINGS">FIG. <b>14</b></figref>. Here, the common reference list is created using the pseudo code represented in Numerical Expression (10).</p><p id="p-0272" num="0253">The WP parameter generating unit <b>902</b> generates WP parameter information corresponding to a reference list and a reference number from the created common reference list based on the relation between the reference list and the reference number that is checked by the reference image checking unit <b>901</b> and outputs the generated WP parameter information so as to be input to the predicted image generating unit <b>805</b>. The WP parameter information has already been described with reference to <figref idref="DRAWINGS">FIGS. <b>9</b>A and <b>9</b>B</figref>, and thus, description thereof will not be presented.</p><p id="p-0273" num="0254">The WP parameter generating unit <b>902</b> uses a common_scan_list( ) function and a common_scan_ref_idx( ) function in accordance with the common list, which is scanned to pull a reference list and a reference number through scanning using and supplements the WP parameter information in missing places.</p><p id="p-0274" num="0255">In other words, the common lists illustrated in <figref idref="DRAWINGS">FIGS. <b>15</b> and <b>16</b></figref> are restored in <figref idref="DRAWINGS">FIGS. <b>11</b> and <b>13</b></figref> respectively and assigns index information corresponding to the common list based on the correspondence to the WP parameter information illustrated in <figref idref="DRAWINGS">FIGS. <b>9</b>A and <b>9</b>B</figref>.</p><p id="p-0275" num="0256">In addition, the scanning order illustrated in <figref idref="DRAWINGS">FIG. <b>14</b></figref> is an example, and any other scanning order may be used as long as it is predetermined scanning order. Furthermore, the pseudo code represented in Numerical Expression (10) is an example, a process may be added or a redundant process may be eliminated as long as the purpose of this process can be realized.</p><p id="p-0276" num="0257">Referring back to <figref idref="DRAWINGS">FIG. <b>38</b></figref>, the predicted image generating unit <b>805</b> generates a predicted image by using the motion information input from the decoding unit <b>801</b>, the WP parameter information input from the index setting unit <b>806</b>, and the decoded image input from the addition unit <b>804</b>.</p><p id="p-0277" num="0258">Here, the predicted image generating unit <b>805</b> will be described in detail with reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>. The predicted image generating unit <b>805</b>, similarly to the predicted image generating unit <b>107</b>, includes: a multi-frame motion compensation unit <b>201</b>; a memory <b>202</b>; a single-directional motion compensation unit <b>203</b>; a prediction parameter control unit <b>204</b>; a reference image selector <b>205</b>; a frame memory <b>206</b>; and a reference image control unit <b>207</b>.</p><p id="p-0278" num="0259">The frame memory <b>206</b> stores the decoded image input from the addition unit <b>106</b> as a reference image under the control of the reference image control unit <b>207</b>. The frame memory <b>206</b> includes a plurality of memory sets FM<b>1</b> to FMN (here, N <b>2</b>) used for temporarily storing the reference image.</p><p id="p-0279" num="0260">The prediction parameter control unit <b>204</b> prepares a plurality of combinations each of a reference image number and a prediction parameter as a table based on the motion information input from the decoding unit <b>801</b>. Here, the motion information represents information of a motion vector representing the deviation of a motion that is used for the motion-compensated prediction, the reference image number, and a prediction mode such as a single-directional/bidirectional prediction. The prediction parameter represents information relating to the motion vector and the prediction mode. Then, the prediction parameter control unit <b>204</b> selects a combination of a reference number and a prediction parameter used for generating a predicted image based on the motion information and outputs the selected combination so as to allow the reference image number to be input to the reference image selector <b>205</b> and allow the prediction parameter to be input to the single-directional motion compensation unit <b>203</b>.</p><p id="p-0280" num="0261">The reference image selector <b>205</b> is a switch that changes one of output terminals of the frame memories FM<b>1</b> to FMN, which are included in the frame memory <b>206</b>, to be switched to based on a reference image number input from the prediction parameter control unit <b>204</b>. For example, when the reference image number is &#x201c;0&#x201d;, the reference image selector <b>205</b> connects the output terminal of the frame memory FM<b>1</b> to the output terminal of the reference image selector <b>205</b>, and, when the reference image number is N&#x2212;1, the reference image selector <b>205</b> connects the output terminal of the frame memory FMN to the output terminal of the reference image selector <b>205</b>. The reference image selector <b>205</b> outputs a reference image stored in the frame memory of which the output terminal is connected thereto from among the frame memories FM<b>1</b> to FMN included in the frame memory <b>206</b> so as to be input to the single-directional motion compensation unit <b>203</b>. In the decoding device <b>800</b>, the reference image is not used by any unit other than the predicted image generating unit <b>805</b>, and accordingly, the reference image may not be output to the outside of the predicted image generating unit <b>805</b>.</p><p id="p-0281" num="0262">The single-directional predicted motion compensation unit <b>203</b> performs a motion-compensated prediction process based on the prediction parameter input from the prediction parameter control unit <b>204</b> and the reference image input from the reference image selector <b>205</b>, thereby generating a single-directional predicted image. The motion compensated prediction has already been described with reference to <figref idref="DRAWINGS">FIG. <b>5</b></figref>, and thus, description thereof will not be presented.</p><p id="p-0282" num="0263">The single-directional predicted motion compensation unit <b>203</b> outputs a single-directional predicted image and temporarily stores the single-directional predicted image in the memory <b>202</b>. Here, in a case where the motion information (prediction parameter) represents a bi-directional prediction, the multi-frame motion compensation unit <b>201</b> makes a weighted prediction using two types of single-directional predicted images. Accordingly, the single-directional predicted motion compensation unit <b>203</b> stores a single-directional predicted image corresponding to the first type in the single-directional predicted image in the memory <b>202</b> and directly outputs a single-directional predicted image corresponding to the second type to the multi-frame motion compensation unit <b>201</b>. Here, the single-directional predicted image corresponding to the first type will be referred to as a first predicted image, and the single-directional predicted image corresponding to the second type will be referred to as a second predicted image.</p><p id="p-0283" num="0264">In addition, two single-directional motion compensation units <b>203</b> may be prepared and generate two single-directional predicted images. In such a case, when the motion information (prediction parameter) represents a single-directional prediction, the single-directional motion compensation unit <b>203</b> may directly output the first single-directional predicted image to the multi-frame motion compensation unit <b>201</b> as a first predicted image.</p><p id="p-0284" num="0265">The multi-frame motion compensation unit <b>201</b> makes a weighted prediction by using the first predicted image input from the memory <b>202</b>, the second predicted image input from the single-directional predicted motion compensation unit <b>203</b>, and the WP parameter information input from the motion evaluating unit <b>109</b>, thereby generating a predicted image. The multi-frame motion compensation unit <b>201</b> outputs the predicted image so as to be input to the addition unit <b>804</b>.</p><p id="p-0285" num="0266">Here, the multi-frame motion compensation unit <b>201</b> will be described in detail with reference to <figref idref="DRAWINGS">FIG. <b>6</b></figref>. Similarly to the predicted image generating unit <b>107</b>, the multi-frame motion compensation unit <b>201</b> includes: a default motion compensation unit <b>301</b>; a weighted motion compensation unit <b>302</b>; a WP parameter control unit <b>303</b>; and WP selectors <b>304</b> and <b>305</b>.</p><p id="p-0286" num="0267">The WP parameter control unit <b>303</b> outputs a WP application flag and weighting information based on the WP parameter information input from the index setting unit <b>806</b> so as to input the WP application flag to the WP selectors <b>304</b> and <b>305</b> and input the weighting information to the weighted motion compensation unit <b>302</b>.</p><p id="p-0287" num="0268">Here, the WP parameter information includes information of the fixed point precision of the weighting factor, a first WP application flag, a first weighting factor, and a first offset corresponding to the first predicted image, and a second WP application flag, a second weighting factor, and a second offset corresponding to the second predicted image. The WP application flag is a parameter that can be set for each corresponding reference image and signal component and represents whether or not a weighted motion compensation prediction is made. The weighting information includes information of the fixed point precision of the weighting factor, the first weighting factor, the first offset, the second weighting factor, and the second offset. Here, the WP parameter information represents the same information as that of the first embodiment.</p><p id="p-0288" num="0269">Described in detail, when the WP parameter information is input from the index setting unit <b>806</b>, the WP parameter control unit <b>303</b> outputs the WP parameter information with being divided into the first WP application flag, the second WP application flag, and the weighting information, thereby inputting the first WP application flag to the WP selector <b>304</b>, inputting the second WP application flag to the WP selector <b>305</b>, and inputting the weighting information to the weighted motion compensation unit <b>302</b>.</p><p id="p-0289" num="0270">The WP selectors <b>304</b> and <b>305</b> change the connection ends of the predicted images based on the WP application flags input from the WP parameter control unit <b>303</b>. In a case where the corresponding WP application flag is &#x201c;0&#x201d;, each one of the WP selectors <b>304</b> and <b>305</b> connects the output end thereof to the default motion compensation unit <b>301</b>. Then, the WP selectors <b>304</b> and <b>305</b> output the first and second predicted images so as to be input to the default motion compensation unit <b>301</b>. On the other hand, in a case where the corresponding WP application flag is &#x201c;1&#x201d;, each one of the WP selectors <b>304</b> and <b>305</b> connects the output end thereof to the weighted motion compensation unit <b>302</b>. Then, the WP selectors <b>304</b> and <b>305</b> output the first and second predicted images so as to be input to the weighted motion compensation unit <b>302</b>.</p><p id="p-0290" num="0271">The default motion compensation unit <b>301</b> performs average processing based on the two single-directional predicted images (the first and second predicted images) input from the WP selectors <b>304</b> and <b>305</b>, thereby generating a predicted image. More specifically, in a case where the first and second WP application flags are &#x201c;0&#x201d;, the default motion compensation unit <b>301</b> performs average processing based on Numerical Expression (1).</p><p id="p-0291" num="0272">In addition, in a case where the prediction mode represented by the motion information (prediction parameter) is the single-directional prediction, the default motion compensation unit <b>301</b> calculates a final predicted image using only the first predicted image based on Numerical Expression (4).</p><p id="p-0292" num="0273">The weighted motion compensation unit <b>302</b> performs weighted motion compensation based on the two single-directional predicted images (the first and second predicted images) input from the WP selectors <b>304</b> and <b>305</b> and the weighting information input from the WP parameter control unit <b>303</b>. More specifically, the weighted motion compensation unit <b>302</b> performs the weighting process based on Numerical Expression (7) in a case where the first and second WP application flags are &#x201c;1&#x201d;.</p><p id="p-0293" num="0274">In addition, in a case where the calculation precision of the first and second predicted images and the calculation precision of the predicted image are different from each other, the weighted motion compensation unit <b>302</b> realizes a rounding process by controlling log WD<sub>C</sub>, which is fixed point precision, as in Numerical Expression (8).</p><p id="p-0294" num="0275">In addition, in a case where the prediction mode represented by the motion information (prediction parameter) is a single directional prediction, the weighted motion compensation unit <b>302</b> calculates a final predicted image using only the first predicted image based on Numerical Expression (9).</p><p id="p-0295" num="0276">In addition, in a case where the calculation precision of the first and second predicted images and the calculation precision of the predicted image are different from each other, the weighted motion compensation unit <b>302</b> realizes a rounding process by controlling log WD<sub>C</sub>, which is fixed point precision, as in Numerical Expression (8), similarly to the case of the bi-directional prediction.</p><p id="p-0296" num="0277">The fixed point precision of the weighing factor has already been described with reference to <figref idref="DRAWINGS">FIG. <b>7</b></figref>, and thus, description thereof will not be presented. In addition, in the case of a single directional prediction, various parameters (the second WP application flag, the second weighting factor, and the second offset information) corresponding to the second predicted image are not used and may be set to initial values determined in advance.</p><p id="p-0297" num="0278">The decoding unit <b>801</b> uses syntax <b>500</b> represented in <figref idref="DRAWINGS">FIG. <b>18</b></figref>. The syntax <b>500</b> represents the structure of encoded data that is a decoding target of the decoding unit <b>801</b>. The syntax <b>500</b> has already been described with reference to <figref idref="DRAWINGS">FIG. <b>18</b></figref>, and thus, description thereof will not be presented. In addition, the picture parameter set syntax <b>505</b> has been described with reference to <figref idref="DRAWINGS">FIG. <b>19</b></figref> except that decoding is used instead of encoding, and thus, description thereof will not be presented. Furthermore, the slice header syntax <b>507</b> has already been described with reference to <figref idref="DRAWINGS">FIG. <b>20</b></figref> except that decoding is used instead of encoding, and thus, description thereof will not be presented. In addition, the pred weight table syntax <b>508</b> has already been described with reference to <figref idref="DRAWINGS">FIG. <b>21</b></figref> except that decoding is used instead of encoding, and thus, description thereof will not be presented.</p><p id="p-0298" num="0279">As above, according to the fourth embodiment, when weighted motion compensation is performed using two indices having the same reference image in a bi-directional prediction slice in which a bi-directional prediction can be selected but having mutually-different reference image numbers, the encoding device <b>800</b> solves the problem of a decrease in the encoding efficiency due to decoding the index having the same value twice.</p><p id="p-0299" num="0280">By rearranging two reference lists included in the bi-directional prediction slice and a reference number set for each list to a common list and a common reference number, a combination of reference numbers representing the same reference image is not included in the reference list, and accordingly, the encoding device <b>800</b> can reduce the code amount of a redundant index.</p><heading id="h-0011" level="1">Modification of Fourth Embodiment</heading><p id="p-0300" num="0281">A modification of the fourth embodiment will be described. In the modification of the fourth embodiment, syntax elements used by the decoding unit <b>801</b> are different from those of the fourth embodiment. The sequence parameter set syntax <b>504</b> has already been described with reference to <figref idref="DRAWINGS">FIG. <b>22</b></figref> except that decoding is performed instead of encoding, and thus, the description thereof will not be presented. In addition, the adaptation parameter set syntax <b>506</b> has already been described with reference to <figref idref="DRAWINGS">FIG. <b>23</b></figref> except that decoding is performed instead of encoding, and thus, the description thereof will not be presented. Furthermore, the pred weight table syntax <b>508</b> has already been described with reference to <figref idref="DRAWINGS">FIG. <b>24</b></figref> except that decoding is performed instead of encoding, and thus, the description thereof will not be presented.</p><p id="p-0301" num="0282">As above, according to the modification of the fourth embodiment, by employing a structure in which the pred weight table syntax <b>508</b> is called from the adaptation parameter set syntax <b>506</b>, the code amount of the WP parameter information at the time of dividing one frame into a plurality of slices can be markedly reduced.</p><p id="p-0302" num="0283">For example, the adaptation parameter set syntax <b>506</b> having mutually-different three types of WP parameter information to be used is decoded previously, aps_id is used depending on the situations by the slice header syntax <b>507</b>, necessary WP parameter information is called such that the code amount can be configured to be smaller than that of a configuration in which the WP parameter information is constantly decoded using the slice header syntax <b>507</b>.</p><heading id="h-0012" level="1">Fifth Embodiment</heading><p id="p-0303" num="0284">In a fifth embodiment, a decoding device that decodes encoded data encoded by the encoding device according to the second embodiment will be described. In the decoding device <b>1000</b> according to the fifth embodiment, the configuration of an index setting unit <b>1006</b> is different from the decoding device <b>800</b> according to the fourth embodiment. Hereinafter, differences from the fourth embodiment will be mainly described, the same name/reference numeral as those of the fourth embodiment will be assigned to each constituent element having the same function as that of the fourth embodiment, and the description thereof will not be presented.</p><p id="p-0304" num="0285"><figref idref="DRAWINGS">FIG. <b>40</b></figref> is a block diagram that illustrates an example of the configuration of the index setting unit <b>1006</b> according to the fifth embodiment. The index setting unit <b>1006</b>, as illustrated in <figref idref="DRAWINGS">FIG. <b>40</b></figref>, includes a reuse determining unit <b>1001</b> and a WP parameter generating unit <b>1002</b>.</p><p id="p-0305" num="0286">The reuse determining unit <b>1001</b> checks whether to reuse a WP parameter of List 0 by checking a reuse flag. In a case WP parameters of List 0 are reused, the WP parameter generating unit <b>1002</b> copies WP parameters of List 0 of a reference destination to WP parameters of List 1 based on the information that which WP parameter of List 0 is to be copied. Information relating to a reference number representing a reuse flag and a reference destination is described in a syntax element, and restores WP parameter information by allowing the WP parameter generating unit <b>1002</b> to analyze the index information.</p><p id="p-0306" num="0287">The reuse determining unit <b>1001</b>, for example, checks whether to use WP parameters of List 0 based on the pseudo code represented in Numerical Expression (11).</p><p id="p-0307" num="0288">The pred weight table syntax <b>508</b> has already been described with reference to <figref idref="DRAWINGS">FIG. <b>27</b></figref> except that decoding is performed instead of encoding, and thus, the description thereof will not be presented.</p><p id="p-0308" num="0289">As above, according to the fifth embodiment, when weighted motion compensation is performed using two indices having the same reference image in a bi-directional prediction slice in which a bi-directional prediction can be selected but having mutually-different reference image numbers, the encoding device <b>1000</b> solves the problem of a decrease in the coding efficiency due to decoding an index having the same value twice.</p><p id="p-0309" num="0290">The decoding device <b>1000</b> decodes the index of List 0 in a conventional manner, checks whether or not there is a combination of reference numbers representing the same reference image at the time of decoding the index of List 1, and, in the case of the same index, by reusing indices used in List 1 as indices used in List 0, the same indices are prevented from being decoded twice, and accordingly, the code amount of a redundant index can be reduced.</p><heading id="h-0013" level="1">Sixth Embodiment</heading><p id="p-0310" num="0291">In a six embodiment, a decoding device that decodes encoded data encoded by the encoding device according to the third embodiment will be described. <figref idref="DRAWINGS">FIG. <b>41</b></figref> is a block diagram that illustrates an example of the configuration of the decoding device <b>1100</b> according to a sixth embodiment. The decoding device <b>1100</b> according to the sixth embodiment differs from the decoding device <b>800</b> according to the fourth embodiment in that the decoding device <b>1100</b> further includes a motion information memory <b>1101</b>, a motion information acquiring unit <b>1102</b>, and a selector switch <b>1103</b>. Hereinafter, differences from the fourth embodiment will be mainly described, the same name/reference numeral as those of the fourth embodiment will be assigned to each constituent element having the same function as that of the fourth embodiment, and the description thereof will not be presented.</p><p id="p-0311" num="0292">The motion information memory <b>1101</b> temporarily stores motion information applied to a pixel block that has been decoded as reference motion information. The motion information memory <b>1101</b> may reduce the information amount by performing a compression process such as sub-sampling for the motion information. The motion information memory <b>1101</b>, as illustrated in <figref idref="DRAWINGS">FIG. <b>29</b></figref>, is maintained in units of frames or slices and further includes a spatial-directional reference motion information memory <b>701</b>A that stores motion information on a same frame as reference motion information <b>710</b> and a time-directional reference motion information memory <b>701</b>B that stores motion information of a frame that has been decoded as reference motion information <b>710</b>. A plurality of time-directional reference motion information memories <b>701</b>B may be arranged in accordance with the number of reference frames used by the decoding target frame for a prediction.</p><p id="p-0312" num="0293">The reference motion information <b>710</b> is maintained in the spatial-directional reference motion information memory <b>701</b>A and the time-directional reference motion information memory <b>701</b>B in units of predetermined areas (for example, in units of 4&#xd7;4 pixel blocks). The reference motion information <b>710</b> further includes information representing whether the area is encoded by an inter-prediction to be described later or is encoded by an intra-prediction to be described later. In addition, also in a case where a pixel block (a coding unit or a prediction unit) is inter-predicted using motion information predicted from a decoded area without the value of a motion vector included in the motion information being decoded as in a skip mode or a direct mode defined in H.264 or a merge mode to be described later, the motion information of the pixel block is maintained as the reference motion information <b>710</b>.</p><p id="p-0313" num="0294">When the decoding process for the decoding target frame or the encoding target slice is completed, the spatial-directional reference motion information memory <b>701</b>A of the frame is changed to be treated as the time-directional reference motion information memory <b>701</b>B used for a frame to be decoded next. At this time, in order to reduce the capacity of the time-directional reference motion information memory <b>701</b>B, it may be configured such that the motion information is compressed, and the compressed motion information is stored in the time-directional reference motion information memory <b>701</b>B.</p><p id="p-0314" num="0295">The motion information acquiring unit <b>1102</b> receives reference motion information from the motion information memory <b>1101</b> as input and outputs motion information B used for a decoding pixel block. Since the operation of the motion information acquiring unit <b>1102</b> is the same as that of the motion information acquiring unit <b>702</b> according to the third embodiment, the description thereof will not be presented.</p><p id="p-0315" num="0296">The selector switch <b>1103</b> selects one of motion information B output from the motion information acquiring unit <b>1102</b> and motion information A output from the decoding unit <b>801</b> in accordance with prediction mode information to be described later and outputs the selected motion information to the predicted image generating unit <b>805</b> as motion information. The motion information A output from the decoding unit <b>801</b> is used for decoding information of a difference from a predicted motion vector acquired from a predicted motion vector acquiring unit not illustrated in the figure and predicted motion vector acquiring position information. Hereinafter, such a prediction mode will be referred to as an inter-mode. On the other hand, the motion information B output from the motion information acquiring unit <b>1102</b> in accordance with prediction mode information is used for merging motion information from adjacent pixel blocks and is directly applied to a decoding pixel block, and accordingly, information (for example, motion vector difference information) relating to the other motion information does not need to be decoded. Hereinafter, such a prediction mode will be referred to as a merge mode.</p><p id="p-0316" num="0297">The prediction mode information is in accordance with a prediction mode that is controlled by the decoding control unit <b>807</b> and includes switching information of the selector switch <b>1103</b>.</p><p id="p-0317" num="0298">The pred unit syntax <b>512</b> has already been described with reference to <figref idref="DRAWINGS">FIG. <b>36</b></figref> except that decoding is performed instead of encoding, and thus, the description thereof will not be presented.</p><p id="p-0318" num="0299">The advantages of the sixth embodiment are the same as those of the third embodiment, and thus, the description thereof will not be presented.</p><p id="p-0319" num="0300">In addition, as another example of the storage process for MergeCandList, it may be configured such that it is determined whether or not the WP parameter of the reference frame of a block T that is adjacent in time and the WP parameter of the reference frame of the decoding pixel block match each other when the motion information of the block T that is adjacent in time is stored in MergeCandList, and the motion information is stored in MergeCandList only in a case where the WP parameters match each other. The reason for this is that, in a case where the WP parameter of the reference frame of the block T that is adjacent in time and the WP parameter of the reference frame of the decoding pixel block are different from each other, it can be estimated that the correlation between the motion information of the block T that is adjacent in time and the motion information of the decoding pixel block is lowered.</p><p id="p-0320" num="0301">In addition, as a further another example, in a case where WP parameter of the reference frame of the block T that is adjacent in time and the WP parameter of the reference frame of the decoding pixel block are different from each other, a block having the same WP parameter in the reference frame of the decoding pixel block out of blocks adjacent to the block T in space may be replaced with the block T. At this time, the correlation between the motion information of the block T that is adjacent in time and the motion information of the decoding pixel block is not lowered.</p><heading id="h-0014" level="1">Modification</heading><p id="p-0321" num="0302">The first to third embodiments may be selectively used. In other words, it may be configured such that information for selecting a method is set in advance, and, in a case where a bi-directional slice is selected, the technique to be used may be changed based on the information representing whether the technique of the first embodiment is used or the technique of the second embodiment is used. For example, by setting a flag used for changing such a technique in the slice header syntax illustrated in <figref idref="DRAWINGS">FIG. <b>20</b></figref>, the technique to be used can be easily selected depending on the encoding situations. In addition, a technique to be used may be set in advance in accordance with the configuration of specific hardware. The predicted image generating units and the index setting units according to the first to third embodiments may be appropriately mounted either by hardware or by software.</p><p id="p-0322" num="0303">The fourth to sixth embodiments may be selectively used. In other words, it may be configured such that information for selecting a method is set in advance, and, in a case where a bi-directional slice is selected, the technique to be used may be changed based on the information representing whether the technique of the fourth embodiment is used or the technique of the fifth embodiment is used. For example, by setting a flag used for changing such a technique in the slice header syntax illustrated in <figref idref="DRAWINGS">FIG. <b>20</b></figref>, the technique to be used can be easily selected depending on the encoding situations. In addition, a technique to be used may be set in advance in accordance with the configuration of specific hardware. The predicted image generating units and the index setting units according to the fourth to sixth embodiments may be appropriately mounted either by hardware or by software.</p><p id="p-0323" num="0304">In addition, between rows of the syntax tables illustrated in the first to sixth embodiments as examples, a syntax element not defined in the embodiment may be inserted, or a description relating to the other conditional branch may be included. Furthermore, the syntax table may be divided into a plurality of tables, or a plurality of the syntax tables may be integrated. In addition, the term of each syntax element represented as an example may be arbitrarily changed.</p><p id="p-0324" num="0305">The modification of the first embodiment may be easily applied to the second embodiment. In such a case, a maximum number of reference images corresponding to Lists 0 and 1 may be set, and the maximum number may be set in advance.</p><p id="p-0325" num="0306">The modification of the fourth embodiment may be easily applied to the fifth embodiment. In such a case, a maximum number of reference images corresponding to Lists 0 and 1 may be set, and the maximum number may be set in advance.</p><p id="p-0326" num="0307">In the first to sixth embodiments described above, an example has been described in which the frame is divided into rectangular blocks each having a pixel size of 16&#xd7;16 or the like and is encoded/decoded in order from an upper left block of the screen toward the lower right block (see <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>). However, the encoding order and the decoding order are not limited to those illustrated in this example. For example, the encoding and the decoding may be performed in order from the lower right side toward the upper left side, or the encoding and the decoding may be performed so as to draw a whirlpool from the center of the screen toward the end of the screen. In addition, the encoding and the decoding may be performed in order from the upper right side toward the lower left side, or the encoding and the decoding may be performed so as to draw a whirlpool from the end of the screen toward the center of the screen. In such a case, since the position of an adjacent pixel block that can be referred to in accordance with the encoding order changes, the position may be changed to an appropriately usable position.</p><p id="p-0327" num="0308">In the first to sixth embodiments described above, while the description has been presented with the size of a prediction target block such as a 4&#xd7;4 pixel block, a 8&#xd7;8 pixel block, a 16&#xd7;16 pixel block or the like being illustrated as an example, the prediction target block may not have a uniform block shape. For example, the size of the prediction target bock may be a 16&#xd7;8 pixel block, a 8&#xd7;16 pixel block, a 8&#xd7;4 pixel block, a 4&#xd7;8 pixel block, or the like. In addition, it is not necessary to uniformize all the block sizes within one coding tree block, and a plurality of block sizes different from each other may be mixed. In a case where a plurality of block sizes different from each other are mixed within one coding tree block, the code amount for encoding or decoding division information increases in accordance with an increase in the number of divisions. Thus, it is preferable to select a block size in consideration of the balance between the code amount of the division information and the quality of a local encoded image or a decoded image.</p><p id="p-0328" num="0309">In the first to sixth embodiments described above, for the simplification, a comprehensive description has been presented for a color signal component without the prediction processes of the luminance signal and the color difference signal not being differentiated from each other. However, in a case where the prediction processes of the luminance signal and the color difference signal are different from each other, the same prediction method or prediction methods different from each other may be used. In a case where prediction methods different from each other are used for the luminance signal and the color difference signal, encoding or decoding may be performed using the prediction method selected for the color difference signal similarly to that for the luminance signal.</p><p id="p-0329" num="0310">In the first to sixth embodiments described above, for the simplification, a comprehensive description has been presented for a color signal component without the weighted motion compensated prediction processes of the luminance signal and the color difference signal not being differentiated from each other. However, in a case where the weighted prediction processes of the luminance signal and the color difference signal are different from each other, the same weighted prediction method or weighted prediction methods different from each other may be used. In a case where weighted prediction methods different from each other are used for the luminance signal and the color difference signal, encoding or decoding may be performed using the weighted prediction method selected for the color difference signal similarly to that for the luminance signal.</p><p id="p-0330" num="0311">In the first to sixth embodiments described above, between the rows of the table represented in the syntax configuration, a syntax element not defined in this embodiment may be inserted, and a technique relating to other conditional branches may be included. Alternatively, a syntax table may be divided into a plurality of tables, or syntax tables may be integrated together. In addition, the same term may not be necessarily used, but the term may be arbitrarily changed in accordance with a used form.</p><p id="p-0331" num="0312">As described above, according to each embodiment, the problem of encoding redundant information at the time of performing a weighted motion compensation prediction is solved, and the weighted motion compensated prediction process having high efficiency is realized. Therefore, according to each embodiment, the coding efficiency is improved, and subjective image quality is improved.</p><p id="p-0332" num="0313">Next, a hardware configuration of the device (the encoding device, and the decoding device) according to each embodiment will be described with reference to <figref idref="DRAWINGS">FIG. <b>42</b></figref>. <figref idref="DRAWINGS">FIG. <b>42</b></figref> is an explanatory view illustrating a hardware configuration of the device according to each embodiment. The encoding device and the decoding device each comprise a control unit <b>2801</b>, such as a CPU (Central Processing Unit) which controls the overall device, a main storage <b>2802</b>, such as a ROM (Read Only Memory) or a RAM (Random Access Memory) which stores various data or programs, an auxiliary storage <b>2803</b>, such as an HDD (Hard Disk Drive) or a CD (Compact Disk) drive which stores various data or programs, and a bus connecting these elements. This is a hardware configuration utilizing a conventional computer. Further, the encoding device and the decoding device are connected wirelessly or through a wire to a communication I/F (Interface) <b>2804</b> which controls communication with an external apparatus, a display <b>2805</b> which displays information, and an operating unit <b>2806</b>, such as a keyboard or a mouse which receives instructions input by the user. Data to be encoded and data to be decoded may be stored in the HDD, or input by the disk drive apparatus, or input externally via the communication I/F <b>2804</b>.</p><p id="p-0333" num="0314">The hardware configuration shown in <figref idref="DRAWINGS">FIG. <b>42</b></figref> is a mere example. The encoding device and the decoding device of each embodiment may be implemented partly or entirely by an integrated circuit such as an LSI (Large Scale Integration) circuit or an IC (Integrated Circuit) chip set. The functional blocks of the encoding device and the decoding device may be individually formed of a processor, or may be integrated partly or entirely as a processor. Integration of the circuits of the configuration is not limited to LSI, but may be implemented as a dedicated circuit or a general-purpose processor.</p><p id="p-0334" num="0315">While several embodiments of the present invention have been described, such embodiments are presented as examples and are not for the purpose of limiting the scope of the invention. These novel embodiments can be performed in other various forms, and various omissions, substitutions, and changes can be made therein in a range not departing from the concept of the invention. These embodiments and modifications thereof belong to the scope or the concept of the invention and belong to the invention described in the claims and a scope equivalent thereto.</p><p id="p-0335" num="0316">For example, a program realizing the process of each embodiment described above may be provided with being stored in a computer-readable storage medium. As the storage medium, a storage medium that can store a program and can be read by a computer such as a magnetic disk, an optical disc (a CD-ROM, a CD-R, a DVD, or the like), an magneto-optical disk (an MO or the like), or a semiconductor memory may be used regardless of the storage form.</p><p id="p-0336" num="0317">In addition, the program realizing the process of each embodiment may be stored in a computer (server) connected to a network such as the Internet and be downloaded to a computer (client) through the network.</p><p id="p-0337" num="0318">While certain embodiments have been described, these embodiments have been presented by way of example only, and are not intended to limit the scope of the inventions. Indeed, the novel embodiments described herein may be embodied in a variety of other forms; furthermore, various omissions, substitutions and changes in the form of the embodiments described herein may be made without departing from the spirit of the inventions. The accompanying claims and their equivalents are intended to cover such forms or modifications as would fall within the scope and spirit of the inventions.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230007275A1-20230105-M00001.NB"><img id="EMI-M00001" he="51.48mm" wi="76.20mm" file="US20230007275A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002" nb-file="US20230007275A1-20230105-M00002.NB"><img id="EMI-M00002" he="51.48mm" wi="76.20mm" file="US20230007275A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An encoding method comprising:<claim-text>generating encoded data, the encoded data including:<claim-text>a syntax describing a first syntax element including a first number of reference images in a first reference image list and a second syntax element including a second number of reference images in a second reference image list, and</claim-text><claim-text>a syntax describing a plurality of parameters that are obtained by using a common index commonly used for the first reference image list and the second reference image list; and</claim-text></claim-text><claim-text>performing an encoding process that uses the common index.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. An encoding device comprising:<claim-text>circuitry configured to:<claim-text>generate encoded data, the encoded data including:<claim-text>a syntax describing a first syntax element including a first number of reference images in a first reference image list and a second syntax element including a second number of reference images in a second reference image list, and</claim-text><claim-text>a syntax describing a plurality of parameters that are obtained by using a common index commonly used for the first reference image list and the second reference image list; and</claim-text></claim-text><claim-text>perform an encoding process that uses the common index.</claim-text></claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. A storage device comprising:<claim-text>one or more processors configured to generate encoded data in which<claim-text>data of a first syntax is encoded, the first syntax describing a first syntax element including a first number of reference images in a first reference image list and a second syntax element including a second number of reference images in a second reference image list, and</claim-text><claim-text>data of a second syntax is encoded, the second syntax describing a plurality of parameters that are obtained by using a common index commonly used for the first reference image list and the second reference image list; and</claim-text></claim-text><claim-text>a memory configured to store the encoded data, wherein</claim-text><claim-text>the one or more processors is configured to generate encoded data using the common index.</claim-text></claim-text></claim></claims></us-patent-application>