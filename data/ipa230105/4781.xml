<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004782A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004782</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17756461</doc-number><date>20201123</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="regional"><country>EP</country><doc-number>19211218.3</doc-number><date>20191125</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>04</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>0454</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">METHOD AND SYSTEM FOR DETERMINING TASK COMPATIBILITY IN NEURAL NETWORKS</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Continental Automotive GmbH</orgname><address><city>Hannover</city><country>DE</country></address></addressbook><residence><country>DE</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Strenner</last-name><first-name>Balazs</first-name><address><city>Budapest</city><country>HU</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Nemes</last-name><first-name>Csaba</first-name><address><city>Dunakeszi</city><country>HU</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>Continental Automotive GmbH</orgname><role>03</role><address><city>Hannover</city><country>DE</country></address></addressbook></assignee></assignees><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/EP2020/083011</doc-number><date>20201123</date></document-id><us-371c12-date><date>20220525</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">The example embodiments relate to a computer-implemented method for determining clusters of tasks, the clusters at least partially including multiple tasks to be executed in a joint encoder portion of a neural network. The embodiments suggest estimating information share measures based on an auxiliary neural network in order to determine clusters of tasks to be executed in a joint encoder portion of a neural network.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="59.10mm" wi="104.22mm" file="US20230004782A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="189.06mm" wi="142.92mm" file="US20230004782A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="222.67mm" wi="150.11mm" file="US20230004782A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="226.31mm" wi="129.37mm" file="US20230004782A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application claims priority to PCT Application PCT/EP2020/083011, filed Nov. 23, 2020, which claims priority to European Application 19211218.3, filed Nov. 25, 2019. The disclosures of the above applications are incorporated herein by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">FIELD OF INVENTION</heading><p id="p-0003" num="0002">The present invention relates generally to the field of artificial neural networks. More specifically, the invention relates to a method and a system for determining the compatibility of tasks to be grouped in cluster of tasks in order to be executed in a joint encoder portion of a neural network. The neural network may be, for example, a deep neural network.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">Specifically in the field of image processing, it is known to process image information according to certain tasks, specifically, perception related tasks, i.e., if certain features can be perceived in an image. Such tasks are, for example, detection of pedestrians, detection of traffic signs, pose detection of pedestrians, detection of drivable area, etc.</p><p id="p-0005" num="0004">Specifically in vehicles, computational resources are limited. As such, detection software, specifically neural networks, are optimized to execute as many tasks as possible in order to reduce computational complexity. In other words, shared neural networks are used which are trained to perform multiple tasks (also referred to as multi-task learning) to meet computational resource constraints. So, a practical approach is to partition the tasks into clusters of tasks and to train the clusters of tasks together. A separate model is trained for each cluster, and the overall compute budget of the models is set to meet the hardware constraints.</p><p id="p-0006" num="0005">However, not all tasks perform well together on a shared neural network. In other words, the efficiency gain due to task clustering strongly depends on the compatibility of tasks to be executed by a joint neural network, specifically a joint encoder portion of a neural network. So, the key to the success of multitask learning is to find out which tasks should be trained together in a joint neural network, specifically a joint encoder portion.</p><p id="p-0007" num="0006">An approach for measuring task compatibility is to explicitly train all combinations of tasks on a range of models (which may have different size). After training all combinations, a subset of models is selected which covers all tasks, which meets the hardware constraints, and which executes each task with required accuracy. However, in real-life use cases, training all combinations is computationally expensive, takes a long time and is therefore impractical in most fields of application.</p><p id="p-0008" num="0007">SIMON VANDENHENDE ET AL: &#x201c;Branched Multi-Task Networks: Deciding What Layers To Share&#x201d;, ARXIV.org, CORNELL UNIVERSITY LIBRARY, 2010LIN LIBRARY CORNELL UNIVERSITY ITHACA, N.Y. 14853, 2 Nov. 2019, discloses a method for grouping related tasks in a shareable encoder.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0009" num="0008">It is an objective of the embodiments of the present disclosure to provide a method for determining clusters of tasks to be executed in a joint encoder portion of a neural network which on the one hand requires low computational time and resources for predicting a resource-efficient multi-task setup of a neural network and, on the other hand, provides a multi-task setup of a neural network with high computational performance. The objective is addressed by the features of the independent claims. Example embodiments are described in the dependent claims. If not explicitly indicated otherwise, embodiments of the present disclosure can be freely combined with each other.</p><p id="p-0010" num="0009">According to an aspect, the example embodiments refer to a method for determining clusters of tasks, the clusters at least partially including multiple tasks to be executed in a joint encoder portion of a neural network. The method includes the following steps described below.</p><p id="p-0011" num="0010">As a first step, information regarding a set of tasks to be processed by a neural network is provided. So, in other words, it is defined which tasks have to be performed on input information. The input information may be image information provided by an automotive sensor, for example, a camera, etc.</p><p id="p-0012" num="0011">As a further step, a first neural network is trained for a first task of the set of tasks and a second neural network is trained for a second task of the set of tasks. The training includes adapting weights of a neural network in order to improve the performance of the neural network for the respective task.</p><p id="p-0013" num="0012">In addition, an estimation neural network is formed. The estimation neural network includes the trained first neural network, the trained second neural network and an auxiliary neural network which receives information of the trained first and second neural network. The auxiliary neural network is configured to derive information indicative for the information overlap between the output of the trained first neural network and trained second neural network.</p><p id="p-0014" num="0013">After forming the estimation neural network, image information is provided as an input to the estimation neural network. Based on the image information, the trained first neural network provides first encoded image information and the trained second neural network provides second encoded image information. As the first neural network is trained for first task and the second neural network is trained for the second task, the first encoded image information includes information specific to the first task and the second encoded image information includes information specific to the second task. For example, if the first task refers to detection of pedestrians, the first encoded image information, for example, provides information indicative if one or more pedestrians are detected within the image information.</p><p id="p-0015" num="0014">Based on the auxiliary neural network, an information share measure is estimated. The information share measure is a measure regarding how much information contains the second encoded image information about the first encoded image information or vice versa. In other words, information share measure may be indicative for an overlap score which describes an information overlap between the output of the first and second encoded image information. Specifically, the estimated information share value may be conditional entropy of the second encoded image information with respect to the first encoded image information, or vice versa or information derived from the conditional entropy.</p><p id="p-0016" num="0015">In order to be able to compare task tuples and their suitability to be processed in a joint encoder portion, the steps of training neural networks for respective tasks, forming an estimation neural network, providing image information to the estimation neural network and estimating an information share measure are repeated for further tuples of tasks, thereby obtaining multiple information share measures for different tuples of tasks.</p><p id="p-0017" num="0016">In addition, a threshold value for the multiple information share measures is provided, the threshold value indicating a limit for the information overlap according to which a pair of tasks should be executed in a joint encoder portion of a neural network. More specifically, the threshold value may provide an upper bound, i.e. an information share measure below the upper bound may indicate that it is advantageous to process the tuple of tasks in a joint encoder portion.</p><p id="p-0018" num="0017">Finally, clusters of tasks to be executed in a joint encoder portion of a neural network are determined based on the information share measures and the threshold value.</p><p id="p-0019" num="0018">The method is advantageous because based on the above-mentioned method steps, it is possible to predict a multitask set-up of a neural network with reduced computational effort, wherein the predicted multitask set-up of the neural network provides high performance at given computational resources.</p><p id="p-0020" num="0019">According to an embodiment, the step of estimating an information share measure based on the auxiliary neural network includes approximating an upper bound of information missing in the second encoded image information compared to the first encoded image information. Alternatively, the step of estimating an information share measure based on the auxiliary neural network includes approximating a lower bound of information included in the second encoded image information compared to the first encoded image information. Based on at least one of the bounds, it is possible to assess how helpful the second image information is to deduce the first image information from the second image information and therefore to assess the suitability of tasks to be processed together in a joint encoder portion.</p><p id="p-0021" num="0020">According to an embodiment, the step of estimating an information share measure includes reducing a cross entropy loss defined on the information output of the auxiliary neural network by training the auxiliary neural network. More in detail, the auxiliary neural network may implement a parametrized probability function based on which conditional entropy between first and second encoded image information can be deduced. By training the auxiliary neural network, the parameters of the probability function can be refined successively thereby improving the accuracy of the probability function and therefore improving the estimate of the information share measure.</p><p id="p-0022" num="0021">According to an embodiment, the step of estimating an information share measure includes training the auxiliary neural network by adapting the weights of the auxiliary neural network and keeping the weights of the trained first neural network and the weights of trained second neural network constant. So, in other words, first and second neural networks are trained in a first step and the outputs of the trained neural networks are taken as an input while training the auxiliary neural network in a further step. Thereby, the complexity of training the estimation neural network is reduced.</p><p id="p-0023" num="0022">According to an embodiment, the step of estimating an information share measure is performed based on a variational approach, specifically, based on variational mutual information maximization approach. The approach in advantageous because based on variational approach it is possible to derive a bound for information share measure, specifically, for the conditional entropy and the bound is sufficient for comparing the suitability of each task tuple of multiple tuples of tasks to be processed in a joint encoder portion.</p><p id="p-0024" num="0023">According to an embodiment, the step of training a first neural network includes training an encoder of the first neural network for the first task and the step of training a second neural network includes training an encoder of the second neural network for the second task. Thereby, first and second neural networks are optimized for processing image information according to a specific task, for example, an image task.</p><p id="p-0025" num="0024">According to an embodiment, the step of estimating an information share measure based on the auxiliary neural network includes choosing a parameterizable distribution, the parameterizable distribution providing the parameterizable probability distribution function which can be used for determining the conditional entropy of the first encoded image given the second encoded image. In other words, a parameterizable distribution is selected in a first step, the parameterizable distribution implementing a parameterizable probability distribution function. After determining the parameters of the probability distribution function, summing up what the probability distribution function provides for each input image in the validation data set, gives a tight estimate of conditional entropy between the first and second encoded images, that is how much information exists in the first encoded image which was not contained in the second encoded image.</p><p id="p-0026" num="0025">According to an embodiment, the step of estimating an information share measure based on the auxiliary neural network includes determining parameters of the parameterizable distribution by training the auxiliary neural network in order to obtain the probability distribution function. More in detail, by training the auxiliary neural network, the parameters of the probability distribution function are optimized successively and the Kullback-Leibler-distance indicating a measure of how one probability distribution (Q) is different from a second probability distribution (P) is minimized. Thereby, the estimation of information share measure is improved.</p><p id="p-0027" num="0026">According to an embodiment, the step of estimating an information share measure based on the auxiliary neural network includes calculating information share measures for multiple different data points of multi-dimensional image information and calculating a mean information share measure by averaging the information share measures. The averaging further improves estimating which tuples of clusters should be grouped together and should be processed by a joint encoder portion.</p><p id="p-0028" num="0027">According to an embodiment, information share measures for different tuples of tasks, specifically for all tuples of tasks, are calculated in both directions, namely, per each tuple of tasks, a first information share measure being indicative that certain information regarding a second task is also included in first encoded image information, given the fact that the information regarding the second task is included in the second encoded image information and a second information share measure being indicative that certain information regarding a first task is also included in the second encoded image information, given the fact that the information regarding the first task is included in the first encoded image information. Calculating of information share measure in both directions is advantageous because information share measure is not symmetric, i.e. above-mentioned first and second information share measures of a certain task tuple are different. By considering both directions, it can be checked if both information share measures indicate that the task tuple is suitable for being processed in a joint encoder portion.</p><p id="p-0029" num="0028">According to an embodiment, the step of determining clusters of tasks includes grouping tasks together to be executed in a joint encoder portion of a neural network if the first and second information share measure is below the determined threshold value. Thereby, the performance of a neural network including multi-task architecture can be further improved because information share measures referring to both directions are considered for deciding whether a tuple of a task(s) should be processed by a joint encoder portion of the neural network or not.</p><p id="p-0030" num="0029">According to an embodiment, the step of determining clusters of tasks is performed such that the number of task groups and therefore the number of encoder portions performing the task groups is minimized. Thereby, the processing performance of the resulting neural network used for processing the plurality of tasks is further improved.</p><p id="p-0031" num="0030">According to an embodiment, computing resources are allocated to each encoder portion of the neural network based on the number of tasks being handled by the respective encoder portion. Specifically, an encoder portion handling two tasks (so-called joined encoder portion) is provided with twice the computing resources than an encoder portion handling only one task. Thereby, a distribution of computing resources according to the computational load of the respective encoder portion is obtained.</p><p id="p-0032" num="0031">According to an embodiment, the set of tasks to be processed by a neural network are image tasks. The image tasks may be, for example, image tasks in traffic environments, for example, object detection, object classification, traffic sign detection, pose detection, detection of drivable area etc.</p><p id="p-0033" num="0032">According to an embodiment, the method may be performed by a computing entity included in a vehicle, the computing entity implementing a neural network for processing a set of image tasks.</p><p id="p-0034" num="0033">According to a further aspect, the example embodiment(s) relates to a system for determining clusters of tasks, the clusters at least partially including multiple tasks to be executed in a joint encoder portion of a neural network. The system is configured to execute the steps of:</p><p id="p-0035" num="0000">a) Providing information regarding a set of tasks to be processed by a neural network;<br/>b) Training a first neural network for a first task of the set of tasks and a second neural network for a second task of the set of tasks;<br/>c) Forming an estimation neural network, the estimation neural network including the trained first neural network, the trained second neural network and an auxiliary neural network which receives information of the trained first and second neural networks;<br/>d) Providing image information as an input to the estimation neural network based on which trained first neural network provides first encoded image information and trained second neural network provides second encoded image information;<br/>e) Estimating an information share measure based on the auxiliary neural network, the information share measure being a measure regarding how much information contains the second encoded image information about the first encoded image information or vice versa;<br/>f) Repeating steps b)-e) for further tuples of tasks, thereby obtaining multiple information share measures for different tuples of tasks;<br/>g) Providing a threshold value for the multiple information share measures, the threshold value indicating a limit for the information overlap according to which a pair of tasks should be executed in a joint encoder portion of a neural network; and<br/>h) Determining clusters of tasks to be executed in the joint encoder portion of the neural network based on the information share measures and the threshold value.</p><p id="p-0036" num="0034">The term &#x201c;vehicle&#x201d; as used in the present disclosure may refer to a car, truck, bus, train or any other crafts.</p><p id="p-0037" num="0035">The term &#x201c;joined encoder portion&#x201d; as used in the present disclosure may refer to an encoder of a neural network including an encoder-decoder-architecture, wherein the encoder processes multiple tasks.</p><p id="p-0038" num="0036">The term &#x201c;essentially&#x201d; or &#x201c;approximately&#x201d; as used in the invention means deviations from the exact value by +/&#x2212;10%, preferably by +/&#x2212;5% and/or deviations in the form of changes that are insignificant for the function and/or for the traffic laws.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0039" num="0037">The various aspects of the invention, including its particular features and advantages, will be readily understood from the following detailed description and the accompanying drawings, in which:</p><p id="p-0040" num="0038"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows a schematic diagram of a neural network including multiple encoder portions and multiple decoders, the encoder portions at least partially being configured to process multiple tasks (so-called joint encoders);</p><p id="p-0041" num="0039"><figref idref="DRAWINGS">FIG. <b>2</b></figref> schematically illustrates an estimation neural network including first and second neural networks and an auxiliary neural network for estimating task compatibility;</p><p id="p-0042" num="0040"><figref idref="DRAWINGS">FIG. <b>3</b></figref> schematically illustrates mutual information and conditional entropy of first and second neural networks which provide first and second encoded image information;</p><p id="p-0043" num="0041"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates a table, wherein the table entries indicate information share measures for respective tuples of tasks;</p><p id="p-0044" num="0042"><figref idref="DRAWINGS">FIG. <b>5</b></figref> schematically illustrates the basic architecture of a neural network including a multitask architecture according to task compatibility information included in table of <figref idref="DRAWINGS">FIG. <b>4</b></figref>; and</p><p id="p-0045" num="0043"><figref idref="DRAWINGS">FIG. <b>6</b></figref> shows a schematic block diagram illustrating the steps of a method for determining clusters of tasks to be executed in a joint encoder portion.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0046" num="0044">The present invention will now be described more fully with reference to the accompanying drawings, in which example embodiments are shown. The embodiments in the figures may relate to example embodiments, while all elements and features described in connection with embodiments may be used, as far as appropriate, in combination with any other embodiment and feature as discussed herein, in particular related to any other embodiment discussed further above. However, this invention should not be construed as limited to the embodiments set forth herein. Throughout the following description similar reference numerals have been used to denote similar elements, parts, items or features, when applicable.</p><p id="p-0047" num="0045">The features of the present invention disclosed in the specification, the claims, examples and/or the figures may both separately and in any combination thereof be material for realizing the invention in various forms thereof.</p><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0048" num="0046"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows a schematic block diagram of a neural network NN. The neural network NN is configured to process image information in order to perform tasks t<b>1</b>-t<b>5</b>, preferably, multiple image tasks. For example, the neural network <b>1</b> is configured to perform image tasks like detection of pedestrians, traffic signs and/or a drivable area within the image or detecting the pose or motion of a pedestrian. It is worth mentioning that the example embodiments are not limited to the image tasks but can be used for further image issues.</p><p id="p-0049" num="0047">The neural network NN includes an encoder and a decoder. The encoder includes multiple encoder portions E<b>1</b> to E<b>3</b>. At least one of the encoder portions E<b>1</b>-E<b>3</b> is a joint encoder portion which is trained to perform two tasks. In the present example, encoder portions E<b>1</b> and E<b>3</b> are joint encoder portions wherein encoder portion E<b>1</b> is configured to handle tasks t<b>1</b> and t<b>2</b> and encoder portion E<b>3</b> is configured to handle tasks t<b>4</b> and t<b>5</b>.</p><p id="p-0050" num="0048">The decoder of neural network NN includes multiple decoder portions, wherein each decoder portion is configured to perform one task of the set of tasks. Therefore, the output of a joint encoder portion is coupled with the inputs of two decoder portions, wherein each decoder portion handles exactly one task of the pair of tasks handled by the joint encoder portion.</p><p id="p-0051" num="0049">Such neural network architecture with task clustering is beneficial because computational resources can be reduced by grouping tasks together and handling a group of tasks in one encoder portion which is trained for handling the group of tasks.</p><p id="p-0052" num="0050">In order to determine pairs of tasks which lead to reduction of computational resources if handled in a joint encoder portion, it is necessary to find out which tasks of a set of tasks should be grouped together in order to be handled by a joint encoder portion.</p><p id="p-0053" num="0051">In the following, a method and system for determining clusters of tasks to be executed in a joint encoder portion of a neural network are described based on the schematic diagram shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0054" num="0052">In <figref idref="DRAWINGS">FIG. <b>2</b></figref>, an estimation neural network ENN is used to develop a score which is an indicator for the suitability of a pair of tasks to be handled by a joint encoder portion.</p><p id="p-0055" num="0053">The estimation neural network ENN receives image information X at its input. The estimation neural network ENN includes multiple subnetworks. Image information X is provided to a first neural network NNA and a second neural network NNB. In other words, first and second neural networks NNA, NNB receive the same image information X at its input. The first neural network NNA is a neural network being trained for handling task t<b>1</b>. The second neural network NNB is a neural network being trained for handling task t<b>2</b> which is different from task <b>1</b>. For example, task t<b>1</b> may be a task for detecting pedestrians and task t<b>2</b> may be a task for detecting traffic signs.</p><p id="p-0056" num="0054">By processing image information X by the first neural network NNA, first encoded image information A is provided at the output of first neural network NNA. Similarly, by processing image information X by the second neural network NNB, second encoded image information B is provided at the output of second neural network NNB.</p><p id="p-0057" num="0055">Due to training first and second neural networks NNA, NNB for the respective tasks, first encoded image information A contains information regarding task t<b>1</b> and second encoded image information B contains information regarding task t<b>2</b>.</p><p id="p-0058" num="0056">The estimation neural network ENN further includes an auxiliary neural network AUXNN. In the present example, the auxiliary neural network AUXNN receives second encoded image information B provided by second neural network NNB. Based on the auxiliary neural network AUXNN, an information share measure is estimated, the information share measure being a measure regarding how much information contains second encoded image information B about first encoded image information A. The information share measure can be used to define clusters of tasks which are suitable to be grouped together to be handled in a joint encoder portion.</p><p id="p-0059" num="0057">In order to provide a mathematical basis for the detailed description of the method and system for determining clusters of tasks to be executed in joint encoder portions, in the following a mathematical background for estimating an upper bound of how much information includes second encoded image information B about first encoded image information A.</p><p id="p-0060" num="0058">The basis of the present invention is the finding that mutual information I(A, B) between first encoded image information A associated with task t<b>1</b> and second encoded image information B associated with task t<b>2</b> is a reliable indicator whether it is beneficial handling tasks t<b>1</b> and t<b>2</b> in a joint encoder portion or not.</p><p id="p-0061" num="0059">Mutual information I(A;B) between first encoded image information A and second encoded image information B can be defined as:</p><p id="p-0062" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>I</i>(<i>A;B</i>)=<i>H</i>(<i>A</i>)&#x2212;<i>H</i>(<i>A|B</i>);&#x2003;&#x2003;(equation 1)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0063" num="0000">wherein H(A) is the entropy of first encoded image information A and H(A|B) is conditional entropy of first encoded image information A knowing that second encoded image information B occurred.</p><p id="p-0064" num="0060"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates the relationship of I(A;B), H(A) and H(A|B) in a schematic diagram.</p><p id="p-0065" num="0061">Reordering the right side of equation 1 and reformulating conditional entropy H(A|B) leads to</p><p id="p-0066" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>I</i>(<i>A;B</i>)=<i>E</i><sub>B&#x2dc;P</sub>(<sub>B</sub>)[<i>E</i><sub>A</sub><sub><sub2>0</sub2></sub><sub>&#x2dc;P</sub>(<sub>A</sub>|<sub>B</sub>)log <i>P</i>(<i>A</i><sub>0</sub><i>|B</i>)]+<i>H</i>(<i>A</i>);&#x2003;&#x2003;(equation 2)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0067" num="0062">Here, P(A|B) denotes the conditional probability of occurrence of first encoded image information A knowing that second encoded image information B occurred.</p><p id="p-0068" num="0063">The entropy H(A) is not known but can be neglected because not the absolute value of I(A;B) is necessary but multiple mutual information values I(A;B), I(A;C), . . . , I(A;X) have to be compared which all include constant term H(A). Therefore, equation 2 can be rewritten because it is known that entropy H(A) is always a positive number.</p><p id="p-0069" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>I</i>(<i>A;B</i>)&#x2265;<i>E</i><sub>B&#x2dc;P</sub>(<sub>B</sub>)[<i>E</i><sub>A</sub><sub><sub2>0</sub2></sub><sub>&#x2dc;P</sub>(<sub>A</sub>|<sub>B</sub>)log <i>P</i>(<i>A</i><sub>0</sub><i>|B</i>)];&#x2003;&#x2003;(equation 3)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0070" num="0064">Equation 3 can be rewritten by using mathematical approach called Variational Mutual Information Maximization. The approach teaches that a log(P) function, which cannot be computed directly due to hardware and/or time constraints, can be replaced by any random probability function Q and the difference will be the Kullback-Leibler-distance D<sub>KL</sub>(P;Q) between P and Q.</p><p id="p-0071" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>I</i>(<i>A;B</i>)&#x2265;<i>E</i><sub>B&#x2dc;P</sub>(<sub>B</sub>)[<i>E</i><sub>A</sub><sub><sub2>0</sub2></sub><sub>&#x2dc;P</sub>(<sub>A</sub>|<sub>B</sub>)log <i>Q</i>(<i>A</i><sub>0</sub><i>|B</i>)+<i>D</i><sub>KL</sub>(<i>P;Q</i>)];&#x2003;&#x2003;(equation 4)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0072" num="0065">It is a property of Kullback-Leibler-distance that it is always a positive number, so if it is skipped in case of an approximation step, the rest will be a lower bound on equation 3 and on the mutual information according to Equation 1.</p><p id="p-0073" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>I</i>(<i>A;B</i>)&#x2265;<i>E</i><sub>B&#x2dc;P</sub>(<sub>B</sub>)[<i>E</i><sub>A</sub><sub><sub2>0</sub2></sub><sub>&#x2dc;P</sub>(<sub>A</sub>|<sub>B</sub>)log <i>Q</i>(<i>A</i><sub>0</sub><i>|B</i>)];&#x2003;&#x2003;(equation 5)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0074" num="0066">By using the variational approach and assuming that the term according to equation 5 is maximized when training the auxiliary neural network AUXNN providing Q function, the function Q(A<sub>0</sub>|B) approximates to P(A<sub>0</sub>|B), i.e. Kullback-Leibler-distance gets zero.</p><p id="p-0075" num="0067">Hence, after training the auxiliary neural network AUXNN, equation 5 can be used as sufficient estimate of equation 3, which itself is a practical lower bound on the mutual information I(A;B).</p><p id="p-0076" num="0068">So, as a consequence, by training auxiliary neural network AUXNN, a probability function is achieved which provides a good approximation for mutual information and the mutual information itself can be used as an indicator regarding which task tuples should be processed in a joint encoder portion.</p><p id="p-0077" num="0069">In the following, the method and system for determining clusters of tasks to be executed in joint encoder portions based on estimation neural network ENN is explained in greater detail.</p><p id="p-0078" num="0070">Broadly, the auxiliary neural network AUXNN is used to determine parameters of a parametrized probability function Q(A|B), which defines the probability event A occurring given that event B has occurred. Based on the parametrized probability function Q(A|B), which parameters can be evaluated by training the auxiliary neural network AUXNN, it is possible to approximate an upper bound of information content missing from second encoded image information B compared to first encoded image information A, that is conditional entropy H(A|B). The conditional entropy H(A|B) is a measure regarding how difficult it is for task t<b>1</b> to be executed together with task t<b>2</b> in a shared multitask fashion.</p><p id="p-0079" num="0071">More in detail, for a certain tuple of tasks t<b>1</b>, t<b>2</b>, first neural network NNA and second neural network NNB is trained. Each of the first and second neural networks NNA, NNB include an encoder and a decoder head which are specifically trained for the respective task t<b>1</b>, t<b>2</b>. After training, parameters of the trained first and second neural networks NNA, NNB are fixed and not changed which is indicated by the padlocks in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. After training first and second neural networks NNA, NNB, first neural network NNA provides first encoded image information A containing information indicative for task t<b>1</b>. Similarly, after training second neural network NNB provides second encoded image information B containing information indicative for task t<b>2</b>.</p><p id="p-0080" num="0072">In a further step, a parameterizable distribution is selected which includes multiple parameters to be chosen and which is suitable to describe a probability distribution. For example, the parameterizable distribution may describe a probability function in a parametrized way, i.e. the parameterizable distribution defines a function family based on its parameters and after determining the parameters, the parameterizable distribution defines a probability function Q(A|B) which provides the probability of the occurrence of A if B occurred.</p><p id="p-0081" num="0073">After selecting a parameterizable distribution, an auxiliary neural network AUXNN is created which implements the parameterizable distribution.</p><p id="p-0082" num="0074">In the following, the auxiliary neural network AUXNN is connected with first neural network NNA and second neural network NNB as shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref> in order to obtain estimation neural network ENN. Afterwards, the auxiliary neural network AUXNN is trained in order to determine parameters of the parameterizable distribution. In other words, by training the auxiliary neural network AUXNN, the parameters of the parameterizable distribution are modified in order to minimize cross-entropy loss on the output of auxiliary neural network AUXNN (i.e. determining log Q(A|B)) thereby obtaining an approximation of probability function P(A|B) according to equation 3. During the training, the weights of first and second neural network NNA, NNB are kept constant and only the weights of auxiliary neural network AUXNN are modified to minimize cross-entropy loss.</p><p id="p-0083" num="0075">As mentioned before, input information X of estimation neural network ENN is image information which includes a plurality of data points. Preferably, log Q(A|B)-information is calculated for multiple different data points of image information X and an average of log Q(A|B)-information obtained from multiple different data points of image information X is calculated. The average value over log Q(A|B)-information obtained from multiple different data points can be used for estimating conditional entropy H(A|B), based on which it is possible to approximate I(A|B).</p><p id="p-0084" num="0076">The estimate of conditional entropy H(A|B) may be used as information share measure, indicating how much information contains second encoded image information B about first encoded image information A.</p><p id="p-0085" num="0077">It is worth mentioning that information share measure is not symmetric, i.e. conditional entropy H(A|B) is different to conditional entropy H(B|A). As such, for each tuple of tasks, two information share measures has to be calculated.</p><p id="p-0086" num="0078">As mentioned before, information share measure is an indicator how much information contains second encoded image information B about first encoded image information A. When tasks t<b>1</b> and t<b>2</b> are highly compatible to be processed in a joint encoder portion, information share measure (which is in the present example conditional entropy) is a small positive number close to zero (e.g. lower than 1, especially lower than 0.5). When tasks t<b>1</b> and t<b>2</b> are not compatible to be processed in a joint encoder portion, information share measure is a larger positive number (e.g. greater than 1).</p><p id="p-0087" num="0079">In order to determine task compatibility of multiple tasks, two information share measures have to be calculated for each tuple of tasks (i.e. for example H(A|B) and H(B|A)).</p><p id="p-0088" num="0080"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows a table which includes values of information share measure for five tasks t<b>1</b>-t<b>5</b>. The values indicate the task compatibility of tuples of tasks in both directions. For example, task t<b>1</b> may be detecting pedestrians in image information, task t<b>2</b> may be detecting the pose of an object, especially, the pose of pedestrians, task t<b>3</b> may be a task for classifying the future motion, respectively, behavior of an object, especially a pedestrian, task t<b>4</b> may be a task for detecting traffic signs and task t<b>5</b> may be a task for detecting drivable area.</p><p id="p-0089" num="0081">So, for example, the value included in first column, second row is indicative for the difficulty to estimate the pose of an object based on the information &#x201c;pedestrian&#x201d;.</p><p id="p-0090" num="0082">In order to determine tuples of tasks to be processed in a joint encoder portion, a threshold value is chosen. The threshold value indicates a threshold below which a pair of tasks should be processed jointly by a single encoder portion. According to the present example, threshold value is 0.6. All data fields of <figref idref="DRAWINGS">FIG. <b>4</b></figref> fulfilling the condition that information share measure is lower than threshold value are marked by a gray background in <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0091" num="0083">It is worth mentioning, because of above-mentioned non-symmetry of information share measure, is has to be checked if information share measure is lower than threshold value in both fields referring to a certain tuple of tasks. In the present case, information share measures regarding tasks t<b>2</b> and t<b>3</b> and tasks t<b>1</b> and t<b>4</b> are below the threshold value and should therefore be processed in joint encoder portions.</p><p id="p-0092" num="0084">In addition, the distribution of computational resources is performed according to the association of tasks to the encoder portions. More in detail, computational resources are distributed to the encoder portions according to the number of tasks to be processed by the respective encoder portion. Preferably, computational resources are equally distributed based on the number of tasks to be processed. Therefore, in the present example, an encoder portion handling only one task is provided with &#x2155; of whole computational resources and encoder portions handling two tasks are provided with &#x2156; of whole computational resources.</p><p id="p-0093" num="0085"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates the neural network architecture which shows joint encoder portions handling pairs of tasks according to the results of table <b>4</b>. Tasks t<b>1</b> and t<b>4</b>, and tasks t<b>2</b> and t<b>3</b>, are handled by joint encoder portions whereas task t<b>5</b> is handled by a separate encoder portion. The decoder includes separate decoder portions for each task t<b>1</b>-t<b>5</b>. After determining task clusters for multitask learning, the resulting neural network according to <figref idref="DRAWINGS">FIG. <b>5</b></figref> has to be trained.</p><p id="p-0094" num="0086"><figref idref="DRAWINGS">FIG. <b>6</b></figref> shows a block diagram illustrating the method steps of a method for determining clusters of tasks for a multitask-learning neural network.</p><p id="p-0095" num="0087">As a first step, information regarding a set of tasks to be processed by a neural network is provided (S<b>10</b>).</p><p id="p-0096" num="0088">After receiving the task information, a first neural network is trained for a first task of the set of tasks and a second neural network is trained for a second task of the set of tasks (S<b>11</b>).</p><p id="p-0097" num="0089">After training the neural networks, an estimation neural network is formed. The estimation neural network includes the trained first neural network, the trained second neural network and an auxiliary neural network which receives information of the trained first and second neural networks (S<b>12</b>).</p><p id="p-0098" num="0090">After building the estimation neural network, image information is provided as an input to the estimation neural network. Based on the image information, the trained first neural network provides first encoded image information and the trained second neural network provides second encoded image information (S<b>13</b>).</p><p id="p-0099" num="0091">As a further step, an information share measure is estimated based on the auxiliary neural network, the information share measure being a measure regarding how much information contains second encoded image information about first encoded image information or vice versa (S<b>14</b>).</p><p id="p-0100" num="0092">Above-mentioned steps S<b>11</b> to S<b>14</b> are repeated for further tuples of tasks, thereby obtaining multiple information share measures for different tuples of tasks (S<b>15</b>).</p><p id="p-0101" num="0093">A threshold value is determined for the multiple information share measures, the threshold value indicating a limit for the information overlap according to which a pair of tasks should be executed in a joint encoder portion of a neural network (S<b>16</b>).</p><p id="p-0102" num="0094">Finally, clusters of tasks to be executed in a joint encoder portion of a neural network are determined based on the information share measures and the threshold value (S<b>17</b>).</p><p id="p-0103" num="0095">It should be noted that the description and drawings merely illustrate the principles of the proposed invention. Those skilled in the art will be able to implement various arrangements that, although not explicitly described or shown herein, embody the principles of the invention.</p><heading id="h-0007" level="1">LIST OF REFERENCE NUMERALS</heading><p id="p-0104" num="0000"><ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0096">A first encoded image information</li>    <li id="ul0001-0002" num="0097">AUXNN auxiliary neural network</li>    <li id="ul0001-0003" num="0098">B second encoded image information</li>    <li id="ul0001-0004" num="0099">E<b>1</b>-E<b>3</b> encoder portion</li>    <li id="ul0001-0005" num="0100">ENN estimation neural network</li>    <li id="ul0001-0006" num="0101">NN neural network</li>    <li id="ul0001-0007" num="0102">NNA first neural network</li>    <li id="ul0001-0008" num="0103">NNB second neural network</li>    <li id="ul0001-0009" num="0104">t<b>1</b>-t<b>5</b> task</li>    <li id="ul0001-0010" num="0105">X image information</li></ul></p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A computer-implemented method for determining clusters of tasks, the clusters at least partially including multiple tasks to be executed in a joint encoder portion of a neural network, the method comprising:<claim-text>a) providing information regarding a set of tasks to be processed by a neural network;</claim-text><claim-text>b) training a first neural network for a first task of the set of tasks and a second neural network for a second task of the set of tasks;</claim-text><claim-text>c) forming an estimation neural network, the estimation neural network comprising the trained first neural network, the trained second neural network and an auxiliary neural network which receives information of the trained first and second neural networks;</claim-text><claim-text>d) providing image information as an input to the estimation neural network based on which trained first neural network provides first encoded image information and trained second neural network provides second encoded image information;</claim-text><claim-text>e) estimating an information share measure based on the auxiliary neural network, the information share measure being a measure regarding how much information contains second encoded image information about first encoded image information or vice versa;</claim-text><claim-text>f) repeating steps b)-e) for further tuples of tasks, thereby obtaining multiple information share measures for different tuples of tasks;</claim-text><claim-text>g) providing a threshold value for the multiple information share measures, the threshold value indicating a limit for an information overlap according to which a tuple of tasks should be executed in a joint encoder portion of the neural network; and</claim-text><claim-text>h) determining clusters of tasks to be executed in a joint encoder portion of the neural network based on the information share measures and the threshold value.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>estimating an information share measure based on the auxiliary neural network includes approximating an upper bound of information missing in the second encoded image information compared to the first encoded image information; or</claim-text><claim-text>estimating an information share measure based on the auxiliary neural network includes approximating a lower bound of information included in the second encoded image information compared to the first encoded image information.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein estimating an information share measure includes reducing a cross entropy loss defined on information output of the auxiliary neural network by training the auxiliary neural network.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein estimating an information share measure includes training the auxiliary neural network by adapting weights of the auxiliary neural network and keeping weights of the trained first neural network and weights of the trained second neural network constant.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein estimating an information share measure is performed based on a variational approach.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein training a first neural network comprises training an encoder of the first neural network for the first task, and training a second neural network comprises training an encoder of the second neural network for the second task.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein estimating an information share measure based on the auxiliary neural network comprises choosing a parameterizable distribution, the parameterizable distribution providing a parameterizable probability distribution function used for determining the conditional entropy of the first encoded image given the second encoded image, that is how much information content exists in the first encoded image which is not covered in the second encoded image.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein estimating an information share measure based on the auxiliary neural network comprises determining parameters of the parameterizable distribution by training the auxiliary neural network in order to obtain the parameterizable probability distribution function.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein estimating an information share measure based on the auxiliary neural network comprises calculating information share measures for multiple different data points of multi-dimensional image information and calculating a mean information share measure by averaging the information share measures.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein information share measures for different tuples of tasks, specifically for all tuples of tasks, are calculated in both directions, namely, per each tuple of tasks, the first information share measure being indicative that certain information regarding the second task is also included in the first encoded image information, given the fact that the information regarding the second task is included in the second encoded image information and the second information share measure being indicative that certain information regarding the first task is also included in the second encoded image information, given the fact that the information regarding the first task is included in the first encoded image information.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein determining clusters of tasks comprises grouping tasks together to be executed in the joint encoder portion of the neural network if the first and second information share measure is below the threshold value.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein computing resources are allocated to each joint encoder portion of the neural network based on the number of tasks being handled by the respective joint encoder portion.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the set of tasks to be processed by the neural network include at least one of the following tasks: depth estimation, detection of pedestrians, detection of traffic signs, pose detection of pedestrians, detection of drivable area.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. A system for determining clusters of tasks, the clusters at least partially including multiple tasks to be executed in a joint encoder portion of a neural network, the system being configured to execute:<claim-text>a) providing information regarding a set of tasks to be processed by a neural network;</claim-text><claim-text>b) training a first neural network for a first task of the set of tasks and a second neural network for a second task of the set of tasks;</claim-text><claim-text>c) forming an estimation neural network, the estimation neural network comprising the trained first neural network, the trained second neural network and an auxiliary neural network which receives information of the trained first and second neural networks;</claim-text><claim-text>d) providing image information as an input to the estimation neural network based on which trained first neural network provides first encoded image information and trained second neural network provides second encoded image information;</claim-text><claim-text>e) estimating an information share measure based on the auxiliary neural network, the information share measure being a measure regarding how much information contains the second encoded image information about the first encoded image information or vice versa;</claim-text><claim-text>f) repeating steps b)-e) for further tuples of tasks, thereby obtaining multiple information share measures for different tuples of tasks;</claim-text><claim-text>g) providing a threshold value for the multiple information share measures, the threshold value indicating a limit for information overlap according to which a tuple of tasks should be executed in a joint encoder portion of the neural network; and</claim-text><claim-text>h) determining clusters of tasks to be executed in the joint encoder portion of the neural network based on the information share measures and the threshold value.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The method according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the variational approach comprises a variational mutual information maximization approach.</claim-text></claim></claims></us-patent-application>