<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004309A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004309</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17738270</doc-number><date>20220506</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>CN</country><doc-number>202110743316.X</doc-number><date>20210701</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>06</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>0619</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>0659</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>0631</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>067</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>0689</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">METHOD, DEVICE, AND PROGRAM PRODUCT FOR MANAGING COMPUTING SYSTEM BASED ON CLIENT/SERVER ARCHITECTURE</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>EMC IP Holding Company LLC</orgname><address><city>Hopkinton</city><state>MA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Dong</last-name><first-name>Jibing</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Gao</last-name><first-name>Jian</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Kang</last-name><first-name>Jianbin</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Gao</last-name><first-name>Hongpo</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>Gong</last-name><first-name>Shaoqin</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A technique manages a computing system based on a client/server architecture. In particular, an address mapping of a storage system is managed. The address mapping includes an association between storage spaces in a plurality of storage devices in the storage system and a user of the storage system. A data access service is provided to a client associated with the user based on the address mapping. The client includes at least any one of a user data client for accessing user data associated with the user in the storage system and a control data client for accessing control data associated with the user. Based on a client/server architecture, a uniform management mode is provided for user data and control data, so that the storage system is managed more simply and effectively.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="102.53mm" wi="116.16mm" file="US20230004309A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="114.81mm" wi="165.95mm" file="US20230004309A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="167.39mm" wi="161.04mm" file="US20230004309A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="118.53mm" wi="118.19mm" file="US20230004309A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="181.02mm" wi="162.64mm" file="US20230004309A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="173.31mm" wi="147.66mm" file="US20230004309A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="200.83mm" wi="134.54mm" file="US20230004309A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="238.84mm" wi="114.05mm" file="US20230004309A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="168.23mm" wi="140.46mm" file="US20230004309A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="124.29mm" wi="140.29mm" file="US20230004309A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION</heading><p id="p-0002" num="0001">This application claims priority to Chinese Patent Application No. CN202110743316.X, on file at the China National Intellectual Property Administration (CNIPA), having a filing date of Jul. 1, 2021, and having &#x201c;METHOD, DEVICE, AND PROGRAM PRODUCT FOR MANAGING COMPUTER SYSTEM BASED ON CLIENT/SERVER ARCHITECTURE&#x201d; as a title, the contents and teachings of which are herein incorporated by reference in their entirety.</p><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">Various implementations of the present disclosure relate to a storage system, and more particularly, to a method, a device, and a computer program product for managing a storage system based on a client/server architecture.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">With the development of data storage technologies, a variety of data storage devices have been able to provide users with increasingly high data storage capacities and at greatly improved data access speed. While data storage capabilities are improved, users also have increasingly high demands for data reliability and storage system response time. At present, various data storage systems based on a Redundant Array of Independent Disks (RAID) have been developed to improve data reliability. When one or more disks in a storage system fail, data in the failed disks may be reconstructed from data on other normally operating disks.</p><p id="p-0005" num="0004">A Mapped RAID has been developed at present. In the mapped RAID, a disk is a logical concept and may include a plurality of extents. A plurality of extents included in a logical disk may be distributed on different physical storage devices in a resource pool. For a plurality of extents in one stripe of the mapped RAID, the plurality of extents should be distributed on different physical storage devices. This makes it possible to perform, when a physical storage device where one of the plurality of extents is located fails, a reconstruction operation to restore data from physical storage devices where other extents are located.</p><p id="p-0006" num="0005">The storage system may include user data representing stored data per se, such as a user's documents, images, and videos, and control data representing data required to maintain normal operation of the storage system, such as metadata related to user data. During the operation of the storage system, how to manage various types of data in the storage system becomes a technical problem.</p><heading id="h-0004" level="1">SUMMARY OF THE INVENTION</heading><p id="p-0007" num="0006">Therefore, it is desirable to develop and implement a technical solution for managing a storage system more effectively. It is desirable that the technical solution is compatible with existing storage systems, and by modifying various configurations of the existing storage systems, a storage system may be managed more effectively.</p><p id="p-0008" num="0007">According to a first aspect of the present disclosure, a method for managing a storage system is provided. The method is implemented at a server of the storage system. In this method, an address mapping of the storage system is managed. The address mapping includes an association between storage spaces in a plurality of storage devices in the storage system and a user of the storage system. A data access service is provided to a client associated with the user based on the address mapping. The client includes at least any one of a user data client for accessing user data associated with the user in the storage system and a control data client for accessing control data associated with the user.</p><p id="p-0009" num="0008">According to a second aspect of the present disclosure, an electronic device is provided, including: at least one processor; and a memory coupled to the at least one processor, where the memory has instructions stored therein which, when executed by the at least one processor, cause the device to perform the method according to the first aspect of the present disclosure.</p><p id="p-0010" num="0009">According to a third aspect of the present disclosure, a computer program product is provided, which is tangibly stored on a non-transitory computer-readable medium and includes machine-executable instructions, where the machine-executable instructions are used to perform the method according to the first aspect of the present disclosure.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">DESCRIPTION OF THE DRAWINGS</heading><p id="p-0011" num="0010">In combination with the accompanying drawings and with reference to the following detailed description, the features, advantages, and other aspects of the implementations of the present disclosure will become more apparent, and several implementations of the present disclosure are illustrated here by way of examples rather than limitation. In the accompanying drawings,</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>1</b></figref> schematically shows a block diagram of a storage system according to a technical solution;</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>2</b></figref> schematically shows a block diagram of a storage system according to an example implementation of the present disclosure;</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>3</b></figref> schematically shows a flow chart of a method for managing a storage system according to an example implementation of the present disclosure;</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>4</b></figref> schematically shows a block diagram for managing a storage system based on a client/server architecture according to an example implementation of the present disclosure;</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>5</b></figref> schematically shows a block diagram for allocating a storage space to a user of a storage system according to an example implementation of the present disclosure;</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>6</b></figref> schematically shows a block diagram for updating an address mapping in a storage system according to an example implementation of the present disclosure;</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>7</b></figref> schematically shows a block diagram for processing a failed storage device in a storage system according to an example implementation of the present disclosure;</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>8</b></figref> schematically shows a block diagram for reconstructing data in a storage system according to an example implementation of the present disclosure; and</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>9</b></figref> schematically shows a block diagram of a device for managing a storage system according to an example implementation of the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0021" num="0020">The individual features of the various embodiments, examples, and implementations disclosed within this document can be combined in any desired manner that makes technological sense. Furthermore, the individual features are hereby combined in this manner to form all possible combinations, permutations and variants except to the extent that such combinations, permutations and/or variants have been explicitly excluded or are impractical. Support for such combinations, permutations and variants is considered to exist within this document.</p><p id="p-0022" num="0021">It should be understood that the specialized circuitry that performs one or more of the various operations disclosed herein may be formed by one or more processors operating in accordance with specialized instructions persistently stored in memory. Such components may be arranged in a variety of ways such as tightly coupled with each other (e.g., where the components electronically communicate over a computer bus), distributed among different locations (e.g., where the components electronically communicate over a computer network), combinations thereof, and so on.</p><p id="p-0023" num="0022">Preferred implementations of the present disclosure will be described in more detail below with reference to the accompanying drawings. Although the preferred implementations of the present disclosure are illustrated in the accompanying drawings, it should be understood that the present disclosure may be implemented in various forms and should not be limited by the implementations illustrated herein. Instead, these implementations are provided in order to make the present disclosure more thorough and complete, and to fully convey the scope of the present disclosure to those skilled in the art.</p><p id="p-0024" num="0023">The term &#x201c;include&#x201d; and variants thereof used herein indicate open-ended inclusion, that is, &#x201c;including but not limited to.&#x201d; Unless specifically stated, the term &#x201c;or&#x201d; means &#x201c;and/or.&#x201d; The term &#x201c;based on&#x201d; means &#x201c;based at least in part on.&#x201d; The terms &#x201c;one exemplary embodiment&#x201d; and &#x201c;one embodiment&#x201d; mean &#x201c;at least one exemplary embodiment.&#x201d; The term &#x201c;another implementation&#x201d; means &#x201c;at least one additional implementation.&#x201d; The terms &#x201c;first,&#x201d; &#x201c;second,&#x201d; and the like may refer to different or identical objects. Other explicit and implicit definitions may also be included below.</p><p id="p-0025" num="0024">Firstly, an overview of a storage system will be described with reference to <figref idref="DRAWINGS">FIG. <b>1</b></figref>. <figref idref="DRAWINGS">FIG. <b>1</b></figref> schematically shows a block diagram of storage system <b>100</b> according to a technical solution. As shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, storage system <b>100</b> may include a plurality of storage devices <b>110</b>, <b>112</b>, <b>114</b>, . . . , and <b>116</b>. Each storage device may include a plurality of extents, and the plurality of extents may be managed based on a mapped RAID. It will be appreciated that data in the storage system may include user data and control data. In general, the two types of data may have different priorities. For example, the control data requires higher reliability and response speed, and the user data may have lower requirements in both aspects than the control data.</p><p id="p-0026" num="0025">To facilitate management of the storage system, a storage space in the storage device may be divided into a control data portion (as shown in legend <b>120</b>) and a user data portion (as shown in legend <b>122</b>). The two portions are separately managed via control data manager <b>130</b> and user data manager <b>132</b>, respectively. At this time, two independent managers exist in the storage system, which increases the management complexity of the storage system. Further, storage spaces pre-allocated to the control data and the user data are fixed, and the storage spaces of the two types of data cannot be dynamically adjusted during the operation of the storage system.</p><p id="p-0027" num="0026">In order to address the defects in existing technical solutions, an example implementation of the present disclosure proposes a technical solution for managing a storage system. Hereinafter, an overview of an example implementation according to the present disclosure will be described with reference to <figref idref="DRAWINGS">FIG. <b>2</b></figref>. <figref idref="DRAWINGS">FIG. <b>2</b></figref> schematically shows a block diagram of storage system <b>200</b> according to an example implementation of the present disclosure. <figref idref="DRAWINGS">FIG. <b>2</b></figref> schematically shows a storage system based on a client/server architecture. RAID server <b>230</b> may include controller <b>226</b> for managing physical storage devices in the storage system, maintaining an address mapping between an upper-layer data storage space and users, monitoring health conditions of storage devices, managing increase and decrease of storage devices, managing spare storage devices, reconstructing data in the storage system, etc.</p><p id="p-0028" num="0027">As shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the physical storage devices may include basic storage device <b>210</b>, extended storage device <b>212</b>, and non-volatile random access memory (NVRAM) storage device <b>214</b>. Here, basic storage device <b>210</b> may represent a storage device in the storage system before extension, extended storage device <b>212</b> may represent a storage device in the storage system after extension, and NVRAM storage system <b>214</b> may represent a storage device for performing a high-speed access. The storage spaces in these physical storage devices may be added to storage resource pool <b>220</b>.</p><p id="p-0029" num="0028">Further, the storage space in storage resource pool <b>220</b> may be divided into a plurality of smaller extents, and these extents may be added to storage extent layer <b>224</b> for allocation to users. Address mapping <b>222</b> may include an address mapping associated with extents allocated to respective users. At this time, it is not necessary to divide the respective storage devices into a control data portion and a user data portion according to a predetermined ratio, but to manage the respective storage devices uniformly. With example implementations of the present disclosure, control data and user data will not be distinguished, but rather uniform RAID server <b>230</b> is employed to manage the space in the storage system. In other words, RAID server <b>230</b> will manage two types of data simultaneously.</p><p id="p-0030" num="0029">According to an example implementation of the present disclosure, RAID client <b>240</b> for connecting to a server is further provided to maintain an address mapping associated with a user, serve access requests from user equipment, receive and forward requests to increase/decrease a storage space from the user equipment, perform reconstruction operations, etc. Specifically, RAID client <b>240</b> may include address mapping <b>242</b> associated with a user, and a data access request associated with the user is managed via controller <b>244</b>. It will be appreciated that for a certain user, a user data client and a control data client may be started respectively. Here, the user data client is used to access user data associated with the user in the storage system, and the control data client is used to access control data associated with the user. RAID client <b>240</b> may cooperate with RAID server <b>230</b> to manage storage system <b>200</b>.</p><p id="p-0031" num="0030">Hereinafter, more details of an example implementation according to the present disclosure will be described with reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>. <figref idref="DRAWINGS">FIG. <b>3</b></figref> schematically shows a flow chart of method <b>300</b> for managing a storage system according to an example implementation of the present disclosure. At block <b>310</b>, an address mapping of the storage system is managed. The address mapping includes an association between storage spaces in a plurality of storage devices in the storage system and a user of the storage system. In other words, the address mapping may indicate which storage spaces in the plurality of storage devices are allocated to the user. According to an example implementation of the present disclosure, storage spaces in a plurality of storage devices may be divided into a plurality of extents at server <b>230</b>. Hereinafter, more details about the address mapping will be described with reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>4</b></figref> schematically shows block diagram <b>400</b> for managing a storage system based on a client/server architecture according to an example implementation of the present disclosure. For example, each storage device may be divided into a plurality of larger extents according to a predetermined size, and a plurality of stripes are generated by using the larger extents from the plurality of storage devices based on a mapped RAID and placed into storage resource pool <b>220</b>. Further, each stripe may be divided into a plurality of smaller extents (e.g., slices), which are placed onto storage extent layer <b>224</b>.</p><p id="p-0033" num="0032">According to an example implementation of the present disclosure, an address mapping may be provided for each user of the storage system. The address mapping may include an association between all users of storage spaces in a plurality of storage devices in the storage system. <figref idref="DRAWINGS">FIG. <b>4</b></figref> only schematically shows address mapping <b>222</b> for one user. Although not shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, address mapping <b>222</b> may include a portion for control data and a portion for user data, respectively. According to an example implementation of the present disclosure, when the storage system includes a plurality of users, each user may be separately provided with a corresponding address mapping.</p><p id="p-0034" num="0033">Returning to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, at block <b>320</b>, a data access service is provided to a client associated with the user based on address mapping <b>222</b>. It will be appreciated that RAID server <b>230</b> here may provide a uniform service interface for managing control data and user data. At this time, from the perspective of the user, RAID client <b>410</b> for control data and RAID client <b>420</b> for user data may be started, respectively. Both clients may be connected to RAID server <b>230</b> to access corresponding data in the storage system. Here, RAID client <b>410</b> may access user data associated with the user in the storage system, and RAID client <b>420</b> may access control data associated with the user.</p><p id="p-0035" num="0034">As shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, each client may include a corresponding address mapping and a corresponding controller. For example, address mapping <b>412</b> in RAID client <b>410</b> may include a backup of the portion for control data in address mapping <b>222</b>, and address mapping <b>422</b> in RAID client <b>420</b> may include a backup of the portion for control data in address mapping <b>222</b>. It will be appreciated that the address mapping at the client is read only, and when address mapping <b>222</b> at RAID server <b>230</b> is modified, RAID server <b>230</b> may synchronize the modified address mapping to the client.</p><p id="p-0036" num="0035">During the operation of the storage system, RAID clients <b>410</b> and <b>420</b> may be connected to RAID server <b>230</b> to implement data access functions. At this time, it is not necessary to provide two separate sets of managers in the storage system to respectively manage the control data and the user data, but the functions of the storage system may be implemented uniformly by using a client/server architecture shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>. With the example implementation of the present disclosure, the setting and operational complexity of the storage system can be simplified, and management functions can be reused, thereby improving the performance of the storage system.</p><p id="p-0037" num="0036">Hereinafter, various types of services involved during the operation of the storage system will be described respectively. <figref idref="DRAWINGS">FIG. <b>5</b></figref> schematically shows block diagram <b>500</b> for allocating a storage space to a user of a storage system according to an example implementation of the present disclosure. It will be appreciated that allocating a storage space here may involve allocating a storage space of control data and allocating a storage space of user data. When the storage space of the control data is in shortage, RAID client <b>410</b> may request RAID server <b>230</b> to allocate more storage spaces for storing the control data; and when the storage space of the user data is in shortage, RAID client <b>420</b> may request RAID server <b>230</b> to allocate more storage spaces for storing the user data.</p><p id="p-0038" num="0037">According to an example implementation of the present disclosure, the data access service may include a space allocation service. Specifically, the server may acquire a target storage space from the plurality of storage devices according to the received allocation request from a client for allocating the storage space to the user. Further, the server may update the address mapping based on the target storage space and notify the client of the updated address mapping such that the client may access the target storage space via the updated address mapping.</p><p id="p-0039" num="0038">For brevity, when describing the process shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, RAID client <b>410</b> for control data and RAID client <b>420</b> for user data will not be distinguished, but RAID client <b>240</b> will be used to represent both clients. At this time, the allocation processes for both data are similar. As shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, RAID client <b>240</b> may request (<b>510</b>) RAID server <b>230</b> to allocate more storage spaces. After receiving the request, RAID server <b>230</b> may acquire (<b>512</b>) a new space from allocatable extents in storage extent layer <b>224</b> and update (<b>514</b>) address mapping <b>222</b> at the server based on an address of the new space. RAID server <b>230</b> may then synchronize (<b>516</b>) the updated address mapping to RAID client <b>240</b> and inform that the allocation process has been completed. At this time, RAID client <b>240</b> may synchronize (<b>518</b>) the updated address mapping to local.</p><p id="p-0040" num="0039">According to an example implementation of the present disclosure, the process of allocating a new space does not affect data in the storage space already allocated to the user, and RAID client <b>240</b> may continue to serve a data access request from user equipment. In other words, RAID client <b>240</b> does not need to suspend the data access request from the user equipment. After completing the operation shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the user obtains more storage spaces (storage space for control data and/or storage space for user data). The user equipment may be then connected to RAID client <b>240</b> and access the newly allocated storage space by using the already updated address mapping at RAID client <b>240</b>.</p><p id="p-0041" num="0040">In contrast to conventional technical solutions where two sets of managers need to be separately provided, RAID server <b>230</b> may provide a uniform allocation interface for both clients to request allocation of more storage spaces when needed. Further, according to the conventional technical solutions, the storage device is divided into a control data portion and a user data portion in advance, such that when the storage space for one kind of data is in shortage, the allocation cannot be continued. With the example implementation of the present disclosure, the type of each storage extent is not designated when storage extent layer <b>224</b> is established, but the type of a storage extent is specified when an allocation request from a client of a certain type is received. In this way, as long as storage extent layer <b>224</b> includes available extents, a corresponding type of storage extent may be allocated according to the type of the allocation request.</p><p id="p-0042" num="0041">Specifically, upon receiving an allocation request from RAID client <b>410</b> for control data, available extents in storage extent layer <b>224</b> may be allocated to the user for storing control data; and upon receiving an allocation request from RAID client <b>420</b> for user data, available extents in storage extent layer <b>224</b> are allocated to the user for storing user data. In this way, the storage extent layer is common to both data types, thus alleviating the occurrence of shortage of available extents of a certain type. It will be appreciated that the control data and the user data may be stored in different ways in the storage system. For example, the control data may be stored in a mirrored manner to provide higher reliability. RAID 5, RAID 6, or other manners may be used to store the control data to maximize the utilization of the storage space while improving reliability.</p><p id="p-0043" num="0042">According to an example implementation of the present disclosure, the data access service may include an address mapping update service. Specifically, when it is determined that an address mapping needs to be updated, the server may notify the client of a target portion to be updated in the address mapping. The server may receive an acknowledgment from the client for the notification to begin updating the target portion in the address mapping and notify the client of the updated address mapping. Hereinafter, more details about the address mapping will be described with reference to FIG. <b>6</b>. <figref idref="DRAWINGS">FIG. <b>6</b></figref> schematically shows block diagram <b>600</b> for updating an address mapping in a storage system according to an example implementation of the present disclosure.</p><p id="p-0044" num="0043">As shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, RAID server <b>230</b> may notify (<b>610</b>) RAID client <b>240</b> to update an address mapping. It will be appreciated that RAID client <b>240</b> needs to serve a data access request from user equipment based on a copy of a local address mapping, and if the address mapping is updated at RAID server <b>230</b>, a copy of the address mapping at RAID client <b>240</b> will be inconsistent with the address mapping at RAID server <b>230</b>, thereby causing a conflict. Thus, RAID client <b>240</b> needs to quiesce a data access request before updating the address mapping.</p><p id="p-0045" num="0044">According to an example implementation of the present disclosure, RAID client <b>240</b> may perform (<b>612</b>) a quiesce operation upon receiving a notification from RAID server <b>230</b>. The quiesce operation herein involves a data access request that has not been executed yet and a data access request that is being executed. Specifically, the notification may instruct RAID client <b>240</b> to suspend a data access request for accessing the target portion in the address mapping which has not been executed yet. For example, an incoming data access request, if any, may be queued. The notification may also instruct RAID client <b>240</b> to wait for completion of a data access request for accessing the target portion which has already been executed. Assuming RAID client <b>240</b> is executing n (n is a positive integer) data access requests, these requests may be continuously executed until all of the n data access requests have been completed.</p><p id="p-0046" num="0045">After completing the quiesce operation, RAID client <b>240</b> may acknowledge (<b>614</b>) to RAID server <b>230</b> that it is ready to perform an update of the address mapping and allow RAID server <b>230</b> to begin the update process. At this time, RAID server <b>230</b> may perform an update of the address mapping and persist the updated address mapping into a lower-layer storage device. RAID server <b>230</b> may then synchronize (<b>618</b>) the updated address mapping to RAID client <b>240</b>. In other words, the updated address mapping is synchronized to RAID client <b>240</b>. Here, the synchronization operation instructs the client to replace the copy of the address mapping at the client with the updated address mapping, where the copy of the address mapping at RAID client <b>240</b> is consistent with that at RAID server <b>230</b>.</p><p id="p-0047" num="0046">Further, after the updated address mapping has been all synchronized to RAID client <b>240</b>, RAID server <b>230</b> may notify (<b>620</b>) RAID client <b>240</b> to release the quiesce operation. At this time, RAID client <b>240</b> may release (<b>622</b>) the quiesce operation and RAID client <b>240</b> may be restored to normal. If there is any data access request in the queue which has not been executed yet at this moment, RAID client <b>240</b> may process the requests in the queue sequentially. If there are no request in the queue, RAID client <b>240</b> may wait for a new data access request.</p><p id="p-0048" num="0047">With the example implementation of the present disclosure, whether the address mapping to be updated involves control data or user data, it may be processed in the manner shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. At this time, RAID server <b>230</b> may serve both control data and user data. If the address mapping to be updated involves control data, RAID server <b>230</b> may send a notification to RAID client <b>410</b> for the control data. If the address mapping to be updated involves user data, RAID server <b>230</b> may send a notification to RAID client <b>420</b> for the user data. In this way, the complexity of the operation of the storage system can be simplified, thereby improving the performance of the storage system.</p><p id="p-0049" num="0048">According to an example implementation of the present disclosure, the data access service may include a failure processing service. If RAID server <b>230</b> determines that a failed storage device has occurred in the plurality of storage devices, a failure processing service may be initiated. Specifically, a failed storage space involving the failed storage device in the address mapping may be determined, and the corresponding client is notified of the failed storage space. Hereinafter, more details about the failure processing service will be described with reference to <figref idref="DRAWINGS">FIG. <b>7</b></figref>. <figref idref="DRAWINGS">FIG. <b>7</b></figref> schematically shows block diagram <b>700</b> for processing a failed storage device in a storage system according to an example implementation of the present disclosure.</p><p id="p-0050" num="0049">As shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, after a failed storage device is detected, RAID server <b>230</b> may determine (<b>712</b>) all extents which are affected by the failed storage device. It will be appreciated that since the storage system is a mapped RAID-based storage system, one or more extents in storage extent layer <b>224</b> located in the failed storage device will be affected. At this time, the affected extents may be marked as failed extents. If the failed extents have not been allocated yet, the user won't be affected. If the failed extents have been allocated to a user, the user allocated with the failed extents may be found based on the address mapping, and RAID client <b>240</b> associated with the user is notified (<b>714</b>) of the failure. It will be appreciated that, if extents in the failed storage device have been allocated to a plurality of users, similar operations may be performed for each user. Hereinafter, description will be made by way of example only with respect to the operation for one user.</p><p id="p-0051" num="0050">As shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, RAID server <b>230</b> may notify (<b>714</b>) RAID client <b>240</b> to perform a quiesce operation. It will be appreciated that the presence of a failed storage device may cause some storage spaces in the local copy of the address mapping of RAID client <b>240</b> to be unavailable. Thus, RAID client <b>240</b> needs to stop service. Specifically, RAID client <b>240</b> may perform (<b>716</b>) a quiesce operation. According to an example implementation of the present disclosure, RAID client <b>240</b> may quiesce a data access request associated with the failed storage space and allow a data access request to a normal storage space. In the context of the present disclosure, the process of the quiesce operation is similar and will not be described in further detail. RAID client <b>240</b> may acknowledge (<b>718</b>) to RAID server <b>230</b> that the quiesce operation has been completed.</p><p id="p-0052" num="0051">RAID server <b>230</b> may then mark (<b>720</b>) the state of the failed extents as &#x201c;degraded.&#x201d; It will be appreciated that after a failure occurs, some failed storage devices may be restored to normal by rebooting, etc., but some failed storage devices may not be restored to normal. RAID server <b>230</b> may set (<b>722</b>) a timer (e.g., 5 minutes or other time intervals) to check whether the failed storage device may be restored to normal within a predetermined time length. RAID server <b>230</b> may then synchronize (<b>724</b>) the state of the extents and associated metadata to RAID client <b>240</b> and notify RAID client <b>240</b> to release the quiesce operation. RAID client <b>240</b> may then release (<b>726</b>) the quiesce operation.</p><p id="p-0053" num="0052">At this time, RAID server <b>230</b> may determine subsequent processing operations based on a threshold time length associated with the failed storage device. If the failed storage device is restored within a predetermined threshold time length, it indicates that the failed storage device has restored to normal and may continue to be used. The state of the failed storage space may be restored from &#x201c;degraded&#x201d; to a normal state, and the client is notified that the failed storage device has been restored. Specifically, RAID server <b>230</b> may synchronize the latest state of the various extents and associated metadata to RAID client <b>240</b> to instruct that RAID client <b>240</b> may continue to process data access requests.</p><p id="p-0054" num="0053">Continuing to describe the case where the failed storage device is not restored with reference to <figref idref="DRAWINGS">FIG. <b>7</b></figref>, RAID server <b>230</b> may detect (<b>728</b>) that the timer expires and the failed storage device has not been restored. This indicates that the failed storage device has not been restored within a predetermined threshold time length, and that the failed storage device cannot continue to be used. At this time, RAID server <b>230</b> may notify (<b>730</b>) RAID client <b>240</b> to perform the quiesce operation again. RAID client <b>240</b> may perform (<b>732</b>) the quiesce operation and inform RAID server <b>230</b> that the quiesce operation has been completed. Further, RAID server <b>230</b> may search for available spare extents in the storage system to replace (<b>734</b>) the failed extents. Specifically, each failed extent may be replaced by selecting an appropriate spare extent from storage extent layer <b>224</b> based on an RAID standard used. Further, the corresponding address mapping may be updated based on the addresses of respective normal extents.</p><p id="p-0055" num="0054">After all failed extents have been replaced with normal extents and the address mapping at RAID server <b>230</b> has been updated, RAID server <b>230</b> may synchronize (<b>736</b>) the updated address mapping to RAID client <b>240</b> and notify RAID client <b>240</b> to release the quiesce operation. Specifically, RAID server <b>230</b> may instruct the client to update a copy of the address mapping at the client with the updated address mapping. At this time, the local address mapping at RAID client <b>240</b> has been updated, and all storage spaces involved in the address mapping are normal storage spaces and may be used. RAID client <b>240</b> may release (<b>738</b>) the quiesce operation and be restored to normal. At this time, the storage space in the address mapping at RAID server <b>230</b> and RAID client <b>240</b> also includes extents being in a &#x201c;degraded&#x201d; state, so RAID server <b>230</b> may notify (<b>740</b>) RAID client <b>240</b> to perform a reconstruction operation.</p><p id="p-0056" num="0055">With the example implementation of the present disclosure, when processing a failed storage device, various extents in the failed storage device which are used to store control data or user data are not treated differently, but are provided with a uniform failure processing interface through RAID server <b>230</b>. In this way, RAID server <b>230</b> may respectively notify corresponding RAID clients to perform the process shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref> according to the type of data included in the failed extents.</p><p id="p-0057" num="0056">According to an example implementation of the present disclosure, the data access service may include a reconstruction service. Specifically, RAID server <b>230</b> may initiate a reconstruction operation. Hereinafter, more details about the reconstruction will be described with reference to <figref idref="DRAWINGS">FIG. <b>8</b></figref>. <figref idref="DRAWINGS">FIG. <b>8</b></figref> schematically shows block diagram <b>800</b> for reconstructing data in a storage system according to an example implementation of the present disclosure. As shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, RAID server <b>230</b> may notify (<b>740</b>) RAID client <b>240</b> to initiate a reconstruction operation. At this time, RAID client <b>240</b> may traverse (<b>810</b>) all extents in a &#x201c;degraded&#x201d; state and perform (<b>812</b>) the reconstruction process based on the corresponding RAID standard. According to an example implementation of the present disclosure, the specific steps of the reconstruction process are similar to those of the conventional technical solutions and will not be described in detail. After the reconstruction operation has been completed for all the extents in a &#x201c;degraded&#x201d; state, RAID client <b>240</b> may acknowledge (<b>814</b>) to RAID server <b>230</b> that the reconstruction has been completed. At this time, RAID server <b>230</b> may update (<b>816</b>) the state of the relevant extents from &#x201c;degraded&#x201d; to normal.</p><p id="p-0058" num="0057">With the example implementation of the present disclosure, during the reconstruction operation, extents in a &#x201c;degraded&#x201d; state which are used to store control data or user data are not treated differently, but are provided with a uniform data reconstruction interface through RAID server <b>230</b>. In this way, RAID server <b>230</b> may respectively notify corresponding RAID clients to perform the reconstruction operation according to the type of data included in the &#x201c;degraded&#x201d; extents.</p><p id="p-0059" num="0058">It will be appreciated that the specific process of how to manage a storage system based on a client/server architecture has been described above merely by way of example of allocating a storage space, updating an address mapping, processing a failed storage device, and performing a reconstruction operation. According to an example implementation of the present disclosure, other services may also be implemented in the storage system based on the client/server architecture based on the principles described above. For example, RAID client <b>240</b> may receive a data access request from the user equipment and perform read, write, and update operations accordingly. As another example, a new storage device may be added to the storage system when the idle storage space in the storage system is in shortage. At this time, RAID server <b>230</b> may add extents in the new storage device to storage extent layer <b>224</b> for allocation. As another example, when the workload of each storage device is unbalanced, RAID server <b>230</b> may perform load balancing operations and synchronize updated address mappings to corresponding RAID clients based on the method described above.</p><p id="p-0060" num="0059">According to an example implementation of the present disclosure, in a client/server architecture, it may not be necessary to distinguish the types of data stored, but may provide a uniform service interface to control data and user data. In this way, the complexity of the operation of the storage system can be simplified to provide higher performance.</p><p id="p-0061" num="0060">The examples of the method according to the present disclosure have been described in detail above with reference to <figref idref="DRAWINGS">FIGS. <b>2</b> to <b>8</b></figref>, and the implementations of a corresponding apparatus will be described below. According to an example implementation of the present disclosure, an apparatus for managing a storage system is provided. The apparatus operates as a server of the storage system and includes: a management module, configured to manage an address mapping of the storage system, the address mapping including an association between storage spaces in a plurality of storage devices in the storage system and a user of the storage system; and a service module, configured to provide a data access service to a client associated with the user based on the address mapping. Here, the client includes at least any one of a user data client for accessing user data associated with the user in the storage system and a control data client for accessing control data associated with the user. According to an example implementation of the present disclosure, the apparatus further includes modules for performing other steps in method <b>300</b> described above.</p><p id="p-0062" num="0061"><figref idref="DRAWINGS">FIG. <b>9</b></figref> schematically shows a block diagram of device <b>900</b> for managing a computing system according to an example implementation of the present disclosure. As shown in the figure, device <b>900</b> includes central processing unit (CPU) <b>901</b> that may execute various appropriate actions and processing according to computer program instructions stored in read-only memory (ROM) <b>902</b> or computer program instructions loaded from storage unit <b>908</b> into random access memory (RAM) <b>903</b>. Various programs and data required for the operation of device <b>900</b> may also be stored in RAM <b>903</b>. CPU <b>901</b>, ROM <b>902</b>, and RAM <b>903</b> are connected to each other through bus <b>904</b>. Input/output (I/O) interface <b>905</b> is also connected to bus <b>904</b>.</p><p id="p-0063" num="0062">A plurality of components in device <b>900</b> are connected to I/O interface <b>905</b>, including: input unit <b>906</b>, such as a keyboard and a mouse; output unit <b>907</b>, such as various types of displays and speakers; storage unit <b>908</b>, such as a magnetic disk and an optical disc; and communication unit <b>909</b>, such as a network card, a modem, and a wireless communication transceiver. Communication unit <b>909</b> allows device <b>900</b> to exchange information/data with other devices via a computer network, such as the Internet, and/or various telecommunication networks.</p><p id="p-0064" num="0063">The various processes and processing described above, such as method <b>300</b>, may be executed by processing unit <b>901</b>. For example, in some implementations, method <b>300</b> may be implemented as a computer software program that is tangibly included in a machine-readable medium, such as storage unit <b>908</b>. In some implementations, part or all of the computer program may be loaded and/or installed on device <b>900</b> via ROM <b>902</b> and/or communication unit <b>909</b>. When the computer program is loaded into RAM <b>903</b> and executed by CPU <b>901</b>, one or more steps of method <b>300</b> described above may be performed. Alternatively, in other implementations, CPU <b>901</b> may also be configured in any other suitable manner to implement the above-described process/method.</p><p id="p-0065" num="0064">According to an example implementation of the present disclosure, an electronic device includes: at least one processor; a volatile memory; and a memory coupled to the at least one processor, where the memory has instructions stored therein which, when executed by the at least one processor, cause the device to perform a method for managing a storage system. The method is implemented at a server of the storage system, and includes: managing an address mapping of the storage system, the address mapping including an association between storage spaces in a plurality of storage devices in the storage system and a user of the storage system; and providing a data access service to a client associated with the user based on the address mapping, where the client includes at least any one of a user data client for accessing user data associated with the user in the storage system and a control data client for accessing control data associated with the user.</p><p id="p-0066" num="0065">According to an example implementation of the present disclosure, providing the data access service includes: in response to receiving an allocation request from the client for allocating a storage space to the user, acquiring a target storage space from the plurality of storage devices based on the allocation request; updating the address mapping based on the target storage space; and notifying the client of the updated address mapping such that the client accesses the target storage space via the updated address mapping.</p><p id="p-0067" num="0066">According to an example implementation of the present disclosure, providing the data access service includes: in response to determining that the address mapping is to be updated, notifying the client of a target portion to be updated in the address mapping such that the client quiesces a data access request associated with the target portion; updating the target portion in the address mapping in response to receiving an acknowledgment from the client for the notification; and notifying the client of the updated address mapping.</p><p id="p-0068" num="0067">According to an example implementation of the present disclosure, notifying the client of the target portion includes: instructing the client to suspend a data access request for accessing the target portion which has not been executed yet; and instructing the client to wait for completion of a data access request which is being executed for accessing the target portion.</p><p id="p-0069" num="0068">According to an example implementation of the present disclosure, notifying the client of the updated address mapping includes: instructing the client to update an address mapping at the client by using the updated address mapping; and instructing the client to release the quiesce.</p><p id="p-0070" num="0069">According to an example implementation of the present disclosure, providing the data access service includes: in response to determining that a failed storage device has occurred in the plurality of storage devices, determining a failed storage space involving the failed storage device in the address mapping; notifying the client of the failed storage space such that the client quiesces a data access request associated with the failed storage space; marking the failed storage space as a degraded state in response to receiving an acknowledgment from the client for the notification; and processing the failed storage device based on a threshold time length associated with the failed storage device.</p><p id="p-0071" num="0070">According to an example implementation of the present disclosure, processing the failed storage device based on the threshold time length includes: in response to determining that the failed storage device is restored within the threshold time length, setting a state of the failed storage space to a normal state; and notifying the client that the failed storage device has been restored.</p><p id="p-0072" num="0071">According to an example implementation of the present disclosure, processing the failed storage device based on the threshold time length includes: in response to determining that the failed storage device has not been restored within the threshold time length, notifying the client that the failed storage device has not been restored such that the client quiesces a data access request associated with the failed storage space; updating the address mapping based on a normal storage space in the plurality of storage devices; and notifying the client of the updated address mapping.</p><p id="p-0073" num="0072">According to an example implementation of the present disclosure, notifying the client of the updated address mapping includes: instructing the client to update an address mapping at the client by using the updated address mapping; and instructing the client to release the quiesce.</p><p id="p-0074" num="0073">According to an example implementation of the present disclosure, the storage system includes a storage system based on an RAID, and the method further includes: instructing the client to perform a reconstruction operation based on the updated address mapping.</p><p id="p-0075" num="0074">According to an example implementation of the present disclosure, a computer program product is provided, which is tangibly stored on a non-transitory computer-readable medium and includes machine-executable instructions, where the machine-executable instructions are used to perform the method according to the present disclosure.</p><p id="p-0076" num="0075">According to an example implementation of the present disclosure, a computer-readable medium is provided, the computer-readable medium storing machine-executable instructions which, when executed by at least one processor, cause the at least one processor to implement the method according to the present disclosure.</p><p id="p-0077" num="0076">The present disclosure may be a method, a device, a system, and/or a computer program product. The computer program product may include a computer-readable storage medium on which computer-readable program instructions for performing various aspects of the present disclosure are loaded.</p><p id="p-0078" num="0077">The computer-readable storage medium may be a tangible device that may hold and store instructions used by an instruction-executing device. For example, the computer-readable storage medium may be, but is not limited to, an electrical storage device, a magnetic storage device, an optical storage device, an electromagnetic storage device, a semiconductor storage device, or any suitable combination of the foregoing. More specific examples (a non-exhaustive list) of the computer-readable storage medium include: a portable computer disk, a hard disk, a RAM, a ROM, an erasable programmable read only memory (EPROM or flash memory), a static random access memory (SRAM), a portable compact disk read only memory (CD-ROM), a digital versatile disk (DVD), a memory stick, a floppy disk, a mechanical encoding device such as a punch card or a protruding structure within a groove having instructions stored thereon, and any suitable combination of the foregoing. The computer-readable storage medium used herein is not to be interpreted as transient signals per se, such as radio waves or other freely propagating electromagnetic waves, electromagnetic waves propagating through waveguides or other transmission media (e.g., light pulses through fiber-optic cables), or electrical signals transmitted through electrical wires.</p><p id="p-0079" num="0078">The computer-readable program instructions described herein may be downloaded from a computer-readable storage medium to various computing/processing devices or downloaded to an external computer or external storage device via a network, such as the Internet, a local area network, a wide area network, and/or a wireless network. The network may include copper transmission cables, fiber optic transmission, wireless transmission, routers, firewalls, switches, gateway computers, and/or edge servers. A network adapter card or network interface in each computing/processing device receives computer-readable program instructions from a network and forwards the computer-readable program instructions for storage in a computer-readable storage medium in each computing/processing device.</p><p id="p-0080" num="0079">The computer program instructions for performing the operations of the present disclosure may be assembly instructions, instruction set architecture (ISA) instructions, machine instructions, machine-related instructions, microcode, firmware instructions, state setting data, or source code or object code written in any combination of one or more programming languages, wherein the programming languages include object-oriented programming languages such as Smalltalk and C++, and conventional procedural programming languages such as the C language or similar programming languages. The computer-readable program instructions may be executed entirely on a user computer, partly on a user computer, as a stand-alone software package, partly on a user computer and partly on a remote computer, or entirely on a remote computer or a server. In a case where a remote computer is involved, the remote computer can be connected to a user computer through any kind of networks, including a local area network (LAN) or a wide area network (WAN), or can be connected to an external computer (for example, connected through the Internet using an Internet service provider). In some implementations, an electronic circuit, for example, a programmable logic circuit, a field programmable gate array (FPGA), or a programmable logic array (PLA), is personalized by utilizing state information of the computer-readable program instructions, wherein the electronic circuit may execute the computer-readable program instructions so as to implement various aspects of the present disclosure.</p><p id="p-0081" num="0080">Various aspects of the present disclosure are described herein with reference to flow charts and/or block diagrams of the method, the apparatus (system), and the computer program product according to implementations of the present disclosure. It should be understood that each block of the flow charts and/or the block diagrams and combinations of blocks in the flow charts and/or the block diagrams may be implemented by the computer-readable program instructions.</p><p id="p-0082" num="0081">These computer-readable program instructions may be provided to a processing unit of a general-purpose computer, a special-purpose computer, or a further programmable data processing apparatus, thereby producing a machine, such that these instructions, when executed by the processing unit of the computer or the further programmable data processing apparatus, produce means (e.g., specialized circuitry) for implementing the functions/actions specified in one or more blocks in the flow charts and/or block diagrams. These computer-readable program instructions may also be stored in a computer-readable storage medium, and these instructions cause a computer, a programmable data processing apparatus, and/or other devices to operate in a specific manner; and thus the computer-readable medium having instructions stored includes an article of manufacture that includes instructions that implement various aspects of the functions/actions specified in one or more blocks in the flow charts and/or block diagrams.</p><p id="p-0083" num="0082">The computer-readable program instructions may also be loaded to a computer, a further programmable data processing apparatus, or a further device, so that a series of operating steps may be performed on the computer, the further programmable data processing apparatus, or the further device to produce a computer-implemented process, such that the instructions executed on the computer, the further programmable data processing apparatus, or the further device may implement the functions/actions specified in one or more blocks in the flow charts and/or block diagrams.</p><p id="p-0084" num="0083">The flow charts and block diagrams in the drawings illustrate the architectures, functions, and operations of possible implementations of the systems, methods, and computer program products according to a plurality of implementations of the present disclosure. In this regard, each block in the flow charts or block diagrams may represent a module, a program segment, or part of an instruction, the module, program segment, or part of an instruction including one or more executable instructions for implementing specified logical functions. In some alternative implementations, functions marked in the blocks may also occur in an order different from that marked in the accompanying drawings. For example, two successive blocks may actually be executed in parallel substantially, and sometimes they may also be executed in an inverse order, which depends on involved functions. It should be further noted that each block in the block diagrams and/or flow charts as well as a combination of blocks in the block diagrams and/or flow charts may be implemented using a special hardware-based system that executes specified functions or actions, or using a combination of special hardware and computer instructions.</p><p id="p-0085" num="0084">Various implementations of the present disclosure have been described above. The above description is illustrative and not exhaustive, and is not limited to the various implementations disclosed. Numerous modifications and alterations are apparent to persons of ordinary skill in the art without departing from the scope and spirit of the illustrated implementations. The selection of terms as used herein is intended to best explain principles and practical applications of the various implementations or improvements to technologies on the market, or to enable other persons of ordinary skill in the art to understand the implementations disclosed here.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method for managing a storage system, the method being implemented at a server of the storage system, and the method comprising:<claim-text>managing an address mapping of the storage system, the address mapping comprising an association between storage spaces in a plurality of storage devices in the storage system and a user of the storage system; and</claim-text><claim-text>providing a data access service to a client associated with the user based on the address mapping, the client comprising at least any one of a user data client for accessing user data associated with the user in the storage system and a control data client for accessing control data associated with the user.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein providing the data access service comprises: in response to receiving an allocation request from the client for allocating a storage space to the user,<claim-text>acquiring a target storage space from the plurality of storage devices based on the allocation request;</claim-text><claim-text>updating the address mapping based on the target storage space; and</claim-text><claim-text>notifying the client of the updated address mapping such that the client accesses the target storage space via the updated address mapping.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein providing the data access service comprises: in response to determining that the address mapping is to be updated,<claim-text>notifying the client of a target portion to be updated in the address mapping such that the client quiesces a data access request associated with the target portion;</claim-text><claim-text>updating the target portion in the address mapping in response to receiving an acknowledgment from the client for the notification; and</claim-text><claim-text>notifying the client of the updated address mapping.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein notifying the client of the target portion comprises:<claim-text>instructing the client to suspend a data access request for accessing the target portion which has not been executed yet; and</claim-text><claim-text>instructing the client to wait for completion of a data access request which is being executed for accessing the target portion.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein notifying the client of the updated address mapping comprises:<claim-text>instructing the client to update an address mapping at the client by using the updated address mapping; and</claim-text><claim-text>instructing the client to release the quiesce.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein providing the data access service comprises: in response to determining that a failed storage device has occurred in the plurality of storage devices,<claim-text>determining a failed storage space involving the failed storage device in the address mapping;</claim-text><claim-text>notifying the client of the failed storage space such that the client quiesces a data access request associated with the failed storage space;</claim-text><claim-text>marking the failed storage space as a degraded state in response to receiving an acknowledgment from the client for the notification; and</claim-text><claim-text>processing the failed storage device based on a threshold time length associated with the failed storage device.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein processing the failed storage device based on the threshold time length comprises: in response to determining that the failed storage device is restored within the threshold time length,<claim-text>setting a state of the failed storage space to a normal state; and</claim-text><claim-text>notifying the client that the failed storage device has been restored.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein processing the failed storage device based on the threshold time length comprises: in response to determining that the failed storage device has not been restored within the threshold time length,<claim-text>notifying the client that the failed storage device has not been restored such that the client quiesces a data access request associated with the failed storage space;</claim-text><claim-text>updating the address mapping based on a normal storage space in the plurality of storage devices; and</claim-text><claim-text>notifying the client of the updated address mapping.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein notifying the client of the updated address mapping comprises:<claim-text>instructing the client to update an address mapping at the client by using the updated address mapping; and</claim-text><claim-text>instructing the client to release the quiesce.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the storage system comprises a storage system based on a redundant array of independent disks (RAID), and the method further comprises: instructing the client to perform a reconstruction operation based on the updated address mapping.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. An electronic device, comprising:<claim-text>at least one processor;</claim-text><claim-text>a volatile memory; and</claim-text><claim-text>a memory coupled to the at least one processor, wherein the memory has instructions stored therein that, when executed by the at least one processor, cause the device to perform a method for managing a storage system, the method is implemented at a server of the storage system, and the method comprises:<claim-text>managing an address mapping of the storage system, the address mapping comprising an association between storage spaces in a plurality of storage devices in the storage system and a user of the storage system; and</claim-text><claim-text>providing a data access service to a client associated with the user based on the address mapping, the client comprising at least any one of a user data client for accessing user data associated with the user in the storage system and a control data client for accessing control data associated with the user.</claim-text></claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The device according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein providing the data access service comprises: in response to receiving an allocation request from the client for allocating a storage space to the user,<claim-text>acquiring a target storage space from the plurality of storage devices based on the allocation request;</claim-text><claim-text>updating the address mapping based on the target storage space; and</claim-text><claim-text>notifying the client of the updated address mapping such that the client accesses the target storage space via the updated address mapping.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The device according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein providing the data access service comprises: in response to determining that the address mapping is to be updated,<claim-text>notifying the client of a target portion to be updated in the address mapping such that the client quiesces a data access request associated with the target portion;</claim-text><claim-text>updating the target portion in the address mapping in response to receiving an acknowledgment from the client for the notification; and</claim-text><claim-text>notifying the client of the updated address mapping.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The device according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein notifying the client of the target portion comprises:<claim-text>instructing the client to suspend a data access request for accessing the target portion which has not been executed yet; and</claim-text><claim-text>instructing the client to wait for completion of a data access request which is being executed for accessing the target portion.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The device according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein notifying the client of the updated address mapping comprises:<claim-text>instructing the client to update an address mapping at the client by using the updated address mapping; and</claim-text><claim-text>instructing the client to release the quiesce.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The device according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein providing the data access service comprises: in response to determining that a failed storage device has occurred in the plurality of storage devices,<claim-text>determining a failed storage space involving the failed storage device in the address mapping;</claim-text><claim-text>notifying the client of the failed storage space such that the client quiesces a data access request associated with the failed storage space;</claim-text><claim-text>marking the failed storage space as a degraded state in response to receiving an acknowledgment from the client for the notification; and</claim-text><claim-text>processing the failed storage device based on a threshold time length associated with the failed storage device.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The device according to <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein processing the failed storage device based on the threshold time length comprises: in response to determining that the failed storage device is restored within the threshold time length,<claim-text>setting a state of the failed storage space to a normal state; and</claim-text><claim-text>notifying the client that the failed storage device has been restored.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The device according to <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein processing the failed storage device based on the threshold time length comprises: in response to determining that the failed storage device has not been restored within the threshold time length,<claim-text>notifying the client that the failed storage device has not been restored such that the client quiesces a data access request associated with the failed storage space;</claim-text><claim-text>updating the address mapping based on a normal storage space in the plurality of storage devices; and</claim-text><claim-text>notifying the client of the updated address mapping.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The device according to <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein notifying the client of the updated address mapping comprises:<claim-text>instructing the client to update an address mapping at the client by using the updated address mapping; and</claim-text><claim-text>instructing the client to release the quiesce,</claim-text><claim-text>wherein the storage system comprises a storage system based on a redundant array of independent disks (RAID), and the method further comprises: instructing the client to perform a reconstruction operation based on the updated address mapping.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. A computer program product having a non-transitory computer readable medium which stores a set of instructions to manage a storage system; the set of instructions, when carried out by computerized circuitry, causing the computerized circuitry to perform a method of:<claim-text>managing an address mapping of the storage system, the address mapping comprising an association between storage spaces in a plurality of storage devices in the storage system and a user of the storage system; and</claim-text><claim-text>providing a data access service to a client associated with the user based on the address mapping, the client comprising at least any one of a user data client for accessing user data associated with the user in the storage system and a control data client for accessing control data associated with the user.</claim-text></claim-text></claim></claims></us-patent-application>