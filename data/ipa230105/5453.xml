<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005454A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005454</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17756419</doc-number><date>20201126</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>JP</country><doc-number>2019-219694</doc-number><date>20191204</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>09</class><subclass>G</subclass><main-group>5</main-group><subgroup>38</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>74</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>40</main-group><subgroup>16</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>02</class><subclass>B</subclass><main-group>27</main-group><subgroup>01</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>09</class><subclass>G</subclass><main-group>5</main-group><subgroup>38</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>761</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>40</main-group><subgroup>161</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>02</class><subclass>B</subclass><main-group>27</main-group><subgroup>0172</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>2201</main-group><subgroup>07</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">INFORMATION PROCESSING DEVICE, INFORMATION PROCESSING METHOD, AND INFORMATION PROCESSING PROGRAM</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>SONY GROUP CORPORATION</orgname><address><city>TOKYO</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>ICHIKAWA</last-name><first-name>HIROTAKE</first-name><address><city>TOKYO</city><country>JP</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>ISHIHARA</last-name><first-name>ATSUSHI</first-name><address><city>TOKYO</city><country>JP</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>MURATA</last-name><first-name>RYOSUKE</first-name><address><city>TOKYO</city><country>JP</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>WAKABAYASHI</last-name><first-name>HAJIME</first-name><address><city>TOKYO</city><country>JP</country></address></addressbook></inventor></inventors></us-parties><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/JP2020/044031</doc-number><date>20201126</date></document-id><us-371c12-date><date>20220525</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">An information processing device (<b>1</b>) according to an embodiment includes a display control unit (<b>34</b>) and a decision unit (<b>31</b>). The display control unit (<b>34</b>) displays a content image on a head-mounted display. During display of the content image by the display control unit (<b>34</b>), the decision unit (<b>31</b>) decides whether or not a surrounding person exists in a front direction of the head-mounted display on the basis of a camera image obtained by capturing an image of a surrounding environment of the head-mounted display. In a case where the decision unit (<b>31</b>) decides that a surrounding person exists, the display control unit (<b>34</b>) moves a display position of the content image.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="85.94mm" wi="99.82mm" file="US20230005454A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="213.11mm" wi="105.83mm" file="US20230005454A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="237.24mm" wi="150.54mm" file="US20230005454A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="210.31mm" wi="81.53mm" file="US20230005454A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="176.02mm" wi="154.69mm" file="US20230005454A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="170.52mm" wi="154.94mm" file="US20230005454A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="205.66mm" wi="103.55mm" file="US20230005454A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="158.50mm" wi="139.62mm" file="US20230005454A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="135.04mm" wi="86.28mm" file="US20230005454A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="142.58mm" wi="149.01mm" file="US20230005454A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">FIELD</heading><p id="p-0002" num="0001">The present invention relates to an information processing device, an information processing method, and an information processing program.</p><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">Conventionally, there is an information processing device that provides virtual reality for a wearer of a head-mounted display. A field of view of the wearer is optically blocked during display of content by the head-mounted display.</p><p id="p-0004" num="0003">There is disclosed a technology that, in a case where a surrounding person who attempts to communicate with the wearer is detected on the basis of an external situation of the head-mounted display, notifies the wearer of existence of the detected surrounding person (see, for example, Patent Literature 1).</p><heading id="h-0003" level="1">CITATION LIST</heading><heading id="h-0004" level="1">Patent Literature</heading><p id="p-0005" num="0000"><ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0004">Patent Literature 1: WO 2014/156388 A</li></ul></p><heading id="h-0005" level="1">SUMMARY</heading><heading id="h-0006" level="1">Technical Problem</heading><p id="p-0006" num="0005">In such a head-mounted display, it is not easy for a surrounding person to determine which one of the content and a real space the wearer is viewing even if the wearer is viewing content displayed on the head-mounted display.</p><p id="p-0007" num="0006">Therefore, the surrounding person may feel that the wearer of the head-mounted display gazes at the surrounding person himself/herself even if the wearer is viewing a content image displayed on the head-mounted display. This may give an unpleasant feeling to the surrounding person.</p><p id="p-0008" num="0007">The present invention has been made in view of the above, and an object thereof is to provide an information processing device, an information processing method, and an information processing program capable of reducing an unpleasant feeling given to a surrounding person.</p><heading id="h-0007" level="1">Solution to Problem</heading><p id="p-0009" num="0008">In order to Solve the above Problem, and achieve the object, an information processing device according to an embodiment includes a display control unit and a decision unit. The display control unit displays a content image on a head-mounted display. During display of the content image by the display control unit, the decision unit decides whether or not a surrounding person exists in a front direction of the head-mounted display on the basis of a camera image obtained by capturing an image of a surrounding environment of the head-mounted display. In a case where the decision unit decides that a surrounding person exists, the display control unit moves a display position of the content image.</p><heading id="h-0008" level="1">Advantageous Effects of Invention</heading><p id="p-0010" num="0009">According to one aspect of the embodiment, it is possible to reduce an unpleasant feeling given to a surrounding person.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0009" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates an example of an external appearance of a display device according to an embodiment.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates an example of a field of view of a wearer.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates an outline of an information processing method according to an embodiment.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a block diagram of an information processing device according to an embodiment.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a schematic diagram of processing by a decision unit according to an embodiment.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a schematic diagram of processing by a decision unit according to an embodiment.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a schematic diagram of processing by a decision unit according to an embodiment.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates an example of detection processing by a determination unit according to an embodiment.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>9</b></figref> illustrates an example of detection processing by a determination unit according to an embodiment.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>10</b></figref> illustrates an example of score information according to an embodiment.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a transition diagram of center coordinates during movement of a display position.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>12</b></figref> illustrates an example of an amount of rotation of a head.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a flowchart of a processing procedure executed by an information processing device according to an embodiment.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a flowchart of a processing procedure in step S<b>105</b> of <figref idref="DRAWINGS">FIG. <b>13</b></figref>.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a hardware configuration diagram illustrating an example of a computer that implements functions of an information processing device.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0010" level="1">DESCRIPTION OF EMBODIMENTS</heading><p id="p-0026" num="0025">Hereinafter, embodiments of the present disclosure will be described in detail with reference to the drawings. In each of the following embodiments, the same parts are denoted by the same reference signs, and redundant description will be omitted.</p><p id="p-0027" num="0026">First, an outline of a display device according to an embodiment will be described with reference to <figref idref="DRAWINGS">FIGS. <b>1</b> to <b>3</b></figref>. <figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates an example of an external appearance of the display device according to the embodiment. <figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates an example of a field of view of a wearer. <figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates an outline of an information processing method according to the embodiment.</p><p id="p-0028" num="0027">In the example of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, a display device <b>10</b> is a head-mounted display (hereinafter, also referred to as the HMD). More specifically, the display device <b>10</b> is a so-called optical transmissive HMD including a display unit <b>11</b> having optical transmissivity.</p><p id="p-0029" num="0028">In the example of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the display device <b>10</b> also includes a camera <b>13</b> in the vicinity of the display unit <b>11</b>. The camera <b>13</b> captures an image at an angle of view corresponding to a field of view of a wearer U wearing the display device <b>10</b>.</p><p id="p-0030" num="0029">As illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, in a case where a content image is displayed on the HMD, the field of view of the wearer U is optically blocked by the content image. The content image herein is a concept including both a still image and a moving image. In this type of HMD, a light shielding member (not illustrated) is generally provided in order to secure luminance of the display unit <b>11</b>. The light shielding member can be a light control element that can adjust transmittance in accordance with a change in luminance of external light.</p><p id="p-0031" num="0030">Therefore, the field of view (eye movement) of the wearer U of the HMD cannot be confirmed by a surrounding person existing around the wearer U. That is, the field of view of the wearer U is blocked while the wearer U is viewing the content image, and thus the wearer U is less likely to notice the surrounding person, and the surrounding person cannot recognize where the wearer U is looking.</p><p id="p-0032" num="0031">Therefore, in some cases, the surrounding person feels that the wearer is gazing at the surrounding person himself/herself while the wearer U is viewing the content image. However, the wearer U cannot notice the surrounding person and may give an unpleasant feeling to the surrounding person.</p><p id="p-0033" num="0032">Therefore, in the information processing method according to the embodiment, in a case where a surrounding person exists in a front direction of the HMD, a posture of the wearer U, that is, the front direction of the HMD is changed by moving a display position of image content displayed on the display device <b>10</b>.</p><p id="p-0034" num="0033">Specifically, as illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, in a case where the surrounding person exists in the front direction of the wearer U, that is, in the front direction of the HMD, center coordinates C of a content image displayed on the display device <b>10</b> are moved in a predetermined direction from initial coordinates Rp indicating the center of a display region of the display device <b>10</b>. Note that whether or not a surrounding person exists can be decided on the basis of a camera image captured by the camera <b>13</b>.</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates an example where the center coordinates C are moved rightward from the initial coordinates Rp. When the center coordinates C are moved, the wearer U moves his/her line of sight in accordance with the moved center coordinates C.</p><p id="p-0036" num="0035">At this time, the wearer U can comfortably view the content image when the center coordinates C substantially match the initial coordinates Rp, and therefore it is expected that the wearer U performs an action of changing his/her posture such that the center coordinates C substantially match the initial coordinates Rp. That is, it is expected that the wearer U changes the posture by following the center coordinates C such that the initial coordinates Rp substantially match the center coordinates C.</p><p id="p-0037" num="0036">Therefore, in the example of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, it is expected that the wearer U rotates his/her head rightward such that the initial coordinates Rp overlap with the center coordinates C. When the head of the wearer U rotates, the front direction of the HMD rotates, and thus no surrounding person exists in the front direction after the rotation.</p><p id="p-0038" num="0037">That is, the information processing method according to the embodiment prompts the wearer U to change the posture in a direction different from a direction of the surrounding person who has originally existed in the front direction of the HMD, thereby reducing an unpleasant feeling given by the wearer U to the surrounding person.</p><p id="p-0039" num="0038">Next, a configuration example of an information processing device <b>1</b> according to the embodiment will be described with reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>. <figref idref="DRAWINGS">FIG. <b>4</b></figref> is a block diagram of the information processing device <b>1</b> according to the embodiment. <figref idref="DRAWINGS">FIG. <b>4</b></figref> also illustrates the display device <b>10</b>. The information processing device <b>1</b> and the display device <b>10</b> can bidirectionally transmit and receive data in a wireless or wired manner.</p><p id="p-0040" num="0039">First, the display device <b>10</b> will be described. As illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the display device <b>10</b> includes the display unit <b>11</b>, a gyroscope sensor <b>12</b>, the camera <b>13</b>, and a distance measurement sensor <b>14</b>. The display unit <b>11</b> includes, for example, a one-way mirror, has a light transmissive display surface (lens), and displays a content image input from the information processing device <b>1</b>. More specifically, the display unit <b>11</b> repeats total reflection of the content image replicated on a display (not illustrated) in the lens, thereby projecting the content image toward eyeballs of the wearer U.</p><p id="p-0041" num="0040">The gyroscope sensor <b>12</b> detects angular velocities in three axes for detecting movement of the display device <b>10</b>. Because the display device <b>10</b> is the HMD as described above, the gyroscope sensor <b>12</b> detects a change in the posture of the wearer U of the display device <b>10</b> and outputs a posture signal corresponding to the detected change in the posture to the information processing device <b>1</b>.</p><p id="p-0042" num="0041">The camera <b>13</b> includes an image sensor and captures an image of an area in front of the display device <b>10</b>. The camera <b>13</b> preferably includes a wide-angle lens such as a fisheye lens. For example, the camera <b>13</b> captures an image at an angle of view corresponding to the field of view of the wearer U wearing the HMD, captures a captured camera image, and outputs the captured image to the information processing device <b>1</b>.</p><p id="p-0043" num="0042">The distance measurement sensor <b>14</b> is an example of a sensor for sensing a surrounding environment of the display unit <b>11</b> and is, for example, a time-of-flight (ToF) sensor. Instead of the distance measurement sensor <b>14</b>, the image sensor of the camera <b>13</b> may be regarded as a sensor for measuring the surrounding environment. That is, in a case where a distance from a surrounding person can be measured by image analysis, the image sensor may implement a function of the distance measurement sensor <b>14</b>.</p><p id="p-0044" num="0043">Next, the information processing device <b>1</b> will be described. As illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the information processing device <b>1</b> includes a storage unit <b>2</b> and a control unit <b>3</b>. The information processing device <b>1</b> may also include a communication unit (not illustrated) for performing wireless or wired communication with an external device and an operation unit (not illustrated) for accepting a user operation.</p><p id="p-0045" num="0044">The storage unit <b>2</b> is implemented by, for example, a semiconductor memory element such as a RAM or a flash memory or a storage device such as a hard disk or an optical disk. In the example of <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the storage unit <b>2</b> stores model information <b>20</b> and score information <b>21</b>.</p><p id="p-0046" num="0045">The model information <b>20</b> is information regarding a model for detecting a predetermined target object from a camera image captured by the camera <b>13</b>. For example, a feature value of each target object in the camera image is stored in the storage unit <b>2</b> as the model information <b>20</b>.</p><p id="p-0047" num="0046">The score information <b>21</b> is information regarding a score for determining a destination in a case where the display position of the content image is moved. A specific example of the score information <b>21</b> will be described later with reference to <figref idref="DRAWINGS">FIG. <b>10</b></figref>.</p><p id="p-0048" num="0047">Next, the control unit <b>3</b> will be described. The control unit <b>3</b> is implemented by, for example, a central processing unit (CPU) or a micro processing unit (MPU) executing a program stored in the information processing device <b>1</b> by using a random access memory (RAM) or the like as a work area. The control unit <b>3</b> is a controller and may also be implemented by, for example, an integrated circuit such as an application-specific integrated circuit (ASIC) or a field programmable gate array (FPGA).</p><p id="p-0049" num="0048">As illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the control unit <b>3</b> includes an acquisition unit <b>30</b>, a decision unit <b>31</b>, a determination unit <b>32</b>, a calculation unit <b>33</b>, and a display control unit <b>34</b> and implements or executes a function and effect of information processing described below. An internal configuration of the control unit <b>3</b> is not limited to the configuration in <figref idref="DRAWINGS">FIG. <b>4</b></figref> and may be another configuration as long as the configuration performs the information processing described below. The control unit <b>3</b> may be connected to a predetermined network in a wired or wireless manner by using, for example, a network interface card (NIC) and receive various types of information from an external server or the like via the network.</p><p id="p-0050" num="0049">The acquisition unit <b>30</b> acquires various types of information input from the display device <b>10</b>. Specifically, the acquisition unit <b>30</b> acquires a posture signal from the gyroscope sensor <b>12</b> and acquires a camera image from the camera <b>13</b>.</p><p id="p-0051" num="0050">During display of the content image by the display control unit <b>34</b>, the decision unit <b>31</b> decides whether or not a surrounding person exists in the front direction of the display device <b>10</b> on the basis of a camera image obtained by capturing an image of the surrounding environment of the display device <b>10</b>.</p><p id="p-0052" num="0051">For example, the decision unit <b>31</b> detects a person appearing in the camera image captured by the camera <b>13</b> on the basis of the feature value registered in the model information <b>20</b>, and, in a case where the detected person is in the front direction of the display device <b>10</b>, the decision unit <b>31</b> decides that a surrounding person exists in the front direction of the display device <b>10</b>.</p><p id="p-0053" num="0052">At this time, the decision unit <b>31</b> may decide that a surrounding person exists in a case where the decision unit <b>31</b> detects a person satisfying a predetermined decision condition. A specific example of processing by the decision unit <b>31</b> will be described with reference to <figref idref="DRAWINGS">FIGS. <b>5</b> to <b>7</b></figref>. <figref idref="DRAWINGS">FIGS. <b>5</b> to <b>7</b></figref> are schematic diagrams of the processing by the decision unit <b>31</b> according to the embodiment.</p><p id="p-0054" num="0053">Hereinafter, the front direction of the display device <b>10</b> may also be referred to as a front vector V. As illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the decision unit <b>31</b> detects not only a person but also a specific part of an object from the camera image and calculates a direction of the specific part.</p><p id="p-0055" num="0054">In the example of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, a specific part C<b>1</b> is a face of the surrounding person, a line-of-sight vector Va indicates a direction of the specific part C<b>1</b>, that is, a direction of a line of sight (face) of the person, a specific part C<b>2</b> is the center of his/her body, and a direction vector Vb indicates a direction of the specific part C<b>2</b>. The specific parts are not limited to the above examples and may be other parts. For example, in a case where the person is a woman, a chest or buttocks of the woman may be set as the specific parts.</p><p id="p-0056" num="0055">When detecting the specific parts, the decision unit <b>31</b> calculates a relative position between the display device <b>10</b> and each specific part. At this time, for example, the decision unit <b>31</b> may calculate the relative position on the basis of a measurement result of the distance measurement sensor <b>14</b> in <figref idref="DRAWINGS">FIG. <b>4</b></figref> or may calculate the relative position on the basis of the camera image.</p><p id="p-0057" num="0056">Then, the decision unit <b>31</b> decides whether or not the person satisfies the predetermined decision condition, and, when deciding that the predetermined decision condition is satisfied, the decision unit <b>31</b> decides that a surrounding person exists in the front direction of the display device <b>10</b>.</p><p id="p-0058" num="0057">In the example of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, in a case where a state in which an angle error between the front vector V and the line-of-sight vector V<b>1</b> is equal to or less than a predetermined value and a distance d between the front vector V and the specific part C<b>1</b> is equal to or less than a predetermined value continues for a certain period of time, the decision unit <b>31</b> decides that a surrounding person exists in the front direction of the display device <b>10</b>.</p><p id="p-0059" num="0058">The distance d between the front vector V and the specific part C<b>1</b> corresponds to a length of a perpendicular line from the specific part C<b>1</b> toward the front vector V. In the example of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, in a case where a state in which the wearer U and the person seem to face each other continues for a certain period of time, the decision unit <b>31</b> decides that a surrounding person exists in the front direction of the display device <b>10</b>.</p><p id="p-0060" num="0059">In other words, in the decision unit <b>31</b>, the decision condition is satisfied in a case where the surrounding person notices a situation in which the wearer U seems to gaze at the surrounding person. Hereinafter, the above decision condition will also be referred to as a line-of-sight condition.</p><p id="p-0061" num="0060">At this time, the decision condition may be satisfied without considering the direction of the surrounding person. Specifically, as illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, in a case where a state in which the distance d between the front vector V and the specific part C<b>1</b> is equal to or less than a predetermined value continues for a certain period of time, the decision unit <b>31</b> decides that a surrounding person exists in the front direction of the display device <b>10</b>.</p><p id="p-0062" num="0061">In other words, in a case where the person stops for a predetermined period of time or more in the front direction of the display device <b>10</b>, the decision unit <b>31</b> decides that a surrounding person exists. In this case, in a case where a situation in which the wearer U seems to gaze at a specific part of the surrounding person occurs, the decision unit <b>31</b> decides that a surrounding person exists in the front direction of the display device <b>10</b>. Hereinafter, the above decision condition will also be referred to as a distance condition.</p><p id="p-0063" num="0062">Returning to the description of <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the determination unit <b>32</b> will be described. In a case where the display control unit <b>34</b> moves the display position of the content image, the determination unit <b>32</b> determines a destination of the display position on the basis of the camera image.</p><p id="p-0064" num="0063">That is, in a case where the decision unit <b>31</b> decides that a surrounding person exists in the front direction of the display device <b>10</b>, the determination unit <b>32</b> determines the destination of the display position of the content image on the basis of the camera image captured by the camera <b>13</b>.</p><p id="p-0065" num="0064">A series of processing by the determination unit <b>32</b> will be described with reference to <figref idref="DRAWINGS">FIGS. <b>8</b> to <b>10</b></figref>. <figref idref="DRAWINGS">FIGS. <b>8</b> and <b>9</b></figref> illustrate an example of detection processing by the determination unit <b>32</b> according to the embodiment. <figref idref="DRAWINGS">FIG. <b>10</b></figref> illustrates an example of the score information <b>21</b> according to the embodiment.</p><p id="p-0066" num="0065">As illustrated in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, based on a camera image I captured by the camera <b>13</b>, the determination unit <b>32</b> detects people appearing in the camera image I. In the example of <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the people detected by the determination unit <b>32</b> are marked.</p><p id="p-0067" num="0066">At this time, the determination unit <b>32</b> detects not only the people appearing in the camera image I but also a direction of each person. In other words, the determination unit <b>32</b> distinctively detects people facing the wearer U and people facing other directions.</p><p id="p-0068" num="0067">As illustrated in <figref idref="DRAWINGS">FIG. <b>9</b></figref>, the determination unit <b>32</b> detects feature objects that satisfy a visual condition indicating a visual feature among objects appearing in the camera image I. The object herein refers to all things appearing in the camera image I, and the feature object refers to an object that has a predetermined visual feature and is likely to draw attention among the objects.</p><p id="p-0069" num="0068">In the example of <figref idref="DRAWINGS">FIG. <b>9</b></figref>, a sightseeing monument, a signboard, and the like correspond to the feature objects. In other words, the feature object is an object that can easily draw attention of people existing around the object and is an object in which, in a case where the feature object exists in the front direction of the wearer U, it is expected that the surrounding person thinks that the wearer U is looking not at the surrounding person but at the feature object.</p><p id="p-0070" num="0069">Examples of other feature objects include predetermined monuments such as a bronze statue and crowds. A feature value of the feature object satisfying the visual condition may be registered in advance, and an object having the feature value may be detected from the camera image I as the feature object, or the feature object may be detected on the basis of lines of sight of the people appearing in the camera image I.</p><p id="p-0071" num="0070">In this case, in a case where a plurality of people appearing in the camera image I are gazing at a predetermined object, this object is detected as the feature object. Further, a position of the feature object may be registered in a map in advance, and the feature object may be detected on the basis of a relative positional relationship with a current location of the wearer U on the basis of the map.</p><p id="p-0072" num="0071">When detecting the people or feature objects appearing in the camera image I, the determination unit <b>32</b> calculates a score on the basis of each detection result. Specifically, as illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, the field of view of the wearer U is divided into a plurality of regions, and the score is calculated for each divided region.</p><p id="p-0073" num="0072">As an example of a method of calculating the score, a point is deducted in a region where a person exists, and a point is further deducted in a region where a person facing the wearer U exists, whereas a point is added in a region where a feature object exists.</p><p id="p-0074" num="0073">Then, for example, the determination unit <b>32</b> determines a region having the lowest score as the destination of the display position of the image content, that is, the destination of the center coordinates C in <figref idref="DRAWINGS">FIG. <b>3</b></figref>. That is, the destination of the center coordinates C is determined on the basis of the people appearing in the camera image I, and thus a region where no person exists is preferentially determined as the destination of the center coordinates C, and, in a case where a plurality of people exist, a region where a person who is not looking at the wearer U exists is preferentially determined as the destination of the center coordinates C.</p><p id="p-0075" num="0074">That is, the determination unit <b>32</b> determines a direction in which no person satisfying the decision condition exists in the front direction of the moved HMD as the destination of the center coordinates C, thereby reducing a frequency of moving the content image.</p><p id="p-0076" num="0075">Further, the determination unit <b>32</b> determines the destination of the center coordinates C on the basis of the feature object, and thus it is possible to make people around the wearer think that the wearer U is looking at the feature object. Therefore, this makes it possible to eliminate an unpleasant feeling itself that the wearer U gives to the surrounding people.</p><p id="p-0077" num="0076">Note that, depending on the kind of the feature object, the surrounding people may feel strange if the wearer U is gazing at the feature object for too long. Therefore, in a case where a predetermined time elapses after the feature object comes in the front direction of the HMD, the destination of the center coordinates C may be determined again.</p><p id="p-0078" num="0077">Returning to the description of <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the calculation unit <b>33</b> will be described. For example, the calculation unit <b>33</b> calculates an amount of change in the posture of the wearer U on the basis of the detection result of the gyroscope sensor <b>12</b>. Specifically, the calculation unit <b>33</b> calculates an amount of rotation of the head of the wearer as the amount of change on the basis of a posture signal input from the gyroscope sensor <b>12</b>.</p><p id="p-0079" num="0078">In a case where the decision unit <b>31</b> decides that the decision condition is satisfied, the display control unit <b>34</b> described later moves the center coordinates C of the content image from the initial coordinates Rp. In this case, if the head of the wearer U does not rotate, a situation in which the wearer U seems to gaze at the surrounding person continues.</p><p id="p-0080" num="0079">Therefore, the calculation unit <b>33</b> calculates the amount of rotation of the head of the wearer U when the center coordinates C are moved and notifies the display control unit <b>34</b> in a case where, for example, the calculated amount of rotation exceeds a threshold. The threshold herein indicates an amount required for an initial surrounding person to deviate from the front direction of the HMD, but, for example, may be determined on the basis of the destination determined by the determination unit <b>32</b>. That is, in a case where the front direction of the HMD becomes close to the destination determined by the determination unit <b>32</b>, it may be determined that the amount of rotation exceeds the threshold.</p><p id="p-0081" num="0080">In a case where the decision unit <b>31</b> decides that a surrounding person exists in the front direction of the HMD, the display control unit <b>34</b> moves the display position of the content image. At this time, the initial coordinates Rp are set at the center of the display region of the display device <b>10</b> as described above.</p><p id="p-0082" num="0081">Therefore, the display control unit <b>34</b> moves the center coordinates C such that the distance between the initial coordinates Rp and the center coordinates C increases. Further, the display control unit <b>34</b> moves the center coordinates C to the destination determined by the determination unit <b>32</b>.</p><p id="p-0083" num="0082">At this time, for example, when the center coordinates C are rapidly moved from the initial coordinates Rp, visibility of the content image may be impaired in accordance with the display position of the content image. In this case, if the wearer U rapidly rotates the head with the movement of the center coordinates C, the wearer U greatly averts his/her eyes from the surrounding person, and this may give a sense of distrust to the surrounding person.</p><p id="p-0084" num="0083">Therefore, in order to move the display position of the content image, the display control unit <b>34</b> preferably moves the display position such that a moving speed of the content image is equal to or less than a predetermined value and moves the display position such that the distance between the display position before the movement and the display position after the movement falls within a predetermined range.</p><p id="p-0085" num="0084"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a transition diagram of the center coordinates C during the movement of the display position. <figref idref="DRAWINGS">FIG. <b>12</b></figref> illustrates an example of the amount of rotation of the head. As illustrated in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, for example, the center coordinates C of the content image are moved rightward from the initial coordinates Rp. In this case, the display control unit <b>34</b> constantly moves the center coordinates C such that the moving speed of the center coordinates C is equal to or less than the predetermined value and the distance between the center coordinates C before and after the movement is equal to or less than the predetermined value.</p><p id="p-0086" num="0085">This makes it possible to reduce an unpleasant feeling given to a surrounding person without impairing the visibility of the content image, and thus it is possible to minimize the amount of rotation of the head of the wearer U. In a period in which the center coordinates C shift from the initial coordinates Rp, the wearer U is expected to rotate the head, whereas, in a case where the shift between the center coordinates C and the initial coordinates Rp is sufficiently small, the wearer U is assumed to move only the line of sight to the center coordinates C without rotating the head.</p><p id="p-0087" num="0086">Therefore, the distance between the initial coordinates Rp and the moved center coordinates C is preferably equal to or more than a predetermined value. In this case, the display control unit <b>34</b> may acquire the amount of rotation of the head from the calculation unit <b>33</b> every time the center coordinates C are moved, and, only in a case where the head is not rotated, the display control unit <b>34</b> may move the center coordinates C to the next display position.</p><p id="p-0088" num="0087">Thereafter, as illustrated in <figref idref="DRAWINGS">FIG. <b>12</b></figref>, in a case where an amount of rotation &#x3b8; of the front vector V before and after the movement of the display position exceeds the threshold, the display control unit <b>34</b> moves the center coordinates C to the initial coordinates Rp. In other words, in a case where the head of the wearer U faces a desired direction, the display control unit <b>34</b> returns the center coordinates C to the initial coordinates Rp, thereby stopping the rotation of the head of the wearer U.</p><p id="p-0089" num="0088">Next, a processing procedure executed by the information processing device <b>1</b> according to the embodiment will be described with reference to <figref idref="DRAWINGS">FIGS. <b>13</b> and <b>14</b></figref>. <figref idref="DRAWINGS">FIG. <b>13</b></figref> is a flowchart of a processing procedure executed by the information processing device <b>1</b> according to the embodiment. <figref idref="DRAWINGS">FIG. <b>14</b></figref> is a flowchart of a processing procedure in step S<b>105</b> of <figref idref="DRAWINGS">FIG. <b>13</b></figref>. The following processing procedure is repeatedly executed by the control unit <b>3</b>.</p><p id="p-0090" num="0089">As illustrated in <figref idref="DRAWINGS">FIG. <b>13</b></figref>, the information processing device <b>1</b> first decides whether or not a content image is being displayed (step S<b>101</b>). In a case where the content image is being displayed (Step S<b>101</b>, Yes), the information processing device <b>1</b> performs recognition processing of surrounding people on the basis of a camera image (step S<b>102</b>).</p><p id="p-0091" num="0090">Then, the information processing device <b>1</b> decides whether or not the line-of-sight condition in <figref idref="DRAWINGS">FIG. <b>6</b></figref> is satisfied on the basis of a result of the recognition processing of the surrounding people (step S<b>103</b>). In a case where the information processing device <b>1</b> decides that the line-of-sight condition is satisfied in the decision in step S<b>103</b> (Step S<b>103</b>, Yes), the information processing device <b>1</b> proceeds to display control processing (step S<b>105</b>) and terminates the processing.</p><p id="p-0092" num="0091">In a case where the information processing device <b>1</b> decides that the line-of-sight condition is not satisfied in the decision processing in step S<b>103</b> (Step S<b>103</b>, No), the information processing device <b>1</b> decides whether or not the distance condition in <figref idref="DRAWINGS">FIG. <b>7</b></figref> is satisfied (step S<b>104</b>).</p><p id="p-0093" num="0092">In a case where the distance condition is satisfied in the decision in step S<b>104</b> (step S<b>104</b>, Yes), the information processing device <b>1</b> proceeds to the processing in step S<b>105</b>. In a case where the information processing device <b>1</b> decides that the distance condition is not satisfied (step S<b>104</b>, No), the information processing device <b>1</b> terminates the processing. In a case where the content image is not displayed in the decision in step S<b>101</b> (step S<b>101</b>, No), the information processing device <b>1</b> omits the processing in step S<b>102</b> and the subsequent steps and terminates the processing.</p><p id="p-0094" num="0093">Next, the processing procedure in step S<b>105</b> in <figref idref="DRAWINGS">FIG. <b>13</b></figref> will be described with reference to <figref idref="DRAWINGS">FIG. <b>14</b></figref>. As illustrated in <figref idref="DRAWINGS">FIG. <b>14</b></figref>, the information processing device <b>1</b> calculates a score for each region on the basis of a person or feature object appearing in the camera image (step S<b>201</b>).</p><p id="p-0095" num="0094">Then, the information processing device <b>1</b> determines a destination of a display position of image content on the basis of the score of each region (step S<b>202</b>) and moves the display position to the determined destination (step S<b>203</b>).</p><p id="p-0096" num="0095">Then, the information processing device <b>1</b> decides whether or not the amount of rotation of the head of the wearer U is larger than the threshold (step S<b>204</b>) and, in a case where the amount of rotation exceeds the threshold (step S<b>204</b>, Yes), the information processing device <b>1</b> moves the display position to the initial coordinates Rp (step S<b>205</b>) and terminates the processing.</p><p id="p-0097" num="0096">In a case where the amount of rotation is less than the threshold in the decision in step S<b>204</b> (step S<b>204</b>, No), the information processing device <b>1</b> proceeds to the processing in step S<b>203</b>.</p><heading id="h-0011" level="1">Modification Example</heading><p id="p-0098" num="0097">The above embodiment shows that, in a case where a surrounding person exists in the front direction of the HMD, the display position of the content image is moved to guide the line of sight of the wearer U. However, the present invention is not limited thereto. That is, the wearer U may be notified of the existence of the surrounding person by a warning image or a warning sound. In this case, the transmittance of a part of or the entire content image may be increased to cause the wearer U to directly and visually recognize the surrounding person.</p><p id="p-0099" num="0098">Further, the above embodiment shows a case where the display device <b>10</b> is an optical see-through display device. However, the present invention is not limited thereto and is similarly applicable to a video see-through display device.</p><p id="p-0100" num="0099">An information device such as the information processing device according to each embodiment described above is implemented by, for example, a computer <b>1000</b> having a configuration of <figref idref="DRAWINGS">FIG. <b>15</b></figref>. Hereinafter, the information processing device <b>1</b> according to the embodiment will be described as an example. <figref idref="DRAWINGS">FIG. <b>15</b></figref> is a hardware configuration diagram illustrating an example of the computer <b>1000</b> that implements functions of the information processing device <b>1</b>. The computer <b>1000</b> includes a CPU <b>1100</b>, a RAM <b>1200</b>, a read only memory (ROM) <b>1300</b>, a hard disk drive (HDD) <b>1400</b>, a communication interface <b>1500</b>, and an input/output interface <b>1600</b>. Each unit of the computer <b>1000</b> is connected by a bus <b>1050</b>.</p><p id="p-0101" num="0100">The CPU <b>1100</b> operates on the basis of programs stored in the ROM <b>1300</b> or the HDD <b>1400</b>, thereby controlling each unit. For example, the CPU <b>1100</b> develops the programs stored in the ROM <b>1300</b> or the HDD <b>1400</b> in the RAM <b>1200</b> and executes processing corresponding to the various programs.</p><p id="p-0102" num="0101">The ROM <b>1300</b> stores a boot program such as a basic input output system (BIOS) executed by the CPU <b>1100</b> at the time of activation of the computer <b>1000</b>, a program depending on hardware of the computer <b>1000</b>, and the like.</p><p id="p-0103" num="0102">The HDD <b>1400</b> is a computer-readable recording medium that non-transitorily records programs executed by the CPU <b>1100</b>, data used by the programs, and the like. Specifically, the HDD <b>1400</b> is a recording medium that records programs according to the present disclosure serving as an example of program data <b>1450</b>.</p><p id="p-0104" num="0103">The communication interface <b>1500</b> is an interface for connecting the computer <b>1000</b> to an external network <b>1550</b> (e.g., the Internet). For example, the CPU <b>1100</b> receives data from another device or transmits data generated by the CPU <b>1100</b> to another device via the communication interface <b>1500</b>.</p><p id="p-0105" num="0104">The input/output interface <b>1600</b> is an interface for connecting an input/output device <b>1650</b> and the computer <b>1000</b>. For example, the CPU <b>1100</b> receives data from an input device such as a keyboard or a mouse via the input/output interface <b>1600</b>. The CPU <b>1100</b> transmits data to an output device such as a display, a speaker, or a printer via the input/output interface <b>1600</b>. The input/output interface <b>1600</b> may function as a media interface that reads a program or the like recorded in a predetermined recording medium (medium). The medium is, for example, an optical recording medium such as a digital versatile disc (DVD) or a phase change rewritable disk (PD), a magneto-optical recording medium such as a magneto-optical disk (MO), a tape medium, a magnetic recording medium, or a semiconductor memory.</p><p id="p-0106" num="0105">For example, in a case where the computer <b>1000</b> functions as the information processing device <b>1</b> according to the embodiment, the CPU <b>1100</b> of the computer <b>1000</b> implements a function of the acquisition unit <b>30</b> by executing a program loaded on the RAM <b>1200</b>. The HDD <b>1400</b> stores a program according to the present disclosure and data in the storage unit <b>2</b>. The CPU <b>1100</b> reads the program data <b>1450</b> from the HDD <b>1400</b> and executes the program data, but may acquire those programs from another device via the external network <b>1550</b> as another example.</p><p id="p-0107" num="0106">The present technology can also have the following configurations.</p><p id="p-0108" num="0000">(1)</p><p id="p-0109" num="0107">An information processing device comprising:</p><p id="p-0110" num="0108">a display control unit configured to display a content image on a head-mounted display; and</p><p id="p-0111" num="0109">a decision unit configured to, during display of the content image by the display control unit, decide whether or not a surrounding person exists in a front direction of the head-mounted display on the basis of a camera image obtained by capturing an image of a surrounding environment of the head-mounted display, wherein</p><p id="p-0112" num="0110">the display control unit</p><p id="p-0113" num="0111">moves a display position of the content image in a case where the decision unit decides that the surrounding person exists.</p><p id="p-0114" num="0000">(2)</p><p id="p-0115" num="0112">The information processing device according to (1), wherein the head-mounted display has optical transmissivity.</p><p id="p-0116" num="0000">(3)</p><p id="p-0117" num="0113">The information processing device according to (2), wherein the head-mounted display</p><p id="p-0118" num="0114">includes a light shielding member.</p><p id="p-0119" num="0000">(4)</p><p id="p-0120" num="0115">The information processing device according to any one of (1) to (3), wherein</p><p id="p-0121" num="0116">the display control unit</p><p id="p-0122" num="0117">moves the content image such that a distance between initial coordinates and a center of the content image increases, the initial coordinates being set at a center of a display region of the head-mounted display.</p><p id="p-0123" num="0000">(5)</p><p id="p-0124" num="0118">The information processing device according to any one of (1) to (4), wherein</p><p id="p-0125" num="0119">in a case of moving the display position of the content image,</p><p id="p-0126" num="0120">the display control unit moves the display position such that a moving speed of the content image is equal to or less than a predetermined value.</p><p id="p-0127" num="0000">(6)</p><p id="p-0128" num="0121">The information processing device according to any one of (1) to (5), wherein</p><p id="p-0129" num="0122">in a case of moving the display position of the content image,</p><p id="p-0130" num="0123">the display control unit moves the display position such that a distance between the display position before the movement and the display position after the movement falls within a predetermined range.</p><p id="p-0131" num="0000">(7)</p><p id="p-0132" num="0124">The information processing device according to any one of (1) to (6), further comprising</p><p id="p-0133" num="0125">a calculation unit configured to calculate an amount of change in a posture of the head-mounted display before and after the movement of the content image on the basis of a posture signal regarding the posture of the head-mounted display, wherein</p><p id="p-0134" num="0126">the display control unit</p><p id="p-0135" num="0127">returns the display position of the content image to an original position in a case where the amount of change in the posture calculated by the calculation unit exceeds a threshold.</p><p id="p-0136" num="0000">(8)</p><p id="p-0137" num="0128">The information processing device according to (7), wherein:</p><p id="p-0138" num="0129">the calculation unit</p><p id="p-0139" num="0130">calculates an amount of rotation of a head of a wearer of the head-mounted display as the amount of change on the basis of the posture signal; and</p><p id="p-0140" num="0131">the display control unit</p><p id="p-0141" num="0132">returns the display position of the content image to the original position in a case where the amount of rotation exceeds a threshold.</p><p id="p-0142" num="0000">(9)</p><p id="p-0143" num="0133">The information processing device according to any one of (1) to (8), wherein</p><p id="p-0144" num="0134">the decision unit</p><p id="p-0145" num="0135">decides whether or not the surrounding person exists on the basis of a direction of a face of a person existing in the front direction of the head-mounted display.</p><p id="p-0146" num="0000">(10)</p><p id="p-0147" num="0136">The information processing device according to (9), wherein</p><p id="p-0148" num="0137">the decision unit</p><p id="p-0149" num="0138">decides that the surrounding person exists in a case where an angle between the front direction of the head-mounted display and the direction of the face of the person is equal to or less than a predetermined threshold.</p><p id="p-0150" num="0000">(11)</p><p id="p-0151" num="0139">The information processing device according to any one of (1) to (10), wherein</p><p id="p-0152" num="0140">the decision unit</p><p id="p-0153" num="0141">decides that the surrounding person exists in a case where the surrounding person stops for a predetermined period of time or more in the front direction of the head-mounted display.</p><p id="p-0154" num="0000">(12)</p><p id="p-0155" num="0142">The information processing device according to any one of (1) to (11), wherein</p><p id="p-0156" num="0143">the decision unit</p><p id="p-0157" num="0144">decides that the surrounding person exists in a case where a distance between the front direction of the head-mounted display and coordinates of a specific part of the surrounding person is equal to or less than a threshold.</p><p id="p-0158" num="0000">(13)</p><p id="p-0159" num="0145">The information processing device according to any one of (1) to (12), further comprising</p><p id="p-0160" num="0146">a determination unit configured to, in a case where the decision unit decides that the surrounding person exists, determine a destination of the display position of the content image on the basis of the camera image.</p><p id="p-0161" num="0000">(14)</p><p id="p-0162" num="0147">The information processing device according to (13), wherein</p><p id="p-0163" num="0148">the determination unit</p><p id="p-0164" num="0149">determines the destination of the display position of the content image on the basis of a direction of a person appearing in the camera image.</p><p id="p-0165" num="0000">(15)</p><p id="p-0166" num="0150">The information processing device according to (13) or (14), wherein</p><p id="p-0167" num="0151">the determination unit</p><p id="p-0168" num="0152">determines the destination of the display position of the content image in a direction in which there is no person appearing in the camera image.</p><p id="p-0169" num="0000">(16)</p><p id="p-0170" num="0153">The information processing device according to any one of (13) to (15), wherein</p><p id="p-0171" num="0154">the determination unit</p><p id="p-0172" num="0155">determines the destination of the display position in a direction of a feature object that satisfies a visual condition indicating a visual feature among objects appearing in the camera image.</p><p id="p-0173" num="0000">(17)</p><p id="p-0174" num="0156">An information processing method comprising</p><p id="p-0175" num="0157">causing a computer to</p><p id="p-0176" num="0158">display a content image on a head-mounted display,</p><p id="p-0177" num="0159">during display of the content image, decide whether or not a surrounding person exists in a front direction of the head-mounted display on the basis of a camera image obtained by capturing an image of a surrounding environment of the head-mounted display, and</p><p id="p-0178" num="0160">move a display position of the content image in a case where it is decided that the surrounding person exists.</p><p id="p-0179" num="0000">(18)<br/>An information processing program for causing a computer to function as</p><p id="p-0180" num="0161">a display control unit configured to display a content image on a head-mounted display, and</p><p id="p-0181" num="0162">a decision unit configured to, during display of the content image by the display control unit, decide whether or not a surrounding person exists in a front direction of the head-mounted display on the basis of a camera image obtained by capturing an image of a surrounding environment of the head-mounted display, wherein</p><p id="p-0182" num="0163">the display control unit</p><p id="p-0183" num="0164">moves a display position of the content image in a case where the decision unit decides that the surrounding person exists.</p><heading id="h-0012" level="1">REFERENCE SIGNS LIST</heading><p id="p-0184" num="0000"><ul id="ul0002" list-style="none">    <li id="ul0002-0001" num="0000">    <ul id="ul0003" list-style="none">        <li id="ul0003-0001" num="0165"><b>1</b> INFORMATION PROCESSING DEVICE</li>        <li id="ul0003-0002" num="0166"><b>10</b> DISPLAY DEVICE</li>        <li id="ul0003-0003" num="0167"><b>30</b> ACQUISITION UNIT</li>        <li id="ul0003-0004" num="0168"><b>31</b> DECISION UNIT</li>        <li id="ul0003-0005" num="0169"><b>32</b> DETERMINATION UNIT</li>        <li id="ul0003-0006" num="0170"><b>33</b> CALCULATION UNIT</li>        <li id="ul0003-0007" num="0171"><b>34</b> DISPLAY CONTROL UNIT</li>        <li id="ul0003-0008" num="0172">Rp INITIAL COORDINATES</li>        <li id="ul0003-0009" num="0173">C CENTER COORDINATES</li>    </ul>    </li></ul></p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An information processing device comprising:<claim-text>a display control unit configured to display a content image on a head-mounted display; and</claim-text><claim-text>a decision unit configured to, during display of the content image by the display control unit, decide whether or not a surrounding person exists in a front direction of the head-mounted display on the basis of a camera image obtained by capturing an image of a surrounding environment of the head-mounted display, wherein</claim-text><claim-text>the display control unit</claim-text><claim-text>moves a display position of the content image in a case where the decision unit decides that the surrounding person exists.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The information processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the head-mounted display</claim-text><claim-text>has optical transmissivity.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The information processing device according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein<claim-text>the head-mounted display</claim-text><claim-text>includes a light shielding member.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The information processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the display control unit</claim-text><claim-text>moves the content image such that a distance between initial coordinates and a center of the content image increases, the initial coordinates being set at a center of a display region of the head-mounted display.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The information processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>in a case of moving the display position of the content image,</claim-text><claim-text>the display control unit moves the display position such that a moving speed of the content image is equal to or less than a predetermined value.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The information processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>in a case of moving the display position of the content image,</claim-text><claim-text>the display control unit moves the display position such that a distance between the display position before the movement and the display position after the movement falls within a predetermined range.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The information processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising<claim-text>a calculation unit configured to calculate an amount of change in a posture of the head-mounted display before and after the movement of the content image on the basis of a posture signal regarding the posture of the head-mounted display, wherein</claim-text><claim-text>the display control unit</claim-text><claim-text>returns the display position of the content image to an original position in a case where the amount of change in the posture calculated by the calculation unit exceeds a threshold.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The information processing device according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein:<claim-text>the calculation unit</claim-text><claim-text>calculates an amount of rotation of a head of a wearer of the head-mounted display as the amount of change on the basis of the posture signal; and</claim-text><claim-text>the display control unit</claim-text><claim-text>returns the display position of the content image to the original position in a case where the amount of rotation exceeds a threshold.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The information processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the decision unit</claim-text><claim-text>decides whether or not the surrounding person exists on the basis of a direction of a face of a person existing in the front direction of the head-mounted display.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The information processing device according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein<claim-text>the decision unit</claim-text><claim-text>decides that the surrounding person exists in a case where an angle between the front direction of the head-mounted display and the direction of the face of the person is equal to or less than a predetermined threshold.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The information processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the decision unit</claim-text><claim-text>decides that the surrounding person exists in a case where the surrounding person stops for a predetermined period of time or more in the front direction of the head-mounted display.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The information processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the decision unit</claim-text><claim-text>decides that the surrounding person exists in a case where a distance between the front direction of the head-mounted display and coordinates of a specific part of the surrounding person is equal to or less than a threshold.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The information processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising<claim-text>a determination unit configured to, in a case where the decision unit decides that the surrounding person exists, determine a destination of the display position of the content image on the basis of the camera image.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The information processing device according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein<claim-text>the determination unit</claim-text><claim-text>determines the destination of the display position of the content image on the basis of a direction of a person appearing in the camera image.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The information processing device according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein<claim-text>the determination unit</claim-text><claim-text>determines the destination of the display position of the content image in a direction in which there is no person appearing in the camera image.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The information processing device according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein<claim-text>the determination unit</claim-text><claim-text>determines the destination of the display position in a direction of a feature object that satisfies a visual condition indicating a visual feature among objects appearing in the camera image.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. An information processing method comprising<claim-text>causing a computer to</claim-text><claim-text>display a content image on a head-mounted display,</claim-text><claim-text>during display of the content image, decide whether or not a surrounding person exists in a front direction of the head-mounted display on the basis of a camera image obtained by capturing an image of a surrounding environment of the head-mounted display, and</claim-text><claim-text>move a display position of the content image in a case where it is decided that the surrounding person exists.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. An information processing program for causing a computer to function as<claim-text>a display control unit configured to display a content image on a head-mounted display, and</claim-text><claim-text>a decision unit configured to, during display of the content image by the display control unit, decide whether or not a surrounding person exists in a front direction of the head-mounted display on the basis of a camera image obtained by capturing an image of a surrounding environment of the head-mounted display, wherein</claim-text><claim-text>the display control unit</claim-text><claim-text>moves a display position of the content image in a case where the decision unit decides that the surrounding person exists.</claim-text></claim-text></claim></claims></us-patent-application>