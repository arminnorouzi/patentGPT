<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004018A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004018</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17816902</doc-number><date>20220802</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>02</class><subclass>B</subclass><main-group>30</main-group><subgroup>24</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>03</class><subclass>B</subclass><main-group>21</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>02</class><subclass>B</subclass><main-group>30</main-group><subgroup>34</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>02</class><subclass>B</subclass><main-group>30</main-group><subgroup>52</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>02</class><subclass>B</subclass><main-group>27</main-group><subgroup>01</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>02</class><subclass>B</subclass><main-group>5</main-group><subgroup>18</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>03</class><subclass>B</subclass><main-group>35</main-group><subgroup>18</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20200101</date></cpc-version-indicator><section>G</section><class>02</class><subclass>B</subclass><main-group>30</main-group><subgroup>24</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>03</class><subclass>B</subclass><main-group>21</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20200101</date></cpc-version-indicator><section>G</section><class>02</class><subclass>B</subclass><main-group>30</main-group><subgroup>34</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20200101</date></cpc-version-indicator><section>G</section><class>02</class><subclass>B</subclass><main-group>30</main-group><subgroup>52</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>02</class><subclass>B</subclass><main-group>27</main-group><subgroup>0101</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>02</class><subclass>B</subclass><main-group>5</main-group><subgroup>1876</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>03</class><subclass>B</subclass><main-group>35</main-group><subgroup>18</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>02</class><subclass>B</subclass><main-group>5</main-group><subgroup>1828</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>02</class><subclass>B</subclass><main-group>2027</main-group><subgroup>0134</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">THREE DIMENSIONAL VIRTUAL AND AUGMENTED REALITY DISPLAY SYSTEM</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16844464</doc-number><date>20200409</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11474371</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17816902</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16511488</doc-number><date>20190715</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>10670881</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>16844464</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16183619</doc-number><date>20181107</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>10444527</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>16511488</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>15286695</doc-number><date>20161006</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>10191294</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>16183619</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>14591543</doc-number><date>20150107</date></document-id><parent-status>ABANDONED</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>15286695</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>13684489</doc-number><date>20121123</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>8950867</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>14591543</doc-number></document-id></child-doc></relation></continuation><us-provisional-application><document-id><country>US</country><doc-number>61563403</doc-number><date>20111123</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Magic Leap, Inc.</orgname><address><city>Plantation</city><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Macnamara</last-name><first-name>John Graham</first-name><address><city>Plantation</city><state>FL</state><country>US</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>Magic Leap, Inc.</orgname><role>02</role><address><city>Plantation</city><country>US</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A system may comprise a selectively transparent projection device for projecting an image toward an eye of a viewer from a projection device position in space relative to the eye of the viewer, the projection device being capable of assuming a substantially transparent state when no image is projected; an occlusion mask device coupled to the projection device and configured to selectively block light traveling toward the eye from one or more positions opposite of the projection device from the eye of the viewer in an occluding pattern correlated with the image projected by the projection device; and a zone plate diffraction patterning device interposed between the eye of the viewer and the projection device and configured to cause light from the projection device to pass through a diffraction pattern having a selectable geometry as it travels to the eye.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="93.73mm" wi="55.54mm" file="US20230004018A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="164.51mm" wi="137.58mm" file="US20230004018A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="156.63mm" wi="175.77mm" file="US20230004018A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="171.45mm" wi="151.55mm" file="US20230004018A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="197.36mm" wi="99.23mm" file="US20230004018A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="121.16mm" wi="122.00mm" file="US20230004018A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="94.83mm" wi="171.79mm" file="US20230004018A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="190.58mm" wi="152.99mm" file="US20230004018A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="198.12mm" wi="92.03mm" file="US20230004018A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="106.34mm" wi="160.27mm" file="US20230004018A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="218.10mm" wi="127.68mm" file="US20230004018A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="117.52mm" wi="130.05mm" file="US20230004018A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="127.51mm" wi="132.25mm" file="US20230004018A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="127.51mm" wi="135.04mm" file="US20230004018A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">RELATED APPLICATION DATA</heading><p id="p-0002" num="0001">The present application is a continuation application of U.S. patent application Ser. No. 16/844,464, filed on Apr. 9, 2020, which is a continuation of U.S. patent application Ser. No. 16/511,488, filed on Jul. 15, 2019 now U.S. Pat. No. 10,670,881, which is a continuation application of U.S. patent application Ser. No. 16/183,619 filed on Nov. 7, 2018, now U.S. patent Ser. No. 10/444,527, which is a continuation application of Ser. No. 15/286,695 filed on Oct. 6, 2016, now U.S. Pat. No. 10,191,294, which is a continuation application of U.S. patent application Ser. No. 14/591,543 filed on Jan. 7, 2015 now abandoned, which is continuation of U.S. patent application Ser. No. 13/684,489 filed on Nov. 23, 2012 now U.S. Pat. No. 8,950,867, which claims the benefit under 35 U.S.C. &#xa7; 119 to U.S. Provisional Application Ser. No. 61/563,403 filed Nov. 23, 2011. The foregoing applications are hereby incorporated by reference into the present application in their entirety.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">FIELD OF THE INVENTION</heading><p id="p-0003" num="0002">The present invention relates to virtual reality and augmented reality imaging and visualization systems.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">In order for a 3D display to produce a true sensation of depth, and more specifically, a simulated sensation of surface depth, it is desirable for each point in the display's visual field to generate the accommodative response corresponding to its virtual depth. If the accommodative response to a display point does not correspond to the virtual depth of that point, as determined by the binocular depth cues of convergence and stereopsis, the human eye may experience an accommodation conflict, resulting in unstable imaging, harmful eye strain, headaches, and, in the absence of accommodation information, almost a complete lack of surface depth. Referring to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, an augmented reality scenario (<b>8</b>) is depicted with views to the user of actual objects within the user's reality, such as landscaping items including a concrete stage object (<b>1120</b>) in a park setting, and also views of virtual objects added into the view to produce the &#x201c;augmented&#x201d; reality view; here a robot statue (<b>1110</b>) is shown virtually standing upon the stage object (<b>1120</b>), and a bee character (<b>2</b>) is shown flying in the airspace near the user's head. Preferably the augmented reality system is 3-D capable, in which case it provides the user with the perception that the statue (<b>1110</b>) is standing on the stage (<b>1120</b>), and that the bee character (<b>2</b>) is flying close to the user's head. This perception may be greatly enhanced by utilizing visual accommodation cues to the user's eye and brain that the virtual objects (<b>2</b>, <b>1110</b>) have different depths of focus, and that the depth of focus or focal radii for the robot statue (<b>1110</b>) is approximately the same as that for the stage (<b>1120</b>). Conventional stereoscopic 3-D simulation display systems, such as that depicted in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, typically have two displays (<b>74</b>, <b>76</b>), one for each eye, at a fixed radial focal distance (<b>10</b>). As stated above, this conventional technology misses many of the valuable cues utilized by the human eye and brain to detect and interpret depth in three dimensions, including the accommodation cue, which is associated with the eye's repositioning of the crystalline lens within the eye complex to reach a different depth of focus with the eye. There is a need for an accommodation accurate display system which takes into account the accommodation aspects of the human eye/brain image processing complex.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0005" num="0004">One embodiment is directed to a three-dimensional image visualization system, comprising a selectively transparent projection device for projecting an image toward an eye of a viewer from a projection device position in space relative to the eye of the viewer, the projection device being capable of assuming a substantially transparent state when no image is projected; an occlusion mask device coupled to the projection device and configured to selectively block light traveling toward the eye from one or more positions opposite of the projection device from the eye of the viewer in an occluding pattern correlated with the image projected by the projection device; and a zone plate diffraction patterning device interposed between the eye of the viewer and the projection device and configured to cause light from the projection device to pass through a diffraction pattern having a selectable geometry as it travels to the eye and enter the eye with a simulated focal distance from the eye based at least in part upon the selectable geometry of the diffraction pattern. The system further may comprise a controller operatively coupled to the projection device, occlusion mask device, and the zone plate diffraction patterning device and configured to coordinate projection of the image and associated occluding pattern, as well as interposition of the diffraction pattern at the selectable geometry. The controller may comprise a microprocessor. The projection device may comprise a substantially planar transparent digital display substantially occupying a display plane. The display plane may be oriented substantially perpendicularly from a visual axis of the eye of the viewer. The substantially planar transparent digital display may comprise a liquid crystal display. The substantially planar transparent digital display may comprise an organic light emitting diode display. The projection device may be configured to project the image toward the eye in a collimated form such that the depth of focus for the eye of the viewer is an infinite depth of focus. The projection device may comprise a high-speed mini-projector coupled to a substrate-guided delay exit pupil expander device configured to expand the size of the image before delivery to the eye of the viewer. The mini-projector may be mounted substantially perpendicularly to a visual axis of the eye of the viewer, and wherein the substrate-guided delay exit pupil expander device is configured to receive the image from the mini-projector and deliver it to the zone plate diffraction patterning device and to the eye of the viewer in the expanded size with an orientation substantially aligned with the visual axis of the eye. The zone plate diffraction patterning device and projection device may comprise at least one common structure. The zone plate diffraction patterning device may be integrated into a waveguide, such that the projection device comprises a high-speed mini-projector coupled to the waveguide and configured pass the image through the diffraction pattern before the image exits the waveguide en route to the eye of the viewer. The mini-projector may be mounted substantially perpendicularly to a visual axis of the eye of the viewer, and the waveguide may be configured to receive the image from the mini-projector and deliver it to the eye of the viewer in an expanded size with an orientation substantially aligned with the visual axis of the eye. The occlusion mask device my comprise a display configured to either occlude or pass light at each of a plurality of portions of the display, depending upon a pertinent command to occlude or pass light at each portion. The occlusion mask device may comprise one or more liquid crystal displays. The zone plate diffraction patterning device may comprise a high-frequency binary display configured to either occlude or pass light at each of a plurality of portions of the display, depending upon a pertinent command to occlude or pass light at each portion. The zone plate diffraction patterning device may have a refresh rate of between about 500 Hz and about 2,000 Hz. The zone plate diffraction patterning device may have a refresh rate of about 720 Hz. The controller may be configured to operate the projection device and occlusion mask device at between about 30 and about 60 frames per second, and to operate the zone plate diffraction patterning device to digitally display up to about 12 different diffraction patterns for each frame of the projection device and occlusion mask device. The projection device, occlusion mask device, and the zone plate diffraction patterning device collectively may comprise an imaging module for a single eye of the viewer, and the system further may comprise a second imaging module for another eye of the viewer.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0006" num="0005"><figref idref="DRAWINGS">FIG. <b>1</b></figref> depicts an illustration of an augmented reality scenario with certain virtual reality objects, and certain actual reality objects viewed by a person.</p><p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates a conventional stereoscopy system to simulate three-dimensional imaging for the user.</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIGS. <b>3</b>A and <b>3</b>B</figref> illustrate aspects of an accommodation accurate display configuration.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIGS. <b>4</b>A-<b>4</b>C</figref> illustrate relationships between radius of curvature and focal radius.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIGS. <b>5</b>, <b>6</b>A, <b>6</b>B and <b>6</b>C</figref> illustrate aspects of diffraction gratings as applied to the subject configurations.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIGS. <b>7</b>A-<b>7</b>C</figref> illustrate three different focal mechanisms.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>7</b>D</figref> illustrates a Fresnel zone plate.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIGS. <b>8</b>A-<b>8</b>C</figref> illustrate various aspects of diffraction system focusing issues.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>9</b></figref> illustrates one embodiment of a waveguide with embedded diffraction grating.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>10</b></figref> illustrates one embodiment of a waveguide with embedded diffraction grating designed to allow one mode to escape and the other modes to remain trapped in the waveguide.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIGS. <b>11</b>A-<b>11</b>B</figref> illustrate aspects of a diffractive imaging module embodiment.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIGS. <b>12</b>A-<b>12</b>B</figref> illustrate aspects of a diffractive imaging module embodiment.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIGS. <b>13</b>A-<b>13</b>B</figref> illustrate aspects of a diffractive imaging module embodiment.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0019" num="0018">Referring to <figref idref="DRAWINGS">FIGS. <b>3</b>A and <b>3</b>B</figref>, various aspects of an AAD system are depicted. Referring to <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>, a simple illustration shows that in the place of two conventional displays as in stereoscopy (<figref idref="DRAWINGS">FIG. <b>2</b></figref>), two complex images, one for each eye, with various radial focal depths (<b>12</b>) for various aspects (<b>14</b>) of each image may be utilized to provide each eye with the perception of three dimensional depth layering within the perceived image.</p><p id="p-0020" num="0019">Referring to <figref idref="DRAWINGS">FIG. <b>3</b>B</figref>, we have determined that the typical human eye is able to interpret approximately 12 layers (layers L<b>1</b>-L<b>12</b> in <figref idref="DRAWINGS">FIG. <b>3</b>B</figref>&#x2014;drawing element <b>16</b>) of depth based upon radial distance. A near field limit (<b>78</b>) of about 0.25 meters is about the closest depth of focus; a far-field limit (<b>80</b>) of about 3 meters means that any item farther than about 3 meters from the human eye receives infinite focus. The layers of focus get more and more thin as one gets closer to the eye; in other words, the eye is able to perceive differences in focal distance that are quite small relatively close to the eye, and this effect dissipates as objects fall farther away from the eye, as shown in <figref idref="DRAWINGS">FIG. <b>3</b>B</figref>. Element <b>82</b> illustrates that at an infinite object location, a depth of focus/dioptric spacing value is about &#x2153; diopters. One other way of describing the import of <figref idref="DRAWINGS">FIG. <b>3</b>B</figref>: there are about twelve focal planes between the eye of the user and infinity. These focal planes, and the data within the depicted relationships, may be utilized to position virtual elements within an augmented reality scenario for a user's viewing, because the human eye is constantly sweeping around to utilize the focal planes to perceive depth.</p><p id="p-0021" num="0020">Referring to <figref idref="DRAWINGS">FIGS. <b>4</b>A-<b>4</b>C</figref>, if K(R) is a dynamic parameter for curvature equal to 1/R, where R is the focal radius of an item relative to a surface, then with increasing radius (R<b>3</b>, to R<b>2</b>, up to R<b>1</b>), you have decreasing K(R). The light field produced by a point has a spherical curvature, which is a function of how far away the point is from the eye of the user. This relationship may also be utilized for AAD systems.</p><p id="p-0022" num="0021">Referring to <figref idref="DRAWINGS">FIG. <b>5</b></figref>, a conventional diffraction grating (<b>22</b>) is shown, with light passing through the grating spacing (<b>18</b>) at an angle (theta&#x2212;20) which is related to the diffraction order (n), spatial frequency, and K factor, which equals 1/d, using the following equation: d*sin(theta)=n*wavelength (or alternatively substituting the K factor, sin(theta)=n*wavelength*K. <figref idref="DRAWINGS">FIGS. <b>6</b>A-<b>6</b>C</figref> illustrate that with decreased spacing (<b>18</b>, <b>28</b>, <b>30</b>) in the diffraction pattern (<b>22</b>, <b>24</b>, <b>26</b>), the angle (<b>20</b>, <b>32</b>, <b>34</b>) becomes greater.</p><p id="p-0023" num="0022">Referring to <figref idref="DRAWINGS">FIGS. <b>7</b>A-<b>7</b>C</figref>, three different focusing mechanisms are depicted&#x2014;refraction through a lens (<b>36</b>), reflection with a curved mirror (<b>38</b>), and diffraction with a Fresnel zone plate (<b>40</b>), also shown in <figref idref="DRAWINGS">FIG. <b>7</b>D</figref> (<b>40</b>).</p><p id="p-0024" num="0023">Referring to <figref idref="DRAWINGS">FIG. <b>8</b>A</figref>, a simplified version of diffraction is shown to illustrate that an N=&#x2212;1 mode could correspond to a virtual image; an N=+1 mode could correspond to a real image, and an N=0 mode could correspond to a focused-at-infinity image. These images could be confusing to the human eye and brain, and particularly problematic if all focused on-axis, as shown in <figref idref="DRAWINGS">FIG. <b>8</b>B</figref>. Referring to <figref idref="DRAWINGS">FIG. <b>8</b>C</figref>, an off-axis focus configuration may be utilized to allow for blocking of modes/images that are unwanted. For example, a collimated (r=infinity) image may be formed by the N=0 mode; a divergent virtual image may be formed by the N=&#x2212;1 mode; and a convergent image may be formed by the N=+1 mode. The difference in spatial location of these modes/images and their trajectories allows for filtering out or separation to prevent the aforementioned problems associated with diffraction imaging, such as overlaying, ghosting, and &#x201c;multiple exposure&#x201d; perception effects.</p><p id="p-0025" num="0024">Referring to <figref idref="DRAWINGS">FIG. <b>9</b></figref>, a waveguide is shown having an embedded diffraction grating; such waveguides are available, for example, from suppliers such as BAE Systems PLC of London, U.K. and may be utilized to intake an image from the left of <figref idref="DRAWINGS">FIG. <b>9</b></figref> as shown, pass the image through the embedded diffraction grating (<b>44</b>), and pass the resultant image out at an angle (in <figref idref="DRAWINGS">FIG. <b>9</b></figref>, for example, through the side of the waveguide). Thus a dual use of redirection and diffraction may be achieved with such an element. Indeed, off-axis focal techniques, such as those described in reference to <figref idref="DRAWINGS">FIG. <b>8</b>C</figref>, may be combined with diffraction waveguide elements such as that shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref> to result in a configuration such as that shown in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, wherein not only are redirection and diffraction accomplished, but also filtering, since in the depicted embodiment the geometry of the diffracting waveguide is such that the N=&#x2212;1 mode (say the virtual image) is passed out of the waveguide and into the eye of the user, and the other two modes (N=0 and N=+1) are trapped inside of the waveguide by reflection.</p><p id="p-0026" num="0025">Referring to <figref idref="DRAWINGS">FIGS. <b>11</b>A-<b>13</b>C</figref>, the aforementioned concepts are put into play with various augmented reality display configurations.</p><p id="p-0027" num="0026">Referring to <figref idref="DRAWINGS">FIG. <b>11</b>A</figref>, an AAD system comprises an imaging module (<b>46</b>, <b>48</b>) in front of each eye (<b>4</b>, <b>6</b>) through which the user sees the world. <figref idref="DRAWINGS">FIG. <b>11</b>B</figref> illustrates a larger view of the module (<b>46</b>) with its associated (coupled via the depicted electronic control leads; leads may also be wireless) controller (<b>66</b>), which may be a microprocessor, microcontroller, field programmable gate array (FPGA), application specific integrated circuit (ASIC), or the like. The controller preferably is coupled to a power supply and also an information exchange device, such as a wireless internet or Bluetooth adaptor, to allow for the exchange of information between the outside world and the controller (<b>66</b>). The system may be configured to operate at an image refresh rate, such as a rate between 30 and 60 frames per second. The controller may be configured to operate a high-refresh rate digital high resolution display (<b>52</b>), such as a ferro-liquid, bluephase, or bent-core display, to display various zone plate geometries quickly in succession, pertinent to each of the 12 or so depth layers. For example, in an embodiment wherein 60 frames per second overall performance is desired, the zone plate display (<b>52</b>) may be operated at 12 times this, or 720 Hz, to be able to provide simulated accommodation to each of the 12 depth layers as shown in <figref idref="DRAWINGS">FIG. <b>3</b>B</figref>. The occluding mask display (<b>54</b>) is configured to display a blacked out image geometrically corresponding to the image displayed before it on the transparent projector layer (<b>56</b>)&#x2014;blacked out to prevent light from the other side of the occluding mask display from bleeding through or interfering with display of a desired virtual or augmented image in the projector layer (<b>56</b>). Thus in an augmented reality configuration, as shown, light from the real background passes through the non-masked portions of the occlusion mask (<b>54</b>), though the transparent (i.e., not broadcasting a portion of an image) portions of the transparent projector layer (<b>56</b>), and into the zone plate layer (<b>52</b>) for accommodation treatment; images projected at the projecting layer (<b>56</b>) receive mask blocking from background light at the occlusion layer (<b>54</b>) and are projected forward into the zone plate layer (<b>52</b>) for accommodation treatment. The combination of these, or the associated perception of the augmented reality to the user, is very close to &#x201c;true 3-D&#x201d;.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIGS. <b>12</b>A-<b>12</b>B</figref> depict another embodiment wherein an imaging module (<b>58</b>) comprises high-resolution mini projector oriented at an angle approximately perpendicular to the visual axis of the eye; a waveguide comprising a substrate guided delay exit pupil expander device (<b>70</b>) magnifies and redirects the image from the small mini projector and into the zone plate layer (<b>52</b>); the occluding layer (<b>54</b>) provides similar masking functions to protect perception of the projected images from background lighting.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIGS. <b>13</b>A-<b>13</b>B</figref> depict another embodiment elements <b>52</b> and <b>70</b> are combined such that the zone plate and projecting layer are essentially housed within the same integrated module (<b>72</b>) which intakes a small image from the mini projector (<b>68</b>), redirects and magnifies it, and also diffracts it, for passage to the eye; the occluding layer (<b>54</b>) provides similar masking functions to protect perception of the projected images from background lighting.</p><p id="p-0030" num="0029">Various exemplary embodiments of the invention are described herein. Reference is made to these examples in a non-limiting sense. They are provided to illustrate more broadly applicable aspects of the invention. Various changes may be made to the invention described and equivalents may be substituted without departing from the true spirit and scope of the invention. In addition, many modifications may be made to adapt a particular situation, material, composition of matter, process, process act(s) or step(s) to the objective(s), spirit or scope of the present invention. Further, as will be appreciated by those with skill in the art that each of the individual variations described and illustrated herein has discrete components and features which may be readily separated from or combined with the features of any of the other several embodiments without departing from the scope or spirit of the present inventions. All such modifications are intended to be within the scope of claims associated with this disclosure.</p><p id="p-0031" num="0030">The invention includes methods that may be performed using the subject devices. The methods may comprise the act of providing such a suitable device. Such provision may be performed by the end user. In other words, the &#x201c;providing&#x201d; act merely requires the end user obtain, access, approach, position, set-up, activate, power-up or otherwise act to provide the requisite device in the subject method. Methods recited herein may be carried out in any order of the recited events which is logically possible, as well as in the recited order of events.</p><p id="p-0032" num="0031">Exemplary aspects of the invention, together with details regarding material selection and manufacture have been set forth above. As for other details of the present invention, these may be appreciated in connection with the above-referenced patents and publications as well as generally known or appreciated by those with skill in the art. The same may hold true with respect to method-based aspects of the invention in terms of additional acts as commonly or logically employed.</p><p id="p-0033" num="0032">In addition, though the invention has been described in reference to several examples optionally incorporating various features, the invention is not to be limited to that which is described or indicated as contemplated with respect to each variation of the invention. Various changes may be made to the invention described and equivalents (whether recited herein or not included for the sake of some brevity) may be substituted without departing from the true spirit and scope of the invention. In addition, where a range of values is provided, it is understood that every intervening value, between the upper and lower limit of that range and any other stated or intervening value in that stated range, is encompassed within the invention.</p><p id="p-0034" num="0033">Also, it is contemplated that any optional feature of the inventive variations described may be set forth and claimed independently, or in combination with any one or more of the features described herein. Reference to a singular item, includes the possibility that there are plural of the same items present. More specifically, as used herein and in claims associated hereto, the singular forms &#x201c;a,&#x201d; &#x201c;an,&#x201d; &#x201c;said,&#x201d; and &#x201c;the&#x201d; include plural referents unless the specifically stated otherwise. In other words, use of the articles allow for &#x201c;at least one&#x201d; of the subject item in the description above as well as claims associated with this disclosure. It is further noted that such claims may be drafted to exclude any optional element. As such, this statement is intended to serve as antecedent basis for use of such exclusive terminology as &#x201c;solely,&#x201d; &#x201c;only&#x201d; and the like in connection with the recitation of claim elements, or use of a &#x201c;negative&#x201d; limitation.</p><p id="p-0035" num="0034">Without the use of such exclusive terminology, the term &#x201c;comprising&#x201d; in claims associated with this disclosure shall allow for the inclusion of any additional element&#x2014;irrespective of whether a given number of elements are enumerated in such claims, or the addition of a feature could be regarded as transforming the nature of an element set forth in such claims. Except as specifically defined herein, all technical and scientific terms used herein are to be given as broad a commonly understood meaning as possible while maintaining claim validity.</p><p id="p-0036" num="0035">The breadth of the present invention is not to be limited to the examples provided and/or the subject specification, but rather only by the scope of claim language associated with this disclosure.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method of presenting a three-dimensional image to a viewer with a three-dimensional image visualization system, the method comprising:<claim-text>projecting an image from a selectively transparent projection device through a selectively transparent zone plate diffraction patterning device having a selectable geometry to apply an accommodation treatment to the image, wherein the selectively transparent zone plate diffraction patterning device is interposed between an eye of the viewer and the selectively transparent projection device;</claim-text><claim-text>operating a selectively transparent occlusion mask device to selectively block background light from entering the eye of the viewer, wherein the background light travels on an axis that runs through the selectively transparent occlusion mask device and the selectively transparent projection device toward the eye from one or more positions opposite of the selectively transparent projection device from the eye of the viewer; and</claim-text><claim-text>delivering a modified image to the eye of the viewer.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the operating the selectively transparent occlusion mask device to selectively block background light comprises creating an occluded region on at least one portion of the selectively transparent occlusion mask device.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, further comprising correlating the occluded region with the image projected from the selectively transparent projection device.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the correlating comprises selectively controlling the occluded region to geometrically align with the image projected from the selectively transparent projection device.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising displaying a selectable diffraction geometry on at least a portion of the selectively transparent zone plate diffraction patterning device.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the applying the accommodation treatment further comprises passing the image projected from the selectively transparent projection device through the selectable diffraction geometry.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, further comprising correlating the image projected from the selectively transparent projection device with the displayed selectable diffraction geometry.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the correlating comprises controlling the selectable diffraction geometry on at least a portion of the selectively transparent zone plate diffraction patterning device to geometrically align with the image projected from the selectively transparent projection device.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein applying an accommodation treatment to the image further comprises modifying a focal distance of the image projected from the selectively transparent projection device using the selectable diffraction geometry.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein projecting the image from a selectively transparent projection device further comprises redirecting light toward the eye of the viewer.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein redirecting light toward the eye of the viewer further comprises:<claim-text>accepting light into a waveguide at a first angle; and</claim-text><claim-text>outputting light at a second angle.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein applying an accommodation treatment comprises displaying a series of selectable diffraction geometries on at least a portion of a selectively transparent zone plate diffraction patterning device at a selected frequency.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the selected frequency is 720 Hz.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein selectively blocking background light from entering an eye of the viewer further comprises displaying a blacked out image on the selectively transparent occlusion mask device, the blacked out image geometrically correlating to the image projected from the selectively transparent projection device.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising emitting an initial image from an imaging module.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The method of <claim-ref idref="CLM-00015">claim 15</claim-ref>, further comprising transmitting the initial image from the imaging module to the selectively transparent projection device.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref>, further comprising receiving the initial image from the imaging module into the selectively transparent projection device.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The method of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein transmitting the initial image from the imaging module to the selectively transparent projection device includes magnifying and diffracting the initial image.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The method of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein transmitting the initial image further comprises directing the initial image toward the selectively transparent projection device on an initial image trajectory substantially perpendicular to a modified image trajectory of the modified image traveling to the eye of the viewer.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein delivering the modified image to the eye of the viewer comprises delivering an image having eye accommodation cues to the eye of the viewer.</claim-text></claim></claims></us-patent-application>