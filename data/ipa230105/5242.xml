<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005243A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005243</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17780279</doc-number><date>20201202</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="regional"><country>EP</country><doc-number>19214123.2</doc-number><date>20191206</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>762</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>764</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>26</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>28</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>34</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>56</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>763</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>764</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>26</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>28</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>34</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>56</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>7635</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">USER-GUIDED IMAGE SEGMENTATION METHODS AND PRODUCTS</invention-title><us-related-documents><us-provisional-application><document-id><country>US</country><doc-number>62944847</doc-number><date>20191206</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Dolby Laboratories Licensing Corporation</orgname><address><city>San Francisco</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>KHALILIAN-GOURTANI</last-name><first-name>Amirhossein</first-name><address><city>New York</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>GADGIL</last-name><first-name>Neeraj J.</first-name><address><city>Pune</city><country>IN</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Su</last-name><first-name>Guan-Ming</first-name><address><city>Fremont</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>Dolby Laboratories Licensing Corporation</orgname><role>02</role><address><city>San Francisco</city><state>CA</state><country>US</country></address></addressbook></assignee></assignees><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/US2020/062942</doc-number><date>20201202</date></document-id><us-371c12-date><date>20220526</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A method for image segmentation includes (a) clustering, based upon k-means clustering, pixels of an image into first clusters, (b) outputting a cluster map of the first clusters (c) re-clustering the pixels into a new plurality of non-disjoint pixel-clusters, and (d) classifying the non-disjoint pixel-clusters in categories, according to a user-indicated classification. Another method for image segmentation includes (a) forming a graph with each node of the graph corresponding to a first respective non-disjoint pixel-cluster of the image and connected to each terminal of the graph and to all other nodes corresponding to other respective non-disjoint pixel-clusters that, in the image, are within a neighborhood of the first respective non-disjoint pixel-cluster, (b) setting weights of connections of the graph according to a user-indicated classification in categories respectively associated with the terminals, and (c) segmenting the image into the categories by cutting the graph based upon the weights.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="119.21mm" wi="157.06mm" file="US20230005243A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="222.00mm" wi="159.17mm" file="US20230005243A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="216.49mm" wi="124.04mm" file="US20230005243A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="207.35mm" wi="159.09mm" file="US20230005243A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="165.02mm" wi="145.71mm" file="US20230005243A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="218.36mm" wi="159.09mm" file="US20230005243A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="217.51mm" wi="138.77mm" file="US20230005243A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="128.44mm" wi="147.07mm" file="US20230005243A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="192.79mm" wi="159.09mm" file="US20230005243A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="218.10mm" wi="159.17mm" file="US20230005243A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="217.51mm" wi="159.09mm" file="US20230005243A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="217.42mm" wi="159.09mm" file="US20230005243A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="217.51mm" wi="140.21mm" file="US20230005243A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="187.37mm" wi="129.12mm" file="US20230005243A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="89.66mm" wi="129.12mm" file="US20230005243A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="165.95mm" wi="110.32mm" file="US20230005243A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="136.14mm" wi="122.43mm" file="US20230005243A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application claims priority to U.S. Provisional Patent Application No. 62/944,847, filed 6 Dec. 2019, and European Patent Application No. 19214123.2, filed 6 Dec. 2019, both of which are incorporated herein by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">The present application relates to image segmentation, for example for the purpose of selecting a region of interest.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">Image segmentation refers to the process of partitioning a digital image into multiple segments. A wide range of applications benefit from automatic image segmentation, such as machine vision, face recognition, video surveillance, and video editing. In such applications, image segmentation may be used to locate objects or edges in an image. Typically, image segmentation is performed automatically in software. A variety of algorithms exist for this purpose, ranging from very simple to highly complex.</p><p id="p-0005" num="0004">In digital movie production, it is very common to enhance the captured video footage to create a final look for the movie. The originally captured colors may be modified to optimize the movie for viewing on a particular type of screen, correct for variation in lighting/colors, create a special effect, and/or to achieve a desired artistic feel. Such enhancement is typically performed by a colorist on a color-grading computer system equipped with color grading software and a control board providing color controls akin the sound controls in a music studio.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0006" num="0005">In an embodiment, a method for user-guided image segmentation includes (a) clustering, based upon k-means clustering, pixels of an image into a plurality of first clusters, (b) outputting a cluster map of the first clusters, wherein the cluster map comprises non-disjoint pixel-clusters and disjoint pixel-clusters, wherein, within each of the non-disjoint pixel-clusters, any pair of pixels are immediately adjacent each other or connected to each other via one or more other pixels of the non-disjoint pixel-cluster (c) re-clustering, at least in part by processing connectivity of pixels within the first clusters, the pixels into a new plurality of non-disjoint pixel-clusters wherein the step of re-clustering re-clusters only the non-disjoint pixel-clusters of the map of the first clusters and (d) classifying each of the non-disjoint pixel-clusters in one of a plurality of categories, according to a user-indicated classification of a proper subset of the non-disjoint pixel-clusters in the categories.</p><p id="p-0007" num="0006">In an embodiment, the re-clustering step comprises assigning new labels only to non-disjoint pixel clusters containing at least a threshold number of pixels. The new label defines a respective one of the non-disjoint pixel cluster of the new plurality of non-disjoint pixel-clusters.</p><p id="p-0008" num="0007">In an embodiment, the re-clustering step further comprises applying morphological filters to the map of the first clusters to enhance connectivity of pixels within the first clusters.</p><p id="p-0009" num="0008">In an embodiment, the re-clustering comprises performing a raster scan through all non-disjoint pixel clusters of the cluster map in a certain pattern, and merging each non-disjoint pixel cluster that is smaller than a threshold number of pixels into the most recent non-disjoint cluster, in the raster scan, that has at least the threshold number of pixels. Alternatively, and/additionally, merging of each non-disjoint pixel cluster smaller than the threshold is performed with an adjacent non-disjoint pixel cluster that has at least the threshold number of pixels.</p><p id="p-0010" num="0009">In an embodiment, the re-clustering step comprises before applying morphological filters to the map of the first clusters, obtaining a binary mask for each cluster for processing each cluster separately with morphological filters.</p><p id="p-0011" num="0010">In an embodiment, the step of applying morphological filters comprises applying a closing operation function and an opening operation function for filling pixel gaps in each one of the first clusters and obtaining an updated cluster map of the first clusters.</p><p id="p-0012" num="0011">In an embodiment, merging each non-disjoint pixel cluster comprises re-labelling each non-disjoint pixel cluster that is smaller than the threshold with a label of the most recent (and/or adjacent) non-disjoint cluster that has at least the threshold number of pixels. The label defines a respective one of the non-disjoint pixel clusters.</p><p id="p-0013" num="0012">In an embodiment, wherein <img id="CUSTOM-CHARACTER-00001" he="2.79mm" wi="1.78mm" file="US20230005243A1-20230105-P00001.TIF" alt="custom-character" img-content="character" img-format="tif"/><sup>(l) </sup>is the map of non-disjoint pixel clusters for a l region, <img id="CUSTOM-CHARACTER-00002" he="2.79mm" wi="3.22mm" file="US20230005243A1-20230105-P00002.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>(l) </sub>is the number of non-disjoint pixel clusters in the l'th region, <img id="CUSTOM-CHARACTER-00003" he="2.79mm" wi="2.12mm" file="US20230005243A1-20230105-P00003.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>(l)</sub><sup>(c) </sup>is the c'th non-disjoint pixel cluster of l'th region, <img id="CUSTOM-CHARACTER-00004" he="2.79mm" wi="3.22mm" file="US20230005243A1-20230105-P00004.TIF" alt="custom-character" img-content="character" img-format="tif"/>is the number of pixels within the c'th non-disjoint pixel cluster <img id="CUSTOM-CHARACTER-00005" he="2.79mm" wi="2.12mm" file="US20230005243A1-20230105-P00003.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>(l)</sub><sup>(c) </sup>and T<sub>KC </sub>is the threshold number of pixels in c'th non-disjoint pixel cluster. If <img id="CUSTOM-CHARACTER-00006" he="2.79mm" wi="3.22mm" file="US20230005243A1-20230105-P00005.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>(l)</sub>&#x3e;1 and <img id="CUSTOM-CHARACTER-00007" he="2.79mm" wi="3.22mm" file="US20230005243A1-20230105-P00006.TIF" alt="custom-character" img-content="character" img-format="tif"/>&#x2265;T<sub>KC </sub>a new label to the c'th non-disjoint pixel cluster is assigned, and if <img id="CUSTOM-CHARACTER-00008" he="2.79mm" wi="3.22mm" file="US20230005243A1-20230105-P00007.TIF" alt="custom-character" img-content="character" img-format="tif"/>&#x3c;T<sub>KC</sub>, the c'th non-disjoint pixel cluster is merged with an adjacent non-disjoint pixel cluster.</p><p id="p-0014" num="0013">In an embodiment, for each c'th non-disjoint pixel cluster <img id="CUSTOM-CHARACTER-00009" he="2.79mm" wi="2.12mm" file="US20230005243A1-20230105-P00003.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>(l)</sub><sup>(c) </sup>a bounding box is defined surrounding the c'th non-disjoint pixel cluster <img id="CUSTOM-CHARACTER-00010" he="2.79mm" wi="2.12mm" file="US20230005243A1-20230105-P00003.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>(l)</sub><sup>(c) </sup>by taking minimum and maximum horizontal and vertical x-y coordinates of the c'th non-disjoint pixel cluster <img id="CUSTOM-CHARACTER-00011" he="2.79mm" wi="2.12mm" file="US20230005243A1-20230105-P00003.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>(l)</sub><sup>(c) </sup>in a cartesian coordinate system for representing the image. The adjacent non-disjoint pixel cluster is identified by the pixel computed as:</p><p id="p-0015" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>(<i>x</i><sub>(l)</sub><sup>(c)TL</sup><i>,y</i><sub>(l)</sub><sup>(c),TL</sup>)=(max{(<i>x</i><sub>(l)</sub><sup>(c),min</sup>&#x2212;1),0},max{(<i>y</i><sub>(l)</sub><sup>(c),min</sup>&#x2212;1),0}),<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0016" num="0000">(x<sub>(l)</sub><sup>(c)TL</sup>,y<sub>(l)</sub><sup>(c),TL</sup>) represent the horizontal and vertical coordinates of the pixel in the adjacent non-disjoint pixel cluster TL. x<sub>(l)</sub><sup>(c),min </sup>and y<sub>(l)</sub><sup>(c),min </sup>represent the minimum horizontal and vertical coordinates of the c'th non-disjoint pixel cluster <img id="CUSTOM-CHARACTER-00012" he="2.79mm" wi="2.12mm" file="US20230005243A1-20230105-P00003.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>(l)</sub><sup>(c)</sup>.</p><p id="p-0017" num="0014">In an embodiment, a software product for image segmentation includes computer-readable media storing machine-readable instructions. The instructions include clustering instructions that, when executed by a processor, control the processor to (a) cluster, based upon k-means clustering, pixels of an image into a plurality of first clusters and (b) store, to a memory, a k-means-cluster-map of the first clusters. The instructions further include re-clustering instructions that, when executed by the processor, control the processor to (a) retrieve the k-means-cluster-map from memory, (b) process connectivity of pixels within the first clusters of the k-means-cluster-map to re-cluster the pixels into a plurality of non-disjoint pixel-clusters such that any pair of pixels within each of the non-disjoint pixel-clusters are immediately adjacent each other or connected to each other via one or more other pixels of the non-disjoint pixel-cluster, and (c) store, to the memory, a connected-cluster-map of the non-disjoint pixel-clusters. The instructions also include classification instructions that, when executed by the processor, control the processor to classify each of the non-disjoint pixel-clusters in one of a plurality of categories, according to a user-specified classification of a proper subset of the non-disjoint pixel-clusters in the categories.</p><p id="p-0018" num="0015">In an embodiment, another method for user-guided image segmentation includes (a) forming a graph having a plurality of terminals and a plurality of nodes, wherein each of the nodes corresponds to a first respective non-disjoint pixel-cluster of the image and is connected, in the graph, to each of the terminals and all other ones of the nodes corresponding to other respective non-disjoint pixel-clusters that, in the image, are within a neighborhood of the first respective non-disjoint pixel-cluster, (b) initializing the graph by setting weights of connections of the graph at least partly according to a user input indicating classification of some but not all of the non-disjoint pixel-clusters in a plurality of categories respectively associated with the plurality of terminals, and (c) segmenting the image into the categories by cutting the graph based upon the weights.</p><p id="p-0019" num="0016">In an embodiment, another software product for image segmentation includes computer-readable media storing machine-readable instructions. The instructions include graph setup instructions that, when executed by a processor, control the processor to form a graph having a plurality of terminals and a plurality of nodes, wherein each of the nodes corresponds to a respective non-disjoint pixel-cluster of the image and is connected, in the graph, to each of the terminals and all other ones of the nodes corresponding to other respective non-disjoint pixel-clusters that, in the image, are within a neighborhood of the respective non-disjoint pixel-cluster. The instructions further include graph initializing instructions that, when executed by the processor, control the processor to set weights of connections of the graph at least partly according to a user input indicating classification of some but not all of the non-disjoint pixel-clusters in a plurality of categories respectively corresponding to the plurality of terminals. The instructions also include graph cutting instructions that, when executed by the processor, control the processor to cut the graph based upon the weights so as to segment the image into the categories.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0020" num="0017"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a user-guided image segmentation engine in an example use scenario, according to an embodiment.</p><p id="p-0021" num="0018"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates a method for user-guided image segmentation, according to an embodiment.</p><p id="p-0022" num="0019"><figref idref="DRAWINGS">FIGS. <b>3</b>A-C</figref> show an example of processing performed by the method of <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0023" num="0020"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates a method for user-guided image segmentation, which utilizes two clustering steps, according to an embodiment.</p><p id="p-0024" num="0021"><figref idref="DRAWINGS">FIGS. <b>5</b>A and <b>5</b>B</figref> show an example of processing performed by the method of <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0025" num="0022"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates a user-guided image segmentation method that utilizes graph cutting, according to an embodiment.</p><p id="p-0026" num="0023"><figref idref="DRAWINGS">FIGS. <b>7</b>A-C</figref> illustrate, by example, certain aspects of the method of <figref idref="DRAWINGS">FIG. <b>6</b></figref>.</p><p id="p-0027" num="0024"><figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates a computer for user-guided image segmentation, according to an embodiment.</p><p id="p-0028" num="0025"><figref idref="DRAWINGS">FIG. <b>9</b></figref> illustrates a k-means based clustering method for assigning each pixel of an image to one of a plurality of first clusters according to color and location of the pixel, according to an embodiment.</p><p id="p-0029" num="0026"><figref idref="DRAWINGS">FIG. <b>10</b></figref> shows an example that illustrates certain aspects of the method of <figref idref="DRAWINGS">FIG. <b>9</b></figref>.</p><p id="p-0030" num="0027"><figref idref="DRAWINGS">FIG. <b>11</b></figref> illustrates a method for re-clustering a plurality of first clusters into a plurality of non-disjoint pixel-clusters, according to an embodiment.</p><p id="p-0031" num="0028"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is an example that illustrates certain aspects of the method of <figref idref="DRAWINGS">FIG. <b>11</b></figref>.</p><p id="p-0032" num="0029"><figref idref="DRAWINGS">FIG. <b>13</b></figref> illustrates a graph-cutting method for classifying each of a plurality of image regions in one of a plurality of categories, according to a user-indicated classification of a proper subset of the regions in the categories, according to an embodiment.</p><p id="p-0033" num="0030"><figref idref="DRAWINGS">FIGS. <b>14</b>A and <b>14</b>B</figref> are examples that illustrate certain aspects of the method of <figref idref="DRAWINGS">FIG. <b>13</b></figref>.</p><p id="p-0034" num="0031"><figref idref="DRAWINGS">FIGS. <b>15</b>A-C</figref> show clustering of an example image.</p><p id="p-0035" num="0032"><figref idref="DRAWINGS">FIG. <b>16</b>A</figref> illustrates further details of the method of re-clustering a plurality of first clusters into a plurality of non-disjoint pixel-clusters illustrated in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, according to an embodiment.</p><p id="p-0036" num="0033"><figref idref="DRAWINGS">FIG. <b>16</b>B</figref> is an example that illustrates certain aspects of the method of <figref idref="DRAWINGS">FIG. <b>16</b>A</figref>.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DESCRIPTION OF EXAMPLE EMBODIMENTS</heading><p id="p-0037" num="0034"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates one user-guided image segmentation engine <b>112</b> in an example use scenario. In the scenario of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, image segmentation engine <b>112</b> is incorporated in a video processing pipeline <b>100</b> that manipulates the focus of an image <b>180</b> or of several images <b>180</b> of a video stream, to produce a focus-manipulated image <b>184</b> wherein a region of interest is kept in focus while other regions are defocused. In the example depicted in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, image <b>180</b> contains a group of people <b>190</b> of which focus-manipulated image <b>184</b> keeps two people <b>194</b> in focus while remaining people <b>190</b> are defocused.</p><p id="p-0038" num="0035">Pipeline <b>100</b> is useful in movie production and may be used by content creators for a variety of purposes. For example, the focus manipulation produced by pipeline <b>100</b> may serve to draw viewers' attention to a more important portion of a scene (e.g., people <b>194</b> or an advertisement/logo), produce a sense of depth, or give a more lifelike effect to a scene.</p><p id="p-0039" num="0036">Pipeline <b>100</b> includes an encoder <b>110</b> and a decoder <b>120</b>. Encoder <b>110</b> processes image <b>180</b> to generate metadata <b>182</b> specifying which portion of image <b>180</b> should be kept in focus and which portion of image <b>180</b> should be defocused. Decoder <b>120</b> processes image <b>180</b> to produce focus-manipulated image <b>184</b> according to metadata <b>182</b>.</p><p id="p-0040" num="0037">Encoder <b>110</b> includes image segmentation engine <b>112</b> and a region-of-focus (ROF) data encoding unit <b>114</b>. Image segmentation engine <b>112</b> segments image <b>180</b> (or each of several images <b>180</b>) according to desired focus properties of different portions of image <b>180</b>. Image segmentation engine <b>112</b> is semiautomatic and includes a software product that applies an image segmentation process to image <b>180</b> with guidance from a user input <b>188</b> from a user, e.g., a colorist. In the example scenario of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, image segmentation engine <b>112</b> segments image <b>180</b> into a focused portion and a defocused portion, such that each pixel of image <b>180</b> is assigned either to the focused portion or to the defocused portion. ROF data encoding unit <b>114</b> generates metadata <b>182</b> according to the image segmentation determined by image segmentation engine <b>112</b>. Metadata <b>182</b> may encode the image segmentation as a specification of the boundaries between the focus categories. For example, metadata <b>182</b> may specify an outline of people <b>194</b>.</p><p id="p-0041" num="0038">Decoder <b>120</b> includes an ROF data decoding unit <b>122</b> and a defocus filtering engine <b>124</b>. ROF data decoding unit <b>122</b> interprets metadata <b>182</b> to determine which portion of image <b>180</b> should be defocused, and defocus filtering engine <b>124</b> defocuses this portion of image <b>180</b> to produce image <b>184</b>.</p><p id="p-0042" num="0039">Encoder <b>110</b> may further encode, into metadata <b>182</b>, a defocus strength to be applied by defocus filtering engine <b>124</b>. The defocus strength may be defined by a user. In one implementation, metadata <b>182</b> specifies a maximum defocus strength, and defocus filtering engine <b>124</b> defocuses image <b>180</b> such that the degree of focus in focus-manipulated image <b>184</b> gradually transitions from no defocusing at a boundary of a region to kept in focus (e.g., people <b>194</b>) to maximum defocus strength at the greatest distance from such boundaries. The optimal defocus strength may depend on the type of screen on which image <b>184</b> is to be displayed. For example, a screen with a higher dynamic range generally requires a greater degree of defocusing than a screen with a lower dynamic range to produce the same apparent effect.</p><p id="p-0043" num="0040">The functionality of pipeline <b>100</b> is readily extended to segment image <b>180</b> into more than two different focus-level portions, for example a portion that is kept in focus, a portion that is moderately defocused, and a portion that is more strongly defocused.</p><p id="p-0044" num="0041">The capability of image segmentation engine <b>112</b> reaches beyond segmentation for the purpose of focus manipulation. Image segmentation engine <b>112</b> may segment an image for any purpose. In one implementation, image segmentation engine <b>112</b> segments an image in a binary fashion such that all pixels of the image are classified in either one of two possible categories, for example region to be kept in focus versus region to be defocused or region of interest versus region not of interest. In another implementation, image segmentation engine <b>112</b> segments an image into three or more categories. Each of these three or more categories may indicate a desired level of defocusing to be applied or indicate a different type of object. Image segmentation engine <b>112</b> may thus be used by a user for a variety of different image enhancement purposes, and image segmentation engine <b>112</b> may also be used in other technology areas, such as machine vision.</p><p id="p-0045" num="0042"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates one method <b>200</b> for user-guided image segmentation. Method <b>200</b> is encoded in a software product incorporated in image segmentation engine <b>112</b>. In one implementation, image segmentation engine <b>112</b> includes a processor and a non-transitory memory encoding method <b>200</b> as machine-readable instructions executable by the processor. <figref idref="DRAWINGS">FIGS. <b>3</b>A-C</figref> show an example of processing performed by method <b>200</b> <figref idref="DRAWINGS">FIGS. <b>2</b> and <b>3</b>A</figref>-C are best viewed together in the following description.</p><p id="p-0046" num="0043">Method <b>200</b> includes steps <b>210</b> and <b>220</b>. Step <b>210</b> takes an image <b>280</b>, such as image <b>180</b> as an input, and clusters image <b>280</b> into a plurality of clusters of interconnected pixels. Each of these clusters is substantially composed of pixels that are similar to each other in some aspect, for example in color and/or brightness. Step <b>210</b> may output a cluster map <b>286</b> that indicates the clustering of pixels performed by step <b>210</b>. Step <b>210</b> does not require user guidance and may be formed fully automatically, although it is understood that a user may define certain processing parameters, for example influencing the general size scale of the clusters.</p><p id="p-0047" num="0044"><figref idref="DRAWINGS">FIG. <b>3</b>A</figref> shows an image <b>300</b> which is an example of image <b>280</b> Image <b>300</b> depicts a woman <b>302</b> in a field of flowers <b>306</b> with the sky <b>308</b> in the background. Some of flowers <b>306</b> are in a box <b>304</b> at woman <b>302</b>. In this example scenario, a user wishes to select a region of interest, such as woman <b>302</b> box <b>304</b>, and those of flowers <b>306</b> that are in box <b>304</b>. Doing this manually would be tedious, especially if image <b>300</b> is part of a video stream with moving objects and the segmentation is to be applied to the entire video stream. Step <b>210</b> of method <b>200</b> processes image <b>300</b> to form the clusters indicated by cluster map <b>310</b> overlaid on image <b>300</b> in <figref idref="DRAWINGS">FIG. <b>3</b>B</figref>. Individual non-disjoint pixel-clusters of cluster map <b>310</b> are outlined with light gray borders. Herein, a &#x201c;non-disjoint pixel-cluster&#x201d; refers to a cluster of pixels that cannot be decomposed into two or more disjoint segments. In other words, any two pixels in a non-disjoint pixel-cluster are immediately adjacent each other or connected to each other via one or more other pixels of the cluster. In the field of topology, a non-disjoint pixel-cluster would be referred to as a &#x201c;connected set&#x201d; of pixels. It is understood that a single pixel may be considered a non-disjoint pixel-cluster. Alternatively, only clusters of pixels with at least a threshold number of pixels may qualify as a non-disjoint pixel-cluster, and smaller clusters of pixels and single pixels may be instead merged into an adjacent cluster of pixels. It is understood that a &#x201c;disjoint pixel-cluster&#x201d; refers to a cluster of pixels that can be decomposed into one or more disjoint segments. For example, a disjoint pixel cluster may contain pixels of different characteristic such as of different pixel color, pixel location, etc. Cluster map <b>310</b> is an example of cluster map <b>286</b>.</p><p id="p-0048" num="0045">Step <b>220</b> classifies each of the clusters, determined in step <b>210</b>, in one of a plurality of categories. Step <b>220</b> performs this classification according to a user input <b>288</b>. User input <b>288</b> indicates classification of a proper subset of the clusters (i.e., some but not all of the clusters) in these categories. Based on user input <b>288</b> and an evaluation of similarity between clusters, such as color and location similarity, step <b>220</b> automatically classifies all remaining clusters in the categories, to produce an image segmentation <b>290</b>. The number of clusters directly classified by user input <b>288</b> may be as small as a single cluster for each category, although for more complex image scenes and image segmentation demands, a larger number of clusters may need to be directly classified by user input <b>288</b> in order to reliably achieve the desired image segmentation. The user generating user input <b>288</b> may do so based on image <b>280</b> alone or based on a combination of image <b>280</b> and the cluster map generated in step <b>210</b>. When based on image <b>280</b> alone, user input <b>288</b> may simply indicate the classification of certain pixels of image <b>280</b>, and step <b>220</b> then classifies the corresponding clusters accordingly before proceeding to classify the remaining clusters.</p><p id="p-0049" num="0046">In the example of <figref idref="DRAWINGS">FIGS. <b>3</b>A-C</figref>, the associated user input <b>288</b> classifies certain clusters <b>312</b> as belonging to the region of interest and certain other clusters <b>314</b> as not belonging to the region of interest (see <figref idref="DRAWINGS">FIG. <b>3</b>B</figref>). It is understood that the actual number of clusters thus classified by the user may differ from that shown in <figref idref="DRAWINGS">FIG. <b>3</b>B</figref>. For example, a user may classify a larger number of clusters near the boundaries of the region of interest. Based on this user input, step <b>220</b> then classifies the remaining clusters as either belonging or not belonging to the region of interest, so as to arrive at the image segmentation <b>320</b> shown in <figref idref="DRAWINGS">FIG. <b>3</b>C</figref>, where a white border outlines the boundary between region of interest <b>322</b> and the remainder <b>324</b> of image <b>300</b>.</p><p id="p-0050" num="0047">The manual effort required by a user to guide the image segmentation of method <b>200</b> consists merely in indicating the classification of relatively few clusters or pixels. In contrast, completely manual image segmentation would typically require drawing a boundary around the region of interest, which would be a far more tedious process. At the opposite end of the spectrum, fully automatic image segmentation would likely fail unless preconfigured to search for a certain type of object in the image. Compared to completely manual image segmentation and fully automatic image segmentation, method <b>200</b> benefits from a high level of automation combined with relatively effortless user guidance to provide accurate image segmentation in a manner that is inherently adaptable to varying image content and varying image segmentation goals. The level of automation provided by method <b>200</b> is especially advantageous when processing a video stream. As discussed in further detail below in reference to <figref idref="DRAWINGS">FIG. <b>6</b></figref>, user guidance provided for a few images or even a single image of a video stream, such as a video stream of one scene in a movie, may be applied to segmentation of the entire video stream.</p><p id="p-0051" num="0048">Although not shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, method <b>200</b> may include iteratively repeating step <b>220</b>. For example, image segmentation <b>290</b> may be displayed to the user. If dissatisfied with image segmentation <b>290</b>, the user may modify user input <b>288</b>, for example to classify additional clusters or pixels, and feed this revised user input <b>288</b> to another iteration of step <b>220</b>. When method image segmentation engine <b>112</b> implements method <b>200</b>, ROF data encoding unit <b>114</b> encodes image segmentation <b>290</b> in metadata <b>182</b>.</p><p id="p-0052" num="0049"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates one method <b>400</b> for user-guided image segmentation, which utilizes two clustering steps. Method <b>400</b> includes steps <b>410</b> and <b>420</b>, which are embodiments of steps <b>210</b> and <b>410</b>, respectively, of method <b>200</b> Step <b>410</b> clusters pixels of an image into a plurality of non-disjoint pixel-clusters. Step <b>420</b> classifies each of the non-disjoint pixel-clusters, determined in step <b>410</b>, in one of a plurality of categories according to a user-indicated classification of a proper subset of the non-disjoint pixel-clusters in the categories.</p><p id="p-0053" num="0050">Step <b>410</b> includes steps <b>412</b> and <b>414</b>. Step <b>412</b> uses k-means clustering to cluster the pixels of the image into a plurality of first clusters. Each first cluster is not necessarily a non-disjoint pixel-cluster. For example, a first cluster, as determined by step <b>412</b> may include two or more disjoint clusters separated from each other by one or more other pixels. Step <b>414</b> re-clusters the pixels into a plurality of non-disjoint pixel-clusters. The re-clustering in step <b>414</b> is achieved at least in part by processing the connectivity of pixel within the first clusters.</p><p id="p-0054" num="0051"><figref idref="DRAWINGS">FIGS. <b>5</b>A and <b>5</b>B</figref> show an example of processing performed by method <b>400</b>. <figref idref="DRAWINGS">FIG. <b>5</b>A</figref> is a map <b>510</b> of first clusters <b>512</b> generated in step <b>410</b> based on image <b>300</b> of <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>. Clusters <b>512</b> are indicated by different grayscales in <figref idref="DRAWINGS">FIG. <b>5</b>A</figref>. For clarity of illustration, only two clusters <b>512</b> are labeled in <figref idref="DRAWINGS">FIG. <b>5</b>A</figref>. Some regions of cluster map <b>510</b> exhibit a lesser degree of connectedness than other regions of map <b>510</b> For example, clusters <b>512</b> in region <b>514</b> appear relatively disjointed. If cluster map <b>510</b> was fed directly to step <b>420</b>, the complexity of clusters <b>512</b> in region <b>514</b> might degrade the performance of classification in step <b>420</b> and/or require more comprehensive classification to be done by the user to sufficiently guide classification of the remaining clusters.</p><p id="p-0055" num="0052"><figref idref="DRAWINGS">FIG. <b>5</b>B</figref> shows a map <b>520</b> of non-disjoint pixel-clusters <b>522</b>, as determined by step <b>414</b>, overlaid on image <b>300</b>. For clarity of illustration, only two clusters <b>522</b> are labeled in <figref idref="DRAWINGS">FIG. <b>5</b>B</figref>. It is evident that each cluster <b>522</b> is a non-disjoint pixel-cluster. The difference between cluster map <b>510</b> and cluster map <b>520</b> is significant in some regions, for example in region <b>514</b>. As compared to cluster map <b>510</b>, cluster map <b>520</b> is a greatly improved starting point for step <b>420</b>. Even though clusters <b>522</b> of cluster map <b>520</b> are on a more regular grid and, in many places, of more regular shape, the shape of clusters <b>522</b> conforms to significant boundaries, such as the boundary between the head of woman <b>302</b> and sky <b>308</b> and the perimeter of individual flowers <b>306</b>. Without user guidance, however, it would be challenging to automatically delineate between region of interest <b>322</b> and the remainder <b>324</b> of image <b>300</b> (see <figref idref="DRAWINGS">FIG. <b>3</b>C</figref>). For example, user input may be particularly necessary in the field of flowers <b>306</b> to define the desired segmentation between region of interest <b>322</b> and the remainder <b>324</b> of image <b>300</b>.</p><p id="p-0056" num="0053">Referring again to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, step <b>410</b> may implement a step <b>416</b> of clustering the pixels according to location similarity and color similarity. In one embodiment, step <b>410</b> implements step <b>416</b> such that the k-means clustering of step <b>412</b> is based on location similarity and color similarity of pixels. Similarly, step <b>420</b> may implement a step <b>422</b> of classifying the remaining non-disjoint pixel-clusters, not classified by the user input, according to location similarity and color similarity with the user-classified non-disjoint pixel-clusters. Step <b>422</b> helps reduce the number of clusters that must be classified directly by the user in order to achieve satisfactory image segmentation performance. As an example, in the <figref idref="DRAWINGS">FIG. <b>5</b>B</figref>, color-similarity evaluation of clusters <b>522</b> may significantly aid delineation between woman <b>302</b> and sky <b>308</b> if only a few clusters <b>522</b> on either side of the boundary have been classified by the user.</p><p id="p-0057" num="0054">Step <b>410</b> may be preceded by a step <b>402</b> of normalizing each color component of the image according to average and standard deviation of the color component across the image. In an embodiment of step <b>402</b> configured to process an image with three color channels, the color channels have respective means &#x3bc;<sub>1</sub>, &#x3bc;<sub>2</sub>, and &#x3bc;<sub>3 </sub>and respective standard deviations SD<sub>1</sub>, SD<sub>2</sub>, and SD<sub>3</sub>:</p><p id="p-0058" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mrow>  <mrow>   <msub>    <mi>&#x3bc;</mi>    <mi>i</mi>   </msub>   <mo>=</mo>   <mrow>    <mfrac>     <mn>1</mn>     <msub>      <mi>N</mi>      <mi>i</mi>     </msub>    </mfrac>    <mo>&#x2062;</mo>    <mrow>     <munderover>      <mo>&#x2211;</mo>      <mrow>       <mi>p</mi>       <mo>=</mo>       <mn>0</mn>      </mrow>      <mrow>       <msub>        <mi>N</mi>        <mi>i</mi>       </msub>       <mo>-</mo>       <mn>1</mn>      </mrow>     </munderover>     <msubsup>      <mi>I</mi>      <mi>p</mi>      <mrow>       <mo>(</mo>       <mi>i</mi>       <mo>)</mo>      </mrow>     </msubsup>    </mrow>   </mrow>  </mrow>  <mo>,</mo>  <mtext>&#x205f;</mtext>  <mrow>   <mi>i</mi>   <mo>=</mo>   <mn>1</mn>  </mrow>  <mo>,</mo>  <mn>2</mn>  <mo>,</mo>  <mn>3</mn>  <mo>,</mo>  <mspace linebreak="newline"/>  <mrow>   <mrow>    <mi>S</mi>    <mo>&#x2062;</mo>    <msub>     <mi>D</mi>     <mi>i</mi>    </msub>   </mrow>   <mo>=</mo>   <msqrt>    <mrow>     <mfrac>      <mn>1</mn>      <mrow>       <msub>        <mi>N</mi>        <mi>i</mi>       </msub>       <mo>-</mo>       <mn>1</mn>      </mrow>     </mfrac>     <mo>&#x2062;</mo>     <mrow>      <munderover>       <mo>&#x2211;</mo>       <mrow>        <mi>p</mi>        <mo>=</mo>        <mn>0</mn>       </mrow>       <mrow>        <msub>         <mi>N</mi>         <mi>i</mi>        </msub>        <mo>-</mo>        <mn>1</mn>       </mrow>      </munderover>      <msup>       <mrow>        <mo>(</mo>        <mrow>         <msubsup>          <mi>I</mi>          <mi>p</mi>          <mrow>           <mo>(</mo>           <mi>i</mi>           <mo>)</mo>          </mrow>         </msubsup>         <mo>-</mo>         <msub>          <mi>&#x3bc;</mi>          <mi>i</mi>         </msub>        </mrow>        <mo>)</mo>       </mrow>       <mn>2</mn>      </msup>     </mrow>    </mrow>   </msqrt>  </mrow>  <mo>,</mo>  <mrow>   <mi>i</mi>   <mo>=</mo>   <mn>1</mn>  </mrow>  <mo>,</mo>  <mn>2</mn>  <mo>,</mo>  <mn>3</mn>  <mo>,</mo> </mrow></math></maths></p><p id="p-0059" num="0000">wherein N<sub>i </sub>is the number of pixels in the i'th color channel, and I<sub>p</sub><sup>(i) </sup>is the value of the p'th pixel of the i'th color channel. For the p'th of the i'th color channel, the normalized pixel value is</p><p id="p-0060" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mrow>  <mrow>   <msubsup>    <mover>     <mi>I</mi>     <mo>_</mo>    </mover>    <mi>p</mi>    <mrow>     <mo>(</mo>     <mi>i</mi>     <mo>)</mo>    </mrow>   </msubsup>   <mo>=</mo>   <mfrac>    <mrow>     <mo>(</mo>     <mrow>      <msubsup>       <mi>I</mi>       <mi>p</mi>       <mrow>        <mo>(</mo>        <mi>i</mi>        <mo>)</mo>       </mrow>      </msubsup>      <mo>-</mo>      <msub>       <mi>&#x3bc;</mi>       <mi>i</mi>      </msub>     </mrow>     <mo>)</mo>    </mrow>    <mrow>     <mi>S</mi>     <mo>&#x2062;</mo>     <msub>      <mi>D</mi>      <mi>i</mi>     </msub>    </mrow>   </mfrac>  </mrow>  <mo>,</mo>  <mrow>   <mi>i</mi>   <mo>=</mo>   <mn>1</mn>  </mrow>  <mo>,</mo>  <mn>2</mn>  <mo>,</mo>  <mrow>   <mn>3</mn>   <mo>.</mo>  </mrow> </mrow></math></maths></p><p id="p-0061" num="0000">The three color channels may be YCbCr, i.e., luma, blue-difference, and red-difference. Alternatively, the three color channels may be RGB, i.e., red, green, and blue. Step <b>402</b> has been found, empirically, to improve the clustering performance of step <b>410</b>, at least under some circumstances.</p><p id="p-0062" num="0055"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates one user-guided image segmentation method <b>600</b> that utilizes graph cutting. Method <b>600</b> may be performed on its own but also after method <b>200</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref> or method <b>400</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>. In the latter cases, the image may have been already segmented into non-disjoint pixel clusters. Method <b>600</b> may further enhance segmentation. <figref idref="DRAWINGS">FIGS. <b>7</b>A-C</figref> illustrate certain aspects of method <b>600</b>. <figref idref="DRAWINGS">FIGS. <b>6</b> and <b>7</b>A</figref>-C are best viewed together in the following description. Method <b>600</b> includes a step <b>620</b> of classifying each of a plurality of non-disjoint pixel-clusters of an image in one of a plurality of categories, according to a user-indicated classification of a proper subset of the non-disjoint pixel-clusters in those categories. Step <b>620</b> may be preceded by a step <b>620</b> of clustering pixels of the image into the plurality of non-disjoint pixel-clusters. Step <b>620</b> may implement step <b>410</b>, and optionally also step <b>402</b>, of method <b>400</b>. Step <b>620</b> is an embodiment of step <b>210</b> of method <b>200</b> Step <b>620</b> is an embodiment of step <b>220</b> of method <b>200</b> and may be implemented in step <b>420</b> of method <b>400</b>.</p><p id="p-0063" num="0056">Step <b>620</b> includes steps <b>622</b>, <b>624</b>, and <b>628</b>. Step <b>622</b> forms a graph having a plurality of terminals and a plurality of nodes. Each node corresponds to a first respective non-disjoint pixel-cluster of the image. Each node is connected to each of the terminals. Each node is also connected to all other nodes corresponding to other respective non-disjoint pixel-clusters of the image that, in the image, are within a neighborhood of the first respective non-disjoint pixel-cluster of the image. Herein, a &#x201c;neighborhood&#x201d; of a cluster (e.g., a non-disjoint pixel-cluster) refers to a local region or the image that is centered at the cluster and is smaller than the full image. A &#x201c;neighborhood&#x201d; may be (a) all pixels of the image within a certain distance of a center location of the cluster, or (b) all pixels of the image within a square centered on the center location of the cluster, wherein the square has a certain side length.</p><p id="p-0064" num="0057"><figref idref="DRAWINGS">FIG. <b>7</b>A</figref> shows an example image <b>700</b> that has been clustered into non-disjoint pixel-clusters <b>710</b>(<i>i</i>). Each non-disjoint pixel-cluster <b>710</b>(<i>i</i>) has a center location <b>712</b>(<i>i</i>). For clarity of illustration, not all non-disjoint pixel-clusters are labeled in <figref idref="DRAWINGS">FIG. <b>7</b>A</figref>, and only some of center locations <b>712</b>(<i>i</i>), namely center locations <b>712</b>(<b>1</b>), <b>712</b>(<b>2</b>), . . . , <b>712</b>(<b>12</b>), are labeled in <figref idref="DRAWINGS">FIG. <b>7</b>A</figref>. <figref idref="DRAWINGS">FIG. <b>7</b>B</figref> is a portion of a graph <b>702</b> based on non-disjoint pixel-clusters <b>710</b>. Graph <b>702</b> includes a plurality of nodes <b>714</b>(<i>i</i>), also labeled &#x201c;&#x393;i&#x201d; . Each node <b>714</b>(<i>i</i>) corresponds to a respective center location <b>712</b>(<i>i</i>) and thus to a respective non-disjoint pixel-cluster <b>710</b>(<i>i</i>). For clarity of illustration, graph <b>702</b> shows only nodes <b>714</b>(<b>1</b>), <b>714</b>(<b>2</b>), . . . , <b>714</b>(<b>12</b>). It is understood that graph <b>702</b> includes a node <b>714</b> for each non-disjoint pixel-cluster <b>710</b> of image <b>700</b>. Graph <b>702</b> further includes two terminals <b>730</b>(<b>1</b>) and <b>730</b>(<b>2</b>), also labeled &#x201c;T1&#x201d; and &#x201c;T2&#x201d;.</p><p id="p-0065" num="0058">Since graph <b>702</b> has only two terminals, graph <b>702</b> is configured to do binary image segmentation. Without departing from the scope hereof, graph <b>702</b> may instead include more than two terminals to segment image <b>700</b> into more than two different categories.</p><p id="p-0066" num="0059">Each node &#x393;i is connected to terminals T1 and T2 via respective node-to-terminal connections <b>740</b>(<b>1</b>) (solid lines) and <b>740</b>(<b>2</b>) (dashed lines). Each node &#x393;i is further connected, via node-to-node connections <b>770</b> (dash-dot lines), to all other nodes &#x393;j that correspond to center locations <b>712</b>(<i>j</i>) within a neighborhood <b>720</b>(<i>i</i>) of the center location <b>712</b>(<i>i</i>) corresponding to node &#x393;i. For example, in <figref idref="DRAWINGS">FIG. <b>7</b>A</figref>, center location <b>712</b>(<b>7</b>) defines a neighborhood <b>720</b>(<b>7</b>). Only center locations <b>712</b>(<b>2</b>), <b>712</b>(<b>3</b>), <b>712</b>(<b>6</b>), and <b>712</b>(<b>11</b>) are within neighborhood <b>720</b>(<b>7</b>) of center location <b>712</b>(<b>7</b>). Therefore, node &#x393;7 is connected, via node-to-node connections <b>770</b>, only to nodes &#x393;2, &#x393;3, &#x393;6, and &#x393;11. In a similar manner, each of nodes &#x393;1, . . . , &#x393;6, and &#x393;8, . . . , &#x393;12 have node-to-node connections <b>770</b> to nodes that correspond to center locations <b>712</b> within the neighborhood of the center location corresponding to the node under consideration. However, for clarity of illustration, these node-to-node connections are not shown in <figref idref="DRAWINGS">FIG. <b>7</b>B</figref>. In one embodiment, node-to-terminal connections <b>740</b> are directional, such that each node-to-terminal connections <b>740</b>(<b>1</b>) is in the direction from terminal T1 to a node, and each node-to-terminal connections <b>740</b>(<b>2</b>) is in the direction from a node to terminal T2.</p><p id="p-0067" num="0060">Step <b>624</b> initializes the graph, generated in step <b>622</b>, by setting weights of connections of the graph at least partly according to a user input. The user input classifies some but not all of the non-disjoint pixel-clusters in two or more categories. Each category is associated with a respective terminal in the graph. Node-to-terminal connections for nodes that are directly classified by the user input are assigned weights according to the user input, and these weights serve as anchors for the graphs. Step <b>624</b> sets weights for other node-to-node connections according to a consideration of a form of similarity between non-disjoint pixel-clusters associated with the nodes. In one embodiment, step <b>624</b> includes a step <b>626</b> that considers color similarity between non-disjoint pixel-clusters when setting weights for node-to-node connections. The node-to-node connection weights determined based upon a similarity consideration can be viewed as defining strengths of a mesh between nodes, while each node-to-terminal connection weights, set directly according to the user input, anchors a respective point of this mesh more strongly to one terminal than any other terminal.</p><p id="p-0068" num="0061">In the example of graph <b>702</b>, the user input may (a) classify non-disjoint pixel-cluster <b>710</b>(<b>7</b>) in the category associated with terminal T1, (b) classify non-disjoint pixel-cluster <b>710</b>(<b>11</b>) in the category associated with terminal T2, and (c) leave non-disjoint pixel-clusters <b>710</b>(<b>1</b>), <b>710</b>(<b>2</b>), <b>710</b>(<b>3</b>), <b>710</b>(<b>4</b>), <b>710</b>(<b>5</b>), <b>710</b>(<b>6</b>), <b>710</b>(<b>8</b>), <b>710</b>(<b>9</b>), <b>710</b>(<b>10</b>), and <b>712</b>(<b>12</b>) unclassified. In this example, step <b>712</b> sets weights of node-to-terminal connections <b>740</b>(<b>1</b>) and <b>740</b>(<b>2</b>) for nodes &#x393;7 and &#x393;11 directly according to the user input. Step <b>712</b> then sets weights of node-to-node connections <b>770</b> based upon a similarity consideration, such as color similarity between non-disjoint pixel-clusters <b>710</b>(<i>i</i>) associated with nodes connected by node-to-node connections <b>770</b>.</p><p id="p-0069" num="0062">Step <b>628</b> segments the image into the categories by cutting the graph, generated in step <b>622</b>, based upon the weights defined in step <b>624</b>. More specifically, step <b>628</b> cuts connections of the graph, based upon the weights, such that each node is connected, directly or via one or more other nodes, to exactly one terminal. Step <b>628</b> may utilize a graph cutting algorithm known in the art, such as max-flow min-cut. In one example, step <b>628</b> cuts graph <b>702</b> such that each node <b>714</b> is connected, directly or via one or more other nodes <b>714</b>, to one but not both of terminals T1 and T2.</p><p id="p-0070" num="0063">In certain embodiments, method <b>600</b> is applied to a video stream and step <b>622</b> implements a step <b>623</b> to consider several sequential images of the video stream in conjunction with each other. Step <b>623</b> includes &#x201c;temporal connections&#x201d; to one or more preceding and/or subsequent frames of the video stream. The resulting graph includes (a) node-to-node connections <b>770</b> within each frame of the video stream, (b) node-to-terminal connections <b>740</b> for at least one frame of the video stream, and (c) temporal connections that connect nodes of each frame to nodes of at least one other frame so as to link a plurality of sequential frames to each other in the same graph. Step <b>712</b> then sets weights for all connections of the graph, including the temporal connections, and step <b>628</b> cuts the graph to simultaneously segment all frames under consideration. In one embodiment, the temporal connections formed in step <b>623</b> may be similar to the intra-frame node-to-node connections (e.g., node-to-node connections <b>770</b>). In this embodiment, temporal connections for any given node are limited to be extended to nodes of other frames that are within the neighborhood of the node under consideration.</p><p id="p-0071" num="0064"><figref idref="DRAWINGS">FIG. <b>7</b>C</figref> illustrates an example extension of graph <b>702</b> to a graph <b>704</b> that further includes temporal connections. Nodes &#x393;<sub>2</sub>i of a frame <b>760</b>(<b>2</b>) are (a) linked back in time (in direction <b>790</b>) via temporal connections <b>780</b> to nodes &#x393;<sub>1</sub>i of a preceding frame <b>760</b>(<b>1</b>) and (b) linked forward in time (in direction <b>792</b>) via other temporal connections <b>780</b> to nodes &#x393;<sub>3</sub>i of a subsequent frame <b>760</b>(<b>3</b>). Graph <b>704</b> thus links frames <b>760</b>(<b>1</b>), <b>760</b>(<b>2</b>), and <b>760</b>(<b>3</b>) in a single graph. Frame <b>760</b>(<b>2</b>) corresponds to image <b>700</b> of <figref idref="DRAWINGS">FIG. <b>7</b>A</figref>. The same set of terminals, T1 and T2, are used for all frames. For clarity of illustration, node-to-terminal connections <b>740</b> are not drawn in <figref idref="DRAWINGS">FIG. <b>7</b>C</figref>, intra-frame node-to-node connections <b>770</b> are drawn for nodes &#x393;<sub>1</sub>7, &#x393;<sub>2</sub>7, and &#x393;<sub>3</sub>7 only and some of these node-to-node connections are omitted, and temporal connections <b>780</b> are drawn for node &#x393;<sub>2</sub>7 only. It is further understood that graph <b>704</b> includes many more nodes than depicted in <figref idref="DRAWINGS">FIG. <b>7</b>C</figref>, to represent all non-disjoint pixel-clusters <b>710</b> of each frame <b>760</b>.</p><p id="p-0072" num="0065">Since the scene content of frames <b>760</b> may be dynamic, non-disjoint pixel-clusters <b>710</b> may shift, appear, or go away between one frame and the next. In the example depicted in <figref idref="DRAWINGS">FIG. <b>7</b>C</figref>, there is no node &#x393;<sub>1</sub>4 in frame <b>760</b>(<b>1</b>) because non-disjoint pixel-cluster <b>710</b> (<b>4</b>) of frame <b>760</b>(<b>2</b>) does not have a counterpart in frame <b>760</b>(<b>1</b>). Also, frame <b>760</b>(<b>3</b>) includes a node-to-node connection <b>770</b> between nodes &#x393;<sub>3</sub>7 and &#x393;<sub>3</sub>8 because, in frame <b>760</b>(<b>3</b>) node &#x393;<sub>3</sub>8 is within the neighborhood of node &#x393;<sub>3</sub>7. Based on the center locations of the non-disjoint pixel-clusters corresponding to the nodes depicted in <figref idref="DRAWINGS">FIG. <b>7</b>C</figref>, nodes &#x393;<sub>1</sub>3, &#x393;<sub>1</sub>6, and &#x393;<sub>1</sub>7, but not node &#x393;<sub>1</sub>2, are within the neighborhood of node &#x393;<sub>2 </sub>7 . Node &#x393;<sub>2</sub>7 is therefore connected to nodes &#x393;<sub>1</sub>3, &#x393;<sub>1</sub>6, and &#x393;<sub>1</sub>7, via temporal connections <b>780</b>, but not to node &#x393;<sub>1</sub>2 even though node &#x393;<sub>2</sub>7 is connected to node &#x393;<sub>2</sub>2 via intra-frame node-to-node connection <b>770</b>.</p><p id="p-0073" num="0066">In embodiments of method <b>600</b> that process a video stream and include step <b>623</b> to collectively consider a plurality of frames of the video stream, the user-indicated classification utilized in step <b>712</b> may pertain to a single one of the frames or pertain several frames. In one example, method <b>600</b> collectively considers three or more frames, but the user generates all user-indicated classifications from a single frame only. This saves the user from having to consider every single frame of the video stream, and these embodiments of method <b>600</b> may significantly reduce the amount of work required by the user.</p><p id="p-0074" num="0067"><figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates one computer <b>800</b> for user-guided image segmentation. Computer <b>800</b> is an embodiment of user-guided image segmentation engine <b>112</b> and may perform any one of methods <b>200</b>, <b>400</b>, and <b>600</b>. Computer <b>800</b> includes a processor <b>810</b>, a non-transitory memory <b>820</b>, and an interface <b>890</b>. Memory <b>820</b> includes machine-readable instructions <b>830</b> that are executable by processor <b>810</b>. Memory <b>820</b> may further include a data storage <b>880</b>. In operation, computer <b>800</b> receives image <b>180</b> (or a video stream of images <b>180</b>) and user input <b>188</b>. Guided by user input <b>188</b>, computer <b>800</b> segments image(s) <b>180</b> such that each pixel of each image <b>180</b> is classified in one of a plurality of categories. Instructions <b>830</b> include clustering instructions <b>840</b> and classification instructions <b>860</b>.</p><p id="p-0075" num="0068">When executed by processor <b>810</b>, clustering instructions <b>840</b> control processor <b>820</b> to perform step <b>210</b> of method <b>200</b>, based on image <b>180</b> to cluster pixels of image <b>180</b>. In one implementation, clustering instructions <b>840</b> command processor <b>820</b> to retrieve image <b>180</b> from data storage after that processor <b>820</b> has received image <b>180</b> via interface <b>890</b> and stored image <b>180</b> to data storage <b>880</b>. Clustering instructions <b>840</b> may command processor <b>820</b> to store a cluster map <b>882</b>, resulting from the performance of step <b>210</b>, in data storage <b>880</b>.</p><p id="p-0076" num="0069">When executed by processor <b>810</b>, classification instructions <b>860</b> control processor <b>820</b> to perform step <b>220</b> of method <b>200</b> according to user input <b>188</b>. Classification instructions <b>860</b> may command processor <b>820</b> to retrieve cluster map <b>882</b> from data storage <b>880</b>, to apply step <b>220</b> thereto. Classification instructions <b>860</b> may command processor <b>820</b> to store image segmentation <b>290</b>, resulting from the performance of step <b>410</b>, in data storage <b>880</b> or, alternatively, output image segmentation <b>290</b> via interface <b>890</b>.</p><p id="p-0077" num="0070">In certain embodiments, interface <b>890</b> includes a graphical user interface (GUI) <b>892</b>, and classification instructions <b>860</b> are configured to command processor <b>820</b> to display image segmentation <b>290</b> thereon for evaluation by a user. If image segmentation <b>290</b> is unsatisfactory, the user may modify user input <b>188</b> and initiate execution of classification instructions <b>860</b>, by processor <b>810</b>, to perform another iteration of step <b>220</b>.</p><p id="p-0078" num="0071">In an embodiment, clustering instructions <b>840</b> are configured to perform step <b>410</b> of method <b>400</b>. In this embodiment, clustering instructions <b>840</b> include k-means clustering instructions <b>842</b> and re-clustering instructions <b>850</b> that, when executed by processor <b>810</b>, control processor <b>820</b> to perform steps <b>412</b> and <b>414</b>, respectively, of method <b>400</b>. K-means clustering instructions <b>842</b> may command processor <b>820</b> to retrieve k-means clustering parameters <b>884</b> from data storage <b>880</b>, and perform step <b>410</b> according to these parameters. K-means clustering parameters <b>884</b> may include parameters K and w<sub>D</sub>, wherein K is an initial estimate of the number of clusters and w<sub>D </sub>is a weight that define a strength relationship between color similarity and location similarity. Parameters K and w<sub>D </sub>are discussed in further detail below in reference to <figref idref="DRAWINGS">FIG. <b>9</b></figref>. K-means clustering instructions <b>842</b> may command processor <b>820</b> to store cluster map <b>882</b> to data storage <b>880</b> after performing step <b>412</b> to generate a set of first clusters. Re-clustering instructions <b>850</b> may command processor <b>820</b> to retrieve, from data storage <b>880</b>, cluster map <b>882</b> as defined by the first clusters generated in step <b>412</b> and store a revised cluster map <b>882</b> of non-disjoint pixel-clusters to data storage <b>880</b> after performing step <b>414</b> to generate the non-disjoint pixel-clusters. Re-clustering instructions <b>850</b> may include one or both of morphological filter instructions <b>852</b> and connected-component-analysis instructions <b>854</b> (connected components are non-disjoint pixel clusters which are analyzed and &#x201c;connected&#x201d; with re-clustering), discussed in further detail below in reference to <figref idref="DRAWINGS">FIG. <b>11</b></figref>.</p><p id="p-0079" num="0072">In an embodiment, classification instructions <b>860</b> are configured to perform step <b>620</b> of method <b>600</b>. In this embodiment, classification instructions <b>860</b> include graph setup instructions <b>862</b>, graph initialization instructions <b>864</b>, and graph cutting instructions <b>866</b> that, when executed by processor <b>810</b>, command processor <b>820</b> to perform steps <b>622</b>, <b>624</b>, and <b>628</b>, respectively of method <b>600</b>. Graph setup instructions <b>862</b> may command processor <b>820</b> to retrieve cluster map <b>882</b> from data storage <b>880</b> and, after performing step <b>622</b>, store a resulting graph <b>885</b> to data storage <b>880</b>. Graph initialization instructions <b>864</b> may command processor <b>820</b> to retrieve graph <b>885</b> (or, alternatively, cluster map <b>882</b> ) and user input <b>188</b> from data storage and, after performing step <b>712</b> based on user input <b>188</b> and graph <b>885</b> (or cluster map <b>882</b>), store graph connection weights <b>888</b> to data storage <b>880</b>. Graph cutting instructions <b>866</b> may command processor <b>820</b> to retrieve graph <b>885</b> and graph connection weights <b>888</b> from data storage <b>880</b> and, after performing step <b>628</b>, store a resulting image segmentation <b>290</b> to data storage <b>880</b>. Graph cutting instructions <b>866</b> may further utilize graph cutting parameters <b>886</b>, stored in data storage <b>880</b>, and perform the graph cutting according to these parameters.</p><p id="p-0080" num="0073">One or more of graph setup instructions <b>862</b>, graph initialization instructions <b>864</b>, and graph cutting instructions <b>866</b> may further utilize graph parameters <b>886</b> stored in data storage <b>880</b>. In one example, graph setup instructions <b>862</b> utilize a parameter R,<sub>s</sub><sup>Gc </sup>that defines the size of the neighborhood of step <b>412</b> (e.g., the size of neighborhood <b>720</b> of <figref idref="DRAWINGS">FIG. <b>7</b>A</figref>). In another example, graph initialization instructions <b>864</b> utilize a parameter &#x3c9;<sub>0 </sub>that helps define the strength relationship between (a) node-to-terminal connections (e.g., node-to-terminal connections <b>740</b>(<b>1</b>) and <b>740</b>(<b>2</b>)) and (b) node-to-node connections (e.g., node-to-node connections <b>770</b>) and, if included in the graph, temporal connections (e.g., temporal connections <b>780</b>).</p><p id="p-0081" num="0074">Certain implementations of instructions <b>830</b> may include GUI instructions <b>870</b> that, when executed by processor <b>810</b>, display GUI <b>892</b> on a display such that a user may indicate, on GUI <b>892</b>, classification of certain non-disjoint pixel-clusters or image locations in a plurality of categories. GUI instructions <b>870</b> may command processor <b>820</b> to generate (a) an image panel for displaying image <b>180</b> optionally with cluster map <b>882</b> overlaid thereon, and (b) controls that allows a user to point to image locations depicted in the image panel (or point to clusters if cluster map <b>882</b> is displayed) to classify each such image location (or cluster) in a selected category. GUI instructions <b>870</b> may further command the processor to, after execution of classification instructions <b>860</b>, display the image with a classification map overlaid thereon. The classification map indicates spatial segmentation between the categories as defined by image segmentation <b>290</b>, for example in a manner similar to that shown in <figref idref="DRAWINGS">FIG. <b>3</b>C</figref>.</p><p id="p-0082" num="0075"><figref idref="DRAWINGS">FIG. <b>9</b></figref> illustrates one k-means based clustering method <b>900</b> for assigning each pixel of an image to one of a plurality of first clusters according to color and location of the pixel. Method <b>900</b> is an embodiment of step <b>412</b> of method <b>400</b>. Method <b>900</b> may be encoded in k-means clustering instructions <b>842</b> and generate cluster map <b>882</b>. <figref idref="DRAWINGS">FIG. <b>10</b></figref> shows an example that illustrates certain aspects of method <b>900</b>. <figref idref="DRAWINGS">FIGS. <b>9</b> and <b>10</b></figref> are best viewed together in the following description.</p><p id="p-0083" num="0076">Method <b>900</b> includes a step <b>920</b> that assigns each pixel of the image to a particular one of a set of first clusters. For each pixel, step <b>920</b> searches a local search region around the pixel and selects the first cluster that is a smallest &#x201c;cluster-distance&#x201d; from the pixel. Herein, a &#x201c;local search region around a pixel&#x201d; refers to a region of the image that is around the pixel while being smaller than the full image. For example, a local search region around a pixel may be limited to a certain distance from the pixel. The cluster-distance is a weighted combination of (a) color difference between the pixel and the first cluster and (b) location difference between the pixel and the first cluster. Step <b>920</b> then assigns the pixel to the selected first cluster. The search region may be a square region centered at the pixel under consideration. Step <b>920</b> may be preceded by a step <b>910</b> that initializes center locations of the first clusters to be arranged in a regular grid.</p><p id="p-0084" num="0077">In an embodiment, the cluster-distance between a p'th pixel and an l'th first cluster is calculated as</p><p id="p-0085" num="0000"><maths id="MATH-US-00003" num="00003"><math overflow="scroll"> <mrow>  <mrow>   <msubsup>    <mi>D</mi>    <mi>KC</mi>    <mrow>     <mo>{</mo>     <mrow>      <mi>p</mi>      <mo>,</mo>      <mi>l</mi>     </mrow>     <mo>}</mo>    </mrow>   </msubsup>   <mo>=</mo>   <mrow>    <msubsup>     <mi>D</mi>     <mi>color</mi>     <mrow>      <mo>{</mo>      <mrow>       <mi>p</mi>       <mo>&#x2062;</mo>       <mi>l</mi>      </mrow>      <mo>}</mo>     </mrow>    </msubsup>    <mo>+</mo>    <mrow>     <msubsup>      <mi>w</mi>      <mi>KC</mi>      <mi>d</mi>     </msubsup>     <mo>(</mo>     <mfrac>      <msubsup>       <mi>D</mi>       <mi>XY</mi>       <mrow>        <mo>{</mo>        <mrow>         <mi>p</mi>         <mo>,</mo>         <mi>l</mi>        </mrow>        <mo>}</mo>       </mrow>      </msubsup>      <msub>       <mi>S</mi>       <mrow>        <mi>K</mi>        <mo>&#x2062;</mo>        <mi>C</mi>       </mrow>      </msub>     </mfrac>     <mo>)</mo>    </mrow>   </mrow>  </mrow>  <mo>,</mo> </mrow></math></maths></p><p id="p-0086" num="0000">wherein D<sub>color</sub><sup>{p,l}</sup> is the <img id="CUSTOM-CHARACTER-00013" he="2.79mm" wi="2.12mm" file="US20230005243A1-20230105-P00008.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>2</sub>-norm of the color difference between the p'th pixel and the l'th first cluster, D<sub>XY</sub><sup>{p,l}</sup> is the <img id="CUSTOM-CHARACTER-00014" he="2.79mm" wi="2.12mm" file="US20230005243A1-20230105-P00009.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>2</sub>-norm of the location difference between the p'th pixel and the center location of the l'th first cluster, S<sub>KC </sub>is a characteristic center-to-center distance for adjacent first clusters, and w<sub>KC</sub><sup>d </sup>is a weight. w<sub>KC</sub><sup>d </sup>is an example of the parameter w<sub>D</sub>, discussed above in reference to <figref idref="DRAWINGS">FIG. <b>8</b></figref>. S<sub>KC </sub>may be derived from, or define, the parameter K discussed above in reference to <figref idref="DRAWINGS">FIG. <b>8</b></figref>. D<sub>XY</sub><sup>{p,l</sup>} may be calculated as</p><p id="p-0087" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>D</i><sub>XY</sub><sup>{p,l}</sup>=&#x221a;{square root over ((<o ostyle="single"><i>x</i></o><sub>(l)</sub>, &#x2212;<i>x</i><sub>p</sub>)<sup>2</sup>+(<o ostyle="single"><i>y</i></o><sub>(l)&#x2212;</sub><i>y</i><sub>p</sub>)<sup>2</sup>)},<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0088" num="0000">wherein <o ostyle="single">x</o><sub>(l) </sub>and <o ostyle="single">y</o><sub>(l) </sub>are the average horizontal and vertical coordinates, respectively, of the l'th first cluster, and x<sub>p </sub>and y<sub>p </sub>are the horizontal and vertical coordinates, respectively, of the p'th pixel D<sub>color</sub><sup>{p,l</sup>} may be calculated as</p><p id="p-0089" num="0000"><maths id="MATH-US-00004" num="00004"><math overflow="scroll"> <mrow>  <mrow>   <msubsup>    <mi>D</mi>    <mi>color</mi>    <mrow>     <mo>{</mo>     <mrow>      <mi>p</mi>      <mo>,</mo>      <mi>l</mi>     </mrow>     <mo>}</mo>    </mrow>   </msubsup>   <mo>=</mo>   <msqrt>    <mrow>     <munder>      <mo>&#x2211;</mo>      <mi>i</mi>     </munder>     <msup>      <mrow>       <mo>(</mo>       <mrow>        <msubsup>         <mover>          <mi>I</mi>          <mo>_</mo>         </mover>         <mrow>          <mo>(</mo>          <mi>l</mi>          <mo>)</mo>         </mrow>         <mrow>          <mo>(</mo>          <mi>i</mi>          <mo>)</mo>         </mrow>        </msubsup>        <mo>-</mo>        <msubsup>         <mover>          <mi>I</mi>          <mo>_</mo>         </mover>         <mi>p</mi>         <mrow>          <mo>(</mo>          <mi>i</mi>          <mo>)</mo>         </mrow>        </msubsup>       </mrow>       <mo>)</mo>      </mrow>      <mn>2</mn>     </msup>    </mrow>   </msqrt>  </mrow>  <mo>,</mo> </mrow></math></maths></p><p id="p-0090" num="0000">wherein &#x12a;<sub>(l)</sub><sup>(i) </sup>is the value of the normalized i'th color channel averaged over the l'th first cluster, and &#x12a;<sub>(p)</sub><sup>(i) </sup>is the value of the normalized i'th color channel of the p'th pixel. Normalized color channels are discussed above in reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>. Alternatively, D<sub>color</sub><sup>{p,l}</sup> may be based on un-normalized color channel values. The location and normalized color of the p'th pixel may be combined in a centroid &#x393;<sub>p</sub>=(&#x12a;<sub>(p)</sub><sup>(1)</sup>, &#x12a;<sub>(p)</sub><sup>(2)</sup>, &#x12a;<sub>(p)</sub><sup>(3)</sup>, x<sub>p</sub>, y<sub>p</sub>), assuming that there are three color channels. It is understood that the number of color channels may be different, for example four. The average location and normalized color of the l'th first cluster may be combined in a cluster centroid &#x393;<sub>(i)</sub>=(&#x12a;<sub>(l)</sub><sup>(1)</sup>, &#x12a;<sub>(l)</sub><sup>(2)</sup>, &#x12a;<sub>(l)</sub><sup>(3)</sup>, <o ostyle="single">x</o><sub>(l)</sub>, <o ostyle="single">y</o><sub>(l)</sub>), again assuming that there are three color channels.</p><p id="p-0091" num="0078"><figref idref="DRAWINGS">FIG. <b>10</b></figref> shows one example of steps <b>910</b> and <b>920</b>. In this example, a set of first clusters <b>1010</b>(<i>i,j</i>) have been initialized, in step <b>910</b>, such that their respective center locations <b>1012</b>(<i>i,j</i>) are on a regular grid relative to an image <b>1000</b>. Coordinates (<i>i,j</i>) indicate row and column numbers of the grid. For clarity of illustration, not all first clusters <b>1010</b> and not all center locations <b>1012</b> are labeled in <figref idref="DRAWINGS">FIG. <b>10</b></figref>. It is understood that cluster initialization does not require fully specifying the extent of each cluster <b>1010</b>. It is sufficient to initialize the center locations <b>1012</b> of clusters <b>1010</b>. The distance between adjacent center locations <b>1012</b> is S<sub>KC</sub>, such that each first cluster <b>1010</b> initially has side length S<sub>KC</sub>, except possibly for first clusters <b>1010</b> at the perimeter of image <b>1000</b> in situations where an image side length is not divisible by S<sub>KC</sub>. Step <b>920</b> considers each pixel <b>1020</b> of image <b>1000</b>. Only two pixels <b>1020</b>(<b>1</b>) and <b>1020</b>(<b>2</b>) are explicitly indicated in <figref idref="DRAWINGS">FIG. <b>10</b></figref>. For each pixel <b>1020</b>, step <b>920</b> searches a search region <b>1030</b> around the pixel. Search region <b>1030</b> is centered on pixel <b>1020</b> and is, in this example, a square with a side length 2S<sub>KC </sub>. For pixels <b>1020</b> near the perimeter of image <b>1000</b>, such as pixel <b>1020</b>(<b>2</b>), search region <b>1030</b> may be truncated. When step <b>920</b> has assigned all pixels <b>1020</b> of image <b>1000</b> to a first cluster <b>1010</b> the outlines of first clusters <b>1010</b> are most likely different from those of the initial first clusters. For example, after completion of step <b>920</b>, first cluster <b>1010</b> (<b>6</b>,<b>4</b>) may have changed to a modified shape <b>1010</b>&#x2032;(<b>6</b>,<b>4</b>), and its center location <b>1012</b>(<b>6</b>,<b>4</b>) may have shifted to a new center location <b>1012</b>&#x2032;(<b>6</b>,<b>4</b>). In another example, after completion of step <b>920</b>, first cluster <b>1010</b>( <b>6</b>,<b>8</b>) has changed to be in the shape of two separated areas <b>1010</b>&#x2032;(<b>6</b>,<b>8</b>).</p><p id="p-0092" num="0079">Method <b>900</b> may further include a step <b>930</b> that recalculates the cluster-centroid for each of the first clusters, as modified by step <b>920</b>. In one example, step <b>930</b> recalculates the cluster centroid for each cluster <b>1010</b>&#x2032;(<i>i,j</i>) as modified by step <b>920</b>. In certain embodiments, method <b>900</b> performs two or more iterations of steps <b>920</b> and <b>930</b>. In one such embodiment, method <b>900</b> is configured to perform a fixed number of iterations, for example between four and 15 iterations. In another such embodiment, method <b>900</b> keeps reiterating steps <b>920</b> and <b>930</b> until the number of pixel assignment changes in step <b>920</b> drops below a threshold number.</p><p id="p-0093" num="0080"><figref idref="DRAWINGS">FIG. <b>11</b></figref> illustrates one method <b>1100</b> for re-clustering a plurality of first clusters into a plurality of non-disjoint pixel-clusters. Method <b>1100</b> is an embodiment of step <b>414</b>. Method <b>1100</b> may be encoded in re-clustering instructions <b>850</b>. <figref idref="DRAWINGS">FIG. <b>12</b></figref> is an example that illustrates certain aspects of method <b>1100</b>. <figref idref="DRAWINGS">FIGS. <b>11</b> and <b>12</b></figref> are best viewed together in the following description.</p><p id="p-0094" num="0081">Method <b>1100</b> includes steps <b>1110</b> and <b>1120</b>. Step <b>1110</b> applies morphological filters to a map of the first clusters to enhance the connectivity of pixels within the first clusters. In one embodiment, step <b>1110</b> first applies a closing operation to the map in a step <b>1112</b>, and then applies an opening operation to the map in a step <b>1114</b>. Step <b>1112</b> may help fill in holes or small gaps in larger clusters. Step <b>1114</b> may help remove very small clusters or cluster portions. In one example of step <b>1110</b>, a closing operation and an opening operation are applied to cluster map <b>882</b>.</p><p id="p-0095" num="0082">Step <b>1120</b> analyzes non-disjoint pixel-clusters of the map of the first clusters, as processed by step <b>1110</b>. A first cluster of this map may or may not be a non-disjoint pixel-cluster. For example, a first cluster may be composed of two or more disjoint clusters that are not connected to each other. Step <b>1120</b> does not merely consider the first clusters of the map. Instead, step <b>1120</b> considers the non-disjoint pixel-clusters of the map.</p><p id="p-0096" num="0083">Step <b>1120</b> re-clusters only non-disjoint pixel-clusters of the first clusters of the map. Since re-clustering involves analyzing only non-disjoint pixel-clusters, computational efforts are reduced compared to conventional segmentation methods. Step <b>1120</b> may include steps <b>1122</b> and <b>1124</b>. For each non-disjoint pixel-cluster, in the map of the first clusters, having at least a threshold size (i.e., containing at least a threshold number of pixels), step <b>1122</b> assigns a new cluster label to the non-disjoint pixel-cluster. Step <b>1122</b> thus serves to keep each such non-disjoint pixel-cluster in the cluster map.</p><p id="p-0097" num="0084"><figref idref="DRAWINGS">FIG. <b>12</b></figref> shows an example cluster map <b>1200</b>, as having been processed by step <b>1110</b>. Cluster map <b>1200</b> is composed of (a) non-disjoint pixel-clusters <b>1212</b> that are at least of the threshold size and (b) non-disjoint pixel-clusters <b>1220</b> that are smaller than the threshold size. For clarity of illustration, not all non-disjoint pixel-clusters <b>1212</b> and <b>1220</b> are labeled in <figref idref="DRAWINGS">FIG. <b>12</b></figref>. Instead, non-disjoint pixel-clusters <b>1212</b> are white, and non-disjoint pixel-clusters <b>1220</b> are grey. Some of non-disjoint pixel-clusters <b>1212</b> may belong to the same first cluster. For example, non-disjoint pixel-clusters <b>1212</b>(<b>2</b>) and <b>1212</b>(<b>4</b>) may belong to the same first cluster while being separated from each other by a non-disjoint pixel-cluster <b>1212</b>(<b>3</b>) that constitutes its own first cluster. Step <b>1122</b> assigns a new cluster label to each non-disjoint pixel-cluster <b>1212</b>, regardless of whether or not this non-disjoint pixel-cluster <b>1212</b> constituted its own first cluster.</p><p id="p-0098" num="0085">For each non-disjoint pixel-cluster smaller than the threshold size, step <b>1124</b> merges the non-disjoint pixel-cluster into an adjacent non-disjoint pixel-cluster. For example, each non-disjoint pixel-cluster <b>1220</b> in <figref idref="DRAWINGS">FIG. <b>12</b></figref> is merged into an adjacent non-disjoint pixel-cluster <b>1212</b>. Step <b>1124</b> may be configured to abide to a particular merging direction. For example, step <b>1124</b> may raster through all non-disjoint pixel-clusters in a certain pattern (e.g., start at the upper left corner and scan right until reaching the right edge, then start at the left again directly beneath the previous pass, etc.) and merge each non-disjoint pixel-cluster that is smaller than the threshold into the most recent non-disjoint pixel-cluster, in the raster scan, that is at least of the threshold size. When method <b>1100</b> is completed, each new cluster label defines a respective non-disjoint pixel-cluster, and this set of non-disjoint pixel-clusters forms a new cluster map for the image.</p><p id="p-0099" num="0086">Step <b>1110</b> may be encoded in morphological filter instructions <b>852</b>, and step <b>1120</b> may be encoded in connected-non-disjoint pixel cluster-analysis instructions <b>854</b>. Method <b>1100</b> may be configured to refine cluster map <b>882</b>.</p><p id="p-0100" num="0087"><figref idref="DRAWINGS">FIG. <b>16</b>A</figref> illustrates a method <b>2100</b> of re-clustering a plurality of first clusters into a plurality of non-disjoint pixel-clusters. Method <b>2100</b> is an embodiment of step <b>414</b>. Method <b>2100</b> shows further details of method <b>1100</b> of <figref idref="DRAWINGS">FIG. <b>11</b></figref>. Method <b>2100</b> may be encoded in re-clustering instructions <b>850</b>. <figref idref="DRAWINGS">FIG. <b>16</b>B</figref> illustrates some aspects of the method of <figref idref="DRAWINGS">FIG. <b>16</b>A</figref>. <figref idref="DRAWINGS">FIGS. <b>16</b>A and <b>16</b>B</figref> are best viewed together in the following description.</p><p id="p-0101" num="0088">In step <b>2110</b> a binary mask for each cluster derived from the K-means clustering is obtained. This is done to process each cluster separately using morphological operations in subsequent step <b>2120</b>. Let M<sub>KC</sub><sup>(l)</sup>(x<sub>p</sub>,y<sub>p</sub>)be the mask of label l=1, 2, . . . L<sub>KC </sub>from the previous clustering step (K-means clusters map). Thus,</p><p id="p-0102" num="0000"><maths id="MATH-US-00005" num="00005"><math overflow="scroll"> <mrow>  <mrow>   <msubsup>    <mi>M</mi>    <mi>KC</mi>    <mrow>     <mo>(</mo>     <mi>l</mi>     <mo>)</mo>    </mrow>   </msubsup>   <mo>(</mo>   <mrow>    <msub>     <mi>x</mi>     <mi>p</mi>    </msub>    <mo>,</mo>    <msub>     <mi>y</mi>     <mi>p</mi>    </msub>   </mrow>   <mo>)</mo>  </mrow>  <mo>=</mo>  <mrow>   <mo>{</mo>   <mtable>    <mtr>     <mtd>      <mn>1</mn>     </mtd>     <mtd>      <mrow>       <mrow>        <mi>if</mi>        <mo>&#x2062;</mo>        <mtext>   </mtext>        <mrow>         <msubsup>          <mi>L</mi>          <mi>KC</mi>          <mi>map</mi>         </msubsup>         <mo>(</mo>         <mrow>          <msub>           <mi>x</mi>           <mi>p</mi>          </msub>          <mo>,</mo>          <msub>           <mi>y</mi>           <mi>p</mi>          </msub>         </mrow>         <mo>)</mo>        </mrow>       </mrow>       <mo>=</mo>       <mi>l</mi>      </mrow>     </mtd>    </mtr>    <mtr>     <mtd>      <mn>0</mn>     </mtd>     <mtd>      <mi>otherwise</mi>     </mtd>    </mtr>   </mtable>  </mrow> </mrow></math></maths></p><p id="p-0103" num="0089">In step <b>2120</b>, morphological filters are applied on M<sub>KC</sub><sup>(l)</sup>(<i>i,j</i>) to get rid of small discontinuities. In a non-limiting example, the closing operation of step <b>1112</b> of <figref idref="DRAWINGS">FIG. <b>11</b></figref> uses the imclose operation of MATLAB with square object. In a non-limiting example, step <b>1114</b> of <figref idref="DRAWINGS">FIG. <b>11</b></figref> may use the &#x201c;imopen&#x201d; function of MATLAB to perform opening operation to the map with the same square object. For each label l, the morphed mask</p><p id="p-0104" num="0000"><maths id="MATH-US-00006" num="00006"><math overflow="scroll"> <msubsup>  <mi>M</mi>  <mi>KC</mi>  <mrow>   <mrow>    <mo>(</mo>    <mi>l</mi>    <mo>)</mo>   </mrow>   <mo>,</mo>   <mi>&#x2133;</mi>  </mrow> </msubsup></math></maths></p><p id="p-0105" num="0000">is obtained:</p><p id="p-0106" num="0000"><maths id="MATH-US-00007" num="00007"><math overflow="scroll"> <mrow>  <mrow>   <msubsup>    <mi>M</mi>    <mi>KC</mi>    <mrow>     <mrow>      <mo>(</mo>      <mi>l</mi>      <mo>)</mo>     </mrow>     <mo>,</mo>     <mi>&#x2133;</mi>    </mrow>   </msubsup>   <mo>&#x2190;</mo>   <mtext> </mtext>   <mrow>    <mi>imopen</mi>    <mo>&#x2061;</mo>    <mo>(</mo>    <mrow>     <mrow>      <mi>imclose</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mrow>       <msubsup>        <mi>M</mi>        <mi>KC</mi>        <mrow>         <mo>(</mo>         <mi>l</mi>         <mo>)</mo>        </mrow>       </msubsup>       <mo>,</mo>       <msub>        <mi>O</mi>        <mi>morph</mi>       </msub>      </mrow>      <mo>)</mo>     </mrow>     <mo>,</mo>     <msub>      <mi>O</mi>      <mi>morph</mi>     </msub>    </mrow>    <mo>)</mo>   </mrow>  </mrow>  <mo>,</mo> </mrow></math></maths></p><p id="p-0107" num="0000">where O<sub>morph </sub>may be, e.g. a square, structural element. In an example, 5&#xd7;5 square structural elements for HD images (1920&#xd7;1080 pixels) may be used (see Table 1 below). In step <b>2130</b>, the cluster map is updated. Let</p><p id="p-0108" num="0000"><maths id="MATH-US-00008" num="00008"><math overflow="scroll"> <msubsup>  <mi>L</mi>  <mi>KC</mi>  <mrow>   <mi>map</mi>   <mo>,</mo>   <mi>&#x2102;</mi>  </mrow> </msubsup></math></maths></p><p id="p-0109" num="0000">be the new updated cluster-lapel map after morphological operations:</p><p id="p-0110" num="0000"><maths id="MATH-US-00009" num="00009"><math overflow="scroll"> <mrow>  <mrow>   <msubsup>    <mi>L</mi>    <mi>KC</mi>    <mrow>     <mi>map</mi>     <mo>,</mo>     <mi>&#x2102;</mi>    </mrow>   </msubsup>   <mo>(</mo>   <mrow>    <msub>     <mi>x</mi>     <mi>p</mi>    </msub>    <mo>,</mo>    <msub>     <mi>y</mi>     <mi>p</mi>    </msub>   </mrow>   <mo>)</mo>  </mrow>  <mo>=</mo>  <mrow>   <mo>{</mo>   <mtable>    <mtr>     <mtd>      <mi>l</mi>     </mtd>     <mtd>      <mrow>       <mrow>        <mi>if</mi>        <mo>&#x2062;</mo>        <mtext>   </mtext>        <mrow>         <msubsup>          <mi>M</mi>          <mi>KC</mi>          <mrow>           <mrow>            <mo>(</mo>            <mi>l</mi>            <mo>)</mo>           </mrow>           <mo>,</mo>           <mi>&#x2133;</mi>          </mrow>         </msubsup>         <mo>(</mo>         <mrow>          <msub>           <mi>x</mi>           <mi>p</mi>          </msub>          <mo>,</mo>          <msub>           <mi>y</mi>           <mi>p</mi>          </msub>         </mrow>         <mo>)</mo>        </mrow>       </mrow>       <mo>=</mo>       <mn>1</mn>      </mrow>     </mtd>    </mtr>    <mtr>     <mtd>      <mn>0</mn>     </mtd>     <mtd>      <mi>otherwise</mi>     </mtd>    </mtr>   </mtable>  </mrow> </mrow></math></maths></p><p id="p-0111" num="0000">Due to these operations, a new label 0 is introduced. Thus, the mask M<sub>KC</sub><sup>(0),CC </sup>is derived as:</p><p id="p-0112" num="0000"><maths id="MATH-US-00010" num="00010"><math overflow="scroll"> <mrow>  <mrow>   <msubsup>    <mi>M</mi>    <mi>KC</mi>    <mrow>     <mrow>      <mo>(</mo>      <mn>0</mn>      <mo>)</mo>     </mrow>     <mo>,</mo>     <mi>&#x2133;</mi>    </mrow>   </msubsup>   <mo>(</mo>   <mrow>    <msub>     <mi>x</mi>     <mi>p</mi>    </msub>    <mo>,</mo>    <msub>     <mi>y</mi>     <mi>p</mi>    </msub>   </mrow>   <mo>)</mo>  </mrow>  <mo>=</mo>  <mrow>   <mo>{</mo>   <mtable>    <mtr>     <mtd>      <mn>1</mn>     </mtd>     <mtd>      <mrow>       <mi>if</mi>       <mo>&#x2062;</mo>       <mtext>   </mtext>       <mrow>        <mo>(</mo>        <mrow>         <mrow>          <msubsup>           <mi>L</mi>           <mi>KC</mi>           <mrow>            <mi>map</mi>            <mo>,</mo>            <mi>&#x2102;</mi>           </mrow>          </msubsup>          <mo>(</mo>          <mrow>           <msub>            <mi>x</mi>            <mi>p</mi>           </msub>           <mo>,</mo>           <msub>            <mi>y</mi>            <mi>p</mi>           </msub>          </mrow>          <mo>)</mo>         </mrow>         <mo>=</mo>         <mn>0</mn>        </mrow>        <mo>)</mo>       </mrow>      </mrow>     </mtd>    </mtr>    <mtr>     <mtd>      <mn>0</mn>     </mtd>     <mtd>      <mi>otherwise</mi>     </mtd>    </mtr>   </mtable>  </mrow> </mrow></math></maths></p><p id="p-0113" num="0090">Each label l=0, 1, 2, . . . L<sub>KC</sub>, in <img id="CUSTOM-CHARACTER-00015" he="2.79mm" wi="3.89mm" file="US20230005243A1-20230105-P00010.TIF" alt="custom-character" img-content="character" img-format="tif"/>is composed of several non-disjoint pixel clusters. In order to have connected regions, in step <b>2140</b> a new label for each non-disjoint pixel cluster may be introduced. Alternatively, in step <b>2140</b> smaller non-disjoint pixel clusters may be merged to the close-by regions. For example, a raster scan may be performed through all non-disjoint pixel clusters of the updated cluster map. Each non-disjoint pixel cluster that is smaller than a threshold number of pixels may be merged into the most recent non-disjoint cluster, in the raster scan, that has at least the threshold number of pixels. In an example, the bwlabel function in MATLAB may be used to find non-disjoint pixel-cluster mask for each label. For each label, several connected non-disjoint pixel clusters may be obtained, each specified by an index.</p><p id="p-0114" num="0091">Let <img id="CUSTOM-CHARACTER-00016" he="2.79mm" wi="1.78mm" file="US20230005243A1-20230105-P00011.TIF" alt="custom-character" img-content="character" img-format="tif"/><sup>(l) </sup>be the map of non-disjoint pixel clusters for l'th region, such that <img id="CUSTOM-CHARACTER-00017" he="2.79mm" wi="1.78mm" file="US20230005243A1-20230105-P00011.TIF" alt="custom-character" img-content="character" img-format="tif"/>(<sup>(l)</sup>)(x<sub>p</sub>,y<sub>p</sub>) represent non-disjoint pixel cluster index at pixel (x<sub>p</sub>,y<sub>p</sub>) for l'th region. There can be one or many such non-disjoint pixel clusters in one region indexed as c=1, 2, 3 . . . <img id="CUSTOM-CHARACTER-00018" he="2.79mm" wi="2.79mm" file="US20230005243A1-20230105-P00012.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>(l)</sub>), where <img id="CUSTOM-CHARACTER-00019" he="2.79mm" wi="2.79mm" file="US20230005243A1-20230105-P00013.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>(l) </sub>is the number of non-disjoint pixel clusters in the l'th region. So, we have <img id="CUSTOM-CHARACTER-00020" he="2.79mm" wi="1.78mm" file="US20230005243A1-20230105-P00014.TIF" alt="custom-character" img-content="character" img-format="tif"/><sup>(l)</sup>(x<sub>p</sub>,y<sub>p</sub>)=c.</p><p id="p-0115" num="0092">Let the c'th non-disjoint pixel cluster of l'th region be <img id="CUSTOM-CHARACTER-00021" he="2.79mm" wi="2.12mm" file="US20230005243A1-20230105-P00003.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>(l)</sub><sup>(c)</sup>. Let <img id="CUSTOM-CHARACTER-00022" he="2.79mm" wi="2.12mm" file="US20230005243A1-20230105-P00003.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>(l)</sub><sup>(c) </sup>contain <img id="CUSTOM-CHARACTER-00023" he="2.79mm" wi="3.22mm" file="US20230005243A1-20230105-P00015.TIF" alt="custom-character" img-content="character" img-format="tif"/>pixels. For example, a l=4 region can have total <img id="CUSTOM-CHARACTER-00024" he="2.79mm" wi="2.79mm" file="US20230005243A1-20230105-P00016.TIF" alt="custom-character" img-content="character" img-format="tif"/><sup>(4)</sup>=5 non-disjoint pixel-clusters within it. Each of these non-disjoint pixel-clusters c=1, 2, . . . , 5 will be called as <img id="CUSTOM-CHARACTER-00025" he="2.79mm" wi="2.12mm" file="US20230005243A1-20230105-P00003.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>(4)</sub><sup>(1)</sup>, <img id="CUSTOM-CHARACTER-00026" he="2.79mm" wi="2.12mm" file="US20230005243A1-20230105-P00003.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>(4)</sub><sup>(2)</sup>, . . . , <img id="CUSTOM-CHARACTER-00027" he="2.79mm" wi="2.12mm" file="US20230005243A1-20230105-P00003.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>(4)</sub><sup>(5) </sup>respectively and contains <img id="CUSTOM-CHARACTER-00028" he="2.79mm" wi="3.22mm" file="US20230005243A1-20230105-P00017.TIF" alt="custom-character" img-content="character" img-format="tif"/>, <img id="CUSTOM-CHARACTER-00029" he="2.79mm" wi="2.79mm" file="US20230005243A1-20230105-P00018.TIF" alt="custom-character" img-content="character" img-format="tif"/>, . . . , <img id="CUSTOM-CHARACTER-00030" he="2.46mm" wi="2.79mm" file="US20230005243A1-20230105-P00019.TIF" alt="custom-character" img-content="character" img-format="tif"/>number of connected pixels. Let x<sub>(l)</sub><sup>(c),min</sup>, x<sub>(l)</sub><sup>(c),max </sup>be the minimum and maximum X-coordinates of <img id="CUSTOM-CHARACTER-00031" he="2.79mm" wi="2.12mm" file="US20230005243A1-20230105-P00003.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>(l)</sub><sup>(c)</sup>. Let y<sub>(l)</sub><sup>(c),min</sup>, y<sub>(l)</sub><sup>(c),max </sup>be the minimum and maximum Y-coordinates of <img id="CUSTOM-CHARACTER-00032" he="2.79mm" wi="2.12mm" file="US20230005243A1-20230105-P00003.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>(l)</sub><sup>(c)</sup>.</p><p id="p-0116" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>[X<sub>(l)</sub><sup>min</sup>,Y<sub>(l)</sub><sup>min</sup>]=[x<sub>(l)</sub><sup>(c),min</sup>,y<sub>(l)</sub><sup>(c),min</sup><img id="CUSTOM-CHARACTER-00033" he="2.12mm" wi="5.33mm" file="US20230005243A1-20230105-P00020.TIF" alt="custom-character" img-content="character" img-format="tif"/><sup/><?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0117" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>[X<sub>(l)</sub><sup>max</sup>,Y<sub>(l)</sub><sup>max</sup>]=[x<sub>(l)</sub><sup>(c),max</sup>,y<sub>(l)</sub><sup>(c),max</sup><img id="CUSTOM-CHARACTER-00034" he="2.46mm" wi="5.33mm" file="US20230005243A1-20230105-P00021.TIF" alt="custom-character" img-content="character" img-format="tif"/><sup/><?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0118" num="0093">Let &#x2018;bwlabel&#x2019; be a function that gives out the non-disjoint pixel-cluster map with its min/max properties. The &#x2018;bwlabel&#x2019; function used in this example has a 4-point connectivity neighborhood to get connected non-disjoint pixel clusters labeled as c=1,2,3, . . . , <img id="CUSTOM-CHARACTER-00035" he="2.46mm" wi="2.79mm" file="US20230005243A1-20230105-P00022.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>(l)</sub>.</p><p id="p-0119" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>{<img id="CUSTOM-CHARACTER-00036" he="2.79mm" wi="1.44mm" file="US20230005243A1-20230105-P00023.TIF" alt="custom-character" img-content="character" img-format="tif"/><sup>(l)</sup><img id="CUSTOM-CHARACTER-00037" he="2.46mm" wi="2.79mm" file="US20230005243A1-20230105-P00024.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>(l)</sub>,[X<sub>(l)</sub><sup>min</sup>,Y<sub>(l)</sub><sup>min</sup>],[X<sub>(l)</sub><sup>max</sup>,Y<sub>(l)</sub><sup>max</sup>]}=bwlabel ( <img id="CUSTOM-CHARACTER-00038" he="2.79mm" wi="3.89mm" file="US20230005243A1-20230105-P00025.TIF" alt="custom-character" img-content="character" img-format="tif"/>)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0120" num="0000">Thus, (x<sub>(l)</sub><sup>(c),min</sup>,y<sub>(l)</sub><sup>(c),min</sup>) is pixel <b>2210</b> in the example of <figref idref="DRAWINGS">FIG. <b>16</b>B</figref> with minimum X- and Y-coordinate from <img id="CUSTOM-CHARACTER-00039" he="2.79mm" wi="2.12mm" file="US20230005243A1-20230105-P00003.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>(l)</sub><sup>(c)</sup>. Pixel <b>2210</b> may or may not belong to <img id="CUSTOM-CHARACTER-00040" he="2.79mm" wi="2.12mm" file="US20230005243A1-20230105-P00003.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>(l)</sub><sup>(c)</sup>, since we take minimum X and Y-coordinate of <img id="CUSTOM-CHARACTER-00041" he="2.79mm" wi="2.12mm" file="US20230005243A1-20230105-P00003.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>(l)</sub><sup>(c) </sup>separately to get the top-left corner of a bounding box <b>2200</b> which surrounds <img id="CUSTOM-CHARACTER-00042" he="2.79mm" wi="2.12mm" file="US20230005243A1-20230105-P00003.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>(l)</sub><sup>(c) </sup>(see example of <figref idref="DRAWINGS">FIG. <b>16</b>B</figref>). The bottom right corner of the bounding box <b>2200</b> is then defined by ( X <sub>(l)</sub><sup>(c)max</sup>,Y<sub>(l)</sub><sup>(c)max</sup>) pixel, i.e. pixel <b>2230</b> in the example of <figref idref="DRAWINGS">FIG. <b>16</b>B</figref>.</p><p id="p-0121" num="0094">If there are more than one non-disjoint pixel-clusters in a region i.e. <img id="CUSTOM-CHARACTER-00043" he="2.46mm" wi="2.79mm" file="US20230005243A1-20230105-P00026.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>(l)</sub>&#x3e;1 (just like the example above with <img id="CUSTOM-CHARACTER-00044" he="2.46mm" wi="2.79mm" file="US20230005243A1-20230105-P00027.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>(4)</sub>=5), each &#x201c;sizable&#x201d; non-disjoint pixel cluster may be assigned a separate label. Whether a non-disjoint pixel cluster is sizable or not may be determined by a simple threshold T<sub>KC </sub>on the number of pixels in that non-disjoint pixel clusters. If <img id="CUSTOM-CHARACTER-00045" he="3.22mm" wi="3.22mm" file="US20230005243A1-20230105-P00028.TIF" alt="custom-character" img-content="character" img-format="tif"/>&#x3c;T<sub>KC</sub>, that corresponding &#x3a9;<sub>(l)</sub><sup>(c) </sup>may be too small to be considered. A typical value of T<sub>KC </sub>may be 50 to 100 pixels but other threshold values may also be used, e.g. lower than 50 pixels or higher than 100 pixels. Such smaller non-disjoint pixel cluster may be merged to a nearby non-disjoint pixel clusters. For example, such smaller non-disjoint pixel clusters may be merged to the non-disjoint pixel clusters at the top-left location as a convention. If ( x<sub>(l)</sub><sup>(c),min</sup>, y<sub>(l)</sub><sup>(c),min</sup>)=(0,0), a separate label may simply be assigned. If(x(x<sub>(l)</sub><sup>(c),min</sup>,Y<sub>(l)</sub><sup>(c),min</sup>)&#x2260;(0,0), the non-disjoint pixel clusters cluster is assigned its top-left neighbor's label. Let (x<sub>(l)</sub><sup>(c)TL</sup>,y<sub>(l)</sub><sup>(c),TL</sup>) be the top-left, respective to the bounding box <b>2200</b> of <img id="CUSTOM-CHARACTER-00046" he="2.79mm" wi="2.12mm" file="US20230005243A1-20230105-P00003.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>(l)</sub><sup>(c)</sup>, neighboring pixel <b>2220</b>. Note that the pixel (x<sub>(l)</sub><sup>(c)TL</sup>,y<sub>(l)</sub><sup>(c),TL</sup>), <b>2220</b> belongs to a different non-disjoint pixel clusters and it is not part of <img id="CUSTOM-CHARACTER-00047" he="2.79mm" wi="2.12mm" file="US20230005243A1-20230105-P00003.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>(l)</sub><sup>(c)</sup>. This non-disjoint pixel cluster may be equal to (0,0). Pixel (x<sub>(l)</sub><sup>(c)TL</sup>,Y<sub>(l)</sub><sup>(c),TL</sup>) may be computed based on:</p><p id="p-0122" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>(<i>x</i><sub>(l)</sub><sup>(c)TL</sup><i>,y</i><sub>(l)</sub><sup>(c),TL</sup>)=(max{(x<sup>(l)</sup><sub>(c),min&#x2212;</sub>1),0}, max{(<i>y</i><sub>(l)</sub><sup>(c), min</sup>&#x2212;1),0})<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0123" num="0095">Based on the top-left neighboring pixel of the bounding box <b>2200</b> of <img id="CUSTOM-CHARACTER-00048" he="2.79mm" wi="2.12mm" file="US20230005243A1-20230105-P00003.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>(l)</sub><sup>(c)</sup>, the entire small non-disjoint pixel cluster may be merged to its top-left neighbor. Considering the small number of pixels in the non-disjoint pixel cluster, the bounding box <b>2200</b> may be a practical and reasonable assumption. Since relatively small non-disjoint pixel clusters are not analyzed but just merged to a nearby larger non-disjoint pixel clusters, computational resources are considerably reduced. Thus,</p><p id="p-0124" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><img id="CUSTOM-CHARACTER-00049" he="2.46mm" wi="4.57mm" file="US20230005243A1-20230105-P00029.TIF" alt="custom-character" img-content="character" img-format="tif"/>(<i>x</i><sub>p</sub><i>,y</i><sub>p</sub>)=<img id="CUSTOM-CHARACTER-00050" he="2.46mm" wi="4.57mm" file="US20230005243A1-20230105-P00030.TIF" alt="custom-character" img-content="character" img-format="tif"/>(<i>x</i><sub>(l)</sub><sup>(c)TL</sup><i>,y</i><sub>(l)</sub><sup>(c)TL</sup>)&#x2200;{(<i>x</i><sub>p</sub><i>,y</i><sub>p</sub>):<img id="CUSTOM-CHARACTER-00051" he="2.79mm" wi="1.44mm" file="US20230005243A1-20230105-P00031.TIF" alt="custom-character" img-content="character" img-format="tif"/><sup>(l)</sup>(<i>x</i><sub>p</sub><i>,y</i><sub>p</sub>)=<i>c</i>, where <img id="CUSTOM-CHARACTER-00052" he="3.22mm" wi="3.22mm" file="US20230005243A1-20230105-P00032.TIF" alt="custom-character" img-content="character" img-format="tif"/>&#x3c;T<sub>KC</sub>}<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0125" num="0096">Sizable non-disjoint pixel clusters (e.g., with more pixels than T<sub>KC</sub>) may be considered large enough so that a new label for each pixel in that non-disjoint pixel cluster may be introduced.</p><p id="p-0126" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><img id="CUSTOM-CHARACTER-00053" he="2.46mm" wi="4.57mm" file="US20230005243A1-20230105-P00033.TIF" alt="custom-character" img-content="character" img-format="tif"/>(<i>x</i><sub>p</sub><i>,y</i><sub>p</sub>)=nl for all {(<i>x</i><sub>p</sub><i>,y</i><sub>p</sub>):<img id="CUSTOM-CHARACTER-00054" he="2.79mm" wi="1.44mm" file="US20230005243A1-20230105-P00034.TIF" alt="custom-character" img-content="character" img-format="tif"/><sup>(l)</sup>(<i>x</i><sub>p</sub><i>,y</i><sub>p</sub>)=<i>c</i>, where <img id="CUSTOM-CHARACTER-00055" he="3.22mm" wi="3.22mm" file="US20230005243A1-20230105-P00035.TIF" alt="custom-character" img-content="character" img-format="tif"/>&#x2265;<i>T</i><sub>KC</sub>}<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0127" num="0000">Here, nl is the new label added to the map. The counter nl is incremented as the new labels are added to the map. Note that the labeled may be reassigned due to non-disjoint pixel-cluster analysis. Let <img id="CUSTOM-CHARACTER-00056" he="2.79mm" wi="2.79mm" file="US20230005243A1-20230105-P00036.TIF" alt="custom-character" img-content="character" img-format="tif"/> be the new number of labels.</p><p id="p-0128" num="0097">The following Table 1 includes pseudo-code which summarizes the label analysis algorithm. Note that applying morphological operations <b>2120</b> and non-disjoint pixel-clusters analysis <b>2140</b> may be combined into one function and in one-loop. Alternatively, steps <b>2120</b> and <b>2140</b> may be performed by separate self-contained functions. There can be multiple alternative ways to use/reuse <img id="CUSTOM-CHARACTER-00057" he="2.46mm" wi="3.89mm" file="US20230005243A1-20230105-P00037.TIF" alt="custom-character" img-content="character" img-format="tif"/>, M<sub>KC</sub><sup>(.)</sup>,<img id="CUSTOM-CHARACTER-00058" he="2.46mm" wi="3.56mm" file="US20230005243A1-20230105-P00038.TIF" alt="custom-character" img-content="character" img-format="tif"/> 2D arrays. It depends on space/coding complexity vs time. In Table 1 temporary masks are used: M<sub>KC</sub><sup>(Temp)</sup>,<img id="CUSTOM-CHARACTER-00059" he="2.46mm" wi="5.33mm" file="US20230005243A1-20230105-P00039.TIF" alt="custom-character" img-content="character" img-format="tif"/>.</p><p id="p-0129" num="0098">An example implementation is given below in Table 1 where a space-constrained implementation is used.</p><p id="p-0130" num="0000"><tables id="TABLE-US-00001" num="00001"><table frame="none" colsep="0" rowsep="0" pgwide="1"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="259pt" align="center"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE 1</entry></row><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>Connectivity analysis</entry></row><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="259pt" align="left"/><tbody valign="top"><row><entry>// Label analysis algorithm to get connected regions</entry></row><row><entry>Input: L<sub>KC</sub><sup>map</sup></entry></row><row><entry>Set minimum pixels in region threshold value T<sub>KC</sub></entry></row><row><entry>Initialize&#x2009;<img id="CUSTOM-CHARACTER-00060" he="2.46mm" wi="5.33mm" file="US20230005243A1-20230105-P00040.TIF" alt="custom-character" img-content="character" img-format="tif"/> &#x2009;, M<sub>KC</sub><sup>(temp)</sup>,&#x2009;<img id="CUSTOM-CHARACTER-00061" he="2.46mm" wi="7.45mm" file="US20230005243A1-20230105-P00041.TIF" alt="custom-character" img-content="character" img-format="tif"/> &#x2009;zero-arrays of size W &#xd7; H</entry></row><row><entry>// set the morphological analysis object [7]</entry></row><row><entry>O<sub>morph</sub>= (5 &#xd7; 5) square&#x2003;// for HD image</entry></row><row><entry>// Get the binary masks from L<sub>KC</sub><sup>map </sup>for l = 1,2, ... , L<sub>KC </sub>&#x26; do morphological processing</entry></row><row><entry>for (l = 1; l &#x2264; L<sub>KC</sub>; l + +) {</entry></row><row><entry>&#x2003;for (x<sub>p </sub>= 0; x<sub>p </sub>&#x3c; W; x<sub>p </sub>+ +) {</entry></row><row><entry>&#x2003;&#x2003;for (y<sub>p </sub>= 0; y<sub>p </sub>&#x3c; H; y<sub>p </sub>+ +) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;l = L<sub>KC</sub><sup>map</sup>(x<sub>p</sub>, y<sub>p</sub>)&#x2003;// get label l</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;M<sub>KC</sub><sup>(temp)</sup>(x<sub>p</sub>, y<sub>p</sub>) = 1&#x2003;// assign binary mask</entry></row><row><entry>&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;}</entry></row><row><entry>&#x2003;&#x2009;<img id="CUSTOM-CHARACTER-00062" he="2.46mm" wi="7.45mm" file="US20230005243A1-20230105-P00041.TIF" alt="custom-character" img-content="character" img-format="tif"/> &#x2009;&#x2190; imopen (imclose( M<sub>KC</sub><sup>(l)</sup>, O<sub>morph</sub>), O<sub>morph</sub>) // apply open and close</entry></row><row><entry>&#x2003;for (x<sub>p </sub>= 0; x<sub>p </sub>&#x3c; W; x<sub>p </sub>+ +) {</entry></row><row><entry>&#x2003;&#x2003;for (y<sub>p </sub>= 0; y<sub>p </sub>&#x3c; H; y<sub>p </sub>+ +) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;if&#x2009;<img id="CUSTOM-CHARACTER-00063" he="2.46mm" wi="7.45mm" file="US20230005243A1-20230105-P00041.TIF" alt="custom-character" img-content="character" img-format="tif"/> &#x2009;(x<sub>p</sub>, y<sub>p</sub>) = 1</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;L<sub>KC</sub><sup>map</sup>(x<sub>p</sub>, y<sub>p</sub>) = l&#x2003;// reassign label l</entry></row><row><entry>&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;}</entry></row><row><entry>} // all labels reassigned</entry></row><row><entry>// Now, L<sub>KC</sub><sup>map </sup>contains labels l = 0,1,2, ... , L<sub>KC</sub></entry></row><row><entry>// Non-disjoint pixel-cluster analysis (using MATLAB function)</entry></row><row><entry>nl = 1&#x2003;// new label count</entry></row><row><entry>for (l = 0; l &#x2264; L<sub>KC</sub>; l + +) {</entry></row><row><entry>&#x2003;for (x<sub>p </sub>= 0; x<sub>p </sub>&#x3c; W; x<sub>p </sub>+ +) {</entry></row><row><entry>&#x2003;&#x2003;for (y<sub>p </sub>= 0; y<sub>p </sub>&#x3c; H; y<sub>p </sub>+ +) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;if (L<sub>KC</sub><sup>map</sup>(x<sub>p</sub>, y<sub>p</sub>) == l)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2009;<img id="CUSTOM-CHARACTER-00064" he="2.46mm" wi="7.45mm" file="US20230005243A1-20230105-P00041.TIF" alt="custom-character" img-content="character" img-format="tif"/> &#x2009;(x<sub>p</sub>, y<sub>p</sub>) = 1&#x2003;// assign binary mask</entry></row><row><entry>&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;}</entry></row><row><entry>&#x2003;// get the non-disjoint pixel-cluster&#x2009;<img id="CUSTOM-CHARACTER-00065" he="2.46mm" wi="1.78mm" file="US20230005243A1-20230105-P00042.TIF" alt="custom-character" img-content="character" img-format="tif"/> &#x2009;<sup>(l) </sup>,&#x2009;<img id="CUSTOM-CHARACTER-00066" he="2.46mm" wi="2.79mm" file="US20230005243A1-20230105-P00043.TIF" alt="custom-character" img-content="character" img-format="tif"/> &#x2009;<sub>(l) </sub>and&#x2009;<img id="CUSTOM-CHARACTER-00067" he="2.46mm" wi="2.79mm" file="US20230005243A1-20230105-P00044.TIF" alt="custom-character" img-content="character" img-format="tif"/> &#x2009;for each l</entry></row><row><entry>&#x2003;{&#x2009;<img id="CUSTOM-CHARACTER-00068" he="2.46mm" wi="1.78mm" file="US20230005243A1-20230105-P00042.TIF" alt="custom-character" img-content="character" img-format="tif"/> &#x2009;<sup>(l)</sup>, N&#x2009;<img id="CUSTOM-CHARACTER-00069" he="2.46mm" wi="1.78mm" file="US20230005243A1-20230105-P00042.TIF" alt="custom-character" img-content="character" img-format="tif"/> &#x2009;<sub>(l)</sub>, [X<sub>(l)</sub><sup>min</sup>, Y<sub>(l)</sub><sup>min</sup>], [X<sub>(l)</sub><sup>max</sup>, Y<sub>(l)</sub><sup>max</sup>]} = bwlabel (&#x2009;<img id="CUSTOM-CHARACTER-00070" he="2.46mm" wi="7.45mm" file="US20230005243A1-20230105-P00041.TIF" alt="custom-character" img-content="character" img-format="tif"/> &#x2009;)</entry></row><row><entry>&#x2003;// check for all the non-disjoint pixel-clusters</entry></row><row><entry>&#x2003;for (c = 1, c &#x2264;&#x2009;<img id="CUSTOM-CHARACTER-00071" he="2.46mm" wi="2.79mm" file="US20230005243A1-20230105-P00043.TIF" alt="custom-character" img-content="character" img-format="tif"/> &#x2009;<sub>(l)</sub>, c + +){</entry></row><row><entry>&#x2003;&#x2003;// small non-disjoint pixel-cluster and its top-left neighbor exists</entry></row><row><entry>&#x2003;&#x2003;if (&#x2009;<img id="CUSTOM-CHARACTER-00072" he="2.46mm" wi="2.79mm" file="US20230005243A1-20230105-P00045.TIF" alt="custom-character" img-content="character" img-format="tif"/> &#x2009;&#x3c; T<sub>KC</sub>) &#x26;&#x26; ((x<sub>(l)</sub><sup>(c),min</sup>, y<sub>(l)</sub><sup>(c),min</sup>)! = (0,0) ) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;// small non-disjoint pixel-cluster: merge to the label for the value outside top</entry></row><row><entry>left&#x2003;(x<sub>(l)</sub><sup>(c)TL</sup>, y<sub>(l)</sub><sup>(c),TL</sup>) = (max {(x<sub>(l)</sub><sup>(c),min </sup>&#x2212;</entry></row><row><entry>1), 0}, max{(y<sub>(l)</sub><sup>(c),min </sup>&#x2212; 1), 0})</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;for (x<sub>p </sub>= x<sub>(l)</sub><sup>(c),min</sup>; x<sub>p </sub>&#x2264; x<sub>(l)</sub><sup>(c),max</sup>; x<sub>p </sub>+ +) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;for (y<sub>p </sub>= y<sub>(l)</sub><sup>(c),min</sup>; y<sub>p </sub>&#x2264; y<sub>(l)</sub><sup>(c),max</sup>; y<sub>p </sub>+ +) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;if (&#x2009;<img id="CUSTOM-CHARACTER-00073" he="2.46mm" wi="1.78mm" file="US20230005243A1-20230105-P00046.TIF" alt="custom-character" img-content="character" img-format="tif"/> &#x2009;<sup>(l)</sup>(x<sub>p</sub>, y<sub>p</sub>) == c)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2009;<img id="CUSTOM-CHARACTER-00074" he="2.46mm" wi="5.33mm" file="US20230005243A1-20230105-P00047.TIF" alt="custom-character" img-content="character" img-format="tif"/> &#x2009;(x<sub>p</sub>, y<sub>p</sub>) =&#x2009;<img id="CUSTOM-CHARACTER-00075" he="2.46mm" wi="5.33mm" file="US20230005243A1-20230105-P00048.TIF" alt="custom-character" img-content="character" img-format="tif"/> &#x2009;(x<sub>(l)</sub><sup>(c)TL</sup>, y<sub>(l)</sub><sup>(c),TL</sup>)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;else { // introduce new label to the non-disjoint pixel-cluster inside l'th region</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;for (x<sub>p </sub>= x<sub>(l)</sub><sup>(c),min</sup>; x<sub>p </sub>&#x2264; x<sub>(l)</sub><sup>(c),max</sup>; x<sub>p </sub>+ +) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;for (y<sub>p </sub>= y<sub>(l)</sub><sup>(c),min</sup>; y<sub>p </sub>&#x2264; y<sub>(l)</sub><sup>(c),max</sup>; y<sub>p </sub>+ +) {</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;if (&#x2009;<img id="CUSTOM-CHARACTER-00076" he="2.46mm" wi="1.78mm" file="US20230005243A1-20230105-P00046.TIF" alt="custom-character" img-content="character" img-format="tif"/> &#x2009;<sup>(l)</sup>(x<sub>p</sub>, y<sub>p</sub>) == c)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2009;<img id="CUSTOM-CHARACTER-00077" he="2.46mm" wi="5.33mm" file="US20230005243A1-20230105-P00047.TIF" alt="custom-character" img-content="character" img-format="tif"/> &#x2009;(x<sub>p</sub>, y<sub>p</sub>) = nl</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;nl + + // increment label for next non-disjoint pixel cluster</entry></row><row><entry>&#x2003;&#x2003;&#x2003;}</entry></row><row><entry>&#x2003;} // all components done</entry></row><row><entry>} // all clusters done</entry></row><row><entry><img id="CUSTOM-CHARACTER-00078" he="2.46mm" wi="2.46mm" file="US20230005243A1-20230105-P00049.TIF" alt="custom-character" img-content="character" img-format="tif"/> &#x2009;= nl &#x2212; 1</entry></row><row><entry>Outputs&#x2009;<img id="CUSTOM-CHARACTER-00079" he="2.46mm" wi="5.33mm" file="US20230005243A1-20230105-P00047.TIF" alt="custom-character" img-content="character" img-format="tif"/> &#x2009;,&#x2009;<img id="CUSTOM-CHARACTER-00080" he="2.46mm" wi="2.46mm" file="US20230005243A1-20230105-P00049.TIF" alt="custom-character" img-content="character" img-format="tif"/> </entry></row><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0131" num="0099"><figref idref="DRAWINGS">FIG. <b>13</b></figref> illustrates one graph-cutting method <b>1300</b> for classifying each of a plurality of non-disjoint pixel-clusters in one of a plurality of categories, according to a user-indicated classification of a proper subset of the non-disjoint pixel-clusters in the categories. Method <b>1300</b> is an embodiment of step <b>620</b> of method <b>600</b>. Method <b>1300</b> may be encoded in classification instructions <b>860</b>. <figref idref="DRAWINGS">FIGS. <b>14</b>A and <b>14</b>B</figref> are examples that illustrate certain aspects of method <b>1300</b>. <figref idref="DRAWINGS">FIGS. <b>13</b>, <b>14</b>A, and <b>14</b>B</figref> are best viewed together in the following description. Method <b>1300</b> first performs step <b>622</b> of method <b>600</b> to form a graph, and then proceeds to perform steps <b>1310</b>, <b>1320</b>, and <b>1330</b>.</p><p id="p-0132" num="0100">Step <b>1310</b> initializes the graph, formed in step <b>622</b>, by setting weights of connections of the graph. Step <b>1310</b> is an embodiment of step <b>712</b> and includes steps <b>1312</b> and <b>1314</b>. For each node corresponding to a non-disjoint pixel-cluster classified by the user input, step <b>1312</b> sets (a) a maximum weight for its connection to the terminal associated with the category in which the non-disjoint pixel-cluster has been classified by the user, and (b) a zero weight for its connection to each other terminal. In one implementation, the maximum weight is infinity, or the largest numerical value that can be handled by the processor performing step <b>1312</b>.</p><p id="p-0133" num="0101"><figref idref="DRAWINGS">FIG. <b>14</b>A and <b>14</b>B</figref> shows image <b>700</b> and graph <b>702</b>, respectively, and further indicate an example user-indicated classification of non-disjoint pixel-clusters <b>710</b>(<b>3</b>) and <b>710</b>(<b>6</b>). In this example, the user has classified (a) non-disjoint pixel-cluster <b>710</b>(<b>6</b>) in the category associated with terminal T1 and (b) non-disjoint pixel-cluster <b>710</b>(<b>3</b>) in the category associated with terminal T2. As a result, step <b>1312</b> sets (a) a maximum weight for node-to-terminal connection <b>1440</b>(<b>3</b>,<b>2</b>) from node &#x393;3 to terminal T2, and (d) a zero weight for node-to-terminal connection <b>1440</b>(<b>3</b>,<b>1</b>) from node &#x393;3 to terminal T1, (c) a maximum weight for node-to-terminal connection <b>1440</b>(<b>6</b>,<b>1</b>) from node &#x393;6 to terminal T1, and (d) a zero weight for node-to-terminal connection <b>1440</b>(<b>6</b>,<b>2</b>) from node &#x393;6 to terminal T2.</p><p id="p-0134" num="0102">Step <b>1314</b> sets weights for all node-to-node connections of the graph according to color similarity between the corresponding non-disjoint pixel-clusters. For example, if non-disjoint pixel-cluster <b>710</b>(<b>7</b>) has color properties more similar to non-disjoint pixel-cluster <b>710</b>(<b>6</b>) than non-disjoint pixel-cluster <b>710</b>(<b>3</b>), step <b>1314</b> assigns a greater weight to node-to-node connection <b>770</b> from node &#x393;7 to node &#x393;6 than to node-to-node connection <b>770</b> from node &#x393;7 to node &#x393;3. Step <b>1314</b> may estimate color similarity based on a color distance between the n'th node and the p'th node calculated as:</p><p id="p-0135" num="0000"><maths id="MATH-US-00011" num="00011"><math overflow="scroll"> <mrow>  <mrow>   <msub>    <mi>d</mi>    <mi>l2</mi>   </msub>   <mo>=</mo>   <msqrt>    <mrow>     <munder>      <mo>&#x2211;</mo>      <mi>i</mi>     </munder>     <msup>      <mrow>       <mo>(</mo>       <mrow>        <msubsup>         <mover>          <mi>I</mi>          <mo>_</mo>         </mover>         <mrow>          <mo>(</mo>          <mi>n</mi>          <mo>)</mo>         </mrow>         <mrow>          <mo>(</mo>          <mi>i</mi>          <mo>)</mo>         </mrow>        </msubsup>        <mo>-</mo>        <msubsup>         <mover>          <mi>I</mi>          <mo>_</mo>         </mover>         <mi>p</mi>         <mrow>          <mo>(</mo>          <mi>i</mi>          <mo>)</mo>         </mrow>        </msubsup>       </mrow>       <mo>)</mo>      </mrow>      <mn>2</mn>     </msup>    </mrow>   </msqrt>  </mrow>  <mo>,</mo> </mrow></math></maths></p><p id="p-0136" num="0103">wherein &#x12a;<sub>(n)</sub><sup>(l) </sup>is the value of the normalized i'th color channel of the n'th node, and &#x12a;<sub>(p)</sub><sup>(l) </sup>is the value of the normalized i'th color channel of the p'th node. d<sub>12 </sub>is the <img id="CUSTOM-CHARACTER-00081" he="2.79mm" wi="1.78mm" file="US20230005243A1-20230105-P00050.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>2</sub>-norm of color difference between the n'th node and the p'th node.</p><p id="p-0137" num="0104">In one embodiment, step <b>1314</b> implements a step <b>1316</b> of setting the weight of each node-to-node connection between the n'th node &#x393;<sub>n </sub>and the p'th node &#x393;<sub>p </sub>to be inversely proportional to 1+d<sub>12</sub>:</p><p id="p-0138" num="0000"><maths id="MATH-US-00012" num="00012"><math overflow="scroll"> <mrow>  <msub>   <mi>W</mi>   <mrow>    <msub>     <mi>&#x393;</mi>     <mi>n</mi>    </msub>    <mo>&#x2192;</mo>    <msub>     <mi>&#x393;</mi>     <mi>p</mi>    </msub>   </mrow>  </msub>  <mo>=</mo>  <mrow>   <mfrac>    <mn>1</mn>    <mrow>     <mn>1</mn>     <mo>+</mo>     <msub>      <mi>d</mi>      <mrow>       <mi>l</mi>       <mo>&#x2062;</mo>       <mn>2</mn>      </mrow>     </msub>    </mrow>   </mfrac>   <mo>.</mo>  </mrow> </mrow></math></maths></p><p id="p-0139" num="0105">Method <b>1300</b> may be implemented to collectively process several images of a video stream, in which case the graph formed in step <b>622</b> includes temporal connections, as discussed above in reference to <figref idref="DRAWINGS">FIGS. <b>6</b> and <b>7</b>C</figref>. When further taking into account temporal connections, step <b>1316</b> is generalized to set the weight of each node-to-node connection and each temporal connection as:</p><p id="p-0140" num="0000"><maths id="MATH-US-00013" num="00013"><math overflow="scroll"> <mrow>  <mrow>   <msub>    <mi>W</mi>    <mrow>     <msub>      <mi>&#x393;</mi>      <mrow>       <mi>n</mi>       <mo>,</mo>       <msup>        <mrow>         <mo>{</mo>         <mi>l</mi>         <mo>}</mo>        </mrow>        <mo>&#x2192;</mo>       </msup>      </mrow>     </msub>     <mo>&#x2062;</mo>     <msub>      <mi>&#x393;</mi>      <mrow>       <mi>p</mi>       <mo>,</mo>       <mrow>        <mo>{</mo>        <mi>m</mi>        <mo>}</mo>       </mrow>      </mrow>     </msub>    </mrow>   </msub>   <mo>=</mo>   <mfrac>    <mn>1</mn>    <mrow>     <mn>1</mn>     <mo>+</mo>     <msub>      <mi>d</mi>      <mrow>       <mi>l</mi>       <mo>&#x2062;</mo>       <mn>2</mn>      </mrow>     </msub>    </mrow>   </mfrac>  </mrow>  <mo>,</mo> </mrow></math></maths></p><p id="p-0141" num="0106">wherein &#x393;<sub>n,{l }</sub> is the l'th node of the n'th frame and &#x393;<sub>p,{m}</sub> is the m&#x2032;th node of the p' th frame.</p><p id="p-0142" num="0107">In an embodiment, step <b>1310</b> further includes a step <b>1318</b> that sets weights for each node-to-terminal connection not associated with a user-indicated classification and therefore not addressed in step <b>1312</b>. For example, in this embodiment, if the user does not classify non-disjoint pixel-cluster <b>710</b>(<b>7</b>), step <b>1318</b> sets weights for node-to-terminal connection <b>1440</b>(<b>7</b>,<b>1</b>) from node &#x393;7 to terminal T1 and node-to-terminal connection <b>1440</b>(<b>7</b>,<b>2</b>) from node &#x393;7 to terminal T2. Step <b>1318</b> sets such weights based upon color similarity between the non-disjoint pixel-cluster under consideration and non-disjoint pixel-clusters classified by the user. Specifically, step <b>1318</b> sets a greater weight for the node-to-terminal connection that connects the node to a terminal that, via other node-to-terminal connections, is connected to one or more nodes most similar in color to the node under consideration. In the example of <figref idref="DRAWINGS">FIGS. <b>14</b>A and <b>14</b>B</figref>, if non-disjoint pixel-clusters <b>710</b>(<b>3</b>) and <b>710</b>(<b>6</b>) are the only non-disjoint pixel-clusters <b>710</b> classified by the user, and non-disjoint pixel-cluster <b>710</b>(<b>7</b>) has color most similar to non-disjoint pixel-cluster <b>710</b>(<b>6</b>), step <b>1318</b> sets a greater weight for node-to-terminal connection <b>1440</b>(<b>7</b>,<b>1</b>) than for node-to-terminal connection <b>1440</b>(<b>7</b>,<b>2</b>). More generally, step <b>1318</b> may compare the color of a node under consideration with the color of several nodes connected to each terminal of a graph with the maximum weight.</p><p id="p-0143" num="0108">In one implementation, for each terminal, step <b>1318</b> (<i>a</i>) finds the user-classified node, connected to this terminal, that is most similar in color to the un-classified node under consideration, (b) calculates the color distance d<sub>12 </sub>between these two nodes, and (c) sets the weight for the node-to-terminal connection between the terminal and the un-classified node under consideration to be inversely proportional to 1+d<sub>12</sub>.</p><p id="p-0144" num="0109">Step <b>1320</b> cuts the graph, according to the weights, such that each node is connected to only one of the terminals. In one example, step <b>1320</b> cuts graph <b>702</b> such that each node <b>714</b> is connected to only one of terminals T1 and T2.</p><p id="p-0145" num="0110">Step <b>1330</b> classifies each non-disjoint pixel-cluster in the category associated with the terminal to which the corresponding node is connected. In one example, after cutting of graph <b>702</b> in step <b>1320</b>, node &#x393;7 is connected only to terminal T1, and step <b>1330</b> therefore classifies non-disjoint pixel-cluster <b>710</b>(<b>7</b>) in the same category as non-disjoint pixel-cluster <b>710</b>(<b>6</b>) (having been classified by the user).</p><p id="p-0146" num="0111">Step <b>1310</b> is an embodiment of step <b>712</b> and may be encoded in graph initialization instructions <b>864</b> to set graph connection weights <b>888</b>. Steps <b>1320</b> and <b>1330</b> together form an embodiment of step <b>628</b> and may be encoded in graph cutting instructions <b>866</b> to generate image segmentation <b>290</b> by cutting graph <b>885</b> based upon graph connection weights <b>888</b>.</p><heading id="h-0007" level="1">Example 1: Clustering of Image into Non-Disjoint Pixel-Clusters</heading><p id="p-0147" num="0112"><figref idref="DRAWINGS">FIGS. <b>15</b>A</figref>&#x2014;C show clustering of image <b>300</b> into non-disjoint pixel-clusters according to an embodiment of step <b>410</b> of method <b>400</b> that implements method <b>900</b> in step <b>412</b> (with the initial clusters being squares arranged on a regular grid) and method <b>1100</b> in step <b>414</b>. Each of <figref idref="DRAWINGS">FIGS. <b>15</b>A-C</figref> shows the resulting clustering with a different set of choices for the parameters w<sub>KC</sub><sup>d </sup>and K<sub>KC</sub><sup>d</sup>. The parameter K<sub>KC </sub>relates to the parameter S<sub>KC </sub>through the equation</p><p id="p-0148" num="0000"><maths id="MATH-US-00014" num="00014"><math overflow="scroll"> <mrow>  <mrow>   <msub>    <mi>s</mi>    <mi>KC</mi>   </msub>   <mo>=</mo>   <mrow>    <mo>[</mo>    <msqrt>     <mfrac>      <mrow>       <mi>W</mi>       <mo>&#x2a2f;</mo>       <mi>H</mi>      </mrow>      <msub>       <mi>K</mi>       <mrow>        <mi>K</mi>        <mo>&#x2062;</mo>        <mi>C</mi>       </mrow>      </msub>     </mfrac>    </msqrt>    <mo>]</mo>   </mrow>  </mrow>  <mo>,</mo> </mrow></math></maths></p><p id="p-0149" num="0000">wherein W and H are the width and height, respectively, of the image. For <figref idref="DRAWINGS">FIGS. <b>15</b>A-C</figref>, K<sub>KC</sub>=128, and w<sub>KC</sub><sup>d </sup>equals 0.1, 0.5, and 1.0, respectively.</p><p id="p-0150" num="0113">The effect of w<sub>KC</sub><sup>d </sup>is to control the trade-off between the color difference and the location difference. As is observed in <figref idref="DRAWINGS">FIGS. <b>15</b>A-C</figref>, higher value of w<sub>KC</sub><sup>d </sup>leads to boundaries closer to the original square-window shape. Smaller values of w<sub>KC</sub><sup>d </sup>give more importance to the color and tends to break the clusters more out of the square-window shape. A typical value of w<sub>KC</sub><sup>d </sup>in the range from 0.1 to 1.0 shows good results in our experiments. It is further evident from comparing <figref idref="DRAWINGS">FIGS. <b>15</b>A-C</figref> with <figref idref="DRAWINGS">FIG. <b>5</b>B</figref> (K<sub>KC</sub>=128, w<sub>KC</sub><sup>d</sup>=1.0) that increasing K<sub>KC </sub>results in more clusters per frame. The time complexity with respect to K<sub>KC </sub>is shown in Table 2 in an example where image <b>300</b> was processed on a Windows 10 PC with Intel Xeon CPU E5-2637 v3 at 3.5 GHz and 16GB RAM system running MATLAB R2017b version.</p><p id="p-0151" num="0000"><tables id="TABLE-US-00002" num="00002"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="center"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE 2</entry></row></thead><tbody valign="top"><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>Average computation time for different values of </entry></row><row><entry>K<sub>KC </sub>with w<sub>KC</sub><sup>d </sup>= 0.1.</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="1" colwidth="28pt" align="left"/><colspec colname="2" colwidth="28pt" align="center"/><colspec colname="3" colwidth="161pt" align="center"/><tbody valign="top"><row><entry/><entry>K<sub>KC</sub></entry><entry>Avg. computation time (sec.)</entry></row><row><entry namest="1" nameend="3" align="center" rowsep="1"/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="1" colwidth="28pt" align="left"/><colspec colname="2" colwidth="28pt" align="char" char="."/><colspec colname="3" colwidth="161pt" align="center"/><tbody valign="top"><row><entry/><entry>128</entry><entry>5.00</entry></row><row><entry/><entry>512</entry><entry>5.49</entry></row><row><entry/><entry>2025</entry><entry>9.41</entry></row><row><entry namest="1" nameend="3" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><heading id="h-0008" level="1">Combinations of Features</heading><p id="p-0152" num="0114">Features described above as well as those claimed below may be combined in various ways without departing from the scope hereof. For example, it will be appreciated that aspects of one image segmentation method or product, described herein, may incorporate or swap features of another image segmentation method or product described herein. The following examples illustrate some possible, non-limiting combinations of embodiments described above. It should be clear that many other changes and modifications may be made to the methods, products, and systems herein without departing from the spirit and scope of this invention:</p><p id="p-0153" num="0115">(A1) One method for image segmentation includes (a) clustering, based upon k-means clustering, pixels of an image into a plurality of first clusters, (b) re-clustering, at least in part by processing connectivity of the pixels within the first clusters, the pixels into a plurality of non-disjoint pixel-clusters such that, within each of the non-disjoint pixel-clusters, any pair of pixels are immediately adjacent each other or connected to each other via one or more other pixels of the non-disjoint pixel-cluster , and (c) classifying each of the non-disjoint pixel-clusters in one of a plurality of categories, according to a user-indicated classification of a proper subset of the non-disjoint pixel-clusters in the categories.</p><p id="p-0154" num="0116">(A2) In the method denoted as (A1), the plurality of categories may consist of a first category and a second category.</p><p id="p-0155" num="0117">(A3) In the method denoted as (A2), the first category may indicate a portion of the image to be in focus, and the second category may indicate a portion of the image to be defocused.</p><p id="p-0156" num="0118">(A4) In the method denoted as (A1), each category may indicate a portion of the image to be displayed with a respective degree of focus.</p><p id="p-0157" num="0119">(A5) In any of the methods denoted as (A1) through (A4), the step of clustering may include assigning each pixel of the image to one of the first clusters according to color and position of the pixel.</p><p id="p-0158" num="0120">(A6) In the method denoted as (A5), the step of assigning may include, for each pixel, selecting, for the step of assigning, the one of the first clusters from first clusters within a local search region around the pixel, wherein the one of the first clusters is located at a smallest cluster-distance from the pixel, and wherein cluster-distance is a weighted combination of color difference and location difference between the pixel and the first clusters.</p><p id="p-0159" num="0121">(A7) In the method denoted as (A6), for each of the first clusters, the cluster-distance may be referenced to a cluster-centroid of the first cluster, wherein the cluster-centroid indicates average location and color of all pixels of the first cluster, and the step of clustering may include iteratively (i) performing the step of assigning for all the pixels and (ii) recalculating the cluster-centroid for each of the first clusters.</p><p id="p-0160" num="0122">(A8) The method denoted as (A7) may further include, prior to a first iteration of the step of assigning, initializing respective center locations of the first clusters to be arranged in a regular grid.</p><p id="p-0161" num="0123">(A9) Any of the methods denoted as (A5) through (A8) may further include, prior to the step of clustering, normalizing each color component of the image according to average and standard deviation of the color component across the image.</p><p id="p-0162" num="0124">(A10) In any of the methods denoted as (A1) through (A9), the step of re-clustering may include applying morphological filters to a map of the first clusters to enhance connectivity of pixels within the first clusters, and, after the step of applying, analyzing non-disjoint pixel-clusters of the map of the first clusters, wherein the step of analyzing includes (i) for each non-disjoint pixel-cluster containing at least a threshold number of pixels, assigning a new cluster label to the non-disjoint pixel-cluster, the new cluster label defining a respective one of the non-disjoint pixel-clusters, and (ii) for each non-disjoint pixel-cluster containing fewer pixels than the threshold number of pixels, merging the non-disjoint pixel-cluster into an adjacent non-disjoint pixel-cluster.</p><p id="p-0163" num="0125">(A11) In the method denoted as (A10), the step of applying may include applying a closing operation to the map and, after applying the closing operation, applying an opening operation to the map.</p><p id="p-0164" num="0126">(A12) In any of the methods denoted as (A1) through (A11), the step of classifying may include (i) for each non-disjoint pixel-cluster in the proper subset, classifying the non-disjoint pixel-cluster in one of the categories based upon a user input, and (ii) after classifying each non-disjoint pixel-cluster of the proper subset, classifying, by graph cutting, remaining ones of the non-disjoint pixel-clusters in the categories.</p><p id="p-0165" num="0127">(A13) In the method denoted as (A12), the step of classifying the remaining ones of the non-disjoint pixel-clusters may include (I) forming a graph wherein each non-disjoint pixel-cluster is connected to (a) a plurality of terminals respectively associated with the plurality of categories, and (b) all other non-disjoint pixel-clusters within a neighborhood of the non-disjoint pixel-cluster, (II) initializing the graph by (a) for each non-disjoint pixel-cluster of the proper subset, setting a maximum weight for its connection to the one terminal associated with the category in which the non-disjoint pixel-cluster is classified, and setting a zero weight for its connection to each other one of the terminals, and (b) for each of the remaining non-disjoint pixel-clusters within the neighborhood, specifying a weight for each of its connections to other non-disjoint pixel-clusters according to color similarity, (III) cutting the graph, based upon the weights defined in the step of initializing, such that each of the non-disjoint pixel-clusters is connected to only one of the terminals, and (IV) after the step of cutting, classifying each non-disjoint pixel-cluster in the category associated with the terminal that connected to the non-disjoint pixel-cluster.</p><p id="p-0166" num="0128">(A14) In the method denoted as (A13), the neighborhood may be a spatial portion of the image centered at the non-disjoint pixel-cluster, wherein the spatial portion is smaller than the image.</p><p id="p-0167" num="0129">(A15) In either of the methods denoted as (A13) and (A14), the image may be part of a video stream, and the method may further include (i) in the step of forming, including temporal connections between each non-disjoint pixel-cluster of the image and each non-disjoint pixel-cluster within a same neighborhood in each of one or more other preceding or subsequent images of the video stream, and (ii) in the step of initializing, specifying a weight of the temporal connections according to color similarity.</p><p id="p-0168" num="0130">(A16) In the method denoted as (A15), the temporal connections may connect to t<sub>b </sub>preceding images and t<sub>a </sub>subsequent images, wherein t<sub>a </sub>and t<sub>b </sub>are non-negative integers, t<sub>b</sub>&#x2265;t<sub>a</sub>.</p><p id="p-0169" num="0131">(A17) In any of the methods denoted as (A13) through (A16) in the step of initializing, the weight of each connection to the other non-disjoint pixel-clusters may be inversely proportional to 1+d<sub>12</sub>, wherein d<sub>12 </sub>is the <img id="CUSTOM-CHARACTER-00082" he="2.79mm" wi="1.78mm" file="US20230005243A1-20230105-P00051.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>2 </sub>-norm of color difference between the non-disjoint pixel-clusters connected by the connection.</p><p id="p-0170" num="0132">(B1) One software product for image segmentation includes computer-readable media storing machine-readable instructions that include (I) clustering instructions that, when executed by a processor, control the processor to (a) cluster, based upon k-means clustering, pixels of an image into a plurality of first clusters and (b) store, to a memory, a k-means-cluster-map of the first clusters, (II) re-clustering instructions that, when executed by the processor, control the processor to (a) retrieve the k-means-cluster-map from memory, (b) process connectivity of pixels within the first clusters of the k-means-cluster-map to re-cluster the pixels into a plurality of non-disjoint pixel-clusters such that all pixels of each of the non-disjoint pixel-clusters are connected to each other, and (c) store, to the memory, a connected-cluster-map of the non-disjoint pixel-clusters, and (III) classification instructions that, when executed by the processor, control the processor to classify each of the non-disjoint pixel-clusters in one of a plurality of categories, according to a user-specified classification of a proper subset of the non-disjoint pixel-clusters in the categories.</p><p id="p-0171" num="0133">(B2) In the software product denoted as (B1), the plurality of categories may consist of a first category and a second category.</p><p id="p-0172" num="0134">(B3) In the software product denoted as (B2), the first category may indicate a portion of the image to be in focus, and the second category may indicate a portion of the image to be defocused.</p><p id="p-0173" num="0135">(B3) In the software product denoted as (B1), each category may indicate a portion of the image to be displayed with a respective degree of focus.</p><p id="p-0174" num="0136">(B4) In any of the software products denoted as (B1) through (B3), the re-clustering instructions may include morphological filter instructions that, when executed by the processor, control the processor to filter the k-means-cluster-map to enhance connectivity of pixels within the first clusters, and connected-component-analysis instructions that, when executed by the processor, control the processor to analyze non-disjoint pixel-clusters of the k-means-cluster-map, as enhanced by execution of the morphological filter instructions, to generate the connected-cluster-map.</p><p id="p-0175" num="0137">(B5) In the software product denoted as (B4), the connected-component-analysis instructions may be configured to, when executed by the processor, control the processor to (i) for each non-disjoint pixel-cluster containing at least a threshold number of pixels, assign a new cluster label to the non-disjoint pixel-cluster, the new cluster label defining a respective one of the non-disjoint pixel-clusters, and (ii) for each non-disjoint pixel-cluster containing fewer pixels than the threshold number of pixels, merge the non-disjoint pixel-cluster into an adjacent non-disjoint pixel-cluster.</p><p id="p-0176" num="0138">(B6) Any of the software products denoted as (B1) through (B5) may further include (IV) user interface instructions that, when executed by the processor, control the processor to generate, on a display, a graphical user interface configured to receive, from a user, an indication of the user-specified classification of the proper subset of the non-disjoint pixel-clusters, and the classification instructions may include graph cutting instructions that, when executed by the processor, control the processor to classify, by graph cutting and based on the indications received from the user via the graphical user interface, remaining ones of the non-disjoint pixel-clusters in the categories.</p><p id="p-0177" num="0139">(B7) In the software product denoted as (B6), the graphical user interface may include an image panel for displaying the image with the connected-cluster-map overlaid thereon, and one or more controls that allow a user to point to one or more of the non-disjoint pixel-clusters depicted in the image panel to classify the one or more of the non-disjoint pixel-clusters in a selected one of the categories.</p><p id="p-0178" num="0140">(B8) In either of the software products denoted as (B6) and (B7), the user interface instructions may further be configured to, when executed by the processor and after classification of all non-disjoint pixel-clusters in the categories, display the image with a classification map overlaid thereon, wherein the classification map indicates spatial segmentation between the categories.</p><p id="p-0179" num="0141">(C1) One method for user-guided image segmentation includes (a) forming a graph having a plurality of terminals and a plurality of nodes, wherein each of the nodes corresponds to a first respective non-disjoint pixel-cluster of the image and is connected, in the graph, to each of the terminals and all other ones of the nodes corresponding to other respective non-disjoint pixel-clusters that, in the image, are within a neighborhood of the first respective non-disjoint pixel-cluster, (b) initializing the graph by setting weights of connections of the graph at least partly according to a user input indicating classification of some but not all of the non-disjoint pixel-clusters in a plurality of categories respectively associated with the plurality of terminals, and (c) segmenting the image into the categories by cutting the graph based upon the weights.</p><p id="p-0180" num="0142">(C2) In the method denoted as (C1), the plurality of categories may consist of a first category and a second category.</p><p id="p-0181" num="0143">(C3) In the method denoted as (C2), the first category may indicate a portion of the image to be in focus, the second category may indicate a portion of the image to be defocused.</p><p id="p-0182" num="0144">(C4) In the method denoted as (C1), each category may indicate a portion of the image to be displayed with a respective degree of focus.</p><p id="p-0183" num="0145">(C5) In any of the methods denoted as (C1) through (C4), the step of initializing may include (i) setting weights for all node-to-node connections of the graph according to color similarity between the corresponding non-disjoint pixel-clusters, and (ii) for each node corresponding to a non-disjoint pixel-cluster classified by the user input, setting a maximum weight for its connection to the terminal associated with the category in which the non-disjoint pixel-cluster is classified, and setting a zero weight for its connection to each other terminal.</p><p id="p-0184" num="0146">(C6) In the method denoted as (C5), in the step of specifying, the weight of each connection to the other non-disjoint pixel-clusters may be inversely proportional to 1+d<sub>12 </sub>, wherein d<sub>12 </sub>is the <img id="CUSTOM-CHARACTER-00083" he="2.79mm" wi="1.78mm" file="US20230005243A1-20230105-P00052.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>2</sub>-norm of color difference between the non-disjoint pixel-clusters connected by the connection.</p><p id="p-0185" num="0147">(C7) In any of the methods denoted as (C1) through (C6), the step of segmenting may include cutting the graph, based upon the weights, such that each of the nodes is connected to only one of the terminals, and, after the step of cutting, classifying each non-disjoint pixel-cluster in the one of the categories associated with the terminal connected with the node corresponding to the non-disjoint pixel-cluster.</p><p id="p-0186" num="0148">(C8) In any of the methods denoted as (C1) through (C7), the non-disjoint pixel-clusters, corresponding to the nodes, may cooperatively include all pixels of the image.</p><p id="p-0187" num="0149">(C9) In any of the methods denoted as (C1) through (C8), the neighborhood may be a spatial portion of the image centered at the non-disjoint pixel-cluster, wherein the spatial portion is smaller than the image.</p><p id="p-0188" num="0150">(C10) In any of the methods denoted as (C1) through (C9), the image may be part of a video stream, and the method may further include, in the step of forming and for each of the nodes, including temporal connections between the node and each node corresponding to a non-disjoint pixel-cluster within a same neighborhood in each of one or more other preceding or subsequent images of the video stream, and in the step of initializing, setting weights for the node-to-node connections and the temporal connections according to color similarity between the corresponding non-disjoint pixel-clusters.</p><p id="p-0189" num="0151">(C11) In the method denoted as (C10), the temporal connections may connect to t<sub>b </sub>preceding images and t<sub>a </sub>subsequent images, wherein t<sub>a </sub>and t<sub>b </sub>are non-negative integers, t<sub>b</sub>&#x2265;t<sub>a</sub>.</p><p id="p-0190" num="0152">(C12) Any of the methods denoted as (C1) through (C11) may further include prior to the step of initializing, displaying the image with a map of the non-disjoint pixel-clusters overlaid thereon, and, after the step of segmenting, displaying the image with a map of the categories overlaid thereon.</p><p id="p-0191" num="0153">(D1) One software product for image segmentation includes computer-readable media storing machine-readable instructions that include (a) graph setup instructions that, when executed by a processor, control the processor to form a graph having a plurality of terminals and a plurality of nodes, wherein each of the nodes corresponds to a respective non-disjoint pixel-cluster of the image and is connected, in the graph, to each of the terminals and all other ones of the nodes corresponding to other respective non-disjoint pixel-clusters that, in the image, are within a neighborhood of the respective non-disjoint pixel-cluster, (b) graph initializing instructions that, when executed by the processor, control the processor to set weights of connections of the graph at least partly according to a user input indicating classification of some but not all of the non-disjoint pixel-clusters in a plurality of categories respectively corresponding to the plurality of terminals, and (c) graph cutting instructions that, when executed by the processor, control the processor to cut the graph based upon the weights so as to segment the image into the categories.</p><p id="p-0192" num="0154">(D2) In the software product denoted as (D1), the plurality of categories may consist of a first category and a second category.</p><p id="p-0193" num="0155">(D3) In the software product denoted as (D2), the first category may indicate a portion of the image to be in focus, and the second category may indicate a portion of the image to be defocused.</p><p id="p-0194" num="0156">(D4) In the software product denoted as (D1), each category may indicate a portion of the image to be displayed with a respective degree of focus.</p><p id="p-0195" num="0157">(D5) In any of the software products denoted as (D1) through (D4), the graph initializing instructions may be configured to, when executed by the processor, control the processor to (i) set weights for all node-to-node connections of the graph according to color similarity between the corresponding non-disjoint pixel-clusters, and (ii) for each node corresponding to a non-disjoint pixel-cluster classified by the user input, set a maximum weight for its connection to the terminal associated with the category in which the non-disjoint pixel-cluster is classified, and set a zero weight for its connection to each other terminal.</p><p id="p-0196" num="0158">(D6) In any of the software products denoted as (D1) through (D5), the graph cutting instructions may be configured to, when executed by the processor, control the processor to (i) cut the graph, based upon the weights, such that each of the nodes is connected to only one of the terminals, and (ii) classify each non-disjoint pixel-cluster in the category associated with the terminal connected to the node corresponding to the non-disjoint pixel-cluster.</p><p id="p-0197" num="0159">(D7) Any of the software products denoted as (D1) through (D6) may further include user interface instructions that, when executed by the processor, control the processor to generate, on a display, a graphical user interface configured to receive, from a user, an indication of the classification of said some but not all of the non-disjoint pixel-clusters.</p><p id="p-0198" num="0160">(D8) In the software product denoted as (D7), the graphical user interface may include an image panel for displaying the image with a map of the non-disjoint pixel-clusters overlaid thereon, and one or more controls that allow a user to point to one or more of the non-disjoint pixel-clusters depicted in the image panel to classify the one or more of the non-disjoint pixel-clusters in a selected one of the categories.</p><p id="p-0199" num="0161">(D9) In the software product denoted as (D8), the user interface instructions may further be configured to, when executed by the processor and after classification of all non-disjoint pixel-clusters in the categories, display the image with a classification map overlaid thereon, the classification map indicating spatial segmentation between categories.</p><p id="p-0200" num="0162">(E1) One software product for image segmentation includes machine-readable instructions that, when the software product is executed by a computer, causes the computer to carry out the method denoted as any one of (A1) through (A17) and (C1) through (C12).</p><p id="p-0201" num="0163">Changes may be made in the above systems and methods without departing from the scope hereof. It should thus be noted that the matter contained in the above description and shown in the accompanying drawings should be interpreted as illustrative and not in a limiting sense. The following claims are intended to cover generic and specific features described herein, as well as all statements of the scope of the present systems and methods, which, as a matter of language, might be said to fall therebetween.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230005243A1-20230105-M00001.NB"><img id="EMI-M00001" he="20.83mm" wi="76.20mm" file="US20230005243A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002" nb-file="US20230005243A1-20230105-M00002.NB"><img id="EMI-M00002" he="6.69mm" wi="76.20mm" file="US20230005243A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00003" nb-file="US20230005243A1-20230105-M00003.NB"><img id="EMI-M00003" he="6.69mm" wi="76.20mm" file="US20230005243A1-20230105-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00004" nb-file="US20230005243A1-20230105-M00004.NB"><img id="EMI-M00004" he="7.37mm" wi="76.20mm" file="US20230005243A1-20230105-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00005" nb-file="US20230005243A1-20230105-M00005.NB"><img id="EMI-M00005" he="6.35mm" wi="76.20mm" file="US20230005243A1-20230105-M00005.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00006" nb-file="US20230005243A1-20230105-M00006.NB"><img id="EMI-M00006" he="3.13mm" wi="76.20mm" file="US20230005243A1-20230105-M00006.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00007" nb-file="US20230005243A1-20230105-M00007.NB"><img id="EMI-M00007" he="3.56mm" wi="76.20mm" file="US20230005243A1-20230105-M00007.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00008" nb-file="US20230005243A1-20230105-M00008.NB"><img id="EMI-M00008" he="3.13mm" wi="76.20mm" file="US20230005243A1-20230105-M00008.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00009" nb-file="US20230005243A1-20230105-M00009.NB"><img id="EMI-M00009" he="6.69mm" wi="76.20mm" file="US20230005243A1-20230105-M00009.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00010" nb-file="US20230005243A1-20230105-M00010.NB"><img id="EMI-M00010" he="6.69mm" wi="76.20mm" file="US20230005243A1-20230105-M00010.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00011" nb-file="US20230005243A1-20230105-M00011.NB"><img id="EMI-M00011" he="7.37mm" wi="76.20mm" file="US20230005243A1-20230105-M00011.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00012" nb-file="US20230005243A1-20230105-M00012.NB"><img id="EMI-M00012" he="6.01mm" wi="76.20mm" file="US20230005243A1-20230105-M00012.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00013" nb-file="US20230005243A1-20230105-M00013.NB"><img id="EMI-M00013" he="6.01mm" wi="76.20mm" file="US20230005243A1-20230105-M00013.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00014" nb-file="US20230005243A1-20230105-M00014.NB"><img id="EMI-M00014" he="7.37mm" wi="76.20mm" file="US20230005243A1-20230105-M00014.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method for image segmentation, comprising:<claim-text>clustering, based upon k-means clustering, pixels of an image into a plurality of first clusters;</claim-text><claim-text>outputting a cluster map of the first clusters, wherein the cluster map comprises non-disjoint pixel-clusters and disjoint pixel-clusters, wherein, within each of the non-disjoint pixel-clusters, any pair of pixels are immediately adjacent each other or connected to each other via one or more other pixels of the non- disjoint pixel-cluster,</claim-text><claim-text>re-clustering, at least in part by processing connectivity of the pixels within the first clusters, the pixels into a new plurality of non-disjoint pixel-clusters wherein the step of re-clustering re-clusters only the non-disjoint pixel-clusters of the map of the first clusters; and</claim-text><claim-text>classifying each of the non-disjoint pixel-clusters in one of a plurality of categories, according to a user-indicated classification of a proper subset of the non-disjoint pixel-clusters in the categories.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said re-clustering comprises assigning new labels only to non-disjoint pixel clusters containing at least a threshold number of pixels, wherein the new label defines a respective one of the non-disjoint pixel cluster of the new plurality of non-disjoint pixel-clusters.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said re-clustering comprises:<claim-text>performing a raster scan through all non-disjoint pixel clusters of the cluster map in a certain pattern, and</claim-text><claim-text>merging each non-disjoint pixel cluster that is smaller than a threshold number of pixels into the most recent non-disjoint cluster, in the raster scan, that contains at least the threshold number of pixels.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein merging each non-disjoint pixel cluster comprises re-labelling each non-disjoint pixel cluster that is smaller than the threshold with a label of the most recent non-disjoint cluster that contains at least the threshold number of pixels, wherein the label defines a respective one of the non-disjoint pixel clusters.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. (canceled)</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein <img id="CUSTOM-CHARACTER-00084" he="2.79mm" wi="2.12mm" file="US20230005243A1-20230105-P00053.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>(l) </sub>is the map of non-disjoint pixel clusters for a l region, <img id="CUSTOM-CHARACTER-00085" he="2.46mm" wi="2.79mm" file="US20230005243A1-20230105-P00054.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>(l) </sub>is the number of non-disjoint pixel clusters in the l'th region, <img id="CUSTOM-CHARACTER-00086" he="2.79mm" wi="2.12mm" file="US20230005243A1-20230105-P00003.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>(l)</sub><sup>(c) </sup>is the c'th non-disjoint pixel cluster of l'th region, <img id="CUSTOM-CHARACTER-00087" he="3.22mm" wi="3.22mm" file="US20230005243A1-20230105-P00055.TIF" alt="custom-character" img-content="character" img-format="tif"/> is the number of pixels within the c'th non-disjoint pixel cluster <img id="CUSTOM-CHARACTER-00088" he="2.79mm" wi="2.12mm" file="US20230005243A1-20230105-P00003.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>(l)</sub><sup>(c) </sup>and T<sub>KC </sub>is the threshold number of pixels in c'th non-disjoint pixel cluster, wherein if <img id="CUSTOM-CHARACTER-00089" he="2.46mm" wi="3.22mm" file="US20230005243A1-20230105-P00056.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>(l)</sub>&#x3e;1 and <img id="CUSTOM-CHARACTER-00090" he="2.46mm" wi="2.79mm" file="US20230005243A1-20230105-P00057.TIF" alt="custom-character" img-content="character" img-format="tif"/>&#x2265;T<sub>KC </sub>a new label to the c'th non-disjoint pixel cluster is assigned and if <img id="CUSTOM-CHARACTER-00091" he="3.22mm" wi="3.22mm" file="US20230005243A1-20230105-P00058.TIF" alt="custom-character" img-content="character" img-format="tif"/>&#x3c;T<sub>KC</sub>, the c'th non-disjoint pixel cluster is merged with an adjacent non-disjoint pixel cluster.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein for each c'th non-disjoint pixel cluster <img id="CUSTOM-CHARACTER-00092" he="2.79mm" wi="2.12mm" file="US20230005243A1-20230105-P00003.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>(l)</sub><sup>(c) </sup>a bounding box is defined surrounding the c'th non-disjoint pixel cluster <img id="CUSTOM-CHARACTER-00093" he="2.79mm" wi="2.12mm" file="US20230005243A1-20230105-P00003.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>(l)</sub><sup>(c) </sup>by taking minimum and maximum horizontal and vertical x-y coordinates of the c'th non-disjoint pixel cluster <img id="CUSTOM-CHARACTER-00094" he="2.79mm" wi="2.12mm" file="US20230005243A1-20230105-P00003.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>(l)</sub><sup>(c) </sup>in a cartesian coordinate system for representing the image, wherein the adjacent non-disjoint pixel cluster is identified by the pixel computed as:<claim-text><br/><?in-line-formulae description="In-line Formulae" end="lead"?>(<i>x</i><sub>(l)</sub><sup>(c)TL</sup><i>,y</i><sub>(l)</sub><sup>(c),TL</sup>)=(max{(<i>x</i><sub>(l)</sub><sup>(c),min&#x2212;</sup>1 ),0}, max{(<i>y</i><sub>(l)</sub><sup>(c),min</sup>&#x2212;1),0}).<?in-line-formulae description="In-line Formulae" end="tail"?></claim-text></claim-text><claim-text>wherein (x<sub>(l)</sub><sup>(c)TL</sup>,y<sub>(l)</sub><sup>(c),TL</sup>) represent the horizontal and vertical coordinates of the pixel in the adjacent non-disjoint pixel cluster TL, and wherein (x<sub>(l)</sub><sup>(c),min </sup>and y<sub>(l)</sub><sup>(c),min </sup>represent the minimum horizontal and vertical coordinates of the c'th non-disjoint pixel cluster <img id="CUSTOM-CHARACTER-00095" he="2.79mm" wi="2.12mm" file="US20230005243A1-20230105-P00003.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>(l)</sub><sup>(c)</sup>.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of any of the previous <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein re-clustering further comprises:<claim-text>applying morphological filters to the map of the first clusters to enhance connectivity of pixels within the first clusters, and before applying morphological filters to the map of the first clusters, obtaining a binary mask for each cluster for processing each cluster separately with morphological filters.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. (canceled)</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method according to <claim-ref idref="CLM-00008">claim 8</claim-ref> wherein the step of applying morphological filters comprises applying a closing operation function and an opening operation function for filling pixel gaps in each one of the first clusters and obtaining an updated cluster map of the first clusters.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the plurality of categories comprises a first category and a second category and wherein the first category indicates a portion of the image to be in focus, and the second category indicates a portion of the image to be defocused.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of any of the previous <claim-ref idref="CLM-00001">claim 1</claim-ref>, the step of clustering comprising assigning each pixel of the image to one of the first clusters according to color and position of the pixel.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, the step of assigning comprising, for each pixel, selecting, for said assigning, the one of the first clusters from first clusters within a local search region around the pixel, the one of the first clusters being located at a smallest cluster-distance from the pixel, wherein cluster-distance is a weighted combination of color difference and location difference between the pixel and the first clusters.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, for each of the first clusters, the cluster-distance being referenced to a cluster-centroid of the first cluster, the cluster-centroid indicating average location and color of all pixels of the first cluster, the step of clustering comprising iteratively:<claim-text>performing the step of assigning for all the pixels; and</claim-text><claim-text>recalculating the cluster-centroid for each of the first clusters.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref> to further comprising, prior to a first iteration of the step of assigning, initializing respective center locations of the first clusters to be arranged in a regular grid.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising, prior to the step of clustering, normalizing each color component of the image according to average and standard deviation of the color component across the image.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The method of any of the preceding <claim-ref idref="CLM-00001">claim 1</claim-ref>, the step of classifying comprising:<claim-text>for each non-disjoint pixel-cluster in the proper subset, classifying the non-disjoint pixel-cluster in one of the categories based upon a user input; and</claim-text><claim-text>after classifying each non-disjoint pixel-cluster of the proper subset, classifying, by graph cutting, remaining ones of the non-disjoint pixel-clusters in the categories.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The method of <claim-ref idref="CLM-00017">claim 17</claim-ref>, the step of classifying the remaining ones of the non-disjoint pixel-clusters comprising:<claim-text>forming a graph wherein each non-disjoint pixel-cluster is connected to (a) a plurality of terminals respectively associated with the plurality of categories, and (b) all other non-disjoint pixel-clusters within a neighborhood of the non-disjoint pixel-cluster;</claim-text><claim-text>initializing the graph by:<claim-text>for each non-disjoint pixel-cluster of the proper subset, setting a maximum weight for its connection to the one terminal associated with the category in which the non-disjoint pixel-cluster is classified, and setting a zero weight for its connection to each other one of the terminals, and</claim-text><claim-text>for each of the remaining non-disjoint pixel-clusters within the neighborhood, specifying a weight for each of its connections to other non-disjoint pixel-clusters according to color similarity;</claim-text></claim-text><claim-text>cutting the graph, based upon the weights defined in the step of initializing, such that each of the non-disjoint pixel-clusters is connected to only one of the terminals; and</claim-text><claim-text>after the step of cutting, classifying each non-disjoint pixel-cluster in the category associated with the terminal that connected to the non-disjoint pixel-cluster.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The method of <claim-ref idref="CLM-00018">claim 18</claim-ref>, the neighborhood being a spatial portion of the image centered at the non-disjoint pixel-cluster, the spatial portion being smaller than the image.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The method of <claim-ref idref="CLM-00018">claim 18</claim-ref>, the image being part of a video stream, the method further comprising:<claim-text>in the step of forming, including temporal connections between each non-disjoint pixel-cluster of the image and each non-disjoint pixel-cluster within a same neighborhood in each of one or more other preceding or subsequent images of the video stream; and</claim-text><claim-text>in the step of initializing, specifying a weight of the temporal connections according to color similarity.</claim-text></claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. The method of <claim-ref idref="CLM-00020">claim 20</claim-ref>, the temporal connections connecting to t<sub>b </sub>preceding images and t<sub>a </sub>subsequent images, t<sub>a </sub>and t<sub>b </sub>being non-negative integers, t<sub>b</sub>&#x2265;t<sub>a</sub>.</claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. The method of <claim-ref idref="CLM-00018">claim 18</claim-ref> in the step of initializing, the weight of each connection to the other non-disjoint pixel-clusters being inversely proportional to 1+d<sub>l2</sub>, wherein d<sub>l2 </sub>is the <img id="CUSTOM-CHARACTER-00096" he="2.79mm" wi="1.78mm" file="US20230005243A1-20230105-P00059.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>2</sub>-norm of color difference between the non-disjoint pixel-clusters connected by the connection.</claim-text></claim><claim id="CLM-00023" num="00023"><claim-text><b>23</b>. A method for image segmentation includes (a) forming a graph having a plurality of terminals and a plurality of nodes, wherein each of the nodes corresponds to a first respective non-disjoint pixel-cluster of the image and is connected, in the graph, to each of the terminals and all other ones of the nodes corresponding to other respective non-disjoint pixel-clusters that, in the image, are within a neighborhood of the first respective non-disjoint pixel-cluster, (b) initializing the graph by setting weights of connections of the graph at least partly according to a user input indicating classification of some but not all of the non-disjoint pixel-clusters in a plurality of categories respectively associated with the plurality of terminals, and (c) segmenting the image into the categories by cutting the graph based upon the weights.</claim-text></claim><claim id="CLM-00024" num="00024"><claim-text><b>24</b>. The method of <claim-ref idref="CLM-00023">claim 23</claim-ref>, the plurality of categories comprises a first category and a second category, wherein the first category indicates a portion of the image to be in focus, the second category indicates a portion of the image to be defocused.</claim-text></claim><claim id="CLM-00025" num="00025"><claim-text><b>25</b>. The method <claim-ref idref="CLM-00023">claim 23</claim-ref> or <claim-ref idref="CLM-00021">21</claim-ref>, the step of initializing includes (i) setting weights for all node-to-node connections of the graph according to color similarity between the corresponding non-disjoint pixel-clusters, and (ii) for each node corresponding to a non-disjoint pixel-cluster classified by the user input, setting a maximum weight for its connection to the terminal associated with the category in which the non-disjoint pixel-cluster is classified, and setting a zero weight for its connection to each other terminal.</claim-text></claim><claim id="CLM-00026" num="00026"><claim-text><b>26</b>. The method of <claim-ref idref="CLM-00025">claim 25</claim-ref>, in the step of specifying, the weight of each connection to the other non-disjoint pixel-clusters is inversely proportional to 1+d<sub>l2</sub>, wherein d<sub>l2 </sub>is the <img id="CUSTOM-CHARACTER-00097" he="2.79mm" wi="1.78mm" file="US20230005243A1-20230105-P00060.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>2</sub>-norm of color difference between the non-disjoint pixel-clusters connected by the connection.</claim-text></claim><claim id="CLM-00027" num="00027"><claim-text><b>27</b>. The method of any of the <claim-ref idref="CLM-00023">claim 23</claim-ref>, the step of segmenting includes cutting the graph, based upon the weights, such that each of the nodes is connected to only one of the terminals, and, after the step of cutting, classifying each non-disjoint pixel-cluster in the one of the categories associated with the terminal connected with the node corresponding to the non-disjoint pixel-cluster.</claim-text></claim><claim id="CLM-00028" num="00028"><claim-text><b>28</b>. The method <claim-ref idref="CLM-00023">claim 23</claim-ref>, the non-disjoint pixel-clusters, corresponding to the nodes, cooperatively includes all pixels of the image.</claim-text></claim><claim id="CLM-00029" num="00029"><claim-text><b>29</b>. The method of <claim-ref idref="CLM-00023">claim 23</claim-ref>, the neighborhood is a spatial portion of the image centered at the non-disjoint pixel-cluster, wherein the spatial portion is smaller than the image.</claim-text></claim><claim id="CLM-00030" num="00030"><claim-text><b>30</b>. The method as claimed in any of the <claim-ref idref="CLM-00023">claim 23</claim-ref>, the image is part of a video stream, and the method further includes, in the step of forming and for each of the nodes, including temporal connections between the node and each node corresponding to a non-disjoint pixel-cluster within a same neighborhood in each of one or more other preceding or subsequent images of the video stream, and in the step of initializing, setting weights for the node-to-node connections and the temporal connections according to color similarity between the corresponding non-disjoint pixel-clusters.</claim-text></claim><claim id="CLM-00031" num="00031"><claim-text><b>31</b>. The method of <claim-ref idref="CLM-00030">claim 30</claim-ref>, the temporal connections may connect to t<sub>b </sub>preceding images and t<sub>a </sub>subsequent images, wherein t<sub>a </sub>and t<sub>b </sub>are non-negative integers, t<sub>b</sub>&#x2265;t<sub>a</sub>.</claim-text></claim><claim id="CLM-00032" num="00032"><claim-text><b>32</b>. The method of any of the <claim-ref idref="CLM-00023">claim 23</claim-ref>, further comprising prior to the step of initializing, displaying the image with a map of the non-disjoint pixel-clusters overlaid thereon, and, after the step of segmenting, displaying the image with a map of the categories overlaid thereon.</claim-text></claim><claim id="CLM-00033" num="00033"><claim-text><b>33</b>. (canceled)</claim-text></claim></claims></us-patent-application>