<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004776A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004776</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17782079</doc-number><date>20191205</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>04</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>62</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>04</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>6262</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">MODERATOR FOR IDENTIFYING DEFICIENT NODES IN FEDERATED LEARNING</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Telefonaktiebolaget LM Ericsson (publ)</orgname><address><city>Stockholm</city><country>SE</country></address></addressbook><residence><country>SE</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>SATHEESH KUMAR</last-name><first-name>Perepu</first-name><address><city>Chennai</city><country>IN</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>M</last-name><first-name>Saravanan</first-name><address><city>Chennai</city><country>IN</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>Telefonaktiebolaget LM Ericsson (publ)</orgname><role>03</role><address><city>Stockholm</city><country>SE</country></address></addressbook></assignee></assignees><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/IN2019/050883</doc-number><date>20191205</date></document-id><us-371c12-date><date>20220602</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A method for detecting and reducing the impact of deficient nodes in a machine learning system is provided. The method includes receiving a local model update from a first local client node; determining a change in accuracy caused by the local model update; determining that the change in accuracy is below a first threshold; and in response to determining that the change in accuracy is below the first threshold, sending a request to the first local client node signaling the first local client node to compress local model updates.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="104.48mm" wi="137.33mm" file="US20230004776A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="125.31mm" wi="126.66mm" file="US20230004776A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="166.45mm" wi="116.50mm" file="US20230004776A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="211.24mm" wi="143.09mm" file="US20230004776A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="115.06mm" wi="139.36mm" file="US20230004776A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="121.24mm" wi="138.01mm" file="US20230004776A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="153.59mm" wi="132.00mm" file="US20230004776A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="73.07mm" wi="82.80mm" file="US20230004776A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0002" num="0001">Disclosed are embodiments related to federated learning using a moderator.</p><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">Recently, machine learning has led to major breakthroughs in various areas, such as natural language processing, computer vision, speech recognition, and Internet of Things. Machine learning can be advantageous for tasks related to automation and digitalization. Much of the success of machine learning has been based on collecting and processing large amounts of data in a suitable environment. For some applications of machine learning, the amount and types of data collected can be implicate serious privacy concerns.</p><p id="p-0004" num="0003">For example, consider the case of a speech recognition task, where the object is to predict the next word uttered by the user. This is very specific to the particular user and generalizing requires data to be transferred to the cloud from the user. This can cause privacy concerns and possibly generate doubt or distrust in the end users. Other examples of sensitive data involve applications touching medical data, financial records, or location (or tracking) information.</p><p id="p-0005" num="0004">One recent approach to managing user privacy with machine learning is the introduction of the federated learning approach. Federated learning is a new approach to machine learning where the training data does not leave the users' computer at all. Instead of sharing data, users compute weight updates themselves using locally available data stored on local client nodes or computing devices, and then share those weight updates (not the underlying data) with a central server node or computing device. Federated learning is therefore a way of training a central model without a central server node having to directly inspect users' local data. Federated learning is a collaborative form of machine learning where the training and evaluation process is distributed among many users, taking place on local client nodes. A central server node has the role of coordinating everything, but most of the work is not performed by the central server node but instead by a federation of distributed users operating local client nodes.</p><p id="p-0006" num="0005">In typical federated learning approaches, after a central model is initialized, a certain number of local client nodes are randomly selected to improve the central model. Each sampled local client node receives the current central model from the central server node; and each sampled local client node uses its locally available data to compute an update to that model. All of these local updates are then sent back to the central server node where they are combined (e.g., by averaging, weighted by the number of training examples that the local nodes used). The central server node then applies this combined update to the central model, typically (in the case of neural network models) by using some form of gradient descent.</p><p id="p-0007" num="0006">Neural networks commonly have millions of parameters. Sending updates for so many values to a central server node may lead to an inordinate communication cost, especially as the number of users and iterations of training increases. Thus, a naive approach to sharing weight updates is not feasible for larger models. Since uploads are typically much slower than downloads, it is acceptable that local client nodes have to download the full, uncompressed current model. For sending updates, however, local client nodes may be required to use compression methods.</p><p id="p-0008" num="0007">Both lossless and lossy compression methods can be used. Other approaches to managing updates (in addition to, or as alternatives to compression) can also be used, such as only sending updates when a good network connection is available. Additionally, specialized compression techniques for federated learning may be applied. For example, because in some methods of federated learning only a combined update (e.g., averaged over each of the local updates) is required to compute the updated central model, federated-learning specific compression methods may try to encode updates with fewer bits while keeping the combined update (e.g., average) stable. In this circumstance, it may therefore be acceptable that individual updates are compressed in a lossy manner, as long as the overall combination (e.g., average) does not change too much.</p><p id="p-0009" num="0008">Compression algorithms for federated learning can generally be put into two classes: &#x201c;sketched&#x201d; updates and &#x201c;structured&#x201d; updates. Sketched updates refer to when local client nodes compute a normal weight update and perform a compression after the update is computed. The compressed update is often an unbiased estimator of the true update, meaning they are the same on average (e.g., probabilistic optimization). Structured updates refer to when local client nodes perform compression as part of generating the update. For example, the update may be restricted to be of a specific form that allows for an efficient compression. As one example, the updates might be forced to be sparse or low-rank. The optimization then finds the best possible update of this form.</p><p id="p-0010" num="0009">There are no strong guarantees about which method (&#x201c;sketched&#x201d; or &#x201c;structure&#x201d;) works the best. In the general case, it depends heavily on a particular application and the distributions of the updates at the local client nodes. Like in many parts of machine learning, different methods can be tested and are compared empirically.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0011" num="0010">In a typical federated learning implementation, it is difficult to select local client nodes or computing devices to provide updates to the central model. In the worst case, assume that a user is malicious, and actively wants to update the central global model with low quality data from the malicious user's local client node or computing device. In this case, the accuracy of the central model decreases, and this decrease will affect other users also, since the central model is shared with the local client nodes or computing devices of all users. Current approaches attempt to handle this situation by either discarding the updates from the malicious user or using an optimization framework to lessen the effect of a malicious user's updates of the central model. However, in both cases, the malicious user is identified based on the history of the user. This approach is problematic because, for example, it takes time to identify a malicious user, and it also fails to account for non-malicious users who may have occasional periods of poor, deficient or unusual quality data (such that the data does not improve other users' performance).</p><p id="p-0012" num="0011">Embodiments disclosed herein are applicable to not just malicious users, but also users who are not malicious but may have poor, deficient or unusual quality data (such that the data does not improve other users' performance). For example, a user's local data may degrade its own model. Additionally, while a user's local data may locally improve its own performance, if the data is too specific or not generally applicable to other users, the data could cause degradation of the central model. Since, a user's local data is continually being added to, over time the data may become better, and more generally applicable to other users, such that the data may be considered as of good quality. Therefore, completely discarding the user may not be an optimal approach. But treating the user having poor or deficient quality data as any other user may not be optimal either, since a user's data in such circumstances can degrade the central model. Embodiments address this problem by compressing the updates of users with poor or deficient quality data before sending the updates to the global model. Embodiments also differentiate between malicious users, who may want to actively upload bad updates, and poor performing or deficient users, who may inadvertently upload updates that would degrade the central model.</p><p id="p-0013" num="0012">Another problem with typical federated learning approaches, is that the compression such approaches use to compress local model updates can lose much of the important information in the update. Embodiments disclosed herein also provide for improved compression methods for local model updates. Such embodiments may include computing which neurons are firing the most (e.g., contributing the most to the model), selecting those neurons, and sending these selected neurons as the compressed local model update. In this way, the effect of the updates can be maximized on the central model, and bandwidth needed for transmission of the full update is also saved.</p><p id="p-0014" num="0013">According to a first aspect, a method for detecting and reducing the impact of deficient (or poor-performing) nodes in a machine learning system (e.g., a federated learning system) is provided. The method includes: receiving a local model update from a first local client node; determining a change in accuracy caused by the local model update; determining that the change in accuracy is below a first threshold; and in response to determining that the change in accuracy is below the first threshold, sending a request to the first local client node signaling the first local client node to compress local model updates.</p><p id="p-0015" num="0014">In some embodiments, the method is performed by a moderator node interposed between the first local client node and a central server node controlling the machine learning system. In some embodiments, the method further includes sending a representation of the local model update to a central server node. In some embodiments, the method further includes receiving a compressed representation of the local model update from the first local client node, and wherein the representation of the local model update sent to the central server node comprises the compressed representation.</p><p id="p-0016" num="0015">In some embodiments, the method further includes: receiving additional local model updates from the first local client node; determining additional changes in accuracy caused by the additional local model updates; determining that the additional changes in accuracy corresponding to a number of the additional local model updates are below the first threshold, wherein the number of the additional local model updates exceeds a second threshold; and in response to determining that the additional changes in accuracy corresponding to the number of the additional local model updates are below the first threshold, treating the first local client node as malicious such that local model updates from the first local client node are not sent to the central server node.</p><p id="p-0017" num="0016">In some embodiments, the method further includes determining a level of compression, wherein the request includes an indication of the level of compression. In some embodiments, determining a level of compression comprises running a machine learning model. In some embodiments, the request comprises an indication of a compression process. In some embodiments, the compression process comprises choosing a set of top-scoring neurons. In some embodiments, the compression process comprises the method according to any one of the embodiments of the second aspect.</p><p id="p-0018" num="0017">According to a second aspect, a method for a local client node participating in a machine learning system (e.g., a federated learning system) for compressing a local model of the local client node is provided. The method includes: for each sample s of a plurality of training samples, obtaining an output mapping M<sub>s </sub>such that for a given neuron n of layer l in the local model, M<sub>s</sub>(n, l) corresponds to the output of the given neuron n of layer l; obtaining a combined output mapping M such that for a given neuron n of layer l in the local model, M(n, l) corresponds to a combined output of the given neuron n of layer l; and selecting a subset of neurons based on the combined output mapping M.</p><p id="p-0019" num="0018">In some embodiments, the combined output M(n, l) of the given neuron n of layer l is an average of M<sub>s</sub>(n, l) for each sample s of the plurality of training samples. In some embodiments, selecting a subset of neurons based on the combined output mapping M comprises selecting the top x neurons having the highest combined output. In some embodiments, the method further includes sending the selected subset of neurons to a central server node as a compressed representation of the local model.</p><p id="p-0020" num="0019">According to a third aspect, a moderator node for detecting and reducing the impact of deficient (or poor-performing) nodes in a machine learning system (e.g., a federated learning system) is provided. The moderator node includes a memory; and a processor. The processor is configured to: receive a local model update from a first local client node; determine a change in accuracy caused by the local model update; determine that the change in accuracy is below a first threshold; and in response to determining that the change in accuracy is below the first threshold, send a request to the first local client node signaling the first local client node to compress local model updates.</p><p id="p-0021" num="0020">According to a fourth aspect, a local client node participating in a machine learning system (e.g., a federated learning system) is provided. The local client node includes a memory; and a processor. The processor is configured to: for each sample s of a plurality of training samples, obtain an output mapping M<sub>s </sub>such that for a given neuron n of layer l in the local model, M<sub>s</sub>(n, l) corresponds to the output of the given neuron n of layer l; obtain a combined output mapping M such that for a given neuron n of layer l in the local model, M(n, l) corresponds to a combined output of the given neuron n of layer l; and select a subset of neurons based on the combined output mapping M.</p><p id="p-0022" num="0021">According to a fifth aspect, a computer program is provided comprising instructions which when executed by processing circuitry causes the processing circuitry to perform the method of any one of the embodiments of the first or second aspects.</p><p id="p-0023" num="0022">According to a sixth aspect, a carrier is provided containing the computer program of the fifth aspect, wherein the carrier is one of an electronic signal, an optical signal, a radio signal, and a computer readable storage medium.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0024" num="0023">The accompanying drawings, which are incorporated herein and form part of the specification, illustrate various embodiments.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a federated learning system according to an embodiment.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates a flow chart according to an embodiment.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates a message diagram according to an embodiment.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flow chart according to an embodiment.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flow chart according to an embodiment.</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a block diagram of an apparatus according to an embodiment.</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a block diagram of an apparatus according to an embodiment.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a system <b>100</b> of machine learning according to an embodiment. As shown, a central server node or computing device <b>102</b> is in communication with one or more local client nodes or computing devices <b>104</b> via moderator <b>106</b>. Optionally, local client nodes or computing devices <b>104</b> may be in communication with each other utilizing any of a variety of network topologies and/or network communication systems. For example, local client nodes <b>104</b> include user computing devices such as a smart phone, tablet, laptop, personal computer, and so on, and may also be communicatively coupled through a common network such as the Internet (e.g., via WiFi) or a communications network (e.g., LTE or 5G). Central server nodes <b>104</b> may include computing devices such as servers, base stations, mainframes, and cloud computing resources. While a central server node or computing device <b>102</b> is shown, the functionality of central server node <b>102</b> may be distributed across multiple nodes, and may be shared between one or more of local client nodes <b>104</b>.</p><p id="p-0033" num="0032">Moderator <b>106</b> may sit between the central server node <b>102</b> and the local client nodes <b>104</b>. Moderator <b>106</b> may be a separate entity, or it may be part of central server node <b>102</b>. As shown, each local client node <b>104</b> may communicate model updates to moderator <b>106</b>, moderator <b>106</b> may communicate with central server node <b>102</b>, and central server node <b>102</b> may send the updated central model to the local client nodes <b>104</b> through moderator <b>106</b>. The link between local client nodes <b>104</b> and moderator <b>106</b> is shown as being bidirectional between those entities (e.g. with a two-way link, or through a different communication channel). Although not shown, there may be a direct communication link between central server node <b>102</b> and local client nodes <b>104</b>.</p><p id="p-0034" num="0033">Federated learning as described in embodiments herein may involve one or more rounds, where a central model is iteratively trained in each round. Local client nodes <b>104</b> may register with the central server node <b>102</b> to indicate their willingness to participate in the federated learning of the central model, and may do so continuously or on a rolling basis. Upon registration (and potentially at any time thereafter), the central server node <b>102</b> transmit training parameters to local client nodes <b>104</b>. The central server node <b>102</b> may transmit an initial model to the local client nodes <b>104</b>. For example, the central server node <b>102</b> may transmit to the local client nodes <b>104</b> a central model (e.g., newly initialized or partially trained through previous rounds of federated learning). The local client nodes <b>104</b> may train their individual models locally with their own data. The results of such local training may then be reported back to central server node <b>102</b>, which may pool the results and update the global model. Reporting back to the central server node <b>102</b> may be mediated by moderator <b>106</b>. This process may be repeated iteratively. Further, at each round of training the central model, central server node <b>102</b> may select a subset of all registered local client nodes <b>104</b> (e.g., a random subset) to participate in the training round.</p><p id="p-0035" num="0034">Embodiments provide a new federated learning approach that effectively handle both malicious users and users with poor or deficient quality data. In some embodiments, a moderator node <b>106</b> sits between the central server node <b>102</b> (which handles updates to the central model) and local client nodes <b>104</b> (which individually handle updates to their respective local models). The moderator node <b>106</b> may monitor the incoming local model updates from the local client nodes <b>104</b>; the moderator <b>106</b> may also check the authenticity and quality of the local client node <b>104</b> and the data from the local client node <b>104</b>.</p><p id="p-0036" num="0035">In some embodiments, the moderator <b>106</b> may accept all local model updates that it receives from local client nodes <b>104</b> during an initial phase. The moderator may keep its own cached version of the central model, separate from that maintained by the central server node <b>102</b>. The updates that the moderator <b>106</b> receives from local client nodes <b>104</b> may be used to update the moderator's <b>106</b> cached version of the central model. The moderator <b>106</b> may, after updating its cached version of the central model with one or more local model updates, select local client nodes <b>104</b> (e.g., randomly, based on a trusted list of local client nodes <b>104</b>, or otherwise) and send the moderator's <b>106</b> updated version of the central model to those selected local client nodes <b>104</b>. Those selected local client nodes <b>104</b> may then report back to the moderator <b>106</b> on how their respective local models performed with their local data. This is one example for how the moderator <b>106</b> may determine a change in accuracy caused by one or more local model updates.</p><p id="p-0037" num="0036">The moderator <b>106</b> may use various techniques to determine a change in accuracy. For example, the moderator <b>106</b> may average the changes in accuracy at each of the local client nodes <b>104</b> selected to report on the accuracy, the moderator <b>106</b> may weigh the average based on the history of such local client nodes <b>104</b>, the moderator <b>106</b> may discount outliers, and so on. For example, the moderator <b>106</b> may determine if an accuracy of most or all of the selected local client nodes <b>104</b> is decreased, or if there is a mixed result such that some have increased and some decreased. Accordingly, moderator <b>106</b> may determine a change in accuracy, which may be a scalar value indicating direction (e.g., increase or decrease), and moderator <b>106</b> may additionally determine other information related to the change in accuracy (e.g., statistical information related to the individual changes in accuracy from the selected local client nodes <b>104</b>).</p><p id="p-0038" num="0037">Depending on the reduction or increase in accuracy, the moderator <b>106</b> may label a certain local client node <b>104</b> as malicious or as performing poorly or deficient. For example, consider the flow chart illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. The moderator <b>106</b> may determine the change in accuracy at <b>202</b>. At <b>204</b>, the moderator <b>106</b> considers the change in accuracy, and determines whether it is below a threshold (e.g., degrading by X % or more, such as by more than 3%). If the change in accuracy is not below the threshold (e.g., the change is positive, or not very negative), then the local client node <b>104</b> may be labeled as a normal user at <b>206</b>. If, on the other hand, the change is below a threshold, then moderator <b>106</b> may consider how often the change has been below a threshold at <b>208</b>. If the local model updates from a given local client node <b>104</b> are frequently poor (e.g., continuously degrading by the threshold amount for N or more iterations, such as N=10), the moderator <b>106</b> may then determine that the local client node <b>104</b> responsible for those poor local model updates is malicious at <b>208</b>. In some embodiments, instead of requiring the poor performance to be continuous, moderator <b>106</b> may label a node as malicious if the poor performance below the threshold has continued for too long by some other metric, such as for 0 of the last N iterations (e.g., 8 of the last 10 iterations). On the other hand, if the local model update is performing poorly, but the local client node <b>104</b> does not rise to the level of being malicious, the moderator <b>106</b> may (e.g., temporarily) label the local client node <b>104</b> as performing poorly at <b>210</b>.</p><p id="p-0039" num="0038">In some embodiments, moderator <b>106</b> may determine additional factors besides the number of times the local client node <b>104</b> has been poorly performing or deficient in making a determination of maliciousness. For example, the moderator <b>106</b> may be able to determine if a local client node's <b>104</b> updates generally perform well for a small subset of the other local client nodes <b>104</b>, but performs poorly or deficiently for most other local client nodes <b>104</b>. This may indicate that the local client node <b>104</b> is not malicious, but may be receiving data that is of unusual or poor or deficient quality for other local client nodes <b>104</b>. This may warrant additional compression of the local client node <b>104</b>, but may not in some embodiments warrant completely discarding that local client node's <b>104</b> updates.</p><p id="p-0040" num="0039">In some embodiments, if a local client node <b>104</b> is identified as malicious, then the moderator <b>106</b> does not accept local model updates from that local client node <b>104</b>, and does not send such local model updates to the central server node <b>102</b> for updating the central model. In some embodiments, if a local client node <b>104</b> is identified as performing poorly or deficiently (but is not malicious), the local client node <b>104</b> will be requested to send a compressed version of its local model updates (e.g., to moderator <b>106</b> or to central server node <b>102</b>). In some embodiments, the moderator <b>106</b> may compress the local model updates of the local client node <b>104</b> and send the compressed version to the central server node <b>102</b> itself.</p><p id="p-0041" num="0040">In some embodiments, the type of compression requested from the local client node <b>104</b> that is identified as performing poorly or deficiently is to have the local client node <b>104</b> send only top firing neurons to update the central model instead of all the weights. This is a type of structured compression as the model will update only with subset of weights. The moderator <b>106</b> may, in some embodiments, decide on the nature and level of the compression, such as how many weights need to be updated and how many weights need to be discarded. This information may be included in the request that the moderator <b>106</b> sends to the local client node <b>104</b>.</p><p id="p-0042" num="0041">Such compression may also be useful more generally in the case of local client nodes <b>104</b> who have low bandwidth to send local model updates.</p><p id="p-0043" num="0042">To identify compression parameters (e.g., the level of compression to be used), a machine-learning model may be used. The machine-learning model may take additional factors of the local client node <b>104</b> into account to decide on the level of compression. In some embodiments, the level of compression may be determined based at least in part on the change in accuracy. For example, some embodiments may initially determine the level of compression based on the change in accuracy, and then switch to using the machine-learning model after it has seen enough data to be suitably trained.</p><p id="p-0044" num="0043">In some embodiments, the compression method of updating only the most firing neurons may proceed in the following manner. As an initial matter, any neuron output can be represented by the equation below:</p><p id="p-0045" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>y</i>=&#x192;(&#x3a3;<i>wx+b</i>)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0046" num="0000">where w represents the weights of the neurons in the previous layer, b represents the bias of the neurons, and &#x192; represents the activation function. In the equation, x represents the input to the given neuron. In the first hidden layer, this (x) will be the input to the network; in subsequent layers, this (x) will be the output of previous hidden layer. With this background, the compression method (for compressing an update to a given local model) will now be described.</p><p id="p-0047" num="0044">1. First, for one sample in the training data, the local model is trained, such that the model weights are obtained. In addition, every neuron output in every layer of the local model is also obtained. This collection of neuron outputs is referred to here as an output mapping; the output mapping maps a given neuron n of layer l to a specific output for a given sample s. For example, the outputs for one sample may be noted as in the following table:</p><p id="p-0048" num="0000"><tables id="TABLE-US-00001" num="00001"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="1" colwidth="91pt" align="center"/><colspec colname="2" colwidth="28pt" align="center"/><colspec colname="3" colwidth="98pt" align="center"/><thead><row><entry namest="1" nameend="3" align="center" rowsep="1"/></row><row><entry>Layer</entry><entry>Neuron</entry><entry>Output</entry></row><row><entry namest="1" nameend="3" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry>1</entry><entry>1</entry><entry>0.1</entry></row><row><entry>1</entry><entry>2</entry><entry>0.8</entry></row><row><entry>2</entry><entry>1</entry><entry>0.7</entry></row><row><entry namest="1" nameend="3" align="center" rowsep="1"/></row></tbody></tgroup></table></tables><br/>(As previously noted, a neural network may have millions of parameters, resulting in the above table being much larger than shown. Additionally, a local client node <b>104</b> may store an output mapping as above in any suitable format.)</p><p id="p-0049" num="0045">2. This is repeated for every training sample that the local client node <b>104</b> has, resulting in tables (output mappings) as shown above for each of the training samples.</p><p id="p-0050" num="0046">3. A combined output mapping is then obtained from each of the sample-specific output mappings. For example, the combined output mapping may take the average output for each neuron n of layer l, averaged over all of the samples s of the training data. Other methods of combining the sample-specific output mappings may also be used.</p><p id="p-0051" num="0047">4. From the combined output mapping, the top-performing neurons are selected. For example, the top N neurons based on the highest combined output value may be selected (e.g., N=10, 20).</p><p id="p-0052" num="0048">5. These selected neurons then represent the most firing neurons for the local model.</p><p id="p-0053" num="0049">There are various ways of controlling the level of compression using the most firing neurons approach. For example, one parameter to decide is how many weights to update in the model. Embodiments, for example, may cover X % (e.g., 50%) of the neuron weights which cover X % (e.g., 50%) of the weights in the local model, rather than updating the entire local model. This can result in some of the updates being learned by the central model, rather than everything from the local client node <b>104</b> being discarded. In this way, the central model can learn some of the local model updates from the local model.</p><p id="p-0054" num="0050">The level of compression may be an important parameter to control, as it can impact the affect that poor performing or deficient nodes have on the central model, as well as the impact that malicious nodes can have prior to their detection as being malicious. In some embodiments, when a poor performing or deficient node is detected, the level of compression may increase for that node as it continues to send poor performing or deficient updates. For example, it may take N iterations before determining that a given node is malicious, and the compression level for that node may increase at each iteration until the node is finally identified as malicious and updates from that node are no longer accepted. On the other hand, if a poor performing or deficient node starts to have good (or better) model updates (that is, if the change in accuracy is not as bad as previously, or even has a positive impact on accuracy), then the level of compression may be reduced until the node is no longer considered a poor performing or deficient node and is not required to send compressed updates. In general, the level of compression may be selected manually, it may be selected based on a set of predetermined rules, or it may be selected based on a machine-learning model evaluating a number of different input parameters.</p><p id="p-0055" num="0051">As an example of determining the level of compression, consider the case where a local client node <b>104</b> sends an update of its local model to moderator which <b>106</b>, and the update decreases the accuracy by p % (as reported to moderator <b>106</b> by other local client nodes <b>104</b>). The moderator <b>106</b> may penalize the local client node <b>104</b> by requiring the local client node <b>104</b> to compress its local model update by p % (the same amount as the drop in accuracy). In some embodiments, this compression may involve the local client node <b>104</b> collecting the most firing neurons which cover p % of the local model to determine the local model update. In some embodiments, the level of compression may be proportional to the change in accuracy, optionally capped at a certain value. For instance, the level of compression may be max(X %, k*p %), where X and k can be any value, e.g. X=60, and k=2.</p><p id="p-0056" num="0052">As an example of compressing the local model update, for instance to cover a particular percentage of the local model, consider the case where there are 100 neurons in the local model, and where summing the absolute values of all weights results in 96.5. In this case, in order to compress the local model update by 50%, a local client node <b>104</b> may add the absolute values of weights starting from most firing neurons until the absolute sum of weights reaches or exceeds 48.25 (i.e. 96.5*50%). The local client node <b>104</b> may then send only these neuron weights (i.e., only the most firing neurons contributing to the sum) as a local model update. In this way, the central model may be able to learn something from the local model characteristics (that is, the local model update is not entirely discarded), but the impact from a poor performing update may be muted.</p><p id="p-0057" num="0053">In some embodiments, the level of compression may be determined by a machine-learning model. This model may need a sufficient amount of data points in order to perform optimally, and therefore in some embodiments a different method (such as a rule-based method using the change in accuracy to determine the level of compression) may be used during the initial training period. The model used may accept a number of inputs, such as a change in accuracy, the number of weights used in the local model, the location of the local client node <b>104</b>, the trustworthiness of the local client node <b>104</b>, and so on. The model may take the form of any type of machine learning model, including a neural network model, a Classification and Regression Tree (CART) model, and so on. The machine learning model allows for the level of compression to be determined dynamically.</p><heading id="h-0006" level="1">Example</heading><p id="p-0058" num="0054">An example was created using one of the embodiments disclosed herein. The example involved a central model implemented for the application of a keyword-prediction task using a long short-term memory (LSTM) models in all the local models. The neural network used in the models consisted of three hidden layers with ten nodes in each of the layers. The model was used to predict the next keyword based on the previous nine keywords. To train the models, the Google keyword prediction public dataset was used.</p><p id="p-0059" num="0055">In the example, ten local client nodes were used. The training data was divided into ten equal parts, one for each of the local client nodes. After ten iterations, the accuracy of the central model reached 82%. In one of the local client nodes, in order to test the effect of an embodiment, bad data was used, such that the data was forced to be independent and identically distributed. If local model updates from the local client node with the bad data are discarded completely (as current approaches to detecting malicious users would do), the accuracy drops to 81%. However, when using an embodiment disclosed herein, where the poor performing or deficient node is forced to compress its local model updates, the accuracy is increased to 84%. Accordingly, it can be advantageous to allow poor performing or deficient nodes to update the central model where those updates are compressed.</p><p id="p-0060" num="0056"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates a message flow diagram according to an embodiment. At <b>310</b>, a first local client node <b>104</b> sends updates to moderator <b>106</b>. Moderator <b>106</b> may update its cache of the central model using that update, and at <b>312</b>, query other local client nodes about the accuracy of the updated central model. As shown, moderator <b>106</b> queries the second local client node <b>104</b>, but moderator <b>106</b> may also query additional local client nodes <b>104</b>, and may select the set of local client nodes to query based on information that moderator <b>106</b> has regarding the local client nodes <b>104</b>. In this example, moderator <b>106</b> determines that the change in accuracy of the local updates <b>310</b> are beneficial, and moderator <b>106</b> then forwards the local updates at <b>314</b> to the central server node <b>102</b>. In some embodiments, moderator <b>106</b> may instruct the first local client node <b>104</b> to send the local updates to the central server node <b>102</b>.</p><p id="p-0061" num="0057">At <b>316</b>, the second local client node <b>104</b> sends updates to moderator <b>106</b>. Moderator <b>106</b> may update its cache of the central model using that update, and at <b>318</b>, query other local client nodes about the accuracy of the updated central model. As shown, moderator <b>106</b> queries the first local client node <b>104</b>, but as noted above moderator <b>106</b> may also query additional local client nodes <b>104</b>. In this example, moderator <b>106</b> determines at <b>320</b> that the change in accuracy is below a threshold. In this example, moderator <b>106</b> determines that the second local client node is a poor performing or deficient node, but is not identified as being malicious at this time. Accordingly, moderator <b>106</b> sends a compression request to the second local client node <b>104</b> at <b>322</b>. The compression request may indicate the type of compression and level of compression that the second local client node <b>104</b> should apply to its local model updates. After receiving the compression request, the second local client node <b>104</b> sends a compressed local model update to central server node <b>102</b> at <b>324</b>.</p><p id="p-0062" num="0058"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates a flow chart according to an embodiment. Process <b>400</b> is a method for detecting and reducing the impact of poor-performing or deficient nodes in a machine learning system (e.g., a federated learning system). Process <b>400</b> may begin with step s<b>402</b>.</p><p id="p-0063" num="0059">Step s<b>402</b> comprises receiving a local model update from a first local client node <b>104</b>.</p><p id="p-0064" num="0060">Step s<b>404</b> comprises determining a change in accuracy caused by the local model update.</p><p id="p-0065" num="0061">Step s<b>406</b> comprises determining that the change in accuracy is below a first threshold.</p><p id="p-0066" num="0062">Step s<b>402</b> comprises, in response to determining that the change in accuracy is below the first threshold, sending a request to the first local client node signaling the first local client node <b>104</b> to compress local model updates.</p><p id="p-0067" num="0063">In some embodiments, the method is performed by a moderator node <b>106</b> interposed between the first local client node <b>104</b> and a central server node <b>102</b> controlling the federated learning system. In some embodiments, the method further includes sending a representation of the local model update to a central server node <b>102</b>. In some embodiments, the method further includes receiving a compressed representation of the local model update from the first local client node <b>104</b>, and wherein the representation of the local model update sent to the central server node <b>102</b> comprises the compressed representation.</p><p id="p-0068" num="0064">In some embodiments, the method further includes: receiving additional local model updates from the first local client node <b>104</b>; determining additional changes in accuracy caused by the additional local model updates; determining that the additional changes in accuracy corresponding to at a number of the additional local model updates are below the first threshold, wherein the number of the additional local model updates exceeds a second threshold; and in response to determining that the additional changes in accuracy corresponding to the number of the additional local model updates are below the first threshold, treating the first local client node <b>104</b> as malicious such that local model updates from the first local client node <b>104</b> are not sent to the central server node <b>102</b>.</p><p id="p-0069" num="0065">In some embodiments, the method further includes determining a level of compression, wherein the request includes an indication of the level of compression. In some embodiments, determining a level of compression comprises running a machine learning model. In some embodiments, the request comprises an indication of a compression process. In some embodiments, the compression process comprises choosing a set of top-scoring neurons. In some embodiments, the compression process comprises the method according to any one of the embodiments described with respect to <figref idref="DRAWINGS">FIG. <b>5</b></figref>, or according to any other compression process described herein.</p><p id="p-0070" num="0066"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates a flow chart according to an embodiment. Process <b>500</b> is a method for a node participating in a machine learning system (e.g., a federated learning system) for compressing a local model of the node. Process <b>500</b> may begin with step S<b>402</b>.</p><p id="p-0071" num="0067">Step s<b>502</b> comprises, for each sample s of a plurality of training samples, obtaining an output mapping M<sub>s </sub>such that for a given neuron n of layer l in the local model, M<sub>s</sub>(n, l) corresponds to the output of the given neuron n of layer l.</p><p id="p-0072" num="0068">Step s<b>504</b> comprises obtaining a combined output mapping M such that for a given neuron n of layer l in the local model, M (n, l) corresponds to a combined output of the given neuron n of layer l.</p><p id="p-0073" num="0069">Step s<b>506</b> comprises selecting a subset of neurons based on the combined output mapping M.</p><p id="p-0074" num="0070">In some embodiments, the combined output M(n, l) of the given neuron n of layer l is an average of M<sub>s</sub>(n, l) for each sample s of the plurality of training samples. In some embodiments, selecting a subset of neurons based on the combined output mapping M comprises selecting the top x neurons having the highest combined output. In some embodiments, the method further includes sending the selected subset of neurons to a central server node as a compressed representation of the local model.</p><p id="p-0075" num="0071"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a block diagram of an apparatus <b>600</b> (e.g., a local client node <b>104</b> and/or central server node <b>102</b> and/or moderator <b>106</b>), according to some embodiments. As shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the apparatus may comprise: processing circuitry (PC) <b>602</b>, which may include one or more processors (P) <b>655</b> (e.g., a general purpose microprocessor and/or one or more other processors, such as an application specific integrated circuit (ASIC), field-programmable gate arrays (FPGAs), and the like); a network interface <b>648</b> comprising a transmitter (Tx) <b>645</b> and a receiver (Rx) <b>647</b> for enabling the apparatus to transmit data to and receive data from other nodes connected to a network <b>610</b> (e.g., an Internet Protocol (IP) network) to which network interface <b>648</b> is connected; and a local storage unit (a.k.a., &#x201c;data storage system&#x201d;) <b>608</b>, which may include one or more non-volatile storage devices and/or one or more volatile storage devices. In embodiments where PC <b>602</b> includes a programmable processor, a computer program product (CPP) <b>641</b> may be provided. CPP <b>641</b> includes a computer readable medium (CRM) <b>642</b> storing a computer program (CP) <b>643</b> comprising computer readable instructions (CRI) <b>644</b>. CRM <b>642</b> may be a non-transitory computer readable medium, such as, magnetic media (e.g., a hard disk), optical media, memory devices (e.g., random access memory, flash memory), and the like. In some embodiments, the CRI <b>644</b> of computer program <b>643</b> is configured such that when executed by PC <b>602</b>, the CRI causes the apparatus to perform steps described herein (e.g., steps described herein with reference to the flow charts). In other embodiments, the apparatus may be configured to perform steps described herein without the need for code. That is, for example, PC <b>602</b> may consist merely of one or more ASICs. Hence, the features of the embodiments described herein may be implemented in hardware and/or software.</p><p id="p-0076" num="0072"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a schematic block diagram of the apparatus <b>600</b> according to some other embodiments. The apparatus <b>600</b> includes one or more modules <b>700</b>, each of which is implemented in software. The module(s) <b>700</b> provide the functionality of apparatus <b>600</b> described herein (e.g., the steps herein, e.g., with respect to <figref idref="DRAWINGS">FIGS. <b>2</b>-<b>5</b></figref>).</p><p id="p-0077" num="0073">While various embodiments of the present disclosure are described herein, it should be understood that they have been presented by way of example only, and not limitation. Thus, the breadth and scope of the present disclosure should not be limited by any of the above-described exemplary embodiments. Moreover, any combination of the above-described elements in all possible variations thereof is encompassed by the disclosure unless otherwise indicated herein or otherwise clearly contradicted by context.</p><p id="p-0078" num="0074">Additionally, while the processes described above and illustrated in the drawings are shown as a sequence of steps, this was done solely for the sake of illustration. Accordingly, it is contemplated that some steps may be added, some steps may be omitted, the order of the steps may be re-arranged, and some steps may be performed in parallel.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method for detecting and reducing the impact of deficient nodes in a machine learning system, the method comprising:<claim-text>receiving a local model update from a first local client node;</claim-text><claim-text>determining a change in accuracy caused by the local model update;</claim-text><claim-text>determining that the change in accuracy is below a first threshold; and</claim-text><claim-text>in response to determining that the change in accuracy is below the first threshold, sending a request to the first local client node signaling the first local client node to compress local model updates.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the method is performed by a moderator node interposed between the first local client node and a central server node controlling the machine learning system, and the method further comprises sending a representation of the local model update to the central server node.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. (canceled)</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. (canceled)</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>receiving additional local model updates from the first local client node;</claim-text><claim-text>determining additional changes in accuracy caused by the additional local model updates;</claim-text><claim-text>determining that the additional changes in accuracy corresponding to a number of the additional local model updates are below the first threshold, wherein the number of the additional local model updates exceeds a second threshold; and</claim-text><claim-text>in response to determining that the additional changes in accuracy corresponding to the number of the additional local model updates are below the first threshold, treating the first local client node as malicious such that local model updates from the first local client node are not sent to the central server node.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising determining a level of compression by running a machine learning model, wherein the request includes an indication of the level of compression and an indication of a compression process.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. (canceled)</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. (canceled)</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the compression process comprises choosing a set of top-scoring neurons.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. (canceled)</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the machine learning system is a federated learning system.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. A method for a local client node participating in a machine learning system for compressing a local model of the local client node, the method comprising:<claim-text>for each sample s of a plurality of training samples, obtaining an output mapping M<sub>s </sub>such that for a given neuron n of layer l in the local model, M<sub>s</sub>(n, l) corresponds to the output of the given neuron n of layer l;</claim-text><claim-text>obtaining a combined output mapping M such that for a given neuron n of layer l in the local model, M (n, l) corresponds to a combined output of the given neuron n of layer l;</claim-text><claim-text>selecting a subset of neurons based on the combined output mapping M; and</claim-text><claim-text>sending the selected subset of neurons to a central server node as a compressed representation of the local model.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the combined output M(n, l) of the given neuron n of layer l is an average of M<sub>s</sub>(n, l) for each sample s of the plurality of training samples.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein selecting a subset of neurons based on the combined output mapping M comprises selecting the top x neurons having the highest combined output.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. (canceled)</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the machine learning system is a federated learning system.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. A moderator node for detecting and reducing the impact of deficient nodes in a machine learning system, the moderator node comprising:<claim-text>a memory; and</claim-text><claim-text>a processor, wherein said processor is configured to:</claim-text><claim-text>receive a local model update from a first local client node;</claim-text><claim-text>determine a change in accuracy caused by the local model update;</claim-text><claim-text>determine that the change in accuracy is below a first threshold; and</claim-text><claim-text>in response to determining that the change in accuracy is below the first threshold, send a request to the first local client node signaling the first local client node to compress local model updates.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The moderator node of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the moderator node is interposed between the first local client node and a central server node controlling the machine learning system and the processor is further configured to send a representation of the local model update to a central server node.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. (canceled)</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. (canceled)</claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. The moderator node of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the processor is further configured to:<claim-text>receive additional local model updates from the first local client node;</claim-text><claim-text>determine additional changes in accuracy caused by the additional local model updates;</claim-text><claim-text>determine that the additional changes in accuracy corresponding to a number of the additional local model updates are below the first threshold, wherein the number of the additional local model updates exceeds a second threshold; and</claim-text><claim-text>in response to determining that the additional changes in accuracy corresponding to the number of the additional local model updates are below the first threshold, treat the first local client node as malicious such that local model updates from the first local client node are not sent to the central server node.</claim-text></claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. The moderator node of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the processor is further configured to determine a level of compression by running a machine learning model, wherein the request includes an indication of the level of compression and an indication of a compression process.</claim-text></claim><claim id="CLM-00023" num="00023"><claim-text><b>23</b>. (canceled)</claim-text></claim><claim id="CLM-00024" num="00024"><claim-text><b>24</b>. (canceled)</claim-text></claim><claim id="CLM-00025" num="00025"><claim-text><b>25</b>. The moderator node of <claim-ref idref="CLM-00022">claim 22</claim-ref>, wherein the compression process comprises choosing a set of top-scoring neurons.</claim-text></claim><claim id="CLM-00026" num="00026"><claim-text><b>26</b>. (canceled)</claim-text></claim><claim id="CLM-00027" num="00027"><claim-text><b>27</b>. The moderator node of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the machine learning system is a federated learning system.</claim-text></claim><claim id="CLM-00028" num="00028"><claim-text><b>28</b>. A local client node participating in a machine learning system, the local client node comprising:<claim-text>a memory; and</claim-text><claim-text>a processor, wherein said processor is configured to:</claim-text><claim-text>for each sample s of a plurality of training samples, obtain an output mapping M<sub>s </sub>such that for a given neuron n of layer l in the local model, M<sub>s</sub>(n, l) corresponds to the output of the given neuron n of layer l;</claim-text><claim-text>obtain a combined output mapping M such that for a given neuron n of layer l in the local model, M (n, l) corresponds to a combined output of the given neuron n of layer l;</claim-text><claim-text>select a subset of neurons based on the combined output mapping M; and</claim-text><claim-text>send the selected subset of neurons to a central server node as a compressed representation of the local model.</claim-text></claim-text></claim><claim id="CLM-00029" num="00029"><claim-text><b>29</b>. The local client node of <claim-ref idref="CLM-00028">claim 28</claim-ref>, wherein the combined output M(n, l) of the given neuron n of layer l is an average of M<sub>s</sub>(n, l) for each sample s of the plurality of training samples.</claim-text></claim><claim id="CLM-00030" num="00030"><claim-text><b>30</b>. The local client node of <claim-ref idref="CLM-00028">claim 28</claim-ref>, wherein selecting a subset of neurons based on the combined output mapping M comprises selecting the top x neurons having the highest combined output.</claim-text></claim><claim id="CLM-00031" num="00031"><claim-text><b>31</b>. (canceled)</claim-text></claim><claim id="CLM-00032" num="00032"><claim-text><b>32</b>. The local client node of <claim-ref idref="CLM-00028">claim 28</claim-ref>, wherein the machine learning system is a federated learning system.</claim-text></claim><claim id="CLM-00033" num="00033"><claim-text><b>33</b>. (canceled)</claim-text></claim><claim id="CLM-00034" num="00034"><claim-text><b>34</b>. (canceled)</claim-text></claim></claims></us-patent-application>