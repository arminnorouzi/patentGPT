<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005481A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005481</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17943205</doc-number><date>20220913</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>JP</country><doc-number>2018-069788</doc-number><date>20180330</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>22</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20130101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>25</main-group><subgroup>84</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>63</class><subclass>H</subclass><main-group>11</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>63</class><subclass>H</subclass><main-group>30</main-group><subgroup>02</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>22</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>25</main-group><subgroup>84</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>63</class><subclass>H</subclass><main-group>11</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>083</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>63</class><subclass>H</subclass><main-group>30</main-group><subgroup>02</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>2015</main-group><subgroup>223</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">INFORMATION PROCESSOR, INFORMATION PROCESSING METHOD, AND PROGRAM</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16976493</doc-number><date>20200828</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11468891</doc-number></document-id></parent-grant-document><parent-pct-document><document-id><country>WO</country><doc-number>PCT/JP2019/006580</doc-number><date>20190221</date></document-id></parent-pct-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17943205</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Sony Group Corporation</orgname><address><city>Tokyo</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>SAWATA</last-name><first-name>Ryosuke</first-name><address><city>Kanagawa</city><country>JP</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>KOYAMA</last-name><first-name>Yuichiro</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>Sony Group Corporation</orgname><role>03</role><address><city>Tokyo</city><country>JP</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">An information processor including: an operation control unit that controls a motion of an autonomous mobile body acting on the basis of recognition processing, in a case where a target sound that is a target voice for voice recognition processing is detected, the operation control unit moving the autonomous mobile body to a position, around an approach target, where an input level of a non-target sound that is not the target voice becomes lower, the approach target being determined on the basis of the target sound.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="133.60mm" wi="143.17mm" file="US20230005481A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="141.56mm" wi="149.61mm" file="US20230005481A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="144.36mm" wi="139.36mm" file="US20230005481A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="157.65mm" wi="112.61mm" file="US20230005481A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="215.90mm" wi="132.50mm" orientation="landscape" file="US20230005481A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="219.96mm" wi="101.85mm" orientation="landscape" file="US20230005481A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="199.98mm" wi="127.17mm" orientation="landscape" file="US20230005481A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="222.42mm" wi="124.97mm" orientation="landscape" file="US20230005481A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="172.04mm" wi="154.60mm" orientation="landscape" file="US20230005481A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="187.88mm" wi="154.86mm" orientation="landscape" file="US20230005481A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="199.39mm" wi="143.59mm" orientation="landscape" file="US20230005481A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="189.15mm" wi="137.33mm" orientation="landscape" file="US20230005481A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="163.66mm" wi="158.33mm" orientation="landscape" file="US20230005481A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="168.74mm" wi="160.19mm" orientation="landscape" file="US20230005481A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="199.39mm" wi="143.68mm" orientation="landscape" file="US20230005481A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="199.47mm" wi="143.59mm" orientation="landscape" file="US20230005481A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="199.47mm" wi="157.23mm" orientation="landscape" file="US20230005481A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="204.05mm" wi="153.59mm" orientation="landscape" file="US20230005481A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="224.54mm" wi="156.55mm" orientation="landscape" file="US20230005481A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00019" num="00019"><img id="EMI-D00019" he="155.11mm" wi="117.43mm" file="US20230005481A1-20230105-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00020" num="00020"><img id="EMI-D00020" he="220.98mm" wi="151.64mm" file="US20230005481A1-20230105-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00021" num="00021"><img id="EMI-D00021" he="214.12mm" wi="122.43mm" orientation="landscape" file="US20230005481A1-20230105-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00022" num="00022"><img id="EMI-D00022" he="214.21mm" wi="122.60mm" orientation="landscape" file="US20230005481A1-20230105-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00023" num="00023"><img id="EMI-D00023" he="189.74mm" wi="149.78mm" file="US20230005481A1-20230105-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00024" num="00024"><img id="EMI-D00024" he="150.28mm" wi="124.46mm" file="US20230005481A1-20230105-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">The present application is a continuation of U.S. application Ser. No. 16/976,493, filed Aug. 28, 2020, which is based on PCT filing PCT/JP2019/006580, filed Feb. 21, 2019, which claims priority to JP 2018-069788, filed Mar. 30, 2018, the entire contents of each are incorporated herein by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">The present disclosure relates to an information processor, an information processing method, and a program.</p><heading id="h-0003" level="1">BACKGROUND ART</heading><p id="p-0004" num="0003">In recent years, various apparatuses have been developed that estimate a direction of a sound source such as an utterance of a user and execute a motion dependent on the direction of the sound source. Examples of the above-described apparatuses include an autonomous mobile body that executes autonomous movement on the basis of an estimated sound source direction. For example, PTL 1 discloses a technology for moving a robot device toward a direction where a utterance or a face of a user has been recognized.</p><heading id="h-0004" level="1">CITATION LIST</heading><heading id="h-0005" level="1">Patent Literature</heading><p id="p-0005" num="0004">PTL 1: Japanese Unexamined Patent Application Publication No. 2004-130427</p><heading id="h-0006" level="1">SUMMARY OF THE INVENTION</heading><heading id="h-0007" level="1">Problem to be Solved by the Invention</heading><p id="p-0006" num="0005">However, the technology described in PTL 1 does not consider existence of a sound other than the utterance of the user, i.e., noise. For this reason, in a case where the robot device is caused to simply make an approach to an estimated direction of the user, there is a possibility that an input level of the noise increases, thereby causing difficulty in recognition of the utterance of the user.</p><p id="p-0007" num="0006">Therefore, the present disclosure proposes an information processor, an information processing method, and a program that are novel and improved, and make it possible to cause an autonomous mobile body to execute a motion for further improving accuracy of sound recognition.</p><heading id="h-0008" level="1">Means for Solving the Problem</heading><p id="p-0008" num="0007">According to the present disclosure, there is provided an information processor including: an operation control unit that controls a motion of an autonomous mobile body acting on the basis of recognition processing, in a case where a target sound that is a target voice for voice recognition processing is detected, the operation control unit moving the autonomous mobile body to a position, around an approach target, where an input level of a non-target sound that is not the target voice becomes lower, the approach target being determined on the basis of the target sound.</p><p id="p-0009" num="0008">In addition, according to the present disclosure, there is provided an information processing method including causing a processor to: control a motion of an autonomous mobile body acting on the basis of recognition processing, the controlling further including, in a case where a target sound that is a target voice for voice recognition processing is detected, moving the autonomous mobile body to a position, around an approach target, where an input level of a non-target sound that is not the target voice becomes lower, the approach target being determined on the basis of the target sound.</p><p id="p-0010" num="0009">In addition, according to the present disclosure, there is provided a program that causes a computer to function as an information processor, the information processor including an operation control unit that controls a motion of an autonomous mobile body acting on the basis of recognition processing, in a case where a target sound that is a target voice for voice recognition processing is detected, the operation control unit moving the autonomous mobile body to a position, around an approach target, where an input level of a non-target sound that is not the target voice becomes lower, the approach target being determined on the basis of the target sound.</p><heading id="h-0009" level="1">Effect of the Invention</heading><p id="p-0011" num="0010">As described above, according to the present disclosure, it is possible to cause the autonomous mobile body to execute a motion for further improving accuracy of sound recognition.</p><p id="p-0012" num="0011">It is to be noted that above-described effects are not necessarily limitative; in addition to or in place of the above effects, there may be achieved any of the effects described in the present specification or other effects that may be grasped from the present specification.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0010" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a hardware configuration example of an autonomous mobile body according to an embodiment of the present disclosure.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a configuration example of actuators included in the autonomous mobile body according to the same embodiment.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is an explanatory diagram of an operation of the actuator included in the autonomous mobile body according to the same embodiment.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is an explanatory diagram of the operation of the actuator included in the autonomous mobile body according to the same embodiment.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is an explanatory diagram of functions of a display included in the autonomous mobile body according to the same embodiment.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates motion examples of the autonomous mobile body according to the same embodiment.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates a functional configuration example of the autonomous mobile body according to the same embodiment.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>8</b>A</figref> is an explanatory diagram of a motion overview of the autonomous mobile body according to the same embodiment.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>8</b>B</figref> is an explanatory diagram of the motion overview of the autonomous mobile body according to the same embodiment.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is an explanatory diagram of motion control of the autonomous mobile body when a target sound according to the same embodiment is undetected.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is an explanatory diagram of motion control on the basis of a noise map in a case where the target sound according to the same embodiment is detected.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is an explanatory diagram of motion control on the basis of the noise map in the case where the target sound according to the same embodiment is detected.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is an explanatory diagram of motion control in a case where a approach target according to the same embodiment is not an uttering user.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is an explanatory diagram of creation of a noise map on the basis of sound source direction estimation according to the same embodiment.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is an explanatory diagram of creation of a noise map on the basis of sound source direction estimation according to the same embodiment.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>15</b></figref> illustrates an example of a noise map including type information of noise sources according to the same embodiment.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>16</b></figref> is a setting example of execution conditions for noise map creation processing and updating processing according to the same embodiment.</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is an explanatory diagram of noise map integration processing according to the same embodiment.</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>18</b></figref> is an explanatory diagram of creation and updating of a noise map on the basis of user input according to the same embodiment.</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>19</b></figref> is an explanatory diagram of creation and updating of the noise map on the basis of user input according to the same embodiment.</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>20</b></figref> illustrates an example of circumstances under which it is difficult to avoid a noise region according to the same embodiment.</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>21</b></figref> is an explanatory diagram of calculation of an index &#x3b1; indicating a degree of sound likelihood and an index &#x3b2; indicating a degree of stationarity according to the same embodiment.</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>22</b></figref> is an explanatory diagram of calculation of the index &#x3b1; indicating the degree of sound likelihood and the index &#x3b2; indicating the degree of stationarity according to the same embodiment.</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>23</b></figref> is a flowchart illustrating a flow of noise map updating according to the same embodiment.</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>24</b></figref> is a flowchart illustrating a flow of motion control according to the same embodiment.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0011" level="1">MODES FOR CARRYING OUT THE INVENTION</heading><p id="p-0038" num="0037">Hereinafter, description is given in detail of preferred embodiments of the present disclosure with reference to the accompanying drawings. It is to be noted that, in the present specification and drawings, repeated description is omitted for components substantially having the same functional configuration by assigning the same reference numerals.</p><p id="p-0039" num="0038">It is to be noted that description is given in the following order.<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0039">1. Configuration    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0040">1.1. Overview of Autonomous Mobile Body <b>10</b></li>        <li id="ul0002-0002" num="0041">1.2. Hardware Configuration Example of Autonomous Mobile Body <b>10</b></li>        <li id="ul0002-0003" num="0042">1.3. Functional Configuration Example of Autonomous Mobile Body <b>10</b></li>    </ul>    </li>    <li id="ul0001-0002" num="0043">2. Embodiment    <ul id="ul0003" list-style="none">        <li id="ul0003-0001" num="0044">2.1. Overview</li>        <li id="ul0003-0002" num="0045">2.2. Details of Motion Control</li>        <li id="ul0003-0003" num="0046">2.3. Creation and Updating of Noise Map</li>        <li id="ul0003-0004" num="0047">2.4. Motion Control on Basis of Noise Source Avoidance Priorities</li>        <li id="ul0003-0005" num="0048">2.5. Flow of Motion</li>    </ul>    </li>    <li id="ul0001-0003" num="0049">3. Conclusion</li></ul></p><heading id="h-0012" level="1">1. Configuration</heading><heading id="h-0013" level="2">&#x3c;&#x3c;1.1. Overview of Autonomous Mobile Body <b>10</b>&#x3e;&#x3e;</heading><p id="p-0040" num="0050">As described above, in recent years, various apparatuses have been developed that recognize an utterance of a user, and the like and execute a motion on the basis of a recognition result. Examples of the apparatuses as described above include an autonomous mobile body that changes a behavior in accordance with an utterance of a user, a surrounding environment, and the like.</p><p id="p-0041" num="0051">Here, to achieve voice recognition with high accuracy, it is generally important to enhance, in signals of sounds acquired through a microphone, a ratio of power of a target sound (for example, a uttered voice of a user) that is a target voice for voice recognition to power of a non-target sound that is not the target voice, i.e., an SN ratio (Signal-to-Noise Ratio). Specifically, in an autonomous mobile body having a voice recognition function, it is desirable to enhance voice recognition accuracy by performing movement to a position where the SN ratio is to be improved.</p><p id="p-0042" num="0052">However, in a technology described in PTL 1, the non-target sound is not taken into consideration, and a robot device is only moved toward a direction where a utterance or a face of a user has been recognized. For this reason, in the technology described in PTL 1, circumstances are also presumed under which approaching the user and a noise source that emits a non-target sound simultaneously results in a decrease in the SN ratio and a decrease in voice recognition accuracy.</p><p id="p-0043" num="0053">In addition, in the technology described in the PTL 1, a motion of the robot device is controlled by recognition of the utterance or the face of the user as a trigger to approach the user. Accordingly, it is presumed that there is a high possibility that the robot device described in PTL 1 follows the user who exists around the robot device at all times, which may make the user feel annoying.</p><p id="p-0044" num="0054">An information processor, an information processing method, and a program according to an embodiment of the present disclosure has been conceived by focusing on the above-described respects, and make it possible to cause an autonomous mobile body to execute a motion for further improving accuracy of sound recognition.</p><p id="p-0045" num="0055">Here, first, description is given of an overview of an autonomous mobile body <b>10</b> according to an embodiment of the present disclosure. The autonomous mobile body <b>10</b> according to an embodiment of the present disclosure is an information processor that estimates circumstances on the basis of collected sensor information and autonomously selects and executes various motions according to circumstances. One of features of the autonomous mobile body <b>10</b> is to autonomously execute motions that are presumed to be optimal for each circumstance, unlike a robot that simply performs motions in conformity with commands instructed by a user.</p><p id="p-0046" num="0056">For example, in a case where a target sound that is a target voice for voice recognition processing, i.e., an utterance of a user is not detected, the autonomous mobile body <b>10</b> according to an embodiment of the present disclosure may perform an autonomous motion to avoid input of a non-target sound that is not the above-described target voice. In a case where an utterance of a user is detected, this motion makes it possible to effectively increase a possibility of improving accuracy of voice recognition with respect to the utterance without following the user at all times.</p><p id="p-0047" num="0057">In addition, in a case where the target sound is detected, the autonomous mobile body <b>10</b> according to an embodiment of the present disclosure may move to a position where an input level of the non-target sound becomes lower, around an approach target that is determined on the basis of the target sound. That is, the autonomous mobile body <b>10</b> according to an embodiment of the present disclosure performs an moving motion in consideration of the non-target sound, which makes it possible to improve the SN ratio and effectively improve accuracy of voice recognition with respect to the utterance of the user.</p><p id="p-0048" num="0058">In this manner, the autonomous mobile body <b>10</b> according to an embodiment of the present disclosure comprehensively judges its own state, the surrounding environment, and the like similarly to animals including humans, to thereby determine and execute autonomous motions. In the above respects, the autonomous mobile body <b>10</b> according to an embodiment of the present disclosure obviously differs from a passive apparatus that executes, on the basis of instructions, corresponding motions or processing.</p><p id="p-0049" num="0059">The autonomous mobile body <b>10</b> according to an embodiment of the present disclosure may be an autonomous moving type robot that performs an autonomous posture control in a space and executes various motions. The autonomous mobile body <b>10</b> may be, for example, an autonomous moving type robot having a shape simulating a human or an animal such as a dog and having a motion capability. In addition, the autonomous mobile body <b>10</b> may be, for example, an apparatus such as a vehicle or unmanned aerial vehicle having a communication capability with a user. Shapes, capabilities, or levels of desire and the like of the autonomous mobile body <b>10</b> according to an embodiment of the present disclosure may be appropriately designed in accordance with purposes and roles.</p><heading id="h-0014" level="2">&#x3c;&#x3c;1.2. Hardware Configuration Example of Autonomous Mobile Body <b>10</b>&#x3e;&#x3e;</heading><p id="p-0050" num="0060">Next, description is given of a hardware configuration example of the autonomous mobile body <b>10</b> according to an embodiment of the present disclosure. It is to be noted that, in the following, description is given, by way of example, of a case where the autonomous mobile body <b>10</b> is a dog-type four-legged walking robot.</p><p id="p-0051" num="0061"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a hardware configuration example of the autonomous mobile body <b>10</b> according to an embodiment of the present disclosure. As illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the autonomous mobile body <b>10</b> is a dog-type four-legged walking robot having a head, a torso, four leg parts, and a tail. In addition, the autonomous mobile body <b>10</b> is provided with two displays <b>510</b> on the head.</p><p id="p-0052" num="0062">In addition, the autonomous mobile body <b>10</b> includes various sensors. The autonomous mobile body <b>10</b> includes, for example, a microphone <b>515</b>, a camera <b>520</b>, a ToF (Time of Flight) sensor <b>525</b>, a human detection sensor <b>530</b>, a distance measuring sensor <b>535</b>, a touch sensor <b>540</b>, an illuminance sensor <b>545</b>, a sole button <b>550</b>, and an inertia sensor <b>555</b>.</p><heading id="h-0015" level="2">(Microphone <b>515</b>)</heading><p id="p-0053" num="0063">The microphone <b>515</b> has a function of collecting a surrounding sound. Examples of the above-described sound include an utterance of a user and a surrounding environmental sound. The autonomous mobile body <b>10</b> may include, for example, four microphones on the head. Providing a plurality of microphones <b>515</b> makes it possible to collect sounds generated in the surroundings with high sensitivity and to achieve sound localization.</p><heading id="h-0016" level="2">(Camera <b>520</b>)</heading><p id="p-0054" num="0064">The camera <b>520</b> has a function of capturing an image of the user or the surrounding environment. The autonomous mobile body <b>10</b> may include, for example, two wide-angle cameras at the tip of a nose and at a waist. In this case, the wide-angle camera disposed at the tip of the nose captures an image corresponding to a front field of view of the autonomous mobile body <b>10</b> (i.e., a field of view of a dog), and the wide-angle camera at the waist captures an image of a surrounding region centered on an upper side. The autonomous mobile body <b>10</b> is able to extract feature points of a ceiling, and the like, for example, on the basis of images captured by the wide-angle camera disposed at the waist to achieve SLAM (Simultaneous Localization and Mapping).</p><heading id="h-0017" level="2">(ToF Sensor <b>525</b>)</heading><p id="p-0055" num="0065">The ToF sensor <b>525</b> has a function of detecting a distance with respect to an object that exists in front of the head. The ToF sensor <b>525</b> is provided at the tip of the head. According to the ToF sensor <b>525</b>, it is possible to detect distances with respect to various objects with high accuracy, thus making it possible to achieve motions corresponding to relative positions with respect to target objects including the user, obstacles, and the like.</p><heading id="h-0018" level="2">(Human Detection Sensor <b>530</b>)</heading><p id="p-0056" num="0066">The human detection sensor <b>530</b> has a function of detecting a location of the user or a pet raised by the user. The human detection sensor <b>530</b> is disposed, for example, at the chest. According to the human detection sensor <b>530</b>, detecting an animal body that exists in the front makes it possible to achieve various motions for the animal body, e.g., motions corresponding to emotions such as an interest, a fear, and a surprise.</p><heading id="h-0019" level="2">(Distance Measuring Sensor <b>535</b>)</heading><p id="p-0057" num="0067">The distance measuring sensor <b>535</b> has a function of acquiring circumstances of a floor surface of the front of the autonomous mobile body <b>10</b>. The distance measuring sensor <b>535</b> is disposed, for example, at the chest. According to the distance measuring sensor <b>535</b>, it is possible to detect a distance with respect to an object that exists on the floor surface of the front of the autonomous mobile body <b>10</b> with high accuracy, thus making it possible to achieve a motion corresponding to a relative position with respect to the object.</p><heading id="h-0020" level="2">(Touch Sensor <b>540</b>)</heading><p id="p-0058" num="0068">The touch sensor <b>540</b> has a function of detecting a contact by the user. The touch sensor <b>540</b> is disposed, for example, at a location where the user is highly likely to touch the autonomous mobile body <b>10</b>, such as the top of the head, the lower jaw, or the back. The touch sensor <b>540</b> may include, for example, a capacitive or pressure sensitive touch sensor. According to the touch sensor <b>540</b>, it is possible to detect a contact action such as touching, stroking, tapping, or pushing by the user, thus making it possible to perform a motion corresponding to the contact action.</p><heading id="h-0021" level="2">(Illuminance Sensor <b>545</b>)</heading><p id="p-0059" num="0069">The illuminance sensor <b>545</b> detects illuminance in a space in which the autonomous mobile body <b>10</b> is positioned. The illuminance sensor <b>545</b> may be disposed, for example, at the root of the tail on the back surface of the head. According to the illuminance sensor <b>545</b>, it is possible to detect surrounding brightness and execute a motion corresponding to the brightness.</p><heading id="h-0022" level="2">(Sole Button <b>550</b>)</heading><p id="p-0060" num="0070">The sole button <b>550</b> has a function of detecting whether or not a bottom surface of a leg part of the autonomous mobile body <b>10</b> is in contact with the floor. To this end, the sole button <b>550</b> is disposed at each of locations corresponding to pads of the four leg parts. According to the sole button <b>550</b>, it is possible to detect contact or non-contact between the autonomous mobile body <b>10</b> and the floor surface, thus making it possible for the autonomous mobile body <b>10</b> to grasp, for example, having been lifted by the user, and the like.</p><heading id="h-0023" level="2">(Inertia Sensor <b>555</b>)</heading><p id="p-0061" num="0071">The inertia sensor <b>555</b> is a six-axis sensor that detects physical amounts such as velocities, accelerations, and rotations of the head and the torso. That is, the inertia sensor <b>555</b> detects accelerations and angular velocities of an X-axis, a Y-axis, and a Z-axis. The inertia sensor <b>555</b> is disposed at each of the head and the torso. According to the inertia sensor <b>555</b>, it is possible to detect movements of the head and torso of the autonomous mobile body <b>10</b> with high accuracy, thus making it possible to achieve motion control depending on circumstances.</p><p id="p-0062" num="0072">The description has been given above of examples of the sensors included in the autonomous mobile body <b>10</b> according to an embodiment of the present disclosure. It is to be noted that the configuration described above with reference to <figref idref="DRAWINGS">FIG. <b>1</b></figref> is merely an example, and the configurations of the sensors that may be included in the autonomous mobile body <b>10</b> are not limited to such examples. The autonomous mobile body <b>10</b> may further include, aside from the above-described configuration, for example, a temperature sensor, a geomagnetic sensor, and various communication devices including a GNSS (Global Navigation Satellite System) signal receiver. The configurations of the sensors included in the autonomous mobile body <b>10</b> may be flexibly modified in accordance with specifications and operations.</p><p id="p-0063" num="0073">Subsequently, description is given of a configuration example of joints of the autonomous mobile body <b>10</b> according to an embodiment of the present disclosure. <figref idref="DRAWINGS">FIG. <b>2</b></figref> is a configuration example of actuators <b>570</b> included in the autonomous mobile body <b>10</b> according to an embodiment of the present disclosure. The autonomous mobile body <b>10</b> according to an embodiment of the present disclosure has a total of 22 rotational degrees of freedom, in addition to rotary points illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, with two for each of the ears and the tail, and one for the mouth.</p><p id="p-0064" num="0074">For example, the autonomous mobile body <b>10</b> has three degrees of freedom in the head, thereby making it possible to achieve both nodding and neck-tilting motions. In addition, the autonomous mobile body <b>10</b> reproduces a swinging motion of the waist using the actuator <b>570</b> provided at the waist, thereby making it possible to achieve natural and flexible motions closer to real dogs.</p><p id="p-0065" num="0075">It is to be noted that the autonomous mobile body <b>10</b> according to an embodiment of the present disclosure may combine a single-axis actuator and a biaxial actuator, for example, to thereby achieve the above-described 22 rotational degrees of freedom. For example, the single-axis actuator may be employed at elbows and knees of the leg parts, and the biaxial actuator may be employed at shoulders and the bases of thighs.</p><p id="p-0066" num="0076"><figref idref="DRAWINGS">FIGS. <b>3</b> and <b>4</b></figref> are each an explanatory diagram of motions of the actuator <b>570</b> included in the autonomous mobile body <b>10</b> according to an embodiment of the present disclosure. Referring to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the actuator <b>570</b> rotates an output gear using a motor <b>575</b> to thereby drive a movable arm <b>590</b> at any rotational position and rotational speed.</p><p id="p-0067" num="0077">Referring to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the actuator <b>570</b> according to an embodiment of the present disclosure includes a rear cover <b>571</b>, a gear box cover <b>572</b>, a control substrate <b>573</b>, a gear box base <b>574</b>, the motor <b>575</b>, a first gear <b>576</b>, a second gear <b>577</b>, an output gear <b>578</b>, a detection magnet <b>579</b>, and two bearings <b>580</b>.</p><p id="p-0068" num="0078">The actuator <b>570</b> according to an embodiment of the present disclosure may include, for example, a magnetic svGMR (spin-valve Giant Magnetoresistive). The control substrate <b>573</b> rotates the motor <b>575</b> on the basis of control made by a main processor to thereby transmit power to the output gear <b>578</b> via the first gear <b>576</b> and the second gear <b>577</b>, thus making it possible to drive the movable arm <b>590</b>.</p><p id="p-0069" num="0079">In addition, a position sensor included in the control substrate <b>573</b> detects a rotation angle of the detection magnet <b>579</b> that rotates in synchronization with the output gear <b>578</b> to thereby detect the rotation angle, i.e., the rotational position of the movable arm <b>590</b> with high accuracy.</p><p id="p-0070" num="0080">It is to be noted that the magnetic svGMR has advantages of superior durability because of its non-contact type and of being less influenced by signal variation due to distance variations of the detection magnet <b>579</b> and the position sensor when being used in a GMR-saturated region.</p><p id="p-0071" num="0081">The description has been given above of the configuration example of the actuators <b>570</b> included in the autonomous mobile body <b>10</b> according to an embodiment of the present disclosure. According to the above-described configuration, it is possible to control bending and stretching motions of the joints included in the autonomous mobile body <b>10</b> with high accuracy, and to detect rotational positions of the joints accurately.</p><p id="p-0072" num="0082">Subsequently, description is given of functions of the display <b>510</b> included in the autonomous mobile body <b>10</b> according to an embodiment of the present disclosure, with reference to <figref idref="DRAWINGS">FIG. <b>5</b></figref>. <figref idref="DRAWINGS">FIG. <b>5</b></figref> is an explanatory diagram of the functions of the display <b>510</b> included in the autonomous mobile body <b>10</b> according to an embodiment of the present disclosure.</p><heading id="h-0024" level="2">(Display <b>510</b>)</heading><p id="p-0073" num="0083">The display <b>510</b> has a function of visually expressing movements of eyes and emotions of the autonomous mobile body <b>10</b>. As illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the display <b>510</b> is able to express motions of an eyeball, a pupil, and an eyelid in response to emotions and motions. The display <b>510</b> does not intentionally display a letter, a symbol, an image not related to movements of an eyeball, or the like, to thereby produce a natural motion close to an animal such as a real live dog.</p><p id="p-0074" num="0084">In addition, as illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the autonomous mobile body <b>10</b> includes two displays <b>510</b><i>r </i>and <b>510</b><i>l </i>corresponding to the right eye and the left eye, respectively. The displays <b>510</b><i>r </i>and <b>510</b><i>l </i>are implemented, for example, by two independent OLEDs (Organic Light Emitting Diodes). According to the OLED, it is possible to reproduce a curved surface of the eyeball, thus making it possible to achieve a more natural exterior, as compared with a case where a pair of eyeballs is expressed by one flat display or a case where two eyeballs are expressed by respective two independent flat displays.</p><p id="p-0075" num="0085">As described above, according to the displays <b>510</b><i>r </i>and <b>510</b><i>l, </i>it is possible to express lines of sight and emotions of the autonomous mobile body <b>10</b> as illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref> with high accuracy and flexibility. In addition, it is possible for the user to intuitively grasp the state of the autonomous mobile body <b>10</b> from the motion of the eyeballs displayed on the displays <b>510</b>.</p><p id="p-0076" num="0086">The description has been given above of the hardware configuration example of the autonomous mobile body <b>10</b> according to an embodiment of the present disclosure. According to the above-described configuration, controlling the motions of the joints and the eyeballs of the autonomous mobile body <b>10</b> with high accuracy and flexibility makes it possible to achieve a motion and an emotional expression closer to a real living creature, as illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. It is to be noted that <figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates motion examples of the autonomous mobile body <b>10</b> according to an embodiment of the present disclosure; however, <figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates an external structure of the autonomous mobile body <b>10</b> in a simplified manner to give description, focusing on the motions of the joints and the eyeballs of the autonomous mobile body <b>10</b>. Similarly, in the following description, the external structure of the autonomous mobile body <b>10</b> may be illustrated in a simplified manner in some cases; however, the hardware configuration and the exterior of the autonomous mobile body <b>10</b> according to an embodiment of the present disclosure are not limited to the examples illustrated in the drawings, and may be appropriately designed.</p><heading id="h-0025" level="2">&#x3c;&#x3c;1.3. Functional Configuration Example of Autonomous Mobile Body <b>10</b>&#x3e;&#x3e;</heading><p id="p-0077" num="0087">Next, description is given of a functional configuration example of the autonomous mobile body <b>10</b> according to an embodiment of the present disclosure. <figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates a functional configuration example of the autonomous mobile body <b>10</b> according to an embodiment of the present disclosure. Referring to <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the autonomous mobile body <b>10</b> according to an embodiment of the present disclosure includes an input unit <b>110</b>, a recognition unit <b>120</b>, a surrounding environment estimation unit <b>130</b>, a surrounding environment holding unit <b>140</b>, an operation control unit <b>150</b>, a drive unit <b>160</b>, and an output unit <b>170</b>.</p><heading id="h-0026" level="2">(Input Unit <b>110</b>)</heading><p id="p-0078" num="0088">The input unit <b>110</b> has a function of collecting various types of information regarding a user and a surrounding environment. The input unit <b>110</b> collects, for example, an utterance of the user, environmental sounds generated in the surroundings, image information regarding the user and the surrounding environment, and various types of sensor information. To this end, the input unit <b>110</b> includes various sensors illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><heading id="h-0027" level="2">(Recognition Unit <b>120</b>)</heading><p id="p-0079" num="0089">The recognition unit <b>120</b> has a function of performing various recognitions of the user, objects in the surroundings, and the state of the autonomous mobile body <b>10</b> on the basis of various types of information collected by the input unit <b>110</b>. For instances, the recognition unit <b>120</b> may perform human recognition, face recognition, recognition of facial expressions and lines of sight, voice recognition, object recognition, color recognition, shape recognition, marker recognition, obstacle recognition, step recognition, brightness recognition, and the like.</p><heading id="h-0028" level="2">(Surrounding Environment Estimation Unit <b>130</b>)</heading><p id="p-0080" num="0090">The surrounding environment estimation unit <b>130</b> has a function of creating and updating a noise map indicating circumstances under which a non-target sound is generated, on the basis of sensor information collected by the input unit <b>110</b> and a recognition result by the recognition unit <b>120</b>. The details of the function of the surrounding environment estimation unit <b>130</b> are described later separately.</p><heading id="h-0029" level="2">(Surrounding Environment Holding Unit <b>140</b>)</heading><p id="p-0081" num="0091">The surrounding environment holding unit <b>140</b> has a function of holding the noise map created and updated by the surrounding environment estimation unit <b>130</b>.</p><heading id="h-0030" level="2">(Operation Control Unit <b>150</b>)</heading><p id="p-0082" num="0092">The operation control unit <b>150</b> has a function of making an action plan on the basis of the recognition result by the recognition unit <b>120</b> and the noise map held by the surrounding environment holding unit <b>140</b> and controlling operations of the drive unit <b>160</b> and the output unit <b>170</b> on the basis of the action plan. For example, the operation control unit <b>150</b> performs rotational control of the actuators <b>570</b>, display control of the display <b>510</b>, voice output control by a speaker, and the like on the basis of the above-described action plan. The details of the function of the operation control unit <b>150</b> according to an embodiment of the present disclosure are described separately.</p><heading id="h-0031" level="2">(Drive Unit <b>160</b>)</heading><p id="p-0083" num="0093">The drive unit <b>160</b> has a function of bending and stretching a plurality of joints included in the autonomous mobile body <b>10</b> on the basis of control by the operation control unit <b>150</b>. More specifically, the drive unit <b>160</b> drives the actuators <b>570</b> included in the respective joints on the basis of the control by the operation control unit <b>150</b>.</p><heading id="h-0032" level="2">(Output Unit <b>170</b>)</heading><p id="p-0084" num="0094">The output unit <b>170</b> has a function of outputting visual information and sound information on the basis of control by the operation control unit <b>150</b>. To this end, the output unit <b>170</b> includes the display <b>510</b> and a speaker.</p><p id="p-0085" num="0095">The description has been given above of the functional configuration of the autonomous mobile body <b>10</b> according to an embodiment of the present disclosure. It is to be noted that the configuration illustrated in <figref idref="DRAWINGS">FIG. <b>7</b></figref> is merely an example, and the functional configuration of the autonomous mobile body <b>10</b> is not limited to such an example. The autonomous mobile body <b>10</b> according to an embodiment of the present disclosure may include, for example, a communication unit that performs communication with an information processing server and another autonomous mobile body. In addition, the functions of the recognition unit <b>120</b>, the surrounding environment estimation unit <b>130</b>, the operation control unit <b>150</b>, and the like may be implemented as functions of the above-descried information processing server. In this case, the information processing server is able to execute various types of recognition processing, creation or updating of the noise map, and the action plan on the basis of the sensor information collected by the input unit <b>110</b> of the autonomous mobile body <b>10</b>, and control the drive unit <b>160</b> and the output unit <b>170</b> of the autonomous mobile body <b>10</b>. The functional configuration of the autonomous mobile body <b>10</b> according to an embodiment of the present disclosure may be flexibly modified in accordance with specifications and operations.</p><heading id="h-0033" level="1">2. Embodiment</heading><heading id="h-0034" level="2">&#x3c;&#x3c;2.1. Overview&#x3e;&#x3e;</heading><p id="p-0086" num="0096">Next, description is given of an embodiment of the present disclosure. As described above, to improve accuracy of voice recognition with respect to the target sound, the autonomous mobile body <b>10</b> according to an embodiment of the present disclosure performs an autonomous motion to improve an SN ratio of the target sound and the non-target sound.</p><p id="p-0087" num="0097">Here, presumed techniques for improving the SN ratio include a technique of performing signal processing (multimicrophone signal processing and single microphone signal processing) on an input signal, and a technique using a directional microphone or the like. However, it can be said that the SN ratio is most strongly influenced by a physical distance with respect to a target sound source or a non-target sound source (hereinafter also referred to as &#x201c;noise source&#x201d;).</p><p id="p-0088" num="0098">To this end, the autonomous mobile body <b>10</b> according to the present embodiment does not simply approach the target sound source, but stays as far away from the non-target sound source as possible while approaching the target sound source, thus making it possible to effectively improve the SN ratio.</p><p id="p-0089" num="0099"><figref idref="DRAWINGS">FIG. <b>8</b>A</figref> and <figref idref="DRAWINGS">FIG. <b>8</b>B</figref> are explanatory diagrams of a motion overview of the autonomous mobile body <b>10</b> according to the present embodiment. <figref idref="DRAWINGS">FIG. <b>8</b>A</figref> illustrates an example of movement control by a comparison technique according to the present embodiment. In the example illustrated in <figref idref="DRAWINGS">FIG. <b>8</b>A</figref>, in a case where a voice of a user U, i.e., a target sound is detected, a comparator <b>90</b> approaches the user U through the shortest route to increase an input level (a sound pressure level) of the target sound. However, at this time, the comparator <b>90</b> does not consider existence of a noise source NS that emits the non-target sound; therefore, the comparator <b>90</b> approaches the noise source NS simultaneously with approaching the user U. Thus, in the comparison technique, an input level of the non-target sound is also increased together with the input level of the target sound, and as a result, a possibility arises that an effect of improving the SN ratio is reduced and accuracy of voice recognition with respect to an utterance of the user U is decreased.</p><p id="p-0090" num="0100">In contrast, <figref idref="DRAWINGS">FIG. <b>8</b>B</figref> illustrates an example of movement control by an information processing method according to the present embodiment. As illustrated in <figref idref="DRAWINGS">FIG. <b>8</b>B</figref>, in a case where the voice of the user U, i.e., the target sound is detected, the autonomous mobile body <b>10</b> according to the present embodiment performs movement in consideration of existence of the noise source that emits the non-target sound. Specifically, in a case where the target sound is detected, the operation control unit <b>150</b> according to the present embodiment may move the autonomous mobile body <b>10</b> to a position where the input level of the non-target sound becomes lower, around the approach target that is determined on the basis of the target sound.</p><p id="p-0091" num="0101">Here, the above-described approach target may be the user U who emits the target sound, i.e., an uttered voice. That is, in a case where an utterance of the user U is detected, the operation control unit <b>150</b> according to the present embodiment is able to move the autonomous mobile body <b>10</b> to a position where an input level of an utterance of a user becomes higher and the input level of the non-target sound emitted by the noise source NS becomes lower, around the user U that is the approach target.</p><p id="p-0092" num="0102">In the example illustrated in <figref idref="DRAWINGS">FIG. <b>8</b>B</figref>, the operation control unit <b>150</b> according to the present embodiment moves the autonomous mobile body <b>10</b> to a side opposite to a position where the non-target sound is emitted, i.e., the noise source NS with respect to the user U as a center. In this manner, according to the operation control unit <b>150</b> according to the present embodiment, it is possible to move the autonomous mobile body <b>10</b> farther away from the noise source NS and closer to the user U that is the approach target. In addition, in a case where the autonomous mobile body <b>10</b> is moved to the side opposite to the noise source NS with the user U interposed therebetween, the user U acts as a wall, and an effect of more effectively decreasing the input level of the non-target sound emitted by the noise source NS is expected.</p><p id="p-0093" num="0103">According to the above-described function of the operation control unit <b>150</b> according to the present embodiment, it is possible to decrease the input level of the non-target sound or reduce a rate of increase of the input level together with increasing the input level of the target sound, which consequently makes it possible to effectively improve the SN ratio. Thus, according to the operation control unit <b>150</b> according to the present embodiment, it is possible to greatly improve the SN ratio with use of only a moving function intrinsic in the autonomous mobile body <b>10</b> without performing signal processing on an input signal and beamforming with use of a directional microphone.</p><heading id="h-0035" level="2">&#x3c;&#x3c;2.2. Details of Motion Control&#x3e;&#x3e;</heading><p id="p-0094" num="0104">Next, description is given in more detail of motion control of the autonomous mobile body <b>10</b> by the operation control unit <b>150</b> according to the present embodiment. As described above, the operation control unit <b>150</b> according to the present embodiment causes the autonomous mobile body <b>10</b> to execute a motion in consideration of existence of the non-target sound in addition to the target sound, which makes it possible to improve the SN ratio and effectively improve accuracy of voice recognition with respect to the target sound.</p><p id="p-0095" num="0105">Here, it is possible to define the target sound according to the present embodiment as a target voice for voice recognition by the recognition unit <b>120</b>. The above-described target voice may include all voices. For example, it can be said that in a case where the autonomous mobile body <b>10</b> covers all voices outputted from a television, a radio, and the like and uttered voices of a user or a third person as targets for voice recognition, all the voices as described above are target sounds. In this case, the recognition unit <b>120</b> is able to detect all the voices as described above as the target sounds by comparison of pitches, or the like in consideration of a harmonic sound structure of human voices, or the like, for example. It is to be noted that in a case where all the voices as described above are target sounds, the autonomous mobile body <b>10</b> is able to perform some action or the like in response to a voice outputted from a television.</p><p id="p-0096" num="0106">On one hand, the target sound according to the present embodiment may cover only a voice of a predetermined user registered in advance among the voices as described above. In this case, the recognition unit <b>120</b> performs speaker recognition on the basis of voice features of the user registered in advance or face recognition of a person existing in a direction from which an input signal comes, thus making it possible to detect only a uttered voice of the predetermined user as the target sound.</p><p id="p-0097" num="0107">On the other hand, the target sound according to the present embodiment may cover only specific keywords, and specific words related to motion instructions among uttered voices by the predetermined user. In this case, the recognition unit <b>120</b> performs voice recognition on the basis of an input signal, thus making it possible to detect only the uttered voices of the predetermined user including the specific keywords and words as target sounds.</p><p id="p-0098" num="0108">In addition, it is possible to define the non-target sound according to the present embodiment as all sounds other than the target sound. Examples of the non-target sound according to the present embodiment include working sounds in a kitchen and various non-voices generated by apparatuses such as an exhaust fan, a refrigerator, and a car.</p><p id="p-0099" num="0109">The description has been given above of the details of the target sound and the non-target sound according to the present embodiment. Subsequently, referring to <figref idref="DRAWINGS">FIG. <b>9</b></figref>, description is given of motion control in a case where the target sound is not detected. <figref idref="DRAWINGS">FIG. <b>9</b></figref> is an explanatory diagram of motion control of the autonomous mobile body <b>10</b> when the target sound according to the present embodiment is undetected. <figref idref="DRAWINGS">FIG. <b>9</b></figref> illustrates an example of a noise map held by the surrounding environment holding unit <b>140</b>.</p><p id="p-0100" num="0110">Here, the noise map according to the present embodiment includes a map that is created and updated by the surrounding environment estimation unit <b>130</b> and indicates circumstances under which the non-target sound is generated. The noise map according to the present embodiment includes, for example, a noise source existing in a space where the autonomous mobile body <b>10</b> exists, and information regarding a noise region that is a region where the input level of the non-target sound emitted by the noise source is strong (for example, equal to or greater than a threshold value). In the example illustrated in <figref idref="DRAWINGS">FIG. <b>9</b></figref>, the noise map includes noise sources NS<b>1</b> and NS<b>2</b> and information of noise regions NR<b>1</b> and NR<b>2</b> respectively corresponding to the noise sources NS<b>1</b> and NS<b>2</b>.</p><p id="p-0101" num="0111">One of features of the operation control unit <b>150</b> according to the present embodiment is to control a motion of the autonomous mobile body <b>10</b> on the basis of the noise map including information as described above. For example, in a case where the target sound is not detected, the operation control unit <b>150</b> according to the present embodiment may control the motion of the autonomous mobile body <b>10</b> to avoid input of the non-target sound on the basis of the noise map.</p><p id="p-0102" num="0112">More specifically, in a case where the target sound is not detected, the operation control unit <b>150</b> according to the present embodiment is able to limit a movement range of the autonomous mobile body <b>10</b> within a region where the input level of the non-target sound is equal to or lower than the threshold value, on the basis of the noise map. For example, in the example illustrated in <figref idref="DRAWINGS">FIG. <b>9</b></figref>, in a case where the target sound is not detected, the operation control unit <b>150</b> may limit the movement range of the autonomous mobile body <b>10</b> within a region other than both the noise regions NR<b>1</b> and NR<b>2</b> to prevent the autonomous mobile body <b>10</b> from entering the noise regions NR<b>1</b> and NR<b>2</b>. At this time, the operation control unit <b>150</b> may randomly move the autonomous mobile body <b>10</b> within the above-described region, or may move the autonomous mobile body <b>10</b> to a position P<sub>min </sub>where a sound pressure of the non-target sound is expected to be minimum, or the like.</p><p id="p-0103" num="0113">According to the above-described control by the operation control unit <b>150</b> according to the present embodiment, even in a case where the target sound is not detected, operating the autonomous mobile body <b>10</b> to suppress input of the non-target sound as much as possible makes it possible to effectively improve accuracy of voice recognition with respect to the target sound in a case where a user issues a call or the like, i.e., in a case where the target sound is detected.</p><p id="p-0104" num="0114">Subsequently, description is given of details of motion control on the basis of the noise map in a case where the target sound according to the present embodiment is detected. As described above, one of features of the operation control unit <b>150</b> according to the present embodiment is to move the autonomous mobile body <b>10</b> to a position where the input level of the target sound becomes higher and the input level of the non-target sound becomes lower, around the approach target in a case where the target sound is detected. At this time, the operation control unit <b>150</b> according to the present embodiment refers to the noise map, thus making it possible to achieve the above-described motion control with high accuracy.</p><p id="p-0105" num="0115"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is an explanatory diagram of motion control on the basis of the noise map in a case where the target sound according to the present embodiment is detected. In an example illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, the operation control unit <b>150</b> causes the autonomous mobile body <b>10</b> to approach the user U on the basis of detection of an uttered voice UO<b>1</b> of the user U who calls the name of the autonomous mobile body <b>10</b>, i.e., the target sound. At this time, the operation control unit <b>150</b> according to the present embodiment refers to the noise map, and controls movement of the autonomous mobile body <b>10</b> in consideration of the noise source NS and a noise region NR included in the noise map.</p><p id="p-0106" num="0116">For example, in circumstances illustrated in FIG.<b>10</b>, as indicated by a chain double-dashed line in the drawing, in a case where the autonomous mobile body <b>10</b> approaches the user U through the shortest route, the autonomous mobile body <b>10</b> moves in the noise region NR. However, the operation control unit <b>150</b> according to the present embodiment refers to the noise map, thus making it possible to move the autonomous mobile body <b>10</b> to a position farther away from the noise source NS without causing the autonomous mobile body <b>10</b> to enter the noise region NR or stop in the noise region NR. More specifically, the operation control unit <b>150</b> may cause the autonomous mobile body <b>10</b> to go around the noise region NR as indicated by a solid line in the drawing, and may move the autonomous mobile body <b>10</b> to a side opposite to the noise source NS with respect to the user U that is the approach target as a center.</p><p id="p-0107" num="0117">In this manner, the operation control unit <b>150</b> according to the present embodiment refers to the noise map, thus making it possible to grasp the noise source and the noise region accurately and move the autonomous mobile body <b>10</b> to a position where the input level of the target sound is increased and the input level of the non-target sound is decreased. According to the above-described motion control by the operation control unit <b>150</b> according to the present embodiment, it is possible to improve the SN ratio and effectively improve accuracy of voice recognition with respect to the target sound.</p><p id="p-0108" num="0118">It is to be noted that the operation control unit <b>150</b> according to the present embodiment may not necessarily move the autonomous mobile body <b>10</b> to the side opposite to the noise source with respect to the approach target as a center. For example, in an example illustrated in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, a wall exists in a straight line joining the noise source NS and the user U. In a case where an obstacle exists in a straight line joining the noise source and the approach target in this manner, the operation control unit <b>150</b> may stop the autonomous mobile body <b>10</b> at a position that is close to the above-described opposite side and does not enter the noise region NR. Even in this case, it is possible to achieve both an increase in the input level of the target sound and a decrease in the input level of the non-target sound and achieve an effect of improving the SN ratio. It is to be noted that the operation control unit <b>150</b> may grasp existence of an obstacle on the basis of information of a wall, furniture, and the like included in the noise map, and may perform motion control as described above on the basis of existence of an obstacle recognized by the recognition unit <b>120</b>.</p><p id="p-0109" num="0119">Next, description is given of motion control in a case where the approach target according to the present embodiment is not an uttering user. <figref idref="DRAWINGS">FIG. <b>12</b></figref> is an explanatory diagram of motion control in a case where the approach target according to the present embodiment is not an uttering user.</p><p id="p-0110" num="0120">In an example illustrated in <figref idref="DRAWINGS">FIG. <b>12</b></figref>, unlike the examples illustrated in <figref idref="DRAWINGS">FIGS. <b>10</b> and <b>11</b></figref>, a user U<b>1</b> is making a voice utterance UO<b>2</b> that provides an instruction for movement not to the user U<b>1</b> but to a user U<b>2</b>. At this time, the operation control unit <b>150</b> according to the present embodiment sets the user U<b>2</b> as the approach target on the basis of the uttered voice UO<b>2</b> recognized by the recognition unit <b>120</b> and controls the motion of the autonomous mobile body <b>10</b> to cause the autonomous mobile body <b>10</b> to approach the user U<b>2</b>.</p><p id="p-0111" num="0121">In this manner, the approach target according to the present embodiment may be not only the uttering user who makes a voice utterance but also a moving body such as another user specified by voice recognition processing on the basis of the uttered voice, a fixed object such as a charging station, or any position.</p><p id="p-0112" num="0122">Even in a case where the approach target is not the uttering user, the operation control unit <b>150</b> according to the present embodiment refers to the noise map similarly, thus making it possible to cause the autonomous mobile body <b>10</b> to execute movement in consideration of the noise source NS and the noise region NR. In the example illustrated in <figref idref="DRAWINGS">FIG. <b>12</b></figref>, the operation control unit <b>150</b> moves the autonomous mobile body <b>10</b> to the side opposite to the noise source NS with respect to the user U<b>2</b> as a center. According to the above-described motion control by the operation control unit <b>150</b> according to the present embodiment, it is possible to effectively improve accuracy of voice recognition with respect to an uttered voice of the user U<b>2</b> that is expected to be made hereafter, i.e., the target sound.</p><p id="p-0113" num="0123">The description has been given above of motion control on the basis of the noise map according to the present embodiment. As described above, the operation control unit <b>150</b> according to the present embodiment refers to the noise map held by the surrounding environment holding unit <b>140</b> to thereby achieve movement of the autonomous mobile body <b>10</b> in consideration of not only the input level of the target sound but also the input level of the non-target sound and improve the SN ratio, which makes it possible to achieve voice recognition with high accuracy.</p><p id="p-0114" num="0124">It is to be noted that the above has mainly described, by way of example, a case where the operation control unit <b>150</b> according to the present embodiment controls the autonomous mobile body <b>10</b> to move the autonomous mobile body <b>10</b> to the approach target on the basis of detection of the target sound; however, a trigger of movement in the present embodiment is not limited to such an example. The operation control unit <b>150</b> according to the present embodiment may perform control to move the autonomous mobile body <b>10</b> to the approach target on the basis of recognition of the face of the user or recognition of a gesture related to an instruction for movement by the user. Even in this case, referring to the noise map and moving the autonomous mobile body to a position where the input level of the non-target sound become lower makes it possible to enhance accuracy of voice recognition with respect to the target sound that is expected to be made hereafter.</p><heading id="h-0036" level="2">&#x3c;&#x3c;2.3. Creation and Updating of Noise Map&#x3e;&#x3e;</heading><p id="p-0115" num="0125">Next, description is given of details of creation and updating of the noise map according to the present embodiment. The surrounding environment estimation unit <b>130</b> according to the present embodiment is able to create a noise map as described above on the basis of results of sound source direction estimation and sound pressure measurement, for example.</p><p id="p-0116" num="0126">First, description is given of creation of a noise map on the basis of sound source direction estimation according to the present embodiment. <figref idref="DRAWINGS">FIG. <b>13</b></figref> is an explanatory diagram of creation of the noise map on the basis of the sound source direction estimation according to the present embodiment.</p><p id="p-0117" num="0127">In creation of the noise map on the basis of the sound source direction estimation, the surrounding environment estimation unit <b>130</b> first performs sound localization at any given point to estimate a sound source direction. In an example illustrated in <figref idref="DRAWINGS">FIG. <b>13</b></figref>, the surrounding environment estimation unit <b>130</b> estimates respective directions of the noise sources NS<b>1</b> and NS<b>2</b> at a point P<b>1</b>. It is to be noted that at this point in time, it is possible to estimate the directions of the noise sources NS<b>1</b> and NS<b>2</b>, but distances from the autonomous mobile body <b>10</b> to the noise sources NS<b>1</b> and NS<b>2</b> are unknown.</p><p id="p-0118" num="0128">Subsequently, the surrounding environment estimation unit <b>130</b> moves to a point different from the previously estimated sound source direction, and performs sound localization again to estimate the sound source direction. In the example illustrated in <figref idref="DRAWINGS">FIG. <b>13</b></figref>, the surrounding environment estimation unit <b>130</b> estimates the respective directions of the noise sources NS<b>1</b> and NS<b>2</b> again at a point P<b>2</b>. At this time, the surrounding environment estimation unit <b>130</b> is able to estimate positions of the noise sources NS<b>1</b> and NS<b>2</b> in a space from a moving distance of the autonomous mobile body <b>10</b> and an intersection of the directions estimated at the points P<b>1</b> and P<b>2</b>.</p><p id="p-0119" num="0129">Thereafter, the surrounding environment estimation unit <b>130</b> repeats sound source direction estimation at still another point, which makes it possible to improve accuracy of estimation of a sound source position. In the example illustrated in <figref idref="DRAWINGS">FIG. <b>13</b></figref>, the surrounding environment estimation unit <b>130</b> estimates the directions of the noise sources NS<b>1</b> and NS<b>2</b> again at a point P<b>3</b>.</p><p id="p-0120" num="0130">The surrounding environment estimation unit <b>130</b> according to the present embodiment repeats sound source direction estimation at a plurality of points in this manner, which makes it possible to estimate the positions of the noise sources NS<b>1</b> and NS<b>2</b> in a space with high accuracy, and create, for example a noise map in which regions located at predetermined distances from respective estimated positions are set as the noise regions NR<b>1</b> and NR<b>2</b>.</p><p id="p-0121" num="0131">Subsequently, description is given of creation of a noise map on the basis of sound pressure measurement according to the present embodiment. In a case where the autonomous mobile body <b>10</b> does not include a larger number of microphones than the number of source sources simultaneously generated, it is difficult to create the noise map on the basis of the above-described sound source direction estimation. In contrast, the surrounding environment estimation unit <b>130</b> according to the present embodiment is able to create the noise map on the basis of sound pressure measurement to be described below even in a case where the autonomous mobile body <b>10</b> includes only a single microphone. <figref idref="DRAWINGS">FIG. <b>14</b></figref> is an explanatory diagram of creation of the noise map on the basis of the sound source direction estimation according to the present embodiment.</p><p id="p-0122" num="0132">In creation of the noise map on the basis of the sound source direction estimation, the surrounding environment estimation unit <b>130</b> first executes measurement of a sound pressure level at any given point. In an example illustrated in <figref idref="DRAWINGS">FIG. <b>14</b></figref>, the surrounding environment estimation unit <b>130</b> measures a sound pressure level at a point P<b>4</b>. Subsequently, the surrounding environment estimation unit <b>130</b> repeatedly executes measurement of the sound pressure level at another point different from the point at which the measurement has been executed. In the example illustrated in <figref idref="DRAWINGS">FIG. <b>14</b></figref>, the surrounding environment estimation unit <b>130</b> executes measurement of the sound pressure level at points P<b>5</b> and P<b>6</b>.</p><p id="p-0123" num="0133">The surrounding environment estimation unit <b>130</b> according to the present embodiment repeats sound pressure measurement at a plurality of points in this manner, which makes it possible to estimate isobars of the sound pressure level as illustrated in <figref idref="DRAWINGS">FIG. <b>14</b></figref> and set a region in which the sound pressure level is equal to or greater than a threshold value as a noise region. In addition, the surrounding environment estimation unit <b>130</b> is able to estimate a point having the highest sound pressure level in the noise region as the position of the noise source. In the example illustrated in <figref idref="DRAWINGS">FIG. <b>14</b></figref>, the surrounding environment estimation unit <b>130</b> sets the noise regions NR<b>1</b> and NR<b>2</b> on the basis of estimated isobars, and estimates the positions of the noise sources NS<b>1</b> and NS<b>2</b>.</p><p id="p-0124" num="0134">According to the surrounding environment estimation unit <b>130</b> according to the present embodiment, even in case where the autonomous mobile body <b>10</b> includes only a single microphone, repeatedly executing sound pressure measurement at a plurality of points in this manner makes it possible to create a noise map with high accuracy. It is to be noted that, in a case where creation of a noise map on the basis of the sound pressure measurement is performed, it is necessary to separate the target sound and the non-target sound, but such separation is achievable by the above-described function of the recognition unit <b>120</b>.</p><p id="p-0125" num="0135">In addition, the noise map according to the present embodiment may include information such as type of noise source. <figref idref="DRAWINGS">FIG. <b>15</b></figref> illustrates an example of a noise map including type information of the noise sources according to the present embodiment. As can be seen from the example illustrated in <figref idref="DRAWINGS">FIG. <b>15</b></figref>, information of the noise map include that the noise sources NS<b>1</b> and NS<b>2</b> are respectively a kitchen and a television.</p><p id="p-0126" num="0136">The surrounding environment estimation unit <b>130</b> according to the present embodiment is able to create a noise map including type information of the noise sources as illustrated in <figref idref="DRAWINGS">FIG. <b>15</b></figref> on the basis of a result of object recognition by the recognition unit <b>120</b>, for example. According to the above-described function of the surrounding environment estimation unit <b>130</b> according to the present embodiment, the operation control unit <b>150</b> is able to perform motion control with high accuracy in accordance with identification of the noise source.</p><p id="p-0127" num="0137">One example of noise map creation according to the present embodiment has been described above. Subsequently, description is given of a timing of creation or updating of the noise map according to the present embodiment.</p><p id="p-0128" num="0138">For example, a case is presumed where updating of the noise map is performed constantly dynamically. In this case, while it is possible to detect the non-target sounds generated in the surroundings without fail, information such as a sporadic sound that is not useful for the motion control by the operation control unit <b>150</b> is all included as information of the noise map. In addition, in a case where updating of the noise map is executed constantly dynamically, a calculation amount becomes enormous; therefore, high-performance processor or the like is necessary.</p><p id="p-0129" num="0139">Accordingly, the surrounding environment estimation unit <b>130</b> according to the present embodiment may execute noise map creation processing and updating processing only under a highly effective condition in collection of the non-target sound. Here, the above-described highly effective condition includes circumstances under which a large number of non-target sounds may be generated. In addition, the circumstances under which a large number of non-target sounds may be generated include circumstances under which the user carries out activity in a space. Accordingly, the surrounding environment estimation unit <b>130</b> according to the present embodiment may execute the noise map creation processing and updating processing at a timing at which the user exists in a space where the autonomous mobile body <b>10</b> is installed.</p><p id="p-0130" num="0140">At this time, the surrounding environment estimation unit <b>130</b> according to the present embodiment is able to estimate absence or existence of the user on the basis of a schedule of the user and various types of sensor information, and execute the noise map creation processing and updating processing only in a condition in which the user is highly likely to exist.</p><p id="p-0131" num="0141"><figref idref="DRAWINGS">FIG. <b>16</b></figref> is a setting example of execution conditions for the noise map creation processing and updating processing according to the present embodiment. In the example illustrated in <figref idref="DRAWINGS">FIG. <b>16</b></figref>, whether or not to execute creation processing and updating processing is set for each combination of factors such as the schedule of the user (not at home or at home), detection or non-detection of a key sound, detection or non-detection of door opening/closing noise, detection or non-detection of an utterance such as &#x201c;I'm home&#x201d;, detection or non-detection of a moving body by the human detection sensor. It is to be noted that <figref idref="DRAWINGS">FIG. <b>16</b></figref> illustrates an example in which the processing is executed only in a case where it is determined from all the above-described factors that the user exists in the same space as the autonomous mobile body <b>10</b>.</p><p id="p-0132" num="0142">It is to be noted that such propriety of execution as descried above may be settable dynamically in accordance with the features and circumstances of the autonomous mobile body <b>10</b>. Thus, according to the surrounding environment estimation unit <b>130</b> according to the present embodiment, creating or updating the noise map on the basis of the non-target sounds collected in a time zone in which the user exists in a surrounding environment makes it possible to hold a highly accurate noise map.</p><p id="p-0133" num="0143">Next, description is given of noise map updating processing according to the present embodiment. As described above, the surrounding environment estimation unit <b>130</b> according to the present embodiment is able to dynamically update the noise map on the basis of the non-target sounds collected in the time zone in which the user exists in the surrounding environment.</p><p id="p-0134" num="0144">However, in this case, at a timing at which sound collection is performed, a case is also presumed where a different non-target sound is generated. Accordingly, in a case where the noise map is updated simply on the basis of the latest sound collection data, information of a non-target sound that is intrinsically less influenced, such as a sporadic sound is included in the noise map, which may cause a decrease in accuracy of motion control by the operation control unit <b>150</b>.</p><p id="p-0135" num="0145">Accordingly, the noise map may be updated not by overwriting the existing noise map on the basis of the latest sound collection data but by integrating the latest sound collection data into the existing noise map.</p><p id="p-0136" num="0146"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is an explanatory diagram of noise map integration processing according to the present embodiment. In an example illustrated in <figref idref="DRAWINGS">FIG. <b>17</b></figref>, the surrounding environment estimation unit <b>130</b> according to the present embodiment performs noise map integration on the basis of three times of sound collection. Here, a case is presumed where the non-target sounds related to the kitchen and the television are detected in the first sound collection; non-target sounds related to a window and the television are detected in the second sound collection; and non-target sounds related to the kitchen and the television are detected in the third sound collection.</p><p id="p-0137" num="0147">In this case, for example, the surrounding environment estimation unit <b>130</b> according to the present embodiment may integrate sound collection data for three times by averaging or the like to update the noise map. According to the surrounding environment estimation unit <b>130</b> according to the present embodiment, as illustrated in <figref idref="DRAWINGS">FIG. <b>17</b></figref>, it is possible to reflect the frequency of generation of each non-target sound or the like on the noise map, which makes it possible to reduce an influence of the non-target sound that is not frequently generated, such as a sporadic sound. It is to be noted that in the example illustrated in <figref idref="DRAWINGS">FIG. <b>17</b></figref>, the frequency of generation of the non-target sound is indicated by high and low densities of hatching (the higher the density is, the higher the frequency of generation becomes). In this case, the operation control unit <b>150</b> may control movement of the autonomous mobile body <b>10</b> to more intensively avoid the television that frequently generates the non-target sound.</p><p id="p-0138" num="0148">The description has been given above of creation and updating of the noise map according to the present embodiment. It is to be noted that the techniques described above are merely examples, and creation and updating of the noise map according to the present embodiment are not limited to the examples.</p><p id="p-0139" num="0149">The surrounding environment estimation unit <b>130</b> according to the present embodiment may perform creation and updating of the noise map on the basis of information inputted by the user, for example. <figref idref="DRAWINGS">FIG. <b>18</b></figref> and <figref idref="DRAWINGS">FIG. <b>19</b></figref> are explanatory diagrams of creation and updating of the noise map on the basis of user input according to the present embodiment.</p><p id="p-0140" num="0150">For example, the surrounding environment estimation unit <b>130</b> according to the present embodiment may perform creation and updating of the noise map on the basis of furniture layout information inputted by the user via an information processing terminal <b>20</b> or the like, as illustrated in <figref idref="DRAWINGS">FIG. <b>18</b></figref>. At this time, the surrounding environment estimation unit <b>130</b> is able to request to dispose an icon IC corresponding to each piece of furniture in an input region IA simulating a room of the user in a display section included in the information processing terminal <b>20</b>, and to execute creation and updating of the noise map on the basis of inputted information.</p><p id="p-0141" num="0151">In addition, for example, the surrounding environment estimation unit <b>130</b> according to the present embodiment is able to identify the noise source NS on the basis of a gesture such as finger pointing to be performed by the user U and an uttered voice UO<b>3</b> for teaching of the noise source and reflect the identified noise source NS on the noise map.</p><heading id="h-0037" level="2">&#x3c;&#x3c;2.4. Motion Control on Basis of Noise Source Avoidance Priorities&#x3e;&#x3e;</heading><p id="p-0142" num="0152">Next, description is given of motion control on the basis of noise source avoidance priorities according to the present embodiment. The description has been given above of a case where the operation control unit <b>150</b> according to the present embodiment refers to the noise map, and controls movement of the autonomous mobile body <b>10</b> to avoid the noise region.</p><p id="p-0143" num="0153">However, a case is presumed where depending on circumstances, it is difficult to perform movement while avoiding the noise region in some cases. <figref idref="DRAWINGS">FIG. <b>20</b></figref> illustrates an example of circumstances under which avoidance of the noise region according to the present embodiment is difficult. A left side of <figref idref="DRAWINGS">FIG. <b>20</b></figref> illustrates a noise map on the basis of sound source direction estimation, and a right side of <figref idref="DRAWINGS">FIG. <b>20</b></figref> illustrates a noise map on the basis of sound pressure measurement. Here, as can be seen from both the noise maps, the autonomous mobile body <b>10</b> is surrounded by the noise regions NR<b>1</b> to NR<b>4</b> in both the noise maps, and movement to another location is difficult.</p><p id="p-0144" num="0154">In such a case, the operation control unit <b>150</b> according to the present embodiment may control the autonomous mobile body <b>10</b> to move the autonomous mobile body <b>10</b> to a noise region corresponding to a noise source of a lower avoidance priority on the basis of avoidance priorities assigned to the noise sources NS<b>1</b> to NS<b>4</b>.</p><p id="p-0145" num="0155">Here, the avoidance priorities according to the present embodiment may be determined by types and features of non-target sounds generated by noise sources, for example. As described above, the non-target sounds according to the present embodiment include various types of sounds other than the target sound. Meanwhile, influences of the non-target sounds exerted on accuracy of voice recognition with respect to the target sound are different depending on features of the non-target sounds.</p><p id="p-0146" num="0156">Accordingly, the surrounding environment estimation unit <b>130</b> according to the present embodiment may classify the non-target sounds on the basis of magnitude of influence degree on accuracy of voice recognition, and create a noise map in which the avoidance priorities are set in the decreasing order of the influence degree.</p><p id="p-0147" num="0157">Here, an example is described in which non-targets are classified into four categories <b>1</b> to <b>4</b>. For example, the category <b>1</b> may include a non-target sound that has a relatively large sound volume and is not the target sound while being a human voice. Examples of the category <b>1</b> include voices outputted from a television, a radio, and any other apparatus, music including vocals, conversation among third parties other than the user, and the like. The category <b>1</b> may include a non-target sound having the highest influence on accuracy of voice recognition and being of the highest avoidance priority among the four categories.</p><p id="p-0148" num="0158">In addition, the category <b>2</b> may include a non-target sound that is generated unsteadily and has a relatively large sound volume, thus making it difficult to sufficiently achieve an effect of suppressing noise. Examples of the category <b>2</b> include working sounds such as dish washing and cooking, outdoor sounds coming from an open window, and the like. The category <b>2</b> may include a non-target sound having the second highest influence on accuracy of voice recognition and being of the second highest avoidance priority among the four categories.</p><p id="p-0149" num="0159">In addition, the category <b>3</b> may include a non-target sound that is generated steadily, thus making it relatively easy to achieve the effect of suppressing noise. Examples of the category <b>3</b> include sounds generated by an air conditioner, an exhaust fan, a PC fan, and the like. The category <b>2</b> may include a non-target sound having the third highest influence on accuracy of voice recognition and being of the third highest avoidance priority among the four categories.</p><p id="p-0150" num="0160">In addition, the category <b>4</b> may include a non-target sound that is generated sporadically and has only an instantaneous influence. Examples of the category <b>4</b> include door opening/closing noise, a footstep sound, a sound generated by a microwave oven, and the like The category <b>4</b> may include a non-target sound having the lowest influence on accuracy of voice recognition and being of the lowest avoidance priority among the four categories.</p><p id="p-0151" num="0161">Thus, the surrounding environment estimation unit <b>130</b> according to the present embodiment is able to create a noise map in which the avoidance priorities are set in accordance with the features of the non-target sounds.</p><p id="p-0152" num="0162">In addition, the noise source avoidance priorities according to the present embodiment may be set on the basis of an acoustic and some sort of quantitative index related to the non-target sound. Examples of the quantitative index described above include an index indicating a degree of sound likelihood, and an index indicating a degree of stationarity.</p><p id="p-0153" num="0163">In general, the target sound that is a target for voice recognition, i.e., the uttered voice of the user is a &#x201c;non-stationary&#x201d; &#x201c;voice&#x201d;. Meanwhile, examples of the &#x201c;non-stationary&#x201d; &#x201c;voice&#x201d; also include non-target sounds such as conversation among third parties and voices outputted from a television and a radio. Accordingly, to improve accuracy of voice recognition with respect to the target sound that is the &#x201c;non-stationary&#x201d; &#x201c;voice&#x201d;, it is important to avoid the non-target sound that is the &#x201c;non-stationary&#x201d; &#x201c;voice&#x201d; that is difficult to be separated from the target sound.</p><p id="p-0154" num="0164">Meanwhile, &#x201c;non-stationary&#x201d; &#x201c;non-voices&#x201d; include working sounds of dish washing and cooking, and the like, and &#x201c;stationary&#x201d; &#x201c;non-voices&#x201d; include sounds outputted from an air conditioner, an exhaust fan, and a PC fan, and the like. However, such non-target sounds are relatively easily separated from the target sound; therefore, it can be said that the non-target sounds are of a lower avoidance priority, as compared with the non-target sound that is the &#x201c;non-stationary&#x201d; &#x201c;voice&#x201d; described above.</p><p id="p-0155" num="0165">In addition, a &#x201c;stationary&#x201d; &#x201c;voice&#x201d; corresponds to, for example, a case where the same sound is uttered long and continuously, such as &#x201c;Ahhhh&#x201d;; however, such a sound is extremely unlikely to be generated in daily life, and may be therefore ignored.</p><p id="p-0156" num="0166">As described above, the surrounding environment estimation unit <b>130</b> according to the present embodiment may calculate influence degrees of the non-target sounds exerted on voice recognition with respect to the target sound on the basis of an index &#x3b1; indicating a degree of sound likelihood and an index &#x3b2; indicating a degree of stationarity, and set the avoidance priorities on the basis of thus-calculated values.</p><p id="p-0157" num="0167"><figref idref="DRAWINGS">FIG. <b>21</b></figref> and <figref idref="DRAWINGS">FIG. <b>22</b></figref> are explanatory diagrams of calculation of the index &#x3b1; indicating the degree of sound likelihood and the index &#x3b2; indicating the degree of stationarity according to the present embodiment. <figref idref="DRAWINGS">FIG. <b>21</b></figref> illustrates a flow of calculation in a case where the autonomous mobile body <b>10</b> according to the present embodiment includes a plurality of microphones.</p><p id="p-0158" num="0168">In a case where the autonomous mobile body <b>10</b> includes a plurality of microphones, as illustrated in <figref idref="DRAWINGS">FIG. <b>21</b></figref>, the surrounding environment estimation unit <b>130</b> executes beamforming in order toward directions of the noise sources NS<b>1</b> to NS<b>4</b> and calculates the index &#x3b1; and the index &#x3b2;. According to such a technique, it is possible to calculate the index &#x3b1; and the index &#x3b2; of each of the noise sources NR<b>1</b> to NR<b>4</b> without entering the noise regions NR<b>1</b> to NR<b>4</b>; therefore, even in a case where the target sound is detected during calculation, it is possible to maintain accuracy of voice recognition with respect to the target sound.</p><p id="p-0159" num="0169">In addition, <figref idref="DRAWINGS">FIG. <b>22</b></figref> illustrates a flow of calculation in a case where the autonomous mobile body <b>10</b> according to the present embodiment includes a single microphone. In a case where the autonomous mobile body <b>10</b> includes a single microphone, as illustrated in <figref idref="DRAWINGS">FIG. <b>22</b></figref>, the surrounding environment estimation unit <b>130</b> may calculate the index &#x3b1; and the index &#x3b2; around each of the noise sources NS<b>1</b> to NS<b>4</b>. It is to be noted that in a case where calculation of the index &#x3b1; and the index &#x3b2; of each noise source is completed, the operation control unit <b>150</b> may control the autonomous mobile body <b>10</b> to cause the autonomous mobile body <b>10</b> to immediately escape from the noise regions NR<b>1</b> to NR<b>4</b>.</p><p id="p-0160" num="0170">The description has been given above of the flow of calculation of the index &#x3b1; and the index &#x3b2; according to the present embodiment. The surrounding environment estimation unit <b>130</b> according to the present embodiment is able to calculate the influence degree of each of noise sources on the basis of the index &#x3b1; and the index &#x3b2; calculated as described above and set avoidance priorities on the basis of the influence degrees. For example, a total value of the index &#x3b1; and the index &#x3b2; may be defined as the influence degree, and the surrounding environment estimation unit <b>130</b> may set avoidance priorities in the decreasing order of the total values.</p><p id="p-0161" num="0171">It is to be noted that the surrounding environment estimation unit <b>130</b> according to the present embodiment may calculate the index &#x3b1; indicating the degree of sound likelihood on the basis of sound spectral entropy, for example. The sound spectral entropy is an index used for a VAD (Voice Activity Detection) technology, and a human voice tends to have a lower value, as compared with other sounds.</p><p id="p-0162" num="0172">The surrounding environment estimation unit <b>130</b> according to the present embodiment is able to calculate sound spectral entropy, i.e., the index &#x3b1; by the following mathematical expression (1). It is to be noted that f in the mathematical expression (1) indicates a frequency, and S<sub>f </sub>indicates an amplitude spectrum of the frequency f of an observation signal. In addition, P<sub>f </sub>in the mathematical expression (1) is defined by the following mathematical expression (2).</p><p id="p-0163" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>[Math. 1]<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0164" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>&#x3b1;=&#x2212;&#x3a3;<sub>f</sub><i>P</i><sub>f</sub>&#xb7;log <i>P</i><sub>f </sub>&#x2003;&#x2003;(1)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0165" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>&#x3b2;=S<sub>f </sub>&#x2003;&#x2003;(2)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0166" num="0173">In addition, the surrounding environment estimation unit <b>130</b> according to the present embodiment may calculate the index &#x3b2; indicating the degree of stationarity on the basis of a sound kurtosis, for example. The sound kurtosis is an index frequently used to discriminate between stationarity and non-stationarity of a sound, and may be calculated by the following mathematical expression (3). It is to be noted that T in the following mathematical expression (3) indicates a length of a sound segment where a kurtosis is calculated, and a length such as three to five seconds may be set. In addition, t in the mathematical expression (3) indicates a certain time, and x(t) indicates a voice waveform at the time t.</p><p id="p-0167" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mo>{</mo>      <mrow>       <mi>Math</mi>       <mo>.</mo>       <mtext>   </mtext>       <mn>2</mn>      </mrow>     </mrow>     <mo>]</mo>    </mrow>   </mtd>   <mtd>    <mi>&#xf3ba;</mi>   </mtd>  </mtr>  <mtr>   <mtd>    <mrow>     <mi>&#x3b2;</mi>     <mo>=</mo>     <mrow>      <mfrac>       <mrow>        <msubsup>         <mo>&#x2211;</mo>         <mrow>          <mi>t</mi>          <mo>=</mo>          <mn>1</mn>         </mrow>         <mi>T</mi>        </msubsup>        <msup>         <mrow>          <mi>x</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <mi>t</mi>          <mo>)</mo>         </mrow>         <mn>4</mn>        </msup>       </mrow>       <msup>        <mrow>         <mo>(</mo>         <mrow>          <msubsup>           <mo>&#x2211;</mo>           <mrow>            <mi>t</mi>            <mo>=</mo>            <mn>1</mn>           </mrow>           <mi>T</mi>          </msubsup>          <msup>           <mrow>            <mi>x</mi>            <mo>&#x2061;</mo>            <mo>(</mo>            <mi>t</mi>            <mo>)</mo>           </mrow>           <mn>2</mn>          </msup>         </mrow>         <mo>)</mo>        </mrow>        <mn>2</mn>       </msup>      </mfrac>      <mo>&#x2062;</mo>      <mrow>       <mo>-</mo>       <mn>3</mn>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>3</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0168" num="0174">The description has been given above of setting of noise source avoidance priorities according to the present embodiment. According to setting of avoidance priorities according to the present embodiment, the autonomous mobile body <b>10</b> is able to avoid the non-target sound that exerts an influence on voice recognition with respect to the target sound on a priority basis.</p><heading id="h-0038" level="2">&#x3c;&#x3c;2.5. Flow of Motion&#x3e;&#x3e;</heading><p id="p-0169" num="0175">Next, description is given of details of a flow of the motion of the autonomous mobile body <b>10</b> according to the present embodiment. First, description is given of a flow of updating of the noise map according to the present embodiment. <figref idref="DRAWINGS">FIG. <b>23</b></figref> is a flowchart illustrating a flow of noise map updating according to the present embodiment.</p><p id="p-0170" num="0176">Referring to <figref idref="DRAWINGS">FIG. <b>23</b></figref>, first, the surrounding environment estimation unit <b>130</b> estimates a surrounding environment on the basis of sensor information collected by the input unit <b>110</b> and a recognition result by the recognition unit <b>120</b> (S<b>1101</b>). Specifically, the surrounding environment estimation unit <b>130</b> estimates a noise source and a noise region.</p><p id="p-0171" num="0177">Next, the surrounding environment estimation unit <b>130</b> determines whether or not the surrounding environment holding unit <b>140</b> holds an existing noise map (S<b>1102</b>).</p><p id="p-0172" num="0178">Here, in a case where the noise map held by the surrounding environment holding unit <b>140</b> does not exist (S<b>1102</b>: NO), the surrounding environment estimation unit <b>130</b> creates a noise map on the basis of the surrounding environment estimated in the step S<b>1101</b> and stores the noise map in the surrounding environment holding unit <b>140</b> (S<b>1107</b>).</p><p id="p-0173" num="0179">Meanwhile, in a case where the existing noise map exists in the surrounding environment holding unit <b>140</b> (S<b>1102</b>: YES), the surrounding environment estimation unit <b>130</b> next determines whether or not the number of noise sources in the estimated surrounding environment is changed from the number of noise sources in the existing noise map (S<b>1103</b>).</p><p id="p-0174" num="0180">Here, in a case where the number of noise sources is changed (S<b>1103</b>: YES), the surrounding environment estimation unit <b>130</b> integrates the noise map on the basis of the surrounding environment estimated in the step S<b>1101</b> (S<b>1106</b>), and stores the integrated noise map in the surrounding environment holding unit <b>140</b> (S<b>1107</b>).</p><p id="p-0175" num="0181">Meanwhile, in a case where the number of noise sources is not changed (S<b>1103</b>: NO), the surrounding environment estimation unit <b>130</b> next determines whether or not the position of the noise source in the estimated surrounding environment is changed from the position of the noise source in the existing noise map (S<b>1104</b>).</p><p id="p-0176" num="0182">Here, in a case where the position of the noise source is changed (S<b>1104</b>: YES), the surrounding environment estimation unit <b>130</b> integrates the noise map on the basis of the surrounding environment estimated in the step S<b>1101</b> (S<b>1106</b>), and stores the integrated noise map in the surrounding environment holding unit <b>140</b> (S<b>1107</b>).</p><p id="p-0177" num="0183">Meanwhile, in a case where the position of the noise source is not changed (S<b>1104</b>: NO), the surrounding environment estimation unit <b>130</b> next determines whether or not a sound pressure of a non-target sound emitted by the noise source in the estimated surrounding environment is changed from the sound pressure of the non-target sound in the existing noise map (S<b>1105</b>).</p><p id="p-0178" num="0184">Here, in a case where the sound pressure of the non-target sound emitted by the noise source is changed (S<b>1105</b>: YES), the surrounding environment estimation unit <b>130</b> integrates the noise map on the basis of the surrounding environment estimated in the step S<b>1101</b> (S<b>1106</b>), and stores the integrated noise map in the surrounding environment holding unit <b>140</b> (S<b>1107</b>).</p><p id="p-0179" num="0185">Meanwhile, in a case where the sound pressure of the non-target sound emitted by the noise source is not changed (S<b>1105</b>: NO), the surrounding environment estimation unit <b>130</b> does not update the noise map and maintains the existing noise map in the surrounding environment holding unit <b>140</b>.</p><p id="p-0180" num="0186">Next, description is given of details of a flow of motion control according to the present embodiment. <figref idref="DRAWINGS">FIG. <b>24</b></figref> is a flowchart illustrating a flow of motion control according to the present embodiment.</p><p id="p-0181" num="0187">Referring to <figref idref="DRAWINGS">FIG. <b>24</b></figref>, the operation control unit <b>150</b> first reads the noise map held by the surrounding environment holding unit <b>140</b> (S<b>1201</b>).</p><p id="p-0182" num="0188">Subsequently, the operation control unit <b>150</b> causes the autonomous mobile body <b>10</b> to perform an autonomous action avoiding the noise region, on the basis of the noise map read in the step S<b>1201</b> (S<b>1202</b>).</p><p id="p-0183" num="0189">In addition, the operation control unit <b>150</b> continuously determines whether or not the target sound is detected during the autonomous action in the step S<b>1202</b> (S<b>1203</b>).</p><p id="p-0184" num="0190">Here, in a case where the target sound is detected (S<b>1203</b>: YES), the operation control unit <b>150</b> moves the autonomous mobile body <b>10</b> to a position where the input level of the non-target sound becomes lower, around the approach target, on the basis of the noise map read in the step S<b>1201</b> (S<b>1204</b>).</p><p id="p-0185" num="0191">Next, the operation control unit <b>150</b> causes the autonomous mobile body <b>10</b> to execute a corresponding motion on the basis of a result of voice recognition of the target sound (S<b>1205</b>).</p><p id="p-0186" num="0192">The description has been given above of the flow of the motion of the autonomous mobile body <b>10</b> according to the present embodiment. It is to be noted that the above has mainly described that, to improve the SN ratio, the autonomous mobile body <b>10</b> according to the present embodiment performs movement in consideration of the input levels of the target sound and the non-target sound. However, a technique for improving the SN ratio according to the present embodiment is not limited to such an example, and may be used in combination with signal processing or beamforming technology, for example.</p><p id="p-0187" num="0193">For example, the operation control unit <b>150</b> according to the present embodiment may move the autonomous mobile body <b>10</b> to a position between the approach target and the noise source and perform control to extend beamforming toward the direction of the approach target. In a case where the autonomous mobile body <b>10</b> is a dog-type robot device, the operation control unit <b>150</b> may perform control to extend beamforming at an elevation angle corresponding to the height of the face of the user that is the approach target. In this case, an effect of effectively eliminating a non-target sound arrived from a horizontal direction to effectively improve the SN ratio is expected.</p><p id="p-0188" num="0194">In addition, to avoid the noise region, the operation control unit <b>150</b> according to the present embodiment may cause the autonomous mobile body <b>10</b> to perform a motion for guiding the user, for example. For example, in a case where the user that is the approach target exists in the noise region, the operation control unit <b>150</b> causes the autonomous mobile body <b>10</b> to perform a motion for guiding the user to move away from the noise region and approach the autonomous mobile body <b>10</b>, which makes it possible to increase the input level of the target sound without entering the noise region. For example, the above-described guiding may be implemented by a motion such as barking, stopping in front of the noise region, and prowling. In addition, in a case where the autonomous mobile body <b>10</b> has a verbal communication function like a humanoid robot device, for example, the autonomous mobile body <b>10</b> may explicitly provide notification of wanting to move away from the noise region by voice.</p><heading id="h-0039" level="1">3. Conclusion</heading><p id="p-0189" num="0195">As described above, the autonomous mobile body <b>10</b> that is one example of the information processor according to an embodiment of the present disclosure includes the operation control unit <b>150</b> that controls the motion of the autonomous mobile body <b>10</b> on the basis of recognition processing. In addition, one of the features of the operation control unit <b>150</b> according to an embodiment of the present disclosure is to move the autonomous mobile body <b>10</b> to a position where the input level of the non-target sound that is not the target voice becomes lower, around the approach target that is determined on the basis of the target sound in a case where the target sound that is a target voice for voice recognition processing is detected. According to such a configuration, it is possible to cause the autonomous mobile body to execute a motion for further improving accuracy of sound recognition.</p><p id="p-0190" num="0196">Although the description has been given above in detail of preferred embodiments of the present disclosure with reference to the accompanying drawings, the technical scope of the present disclosure is not limited to such examples. It is obvious that a person having ordinary skill in the art of the present disclosure may find various alterations or modifications within the scope of the technical idea described in the claims, and it should be understood that these alterations and modifications naturally come under the technical scope of the present disclosure.</p><p id="p-0191" num="0197">In addition, the effects described herein are merely illustrative or exemplary, and are not limitative. That is, the technology according to the present disclosure may achieve, in addition to or in place of the above effects, other effects that are obvious to those skilled in the art from the description of the present specification.</p><p id="p-0192" num="0198">In addition, respective steps of the series of processing of the autonomous mobile body <b>10</b> in the present specification need not necessarily be processed in chronological order illustrated in the flowcharts. For example, the respective steps of the series of processing of the autonomous mobile body <b>10</b> may be processed in an order different from the order illustrated in the flowcharts, or may be processed in parallel.</p><p id="p-0193" num="0199">It is to be noted that the technical scope of the present disclosure also includes the following configurations.<ul id="ul0004" list-style="none">    <li id="ul0004-0001" num="0200">(1)</li></ul></p><p id="p-0194" num="0201">An information processor including:</p><p id="p-0195" num="0202">an operation control unit that controls a motion of an autonomous mobile body acting on the basis of recognition processing,</p><p id="p-0196" num="0203">in a case where a target sound that is a target voice for voice recognition processing is detected, the operation control unit moving the autonomous mobile body to a position, around an approach target, where an input level of a non-target sound that is not the target voice becomes lower, the approach target being determined on the basis of the target sound.<ul id="ul0005" list-style="none">    <li id="ul0005-0001" num="0204">(2)</li></ul></p><p id="p-0197" num="0205">The information processor according to (1), in which in a case where the target sound is detected, the operation control unit moves the autonomous mobile body to a position, around the approach target determined on the basis of the target sound, where an input level of the target sound becomes higher and the input level of the non-target sound becomes lower.<ul id="ul0006" list-style="none">    <li id="ul0006-0001" num="0206">(3)</li></ul></p><p id="p-0198" num="0207">The information processor according to (1) or (2), in which in a case where the target sound is detected, the operation control unit moves the autonomous mobile body to a position farther away from a noise source that emits the non-target sound and closer to the approach target.<ul id="ul0007" list-style="none">    <li id="ul0007-0001" num="0208">(4)</li></ul></p><p id="p-0199" num="0209">The information processor according to any one of (1) to (3), in which in a case where the target sound is detected, the operation control unit moves the autonomous mobile body to a side opposite to a noise source that emits the non-target sound with respect to the approach target as a center.<ul id="ul0008" list-style="none">    <li id="ul0008-0001" num="0210">(5)</li></ul></p><p id="p-0200" num="0211">The information processor according to any one of (1) to (4), in which</p><p id="p-0201" num="0212">the target sound includes an uttered voice of a user, and</p><p id="p-0202" num="0213">the approach target includes an uttering user that emits the uttered voice.<ul id="ul0009" list-style="none">    <li id="ul0009-0001" num="0214">(6)</li></ul></p><p id="p-0203" num="0215">The information processor according to any one of (1) to (5), in which the approach target includes a moving body, a fixed object, or a position identified by the voice recognition processing on the basis of a uttered voice of a user.<ul id="ul0010" list-style="none">    <li id="ul0010-0001" num="0216">(7)</li></ul></p><p id="p-0204" num="0217">The information processor according to any one of (1) to (6), in which the operation control unit controls the motion of the autonomous mobile body on the basis of a noise map indicating circumstances under which the non-target sound is generated in a surrounding environment.<ul id="ul0011" list-style="none">    <li id="ul0011-0001" num="0218">(8)</li></ul></p><p id="p-0205" num="0219">The information processor according to (7), in which</p><p id="p-0206" num="0220">the noise map includes information of a noise source that emits the non-target sound, and</p><p id="p-0207" num="0221">the operation control unit controls the motion of the autonomous mobile body on the basis of an avoidance priority of the noise source.<ul id="ul0012" list-style="none">    <li id="ul0012-0001" num="0222">(9)</li></ul></p><p id="p-0208" num="0223">The information processor according to (8), in which the avoidance priority of the noise source is determined on the basis of a type of the noise source.<ul id="ul0013" list-style="none">    <li id="ul0013-0001" num="0224">(10)</li></ul></p><p id="p-0209" num="0225">The information processor according to (8), in which the avoidance priority of the noise source is determined on the basis of an influence degree of the non-target sound emitted by the noise source on the voice recognition processing.<ul id="ul0014" list-style="none">    <li id="ul0014-0001" num="0226">(11)</li></ul></p><p id="p-0210" num="0227">The information processor according to (10), in which the influence degree is calculated on the basis of at least one of an index indicating a degree of sound likelihood of the non-target sound or an index indicating a degree of stationarity.<ul id="ul0015" list-style="none">    <li id="ul0015-0001" num="0228">(12)</li></ul></p><p id="p-0211" num="0229">The information processor according to any one of (7) to (10), in which in a case where the target sound is not detected, the operation control unit controls the motion of the autonomous mobile body to avoid input of the non-target sound on the basis of the noise map.<ul id="ul0016" list-style="none">    <li id="ul0016-0001" num="0230">(13)</li></ul></p><p id="p-0212" num="0231">The information processor according to any one of (7) to (12), in which in a case where the target sound is not detected, the operation control unit limits a movement range of the autonomous mobile body within a region where the input level of the non-target sound is equal to or lower than a threshold value, on the basis of the noise map.<ul id="ul0017" list-style="none">    <li id="ul0017-0001" num="0232">(14)</li></ul></p><p id="p-0213" num="0233">The information processor according to any one of (7) to (13), further including a surrounding environment estimation unit that creates the noise map.<ul id="ul0018" list-style="none">    <li id="ul0018-0001" num="0234">(15)</li></ul></p><p id="p-0214" num="0235">The information processor according to (14), in which the surrounding environment estimation unit creates the noise map on the basis of direction estimation of a noise source that emits the non-target sound, or sound pressure measurement.<ul id="ul0019" list-style="none">    <li id="ul0019-0001" num="0236">(16)</li></ul></p><p id="p-0215" num="0237">The information processor according to (14) or (15), in which the surrounding environment estimation unit dynamically updates the noise map on the basis of the non-target sound collected.<ul id="ul0020" list-style="none">    <li id="ul0020-0001" num="0238">(17)</li></ul></p><p id="p-0216" num="0239">The information processor according to (16), in which the surrounding environment estimation unit dynamically updates the noise map on the basis of change in number, position, or sound pressure of the noise sources that emit the non-target sound.<ul id="ul0021" list-style="none">    <li id="ul0021-0001" num="0240">(18)</li></ul></p><p id="p-0217" num="0241">The information processor according to (16) or (17), in which the surrounding environment estimation unit creates or updates the noise map on the basis of the non-target sound collected in a time zone where a user exists in a surrounding environment.<ul id="ul0022" list-style="none">    <li id="ul0022-0001" num="0242">(19)</li></ul></p><p id="p-0218" num="0243">An information processing method including causing a processor to:</p><p id="p-0219" num="0244">control a motion of an autonomous mobile body acting on the basis of recognition processing,</p><p id="p-0220" num="0245">the controlling further including, in a case where a target sound that is a target voice for voice recognition processing is detected, moving the autonomous mobile body to a position, around an approach target, where an input level of a non-target sound that is not the target voice becomes lower, the approach target being determined on the basis of the target sound.<ul id="ul0023" list-style="none">    <li id="ul0023-0001" num="0246">(20)</li></ul></p><p id="p-0221" num="0247">A program that causes a computer to function as an information processor, the information processor including</p><p id="p-0222" num="0248">an operation control unit that controls a motion of an autonomous mobile body acting on the basis of recognition processing,</p><p id="p-0223" num="0249">in a case where a target sound that is a target voice for voice recognition processing is detected, the operation control unit moving the autonomous mobile body to a position, around an approach target, where an input level of a non-target sound that is not the target voice becomes lower, the approach target being determined on the basis of the target sound.</p><heading id="h-0040" level="1">REFERENCE NUMERALS LIST</heading><p id="p-0224" num="0250"><b>10</b>: autonomous mobile body<ul id="ul0024" list-style="none">    <li id="ul0024-0001" num="0251"><b>110</b>: input unit</li>    <li id="ul0024-0002" num="0252"><b>120</b>: recognition unit</li>    <li id="ul0024-0003" num="0253"><b>130</b>: surrounding environment estimation unit</li>    <li id="ul0024-0004" num="0254"><b>140</b>: surrounding environment holding unit</li>    <li id="ul0024-0005" num="0255"><b>150</b>: operation control unit</li>    <li id="ul0024-0006" num="0256"><b>160</b>: drive unit</li>    <li id="ul0024-0007" num="0257"><b>170</b>: output unit</li></ul></p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230005481A1-20230105-M00001.NB"><img id="EMI-M00001" he="12.70mm" wi="76.20mm" file="US20230005481A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An information processor comprising:<claim-text>circuitry configured to:</claim-text><claim-text>detect environment information from sensor data;</claim-text><claim-text>detect an avoidance priority according to the environment information;</claim-text><claim-text>generate map information corresponding to the environment information; and</claim-text><claim-text>control a motion of an autonomous mobile body acting on a basis of the map information and the avoidance priority.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The information processor according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the environment information is a surrounding environment of the autonomous mobile body.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The information processor according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the environment information includes an utterance of a user and environmental sounds generated in the surrounding environment.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The information processor according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the map information is a noise map.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The information processor according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the noise map includes a noise source existing in a space where the autonomous mobile body exists.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The information processor according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the noise map includes information of a noise region corresponding to the noise source.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The information processor according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the sensor data is collected from at least one of a microphone, a camera, a time of flight sensor, a human detection sensor, a distance measuring sensor, a touch sensor, an illuminance sensor, and an inertia sensor.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The information processor according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the avoidance priority is of a noise source.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The information processor according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the avoidance priority of the noise source is determined based on a type of the noise source.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. An information processing method comprising:<claim-text>detecting environment information from sensor data;</claim-text><claim-text>detecting an avoidance priority according to the environment information;</claim-text><claim-text>generating map information corresponding to the environment information; and</claim-text><claim-text>controlling a motion of an autonomous mobile body acting on a basis of the map information and the avoidance priority.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The information processing method according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the environment information is a surrounding environment of the autonomous mobile body.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The information processing method according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the environment information includes an utterance of a user and environmental sounds generated in the surrounding environment.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The information processing method according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the map information is a noise map.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The information processing method according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the noise map includes a noise source existing in a space where the autonomous mobile body exists.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The information processing method according to <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the noise map includes information of a noise region corresponding to the noise source.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The information processing method according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, further comprising collecting the sensor data from at least one of a microphone, a camera, a time of flight sensor, a human detection sensor, a distance measuring sensor, a touch sensor, an illuminance sensor, and an inertia sensor.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The information processing method according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the avoidance priority is of a noise source.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The information processing method according to <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the avoidance priority of the noise source is determined based on a type of the noise source.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. A non-transitory computer readable medium storing executable instructions, which when executed by circuitry, cause the circuitry to perform a method, the method comprising:<claim-text>detecting environment information from sensor data;</claim-text><claim-text>detecting an avoidance priority according to the environment information;</claim-text><claim-text>generating map information corresponding to the environment information; and</claim-text><claim-text>controlling a motion of an autonomous mobile body acting on a basis of the map information and the avoidance priority.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The non-transitory computer readable medium according to <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein the environment information is a surrounding environment of the autonomous mobile body.</claim-text></claim></claims></us-patent-application>