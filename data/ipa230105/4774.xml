<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004775A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004775</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17614081</doc-number><date>20200518</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>FR</country><doc-number>1905681</doc-number><date>20190528</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>04</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>063</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>04</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>063</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">METHOD FOR IMPLEMENTING A HARDWARE ACCELERATOR OF A NEURAL NETWORK</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="obligated-assignee"><addressbook><orgname>BULL SAS</orgname><address><city>LES CLAYES SOUS BOIS</city><country>FR</country></address></addressbook><residence><country>FR</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>BOURGE</last-name><first-name>Alban</first-name><address><city>LES CLAYES SOUS BOIS</city><country>FR</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>MOGNOL</last-name><first-name>Meven</first-name><address><city>LES CLAYES SOUS BOIS</city><country>FR</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>SINITAMBIRIVOUTIN</last-name><first-name>Emrick</first-name><address><city>LES CLAYES SOUS BOIS</city><country>FR</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>BULL SAS</orgname><role>03</role><address><city>LES CLAYES SOUS BOIS</city><country>FR</country></address></addressbook></assignee></assignees><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/FR2020/050822</doc-number><date>20200518</date></document-id><us-371c12-date><date>20220908</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">The invention relates to a method for implementing a hardware accelerator for a neural network, comprising: a step of interpreting an algorithm of the neural network in binary format, converting the neural network algorithm in binary format into a graphical representation, selecting building blocks from a library of predetermined building blocks, creating an organization of the selected building blocks, configuring internal parameters of the building blocks of the organization so that the organization of the selected and configured building blocks corresponds to said graphical representation; a step of determining an initial set of weights for the neural network; a step of completely synthesizing the organization of the selected and configured building blocks on the one hand, in a preselected FPGA programmable logic circuit (<b>41</b>) in a hardware accelerator (<b>42</b>) for the neural network, and on the other hand in a software driver for this hardware accelerator (<b>42</b>), this hardware accelerator (<b>42</b>) being specifically dedicated to the neural network so as to represent the entire architecture of the neural network without needing access to a memory (<b>44</b>) external to the FPGA programmable logic circuit (<b>41</b>) when passing from one layer to another layer of the neural network, a step of loading (<b>48</b>) the initial set of weights for the neural network into the hardware accelerator (<b>42</b>).</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="72.98mm" wi="143.76mm" file="US20230004775A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="203.71mm" wi="146.22mm" file="US20230004775A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="163.15mm" wi="163.32mm" file="US20230004775A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">FIELD OF THE INVENTION</heading><p id="p-0002" num="0001">The invention relates to the field of methods for implementing a hardware accelerator for a neural network, as well as the field of circuit boards implementing a hardware accelerator for a neural network. A method for implementing a hardware accelerator for a neural network is a method for implementing a hardware accelerator for a neural network algorithm.</p><heading id="h-0002" level="1">TECHNOLOGICAL BACKGROUND OF THE INVENTION</heading><p id="p-0003" num="0002">The methods for implementing a hardware accelerator for a neural network make it possible to create and train specific neural networks on a reconfigurable hardware target (FPGA for &#x201c;Field-Programmable Gate Array&#x201d;) and then use them on datasets. Several technical difficulties can be highlighted, both from a hardware and software standpoint.</p><p id="p-0004" num="0003">From the software standpoint, these can concern the need to binarize neural networks, the desire to reduce the loss of precision, the advantage of being able to automate the binarization of a neural network in a floating representation, the portability of the binary network trained on the reconfigurable hardware target FPGA, and the use of a series of complex tools.</p><p id="p-0005" num="0004">From the hardware standpoint, these can concern the complex architecture of neural networks and especially convolutional neural networks, the difficulty of scaling generic components, the current proliferation of new types of neural networks (possibly to be tested), and the search for a performance level per unit of power (in watts) consumed which is quite high and which can become very high in the embedded domain.</p><p id="p-0006" num="0005">In addition to all these difficult and sometimes partially contradictory requirements raised by the prior art, the invention is also interested in achieving, for these methods for implementing a hardware accelerator for a neural network, a simplification of its use for a non-expert user concerning the software aspect and a certain automation in the manner of using a series of tools which are quite complex to manipulate and require a relatively high level of expertise.</p><p id="p-0007" num="0006">According to a first prior art, it is known to implement a hardware accelerator for a neural network in an FPGA programmable logic circuit (Field Programmable Gate Array).</p><p id="p-0008" num="0007">However, the operation of this hardware accelerator has two disadvantages:<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0000">    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0008">it is relatively slow,</li>        <li id="ul0002-0002" num="0009">and its ratio of performance (performance in computation or performance in other data processing) to energy consumed is quite insufficient.</li>    </ul>    </li></ul></p><p id="p-0009" num="0010">Many large manufacturers of FPGA reconfigurable hardware targets have attempted to market deep learning acceleration solutions and then attempted to encompass as many different neural network algorithms as possible, in order to be able to address the greatest number of potential users. As a result, their hardware accelerator for neural networks has adopted an architecture which allows processing, in a similar manner, neural networks of completely different structures and sizes, ranging from simple to complex. This type of architecture is qualified as systolic; it uses computation elements linked together in the form of a matrix, these computation elements being fed with a direct access cache memory (DMA for &#x201c;Direct Access Memory&#x201d;) which chains the loading and recording of weights and activations from a memory external to the FPGA programmable logic circuit.</p><heading id="h-0003" level="1">OBJECTS OF THE INVENTION</heading><p id="p-0010" num="0011">The aim of the invention is to provide a method for implementing a hardware accelerator for a neural network which at least partially overcomes the above disadvantages.</p><p id="p-0011" num="0012">Indeed, according to the analysis of the invention, this implementation of the prior art remains very, general, meaning that it maintains a matrix of calculation elements as an acceleration kernel which must regularly be reloaded, at each new layer of the neural network or at each new operation performed by the neural network, and even reprogrammed with parameters stored in a memory external to the FPGA programmable logic circuit, which in turn has two disadvantages:<ul id="ul0003" list-style="none">    <li id="ul0003-0001" num="0000">    <ul id="ul0004" list-style="none">        <li id="ul0004-0001" num="0013">It takes a long time to reload the acceleration kernel each time,</li>        <li id="ul0004-0002" num="0014">and this &#x201c;general and universal&#x201d; structure is more difficult to optimize in terms of the ratio of performance to energy consumed.</li>    </ul>    </li></ul></p><p id="p-0012" num="0015">The invention also proposes an implementation of the hardware accelerator in an FPGA programmable logic circuit, but unlike the prior art, the invention proposes an implementation of the entire neural network (and therefore all the layers of this neural network) that is both complete and specifically dedicated to this neural network:<ul id="ul0005" list-style="none">    <li id="ul0005-0001" num="0000">    <ul id="ul0006" list-style="none">        <li id="ul0006-0001" num="0016">thus eliminating the continual use of external memory, thus significantly improving the operating speed of this hardware accelerator,</li>        <li id="ul0006-0002" num="0017">and also allowing better optimization of the ratio of performance to energy consumed, thus also improving the operating efficiency of this hardware accelerator,</li>        <li id="ul0006-0003" num="0018">but using predefined building blocks from a common library for the various implementations,</li>        <li id="ul0006-0004" num="0019">although these building blocks are easily usable thanks to the prior specific formatting of the neural network algorithm, this specific formatting corresponding to a graphical representation,</li>        <li id="ul0006-0005" num="0020">this being done in order to advantageously preserve the following features in the implementation method:        <ul id="ul0007" list-style="none">            <li id="ul0007-0001" num="0021">relatively simple and easy to run,</li>            <li id="ul0007-0002" num="0022">and accessible even to designers and implementers who are not seasoned specialists in the manufacture of hardware accelerators for neural networks.</li>        </ul>        </li>    </ul>    </li></ul></p><p id="p-0013" num="0023">Some embodiments of the invention make it possible to implement an automatic construction chain for a hardware accelerator for algorithms of complex binarized convolutional neural networks.</p><p id="p-0014" num="0024">To this end, the invention provides a method for implementing a hardware accelerator for a neural network, comprising: a step of interpreting an algorithm of the neural network in binary format, converting the neural network algorithm in binary format into a graphical representation, selecting building blocks from a library of predetermined building blocks, creating an organization of the selected building blocks, configuring internal parameters of the building blocks of the organization, so that the organization of the selected and configured building blocks corresponds to said graphical representation; a step of determining an initial set of weights for the neural network; a step of completely synthesizing the organization of the selected and configured building blocks on the one hand in a preselected FPGA programmable logic circuit in a hardware accelerator for the neural network and on the other hand in a software driver for this hardware accelerator, this hardware accelerator being specifically dedicated to the neural network so as to represent the entire architecture of the neural network without needing access to a memory external to the FPGA programmable logic circuit when passing from one layer to another layer of the neural network; a step of loading the initial set of weights for the neural network into the hardware accelerator.</p><p id="p-0015" num="0025">To this end, the invention also provides a circuit board comprising: an FPGA programmable logic circuit; a memory external to the FPGA programmable logic circuit; a hardware accelerator for a neural network: fully implemented in the FPGA programmable logic circuit, specifically dedicated to the neural network so as to be representative of the entire architecture of the neural network without requiring access to a memory external to the FPGA programmable logic circuit when passing from one layer to another layer of the neural network, comprising: an interface to the external memory, an interface to the exterior of the circuit board, an acceleration kernel successively comprising: an information reading block, an information serialization block with two output channels, one to send input data to the layers of the neural network, the other to configure weights at the layers of the neural network, the layers of the neural network, an information deserialization block, an information writing block.</p><p id="p-0016" num="0026">Preferably, the information reading block comprises a buffer memory, and the information writing block comprises a buffer memory.</p><p id="p-0017" num="0027">By, using these buffers, the pace of the acceleration kernel is not imposed on the rest of the system.</p><p id="p-0018" num="0028">To this end, the invention also provides an embedded device comprising a circuit board according to the invention.</p><p id="p-0019" num="0029">The fact that a device is embedded makes the gain in speed and performance particularly critical, for a given mass and a consumed energy both of which the user of the neural network seeks to reduce as much as possible while retaining efficiency, and while guaranteeing simplicity and ease of use of the implementation method, for the designer and the implementer of the neural network.</p><p id="p-0020" num="0030">Preferably, the embedded device according to the invention is an embedded device for computer vision.</p><p id="p-0021" num="0031">According to preferred embodiments, the invention comprises one or more of the following features which can be used separately or in partial combinations or in complete combinations, with one or more of the above objects of the invention.</p><p id="p-0022" num="0032">Preferably, the method for implementing a hardware accelerator for a neural network comprises, before the interpretation step, a step of binarization of the neural network algorithm, including an operation of compressing a floating point format to a binary format.</p><p id="p-0023" num="0033">This preliminary compression operation will make it possible to transform the neural network algorithm so that it is even easier to manipulate in the next steps of the implementation method according to the invention which thus can therefore also accept a wider range of neural network algorithms as input.</p><p id="p-0024" num="0034">Preferably, the method for implementing a hardware accelerator for a neural network comprises, before the interpretation step, a step of selecting from a library of predetermined models of neural network algorithms already in binary format.</p><p id="p-0025" num="0035">Thus, the speed of advancement of the implementation method according to the invention can be further significantly accelerated, which is particularly advantageous in the case of similar or repetitive tasks.</p><p id="p-0026" num="0036">Preferably, the internal parameters comprise the size of the neural network input data.</p><p id="p-0027" num="0037">The size of the input data is a very useful element for configuring the building blocks more efficiently, and this element is readily available, therefore is an element advantageously integrated in a preferential manner into the configuration of building blocks.</p><p id="p-0028" num="0038">Other internal parameters may also comprise the number of logic subcomponents included in each block of random access memory (RAM) and/or the parallelism of the computation block, meaning the number of words processed per clock cycle.</p><p id="p-0029" num="0039">A higher number of logic subcomponents included in each computation block can in particular facilitate the implementation of the logic synthesis, at the cost of an increase in its overall complexity and therefore its cost.</p><p id="p-0030" num="0040">The higher the parallelism of the computation block, the greater the number of words processed per clock cycle. The computation block has a higher throughput, at the expense of a higher complexity and cost.</p><p id="p-0031" num="0041">Preferably, the neural network is convolutional, and the internal parameters also comprise the sizes of the convolutions of the neural network.</p><p id="p-0032" num="0042">This type of neural network is of particularly interest, but it is also a little more complex to implement.</p><p id="p-0033" num="0043">Preferably, the neural network algorithm in binary format is in the ONNX format &#x201c;Open Neural Network eXchange&#x201d;).</p><p id="p-0034" num="0044">This format is particularly attractive, making the implementation method more smooth overall.</p><p id="p-0035" num="0045">Preferably, the organization of the selected and configured building blocks is described by a VHDL code representative of an acceleration kernel of the hardware accelerator.</p><p id="p-0036" num="0046">This type of coding is particularly attractive, making the description of the architecture more complete overall.</p><p id="p-0037" num="0047">Preferably, the synthesizing step and the loading step are carried out by communication between a host computer and an FPGA circuit board including the FPGA programmable logic circuit, this communication advantageously being carried out by means of the OpenCL standard through a PCI Express type of communication channel.</p><p id="p-0038" num="0048">The use of direct and immediate communication, between a host computer and an FPGA circuit board including the FPGA programmable logic circuit, makes the implementation method easier to implement overall for both the designer and the implementer of the neural network.</p><p id="p-0039" num="0049">This type of communication channel and standard are particularly attractive, making the implementation method more smooth overall.</p><p id="p-0040" num="0050">Preferably, the neural network is a neural network applied to computer vision, preferably to embedded computer vision.</p><p id="p-0041" num="0051">In this field of application of computer vision, especially when integrated into embedded devices (therefore not devices which would be fixed in a computer station on the ground), the simultaneous requirements of efficiency, speed, and low energy consumption lead to extensive optimizations which are not very compatible with a simplification of the method for implementing a hardware accelerator for the neural network, which the invention has nevertheless chosen to do, because this unusual compromise in fact works well.</p><p id="p-0042" num="0052">Preferably, the application in computer vision is an application in a surveillance camera, or an application in an image classification system, or an application in a vision device embedded in a motor vehicle.</p><p id="p-0043" num="0053">Other features and advantages of the invention will become apparent upon reading the following description of a preferred embodiment of the invention, given as an example and with reference to the accompanying drawings.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0044" num="0054"><figref idref="DRAWINGS">FIG. <b>1</b></figref> schematically represents an example of a functional architecture enabling the implementation of the method for implementing a hardware accelerator for a neural network according to one embodiment of the invention.</p><p id="p-0045" num="0055"><figref idref="DRAWINGS">FIG. <b>2</b></figref> schematically represents an example of a software architecture enabling the implementation of the method for implementing a hardware accelerator for a neural network according to one embodiment of the invention.</p><p id="p-0046" num="0056"><figref idref="DRAWINGS">FIG. <b>3</b></figref> schematically represents an example of a circuit board implementing a hardware accelerator for a neural network according to one embodiment of the invention.</p><p id="p-0047" num="0057"><figref idref="DRAWINGS">FIG. <b>4</b></figref> schematically represents an example of a kernel of a circuit board implementing a hardware accelerator for a neural network according to one embodiment of the invention.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION OF EMBODIMENTS OF THE INVENTION</heading><p id="p-0048" num="0058"><figref idref="DRAWINGS">FIG. <b>1</b></figref> schematically represents an example of a functional architecture enabling the implementation of the method for implementing a hardware accelerator for a neural network according to one embodiment of the invention.</p><p id="p-0049" num="0059">The architecture has three layers: the models layer <b>1</b>, the software stack layer <b>5</b>, and the hardware stack layer <b>8</b>.</p><p id="p-0050" num="0060">The models layer <b>1</b> comprises a library <b>2</b> of binary models of neural network algorithms already in ONNX format, and a set <b>3</b> of models of neural network algorithms which are pre-trained but in a floating point format (32 bit), including in particular TENSORFLOW, PYTORCH, and CAFFEE2. The method for implementing a hardware accelerator for a neural network has two possible inputs: either a model already present in the library <b>2</b> or a model conventionally pre-trained in a non-fixed software architecture format (&#x201c;framework&#x201d;) belonging to the set <b>3</b>. For a software architecture to be easily compatible with this implementation method, it is interesting to note that a converter of this software architecture to the ONNX format exists, ONNX being a transverse representation in all software architectures.</p><p id="p-0051" num="0061">This set <b>3</b> of models of pre-trained neural network algorithms but in a floating point format can be binarized by a binarizer <b>4</b>, possibly equipped with an additional function of re-training the neural network; for example, for PYTORCH, transforming the neural network algorithm models from a floating point format to a binary format, preferably to the binary ONNX format.</p><p id="p-0052" num="0062">The hardware stack layer <b>8</b> comprises a library <b>9</b> of components, more precisely a library <b>9</b> of predetermined building blocks which will be selected and assembled together and each being parameterized, by the software stack layer <b>5</b> and more precisely by the constructor block <b>6</b> of the software stack layer <b>5</b>.</p><p id="p-0053" num="0063">The software stack layer <b>5</b> comprises, on the one hand, the constructor block <b>6</b> which will generate both the hardware accelerator for the neural network and the software driver for this hardware accelerator for the neural network, and on the other hand the driver block <b>7</b> which will use the driver software to drive the hardware accelerator for the neural network.</p><p id="p-0054" num="0064">More precisely, the constructor block <b>6</b> comprises several functions, including: a graph compilation function which starts with a binarized neural network algorithm; a function for generating code in VHDL format (from &#x201c;VHSIC Hardware Description Language&#x201d;, VHSIC meaning &#x201c;Very High Speed Integrated Circuit&#x201d;), this code in VHDL format containing information both for implementing the hardware accelerator for the neural network and for the driver software of this hardware accelerator; and a synthesis function enabling actual implementation of the hardware accelerator for the neural network on an FPGA programmable logic circuit. The two inputs of the method for implementing the neural network accelerator come together in the constructor block step <b>6</b>, which will study the neural network input algorithm and convert it into a clean graph representation. After this conversion of the graph, two products are therefore generated: the VHDL code describing the hardware accelerator including the acceleration kernel as well as the driver software of this hardware accelerator, which remains to be synthesized using synthesis tools, plus the corresponding neural network configuration weights.</p><p id="p-0055" num="0065">More precisely, the driver block <b>7</b> comprises several functions including: a function of loading the VHDL code, a programming interface, and a function of communication between the host computer and the FPGA programmable logic circuit based on the technology of the OpenCL (&#x201c;Open Computing Language&#x201d;) software infrastructure. Once the hardware accelerator has been synthesized on the chosen target as an FPGA programmable logic circuit, for example using a suite of tools specific to the manufacturer of the FPGA programmable logic circuit, the driver block <b>7</b>, which incorporates an application programming interface (API), for example in the Python programming language and the C++ programming language, is used to drive the hardware accelerator. Communication between the host computer and the FPGA is based on OpenCL technology, which is a standard.</p><p id="p-0056" num="0066">Consequently, great freedom is offered to the user, who can create his or her own program after the generation of the acceleration kernel and the configuration. If the user wishes to target a particular FPGA programmable logic circuit not provided for by the method for implementing a hardware accelerator for a neural network according to the invention, it is still possible, in fact it is sufficient, that this type of model of an FPGA programmable logic circuit be supported by the suite of tools from the vendor of this FPGA programmable logic circuit.</p><p id="p-0057" num="0067">One of the advantageous features of the method for implementing a hardware accelerator for a neural network proposed by the invention is to be compatible with complex neural network structures such as &#x201c;ResNet&#x201d; (for &#x201c;Residential Network&#x201d;) or &#x201c;GoogLeNet&#x201d;. These neural networks have the distinctive feature of divergent data paths, which are then merged or not merged according to various techniques (an &#x201c;elt-wise&#x201d; layer being the most common, for &#x201c;element-wise&#x201d;).</p><p id="p-0058" num="0068">The graph compiler located in the constructor block <b>6</b> recognizes these features and translates them correctly into a corresponding hardware accelerator architecture.</p><p id="p-0059" num="0069"><figref idref="DRAWINGS">FIG. <b>2</b></figref> schematically represents an example of a software architecture enabling implementation of the method for implementing a hardware accelerator for a neural network according to one embodiment of the invention. A training block <b>20</b> corresponds to the re-training function of the binarizer <b>4</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, and a block <b>30</b> of an FPGA tool kit corresponds to the constructor block <b>6</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0060" num="0070">A model <b>12</b> of a convolutional neural network algorithm (CNN), for example in a TENSORFLOW, CARTE, or PYTORCH format, is transformed into a model <b>10</b> of a binarized convolutional neural network algorithm in ONNX format which is sent to an input of a training block <b>20</b>. A set <b>11</b> of training data is sent to another input of this training block <b>20</b> to be transformed into trained weights <b>23</b> by interaction with a description <b>22</b> of the neural network. A conversion by internal representation <b>21</b> is made from the model <b>10</b> of the binarized convolutional neural network algorithm in ONNX format to the description <b>22</b> of the neural network which by interaction on the set <b>11</b> of training data gives the trained weights <b>23</b> which will be sent to an input of the block <b>30</b> of the FPGA toolkit. After this, the description <b>22</b> of the neural network is again converted by internal representation <b>24</b> to a binarized convolutional neural network algorithm <b>25</b> in ONNX format which in turn will be sent to another input of the FPGA toolkit block <b>30</b>.</p><p id="p-0061" num="0071">The binarized convolutional neural network algorithm <b>25</b> in ONNX format is converted by internal representation <b>32</b> and transformed by the cooperation of the construction function <b>33</b> and a data converter <b>34</b> having received the trained weights <b>23</b>, in order to output an instantiation <b>35</b> of files (as &#x201c;.vhd&#x201d;) and a set of weights <b>36</b> (as &#x201c;.data&#x201d;), all using libraries <b>37</b> in the C and C++ programming languages. The data converter <b>34</b> puts the training weights in the proper format and associates them, in the form of a header, with guides, in order to reach the correct destinations in the correct layers of the neural network. The internal representation <b>32</b>, the construction function <b>33</b>, and the data converter <b>34</b> are grouped together in a sub-block <b>31</b>.</p><p id="p-0062" num="0072">At the output from the FPGA tool kit block <b>30</b>, the pair formed by the instantiation <b>35</b> of the files and by the set of weights <b>36</b>, can then either be compiled by an FPGA compiler <b>14</b>, which can however take a considerable amount of time, or where appropriate can be associated with an already precompiled model in an FPGA precompiled library <b>13</b>, which will be much faster but of course requires that this pair correspond to an already precompiled model which exists stored in the FPGA precompiled library <b>13</b>. The obtained result, whether it comes from the FPGA precompiled library <b>13</b> or from the FPGA compiler <b>14</b>, is an FPGA configuration stream <b>15</b>.</p><p id="p-0063" num="0073"><figref idref="DRAWINGS">FIG. <b>3</b></figref> schematically represents an example of a circuit board implementing a hardware accelerator for a neural network according to one embodiment of the invention.</p><p id="p-0064" num="0074">A host computer integrating both a host processor <b>46</b> and a random access memory <b>47</b> (RAM), storing the data <b>48</b> required for the hardware accelerator for the neural network, communicates bidirectionally by means of a serial local bus <b>49</b>, advantageously of the PCIe type (for &#x201c;PO Express&#x201d;, with PCI for &#x201c;Peripheral Component Interconnect&#x201d;), with the FPGA circuit board <b>40</b> implementing the hardware accelerator for the neural network, and in particular its acceleration kernel <b>42</b>.</p><p id="p-0065" num="0075">The FPGA circuit board <b>40</b> comprises an FPGA chip <b>41</b>. This FPGA chip <b>41</b> houses the acceleration kernel <b>42</b> as well as a BSP interface <b>43</b> (for &#x201c;Board Support Package&#x201d;). The FPGA chip <b>41</b>, and in particular the acceleration kernel <b>42</b>, communicates with a memory <b>44</b> integrated onto the FPGA circuit board <b>40</b> via a DDR bus <b>45</b>. The memory <b>44</b> is a memory internal to the FPGA circuit board <b>40</b>, but external to the FPGA electronic chip <b>41</b>; it has a high speed. This memory <b>44</b> is advantageously a memory of the DDR or DDR-2 type (in fact DDR SDRAM for &#x201c;Double Data Rate Synchronous Dynamic Random Access Memory&#x201d;).</p><p id="p-0066" num="0076">When passing from one layer to another layer in the neural network, in the invention, neither the memory <b>47</b> external to the FPGA circuit board <b>40</b>, nor the memory <b>44</b> internal to the FPGA circuit board <b>40</b> but external to the FPGA chip <b>41</b>, are read from in order to load part of the hardware accelerator, unlike the prior art. Indeed, for the invention, the entire architecture of the neural network is loaded all at once at the start into the acceleration kernel <b>42</b> of the FPGA chip <b>41</b>, while for the prior art, each layer is loaded separately after the use of the previous layer which it will then replace, requiring an exchange time and volume between the FPGA chip <b>41</b> and to the exterior of this FPGA chip <b>41</b> that are much greater than those of the invention for the same type of operation of the implemented neural network, therefore offering a much lower operating efficiency than that of the invention. It is because it is specifically dedicated to the neural network that the hardware accelerator can be loaded all at once; conversely, in the prior art, the hardware accelerator is general-purpose, and it must then be loaded layer by layer in order to &#x201c;reprogram&#x201d; it for each new layer, a loading all at once not being possible in the prior art without resorting to a very large size for the hardware accelerator. In the specific (and dedicated) hardware accelerator of the invention, the topology is multi-layered, which allows it, to be entirely implemented all at once without requiring too large of a size for the hardware accelerator, while in the prior art, the general-purpose hardware accelerator implements different topologies, one topology for each layer.</p><p id="p-0067" num="0077"><figref idref="DRAWINGS">FIG. <b>3</b></figref> thus represents the overall architecture of the system, comprising on the one hand the host machine including the host processor <b>46</b> and host memory <b>47</b>, the user performing actions on said host machine, and on the other hand the hardware accelerator implemented on the FPGA circuit board <b>40</b>. The host processor <b>46</b>, which is general-purpose, controls and sends inputs/outputs, via a high-speed communication channel <b>49</b>, to the accelerator FPGA circuit board <b>40</b> equipped with an FPGA chip <b>41</b> (FPGA programmable logic circuit), said FPGA chip <b>41</b> advantageously supporting the &#x201c;OpenCL&#x201d; standard.</p><p id="p-0068" num="0078"><figref idref="DRAWINGS">FIG. <b>4</b></figref> schematically represents an example of a kernel of a circuit board implementing a hardware accelerator for a neural network according to one embodiment of the invention.</p><p id="p-0069" num="0079">The acceleration kernel <b>42</b> communicates with the BSP interface <b>43</b> (also based on the &#x201c;OpenCL&#x201d; communication standard), this communication being represented more specifically in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, via an &#x201c;Avalon&#x201d; read interface <b>52</b> to a reading unit <b>50</b>, in particular in order to receive from the host computer the input images and the configuration of the neural network, and via an &#x201c;Avalon&#x201d; write interface <b>92</b> to a writing block <b>90</b>, in order to provide the obtained results that are output from the neural network. Furthermore, the acceleration kernel <b>42</b> receives the external parameters supplied by the user and more particularly from its call via the host processor, these external parameters arriving at the buffer memory <b>55</b> of the reading unit <b>50</b>, the serialization unit <b>60</b>, and the buffer memory <b>95</b> of the writing block <b>90</b>.</p><p id="p-0070" num="0080">The acceleration kernel <b>42</b> successively comprises, in series, firstly the reading unit <b>50</b>, then the serialization block <b>60</b>, then the layers <b>70</b> of the neural network itself, then the deserialization block <b>80</b>, and finally the writing block <b>90</b>. The signals reach the reading unit <b>50</b> via the read interface <b>52</b>, and exit from their source writing block <b>90</b> via the write interface <b>92</b> by passing successively through the serialization block <b>60</b>, layers <b>70</b> of the neural network, and the deserialization block <b>80</b>. Packet management is ensured from start to end, from packet management <b>54</b> in the reading unit <b>50</b> to packet management <b>94</b> in the writing block <b>90</b>, travelling successively (dotted lines) through the serialization block <b>60</b>, the layers <b>70</b> of the neural network, and the deserialization block <b>80</b>.</p><p id="p-0071" num="0081">The reading unit <b>50</b> comprises a read interface <b>52</b> at its input, and comprises at its output a line <b>53</b> for sending input data (for the next serialization block <b>60</b>) confirmed as ready for use. The reading unit <b>50</b> comprises a buffer memory <b>55</b> including registers <b>56</b> and <b>57</b> respectively receiving the external parameters &#x201c;pin&#x201d; and &#x201c;iter_i&#x201d;.</p><p id="p-0072" num="0082">The serialization block <b>60</b> transforms the data <b>53</b> arriving from the reading unit <b>50</b> into data <b>65</b> stored in registers <b>61</b> to <b>64</b>, for example in 512 registers although only 4 registers are represented in <figref idref="DRAWINGS">FIG. <b>4</b></figref>. These data stored in registers <b>61</b> to <b>64</b> will then be sent into the layers <b>70</b> of the neural network, either by the inference path <b>77</b> for the input data of the neural network, or by the configuration path <b>78</b> for the configuration weights of the neural network. A selector <b>68</b> selects either the inference path <b>77</b> or the configuration path <b>78</b>, depending on the type of data to be sent to the layers <b>70</b> of the neural network.</p><p id="p-0073" num="0083">The layers <b>70</b> of the neural network implement the multilayer topology of the neural network; here only 6 layers <b>71</b>, <b>72</b>, <b>73</b>, <b>74</b>, <b>75</b> and <b>76</b> are shown, but there may be more, or even significantly more, and also slightly fewer. Preferably, the neural network comprises at least 2 layers, more preferably at least 3 layers, even more preferably at least 5 layers, advantageously at least 10 layers. It is preferably a convolutional neural network.</p><p id="p-0074" num="0084">The deserialization block <b>80</b> stores in registers <b>81</b> to <b>84</b> the data <b>87</b> arriving by the inference path <b>77</b>, for example in 512 registers although only 4 registers are represented in <figref idref="DRAWINGS">FIG. <b>4</b></figref>. These data stored in registers <b>81</b> to <b>84</b> will then be sent to the writing block <b>90</b>, more specifically from the output <b>85</b> of the deserialization block <b>80</b> to then be transmitted to the input <b>93</b> of the writing block <b>90</b>. These data <b>87</b> are the data output from the neural network, the data resulting from the successive passage through layers <b>71</b> to <b>76</b> of neurons, meaning that they correspond to the desired result obtained after processing by the neural network.</p><p id="p-0075" num="0085">The writing block <b>90</b> at its output comprises a write interface <b>92</b>, and at its input comprises a line <b>93</b> for receiving the output data (from the previous deserialization block <b>80</b>) confirmed ready to be transmitted to outside the acceleration kernel <b>42</b>. The writing block <b>90</b> comprises a buffer memory <b>95</b> including registers <b>96</b> and <b>97</b> respectively receiving the external parameters &#x201c;pout&#x201d; and &#x201c;iter_o&#x201d;.</p><p id="p-0076" num="0086">An example of a possible use of the method for implementing a hardware accelerator for a neural network according to the invention is now presented. A user will offload the inference of a &#x201c;ResNet-50&#x201d; type of network from a general-purpose microprocessor of the central processing unit type (CPU) to a more suitable hardware target, particularly from an energy performance standpoint. This user selects a target FPGA programmable logic circuit. He or she can use a pre-trained model of a neural network algorithm in a format such as &#x201c;PyTorch&#x201d;, which can be found on the Internet. This model of a neural network algorithm contains the configuration weights in a floating point representation of the neural network trained on a particular data set (&#x201c;CIFAR-10&#x201d; for example). The user can then select this model of a neural network algorithm in order to use the method for implementing a hardware accelerator for a neural network according to the invention. The user will then obtain an FPGA project as output, which the user will then synthesize before passing it on to a circuit board, as well as a binarized configuration compatible with the binary representation of the hardware accelerator for the neural network. This step will require the installation of proprietary tools corresponding to the target FPGA programmable logic circuit.</p><p id="p-0077" num="0087">Next, the user runs the scripts of the constructor block <b>6</b> automatically generating the configuration of the target FPGA programmable logic circuit, in order to provide the hardware accelerator. When the user has this output, he or she uses the driver block <b>7</b> to load the description of the accelerator (&#x201c;ResNet-50&#x201d; network) into the target FPGA programmable logic circuit, provide the configuration of the pre-trained then binarized weights to the hardware accelerator, provide a set of input images, and retrieve the results of the neural network algorithm as output from the hardware accelerator.</p><p id="p-0078" num="0088">It is possible to dispense with the relatively time-consuming portion of generating the hardware architecture from the &#x201c;PyTorch&#x201d; representation, provided that models are used from the library of precompiled networks. If the user chooses a hardware accelerator whose topology has already been generated (by the user or provided by the library of precompiled neural network algorithms), he or she only has to go through the step of model weight binarization, which is very fast, for example about a second.</p><p id="p-0079" num="0089">Of course, the invention is not limited to the examples and to the embodiment described and shown, but is capable of numerous variants accessible to those skilled in the art.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method for implementing a hardware accelerator for a neural network, comprising:<claim-text>interpreting an algorithm of the neural network algorithm in binary format;<claim-text>converting the neural network algorithm in binary format (<b>25</b>) into a graphical representation by:</claim-text><claim-text>selecting building blocks from a library (<b>37</b>) of predetermined building blocks;</claim-text><claim-text>creating (<b>33</b>) an organization of the selected building blocks; and</claim-text><claim-text>configuring internal parameters of the building blocks of the organization;</claim-text><claim-text>where the organization of the selected and configured building blocks corresponds to said graphical representation,</claim-text></claim-text><claim-text>determining an initial set (<b>36</b>) of weights for the neural network,</claim-text><claim-text>completely synthesizing (<b>13</b>, <b>14</b>) the organization of the selected and configured building blocks on the one hand in a preselected FPGA programmable logic circuit (<b>41</b>) in a hardware accelerator (<b>42</b>) for the neural network and on the other hand in a software driver for the hardware accelerator (<b>42</b>), the hardware accelerator (<b>42</b>) being specifically dedicated to the neural network so as to represent an entire architecture of the neural network without needing access to a memory (<b>44</b>) external to the FPGA programmable logic circuit (<b>41</b>) when passing from one layer (<b>71</b> to <b>75</b>) to another layer (<b>72</b> to <b>76</b>) of the neural network; and</claim-text><claim-text>loading (<b>48</b>) the initial set of weights for the neural network into the hardware accelerator (<b>42</b>).</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method for implementing a hardware accelerator for a neural network according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising, before the interpretation step (<b>6</b>, <b>30</b>)), binarizing (<b>4</b>, <b>20</b>) of the neural network algorithm, including an operation of compressing a floating point format to a binary format.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method for implementing a hardware accelerator for a neural network according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising, before the interpretation step (<b>6</b>, <b>30</b>), selecting from a library (<b>8</b>, <b>37</b>) of predetermined models of neural network algorithms already in binary format.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method for implementing a hardware accelerator for a neural network according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the internal parameters comprise a size of the neural network input data.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method for implementing a hardware accelerator for a neural network according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the neural network is convolutional, and the internal parameters also comprise sizes of the convolutions of the neural network.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method for implementing a hardware accelerator for a neural network according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the neural network algorithm in binary format (<b>25</b>) is in an ONNX format.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method for implementing a hardware accelerator for a neural network according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the organization of the selected and configured building blocks is described by a VHDL code (<b>15</b>) representative of an acceleration kernel of the hardware accelerator (<b>42</b>).</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method for implementing a hardware accelerator for a neural network according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the synthesizing step (<b>13</b>, <b>14</b>) and the loading step (<b>48</b>) are carried out by communication between a host computer (<b>46</b>, <b>47</b>) and an FPGA circuit board (<b>40</b>) including the FPGA programmable logic circuit (<b>41</b>), this communication advantageously being carried out by means of an OpenCL standard through a PCI Express type of communication channel (<b>49</b>).</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method for implementing a hardware accelerator for a neural network according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the neural network is a neural network configured for an application in computer vision.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method for implementing a hardware accelerator for a neural network according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the application in computer vision is an application in a surveillance camera, or an application in an image classification system, or an application in a vision device embedded in a motor vehicle.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. A circuit board (<b>40</b>) comprising:<claim-text>an FPGA programmable logic circuit (<b>41</b>);</claim-text><claim-text>a memory external to the FPGA programmable logic circuit (<b>44</b>); and</claim-text><claim-text>a hardware accelerator for a neural network that is fully implemented in the FPGA programmable logic circuit (<b>41</b>), and specifically dedicated to the neural network so as to be representative of an entire architecture of the neural network without requiring access to a memory (<b>44</b>) external to the FPGA programmable logic circuit when passing from one layer (<b>71</b> to <b>75</b>) to another layer (<b>72</b> to <b>76</b>) of the neural network, the hardware accelerator comprising:<claim-text>an interface (<b>45</b>) to the external memory;</claim-text><claim-text>an interface (<b>49</b>) to an exterior of the circuit board; and</claim-text><claim-text>an acceleration kernel (<b>42</b>) successively comprising:<claim-text>an information reading block (<b>50</b>);</claim-text><claim-text>an information serialization block (<b>60</b>) with two output channels (<b>77</b>, <b>78</b>) including a first output channel (<b>77</b>) to send input data to the layers (<b>70</b>-<b>76</b>) of the neural network, and a second output channel (<b>78</b>) to configure weights at the layers (<b>70</b>-<b>76</b>) of the neural network;</claim-text><claim-text>the layers (<b>70</b>-<b>76</b>) of the neural network;</claim-text><claim-text>an information deserialization block (<b>80</b>); and</claim-text><claim-text>an information writing block (<b>90</b>).</claim-text></claim-text></claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The circuit board according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the information reading block (<b>50</b>) comprises a buffer memory (<b>55</b>), and the information writing block (<b>90</b>) comprises a buffer memory (<b>95</b>).</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. An embedded device, comprising a circuit board according to <claim-ref idref="CLM-00011">claim 11</claim-ref>.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The embedded device according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the embedded device is an embedded device for computer vision.</claim-text></claim></claims></us-patent-application>