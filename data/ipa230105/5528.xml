<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005529A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005529</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17747491</doc-number><date>20220518</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>KR</country><doc-number>10-2021-0087943</doc-number><date>20210705</date></priority-claim><priority-claim sequence="02" kind="national"><country>KR</country><doc-number>10-2021-0139252</doc-number><date>20211019</date></priority-claim><priority-claim sequence="03" kind="national"><country>KR</country><doc-number>10-2021-0154801</doc-number><date>20211111</date></priority-claim><priority-claim sequence="04" kind="national"><country>KR</country><doc-number>10-2021-0181445</doc-number><date>20211217</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>11</class><subclass>C</subclass><main-group>11</main-group><subgroup>54</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>063</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>11</class><subclass>C</subclass><main-group>13</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>11</class><subclass>C</subclass><main-group>11</main-group><subgroup>16</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>11</class><subclass>C</subclass><main-group>11</main-group><subgroup>54</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>0635</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>11</class><subclass>C</subclass><main-group>13</main-group><subgroup>004</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>11</class><subclass>C</subclass><main-group>11</main-group><subgroup>161</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e116">NEUROMORPHIC DEVICE AND ELECTRONIC DEVICE INCLUDING THE SAME</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Samsung Electronics Co., Ltd.</orgname><address><city>Suwon-si</city><country>KR</country></address></addressbook><residence><country>KR</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>HWANG</last-name><first-name>Youngnam</first-name><address><city>Hwaseong-si</city><country>KR</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>Samsung Electronics Co., Ltd.</orgname><role>03</role><address><city>Suwon-si</city><country>KR</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A neuromorphic device includes a plurality of cell tiles including a cell array including a plurality of memory cells storing a weight of a neural network, a row driver connected to the plurality of memory cells, and cell analog-digital converters connected to the plurality of memory cells and converting cell currents into a plurality of pieces of digital cell data, a reference tile including a plurality of reference cells, a reference row driver connected to the plurality of reference cells, and reference analog-digital converters connected to the plurality of reference cells and converting reference currents read via the plurality of reference column lines into a plurality of pieces of digital reference data, and a comparator circuit configured to compare the plurality of pieces of digital cell data with the plurality of pieces of digital reference data, respectively.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="66.80mm" wi="152.40mm" file="US20230005529A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="201.93mm" wi="154.43mm" file="US20230005529A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="156.97mm" wi="120.99mm" file="US20230005529A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="118.36mm" wi="103.12mm" file="US20230005529A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="111.34mm" wi="101.77mm" file="US20230005529A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="123.02mm" wi="122.94mm" file="US20230005529A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="111.34mm" wi="104.90mm" file="US20230005529A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="177.63mm" wi="156.38mm" file="US20230005529A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="208.96mm" wi="150.54mm" orientation="landscape" file="US20230005529A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="186.86mm" wi="156.80mm" orientation="landscape" file="US20230005529A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="205.66mm" wi="151.05mm" file="US20230005529A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="176.87mm" wi="153.25mm" file="US20230005529A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="217.42mm" wi="167.13mm" orientation="landscape" file="US20230005529A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="217.42mm" wi="166.45mm" orientation="landscape" file="US20230005529A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="217.42mm" wi="167.72mm" orientation="landscape" file="US20230005529A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="217.42mm" wi="166.12mm" orientation="landscape" file="US20230005529A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="129.54mm" wi="158.67mm" file="US20230005529A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="137.84mm" wi="158.83mm" file="US20230005529A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="223.94mm" wi="150.03mm" file="US20230005529A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00019" num="00019"><img id="EMI-D00019" he="226.74mm" wi="130.39mm" file="US20230005529A1-20230105-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00020" num="00020"><img id="EMI-D00020" he="211.16mm" wi="155.28mm" orientation="landscape" file="US20230005529A1-20230105-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00021" num="00021"><img id="EMI-D00021" he="220.56mm" wi="129.46mm" file="US20230005529A1-20230105-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00022" num="00022"><img id="EMI-D00022" he="216.49mm" wi="130.22mm" file="US20230005529A1-20230105-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00023" num="00023"><img id="EMI-D00023" he="203.37mm" wi="164.51mm" orientation="landscape" file="US20230005529A1-20230105-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00024" num="00024"><img id="EMI-D00024" he="158.24mm" wi="157.73mm" orientation="landscape" file="US20230005529A1-20230105-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00025" num="00025"><img id="EMI-D00025" he="199.98mm" wi="158.07mm" orientation="landscape" file="US20230005529A1-20230105-D00025.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00026" num="00026"><img id="EMI-D00026" he="212.77mm" wi="161.80mm" orientation="landscape" file="US20230005529A1-20230105-D00026.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00027" num="00027"><img id="EMI-D00027" he="212.77mm" wi="162.14mm" orientation="landscape" file="US20230005529A1-20230105-D00027.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00028" num="00028"><img id="EMI-D00028" he="122.00mm" wi="108.03mm" file="US20230005529A1-20230105-D00028.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00029" num="00029"><img id="EMI-D00029" he="162.48mm" wi="160.10mm" file="US20230005529A1-20230105-D00029.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00030" num="00030"><img id="EMI-D00030" he="147.57mm" wi="108.37mm" file="US20230005529A1-20230105-D00030.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00031" num="00031"><img id="EMI-D00031" he="178.90mm" wi="160.10mm" file="US20230005529A1-20230105-D00031.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00032" num="00032"><img id="EMI-D00032" he="211.67mm" wi="152.40mm" file="US20230005529A1-20230105-D00032.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00033" num="00033"><img id="EMI-D00033" he="167.98mm" wi="166.96mm" file="US20230005529A1-20230105-D00033.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00034" num="00034"><img id="EMI-D00034" he="98.89mm" wi="145.12mm" file="US20230005529A1-20230105-D00034.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00035" num="00035"><img id="EMI-D00035" he="207.94mm" wi="90.59mm" file="US20230005529A1-20230105-D00035.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00036" num="00036"><img id="EMI-D00036" he="208.45mm" wi="89.83mm" file="US20230005529A1-20230105-D00036.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00037" num="00037"><img id="EMI-D00037" he="188.21mm" wi="146.73mm" orientation="landscape" file="US20230005529A1-20230105-D00037.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00038" num="00038"><img id="EMI-D00038" he="114.47mm" wi="126.75mm" file="US20230005529A1-20230105-D00038.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00039" num="00039"><img id="EMI-D00039" he="190.08mm" wi="116.16mm" orientation="landscape" file="US20230005529A1-20230105-D00039.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00040" num="00040"><img id="EMI-D00040" he="189.74mm" wi="152.82mm" orientation="landscape" file="US20230005529A1-20230105-D00040.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00041" num="00041"><img id="EMI-D00041" he="215.56mm" wi="91.27mm" file="US20230005529A1-20230105-D00041.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00042" num="00042"><img id="EMI-D00042" he="95.00mm" wi="89.83mm" file="US20230005529A1-20230105-D00042.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00043" num="00043"><img id="EMI-D00043" he="128.35mm" wi="151.98mm" file="US20230005529A1-20230105-D00043.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00044" num="00044"><img id="EMI-D00044" he="214.88mm" wi="154.35mm" file="US20230005529A1-20230105-D00044.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00045" num="00045"><img id="EMI-D00045" he="167.72mm" wi="146.90mm" file="US20230005529A1-20230105-D00045.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00046" num="00046"><img id="EMI-D00046" he="166.96mm" wi="130.81mm" file="US20230005529A1-20230105-D00046.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00047" num="00047"><img id="EMI-D00047" he="100.58mm" wi="142.66mm" file="US20230005529A1-20230105-D00047.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">CROSS TO REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application claims benefit of priority to Korean Patent Application No. 10-2021-0181445 filed on Dec. 17, 2021, Korean Patent Application No. 10-2021-0154801 filed on Nov. 11, 2021, Korean Patent Application No. 10-2021-0139252 filed on Oct. 19, 2021, and Korean Patent Application No. 10-2021-0087943 filed on Jul. 5, 2021 in the Korean Intellectual Property Office, the disclosures of which are incorporated herein by reference in their entirety.</p><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">Example embodiments of the present disclosure relate to a neuromorphic device and an electronic device including the same.</p><p id="p-0004" num="0003">A neuromorphic device is a semiconductor device simulating an information processing method of the human brain by manufacturing an artificial nervous system at the neuron level, and may implement, e.g., a deep learning neural network for neuromorphic computing. A neuromorphic device may execute a multiply and accumulate (MAC) computation of multiplying input data by a weight and summing results of the multiplication, and may include a plurality of memory cells for storing a weight in the form of data to execute the computation. Various methods for improving performance of a neuromorphic device, increasing integration density, and/or reducing computation burden and power consumption have been suggested.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0005" num="0004">An example embodiment of the present disclosure is to provide a neuromorphic device which may, by separately implementing a reference cell array outputting a reference current to obtain accurate results of computations in various environments, improve integration density and power consumption, and also to provide a structure and a method for efficiently processing computation using a zero point weight in a neuromorphic device performing computations by quantizing a weight and an input value.</p><p id="p-0006" num="0005">According to an example embodiment of the present disclosure, a neuromorphic device includes a plurality of cell tiles including a cell array including a plurality of memory cells storing a weight of a neural network, a row driver connected to the plurality of memory cells via a plurality of row lines, and cell analog-digital converters connected to the plurality of memory cells via a plurality of column lines and configured to convert cell currents read via the plurality of column lines into a plurality of pieces of digital cell data, respectively, a reference tile including a reference cell array including a plurality of reference cells, a reference row driver connected to the plurality of reference cells via a plurality of reference row lines, and reference analog-digital converters connected to the plurality of reference cells via a plurality of reference column lines and configured to convert reference currents read via the plurality of reference column lines into a plurality of pieces of digital reference data, and a comparator circuit configured to compare the plurality of pieces of digital cell data with the plurality of pieces of digital reference data, respectively.</p><p id="p-0007" num="0006">According to an example embodiment of the present disclosure, a neuromorphic device includes a plurality of cell arrays in which a plurality of memory cells storing weights included in a plurality of layers of a neural network are disposed, respectively, a reference cell array in which a plurality of reference cells having the same structure as a structure of the plurality of memory cells are disposed, and a comparator circuit configured to compare a plurality of pieces of cell data obtained from at least one of the plurality of cell arrays with a plurality of pieces of reference data obtained from the reference array, wherein the plurality of cell arrays include a first cell array and a second cell array configured to store the weights included in the single layer among the plurality of layers in a distributed manner, and wherein the comparator circuit simultaneously compares the plurality of pieces of reference data obtained from one of the reference cell arrays with the plurality of pieces of cell data obtained from the first cell array and the plurality of pieces of cell data obtained from the second cell array.</p><p id="p-0008" num="0007">According to an example embodiment of the present disclosure, a neuromorphic device includes a plurality of cell tiles in which a plurality of memory cells storing weights included in a plurality of layers of a neural network are disposed, respectively, a buffer configured to store a plurality of pieces of digital reference data obtained by digitally converting reference currents when an inference operation using the neural network starts, and a comparator circuit configured to compare a plurality of pieces of digital cell data output by at least one of the plurality of cell tiles with the plurality of pieces of digital reference data received from the buffer.</p><p id="p-0009" num="0008">According to an example embodiment of the present disclosure, a neuromorphic device includes at least one weight array including a plurality of weight cells connected to a plurality of weight row lines and a plurality of weight column lines, a zero point array including a plurality of zero point cells connected to a plurality of zero point row lines and a plurality of zero point column lines, and a logic circuit configured to generate quantized weights and zero point weights by quantizing real weights included in each of a plurality of layers of a neural network, and to store the quantized weights in the weight cells and to store the zero point weights in the zero point cells, wherein the weight row lines and the zero point row lines are separated from each other, and the weight column lines and the zero point column lines are separated from each other.</p><p id="p-0010" num="0009">According to an example embodiment of the present disclosure, a neuromorphic device includes a weight array including weight cells configured to store quantized weights among quantized weights and zero point weights generated from real weights included in a single layer among a plurality of layers of a neural network system, a zero point array including zero point cells in which the zero point weight is stored, and a logic circuit configured to generate quantized input values and zero point input values by quantizing input values input to the single layer, to obtain a result of weight computation and a result of zero point computation by inputting the quantized input values and the zero point input value to the weight array and the zero point array, respectively, and to obtain output values of the single layer by summing the result of weight computation and the result of zero point computation.</p><p id="p-0011" num="0010">According to an example embodiment of the present disclosure, a neuromorphic device includes weight arrays configured to store quantized weights obtained by quantizing real weights included in respective hidden layers of a neural network, a zero point array configured to store a zero point weight, and a logic circuit configured to input quantized input values obtained by quantizing input values and zero point input values to the weight arrays and the zero point array corresponding to a single layer among the hidden layers and to obtain a result of multiplication and accumulation computations with respect to the single layer, wherein the zero point array includes a plurality of zero point areas storing the zero point weights of the hidden layers, and zero point cells connected to a single zero point column line in each of the zero point areas store the same data, and wherein the logic circuit includes a buffer configured to store results of zero point computation obtained by multiplying the quantized input values and the zero point input value by the zero point weights, and the buffer stores the results of zero point computation according to the number of 1s included in the hidden layers and the input values.</p><p id="p-0012" num="0011">According to an example embodiment of the present disclosure, a neuromorphic device includes a computational processor configured to, from among zero point weights and quantized weights generated by quantizing real weights included in each of a plurality of layers of a neural network, compute quantized weights and input values input to each of the plurality of layers and to output a result of weight computation, a counter circuit configured to count the number of 1s included in the input values, a buffer configured to receive results of zero point computation obtained by computing the input values and the zero point weight from an external host and to store the results, and to output one of the results of zero point computation based on the number of 1s included in the input values, and an adder circuit configured to add the result of zero point computation output by the buffer to the result of weight computation and to output computation results of one of the plurality of layers.</p><p id="p-0013" num="0012">According to an example embodiment of the present disclosure, a neuromorphic device includes a computational processor configured to, from among zero point weights and quantized weights obtained by quantizing real weights included in at least one of a plurality of layers of a neural network, receive the quantized weights, to compute input values input to the at least one of the plurality of layers with the quantized weights and to output a result of weight computation, a buffer configured to receive results of zero point computation obtained by computing the input values and the zero point weight from an external host and to store the result, and to output one of the results of zero point computation based on the number of 1s included in the input values, and an adder circuit configured to add the result of zero point computation output by the buffer to the result of weight computation and to output computation results of one of the plurality of layers.</p><p id="p-0014" num="0013">According to an example embodiment of the present disclosure, an electronic device includes a neuromorphic device configured to generate output data by executing a computation corresponding to a plurality of layers included in a neural network, and a host connected to the neuromorphic device, wherein the host is configured to, from among zero point weights and quantized weights obtained by quantizing real weights included in each of the plurality of layers, generate results of zero point computation obtained by multiplying and accumulating the zero point weights and input values input to the plurality of layers, respectively, and to transmit the results to the neuromorphic device, wherein the neuromorphic device includes a buffer configured to store the results of zero point computation, a computational processor configured to output a result of weight computation obtained by multiplying and accumulating the input values and the quantized weights to execute accumulation and multiplication computations corresponding to each of the plurality of layers, and an adder circuit configured to add the result of zero point computation to the result of weight computation and to output result values of computation for each of the plurality of layers.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0015" num="0014">The above and other aspects, features, and advantages of the present disclosure will be more clearly understood from the following detailed description, taken in conjunction with the accompanying drawings, in which:</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref> are diagrams illustrating a neural network implemented as a neuromorphic device according to some example embodiments of the present disclosure;</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIGS. <b>2</b>A and <b>2</b>B</figref> are diagrams illustrating a cell array included in a neuromorphic device according to some example embodiments of the present disclosure;</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIGS. <b>3</b>, <b>4</b>A, and <b>4</b>B</figref> are diagrams illustrating a neuromorphic device according to a comparative example and some example embodiments of the present disclosure;</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIGS. <b>5</b> and <b>6</b></figref> are diagrams illustrating operations of a neuromorphic device according to some example embodiments of the present disclosure;</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a diagram illustrating a structure of a reference tile included in neuromorphic device according to some example embodiments of the present disclosure;</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a diagram illustrating a neuromorphic device according to some example embodiments of the present disclosure;</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a flowchart illustrating operations of a neuromorphic device according to some example embodiments of the present disclosure;</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIGS. <b>10</b>A to <b>10</b>C</figref> are diagrams illustrating operations of a neuromorphic device according to some example embodiments of the present disclosure;</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIGS. <b>11</b> to <b>14</b></figref> are diagrams illustrating neuromorphic devices according to some example embodiments of the present disclosure;</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIGS. <b>15</b> and <b>16</b></figref> are diagrams illustrating comparator circuits included in a neuromorphic device according to some example embodiments of the present disclosure;</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is a diagram illustrating a neuromorphic device according to some example embodiments of the present disclosure;</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>18</b></figref> is a block diagram illustrating a neuromorphic device according to some example embodiments of the present disclosure;</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>19</b></figref> is a block diagram illustrating a neuromorphic device according to some example embodiments of the present disclosure;</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIGS. <b>20</b>A to <b>20</b>C</figref> are diagrams illustrating quantization in a neuromorphic device according to some example embodiments of the present disclosure;</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>21</b></figref> is a diagram illustrating quantization in a neuromorphic device according to some example embodiments of the present disclosure;</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>22</b></figref> is a diagram illustrating a neuromorphic device according to some example embodiments of the present disclosure;</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIGS. <b>23</b> and <b>24</b></figref> are diagrams illustrating neuromorphic devices according to some example embodiments of the present disclosure;</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIGS. <b>25</b> and <b>26</b></figref> are diagrams illustrating neuromorphic device according to some example embodiments of the present disclosure;</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>27</b></figref> is a flowchart illustrating operations of a neuromorphic device according to an example embodiment of the present disclosure;</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>28</b></figref> is a diagram illustrating operations of a neuromorphic device according to some example embodiments of the present disclosure;</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>29</b></figref> is a flowchart illustrating operations of a neuromorphic device according to some example embodiments of the present disclosure;</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>30</b></figref> is a diagram illustrating operations of a neuromorphic device according to some example embodiments of the present disclosure;</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>31</b></figref> is a diagram illustrating data stored in a buffer of a neuromorphic device according to some example embodiments of the present disclosure;</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIGS. <b>32</b> and <b>33</b></figref> are block diagrams illustrating electronic devices including a neuromorphic device according to some example embodiments of the present disclosure;</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>34</b></figref> is a block diagram illustrating a neuromorphic device according to some example embodiments of the present disclosure;</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIGS. <b>35</b>A to <b>35</b>D</figref> are diagrams illustrating quantization in a neuromorphic device according to some example embodiments of the present disclosure;</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>36</b></figref> is a block diagram illustrating a neuromorphic device according to some example embodiments of the present disclosure;</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. <b>37</b></figref> is a diagram illustrating data stored in a buffer of a neuromorphic device according to some example embodiments of the present disclosure;</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIGS. <b>38</b> and <b>39</b></figref> are block diagrams illustrating neuromorphic devices according to some example embodiments of the present disclosure;</p><p id="p-0045" num="0044"><figref idref="DRAWINGS">FIGS. <b>40</b>A to <b>40</b>C</figref> are diagrams illustrating quantization in a neuromorphic device according to some example embodiments of the present disclosure;</p><p id="p-0046" num="0045"><figref idref="DRAWINGS">FIGS. <b>41</b> and <b>42</b></figref> are diagrams illustrating operations of a computational processor included in a neuromorphic device according to some example embodiments of the present disclosure;</p><p id="p-0047" num="0046"><figref idref="DRAWINGS">FIGS. <b>43</b> and <b>44</b></figref> are diagrams illustrating operations of a computational processor included in a neuromorphic device according to some example embodiments of the present disclosure; and</p><p id="p-0048" num="0047"><figref idref="DRAWINGS">FIG. <b>45</b></figref> is a block diagram illustrating a semiconductor device including a neuromorphic device according to some example embodiments of the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0049" num="0048">Hereinafter, embodiments of the present disclosure will be described as follows with reference to the accompanying drawings.</p><p id="p-0050" num="0049"><figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref> are diagrams illustrating a neural network implemented as a neuromorphic device according to some example embodiments.</p><p id="p-0051" num="0050">Referring to <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>, a network structure of a general neural network may include an input layer IL, a plurality of hidden layers HL<b>1</b>-HLn, and an output layer OL. The input layer IL may include an i number of input nodes xl-xi (where i is a natural number), and vector input data IDAT having a length of i may be input to each input node.</p><p id="p-0052" num="0051">The input data IDAT may be input to a hidden layer including n number of hidden layers HL<b>1</b>-HLn (where n is a natural number), and each of the hidden layers HL<b>1</b>-HLn may include a plurality of hidden nodes. For example, the first hidden layer HL<b>1</b> may include m number of hidden nodes h<b>11</b>-h<b>1</b><i>m </i>(where m is a natural number), and the n-th hidden layer HLn may include them number of hidden nodes hn<b>1</b>-hnm.</p><p id="p-0053" num="0052">In the example illustrated in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>, each of the hidden layers HL<b>1</b> to HLn may include the same number of hidden nodes, but the example embodiments are not limited thereto. For example, at least a portion of the hidden layers HL<b>1</b>-HLn may include different numbers of hidden nodes.</p><p id="p-0054" num="0053">The output layer OL may include a j number of output nodes y<b>1</b>-yj (where j is a natural number) corresponding to the class to be classified. For example, the output layer OL may output a result (e.g., a score and/or a class score) for each class with respect to the input data IDAT as the output data ODAT. For example, in some example embodiments, the score (and/or class score) may include the probabilities of the input data IDAT being related to (and/or included in) the classifications corresponding to the output nodes. The result may also be referred to as an inference, and the operation resulting in the inference may be referred to as an inference operation.</p><p id="p-0055" num="0054">The neural network illustrated in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref> may include a branch between nodes illustrated as a linear line between two nodes, and a weight used in each branch. In some examples, the nodes included in single layer may not be connected to each other, and the nodes included in different layers may be entirely or partially connected to each other. For example, a node (e.g., node h<b>11</b> of the HL<b>1</b> layer) may be connected to all and/or some of the nodes of a subsequent layer (e.g., nodes h<b>21</b> through h<b>2</b><i>m </i>of the HL<b>2</b> layer).</p><p id="p-0056" num="0055">Each node in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref> may receive an output of a previous node and perform a computation, and may output a result of the computation to a subsequent node. For example, each node may apply the input value to a specific function (for example a nonlinear function) and may compute a value to be output.</p><p id="p-0057" num="0056">Generally, the structure of a neural network may be predetermined (and/or otherwise determined), and the weights according to the branches between nodes may be determined as appropriate values using, e.g., a data set of which a correct answer is already known. A data set of which a correct answer is already known and which may be used to determine the weights, may be referred to as training data, and the process of determining weights using the training data may be referred to as learning.</p><p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. <b>1</b>B</figref> is a diagram illustrating some example embodiments of a computation performed in a single node ND among the nodes included in, e.g., the neural network in FIG. <b>1</b>A. Referring to <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>, when n number of inputs A<b>1</b>-An are provided to a single node ND, the node ND may multiply the n number of inputs A<b>1</b>-An with an n number of weights W<b>1</b>-Wn corresponding thereto, may sum the products, and may add an offset (b) to the summed value. Also, the node ND may generate a single output value z by applying the value reflected with the offset to a specific function <b>6</b>.</p><p id="p-0059" num="0058">When one of the layers included in the neural network according to the example embodiment illustrated in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref> includes an m number of nodes ND illustrated in <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>, the output values of the layer may be obtained as in Equation 1 as below:</p><p id="p-0060" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>Z=W*A</i>&#x2003;&#x2003;[Equation 1]<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0061" num="0059">In Equation 1 above, W represents a weight corresponding to all branches included in the layer, and may be represented in the form of an (m&#xd7;n) matrix. A represents an n number of inputs A<b>1</b>-An received by the layer, and may be implemented in the form of an (n&#xd7;1) matrix. Z represents an m number of outputs Z<b>1</b>-Zm output by the layer, and may be represented in the form of an (m&#xd7;1) matrix.</p><p id="p-0062" num="0060"><figref idref="DRAWINGS">FIGS. <b>2</b>A and <b>2</b>B</figref> are diagrams illustrating a cell array included in a neuromorphic device according to an example embodiment.</p><p id="p-0063" num="0061">Referring first to <figref idref="DRAWINGS">FIG. <b>2</b>A</figref>, a cell array <b>10</b> may include a plurality of row lines, a plurality of column lines, and a plurality of memory cells MC. Each of the plurality of memory cells MC may include a switch device SW and a memory device ME. For example, the plurality of row lines may include a plurality of word lines WL<b>1</b>-WLm, and the plurality of column lines may include a plurality of bit lines BL<b>1</b>-BLn and a plurality of source lines SL<b>1</b>-SLn.</p><p id="p-0064" num="0062">The memory device ME included in each of the plurality of memory cells MC may be a variable resistor device, and a resistance value of the memory device ME may be determined by a voltage/current applied via a plurality of word lines WL<b>1</b>-WLm and a plurality of bit lines BL<b>1</b>-BLn, and a plurality of source lines SL<b>1</b> to SLn. For example, a turn-on voltage for turning the switch device SW on may be input to a selected word line, and a predetermined (and/or otherwise determined) bias voltage may be input to a selected bit line and a selected source line, such that a resistance value of the memory device ME included in the selected memory cell may increase or decrease. Data may be stored in the selected memory cell according to the resistance value of the memory device ME, and the relationship between the data stored in the selected memory cell and the resistance value of the memory device ME may be determined in various manners. The stored data may be digital (e.g., &#x201c;0&#x201d; and/or &#x201c;1&#x201d;) and/or analog data.</p><p id="p-0065" num="0063">For example, in some example embodiments, when a neural network including the cell array <b>10</b> is determined, weights corresponding to a plurality of layers included in the neural network may be converted into data and may be stored in the memory cells MC. When inference using the neural network starts, a voltage and/or a current corresponding to input data may be input via the plurality of source lines SL<b>1</b>-SLn while the plurality of word lines WL<b>1</b>-WLm are activated in sequence, and a voltage and/or a current may be detected via the plurality of bit lines BL<b>1</b>-BLn. Accordingly, the computation of multiplying one of the n number of inputs A<b>1</b>-An by one of an n number of weights W<b>1</b>-Wn described above with reference to <figref idref="DRAWINGS">FIG. <b>1</b>B</figref> may be executed. When entirety of the plurality of layers included in the neural network are implemented in the form of the cell array <b>10</b> as described above, a neuromorphic device storing data and performing computational operations may be implemented.</p><p id="p-0066" num="0064">Each of the plurality of memory cells MC may be implemented as a resistive memory cell such as a floating gate cell, a phase change random access memory (PRAM) cell, a Resistance Random Access Memory (RRAM) cell, a magnetic random access memory (MRAM) cell, a ferroelectric random access memory (FRAM) cell, and/or the like. In some example embodiments, the memory device ME may include a phase change material of which crystal state changes according to an amount of current. The phase change material may include various types of materials such as compound materials like GaSb, InSb, InSe, Sb<sub>2</sub>Te<sub>3</sub>, and GeTe which may be a combination of two elements, GeSbTe, GaSeTe, InSbTe, SnSb<sub>2</sub>Te<sub>4</sub>, and InSbGe which may be a combination of three elements, and/or AgInSbTe, (GeSn)SbTe, GeSb(SeTe), and Te<sub>81</sub>Ge<sub>15</sub>Sb<sub>2</sub>S<sub>2 </sub>which may be a combination of four elements, and/or the like. In some example embodiments, the memory device ME may include perovskite compounds, transition metal oxides, magnetic materials, ferromagnetic materials, antiferromagnetic materials, and/or the like. However, the material and/or materials included in the memory device ME are not limited to the above-described materials.</p><p id="p-0067" num="0065">Referring to <figref idref="DRAWINGS">FIG. <b>2</b>B</figref>, a cell array <b>10</b>A may include a plurality of memory cells MC, and the plurality of memory cells MC may be connected to a plurality of row lines and a plurality of column lines. The plurality of row lines may include a plurality of word lines WL<b>1</b>-WLm, and the plurality of column lines may include a plurality of bit lines BL<b>1</b>-BLn. Each of the plurality of memory cells MC may include a memory device ME, and for example, the memory device ME may be implemented as a resistive device. Resistance and conductivity of the memory device ME may correspond to a weight included in the neural network.</p><p id="p-0068" num="0066">In the example embodiment described with reference to <figref idref="DRAWINGS">FIGS. <b>2</b>A and <b>2</b>B</figref>, the cell arrays <b>10</b> and <b>10</b>A may have a two-dimensional array structure, but the example embodiments thereof are not limited thereto. For example, the cell array may be formed in a three-dimensional vertical array structure. The structure of the plurality of memory cells MC may also be varied in some example embodiments.</p><p id="p-0069" num="0067"><figref idref="DRAWINGS">FIGS. <b>3</b>, <b>4</b>A, and <b>4</b>B</figref> are diagrams illustrating a neuromorphic device according to a comparative example and some example embodiments.</p><p id="p-0070" num="0068">Referring <figref idref="DRAWINGS">FIG. <b>3</b></figref>, a neuromorphic device <b>20</b> may include a plurality of tiles <b>30</b>. Each of the plurality of tiles <b>30</b> may include a plurality of memory cells. A plurality of memory cells in each of the plurality of tiles <b>30</b> may store weights included in each of a plurality of layers of a neural network. For example, the weights may be quantized and stored in a plurality of memory cells.</p><p id="p-0071" num="0069">In some example embodiments, weights included in a single layer among a plurality of layers included in a neural network may be stored in two or more tiles <b>30</b> in a distributed manner Two or more tiles <b>30</b> dividing and storing weights included in a single layer may be disposed adjacent to each other in the neuromorphic device <b>20</b>.</p><p id="p-0072" num="0070">Each of the plurality of tiles <b>30</b> may include a cell array in which a plurality of memory cells is disposed, a row driver connected to the cell array via row lines, and an analog-to-digital converter (ADC) connected to the cell array via column lines. Similarly to the example described above with reference to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the row driver may be connected to a plurality of memory cells via word lines, and the ADC circuit may be connected to a plurality of memory cells via a plurality of bit lines and a plurality of source lines. The ADC circuit may include at least one ADC. For example, when the ADC circuit includes a plurality of ADCs, the number of the plurality of ADCs may be equal to the number of the plurality of bit lines and the number of the plurality of source lines.</p><p id="p-0073" num="0071"><figref idref="DRAWINGS">FIG. <b>4</b>A</figref> is a diagram illustrating the structure of a plurality of tiles <b>30</b> in a comparative example (different from an example embodiment). Referring to <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, in the comparative example, each of the plurality of tiles <b>30</b> may include a cell array <b>31</b>, a reference cell array <b>32</b>, a row driver <b>33</b>, and an ADC circuit <b>34</b>. A plurality of memory cells for storing weights of the neural network may be disposed in the cell array <b>31</b>, and a plurality of reference cells may be disposed in the reference cell array <b>32</b>.</p><p id="p-0074" num="0072">Each of the plurality of memory cells may include a memory device having variable resistance properties as described above, and when learning of the neural network is completed, the weight of the neural network may be stored in the plurality of memory cells. However, resistance of the memory device programmed to store the weights may change depending, for example, on the time elapsed from the time at which the weights are stored and/or the temperature during the execution of the inference operation using the neuromorphic device <b>30</b>.</p><p id="p-0075" num="0073">To address the issue above, a plurality of reference cells disposed in the reference cell array <b>32</b> may be formed to have the same structure as that of the plurality of memory cells, and when an inference operation is executed, the plurality of reference cells output by a plurality of reference currents may be input to the ADC circuit <b>34</b>. Similarly to the resistance of the plurality of memory cells disposed in the cell array <b>31</b>, the resistance of the plurality of reference cells may also change, and accordingly, reference currents may also change according to time elapsed and a temperature. Accordingly, the ADC circuit <b>34</b> may output accurate result data DATA for the MAC computation of the inference operation.</p><p id="p-0076" num="0074">However, in the comparative example illustrated in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, since each of the plurality of tiles <b>30</b> includes the reference cell array <b>32</b>, the integration density of the neuromorphic device <b>20</b> may be reduced. For example, when weights of a single layer among a plurality of layers included in the neural network are divided and stored in two or more of the plurality of tiles <b>30</b>, the number of reference cell arrays <b>32</b> may increase, which may further reduce integration density of the neuromorphic device <b>20</b>.</p><p id="p-0077" num="0075">In some example embodiments, as illustrated in <figref idref="DRAWINGS">FIG. <b>4</b>B</figref>, a plurality of cell tiles <b>30</b>A in which a plurality of memory cells storing weights are disposed, and at least one reference tile <b>30</b>B in which a plurality of reference cells for generating reference currents are disposed may be separately implemented. For example, in the example illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, a portion of the plurality of tiles <b>30</b> may be allocated as the plurality of cell tiles <b>30</b>A, and at least one tile may be allocated as the reference tile <b>30</b>B. For example, reference currents output by one reference tile <b>30</b>B may be shared by two or more cell tiles <b>30</b>A. Accordingly, the area occupied by the tiles <b>30</b> included in the neuromorphic device <b>20</b>A may be reduced, and integration density of the neuromorphic device <b>20</b>A may improve.</p><p id="p-0078" num="0076">In some example embodiments, each of the plurality of cell tiles <b>30</b>A may include a cell array, a row driver, and a cell ADC circuit. At least one reference tile <b>30</b>B may include a reference cell array, a reference row driver, and a reference ADC circuit. Accordingly, the plurality of cell tiles <b>30</b>A and the at least one reference tile <b>30</b>B may operate independently of each other.</p><p id="p-0079" num="0077"><figref idref="DRAWINGS">FIGS. <b>5</b> and <b>6</b></figref> are diagrams illustrating operations of a neuromorphic device according to some example embodiments.</p><p id="p-0080" num="0078">The diagrams illustrate example operations of a cell ADC circuit connected to a cell array when the neuromorphic device executes an inference operation in different operating environments. In the first operating environment illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the neuromorphic device may obtain a read current I<sub>RD </sub>from one of a plurality of cell tiles to execute a MAC computation for an inference operation. For example, the read current I<sub>RD </sub>may be obtained from one of a plurality of column lines by selecting one of a plurality of row lines connected to a plurality of memory cells.</p><p id="p-0081" num="0079">The read current I<sub>RD </sub>may be converted into digital data in the ADC. The ADC may compare the read current I<sub>RD </sub>with the plurality of reference currents I<sub>R1</sub>-I<sub>R31</sub>. The number of reference currents compared to the read current I<sub>RD </sub>in the ADC may be varied depending on precision of the ADC. In the example in <figref idref="DRAWINGS">FIGS. <b>5</b> and <b>6</b></figref>, the ADC may output digital data by comparing the read current I<sub>RD </sub>with 31 reference currents I<sub>R1</sub>-I<sub>R31</sub>, and accordingly, the ADC may have 5-bit precision. For example, in the example embodiment illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the ADC may convert the read current I<sub>RD </sub>into digital data of &#x201c;00011.&#x201d;</p><p id="p-0082" num="0080">However, as described above, conductivity of each of the plurality of memory cells may change due to the elapsed time after the program operation for storing the weight of the neural network in the plurality of memory cells, and/or the temperature during the execution of the inference operation. Accordingly, even when the same input data is input to the memory cell due to the elapsed time and/or temperature, the magnitude of the read current I<sub>RD </sub>may be varied. For example, as the elapsed time increases, the resistance of the memory device included in each of the plurality of memory cells may decrease, and accordingly, the magnitude of the read current I<sub>RD </sub>may increase. Accordingly, when the read current I<sub>RD </sub>is digitally converted using constant reference currents I<sub>R1</sub>-I<sub>R31 </sub>without compensation for the elapsed time and temperature, as illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the read current I<sub>RD </sub>may be erroneously converted to digital data of &#x201c;00100.&#x201d;</p><p id="p-0083" num="0081">In an example embodiment, the magnitude of the reference currents I<sub>R1</sub>-I<sub>R31 </sub>input to the ADC may change according to the elapsed time and/or temperature similarly to conductivity of the memory cells, and accordingly, the above issue may be addressed. Referring to <figref idref="DRAWINGS">FIG. <b>6</b></figref>, as the read current I<sub>RD </sub>increases due to the elapsed time, the magnitude of each of the reference currents I<sub>R1</sub>-I<sub>R31 </sub>may also increase. Accordingly, the ADC may accurately convert the read current I<sub>RD </sub>obtained from the memory cell into digital data of &#x201c;00011&#x201d; as in <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0084" num="0082">Also, in some example embodiments, as described above, the reference cell array outputting the reference currents I<sub>R1</sub>-I<sub>R31 </sub>may be disposed in a reference tile implemented independently of the cell tiles including a plurality of memory cells. Accordingly, two or more cell tiles may share the reference currents I<sub>R1</sub>-I<sub>R31 </sub>output by a single reference cell array, and integration density of the neuromorphic device may improve. Also, since the plurality of cell tiles use the reference currents I<sub>R1</sub>-I<sub>R31 </sub>output by a single reference tile, by reducing the number of computations for generating the reference currents I<sub>R1</sub>-I<sub>R31</sub>, power consumption of the neuromorphic device may be reduced.</p><p id="p-0085" num="0083"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a diagram illustrating a structure of a reference tile included in a neuromorphic device according to some example embodiments.</p><p id="p-0086" num="0084">Referring to <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the neuromorphic device according to some example embodiments may include a reference tile <b>50</b> for outputting reference currents. The reference tile <b>50</b> may include a first reference cell array <b>51</b>, a second reference cell array <b>52</b>, a first reference row driver <b>53</b>, a second reference row driver <b>54</b>, and an ADC circuit <b>55</b>. A plurality of reference cells RC may be disposed in each of the first reference cell array <b>51</b> and the second reference cell array <b>52</b>, and each of the plurality of reference cells RC may include a switch device SW and a memory device ME. The plurality of reference cells RC may have the same structure as that of a memory cell included in each of the cell tiles implemented as a separate tile different from the reference tile <b>50</b> and storing a weight.</p><p id="p-0087" num="0085">Each of the first reference row driver <b>53</b> and the second reference row driver <b>54</b> may be connected to a plurality of reference cells RC via a plurality of word lines. For example, the first reference row driver <b>53</b> may be connected to the plurality of reference cells RC via the plurality of off-row lines OFF_WL<b>1</b>-OFF_WLm, and the second reference row driver <b>54</b> may be connected to the plurality of reference cells RC via the plurality of off-row lines OFF_WL<b>1</b>-OFF_WLm. The ADC circuit <b>55</b> may be connected to the plurality of reference cells RC via the plurality of reference column lines LB<b>1</b>-LBn, HB<b>1</b>-HBn, LS<b>1</b>-LSn, and HS<b>1</b>-HSn.</p><p id="p-0088" num="0086">While the reference currents for the inference operation are generated, the first reference row driver <b>53</b> may maintain the switch device SW of each of the reference cells RC connected to the plurality of off-row lines OFF_WL<b>1</b>-OFF_WLm in a turned-off state. The second reference row driver <b>54</b> may maintain the switch device SW of each of the reference cells RC connected to the plurality of on-row lines ON_WL<b>1</b>-ON_WLm in a turned-on state.</p><p id="p-0089" num="0087">Each of the plurality of reference cells RC may be programmed into one of a first state in which the memory device ME has a low resistance or a second state in which the memory device ME has a high resistance. In the example in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the memory device ME marked a thick line may have high resistance, and the memory device ME marked a thin line may have low resistance. For example, among the plurality of reference cells RC connected to the first row bit line LB<b>1</b> and the first row source line LS<b>1</b>, only the memory device ME of the reference cell RC connected to the first off-row line OFF_WL<b>1</b> may have low resistance. However, the example embodiments are not limited thereto.</p><p id="p-0090" num="0088">The ADC circuit <b>55</b> may obtain currents via the bit lines LB<b>1</b>-LBn and HB<b>1</b>-HBn among the plurality of reference column lines LB<b>1</b>-LBn, HB<b>1</b>-HBn, LS<b>1</b>-LSn, and HS<b>1</b>-HSn, and may output a plurality of reference currents using an average value of currents obtained from a pair of bit lines among the bit lines LB<b>1</b>-LBn and HB<b>1</b>-HBn. For example, the ADC circuit <b>55</b> may output an average value of currents obtained from the first low bit line LB<b>1</b> and the first high bit line HB<b>1</b> as a first reference current. In the example embodiment illustrated in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the ADC circuit <b>55</b> may output an n number of reference currents.</p><p id="p-0091" num="0089"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a diagram illustrating a neuromorphic device according to some example embodiments.</p><p id="p-0092" num="0090">Referring to <figref idref="DRAWINGS">FIG. <b>8</b></figref>, a neuromorphic device <b>100</b> according to some example embodiments may include a plurality of cell tiles <b>110</b>, a reference tile <b>120</b>, and a comparator circuit <b>130</b>. The neuromorphic apparatus <b>100</b> may include only a single reference tile <b>120</b>, and/or the number of reference tiles <b>120</b> may be two or more. The number of the reference tiles <b>120</b>, however, may be less than the number of the plurality of cell tiles <b>110</b>.</p><p id="p-0093" num="0091">Each of the plurality of cell tiles <b>110</b> may include a cell array <b>111</b>, a row driver <b>112</b>, and a cell ADC circuit <b>113</b>. The cell array <b>111</b> may include a plurality of memory cells, may be connected to the row driver <b>112</b> via a plurality of row lines, and may be connected to the cell ADC circuit <b>113</b> via a plurality of column lines. The cell ADC circuit <b>113</b> may include at least one cell ADC converting cell currents read from a plurality of memory cells via a plurality of column lines into a plurality of pieces of digital cell data CD.</p><p id="p-0094" num="0092">The reference tile <b>120</b> may include a reference cell array <b>121</b>, a first reference row driver <b>122</b>A, a second reference row driver <b>122</b>B, and a reference ADC circuit <b>123</b>. The reference cell array <b>121</b> may include a plurality of reference cells, and the reference cells may have the same structure as that of memory cells. The plurality of reference cells may be connected to the first reference row driver <b>122</b>A and/or the second reference row driver <b>122</b>B via a plurality of row lines, and may be connected to the reference ADC circuit <b>123</b> via a plurality of reference column lines. The reference ADC circuit <b>123</b> may include at least one reference ADC converting reference currents read from a plurality of reference cells via a plurality of reference column lines into a plurality of pieces of digital reference data RD.</p><p id="p-0095" num="0093">The comparator circuit <b>130</b> may compare each of pieces of the digital cell data CD with the digital reference data RD and may output result data DATA required for an inference operation. For example, the result data DATA may include a result of MAC computation using an input value and a weight.</p><p id="p-0096" num="0094">Each of the cell ADC circuit <b>113</b> and the reference ADC circuit <b>123</b> may receive a plurality of default currents I<sub>DEF</sub>. The cell ADC circuit <b>113</b> may compare each of the cell currents with a plurality of default currents I<sub>DEF </sub>and may generate digital cell data CD, and the reference ADC circuit <b>123</b> may compare each of the reference currents with the plurality of default currents I<sub>DEF </sub>and may generate a plurality of pieces of digital reference data RD.</p><p id="p-0097" num="0095">Each of the plurality of default currents I<sub>DEF </sub>may have a fixed magnitude not affected by an operating environment such as time elapsed after programming and a temperature of the neuromorphic apparatus <b>100</b>. However, the magnitude of each of the cell currents which the cell ADC circuit <b>113</b> obtains from the cell array <b>111</b> and the reference currents which the reference ADC circuit <b>123</b> obtains from the reference cell array <b>121</b> may be varied depending on the operating environment such as time and temperature. For example, without being limited to a specific theory and/or cause, the magnitude of each of the cell currents may change because the conductivity of each of the plurality of reference cells disposed in the reference cell array <b>121</b> may change according to temperature (e.g., during an inference operation) and/or based on the time elapsed after programming.</p><p id="p-0098" num="0096">Accordingly, depending on the elapsed time after programming and/or temperature, the magnitudes of the cell currents and the reference currents may become the same as the magnitudes of the plurality of default currents I<sub>DEF</sub>. In these cases, accuracy of the cell ADC circuit <b>113</b> and the reference ADC circuit <b>123</b> may decrease. To address the above issue, in some example embodiments, the number of the plurality of default currents I<sub>DEF </sub>may be determined to be greater than the number of the cell ADC circuit <b>113</b> and the reference ADC circuit <b>123</b> determined in consideration of precision. For example, when each of the data CD and RD output by the cell ADC circuit <b>113</b> and the reference ADC circuit <b>123</b> is N-bit data, the number of the plurality of default currents I<sub>DEF </sub>may be greater than 2<sup>N</sup>.</p><p id="p-0099" num="0097"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a flowchart illustrating operations of a neuromorphic device according to some example embodiments.</p><p id="p-0100" num="0098">Referring to <figref idref="DRAWINGS">FIG. <b>9</b></figref>, the neuromorphic device according to some example embodiments may start an inference operation based on a neural network (S<b>10</b>). The neuromorphic device may include a plurality of tiles, and as described above, the plurality of tiles may include a plurality of cell tiles and at least one reference tile. For example, the plurality of cell tiles and the reference tile may have a structure similarly to the structures described in the example embodiments described above with reference to <figref idref="DRAWINGS">FIG. <b>8</b></figref>.</p><p id="p-0101" num="0099">Once the inference operation starts, the neuromorphic device may input a plurality of default currents to the cell ADCs and the reference ADCs included in each of the plurality of cell tiles (S<b>11</b>). The plurality of default currents may have a fixed value regardless of time elapsed after programming and/or the temperature of the memory cells and the reference cells. However, in some example embodiments, the number of the cell ADCs included in the plurality of cell tiles, the number of the plurality of default currents input to the ADCs included in the reference tile, and/or the magnitude of each of the plurality of default currents may be varied depending on at least one of time information and/or temperature information.</p><p id="p-0102" num="0100">As an example, each of the plurality of cell tiles may include a plurality of memory cells and weights allocated to branch paths of a plurality of nodes in a neural network trained (e.g., in advance) may be stored in the plurality of memory cells. Cell ADCs connected to a plurality of memory cells may, by comparing cell currents read from the plurality of memory cells with a plurality of default currents, output digital cell data corresponding to a result of computation using an input value and a weight in the neural network.</p><p id="p-0103" num="0101">At least one reference tile may include a plurality of reference cells, and the plurality of reference cells may have the same structure as that of a plurality of memory cells. Reference ADCs connected to the plurality of reference cells may compare reference currents read from the plurality of reference cells with the plurality of default currents and may output digital reference data.</p><p id="p-0104" num="0102">The neuromorphic device may include a comparator circuit connected to cell ADCs and reference ADCs, and the comparator circuit may compare digital cell data with digital reference data, respectively (S<b>12</b>). For example, each of the digital cell data may be compared with digital reference data. The comparator circuit may compare cell currents converted into digital data with reference currents. Accordingly, the output of the comparator circuit may correspond to the result of computation in the neural network, and the result of computation may be obtained (S<b>13</b>).</p><p id="p-0105" num="0103">For example, weights of a neural network of which training has been completed may be quantized and may be stored in a plurality of memory cells. Also, input values input to a plurality of nodes in each of a plurality of layers included in the neural network may also be quantized and may be input via a plurality of row lines. For example, the input values may be quantized as &#x201c;q<b>1</b>&#x201d; and &#x201c;Z<b>1</b>&#x201d;, respectively, and Z<b>1</b> may be a zero point input value which may be a reference for quantizing the input values. Similarly, the weights may be quantized as &#x201c;q<b>2</b>&#x201d; and &#x201c;Z<b>2</b>&#x201d;, respectively, and Z<b>2</b> may be a zero point weight which may be a reference for quantizing weights.</p><p id="p-0106" num="0104">When the quantized weight q<b>2</b> and the zero point weight Z<b>2</b> are stored in a plurality of cell tiles, the digital cell data output by the cell ADCs may be data obtained by converting the result of computations such as q<b>1</b>*q<b>2</b>, q<b>1</b>*Z<b>2</b>, q<b>2</b>*Z<b>1</b>, Z<b>1</b>*Z<b>2</b> output in the form of currents. The comparator circuit may perform digital processing by comparing digital cell data with digital reference data, and an output of the comparator circuit may be transferred to an adder circuit. The adder circuit may include an adder tree, and the data of &#x201c;q<b>1</b>*q<b>2</b>&#x2212;q<b>1</b>*Z<b>2</b>&#x2212;q<b>2</b>*Z<b>1</b>+Z<b>1</b>*Z<b>2</b>&#x201d; corresponding to the result of MAC computation of each node included in the neural network may be output.</p><p id="p-0107" num="0105"><figref idref="DRAWINGS">FIGS. <b>10</b>A to <b>10</b>C</figref> are diagrams illustrating operations of a neuromorphic device according to some example embodiments.</p><p id="p-0108" num="0106"><figref idref="DRAWINGS">FIGS. <b>10</b>A to <b>10</b>C</figref> may be diagrams illustrating a method of determining a plurality of default currents input to cell ADCs and reference ADCs in a neuromorphic device according to some example embodiments. For example, in each of <figref idref="DRAWINGS">FIGS. <b>10</b>A to <b>10</b>C</figref>, a dotted line may indicate a plurality of default currents I<sub>DEF</sub>, and a solid line may correspond to currents I<sub>RD </sub>read from a plurality of memory cells or a plurality of reference cells.</p><p id="p-0109" num="0107">Each of the examples illustrated in <figref idref="DRAWINGS">FIGS. <b>10</b>A to <b>10</b>C</figref> may represent operations in neuromorphic devices having different time information and temperature information. As an example, <figref idref="DRAWINGS">FIG. <b>10</b>A</figref> may correspond to an example in which a first elapsed time has elapsed from a programming time point in which a plurality of memory cells and a plurality of reference cells are programmed, and an inference operation is executed at a first temperature. Similarly, <figref idref="DRAWINGS">FIG. <b>10</b>B</figref> may correspond to an example in which a second elapsed time has elapsed from a program time point and an inference operation is executed at a second temperature.</p><p id="p-0110" num="0108">Referring to <figref idref="DRAWINGS">FIGS. <b>10</b>A to <b>10</b>C</figref>, the number of the plurality of default currents may be the same, and the plurality of default currents I<sub>DEF </sub>may have different magnitudes. However, according to some example embodiments, the number of the plurality of default currents I<sub>DEF </sub>may also vary as time elapsed and temperature change. Referring to <figref idref="DRAWINGS">FIGS. <b>10</b>A to <b>10</b>C</figref>, the number of the plurality of default currents I<sub>DEF </sub>and a magnitude of each of the plurality of default currents I<sub>DEF </sub>may be determined such that the currents I<sub>RD </sub>read from the plurality of memory cells and the plurality of reference cells and the plurality of default currents I<sub>DEF </sub>may not have the same value. The number of the plurality of default currents I<sub>DEF </sub>and the magnitude of each of the plurality of default currents I<sub>DEF </sub>according to the elapsed time and temperature may be predetermined (and/or otherwise determined) via simulation and may be stored in an internal or external memory of the neuromorphic device.</p><p id="p-0111" num="0109"><figref idref="DRAWINGS">FIGS. <b>11</b> to <b>14</b></figref> are diagrams illustrating neuromorphic devices according to some example embodiments.</p><p id="p-0112" num="0110">Referring first to <figref idref="DRAWINGS">FIG. <b>11</b></figref>, a neuromorphic device <b>300</b> according to some example embodiment may include a plurality of cell tiles <b>310</b> and <b>320</b> and at least one reference tile <b>330</b>. For example, weights corresponding to a plurality of layers, respectively, included in the neural network, may be stored in two or more cell tiles <b>310</b> and <b>320</b> in a distributed manner.</p><p id="p-0113" num="0111">For example, the weights included in the first layer may be stored in the plurality of cell tiles <b>310</b> in a distributed manner, and the weights included in the N-th layer may also be stored in the plurality of cell tiles <b>320</b> in a distributed manner. The number of cell tiles <b>310</b> and <b>320</b> allocated to store weights of each of the plurality of layers may be varied according to, e.g., the number of nodes included in each of the plurality of layers. For example, the number of the plurality of cell tiles <b>310</b> storing the weights of the first layer may be different from the number of the plurality of cell tiles <b>320</b> storing the weights of the N-th layer.</p><p id="p-0114" num="0112">The plurality of cell tiles <b>310</b> and <b>320</b> may have similar structures. For example, the cell tile <b>310</b> storing the weights of the first layer may include a cell array <b>311</b> in which a plurality of memory cells is disposed, a row driver <b>312</b>, and a cell ADC circuit <b>313</b>, and may be connected to a comparator circuit <b>314</b>. The cell tile <b>320</b> storing the weights of the N-th layer also may include a cell array <b>321</b>, a row driver <b>322</b>, and a cell ADC circuit <b>323</b>, and may be connected to the comparator circuit <b>324</b>. In some example embodiments, each of the cell ADC circuits <b>313</b> and <b>323</b> may include a plurality of cell ADCs, and each of the comparator circuits <b>314</b> and <b>324</b> may include a plurality of comparators. For example, the number of the plurality of comparators included in each of the comparator circuits <b>314</b> and <b>324</b> may be the same as the number of the plurality of cell ADCs included in each of the cell ADC circuits <b>313</b> and <b>323</b>. Accordingly, an output terminal of each of the plurality of cell ADCs may be connected to an input terminal of each of the plurality of comparators.</p><p id="p-0115" num="0113">The at least one reference tile <b>330</b> may include a reference cell array <b>331</b> in which a plurality of reference cells are disposed, a first reference row driver <b>332</b>A, a second reference row driver <b>322</b>B, a reference ADC circuit <b>333</b>, and a buffer <b>334</b>. The reference ADC circuit <b>333</b> may include a plurality of reference ADCs. While reference currents are obtained from the plurality of reference cells, the first reference row driver <b>332</b>A may maintain a portion off-row lines in a turned-off state, and the second reference row driver <b>322</b>B may maintain the other on-row lines in a turned-on state.</p><p id="p-0116" num="0114">Once an inference operation starts, while the first reference row driver <b>332</b>A maintains off-row lines in a turned-off state and the second reference row driver <b>322</b>B maintains on-row lines in a turned-on state in the reference tile <b>330</b>, the reference ADC circuit <b>333</b> may read the reference currents and may output digital reference data. The digital reference data may be stored in the buffer <b>334</b>. Accordingly, digital reference data required for the inference operation may be generated and may be stored in the buffer <b>334</b> only by a single read operation of reading the reference currents, and power consumed by the neuromorphic device <b>300</b> in the inference operation may be reduced.</p><p id="p-0117" num="0115">The inference operation may be performed according to the order of a plurality of layers included in the neural network. In some example embodiments, an inference operation may be preferentially performed on the plurality of cell tiles <b>310</b> corresponding to a first layer. When the row driver <b>312</b> inputs input values to the cell array <b>311</b> via row lines, the cell ADC circuit <b>313</b> may obtain cell currents from the cell array <b>311</b> via column lines. The cell ADC circuit <b>313</b> may generate digital cell data by comparing cell currents with a plurality of default currents and may output the digital cell data to the comparator circuit <b>314</b>.</p><p id="p-0118" num="0116">Referring to <figref idref="DRAWINGS">FIG. <b>11</b></figref>, the comparator circuit <b>314</b> may receive a plurality of pieces of digital cell data output by the cell ADC circuit <b>313</b> and a plurality of pieces of digital reference data output by the buffer <b>334</b>. Each of the plurality of comparators included in the comparator circuit <b>314</b> may receive one of digital cell data and digital reference data. Accordingly, each of the plurality of comparators may compare a single piece of received digital cell data among the plurality of pieces of digital cell data with digital reference data and may output result data corresponding to the result of MAC computation. The operation of the plurality of cell tiles <b>320</b> corresponding to the N-th layer may also be understood with reference to the aforementioned example embodiments.</p><p id="p-0119" num="0117">In some example embodiment, each of the cell ADC circuits <b>313</b> and <b>323</b> in each of the plurality of cell tiles <b>310</b> and <b>320</b> may include a plurality of cell ADCs, and each of the comparator circuits <b>314</b> and <b>324</b> may include a plurality of comparators. Accordingly, the operation of comparing a single piece of digital cell data with the plurality of pieces of digital reference data may be simultaneously executed in each of the plurality of comparators, such that a high computation speed may be realized.</p><p id="p-0120" num="0118">Referring to <figref idref="DRAWINGS">FIG. <b>12</b></figref>, a neuromorphic device <b>400</b> may include a plurality of cell tiles <b>410</b> and <b>420</b> and at least one reference tile <b>430</b>. Weights corresponding to each of a plurality of layers included in a neural network may be stored in two or more cell tiles <b>410</b> and <b>420</b> in a distributed manner, and the number of the plurality of cell tiles <b>410</b> and <b>420</b> included in each layer may be varied depending on the structure of the neural network and the number of memory cells included in each of the plurality of cell tiles <b>410</b> and <b>420</b>.</p><p id="p-0121" num="0119">The structures of the plurality of cell tiles <b>410</b> and <b>420</b> and the at least one reference tile <b>430</b> may be similar to those of the aforementioned example embodiments, e.g., as described with reference to <figref idref="DRAWINGS">FIG. <b>11</b></figref>. However, in the example illustrated in <figref idref="DRAWINGS">FIG. <b>12</b></figref>, each of the plurality of cell tiles <b>410</b> and <b>420</b> may include only one comparator <b>414</b> and <b>424</b>.</p><p id="p-0122" num="0120">Once an inference operation starts, the reference ADC circuit <b>433</b> may generate a plurality of pieces of digital reference data by comparing the reference currents obtained from the reference cell array <b>431</b> with a plurality of default currents. The plurality of pieces of digital reference data may be stored in the buffer <b>434</b> and may be transferred to comparators <b>414</b> and <b>424</b> included in each of the plurality of cell tiles <b>410</b> and <b>420</b>.</p><p id="p-0123" num="0121">As described above, each of the plurality of cell tiles <b>410</b> and <b>420</b> may include only one comparator <b>414</b> and <b>424</b>, respectively. For example, the comparator <b>414</b> of each of the cell tiles <b>410</b> corresponding to the first layer may receive the plurality of pieces of digital cell data output by the cell ADC circuit <b>413</b> in sequence and may compare the data with a plurality of pieces of digital reference data. Accordingly, in the example embodiments illustrated in <figref idref="DRAWINGS">FIG. <b>12</b></figref>, the computation speed may be slower than in the example embodiments illustrated in <figref idref="DRAWINGS">FIG. <b>11</b></figref>. However, the number of wirings connecting the reference tile <b>430</b> to the plurality of cell tiles <b>410</b> and <b>420</b> may be reduced, difficulty of wiring design may be reduced, and integration density of the neuromorphic device <b>400</b> may improve.</p><p id="p-0124" num="0122">Referring to <figref idref="DRAWINGS">FIG. <b>13</b></figref>, a neuromorphic device <b>500</b> may include a plurality of cell tiles <b>510</b> and <b>520</b> and at least one reference tile <b>530</b>. Weights corresponding to each of a plurality of layers included in a neural network may be stored in two or more cell tiles <b>510</b> and <b>520</b> in a distributed manner, and the number of the plurality of cell tiles <b>510</b> and <b>520</b> included in each layer may be varied depending on the structure of the neural network and the number of memory cells included in each of the plurality of cell tiles <b>510</b> and <b>520</b>.</p><p id="p-0125" num="0123">The structures of the plurality of cell tiles <b>510</b> and <b>520</b> and the at least one reference tile <b>530</b> may be similar to those of the aforementioned example embodiments described with reference to <figref idref="DRAWINGS">FIG. <b>11</b></figref>. However, in the example illustrated in <figref idref="DRAWINGS">FIG. <b>13</b></figref>, two or more of the plurality of cell tiles <b>510</b> and <b>520</b> may share a single comparator circuit <b>514</b> and <b>524</b>. For example, two or more of the plurality of cell tiles <b>510</b> corresponding to a first layer may share a single comparator circuit <b>514</b>. Similarly, two or more of the plurality of cell tiles <b>520</b> corresponding to the N-th layer may share a single comparator circuit <b>524</b>. Each of the comparator circuits <b>514</b> and <b>524</b> may include a plurality of comparators. For example, the number of the plurality of comparators included in each of the comparator circuits <b>514</b> and <b>524</b> may be the same as the number of the plurality of cell ADCs included in each of the cell ADC circuits <b>513</b> and <b>523</b>.</p><p id="p-0126" num="0124">Once an inference operation starts, the reference ADC circuit <b>533</b> may generate digital reference data by comparing the reference currents obtained from the reference cell array <b>531</b> with a plurality of default currents. The digital reference data may be stored in the buffer <b>534</b> and may be transferred to the comparator circuits <b>514</b> and <b>524</b> included in each of the plurality of cell tiles <b>510</b> and <b>520</b>.</p><p id="p-0127" num="0125">Each of the comparator circuits <b>514</b> may be connected to two or more cell tiles <b>510</b>. Each of the comparator circuits <b>514</b> may receive a plurality of pieces of digital cell data output by each of the two or more cell tiles <b>510</b> in sequence, and may compare the data with a plurality of pieces of digital reference data received from the buffer <b>534</b>.</p><p id="p-0128" num="0126">Accordingly, a computation speed of the neuromorphic device <b>500</b> according to the example illustrated in <figref idref="DRAWINGS">FIG. <b>13</b></figref> may be slower than the computation speed of the neuromorphic device <b>300</b> in the example illustrated in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, but may be faster than the computation speed of the neuromorphic apparatus <b>400</b> in the example illustrated in <figref idref="DRAWINGS">FIG. <b>12</b></figref>. Also, the number of wirings connecting the reference tile <b>530</b> to the plurality of cell tiles <b>510</b> and <b>520</b> and complexity of the wirings may also be intermediate between the example illustrated in <figref idref="DRAWINGS">FIG. <b>11</b></figref> and the example illustrated in <figref idref="DRAWINGS">FIG. <b>12</b></figref>.</p><p id="p-0129" num="0127">Referring to <figref idref="DRAWINGS">FIG. <b>14</b></figref>, a neuromorphic device <b>600</b> in an example may include a plurality of cell tiles <b>610</b> and <b>620</b> and at least one reference tile <b>630</b>. Weights corresponding to each of a plurality of layers included in the neural network may be stored in two or more cell tiles <b>610</b> and <b>620</b> in a distributed manner, and the number of the plurality of cell tiles <b>610</b> and <b>620</b> included in each layer may be varied depending on the structure of the neural network and the number of memory cells included in each of the plurality of cell tiles <b>610</b> and <b>620</b>.</p><p id="p-0130" num="0128">The structures of the plurality of cell tiles <b>610</b> and <b>620</b> and the at least one reference tile <b>630</b> may be similar to those of the aforementioned example embodiments described with reference to <figref idref="DRAWINGS">FIG. <b>11</b></figref>. However, in the example illustrated in <figref idref="DRAWINGS">FIG. <b>14</b></figref>, two or more of the plurality of cell tiles <b>610</b> and <b>620</b> may share a single comparator <b>614</b> and <b>624</b>. Referring to <figref idref="DRAWINGS">FIG. <b>14</b></figref>, two or more of the plurality of cell tiles <b>610</b> corresponding to the first layer may share the comparator <b>614</b>. Similarly, two or more of the plurality of cell tiles <b>620</b> corresponding to the N-th layer may share the comparator <b>624</b>.</p><p id="p-0131" num="0129">Once an inference operation starts, a plurality of pieces of digital reference data output by the reference ADC circuit <b>633</b> may be stored in the buffer <b>634</b>. The plurality of pieces of digital reference data stored in the buffer <b>634</b> may be transferred to the comparators <b>614</b> and <b>624</b> included in the plurality of cell tiles <b>610</b> and <b>620</b>, respectively. Each of the comparators <b>614</b> corresponding to a first layer may receive a plurality of pieces of digital cell data output by each of the two or more cell tiles <b>610</b> in sequence, and may compare the plurality of pieces of digital cell data with the plurality of pieces of digital reference data received from the buffer <b>634</b>.</p><p id="p-0132" num="0130">Since the single comparator <b>614</b> receives a plurality of digital cell data output by two or more cell tiles <b>610</b> one by one and may compare the data with the plurality of pieces of digital reference data, the computation speed of the neuromorphic device <b>600</b> in <figref idref="DRAWINGS">FIG. <b>14</b></figref> may be slower than those of the neuromorphic devices <b>300</b>-<b>500</b> in the examples described with reference to <figref idref="DRAWINGS">FIGS. <b>11</b> to <b>13</b></figref>. However, since two or more cell tiles <b>610</b> and <b>620</b> may share a single comparator <b>614</b> and <b>624</b>, the integration density of the neuromorphic device <b>600</b> may improve more than the other example embodiments described above.</p><p id="p-0133" num="0131"><figref idref="DRAWINGS">FIGS. <b>15</b> and <b>16</b></figref> are diagrams illustrating comparator circuits included in a neuromorphic device according to some example embodiments.</p><p id="p-0134" num="0132"><figref idref="DRAWINGS">FIGS. <b>15</b> and <b>16</b></figref> may be circuit diagrams illustrating implementation examples of a comparator included in a neuromorphic device. When a comparator circuit connected to a single cell tile includes a plurality of comparators, a single comparator circuit may include two or more comparators according to at least one of the example embodiments illustrated in <figref idref="DRAWINGS">FIGS. <b>15</b> and <b>16</b></figref>.</p><p id="p-0135" num="0133">Referring to <figref idref="DRAWINGS">FIG. <b>15</b></figref>, the comparator may include an AND gate, an OR gate, and an XOR gate. The comparator may compare 5-bit digital cell data (X4-X0: X) and 5-bit digital reference data (Y4-Y0: Y) bit by bit. The output COMP of the comparator may have a high logic value when the digital cell data X is greater than the digital reference data Y.</p><p id="p-0136" num="0134">In some example embodiments, the comparator may compare the digital cell data (X) and the digital reference data (Y) from upper bits. The operations of the comparator according to the example illustrated in <figref idref="DRAWINGS">FIG. <b>15</b></figref> may be as in Table 1.</p><p id="p-0137" num="0000"><tables id="TABLE-US-00001" num="00001"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="6"><colspec colname="1" colwidth="42pt" align="center"/><colspec colname="2" colwidth="35pt" align="center"/><colspec colname="3" colwidth="42pt" align="center"/><colspec colname="4" colwidth="35pt" align="center"/><colspec colname="5" colwidth="35pt" align="center"/><colspec colname="6" colwidth="28pt" align="center"/><thead><row><entry namest="1" nameend="6" rowsep="1">TABLE 1</entry></row><row><entry namest="1" nameend="6" align="center" rowsep="1"/></row><row><entry>X4Y4</entry><entry>X3Y3</entry><entry>X2Y2</entry><entry>X1Y1</entry><entry>X0Y0</entry><entry>COMP</entry></row><row><entry namest="1" nameend="6" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry>X4 &#x3e; Y4</entry><entry>DON'T</entry><entry>DON'T</entry><entry>DON'T</entry><entry>DON'T</entry><entry>1</entry></row><row><entry/><entry>CARE</entry><entry>CARE</entry><entry>CARE</entry><entry>CARE</entry></row><row><entry>X4 = Y4</entry><entry>X3 &#x3e; Y3</entry><entry>DON'T</entry><entry>DON'T</entry><entry>DON'T</entry><entry>1</entry></row><row><entry/><entry/><entry>CARE</entry><entry>CARE</entry><entry>CARE</entry></row><row><entry>X4 = Y4</entry><entry>X3 = Y3</entry><entry>X2 &#x3e; Y2</entry><entry>DON'T</entry><entry>DON'T</entry><entry>1</entry></row><row><entry/><entry/><entry/><entry>CARE</entry><entry>CARE</entry></row><row><entry>X4 = Y4</entry><entry>X3 = Y3</entry><entry>X2 = Y2</entry><entry>X1 &#x3e; Y1</entry><entry>DON'T</entry><entry>1</entry></row><row><entry/><entry/><entry/><entry/><entry>CARE</entry></row><row><entry>X4 = Y4</entry><entry>X3 = Y3</entry><entry>X2 = Y2</entry><entry>X1 = Y1</entry><entry>X0 &#x3e; Y0</entry><entry>1</entry></row><row><entry namest="1" nameend="6" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0138" num="0135">Referring to <figref idref="DRAWINGS">FIG. <b>16</b></figref>, a comparator may include a NAND gate, a NOR gate and an XOR gate. The comparator may compare 5-bit digital cell data (X4-X0: X) and 5-bit digital reference data (Y4-Y0: Y) bit by bit. The output COMP of the comparator may be a high logic value when the digital cell data (X) is greater than the digital reference data (Y), and the comparator may compare the digital cell data (X) and the digital reference data (Y) from upper bits. The operations of the comparator according to the embodiment illustrated in <figref idref="DRAWINGS">FIG. <b>16</b></figref> may also be as in Table 1 above.</p><p id="p-0139" num="0136">Several tens of NMOS transistors and several tens of PMOS transistors may be included in the comparator according to the example illustrated in <figref idref="DRAWINGS">FIG. <b>16</b></figref>. Even when assuming a structure in which each of the plurality of cell tiles is connected to a comparator circuit including a plurality of comparators, the area of the neuromorphic device may be further reduced than a general structure in which each of the plurality of cell tiles includes a plurality of reference cells, and accordingly, integration density may improve.</p><p id="p-0140" num="0137"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is a diagram illustrating a neuromorphic device according to some example embodiments.</p><p id="p-0141" num="0138">Referring to <figref idref="DRAWINGS">FIG. <b>17</b></figref>, a neuromorphic device <b>700</b> may include a plurality of cell tiles <b>710</b> and <b>720</b> and a plurality of reference tiles <b>730</b> and <b>740</b>. For example, weights corresponding to each of a plurality of layers included in a neural network may be stored in two or more cell tiles <b>710</b> and <b>720</b> in a distributed manner A structure of each of the plurality of cell tiles <b>710</b> and <b>720</b> may be understood with reference to the aforementioned example illustrated in <figref idref="DRAWINGS">FIGS. <b>11</b> to <b>14</b></figref>. For example, though the plurality of cell tiles <b>710</b> and <b>720</b> are illustrated as including the comparative circuits <b>714</b> and <b>724</b> (which may be substantially similar to the comparative circuits <b>314</b> and <b>324</b> of <figref idref="DRAWINGS">FIG. <b>11</b></figref>), the example embodiments are not limited thereto, and the plurality of cell tiles <b>710</b> and <b>720</b> may be substantially similar to the plurality of cell tiles <b>410</b>, <b>420</b>, <b>510</b>, <b>520</b>, <b>610</b>, and/or <b>620</b>, of <figref idref="DRAWINGS">FIGS. <b>12</b> to <b>14</b></figref>. Therefore, for brevity, the similarities between the example of <figref idref="DRAWINGS">FIG. <b>17</b></figref> and the aforementioned examples will be omitted. In the embodiment in <figref idref="DRAWINGS">FIG. <b>17</b></figref>, the neuromorphic apparatus <b>700</b> may include a plurality of reference tiles <b>730</b> and <b>740</b>.</p><p id="p-0142" num="0139">For example, the number of the plurality of reference tiles <b>730</b> and <b>740</b> included in the neuromorphic device <b>700</b> may be equal to the number of layers included in a neural network to be implemented as the neuromorphic device <b>700</b>. The comparator circuits <b>714</b> connected to the plurality of cell tiles <b>710</b> corresponding to a first layer may receive digital reference data from the buffer <b>734</b> of the first reference tile <b>730</b>. The comparator circuits <b>724</b> connected to the plurality of cell tiles <b>720</b> corresponding to the N-th layer may receive digital reference data from the buffer <b>744</b> of the N-th reference tile <b>740</b>.</p><p id="p-0143" num="0140">Once an inference operation starts, a plurality of default currents may be input to the cell ADC circuits <b>713</b> and <b>723</b> and the reference ADC circuits <b>733</b> and <b>734</b>. For example, a plurality of pieces of digital reference data output by the buffer <b>734</b> of the first reference tile <b>730</b> and a plurality of pieces of digital reference data output by the buffer <b>744</b> of the N-th reference tile <b>740</b> may obtained by comparing the reference currents with the same default currents and converting the currents into digital data.</p><p id="p-0144" num="0141">In the inference operation, each of the comparator circuits <b>714</b> and <b>724</b> may receive the plurality of pieces of digital cell data output by the cell ADC circuits <b>713</b> and <b>723</b> and may compare the data with the plurality of pieces of digital reference data. In some example embodiments, each of the comparator circuits <b>714</b> and <b>724</b> may include a plurality of comparators, and each of the plurality of comparators may compare a single piece of received digital cell data from among the plurality of pieces of digital cell data with the plurality of pieces of digital reference data and may output result data corresponding to the MAC computation. However, a method of implementing each of the comparator circuits <b>714</b> and <b>724</b> may be varied in consideration of integration density, power consumption, a computation speed of the neuromorphic device <b>700</b> as described above with reference to <figref idref="DRAWINGS">FIGS. <b>11</b> to <b>14</b></figref>.</p><p id="p-0145" num="0142"><figref idref="DRAWINGS">FIG. <b>18</b></figref> is a block diagram illustrating a neuromorphic device according to some example embodiments.</p><p id="p-0146" num="0143">Referring to <figref idref="DRAWINGS">FIG. <b>18</b></figref>, a neuromorphic device <b>800</b> may include a cell array <b>810</b>, a row driver <b>820</b>, a column driver <b>830</b>, a current-voltage converter circuit <b>840</b>, an analog-to-digital converter circuit <b>850</b>, an adder circuit <b>860</b>, and a shift register circuit <b>870</b>.</p><p id="p-0147" num="0144">The cell array <b>810</b> may include a plurality of memory cells MC, and the plurality of memory cells MC may be connected to a plurality of row lines RL<b>1</b>-RLn and a plurality of column lines CL<b>1</b>-CLm. Each of the plurality of memory cells MC may include a memory device ME, and for example, the memory device ME may be implemented as a resistive device.</p><p id="p-0148" num="0145">The row driver <b>820</b> may be connected to the cell array <b>810</b> via the plurality of row lines RL<b>1</b>-RLn. The row driver <b>820</b> may include a circuit for selecting and driving at least one of the plurality of row lines RL<b>1</b> to RLn. Similarly, the column driver <b>830</b> may be connected to the cell array <b>810</b> via the plurality of column lines CL<b>1</b>-CLm, and may include a circuit for selecting and driving at least one of the plurality of column lines CL<b>1</b>-CLm.</p><p id="p-0149" num="0146">Data may be stored in the plurality of memory cells MC of the cell array <b>810</b>. For example, data may be stored in the cell array <b>810</b> using changes in the resistance of the memory device ME included in each of the plurality of memory cells MC. Also, the cell array <b>810</b> may output read current Iread via a plurality of column lines CL<b>1</b>-CLm based on a plurality of input voltages input to the plurality of row lines RL<b>1</b> to RLn and data stored in the plurality of memory cells MC. The read current Iread may be converted into a signal voltage Vsig by sense amplifiers <b>842</b> included in a sense amplifier circuit <b>840</b> and connected to the plurality of column lines CL<b>1</b>-CLm, respectively.</p><p id="p-0150" num="0147">The signal voltage Vsig may be converted into a digital signal DS by analog-to-digital converters <b>852</b> included in the analog-to-digital converter circuit <b>850</b>. For example, each of the analog-to-digital converters <b>852</b> may include at least one resistive memory device RME including the same resistive material as that of the memory device ME included in the cell array <b>810</b>, and may generate a full scale voltage using a reference voltage Vref and at least one resistive memory device RME. Each of the analog-to-digital converters <b>852</b> may generate a plurality of divided voltages using a full-scale voltage, may compare the signal voltage Vsig with the plurality of division voltages and may output a digital signal DS.</p><p id="p-0151" num="0148">The adder circuit <b>860</b> may include a plurality of adders <b>862</b>. Each of the plurality of adders <b>862</b> may generate a summed digital signal ADS by summing the digital signal DS. For example, the adder circuit <b>860</b> may sum up the digital signals DS output by the plurality of memory cells MC connected to one of the plurality of column lines CL<b>1</b>-CLm and may output the summed digital signal ADS.</p><p id="p-0152" num="0149">The shift register circuit <b>870</b> may include a plurality of shift registers <b>872</b>. The plurality of shift registers <b>872</b> may generate output data DAT using the summed digital signal ADS. For example, the output data DAT may correspond to a final result of multiplication and accumulation computations executed in a neural network system implemented by a neuromorphic device.</p><p id="p-0153" num="0150">In some example embodiments, at least one of the number of sense amplifiers <b>842</b>, the number of analog-to-digital converters <b>852</b>, the number of adders <b>862</b>, and the number of shift registers <b>872</b> may be equal to or less than the number of the plurality of column lines CL<b>1</b> to CLm. The neuromorphic device <b>800</b> may include a logic circuit for controlling a row driver <b>820</b>, a column driver <b>830</b>, a sense amplifier circuit <b>840</b>, an analog-to-digital converter circuit <b>850</b>, and an adder circuit <b>860</b>, and a shift register circuit <b>870</b>, and a voltage regulator in addition to the components illustrated in <figref idref="DRAWINGS">FIG. <b>18</b></figref>.</p><p id="p-0154" num="0151"><figref idref="DRAWINGS">FIG. <b>19</b></figref> is a block diagram illustrating a neuromorphic device according to some example embodiments.</p><p id="p-0155" num="0152">Referring to <figref idref="DRAWINGS">FIG. <b>19</b></figref>, a neuromorphic device <b>900</b> may include a plurality of tiles <b>910</b>. Each of the plurality of tiles <b>910</b> may include a cell array <b>911</b> in which a plurality of memory cells is disposed, a row driver <b>912</b>, sense amplifiers <b>913</b>, and an analog-to-digital converter circuit <b>914</b>. For example, a plurality of memory cells disposed in the cell array <b>911</b> may be connected to the row driver <b>912</b> via a plurality of row lines, and may be connected to the sense amplifiers <b>913</b> and the analog-to-digital converter circuit <b>914</b> via the plurality of column lines.</p><p id="p-0156" num="0153">A plurality of tiles <b>910</b> may share a logic circuit <b>905</b>, and the logic circuit <b>905</b> may store weights of a neural network of which training has been completed in the plurality of tiles <b>910</b>, and may execute a computation corresponding to the operation of the neural network. Also, the logic circuit <b>905</b> may include input/output pads connected to an external device.</p><p id="p-0157" num="0154">In some example embodiments, the number of the plurality of tiles <b>910</b> may be equal to or greater than the number of hidden layers included in the neural network. When the neural network implemented by the neuromorphic device <b>900</b> includes an n number of hidden layers, the hidden layers may be implemented using an n number of tiles <b>910</b>. For example, the n number of tiles <b>910</b> may correspond to the hidden layers, respectively. Alternatively, a single hidden layer may be implemented with two or more tiles <b>910</b>. For example, the number of tiles <b>910</b> may be greater than the number of hidden layers.</p><p id="p-0158" num="0155">As described above, the plurality of memory cells included in the cell array <b>911</b> may store weights (e.g., between nodes) included in different layers in a neural network of which training has been completed. In some example embodiments, each value of weights in a neural network of which training has been completed may be represented as a floating point and may have a positive or negative sign. Since weights are recorded as resistance values or conductance values in each of the plurality of memory cells, the weights may be quantized to store the weights in the cell array.</p><p id="p-0159" num="0156">As an example, weights may be quantized according to distribution of weights included in each of the hidden layers in a neural network of which training has been completed. For example, a reference value may be determined based on the distribution of weights in each of the hidden layers, the reference value may be determined as a zero point weight, and weights greater than the reference value and weights less than the reference value may be quantized as quantized weights. Accordingly, when the quantization is completed, each value of the real weights represented as floating points may be represented as quantized weights and zero point weights. In some example embodiments, both the quantized weight and the zero point weight may be represented in the form of natural numbers. Hereinafter, a process of quantizing weights will be described in greater detail with reference to <figref idref="DRAWINGS">FIGS. <b>20</b>A to <b>20</b>C</figref>.</p><p id="p-0160" num="0157"><figref idref="DRAWINGS">FIGS. <b>20</b>A to <b>20</b>C</figref> are diagrams illustrating quantization in a neuromorphic device according to some example embodiments.</p><p id="p-0161" num="0158"><figref idref="DRAWINGS">FIGS. <b>20</b>A to <b>20</b>C</figref> illustrate graphs indicating distribution of weights included in one of layers in a neural network of which training has been completed. In the example embodiments in <figref idref="DRAWINGS">FIGS. <b>20</b>A to <b>20</b>C</figref>, the horizontal axis may indicate values of weights, and the vertical axis may correspond to the number of weights having the respective values.</p><p id="p-0162" num="0159">Referring to <figref idref="DRAWINGS">FIG. <b>20</b>A</figref>, the distribution of real weights included in one of the layers of the neural network of which training has been completed may be obtained, and a representative value of the distribution may be selected as a zero point weight ZP<b>1</b>. In the example illustrated in <figref idref="DRAWINGS">FIG. <b>20</b>A</figref>, the real weights represented as floating points may be quantized by dividing the weights into a plurality of sections A<b>1</b>-A<b>5</b> having the same length. As an example, the quantization may be performed as in Equation 2 below:</p><p id="p-0163" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>r=s</i>(<i>q&#x2212;z</i>)&#x2003;&#x2003;[Equation 2]<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0164" num="0160">In Equation 2, r may correspond to a real weight, q may correspond to a quantized weight allocated to each of the plurality of sections A<b>1</b>-A<b>5</b>, and z may correspond to a zero point weight ZP<b>1</b>, and s may be a scale value applied to the quantization. For example, the real weights included in the first section A<b>1</b>, greater than the zero point weight ZP<b>1</b>, may be quantized as first quantized weights, and the real weights included in the second section A<b>2</b>, smaller than the zero point weight ZP<b>1</b>, may be quantized as second quantized weights. For example, a difference between the first quantized weight and the zero point weight ZP<b>1</b> may be the same as the difference between the second quantized weight and the zero point weight ZP<b>1</b> and may have an opposite sign. For example, when the difference between the first quantized weight and the zero point weight ZP<b>1</b> is +&#x3b1;, the difference between the second quantized weight and the zero point weight ZP<b>1</b> may be &#x2212;&#x3b1;.</p><p id="p-0165" num="0161">In the example illustrated in <figref idref="DRAWINGS">FIG. <b>20</b>A</figref>, the plurality of sections A<b>1</b>-A<b>5</b> divided with reference to the zero point weight ZP<b>1</b> may have the same length. In the example illustrated in <figref idref="DRAWINGS">FIG. <b>20</b>B</figref>, the plurality of sections B<b>1</b>-B<b>5</b> determined for quantization may have different lengths.</p><p id="p-0166" num="0162">In the example illustrated in <figref idref="DRAWINGS">FIG. <b>20</b>B</figref>, a different quantization method may be applied to the weights of the same layer. For example, at least a portion of the plurality of sections B<b>1</b>-B<b>5</b> determined with reference to the same zero point weight ZP<b>1</b> as in <figref idref="DRAWINGS">FIG. <b>20</b>A</figref> may have different lengths. For example, the first section B<b>1</b> and the second section B<b>2</b> approximate to the zero point weight ZP<b>1</b> may have a relatively smaller length than those of the other sections B<b>3</b>-B<b>5</b>. Accordingly, in the sections closer to the zero point weight ZP<b>1</b>, the quantized weights may be arranged more densely, and a lower quantization error may be obtained as compared to the example illustrated in <figref idref="DRAWINGS">FIG. <b>20</b>A</figref>.</p><p id="p-0167" num="0163"><figref idref="DRAWINGS">FIG. <b>20</b>C</figref> is a diagram illustrating quantization of weights included in a layer different from the examples illustrated in <figref idref="DRAWINGS">FIGS. <b>20</b>A and <b>20</b>B</figref>. Referring to <figref idref="DRAWINGS">FIG. <b>20</b>C</figref>, as the distribution of weights in different layers is different, a zero point weight ZP<b>2</b> different from the example embodiments described with reference to <figref idref="DRAWINGS">FIGS. <b>20</b>A and <b>20</b>B</figref> may be selected. Also, a plurality of sections C<b>1</b>-C<b>5</b> are selected with reference to the zero point weight ZP<b>2</b>, and quantized weights for quantization may be determined for each of the plurality of sections C<b>1</b>-C<b>5</b>.</p><p id="p-0168" num="0164">In the examples illustrated in <figref idref="DRAWINGS">FIGS. <b>20</b>A to <b>20</b>C</figref>, the number of the plurality of sections A<b>1</b>-A<b>5</b>, B<b>1</b>-B<b>5</b>, and C<b>1</b>-C<b>5</b> determined with reference to the zero point weights ZP<b>1</b> and ZP<b>2</b> may be varied. For example, to reduce a quantization error, the number of the plurality of sections A<b>1</b>-A<b>5</b>, B<b>1</b>-B<b>5</b>, and C<b>1</b>-C<b>5</b> may be increased. To reduce computation burden and power consumption in the neuromorphic device implementing a neural network, the number of the plurality of sections A<b>1</b>-A<b>5</b>, B<b>1</b>-B<b>5</b>, and C<b>1</b>-C<b>5</b> may be reduced.</p><p id="p-0169" num="0165">The quantized weights and the zero point weights obtained by quantizing the weights included in the layers of the neural network may be stored in tiles included in the neuromorphic device. Only one zero point weight may be applied to the weights of a single layer among the layers included in the neural network, and the zero point weight may be stored in zero point cells connected to a portion of column lines in a tile for storing the weights of a single layer.</p><p id="p-0170" num="0166">As an example, when each of the memory cells operates as a single level cell SLC and the zero point weight is represented as 3-bit binary data, 3-bit data of the zero point weight may be stored in a plurality of zero point cells connected to three column lines. Also, since the same zero point weight is applied to the weights of a single layer, the same data may be stored in zero point cells at points in which a plurality of row lines and one of the three column lines intersect.</p><p id="p-0171" num="0167">In some example embodiments, the quantized weights obtained in the process of quantizing the weights of the layers included in the neural network may be stored in weight arrays, and the zero point weights may be stored in a zero point array different from the weight array. Each of the weight arrays may be included in the weight tile, and the zero point array may be included in the zero point tile.</p><p id="p-0172" num="0168">Accordingly, multiplication and accumulation computation of input values and quantized weights may be performed separately from the multiplication and accumulation computation of input values and zero point weights, and the computation burden of the neuromorphic device may be reduced. Also, when quantized weights corresponding to weights included in a single layer are stored in a plurality of weight arrays in a distributed manner in the neural network, it may not be necessary to store a zero point weight in each of the weight tiles including the plurality of weight arrays, integration density and power consumption of the neuromorphic device may improve.</p><p id="p-0173" num="0169">Similarly to the weights, input values received by nodes of each of the layers of the neural network may also be represented in floating point values, such that, in some example embodiments, the input values may also be quantized and may be input to the weight arrays and the zero point array, which will be described with reference to <figref idref="DRAWINGS">FIG. <b>21</b></figref> in the description below.</p><p id="p-0174" num="0170"><figref idref="DRAWINGS">FIG. <b>21</b></figref> is a diagram illustrating quantization in a neuromorphic device according to an example embodiment.</p><p id="p-0175" num="0171">In the neural network, nodes included in each of the layers may receive input values from nodes included in a previous layer. For example, output values of nodes included in the previous layer may be input values of the respective nodes. Since the output values of the nodes included in the previous layer are known values, the distribution of input values received by each of the layers of the neural network may be indicated as illustrated in <figref idref="DRAWINGS">FIG. <b>21</b></figref>.</p><p id="p-0176" num="0172">The zero point input value ZP may be determined by a representative value in the distribution of input values, and the input values may be quantized by setting a plurality of sections D<b>1</b>-D<b>4</b> with reference to the zero point weight ZP. The input values may be quantized as in Equation 2 described above, and may be represented as a quantized input value, a zero point input value, and a scale value. The quantized input value may have different values in the plurality of sections D<b>1</b>-D<b>4</b>.</p><p id="p-0177" num="0173">Hereinafter, an operation process executed in nodes included in layers of a neural network will be described. The nodes included in different layers may be connected to each other by weights. As an example, assuming a first hidden layer and a second hidden layer connected to each other, the first hidden nodes included in the first hidden layer and the second hidden nodes included in the second hidden layer may be connected to each other by predetermined (and/or otherwise determined) weights. The weights may be determined as training of the neural network is completed.</p><p id="p-0178" num="0174">Input values r<b>1</b> output by the first hidden nodes and received by the second hidden nodes, and real weights r<b>2</b> connecting the first hidden nodes to the second hidden nodes may be quantized as in Equation 3 as below:</p><p id="p-0179" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>r</i>1=<i>s</i>1(<i>q</i>1&#x2212;<i>z</i>1)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0180" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>r</i>2=<i>s</i>2(<i>q</i>2&#x2212;<i>z</i>2)&#x2003;&#x2003;[Equation 3]<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0181" num="0175">As described above, multiplication and accumulation computations may be performed between the layers included in the neural network. Accordingly, the output values r<b>3</b> output by the second hidden nodes may be represented as in Equation 4 as below. In Equation 4, q<b>3</b> is a quantized output value obtained by quantizing the actual output values r<b>3</b>, z<b>3</b> is a zero point output value determined in the process of quantizing the output values r<b>3</b>, and s<b>3</b> is a scale value determined in the process of quantizing the output values r<b>3</b>.</p><p id="p-0182" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>r</i>3=<i>s</i>3(<i>q</i>3&#x2212;<i>z</i>3)=&#x3a3;<i>s</i>1(<i>q</i>1&#x2212;<i>z</i>1)*<i>s</i>2(<i>q</i>2&#x2212;<i>z</i>2)&#x2003;&#x2003;[Equation 4]<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0183" num="0176">Equation 4 may also be represented as in Equation 5 as below, where M is a value calculated from the scale values s<b>1</b>-s<b>3</b> and may be s<b>1</b>*s<b>2</b>/s<b>3</b>.</p><p id="p-0184" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>q</i>3=<i>z</i>3+<i>M</i>&#x3a3;(<i>q</i>1*<i>q</i>2&#x2212;<i>q</i>1*<i>z</i>2&#x2212;<i>q</i>2*<i>z</i>1+<i>z</i>1*<i>z</i>2)&#x2003;&#x2003;[Equation 5]<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0185" num="0177">In the process of quantizing the weights of each of the layers included in the neural network, a single zero point weight z<b>2</b> may be applied to each of the layers. For example, the real weights r<b>2</b> included in one of the layers may have different values due to the quantized weight q<b>2</b>, and the zero point weight z<b>2</b> may have the same value.</p><p id="p-0186" num="0178">In some example embodiments, the zero point weight z<b>2</b> applied to each of the layers included in the neural network may be collected and implemented as an array. For example, the quantized weight q<b>2</b> generated by quantizing the real weights r<b>2</b> included in one of the layers may be stored in memory cells of the cell array, and the zero point weight z<b>2</b> may be stored in the zero point cells of the zero point array different from the cell array. As described above, by storing the zero point weight z<b>2</b> applied to quantize the weights of each layer in a zero point array different from the cell array, integration density of the neuromorphic device, power consumption and computation burden may be reduced.</p><p id="p-0187" num="0179"><figref idref="DRAWINGS">FIG. <b>22</b></figref> is a diagram illustrating a neuromorphic device according to some example embodiments.</p><p id="p-0188" num="0180"><figref idref="DRAWINGS">FIG. <b>22</b></figref> illustrates a plurality of tiles included in a neuromorphic device. In the neuromorphic apparatus <b>900</b> according to an example embodiment, the plurality of tiles <b>910</b>-<b>930</b> may correspond to layers included in a neural network to be implemented by the neuromorphic apparatus <b>900</b>, respectively. For example, the first tile <b>910</b> may correspond to a first hidden layer connected to the input layer, and the second tile <b>920</b> may correspond to a second hidden layer connected to the first hidden layer.</p><p id="p-0189" num="0181">Referring to the first tile <b>910</b> as an example, the cell array of the first tile <b>910</b> may include a plurality of memory cells and may be divided into a first weight area WA<b>1</b> and a first zero point area ZA<b>1</b>. Memory cells of the first weight area WA<b>1</b> may be allocated as weight cells, and memory cells of the first zero point area ZA<b>1</b> may be allocated as zero point cells. The weight cells included in the first weight area WA<b>1</b> and the zero point cells included in the first zero point area ZA<b>1</b> may share a plurality of row lines and may be connected to different column lines.</p><p id="p-0190" num="0182">In the weight cells, quantized weights generated by quantizing the weights of the first hidden layer may be stored. The zero point weight obtained by quantizing the weights of the first hidden layer may be stored in the zero point cells. In the process of quantizing the weights of the first hidden layer, a single zero point weight may be generated, such that zero point cells connected to a single column line in common in the first zero point area ZA<b>1</b> may store the same data. The first weight area WA<b>1</b> and the first zero point area ZA<b>1</b> may be connected to the row driver <b>912</b>, the sense amplifier circuit <b>913</b>, and the analog-to-digital converter circuit <b>914</b>. The sense amplifier circuit <b>913</b> may include a current-voltage converter converting a current output to a plurality of column lines into a voltage.</p><p id="p-0191" num="0183">In the neuromorphic apparatus <b>900</b>, each of the plurality of tiles <b>910</b>-<b>930</b> may include zero point areas ZA<b>1</b>-ZAn, whereas, in the neuromorphic apparatus <b>1000</b> according to another example embodiment, at least one zero point tile <b>1040</b> among a plurality of tiles <b>1010</b>-<b>1040</b> may store the zero point weights obtained in the process of quantizing weights included in layers of a neural network. However, the number of zero point tiles <b>1040</b> may be varied in the example embodiments.</p><p id="p-0192" num="0184">Accordingly, the neuromorphic apparatus <b>1000</b> may include a plurality of weight tiles <b>1010</b>-<b>1030</b> and at least one zero point tile <b>1040</b>. Each of the cell arrays of the plurality of weight tiles <b>1010</b>-<b>1030</b> may be a weight array, and may correspond to the above-described weight areas WA<b>1</b>-WAn. For example, memory cells included in a cell array of each of the plurality of weight tiles <b>1010</b>-<b>1030</b> may be allocated to weight cells, and the weight cells may store the quantized weights obtained by quantizing real weights included in each layer of the neural network. For example, the quantized weights obtained by quantizing real weights included in the first hidden layer of the neural network may be stored in weight cells of the first weight tile <b>1010</b>.</p><p id="p-0193" num="0185">The zero point weights applied in the process of quantizing weights included in each of the layers of the neural network may be stored in the zero point tile <b>1040</b>. As an example, referring to <figref idref="DRAWINGS">FIG. <b>22</b></figref>, the zero point array of the zero point tile <b>1040</b> may include a first zero point area ZA<b>1</b> in which a first zero point weight of the first hidden layer is stored, and a second zero point area ZA<b>2</b> in which a second zero point weight of the second hidden layer is stored. The zero point areas ZA<b>1</b>-ZAn included in the zero point array of the zero point tile <b>1040</b> may share a plurality of row lines and may be connected to different column lines.</p><p id="p-0194" num="0186">For example, the number of column lines connected to each of the zero point areas ZA<b>1</b>-ZAn may be the same, which may be because, in the process of quantizing the weights included in the neural network and converting the weights into binary data, the zero point weights may be converted into binary data having the same number of bits. However, when a portion of the weights are converted into binary data having a different number of bits, the number of column lines connected to a portion of the zero point areas ZA<b>1</b>-ZAn may be different from each other.</p><p id="p-0195" num="0187">Accordingly, in some example embodiments of the neuromorphic apparatus <b>1000</b>, the zero point weight and the quantized weights obtained by quantizing the weights included in one of the layers of the neural network may be stored in the plurality of tiles <b>1010</b>-<b>1040</b> in a distributed manner. For example, the quantized weights obtained by quantizing the weights of the first hidden layer may be stored in the first weight tile <b>1010</b>, and the zero point weight may be stored in the first zero point area ZA<b>1</b>.</p><p id="p-0196" num="0188">In the operation process of the neuromorphic apparatus <b>1000</b>, one of the plurality of weight tiles <b>1010</b>-<b>1030</b> and the zero point tile <b>1040</b> may operate simultaneously. For example, when multiplication and accumulation computations corresponding to the second hidden layer of the neural network are performed in the neuromorphic apparatus <b>1000</b>, the second weight tile <b>1020</b> and the zero point tile <b>1040</b> may operate simultaneously. The logic circuit of the neuromorphic apparatus <b>1000</b> may output a result of computation corresponding to the second hidden layer using the result of weight computation of the second weight tile <b>1020</b> and the result of zero point computation of the zero point tile <b>1040</b>.</p><p id="p-0197" num="0189">As an example, the result of weight computation output by the second weight tile <b>1020</b> may include (q<b>1</b>*q<b>2</b>) and (q<b>2</b>*z<b>1</b>) in Equation 5, and the result of zero point computation output via at least one column line connected to the second zero point area ZA<b>2</b> may include (z<b>1</b>*z<b>2</b>) and (q<b>1</b>*z<b>2</b>) in Equation 5. The logic circuit may obtain the result of computation of the second hidden layer by summing the result of weight computation and the result of zero point computation as in Equation 5.</p><p id="p-0198" num="0190">The result of zero point computation output by the zero point tile <b>1040</b> may be, e.g., a dot product operation of input values and a zero point weight. Also, since a single zero point weight is stored in each of the first to N-th zero point areas ZA<b>1</b>-ZAn, the result of zero point computation may be varied only according to the number of ones included in the input values. Accordingly, in some example embodiments, the result of zero point computation according to the number of 1s included in the input values may be previously stored in a buffer, and the number of 1s included in the input values input to one of the weight tiles <b>1010</b>-<b>1030</b> may be counted and one of the values stored in the buffer may be selected, thereby swiftly calculating the result of zero point computation. For example, the buffer may be included in a logic circuit of the neuromorphic device. The computation operation as above will be described in greater detail later.</p><p id="p-0199" num="0191">Each of the plurality of tiles <b>1010</b>-<b>1040</b> may include a row driver <b>1012</b>, <b>1022</b>, <b>1032</b>, and <b>1042</b>, sense amplifier circuits <b>1013</b>, <b>1023</b>, <b>1033</b>, and <b>1043</b>, and analog-to-digital converter circuit <b>014</b>, <b>1024</b>, <b>1034</b>, and <b>1044</b>. In some example embodiments, a reference current may be input to the analog-to-digital converter circuits <b>1014</b>, <b>1024</b>, <b>1034</b>, and <b>1044</b> of each of the plurality of tiles <b>1010</b>-<b>1040</b>. Alternatively, the reference current may not be input to the analog-to-digital converter circuits <b>1014</b>, <b>1024</b>, <b>1034</b>, and <b>1044</b>, and as described above with reference to <figref idref="DRAWINGS">FIGS. <b>3</b> to <b>17</b></figref>, the result of computation may be digitally converted by comparing the digital cell data with the digital reference data.</p><p id="p-0200" num="0192"><figref idref="DRAWINGS">FIGS. <b>23</b> and <b>24</b></figref> are diagrams illustrating a neuromorphic device according to some example embodiments.</p><p id="p-0201" num="0193">Referring to <figref idref="DRAWINGS">FIG. <b>23</b></figref>, to implement one of the layers of the neural network, a single tile <b>1100</b> included in the neuromorphic device may include a cell array <b>1101</b>, a row driver <b>1102</b>, a sense amplifier circuit <b>1103</b>, and an analog-to-digital converter circuit <b>1104</b>. The tile <b>1100</b> may obtain zero point weights and quantized weights by quantizing real weights included in the single layer. The zero point weight may be stored in zero point cells disposed in the zero point area ZA of the cell array <b>1101</b>, and the quantized weights may be stored in memory cells disposed in the weight area WA of the cell array <b>1101</b>.</p><p id="p-0202" num="0194">However, it may be difficult to implement a layer with the single tile <b>1101</b> depending on the number of nodes included in each of the layers in the neural network. Accordingly, as illustrated in <figref idref="DRAWINGS">FIG. <b>23</b></figref>, the tile <b>1101</b> for implementing the layer may be divided into a plurality of tiles <b>1110</b>-<b>1160</b>. For example, when the tile <b>1101</b> includes 496 row lines and 128 column lines, the tile <b>1101</b> may be divided into tiles <b>1110</b>-<b>1160</b> each having 31 row lines and 32 column lines. In this case, the number of tiles <b>1110</b>-<b>1160</b> may be 64.</p><p id="p-0203" num="0195">In each of the plurality of tiles <b>1110</b>-<b>1160</b> in which weights included in a single layer are stored, the cell array may include a weight area WA and a zero point area ZA. Since the plurality of tiles <b>1110</b>-<b>1160</b> store real weights included in single layer in a divided manner, the weight area WA may store different data. Since only a single zero point weight is generated by quantizing real weights included in a single layer, the zero point area ZA of each of the plurality of tiles <b>1110</b>-<b>1160</b> may include the same data.</p><p id="p-0204" num="0196">The number of zero point cells included in the zero point area ZA in each of the plurality of tiles <b>1110</b>-<b>1160</b> may be smaller than the number of weight cells included in the weight area WA. For example, when the zero point weight is 3-bit data, the zero point area ZA in each of the plurality of tiles <b>1110</b>-<b>1160</b> may include first to third zero point column lines. Also, the zero point cells connected to the first zero point column line may store the same data, the zero point cells connected to the second zero point column line may also store the same data, and the zero point cells connected to the third zero point column line may also store the same data.</p><p id="p-0205" num="0197">In the example illustrated in <figref idref="DRAWINGS">FIG. <b>23</b></figref>, since the zero point area ZA storing a single zero point weight as data is included in each of the plurality of tiles <b>1110</b>-<b>1160</b>, integration density of the neuromorphic device may be lowered. Accordingly, in some example embodiments, the zero point weight stored in the zero point area ZA of each of the plurality of tiles <b>1110</b>-<b>1160</b> may be stored in a different tile. Hereinafter, it will be described in greater detail with reference to <figref idref="DRAWINGS">FIG. <b>24</b></figref>.</p><p id="p-0206" num="0198">Referring to <figref idref="DRAWINGS">FIG. <b>24</b></figref>, to implement one of the layers of the neural network, a single tile <b>1200</b> included in the neuromorphic device may include a cell array <b>1201</b>, a row driver <b>1202</b>, a sense amplifier circuit <b>1203</b>, and an analog-to-digital converter circuit <b>1204</b>. As described with reference to <figref idref="DRAWINGS">FIG. <b>23</b></figref>, zero point weights and quantized weights may be generated by quantizing real weights included in a single layer, and the zero point weights may be stored in the zero point area ZA of the cell array <b>1201</b> and quantized weights may be stored in the weight area WA of the cell array <b>1201</b>.</p><p id="p-0207" num="0199">The tile <b>1200</b> in which weights included in a single layer are stored may be divided into a plurality of tiles <b>1210</b>-<b>1270</b>. In some example embodiments, the quantized weights stored in the weight area WA may be stored in the plurality of weight tiles <b>1210</b>-<b>1260</b> in a distributed manner, and the zero point weights stored in the zero point area ZA may be stored in one of the zero point areas ZA<b>1</b>-ZAn included in the zero point tile <b>1270</b>.</p><p id="p-0208" num="0200">For example, when the cell array <b>1201</b> includes 496 row lines and 128 column lines, data may be stored in the weight tiles <b>1210</b>-<b>1260</b> each having 31 row lines and 32 column lines and in the single zero point tile <b>1270</b>. Accordingly, in the example illustrated in <figref idref="DRAWINGS">FIG. <b>24</b></figref>, the number of tiles <b>1210</b>-<b>1270</b> used to implement a single layer may be greater than in the example illustrated in <figref idref="DRAWINGS">FIG. <b>23</b></figref>.</p><p id="p-0209" num="0201">However, differently from the example illustrated in <figref idref="DRAWINGS">FIG. <b>23</b></figref>, in the example illustrated in <figref idref="DRAWINGS">FIG. <b>24</b></figref>, the zero point weight may be stored in one of the zero point areas ZA<b>1</b>-ZAn of the separately implemented zero point tile <b>1270</b>. The result of the multiplication and accumulation computation performed in a single layer may correspond to the result of weight computation obtained by quantizing the input values and inputting the values to the plurality of weight tiles <b>1210</b>-<b>1260</b> and the result of zero point computation obtained by quantizing the input values and inputting the values to one of the zero point areas ZA<b>1</b>-ZAn.</p><p id="p-0210" num="0202">Accordingly, since the multiplication and accumulation computations of the zero point weight of a single layer and the quantized input values may be executed once in the example illustrated in <figref idref="DRAWINGS">FIG. <b>24</b></figref>, the power consumption of the neuromorphic device <b>1200</b> may be reduced. Also, differently from the example in <figref idref="DRAWINGS">FIG. <b>23</b></figref> in which an entirety of the plurality of tiles <b>1110</b>-<b>1160</b> have the zero point area ZA, the zero point areas ZA<b>1</b>-ZAn may be collected in a different zero point tile <b>1270</b>, such that integration density of the neuromorphic device <b>1200</b> may also improve.</p><p id="p-0211" num="0203"><figref idref="DRAWINGS">FIGS. <b>25</b> and <b>26</b></figref> are diagrams illustrating a neuromorphic device according to some example embodiments.</p><p id="p-0212" num="0204">In some example embodiments, a neuromorphic device <b>1300</b> may include a logic circuit <b>1305</b> and a plurality of tiles <b>1310</b>-<b>1360</b>. The logic circuit <b>1305</b> may include a circuit for writing data to and reading data from the plurality of tiles <b>1310</b>-<b>1360</b>, an input/output interface circuit connected to an external device, and a power circuit. For example, the logic circuit <b>1305</b> may perform multiplication and accumulation computations using the plurality of banks <b>1310</b>-<b>1360</b>.</p><p id="p-0213" num="0205">Each of the plurality of banks <b>1310</b>-<b>1360</b> may include a cell array in which a plurality of memory cells is disposed, a row driver RD, a sense amplifier circuit S/A, and an analog-to-digital converter (ADC) circuit ADC, and the cell array may include a weight area in which weight cells among the memory cells are disposed and a zero point area in which zero point cells among the memory cells are disposed. As described above, quantized weights obtained by quantizing weights included in one of the layers of the neural network may be stored in the weight cells. Also, the zero point weights obtained by quantizing the weights may be stored in the zero point cells.</p><p id="p-0214" num="0206">In some example embodiment, real weights included in each of the layers of the neural network may be stored in two of the tiles <b>1310</b>-<b>1360</b> in a distributed manner. For example, real weights included in the first hidden layer among the layers of the neural network may be quantized and may be stored in the first tile <b>1310</b> and the second tile <b>1320</b> in a distributed manner. Also, real weights included in the second hidden layer may be quantized and may be stored in the third tile <b>1330</b> and the fourth tile <b>1340</b> in a distributed manner.</p><p id="p-0215" num="0207">A zero point weight generated in the process of quantizing real weights included in each of the layers of the neural network may have a single value in a single layer. For example, the real weights included in a single layer may have the same zero point weight after quantization. Accordingly, the first zero point area ZA<b>1</b> of the first tile <b>1310</b> and the second tile <b>1320</b> may store the same data, and the second zero point area ZA<b>2</b> of the third tile <b>1330</b> and the fourth tile <b>1340</b> may also store the same data. Accordingly, since the zero point areas ZA<b>1</b>-ZAn storing the same data are repeatedly arranged, integration density of the neuromorphic apparatus <b>1300</b> may be reduced.</p><p id="p-0216" num="0208">In some example embodiments, the above issue may be addressed by collecting zero point areas and arranging the areas on a single zero point tile. For example, referring to <figref idref="DRAWINGS">FIG. <b>26</b></figref>, zero point weights generated in the process of quantizing real weights of respective layers of the neural network may be stored in a zero point tile <b>1480</b>. The zero point tile <b>1480</b> may include a zero point array <b>1481</b> including zero point cells in which zero point weights are stored.</p><p id="p-0217" num="0209">The quantized weights generated by quantizing the real weights of each of the layers of the neural network may be stored in the weight tiles <b>1410</b>-<b>1470</b> in a distributed manner for each layer. Similarly to the example described above with reference to <figref idref="DRAWINGS">FIG. <b>25</b></figref>, quantized weights generated from real weights included in a single layer may be stored in two of the weight tiles <b>1410</b>-<b>1470</b>.</p><p id="p-0218" num="0210">As for the examples in <figref idref="DRAWINGS">FIGS. <b>25</b> and <b>26</b></figref>, a single first zero point area ZA<b>1</b> in which zero point weights obtained by quantizing real weights included in the first hidden layer are stored may be present in the example in <figref idref="DRAWINGS">FIG. <b>26</b></figref>, differently from <figref idref="DRAWINGS">FIG. <b>25</b></figref>. Since zero point weights corresponding to real weights of the neural network are stored in the zero point tile <b>1480</b>, the neuromorphic device <b>1400</b> according to the example illustrated in <figref idref="DRAWINGS">FIG. <b>26</b></figref> may have integration density higher than that of the example illustrated in <figref idref="DRAWINGS">FIG. <b>25</b></figref>.</p><p id="p-0219" num="0211">Also, in the neuromorphic apparatus <b>1400</b> according to some example embodiments, the amount of computation may be reduced. When multiplication and accumulation computations are performed by inputting input values to the first hidden layer, in the example embodiment illustrated in <figref idref="DRAWINGS">FIG. <b>26</b></figref>, the result of weight computation obtained by inputting input values to each of the first weight tile <b>1410</b> and the second weight tile <b>1420</b> and the result of zero point computation obtained by inputting input values into the first zero point area ZA<b>1</b> included in the zero point array <b>1481</b> may be summed. In other words, the multiplication and accumulation computations performed by inputting the input values into the first zero point area ZA<b>1</b> may only be executed once, and accordingly, the amount of computation may be reduced.</p><p id="p-0220" num="0212"><figref idref="DRAWINGS">FIG. <b>27</b></figref> is a flowchart illustrating operations of a neuromorphic device according to some example embodiments. <figref idref="DRAWINGS">FIG. <b>28</b></figref> is a diagram illustrating operations of a neuromorphic device according to some example embodiments.</p><p id="p-0221" num="0213">Referring to <figref idref="DRAWINGS">FIGS. <b>27</b> and <b>28</b></figref>, the operation of the neuromorphic device <b>1500</b> may start by quantizing an input value (S<b>20</b>). For example, a quantized input value <b>1505</b> may be input to the neuromorphic device <b>1500</b>. In operation S<b>20</b>, an actual input value represented as a floating point may be quantized, and accordingly, a quantized input value q<b>1</b> and a zero point input value z<b>1</b> may be generated.</p><p id="p-0222" num="0214">Thereafter, the neuromorphic device <b>1500</b> may input the quantized input value q<b>1</b> and the zero point input value z<b>1</b> to at least one weight array among the weight tiles <b>1510</b> and may obtain a result of weight computation (S<b>21</b>). Similarly to the input values q<b>1</b> and z<b>1</b>, real weights included in layers in the neural network may be quantized and may be stored in the neuromorphic device <b>1500</b>. For example, quantized weights and zero point weights may be obtained by quantizing real weights represented in floating point numbers, and the quantized weights may be stored in a weight array of each of the weight tiles <b>1510</b>, and the zero point weight may be stored in a zero point array of a different zero point tile <b>1520</b>.</p><p id="p-0223" num="0215">In some example embodiments, the quantized weights obtained by quantizing the real weights included in one of the layers of the neural network implemented by the neuromorphic device <b>1500</b> may be stored in a plurality of weight tiles <b>1510</b> in a distributed manner Each of the plurality of weight tiles <b>1510</b> may include a weight area WA in which weight cells for storing weights are disposed, a row driver RD, a sense amplifier circuit S/A, and an analog-to-digital converter (ADC) circuit ADC. The zero point weight obtained by quantizing the real weights included in the layer may be stored in the zero point tile <b>1520</b>. For example, when the single layer is the first hidden layer, the zero point weight may be stored in the first zero point area ZA<b>1</b>.</p><p id="p-0224" num="0216">When the quantized weights are defined as q<b>2</b> and the zero point weight is defined as z<b>2</b>, the result of weight computations output by the plurality of weight tiles <b>1511</b>-<b>1514</b> may include a result (q<b>1</b>*q<b>2</b>) of multiplication of the quantized input values q<b>1</b> and the quantized weights q<b>2</b> and a result (q<b>2</b>*z<b>1</b>) of multiplication of the zero point input value z<b>1</b> and the quantized weights q<b>2</b> as described with reference to Equation 5.</p><p id="p-0225" num="0217">The neuromorphic apparatus <b>1500</b> may obtain a result of zero point computation by inputting the quantized input values q<b>1</b> and the zero point input value z<b>1</b> to the zero point tile <b>1520</b> (S<b>22</b>). The zero point weight z<b>2</b> may be stored in one of the zero point areas ZA<b>1</b>-ZAn included in the zero point tile <b>1520</b>. The neuromorphic device <b>900</b> may input the quantized input values q<b>1</b> and the zero point input value z<b>1</b> to the zero point tile <b>1520</b> via the row driver RD, and the result of zero point computation may be read via column lines connected to a single area among the zero point areas ZA<b>1</b>-ZAn T in which the zero point weight z<b>2</b> is stored.</p><p id="p-0226" num="0218">As described with reference to Equation 5, the result of zero point computation may include a result (q<b>1</b>*z<b>2</b>) of multiplication of the quantized input values q<b>1</b> and the zero point weight z<b>2</b>, and a result (z<b>1</b>*z<b>2</b>) of multiplication of the zero point input value z<b>1</b> and the zero point weight z<b>2</b>. For example, operations S<b>21</b> and S<b>22</b> may be simultaneously executed.</p><p id="p-0227" num="0219">The neuromorphic apparatus <b>1500</b> may obtain a result of computation <b>1540</b> including output values of the single layer included in the neural network by summing the result of weight computation and the result of zero point computation (S<b>23</b>). Referring to <figref idref="DRAWINGS">FIG. <b>28</b></figref>, the result of weight computation output by at least one of the weight tiles <b>1510</b> and the result of zero point computation output by the zero point tile <b>1520</b> may be summed by a summer <b>1530</b>. Since both the input values q<b>1</b> and z<b>1</b> and the weights q<b>2</b> and z<b>2</b> are represented as X-bit data, the summer <b>1530</b> may preferentially perform a multiplication of a multiplier of 2 before summing the result of weight computation and the result of zero point computation.</p><p id="p-0228" num="0220"><figref idref="DRAWINGS">FIG. <b>29</b></figref> is a flowchart illustrating operations of a neuromorphic device according to some example embodiments. <figref idref="DRAWINGS">FIG. <b>30</b></figref> is a diagram illustrating operations of a neuromorphic device according to some example embodiments.</p><p id="p-0229" num="0221">Referring to <figref idref="DRAWINGS">FIGS. <b>29</b> and <b>30</b></figref>, the operation of the neuromorphic device <b>1600</b> may start with quantizing input values <b>1605</b> (S<b>30</b>). For example, quantized input values q<b>1</b> and zero point input values z<b>1</b> may be obtained by quantizing the input values.</p><p id="p-0230" num="0222">The neuromorphic apparatus <b>1600</b> may obtain a result of weight computation by inputting the input values q<b>1</b> and z<b>1</b> into at least one weight array among weight tiles <b>1610</b> (S<b>31</b>). The weight tiles <b>1610</b> may store quantized weights q<b>2</b> obtained by quantizing weights included in one of the layers of the neural network. For example, the result of weight computation may include a result of multiplication (q<b>1</b>*q<b>2</b>) of the quantized input values q<b>1</b> and the quantized weights q<b>2</b>, and a result (q<b>2</b>*z<b>1</b>) of multiplication of the zero point input value z<b>1</b> and the quantized weights q<b>2</b>.</p><p id="p-0231" num="0223">In some example embodiments, the neuromorphic device <b>1600</b> may store the zero point weight z<b>2</b> obtained by quantizing the weights included in the single layer in one of the zero point areas ZA<b>1</b>-ZAn of the zero point tile <b>1020</b>. However, instead of obtaining a result of zero point computation by directly inputting the input values q<b>1</b> and z<b>1</b> to the zero point tile <b>1620</b>, the neuromorphic device <b>1600</b> may count the number of 1s included in the input values q<b>1</b> and z<b>1</b> (S<b>32</b>), and may select one of the results of zero point computation pre-stored in the buffer <b>1640</b> according to the count value (S<b>33</b>).</p><p id="p-0232" num="0224">As described above, the zero point weight z<b>2</b> generated by quantizing the weights included in single layer may have a single value. Accordingly, the zero point cells connected to a single column line in common in the zero point tile <b>1620</b> may store the same data.</p><p id="p-0233" num="0225">Since zero point cells connected to each of the column lines store the same data, data output by each of the column lines storing a single zero point weight z<b>2</b> may be determined by the number of 1s included in the input values q<b>1</b> and z<b>1</b> input to the zero point cells via the row driver RD. In an example embodiment, the result of zero point computation according to the number of 1s included in the input values q<b>1</b> and z<b>1</b> may be stored in the buffer <b>1640</b> in advance. The buffer <b>1640</b> may classify the result of zero point computation according to the layers included in the neural network and the number of 1s included in the input values q<b>1</b> and z<b>1</b> and may store the result. This may be because different zero point weights may be generated when weights are quantized in each of the layers included in the neural network.</p><p id="p-0234" num="0226">The neuromorphic device <b>1600</b> may count the number of 1s in the input values q<b>1</b> and z<b>1</b> using the counter circuit <b>1630</b>, and may read the result of zero point computation corresponding to the input values q<b>1</b> and z<b>1</b> from the buffer <b>1640</b> with reference to the count value. The neuromorphic apparatus <b>1600</b> may sum the result of zero point computation read from the buffer <b>1640</b> and the result of weight computation output by at least one of the weight tiles <b>1610</b> in the summer <b>1650</b>, and accordingly, the result of computation <b>1660</b> may be obtained. Similarly to the example described above, the summer <b>1650</b> may preferentially perform a computation of multiplying a multiplier of 2 before summing the result of weight computation and the result of zero point computation.</p><p id="p-0235" num="0227">In some example embodiments, the results of zero point computation according to the number of 1s included in the input values q<b>1</b> and z<b>1</b> may be classified according to the layers included in the neural network and may store the results in the buffer <b>1640</b> in advance. Also, in the process of executing multiplication and accumulation computations of the neural network, the neuromorphic device <b>1600</b> may obtain the result of zero point computation by simply counting the number of 1s in the input values q<b>1</b> and z<b>1</b> input to the corresponding layer and reads from the buffer <b>1640</b> and reading the number of 1s. Accordingly, the computation burden and power consumption of the neuromorphic device <b>1600</b> may be reduced.</p><p id="p-0236" num="0228"><figref idref="DRAWINGS">FIG. <b>31</b></figref> is a diagram illustrating data stored in a buffer of a neuromorphic device according to some example embodiments.</p><p id="p-0237" num="0229">Referring to <figref idref="DRAWINGS">FIG. <b>31</b></figref>, results of zero point computations BUF<b>1</b>-BUF<b>32</b> may be stored in the buffer of the neuromorphic device. As described above with reference to Equation 5, the results of zero point computation BUF<b>1</b>-BUF<b>32</b> may include the result (q<b>1</b>*z<b>2</b>) of multiplication of the quantized input values q<b>1</b> and the zero point weight z<b>2</b>, and a result (z<b>1</b>*z<b>2</b>) of multiplication of the zero point input value z<b>1</b> and the zero point weight z<b>2</b>.</p><p id="p-0238" num="0230">In some example embodiments, the results of zero point computation BUF<b>1</b>-BUF<b>32</b> may be classified according to the number of 1s included in the input values In[<b>1</b>]-In[<b>32</b>] and the layers Layer<b>1</b>-LayerN included in the neural network and may be stored. This may be because different zero point weights may be generated in the process of quantizing the real weights of each of the layers included in the neural network, and the result of zero point computation may be determined based on the number of 1s included in the input values In[<b>1</b>]-In[<b>32</b>].</p><p id="p-0239" num="0231">When multiplication and accumulation computations corresponding to one of the layers Layer<b>1</b>-LayerN included in the neural network are executed in the neuromorphic device, the neuromorphic device may count the number of 1s included in the input value In[<b>1</b>]-In[<b>32</b>]. Also, the neuromorphic apparatus may select one of the results of zero point computation BUF<b>1</b>-BUF<b>32</b> stored in the buffer based on the count value, and may add the result of weight computation to the selected result of zero point computation. Accordingly, since a result of zero point computation may be obtained by simply executing the computation for storing the results of zero point computation BUF<b>1</b>-BUF<b>32</b> in the buffer in advance and counting the number of 1s included in the input value In[<b>1</b>]-In[<b>32</b>] in the actual multiplication and accumulation computations using the neural network, the computation burden and power consumption of the neuromorphic device may be reduced.</p><p id="p-0240" num="0232"><figref idref="DRAWINGS">FIGS. <b>32</b> and <b>33</b></figref> are block diagrams illustrating electronic devices including a neuromorphic device according to some example embodiments.</p><p id="p-0241" num="0233">Referring to <figref idref="DRAWINGS">FIG. <b>32</b></figref> a neuromorphic device <b>1710</b> may be included in the electronic device <b>1700</b> in a stand-alone form. The electronic device <b>1700</b> may include a host <b>1720</b>, an interface <b>1730</b>, a memory <b>1740</b>, and/or a storage <b>1750</b> in addition to the neuromorphic device <b>1710</b>. For example, the electronic device <b>1710</b> may be an application server and/or a storage server disposed in a data center, and may be connected to a network via the interface <b>1730</b>.</p><p id="p-0242" num="0234">The host <b>1720</b> may be (and/or include) at least one processor (e.g., a central processing unit (CPU)) controlling overall operation of the electronic device <b>1710</b>, and/or may be implemented as at least one processor including at least one core. The host <b>1720</b> may write data in the storage <b>1750</b> including a plurality of storage devices, may read data stored in the storage <b>1750</b>, and/or may delete data stored in the storage <b>1750</b>. The memory <b>1740</b> may be implemented as volatile and/or non-volatile memory. For example the memory <b>1740</b> may include a dynamic random access memory, and the host <b>1720</b> may execute commands and/or data loaded to the memory <b>1740</b>.</p><p id="p-0243" num="0235">The neuromorphic device <b>1710</b> may execute a neuromorphic computation for the operation of the electronic device <b>1700</b>, and may be implemented in a stand-alone form as described above. For example, in some example embodiments, the neuromorphic device <b>1710</b> is implemented separately from the host <b>1720</b>, and for example, the neuromorphic device <b>1710</b> and the host <b>1720</b> may be included in the electronic device <b>1700</b> in the form of separate packages.</p><p id="p-0244" num="0236">In some example embodiments, the host <b>1720</b> may process a portion of the neuromorphic computations of the neuromorphic device <b>1710</b>, and may transfer the result of the computation processing to the neuromorphic device <b>1710</b>. As an example, the neuromorphic device <b>1710</b> may execute a MAC computation corresponding to the neural network as described above, and may process the MAC computation by quantizing each of the weights and input values of one of the layers of the neural network. In some example embodiments, to reduce the computation burden of the neuromorphic device <b>1710</b>, the host <b>1720</b> may process the multiplication computation of the input values and the zero point weight generated by quantizing the weights, and may transfer the result of the computation processing to the neuromorphic device <b>1710</b>.</p><p id="p-0245" num="0237">Referring next to <figref idref="DRAWINGS">FIG. <b>33</b></figref>, the neuromorphic device <b>1833</b> may be included in an application processor (AP) <b>1830</b> (e.g., in the form of a system-on-chip) together with a CPU <b>1831</b>, a NAND controller <b>1832</b> and a DRAM controller <b>1834</b>. In some example embodiments, the electronic device <b>1800</b> may be implemented as a mobile device such as a laptop computer, a mobile phone, a smartphone, a tablet personal computer, a wearable device, a healthcare device, Internet-of-Things (IoT), and/or the like.</p><p id="p-0246" num="0238">Referring to <figref idref="DRAWINGS">FIG. <b>33</b></figref>, the electronic device <b>1800</b> may include an input/output device <b>1821</b>, a modem <b>1822</b>, an audio <b>1823</b>, a display <b>1824</b>, a camera <b>1825</b>, an application processor <b>1830</b>, and a DRAM <b>1841</b>-<b>1842</b> (<b>1840</b>), and a NAND memory <b>1851</b>-<b>1853</b> (<b>1850</b>).</p><p id="p-0247" num="0239">The camera <b>1825</b> may obtain a still image and/or a video (e.g., according to a user control_. The electronic device <b>1800</b> may obtain and recognize specific information using still images and videos obtained by the camera <b>1825</b>, may inform relevant information, may convert still images and videos into other types of data such as text, and may store the data. Alternatively, the electronic device <b>1800</b> may recognize a character string included in a still image/video obtained by the camera <b>1825</b> and may provide a text/audio translation of the text corresponding to the character string.</p><p id="p-0248" num="0240">The display <b>1824</b> may be implemented in various forms such as a liquid crystal display (LCD), organic light emitting diodes (OLED) display, active-matrix organic light-emitting diode (AM-OLED), plasma display panel (PDP), field emission display (FED), an electronic paper, and/or the like. In some example embodiments, the display <b>1824</b> may be used as an input device of the electronic device <b>1800</b> by providing a touch screen function, and/or may be integrated with a fingerprint sensor and may provide a security function of the electronic device <b>1800</b>.</p><p id="p-0249" num="0241">The audio <b>1823</b> may process audio data stored in the electronic device <b>1800</b> and/or audio data included in the content of the network received from an external entity via the modem <b>1822</b> or the input/output device <b>1821</b>, and may output the data. The audio <b>1823</b> may, for example, include a speaker and/or a microphone to receive and/or output data as audio. The modem <b>1822</b> may modulate a signal to transmit/receive wired/wireless data and may transmit the signal, and may demodulate original data from a signal received from an external entity. The input/output device <b>1821</b> may provide digital input/output, and may include a port connectable to an external recording medium, an input device such as a touch screen or a mechanical button key, and an output device outputting vibration in a haptic manner Though illustrated as separate functional blocks, the example embodiments are not so limited. For example, more or fewer functional blocks may be included, and/or some of the functional blocks may combined.</p><p id="p-0250" num="0242">The application processor <b>1830</b> may control the overall operation of the electronic device <b>1800</b>. Specifically, the application processor <b>1830</b> may control the display <b>1824</b> such that a portion of the content stored in the NAND memory <b>50</b> may be displayed on the display <b>1824</b>. Also, the application processor <b>1830</b> may perform a control operation corresponding to a user input received via the input/output device <b>1821</b>.</p><p id="p-0251" num="0243">The application processor <b>1830</b> may be provided as a system-on-chip (hereinafter &#x201c;SoC&#x201d;) for driving an application program, and an operating system (OS). Accordingly, the application processor <b>1830</b> may include the CPU <b>1831</b> executing an operation or driving an application program and/or an operating system, and various other peripheral components connected to the CPU <b>1831</b> via a system bus. The peripheral components may include a DRAM controller <b>1834</b>, a neuromorphic device <b>1833</b>, a NAND controller <b>1832</b>, an internal memory, a power management block, an error detection block, and a monitoring block. The CPU <b>1831</b> may include one or more cores.</p><p id="p-0252" num="0244">In some example embodiment, the neuromorphic device <b>1833</b> may include a dedicated circuit for neuromorphic computations. The neuromorphic device <b>1833</b> may be directly connected to at least one of the DRAM <b>1840</b> and/or the NAND memory <b>1850</b> and may include, for example, a computation circuit for implementing a neural network. The neuromorphic device <b>1833</b> may include an analog circuit and/or a digital circuit for AI data operation, and similarly to the example described with reference to <figref idref="DRAWINGS">FIG. <b>32</b></figref>, a portion of the neuromorphic computations of the neuromorphic device <b>1833</b> may be processed in the CPU <b>1831</b>. Accordingly, the computation burden of the neuromorphic device <b>1833</b> and an area required to implement the neuromorphic device <b>1833</b> may be reduced.</p><p id="p-0253" num="0245"><figref idref="DRAWINGS">FIG. <b>34</b></figref> is a block diagram illustrating a neuromorphic device according to some example embodiments. <figref idref="DRAWINGS">FIGS. <b>35</b>A to <b>35</b>D</figref> are diagrams illustrating quantization in a neuromorphic device according to an example embodiment.</p><p id="p-0254" num="0246">Referring to <figref idref="DRAWINGS">FIG. <b>34</b></figref>, a neuromorphic device <b>1900</b> may include a computational processor <b>1910</b>, a counter circuit <b>1920</b>, a buffer <b>1930</b>, and an adder circuit <b>1940</b>. The neuromorphic device <b>1900</b> may receive input values <b>1901</b> from an external entity, and may perform a MAC computation using weights and input values <b>1901</b> included in at least one of the layers included in the neural network.</p><p id="p-0255" num="0247">In some example embodiments, the neuromorphic apparatus <b>1900</b> may quantize each of the input values <b>1901</b> and weights and may execute a MAC computation. Hereinafter, a quantization method for an MAC computation will be described with reference to <figref idref="DRAWINGS">FIGS. <b>35</b>A to <b>35</b>D</figref>.</p><p id="p-0256" num="0248"><figref idref="DRAWINGS">FIGS. <b>35</b>A to <b>35</b>D</figref> are diagrams illustrating examples of the distribution of real weights connecting at least a portion of the layers included in a neural network to each other and a result of quantization thereof. Real weights in each of the layers of the neural network of which training has been completed may be represented as floating point numbers and may have positive or negative signs. Quantization of the real weights may be determined according to the distribution of weights included in each of the hidden layers in the neural network of which training has been completed. For example, a reference value may be determined based on the distribution of weights in each of the hidden layers, the reference value may be determined as a zero point weight, and weights greater than the reference value and weights less than the reference value may be quantized as quantized weights. Accordingly, when quantization is completed, each value of the real weights represented as floating-point values may be represented as a quantized weight and a zero point weight. Both the quantized weight and the zero point weight may be represented in an integer form.</p><p id="p-0257" num="0249">Similarly, the input values <b>1901</b> may also be quantized into a zero point input value corresponding to a reference value, and quantized input values. As an example, the quantization of the weights and the input values <b>1901</b> may be performed as in Equation 2 described above. For example, as illustrated in Equation 2, the actual value r of the weights may be represented as a zero point weight z, a quantized weight q, and a scale value s applied to quantization.</p><p id="p-0258" num="0250">In each of the example illustrated in <figref idref="DRAWINGS">FIGS. <b>35</b>A to <b>35</b>D</figref>, the number of quantum states qs<b>1</b> allocated to real weights greater than the zero point weight z may be equal to the number of quantum states qs<b>2</b> allocated to the real weights smaller than the zero point weight z. However, according to the distribution of real weights, as illustrated in <figref idref="DRAWINGS">FIG. <b>5</b>D</figref>, the number of real weights allocated to each of the quantum states qs<b>1</b> and qs<b>2</b> symmetric to each other with respect to the zero point weight z may be varied.</p><p id="p-0259" num="0251">The quantization method for the input values <b>1901</b> may be similarly to the quantization method for the weights, and may be performed as in Equation 3 described above. For example, in a first hidden layer and a second hidden layer adjacent to each other among the plurality of layers of the neural network, input values r<b>1</b> output by the first hidden layer and received by the second hidden layer, and each of the real weights r<b>2</b> connecting the first hidden layer to the second hidden layer may be quantized as in Equation 3.</p><p id="p-0260" num="0252">Multiplication and accumulation computations may be performed between layers included in the neural network. Accordingly, the output values r<b>3</b> output by the second hidden nodes may be represented as in Equation 4 described above.</p><p id="p-0261" num="0253">In the process of quantizing the real weights r<b>2</b>, the real weights r<b>2</b> connecting a single hidden layer to another hidden layer may be quantized based on one zero point weight z<b>2</b>. Accordingly, the result of zero point computation may be varied only according to the number of 1s included in the input values <b>1901</b>. In some example embodiments, the result of zero point computation according to the number of 1s included in the input values <b>1901</b> may be stored in the buffer <b>1930</b> in advance, and the counter circuit <b>1920</b> may count the number of is included in the input values <b>1901</b> and may select one of the values stored in the buffer, thereby swiftly calculating the result of zero point computation.</p><p id="p-0262" num="0254">The result of zero point computation stored in the buffer <b>1930</b> may be calculated by an external device of the neuromorphic device <b>1900</b>, such as, for example, a host able to communicate with the neuromorphic device <b>1900</b>, rather than by the neuromorphic device <b>1900</b>, and may be stored in the buffer <b>1930</b> of the neuromorphic device <b>1900</b>. In other words, the MAC computation for obtaining the result of zero point computation may be performed by an external host, not by the neuromorphic device <b>1900</b>. For example, the MAC computation for obtaining the result of zero point computation may be executed in a digital computation circuit of the host. Accordingly, power consumption and computation burden of the neuromorphic device <b>1900</b> may be reduced, and the area of a circuit required for the MAC computation may be reduced.</p><p id="p-0263" num="0255">A method for the neuromorphic device <b>1900</b> to receive a result of zero point computation from an external host and to store the result in the buffer <b>1930</b> may be varied in example embodiments. As described above, the zero point result of computation may vary only depending on the number of 1s included in the input values <b>1901</b>, and the number of 1s able to be included in the input values <b>1901</b> may depend on the number of bits of the input values <b>1901</b>, and accordingly, the external host may generate the result of zero point computation regardless of the actual output value of each of the layers. The external host may generate the result of zero point computation in advance using the zero point weight for the real weights of each of the layers of the neural network and the number of 1s included in the input values <b>1901</b>, and may store the result in an external storage of the neuromorphic device <b>1900</b> in advance. When a neural network to perform the MAC computation is determined in the neuromorphic device <b>1900</b>, the external host may read out the results of zero point computation corresponding to the neural network from among the results of zero point computation stored in the storage, and may transfer the result to the neuromorphic device <b>1900</b>. In this case, the buffer <b>130</b> may be implemented as a volatile memory.</p><p id="p-0264" num="0256">Alternatively, the buffer <b>1930</b> may be implemented as a nonvolatile memory. The external host may generate a result of zero point computation in advance using the zero point weight for the real weights of each of the layers of the neural network and the number of 1s included in the input values <b>1901</b>, and may store the result in the buffer <b>1930</b> in the neuromorphic device <b>1900</b>. Accordingly, when the counter circuit <b>1920</b> determines the number of 1s in the input values <b>1901</b>, the result of zero point computation stored in the buffer <b>1930</b> may be transmitted to the adder circuit <b>1940</b>. In the above embodiment, while the neuromorphic device <b>1900</b> performs the MAC computation, the result of zero point computation may not be separately received from the external host.</p><p id="p-0265" num="0257">The external host may obtain the result of zero point computation using the zero point weights stored in the nonvolatile memory area in the neuromorphic device <b>1900</b>. For example, when the neural network is determined, the external host may receive zero point weights stored in the neuromorphic device <b>1900</b> and may generate a result of zero point computation, and may transfer the result to the neuromorphic device <b>1900</b>. The result of zero point computation may be stored in the buffer <b>1930</b>, and in this case, the buffer <b>1930</b> may be implemented as a volatile memory.</p><p id="p-0266" num="0258">The external host may generate a result of zero point computation in the process of generating the neural network, and may store the result in the buffer <b>1930</b> of the neuromorphic device <b>1900</b>. For example, the external host may determine the zero point weights by quantizing real weights included in the plurality of layers in the neural network of which training has been completed. Since the number of 1s included in the input values may vary depending on the number of bits of the input values, the external host may generate the result of zero point computation according to the number of 1s included in the input values and may transmit the result to the neuromorphic device <b>1900</b>. The neuromorphic apparatus <b>1900</b> may read out the result of zero point computation stored in the buffer <b>1930</b> and may use the result when performing inference using the neural network. Accordingly, since the computation using the zero point weights during the inference operation based on the neural network may not be executed in the neuromorphic device <b>1900</b> and also in the external host, the computation burden and power consumption may be reduced.</p><p id="p-0267" num="0259"><figref idref="DRAWINGS">FIG. <b>36</b></figref> is a block diagram illustrating a neuromorphic device according to some example embodiments.</p><p id="p-0268" num="0260">Referring to <figref idref="DRAWINGS">FIG. <b>36</b></figref>, the neuromorphic device <b>2010</b> may include a computational processor <b>2011</b>, a counter circuit <b>2012</b>, a buffer <b>2013</b> and an adder circuit <b>2014</b>, and may be mounted on the electronic device <b>2000</b> together with the external host <b>2020</b> and the storage device <b>2030</b>. As described above, the neuromorphic device <b>2010</b> may be implemented as a single semiconductor package with the host <b>2020</b>, and/or may be implemented as a stand-alone type neuromorphic device and may be provided as a semiconductor package separate from the host <b>2020</b>.</p><p id="p-0269" num="0261">The neuromorphic device <b>2010</b> may execute a MAC computation corresponding to the neural network. For example, the neuromorphic device <b>2010</b> may perform the MAC computation by quantizing weights corresponding to nodes connecting the layers of the neural network to each other and input values <b>2001</b> transferred between the layers.</p><p id="p-0270" num="0262">The result <b>2002</b> of zero point computation obtained by executing the MAC computation using the zero point weight and the input values <b>2001</b> may be executed in the host <b>2020</b> instead of the neuromorphic device <b>2010</b>. As described above, when the zero point weight is determined by quantizing the weights included in the neural network, the result <b>2002</b> of zero point computation may be varied only depending on the number of 1s included in the input values <b>2001</b>. Since the number of 1s included in the input values <b>2001</b> depends on the number of bits of the input values <b>2001</b>, the host <b>2020</b> may obtain the result <b>2002</b> of zero point computation using the zero point weight corresponding to each of the layers of the neural network in advance and may store the result in the storage device <b>2030</b>.</p><p id="p-0271" num="0263">When the MAC computation for the neural network starts in the neuromorphic device <b>2010</b>, the host <b>2020</b> may read out the result <b>2002</b> of zero point computation stored in the storage device <b>2030</b> and may transmit the results to the neuromorphic device <b>2010</b> along with the input values <b>2001</b>. The neuromorphic device <b>2010</b> may store the result <b>2002</b> of zero point computation in the buffer <b>2013</b>, and the buffer <b>2013</b> may be implemented as a volatile memory. For example, the neuromorphic device <b>2010</b> may receive the result <b>2002</b> of zero point computation for each of the layers included in the neural network in sequence and may store the result in the buffer <b>2013</b>, or may simultaneously receive the results <b>2002</b> of zero point computation for the layers included in the neural network and may store the result in the buffer <b>2013</b>.</p><p id="p-0272" num="0264">The input values <b>2001</b> received from the host <b>2020</b> may be transmitted to the computational processor <b>2011</b> and the counter circuit <b>2012</b>. The computational processor <b>2011</b> may execute a MAC computation using the input values <b>2001</b> and the quantized weights. The computational processor <b>2011</b> may execute the MAC computation in an analog or digital manner. For example, the computational processor <b>2011</b> may include a cell array in which a plurality of memory cells storing quantized weights are arranged, and by inputting the input values <b>2001</b> into the cell array, the result of weight computation, which may be the result of MAC computation of the quantized weights, may be obtained.</p><p id="p-0273" num="0265">Alternatively, the computational processor <b>2011</b> may include a computation circuit implemented with a plurality of logic gates, receiving the input values <b>2001</b> and quantized weights and outputting a result of weight computation. For example, the computational processor <b>2011</b> may receive input values <b>2001</b> necessary for generating the result of weight computation and also the quantized weights from the host <b>2020</b>.</p><p id="p-0274" num="0266">The counter circuit <b>2012</b> receiving the input values <b>2001</b> may count the number of is included in the input values <b>2001</b> and may transmit the count result to the buffer <b>2013</b>. The buffer <b>2013</b> may read out the result <b>2002</b> of zero point computation corresponding to the number of 1s received from the counter circuit <b>2012</b> and may transmit the result to the adder circuit <b>2014</b>. The adder circuit <b>2014</b> may sum the result <b>2002</b> of zero point computation and the result of weight computation output by the computational processor <b>2011</b> and may output a result of computation value for at least one of the layers included in the neural network.</p><p id="p-0275" num="0267"><figref idref="DRAWINGS">FIG. <b>37</b></figref> is a diagram illustrating data stored in a buffer of a neuromorphic device according to some example embodiments.</p><p id="p-0276" num="0268">Referring to <figref idref="DRAWINGS">FIG. <b>37</b></figref>, a result of zero point computation may be stored in the buffer of the neuromorphic device. As described above with reference to Equation 5, the result of zero point computation may include a result of multiplication computation of quantized input values and a zero point weight, and a result of multiplication computation of a zero point input value and a zero point weight. However, as described above, the result of zero point computation may be determined according to the number of 1s included in the input values. In the example illustrated in <figref idref="DRAWINGS">FIG. <b>37</b></figref>, the number of 1s included in each of the input values may range from a minimum of 0 to a maximum of 32.</p><p id="p-0277" num="0269">In some example embodiments, the results of zero point computation may be classified according to the number of 1s included in input values and layers Layer<b>1</b>-LayerN included in the neural network and may be stored. This may be because different zero point weights may be generated in the process of quantizing the real weights of each of the layers included in the neural network, and the result of zero point computation may be determined according to the number of 1s included in the input value.</p><p id="p-0278" num="0270">As an example, a result of multiplying a first zero point weight L<b>1</b>_Z<b>2</b> obtained by quantizing the weights of nodes connecting the first layer LAYER<b>1</b> to the immediately preceding layer by the number of 1s of each of the input values input from the to the immediately preceding layer to the first layer LAYER<b>1</b> may be stored in the buffer as a result of the zero point computation of the first layer LAYER<b>1</b>. Also, result of multiplying a second zero point weight L<b>2</b>_Z<b>2</b> obtained by quantizing the weights of the nodes connecting the second layer LAYER<b>2</b> to the first layer LAYER<b>1</b> by the number of 1s included in each of the input values input from the first layer LAYER<b>1</b> to the second layer LAYER<b>2</b> may be stored in the buffer as a result of the zero point computation of the second layer LAYER<b>2</b>.</p><p id="p-0279" num="0271">As such, since the result of zero point computation is determined according to the number of 1s included in each of the input values and the zero point weight, the neuromorphic device may count the number of 1s included in the input values received from an external entity, and may read the result of zero point computation from the buffer. Accordingly, the neuromorphic device may not execute the computation for obtaining the result of zero point computation for every MAC computation, and the computation burden and power consumption of the neuromorphic device may be reduced, and the circuit area of the neuromorphic device may also be reduced.</p><p id="p-0280" num="0272"><figref idref="DRAWINGS">FIGS. <b>38</b> and <b>39</b></figref> are block diagrams illustrating neuromorphic devices according to some example embodiments.</p><p id="p-0281" num="0273">Referring first to <figref idref="DRAWINGS">FIG. <b>38</b></figref>, the neuromorphic device <b>2010</b>A may include a computational processor <b>2011</b>, a counter circuit <b>2012</b>, a nonvolatile memory <b>2013</b>A, and an adder circuit <b>2014</b>, and may be mounted on the electronic device <b>2000</b>A together with the external host <b>2020</b>. The neuromorphic device <b>2010</b>A may execute a MAC computation corresponding to the neural network.</p><p id="p-0282" num="0274">The result <b>2002</b> of zero point computation obtained by executing the MAC computation using the zero point weight and the input values <b>2001</b> may be executed in the host <b>2020</b> instead of the neuromorphic device <b>2010</b>. As described above, when the zero point weight is determined by quantizing the weights included in the neural network, the result <b>2002</b> of zero point computation may be varied only depending on the number of 1 s included in the input values <b>2001</b>. Since the number of 1s included in the input values <b>2001</b> may depend on the number of bits of the input values <b>2001</b>, the host <b>2020</b> may obtain the result <b>2002</b> of zero point computation using the zero point weight corresponding to each of the layers of the neural network in advance and may store the result in the nonvolatile memory <b>2013</b>A in the neuromorphic device <b>2010</b>A.</p><p id="p-0283" num="0275">When the MAC computation for the neural network starts, the neuromorphic device <b>2010</b> may receive input values <b>2001</b> for the neural network from the host <b>2020</b>. The input values <b>2001</b> may be quantized by the neuromorphic device <b>2010</b> or may be transmitted to the neuromorphic device <b>2010</b> after being quantized by the host <b>2020</b>. The computational processor <b>2011</b> may perform a MAC computation of the zero point input value obtained by quantizing the input values <b>2001</b> and the quantized input values with the quantized weights and may output the result of weight computation to the adder circuit <b>2014</b>. The computational processor <b>2011</b> may generate a result of weight computation using a computation circuit implemented as a cell array or a logic gate.</p><p id="p-0284" num="0276">The counter circuit <b>2012</b> may transmit a result of counting the number of 1s included in the input values <b>2001</b> to the nonvolatile memory <b>2013</b>A. The nonvolatile memory <b>2013</b>A may receive the result <b>2002</b> of zero point computation from the host <b>2020</b> in advance and may store the result, and may select one of the results <b>2002</b> of zero point computation based on the count result received from the counter circuit <b>2012</b> and may output the result to the adder circuit <b>2014</b>. The adder circuit <b>2014</b> may sum the result of weight computation and the result of zero point computation and may output a result value of computation for at least one of the layers included in the neural network.</p><p id="p-0285" num="0277">Referring to <figref idref="DRAWINGS">FIG. <b>39</b></figref>, the neuromorphic device <b>2010</b>B may include a computational processor <b>2011</b>, a counter circuit <b>2012</b>, a buffer <b>2013</b>B, an adder circuit <b>2014</b>, and a nonvolatile memory <b>2015</b>, and may be mounted in the electronic device <b>2000</b>B together with an external host <b>2020</b>. The neuromorphic device <b>2010</b>B may execute a MAC computation corresponding to the neural network.</p><p id="p-0286" num="0278">The result <b>2002</b> of zero point computation obtained by executing the MAC computation using the zero point weight and input values <b>2001</b> may be executed in the host <b>2020</b> instead of the neuromorphic device <b>2010</b>. For example, the zero point weight <b>2003</b> generated by quantizing the weights in each of the layers included in the neural network may be stored in the nonvolatile memory <b>2015</b> in the neuromorphic device <b>2010</b>. When the MAC computation for the neural network starts, the host <b>2010</b> may receive the zero point weight <b>2003</b> stored in the nonvolatile memory <b>2015</b> of the neuromorphic device <b>2010</b>B. The host <b>2010</b> may generate a result of zero point computation using the zero point weight <b>2003</b> and the input values <b>2001</b>, and may transmit the result to the neuromorphic device <b>2010</b>B together with the input values <b>2001</b>. For example, the host <b>2020</b> may generate a result of zero point computation using a zero point input value obtained by quantizing the input values <b>2001</b> and the quantized input values, and may transmit the zero point input value and the quantized input values to the neuromorphic device <b>2010</b>B together with the result <b>2002</b> of zero point computation.</p><p id="p-0287" num="0279">The neuromorphic device <b>2010</b>B may store the result <b>2002</b> of zero point computation received from the host <b>2020</b> in the buffer <b>2013</b>B. The computational processor <b>2011</b> may perform a MAC computation of the zero point input value obtained by quantizing the input values <b>2001</b> and the quantized input values with the quantized weights and may output the result of weight computation to the adder circuit <b>2014</b>. The computational processor <b>2011</b> may generate a result of weight computation using a computation circuit implemented as a cell array or a logic gate.</p><p id="p-0288" num="0280">The counter circuit <b>2012</b> may transmit a result of counting the number of 1s included in the input values <b>2001</b> to the buffer <b>213</b>. The buffer <b>2013</b>B may select one of the results <b>2002</b> of zero point computation based on the count result received from the counter circuit <b>2012</b> and may output the result to the adder circuit <b>2014</b>. The adder circuit <b>2014</b> may output a result value of the MAC computation for at least one of the layers included in the neural network by summing the result of weight computation and the result of zero point computation.</p><p id="p-0289" num="0281"><figref idref="DRAWINGS">FIGS. <b>40</b>A to <b>40</b>C</figref> are diagrams illustrating quantization in a neuromorphic device according to some example embodiments.</p><p id="p-0290" num="0282">Referring to <figref idref="DRAWINGS">FIGS. <b>40</b>A to <b>40</b>C</figref> are diagrams illustrating example distributions of real weights corresponding to nodes connecting layers adjacent to each other in a neural network, and quantum states allocated to quantize the real weights. Referring to <figref idref="DRAWINGS">FIGS. <b>40</b>A to <b>40</b>C</figref>, differently from the example embodiment described with reference to <figref idref="DRAWINGS">FIGS. <b>35</b>A to <b>35</b>D</figref>, the quantum states may be asymmetrically allocated around a zero point weight. When a range in which real weights equal to or greater than the zero point weight are distributed may be defined as a first range and a range in which real weights equal to or less than the zero point weight are distributed may be defined as a second range, in the examples illustrated in <figref idref="DRAWINGS">FIGS. <b>40</b>A to <b>40</b>C</figref>, real weights may be quantized differently the first range RA<b>1</b> and in the second range RA<b>2</b>. In quantization such as the example embodiments illustrated in <figref idref="DRAWINGS">FIGS. <b>40</b>A to <b>40</b>C</figref>, quantized weights may have values represented as unsigned integers.</p><p id="p-0291" num="0283">Referring to <figref idref="DRAWINGS">FIGS. <b>40</b>A and <b>40</b>B</figref>, three quantum states including a zero point weight may be allocated to the first range RA<b>1</b>, and five quantum states including a zero point weight may be allocated to the second range RA<b>2</b>. This may be because more real weights may be distributed in the second range RA<b>2</b> than in the first range RA<b>1</b>. In the example illustrated in <figref idref="DRAWINGS">FIG. <b>40</b>C</figref>, real weights may be distributed only in the first range RA<b>1</b> with reference to the zero point weight. Accordingly, as illustrated in <figref idref="DRAWINGS">FIG. <b>40</b>C</figref>, a plurality (e.g., eight) of quantum states may be allocated only to the first range RA<b>1</b>. As described above, by allocating different numbers of quantum states to each of the first range RA<b>1</b> and the second range RA<b>2</b> according to the distribution of real weights, a decrease in accuracy due to quantization of real weights may be reduced.</p><p id="p-0292" num="0284">The quantization described with reference to <figref idref="DRAWINGS">FIGS. <b>40</b>A to <b>40</b>C</figref> may be similarly applied to quantization of input values. Also in quantizing input values, by defining each range above and below the zero point input value, which is a reference value, and allocating different numbers of quantum states according to the distribution of input values, a decrease in accuracy due to quantization may improve.</p><p id="p-0293" num="0285"><figref idref="DRAWINGS">FIGS. <b>41</b> and <b>42</b></figref> are diagrams illustrating operations of a computational processor included in a neuromorphic device according to some example embodiments.</p><p id="p-0294" num="0286"><figref idref="DRAWINGS">FIGS. <b>41</b> and <b>42</b></figref> may correspond to examples in which a computational processor of a neuromorphic device digitally executes a MAC computation and outputs a result of weight computation. For example, in the examples described with reference to <figref idref="DRAWINGS">FIGS. <b>41</b> and <b>42</b></figref>, the computational processor may not include memory cells storing quantized weights. Accordingly, the neuromorphic device may externally receive input values <b>2101</b> and <b>2201</b> and quantized weights <b>2102</b> and <b>2202</b> necessary for MAC computation.</p><p id="p-0295" num="0287"><figref idref="DRAWINGS">FIG. <b>41</b></figref> is a diagram illustrating an example method of digitally executing a MAC computation. Referring to <figref idref="DRAWINGS">FIG. <b>41</b></figref>, the computational processor <b>2100</b> of the neuromorphic device may include a plurality of MAC computation circuits <b>2110</b>-<b>2130</b>. The plurality of MAC computation circuits <b>2110</b>-<b>2130</b> may receive the input values <b>2101</b> and the quantized weights <b>2102</b> from an external entity and may execute the MAC computation digitally. The quantized weights <b>2102</b> may be values allocated to quantum states for quantizing real weights provided to paths connecting layers to each other in a neural network, and may be a value obtained by converting an unsigned integer into binary data. The input values <b>2101</b> may also be quantized and may be input to the computational processor <b>2100</b>.</p><p id="p-0296" num="0288">For example, when the computational processor <b>2100</b> executes a MAC computation connecting an N number of previous nodes to an M number of current nodes, input values output by the N number of previous nodes may be input to the first MAC computation circuit <b>2110</b>. Also, quantized weights <b>2102</b> obtained by quantizing real weights provided to paths connecting the first current node to N number of previous nodes may be input to the first MAC computation circuit <b>2110</b>. Similarly, input values <b>2101</b> output by the N number of previous nodes may be input to the second MAC computation circuit <b>2120</b>, and quantized weights <b>2102</b>, obtained by quantizing real weights given to paths connecting the second current node to the N number of previous nodes, may be input to the second MAC computation circuit <b>2120</b>.</p><p id="p-0297" num="0289">Accordingly, each of the plurality of MAC computation circuits <b>2110</b>-<b>2130</b> may output a result of weight computation <b>2103</b> corresponding to each of the M number of current nodes. As described above, the neuromorphic device may determine the output value of each of the M number of current nodes by adding the result of weight computation <b>2103</b> output by the computational processor <b>2100</b> to the result of zero point computation read from the buffer.</p><p id="p-0298" num="0290"><figref idref="DRAWINGS">FIG. <b>42</b></figref> is an example illustrating a computational processor <b>2200</b> digitally executing a MAC computation. Referring to <figref idref="DRAWINGS">FIG. <b>42</b></figref>, the computational processor <b>2200</b> may include a multiplier array <b>2210</b> and an accumulator <b>2220</b> for MAC computation. The multiplier array <b>2210</b> may include a plurality of multipliers arranged along a plurality of rows and a plurality of columns, may receive input values <b>2201</b> in a plurality of rows, and may receive quantized weights <b>2202</b> quantized in a plurality of columns. The input values <b>2201</b> may be quantized and may be input to the multiplier array <b>2210</b> as a zero point input value and quantized input values.</p><p id="p-0299" num="0291">In some example embodiments, each of the plurality of multipliers included in the multiplier array <b>2210</b> may multiply n-bit data and may output 2n-bit result data. Two or more multipliers connected to a single column among the plurality of multipliers may be connected to a single register included in an accumulator <b>2220</b>. The accumulator <b>2220</b> may include a register and an adder, and may accumulate 2n-bit result data output by multipliers connected to a single column.</p><p id="p-0300" num="0292">When weight computation for a single layer among a plurality of layers included in the neural network is completed, the accumulator <b>2220</b> may output a result of weight computation <b>2203</b>. The neuromorphic device may add the result of weight computation output by the computational processor <b>2200</b> with the result of zero point computation selected in the buffer and may complete the MAC computation for the corresponding layer, and may start the MAC computation for a subsequent layer, or may output the result of MAC computation to an external host.</p><p id="p-0301" num="0293">Each of the plurality of multipliers included in the multiplier array <b>2210</b> may include a plurality of AND gates and an adder circuit, and may execute a multiplication computation according to various multiplication rules. Hereinafter, an example method of performing a multiplication computation in each of a plurality of multipliers will be described in greater detail with reference to <figref idref="DRAWINGS">FIGS. <b>43</b> and <b>44</b></figref>.</p><p id="p-0302" num="0294"><figref idref="DRAWINGS">FIGS. <b>43</b> and <b>44</b></figref> are diagrams illustrating operations of a computational processor included in a neuromorphic device according to some example embodiments.</p><p id="p-0303" num="0295">When the computational processor does not include memory cells storing quantized weights, the computational processor may digitally execute the multiplication computation. As an example, the multiplier of the computational processor may execute a multiplication computation using at least one computation rule of DADDA multiplication, WALLACE multiplication, lattice multiplication, and booth multiplication, and a digital logic circuit for executing at least one of the computation rules.</p><p id="p-0304" num="0296"><figref idref="DRAWINGS">FIG. <b>43</b></figref> may be a diagram illustrating an example in which multiplication is performed by a lattice multiplication rule. For example, the multiplier of the computational processor may multiply the input value <b>2201</b> converted into 8-bit data by a weight <b>2202</b> converted into 8-bit data. In some example embodiments, the input value may be a zero point input value and/or a quantized input value, and the weight may be a quantized weight. In some example embodiments, the multiplication computation of the zero point weight and the input value may be performed in an external host (e.g., not in a neuromorphic device) including a computational processor.</p><p id="p-0305" num="0297">Referring to <figref idref="DRAWINGS">FIG. <b>43</b></figref>, an input value <b>2201</b> may be disposed on an upper end of a lattice, a weight <b>2202</b> may be disposed on the right side of the lattice, and an AND computation may be performed bit by bit, and 64 pieces of data included in an 8&#xd7;8 matrix may be defined. When the AND computation is finished, the MAC computation circuit may perform an addition computation by a bit unit in a diagonal direction illustrated in <figref idref="DRAWINGS">FIG. <b>43</b></figref> from the lower right with respect to the lattice. For example, When a carry-on number appears as a result of performing the addition computation in the diagonal direction, &#x201c;1&#x201d; may be added to a plurality of pieces of data arranged in the subsequent diagonal direction. According to the above rule, the result of the addition computation in the fourth diagonal direction from the lower right side may be &#x201c;0,&#x201d; and the result of the addition computation in the fifth diagonal direction may be &#x201c;1.&#x201d;</p><p id="p-0306" num="0298">When the addition computation in the diagonal direction for overall data in the lattice is completed, the output value <b>2203</b> may be derived by arranging bits in sequence from the upper left end of the lattice. In the example embodiment in <figref idref="DRAWINGS">FIG. <b>43</b></figref>, the result of multiplying the input value &#x201c;01101001&#x201d; by the weight &#x201c;10001001&#x201d; may be obtained as &#x201c;0011100000110101.&#x201d;</p><p id="p-0307" num="0299">To perform multiplication in the lattice multiplication method as described with reference to <figref idref="DRAWINGS">FIG. <b>43</b></figref>, the multiplier of the MAC computation circuit may include an AND gate and an adder circuit. For example, when a single multiplier may include 64 AND gates, the AND computation of each bit of the 8-bit input value <b>2201</b> and the 8-bit weight <b>2202</b> may be simultaneously performed, and 64 pieces of data included in the lattice may be combined. However, the number of AND gates included in the multiplier may be appropriately selected if desired.</p><p id="p-0308" num="0300"><figref idref="DRAWINGS">FIG. <b>44</b></figref> may be a diagram illustrating an example in which multiplication is performed by the DADDA rule. In the example illustrated in <figref idref="DRAWINGS">FIG. <b>44</b></figref>, similarly to the example described with reference to <figref idref="DRAWINGS">FIG. <b>43</b></figref> above, the multiplier of the computational processor may multiply the input value converted into 8-bit data with a weight converted into 8-bit data. In some example embodiments, the input value may be a zero point input value and/or a quantized input value, and the weight may be a quantized weight. The multiplication computation of the zero point weight and the input value may be performed in an external host, rather than in a neuromorphic device including a computational processor.</p><p id="p-0309" num="0301">By performing, an AND computation on an input value with each of the 8 bits included in the weight, 8 pieces of 8-bit data may be generated, and may be aligned as in the first stage STAGE<b>1</b> illustrated in <figref idref="DRAWINGS">FIG. <b>44</b></figref>. Accordingly, the height of the first stage STAGE<b>1</b> may be 8. Thereafter, a portion of bits arranged in the vertical direction among eight pieces of data included in the first stage STAGE<b>1</b> may be summed to obtain each sum and a carry-on number, and a second stage STAGE<b>2</b> may be configured using the obtained results. In the second stage STAGE<b>2</b>, the height may be reduced to 6.</p><p id="p-0310" num="0302">In the DADDA multiplication, the result of multiplication may be obtained by repeating the process of calculating the sum and the number of carry-on numbers by summing a portion of the plurality of pieces of data included in the previous stage, and transferring the calculated result to a subsequent stage together with the remaining data. For example, the heights may decrease one by one in sequence in the third to fifth stages STAGE<b>3</b>-STAGES, and finally, a result of multiplication may be obtained in the sixth stage STAGE<b>6</b>.</p><p id="p-0311" num="0303">For example, each of the areas R<b>12</b>-R<b>24</b> illustrated in <figref idref="DRAWINGS">FIG. <b>44</b></figref> may be areas for classifying the pieces of data on which a computation for matching the heights of the stages STAGE<b>1</b>-STAGE<b>6</b> is executed. In each of the areas R<b>12</b>-R<b>24</b> illustrated in <figref idref="DRAWINGS">FIG. <b>44</b></figref>, the bits indicated by the diagonal lines may be added by a half adder, and the bits indicated by the letter X may be added by the full adder.</p><p id="p-0312" num="0304">To perform the multiplication in the DADDA multiplication method as described with reference to <figref idref="DRAWINGS">FIG. <b>44</b></figref>, the multiplier of the MAC computation circuit may include an AND gate, and an adder circuit. The adder circuit may include at least one half adder circuit and at least one full adder circuit.</p><p id="p-0313" num="0305">In the neuromorphic device, the computational processor may digitally execute a MAC computation of input values and quantized weights. The digitally operating computational processor may not include a cell array storing quantized weights, and accordingly, the digitally operating computational processor may receive input values necessary for computation and quantized weights from an external entity and may perform the MAC computation. However, in some example embodiments, the computational processor may include a cell array storing quantized weights, and may perform a MAC computation using the cell array.</p><p id="p-0314" num="0306">A computational processor digitally executing a MAC computation may include a plurality of MAC computation circuits, and each of the plurality of MAC computation circuits may include a multiplier, an adder, and a register. In each of the MAC computation circuits, the multiplier may include an AND gate, an adder logic circuit to digitally multiply N-bit input values and M-bit quantized weights.</p><p id="p-0315" num="0307">Also, the computational processor may not execute the MAC computation using the zero point weight and input values. Instead, the MAC computation using the zero point weight and input values may be executed in an external host connected to the neuromorphic device, and the neuromorphic device may store the zero point result of computation transmitted by the external host as a result of the MAC computation in a buffer.</p><p id="p-0316" num="0308">When the operation corresponding to the neural network starts, the neuromorphic device may count the number of 1s included in the input values transmitted between the layers of the neural network and may select the result of zero point computation from the buffer, and the output value of each of the nodes of the corresponding layer may be determined by summing the number of 1s with the result of weight computation output by the computational processor. In some example embodiments, the external host may execute the result of zero point computation and the computational processor may be digitally implemented if desired, thereby improving integration density and reducing power consumption and computation burden of the neuromorphic device, and decreasing the reduction in computational accuracy caused by quantization of input values and weights.</p><p id="p-0317" num="0309"><figref idref="DRAWINGS">FIG. <b>45</b></figref> is a block diagram illustrating a semiconductor device including a neuromorphic device according to some example embodiments.</p><p id="p-0318" num="0310">Referring to <figref idref="DRAWINGS">FIG. <b>45</b></figref>, a semiconductor device <b>2300</b> may include a CPU <b>2310</b>, a GPU <b>2320</b>, a DSP <b>2330</b>, an NPU <b>2340</b>, a sensor interface <b>2350</b>, a display interface <b>2360</b>, and a memory interface <b>2370</b>. In some example embodiment, the semiconductor device <b>2300</b> illustrated in <figref idref="DRAWINGS">FIG. <b>45</b></figref> may be an application processor, a system-on-chip (SoC) mounted on an electronic device.</p><p id="p-0319" num="0311">The CPU <b>2310</b> may include one or more cores, cache memories, a bus, and/or a controller. The GPU <b>2320</b> may execute an operation related to graphic data processing, and the DSP <b>2330</b> may be an IP block processing a digital signal.</p><p id="p-0320" num="0312">The NPU <b>2340</b> may execute a computation based on a neural network, and may include the neuromorphic device according to any of the example embodiments. For example, the NPU <b>2340</b> may quantize real weights of each of the layers included in the neural network, and may store the quantized weights in weight arrays and may store zero point weights in a zero point array, respectively. Accordingly, by preventing zero point cells storing zero point weights of each of the layers included in the neural network from overlapping, integration density of the NPU <b>2340</b> may improve.</p><p id="p-0321" num="0313">Also, the NPU <b>2340</b> may execute a multiplication computation of the zero point weight and the input values in advance and may store the result thereof in the buffer as results of zero point computation, may select one of the results of zero point computation stored in the buffer according to the number of 1s included in the input values, and may sum the result and the result of weight computation. Accordingly, the computation burden and power consumption of the NPU <b>2340</b> may be reduced.</p><p id="p-0322" num="0314">In the NPU <b>2340</b>, a cell tile in which memory cells for storing weights are disposed and a reference tile in which reference cells for generating a reference current are disposed may be separately provided. The cell tile may output digital cell data, the reference tile may output digital reference data, and a comparator circuit may output the result of MAC computation by comparing the digital cell data with the digital reference data.</p><p id="p-0323" num="0315">According to some of the aforementioned example embodiments, weights corresponding to a plurality of layers included in the neural network may be stored in memory cells of each of the cell arrays, and a reference cell array may provide reference currents separately from the cell arrays for accurate computation while the inference operation is executed. By obtaining results of computation from the plurality of cell arrays using the reference currents output by a single reference cell array, integration density and power consumption of the neuromorphic device may improve.</p><p id="p-0324" num="0316">Also, the zero point weights and the quantized weights generated by quantizing the real weights may be stored in weight arrays and at least one zero point array, which are different cell arrays, in a divided manner Also, by configuring a plurality of weight arrays in which quantized weights are stored to share a single zero point array, integration density and power consumption of the neuromorphic device may improve, and computation burden may be reduced.</p><p id="p-0325" num="0317">Further, the neuromorphic device may not directly process the zero point computation using the zero point weight, and may receive the result of zero point computation from the external host and may execute the computation for implementing the neural network. Accordingly, integration density and power consumption of the neuromorphic device may increase, and computation errors may be reduced, thereby improving performance.</p><p id="p-0326" num="0318">While some example embodiments have been illustrated and described above, it will be apparent to those skilled in the art that modifications and variations could be made without departing from the scope of the present disclosure as defined by the appended claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A neuromorphic device, comprising:<claim-text>a plurality of cell tiles, each of the plurality of cell tiles including</claim-text><claim-text>a cell array including a plurality of memory cells configured to store weights of a neural network,</claim-text><claim-text>a row driver connected to the plurality of memory cells via a plurality of row lines, and</claim-text><claim-text>at least one cell analog-digital converter (ADC) connected to the plurality of memory cells via a plurality of column lines, the at least one cell ADC configured to convert cell currents read via the plurality of column lines into a plurality of pieces of digital cell data;</claim-text><claim-text>a reference tile including</claim-text><claim-text>a reference cell array including a plurality of reference cells,</claim-text><claim-text>a reference row driver connected to the plurality of reference cells via a plurality of reference row lines, and</claim-text><claim-text>at least one reference ADC connected to the plurality of reference cells via a plurality of reference column lines, the at least one reference ADC configured to convert reference currents read via the plurality of reference column lines into a plurality of pieces of digital reference data; and</claim-text><claim-text>at least one comparator circuit configured to compare the plurality of pieces of digital cell data with the plurality of pieces of digital reference data.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The neuromorphic device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the reference tile further includes:<claim-text>a buffer configured to store the plurality of pieces of digital reference data output by the at least one reference ADC.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The neuromorphic device of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein, the neuromorphic device is configured such that, when an inference using the neural network starts,<claim-text>the reference tile converts the reference currents into the plurality of pieces of digital reference data and stores the data in a buffer, and</claim-text><claim-text>the at least one comparator compares the plurality of pieces of digital cell data output by each of the plurality of cell tiles with the plurality of pieces of digital reference data stored in the buffer.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The neuromorphic device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the at least one comparator circuit includes a plurality of comparator circuits, each of the plurality of comparator circuits including a plurality of comparators, respectively, and the plurality of comparator circuits are connected to the plurality of cell tiles, respectively, and</claim-text><claim-text>an amount of the plurality of comparators included in the plurality of comparator circuits, respectively, is equal to an amount of the plurality of column lines included in the plurality of cell tiles.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The neuromorphic device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the at least one comparator circuit includes a plurality of comparators, and the plurality of comparators are connected to the plurality of cell tiles, respectively.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The neuromorphic device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the at least one comparator circuit includes a plurality of comparator circuits having a plurality of comparators, respectively,</claim-text><claim-text>each of the plurality of comparator circuits are connected to two or more of the plurality of cell tiles; and</claim-text><claim-text>the two or more cell tiles store weights included in a single layer among a plurality of layers included in the neural network.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The neuromorphic device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the comparator circuit includes a plurality of comparators, and each of the plurality of comparators are connected to two or more of the plurality of cell tiles.</claim-text></claim><claim id="CLM-08-10" num="08-10"><claim-text><b>8</b>.-<b>10</b>. (canceled)</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. A neuromorphic device, comprising:<claim-text>at least one weight array including a plurality of weight cells connected to a plurality of weight row lines and a plurality of weight column lines;</claim-text><claim-text>a zero point array including a plurality of zero point cells connected to a plurality of zero point row lines and a plurality of zero point column lines; and</claim-text><claim-text>a logic circuit configured to generate quantized weights and zero point weights by quantizing real weights included in each of a plurality of layers of a neural network, and to store the quantized weights in the plurality of weight cells and to store the zero point weights in the plurality of zero point cells,</claim-text><claim-text>wherein the weight row lines and the zero point row lines are separated from each other, and the weight column lines and the zero point column lines are separated from each other.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The neuromorphic device of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein a portion of the zero point cells sharing one of the plurality of zero point column lines store the same data.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The neuromorphic device of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein<claim-text>the at least one weight array includes a plurality of weight arrays separated from each other, and</claim-text><claim-text>an amount of the weight row lines included in each of the weight arrays is equal to an amount of the zero point row lines.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The neuromorphic device of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein<claim-text>each of the plurality of weight cells and each of the plurality of zero point cells include a switch device and a resistive memory device, and</claim-text><claim-text>a resistance value of the resistive memory device in each of the weight cells corresponds to one of the quantized weights, and a resistance value of the resistive memory device in each of the zero point cells corresponds to the zero point weight.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The neuromorphic device of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the resistive memory devices connected to a same one of the zero point column lines have the same resistance value.</claim-text></claim><claim id="CLM-16-31" num="16-31"><claim-text><b>16</b>.-<b>31</b>. (canceled)</claim-text></claim><claim id="CLM-00032" num="00032"><claim-text><b>32</b>. A neuromorphic device, comprising:<claim-text>a computational processor configured to, from among zero point weights and quantized weights obtained by quantizing real weights included in at least one of a plurality of layers of a neural network, receive the quantized weights, to compute input values input to the at least one of the plurality of layers with the quantized weights, and to output a result of weight computation,</claim-text><claim-text>a buffer configured to receive results of zero point computation obtained by computing the input values and the zero point weight from an external host and to store the result, and to output one of the results of zero point computation based on an amount of 1s included in the input values; and</claim-text><claim-text>an adder circuit configured to add the result of zero point computation output by the buffer to the result of weight computation and to output computation results of one of the plurality of layers.</claim-text></claim-text></claim><claim id="CLM-00033" num="00033"><claim-text><b>33</b>. The neuromorphic device of <claim-ref idref="CLM-00032">claim 32</claim-ref>, further comprising:<claim-text>a counter circuit configured to count the amount of 1s included in the input values and to transfer the counted amount of 1s to the buffer.</claim-text></claim-text></claim><claim id="CLM-00034" num="00034"><claim-text><b>34</b>. The neuromorphic device of <claim-ref idref="CLM-00032">claim 32</claim-ref>, further comprising:<claim-text>a memory configured to store the zero point weights corresponding to the plurality of layers,</claim-text><claim-text>wherein the external host receives one of the zero point weights from the memory and generates the results of zero point computation.</claim-text></claim-text></claim><claim id="CLM-00035" num="00035"><claim-text><b>35</b>. The neuromorphic device of <claim-ref idref="CLM-00032">claim 32</claim-ref>, wherein the computational processor is configured to receive the quantized weights together with the input values for computation on the at least one of the plurality of layers.</claim-text></claim><claim id="CLM-00036" num="00036"><claim-text><b>36</b>. The neuromorphic device of <claim-ref idref="CLM-00032">claim 32</claim-ref>, wherein the neuromorphic device is configured such that<claim-text>the real weights are distributed in a first range and a second range with reference to the zero point weight, and</claim-text><claim-text>the real weights are quantized differently in the first range and the second range.</claim-text></claim-text></claim><claim id="CLM-00037" num="00037"><claim-text><b>37</b>. The neuromorphic device of <claim-ref idref="CLM-00036">claim 36</claim-ref>, wherein the neuromorphic device is configured such that, when the first range is greater than the second range, the quantized weights are further allocated to the first range than the second range.</claim-text></claim><claim id="CLM-00038" num="00038"><claim-text><b>38</b>. The neuromorphic device of <claim-ref idref="CLM-00032">claim 32</claim-ref>, wherein<claim-text>the computational processor includes a plurality of multiply and accumulate (MAC) computation circuits, and</claim-text><claim-text>the plurality of MAC computation circuits is arranged in an array form.</claim-text></claim-text></claim><claim id="CLM-00039" num="00039"><claim-text><b>39</b>. The neuromorphic device of <claim-ref idref="CLM-00038">claim 38</claim-ref>, wherein each of multipliers included in the plurality of MAC computation circuits includes a plurality of AND gates, and a plurality of adder circuits.</claim-text></claim><claim id="CLM-00040" num="00040"><claim-text><b>40</b>. (canceled)</claim-text></claim></claims></us-patent-application>