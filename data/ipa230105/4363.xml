<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004364A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004364</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17940831</doc-number><date>20220908</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>8</main-group><subgroup>41</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>20</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>02</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>8</main-group><subgroup>427</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20190101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>20</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>02</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>8</main-group><subgroup>433</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">MATCHING GRAPHS GENERATED FROM SOURCE CODE</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>17009306</doc-number><date>20200901</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11455152</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17940831</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>X Development LLC</orgname><address><city>Mountain View</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Zhang</last-name><first-name>Qianyu</first-name><address><city>Sunnyvale</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Techniques are described herein for training a machine learning model and using the trained machine learning model to more accurately determine alignments between matching/corresponding nodes of predecessor and successor graphs representing predecessor and successor source code snippets. A method includes: obtaining a first abstract syntax tree that represents a predecessor source code snippet and a second abstract syntax tree that represents a successor source code snippet; determining a mapping across the first and second abstract syntax trees; obtaining a first control-flow graph that represents the predecessor source code snippet and a second control-flow graph that represents the successor source code snippet; aligning blocks in the first control-flow graph with blocks in the second control-flow graph; and applying the aligned blocks as inputs across a trained machine learning model to generate an alignment of nodes in the first abstract syntax tree with nodes in the second abstract syntax tree.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="106.34mm" wi="158.75mm" file="US20230004364A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="217.59mm" wi="145.20mm" file="US20230004364A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="229.36mm" wi="159.68mm" orientation="landscape" file="US20230004364A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="206.84mm" wi="165.78mm" orientation="landscape" file="US20230004364A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="202.86mm" wi="135.38mm" file="US20230004364A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="206.84mm" wi="135.38mm" file="US20230004364A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="157.73mm" wi="113.54mm" orientation="landscape" file="US20230004364A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">BACKGROUND</heading><p id="p-0002" num="0001">One way to identify changes made to a piece or &#x201c;snippet&#x201d; of source code is to generate graphs that represent the source code snippet before and after editing. These graphs may represent what will be referred to herein as &#x201c;predecessor&#x201d; and &#x201c;successor&#x201d; source code snippets. The predecessor source code snippet may be the source code snippet of interest prior to some changes being made to it, and may be represented by a first graph. The successor source code snippet may be the same source code snippet after the changes have been made, and may be represented by a second graph. In some cases, a change graph may be determined from the first and second graphs, and may represent the changes made to the source code snippet. Each graph may take various forms, such as an abstract syntax tree (AST), a control flow graph (CFG), etc. Heuristics exist to map matching nodes of the (predecessor) first graph to nodes of the second (successor) graph. However, these heuristics tend to be somewhat inaccurate, which in turn can cause downstream operations that rely on the mappings to be inaccurate as well.</p><heading id="h-0002" level="1">SUMMARY</heading><p id="p-0003" num="0002">Implementations are described herein for generating an alignment of nodes in a first graph representing a predecessor source code snippet and a second graph representing a successor source code snippet. In various implementations, a machine learning model such as a graph neural network may be trained to generate an alignment of nodes across abstract syntax trees using, as inputs, aligned blocks in control-flow graphs representing the predecessor and successor source code snippets. This alignment may be used for purposes such as generating a change graph that represents one or more edits made to the predecessor source code snippet to yield the successor source code snippet.</p><p id="p-0004" num="0003">In some implementations, the first control-flow graph and the second control-flow graph may be obtained using a parser to generate the first control-flow graph from the predecessor source code snippet and the second control-flow graph from the successor source code snippet, and the blocks in control-flow graphs may be aligned based on a mapping across the first abstract syntax tree and the second abstract syntax tree. The mapping may be generated using a tree-based code differencing algorithm.</p><p id="p-0005" num="0004">In another aspect, a synthetic training dataset may be generated using a first abstract syntax tree that represents a source code snippet. In various implementations, a second abstract syntax tree may be generated from the first abstract syntax tree. In some implementations, field values in various nodes in the second abstract syntax tree may be changed. One or more nodes may be deleted in the first abstract syntax tree and/or the second abstract syntax tree. Additionally, a parent node may be changed for one or more nodes in the second abstract syntax tree. In various implementations, a machine learning model may be trained to generate an alignment of nodes based on the first abstract syntax tree and the second abstract syntax tree.</p><p id="p-0006" num="0005">In various implementations, a method implemented by one or more processors may include: obtaining a first abstract syntax tree that represents a predecessor source code snippet and a second abstract syntax tree that represents a successor source code snippet; determining a mapping across the first abstract syntax tree and the second abstract syntax tree between pairs of matching nodes; obtaining a first control-flow graph that represents the predecessor source code snippet and a second control-flow graph that represents the successor source code snippet; aligning blocks in the first control-flow graph with blocks in the second control-flow graph based on the mapping across the first abstract syntax tree and the second abstract syntax tree; and applying the aligned blocks as inputs across a trained machine learning model to generate an alignment of nodes in the first abstract syntax tree with nodes in the second abstract syntax tree.</p><p id="p-0007" num="0006">In some implementations, the trained machine learning model may include a graph neural network. In some implementations, the determining the mapping across the first abstract syntax tree and the second abstract syntax tree may include using a tree-based code differencing algorithm. In some implementations, the obtaining the first control-flow graph and the second control-flow graph may include using a parser to generate the first control-flow graph from the predecessor source code snippet and the second control-flow graph from the successor source code snippet.</p><p id="p-0008" num="0007">In some implementations, the aligning the blocks in the first control-flow graph with the blocks in the second control-flow graph based on the mapping across the first abstract syntax tree and the second abstract syntax tree may include: determining a mapping across the first control-flow graph and the second control-flow graph between pairs of similar blocks identified using the mapping across the first abstract syntax tree and the second abstract syntax tree; and using the mapping across the first control-flow graph and the second control-flow graph to align the blocks in the first control-flow graph with the blocks in the second control-flow graph.</p><p id="p-0009" num="0008">In some implementations, in the applying the aligned blocks as inputs across the trained machine learning model to generate the alignment of nodes in the first abstract syntax tree with nodes in the second abstract syntax tree, candidate node alignments may be constrained based on nodes in the aligned blocks.</p><p id="p-0010" num="0009">In some implementations, a change graph may be generated based on the alignment of the nodes in the first abstract syntax tree with the nodes in the second abstract syntax tree. The change graph may represent one or more edits made to the predecessor source code snippet to yield the successor source code snippet.</p><p id="p-0011" num="0010">In some additional or alternative implementations, a computer program product may include one or more non-transitory computer-readable storage media having program instructions collectively stored on the one or more non-transitory computer-readable storage media. The program instructions may be executable to: obtain a first abstract syntax tree that represents a predecessor source code snippet and a second abstract syntax tree that represents a successor source code snippet; determine a mapping across the first abstract syntax tree and the second abstract syntax tree between pairs of matching nodes; obtain a first control-flow graph that represents the predecessor source code snippet and a second control-flow graph that represents the successor source code snippet; align blocks in the first control-flow graph with blocks in the second control-flow graph based on the mapping across the first abstract syntax tree and the second abstract syntax tree; and apply the aligned blocks as inputs across a trained machine learning model to generate an alignment of nodes in the first abstract syntax tree with nodes in the second abstract syntax tree.</p><p id="p-0012" num="0011">In some implementations, the program instructions may be further executable to generate a change graph based on the alignment of the nodes in the first abstract syntax tree with the nodes in the second abstract syntax tree. The change graph may represent one or more edits made to the predecessor source code snippet to yield the successor source code snippet.</p><p id="p-0013" num="0012">In some additional or alternative implementations, a system may include a processor, a computer-readable memory, one or more computer-readable storage media, and program instructions collectively stored on the one or more computer-readable storage media. The program instructions may be executable to: obtain a first abstract syntax tree that represents a predecessor source code snippet and a second abstract syntax tree that represents a successor source code snippet; determine a mapping across the first abstract syntax tree and the second abstract syntax tree between pairs of matching nodes; obtain a first control-flow graph that represents the predecessor source code snippet and a second control-flow graph that represents the successor source code snippet; align blocks in the first control-flow graph with blocks in the second control-flow graph based on the mapping across the first abstract syntax tree and the second abstract syntax tree; and apply the aligned blocks as inputs across a trained machine learning model to generate an alignment of nodes in the first abstract syntax tree with nodes in the second abstract syntax tree.</p><p id="p-0014" num="0013">It should be appreciated that all combinations of the foregoing concepts and additional concepts described in greater detail herein are contemplated as being part of the subject matter disclosed herein. For example, all combinations of claimed subject matter appearing at the end of this disclosure are contemplated as being part of the subject matter disclosed herein.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>1</b></figref> depicts an example environment in which selected aspects of the present disclosure may be implemented, in accordance with various implementations.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram showing an example of how one or more machine learning models trained using techniques described herein may be used to make inferences, in accordance with various implementations.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a block diagram showing an example of how a synthetic training dataset may be generated and used to train one or more machine learning models to make inferences, in accordance with various implementations.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>4</b></figref> depicts a flowchart illustrating an example method for using one or more machine learning models trained using techniques described herein to make inferences, in accordance with various implementations.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>5</b></figref> depicts a flowchart illustrating an example method for generating a synthetic training dataset and using the synthetic training dataset to train one or more machine learning models, in accordance with various implementations.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates an example architecture of a computing device.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0004" level="1">DETAILED DESCRIPTION</heading><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>1</b></figref> depicts an example environment <b>100</b> in which selected aspects of the present disclosure may be implemented, in accordance with various implementations. Any computing devices depicted in <figref idref="DRAWINGS">FIG. <b>1</b></figref> or elsewhere in the figures may include logic such as one or more microprocessors (e.g., central processing units or &#x201c;CPUs&#x201d;, graphical processing units or &#x201c;GPUs&#x201d;) that execute computer-readable instructions stored in memory, or other types of logic such as application-specific integrated circuits (&#x201c;ASIC&#x201d;), field-programmable gate arrays (&#x201c;FPGA&#x201d;), and so forth. Some of the systems depicted in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, such as a code knowledge system <b>102</b>, may be implemented using one or more server computing devices that form what is sometimes referred to as a &#x201c;cloud infrastructure,&#x201d; although this is not required.</p><p id="p-0022" num="0021">Code knowledge system <b>102</b> may be configured to perform selected aspects of the present disclosure in order to help one or more clients <b>110</b><sub>1-P </sub>make various inferences based on implemented and/or potential changes to the clients' respective source code bases <b>112</b><sub>1-P</sub>. For example, code knowledge system <b>102</b> may be configured to determine alignments of nodes between graphs representing predecessor source code snippets and graphs representing successor source code snippets associated with source code bases <b>112</b><sub>1-P </sub>of clients <b>110</b><sub>1-P</sub>.</p><p id="p-0023" num="0022">These alignments may then be used for a variety of different purposes, such as to generate a change graph for use as input to other downstream source code predictions, such as to predict code change intents (e.g., change log entries), comments to be embedded into source code, identification of coding mistakes, etc. These change graphs can also be used in other contexts to train other types of machine learning models. For example, a machine learning model such as a graph neural network (GNN) may be trained using change graphs generated as described herein to predict code changes, e.g., during a large-scale source code update and/or migration.</p><p id="p-0024" num="0023">Each client <b>110</b> may be, for example, an entity or organization such as a business (e.g., financial institute, bank, etc.), non-profit, club, university, government agency, or any other organization that operates one or more software systems. For example, a bank may operate one or more software systems to manage the money under its control, including tracking deposits and withdrawals, tracking loans, tracking investments, and so forth. An airline may operate one or more software systems for booking/canceling/rebooking flight reservations, managing delays or cancelations of flight, managing people associated with flights, such as passengers, air crews, and ground crews, managing airport gates, and so forth.</p><p id="p-0025" num="0024">Many of these entities' code bases <b>112</b> may be highly complex, requiring teams of programmers and/or software engineers to perform code base migrations, maintenance, and/or updates. Many of these personnel may be under considerable pressure, and may place low priority on tasks that might be considered &#x201c;menial&#x201d; or expendable, such as composing descriptive and/or helpful code change intents, in embedded comments or as part of change list entries. Moreover, a mass code update or migration may require myriad small changes to source code at numerous locations, further challenging these personnel.</p><p id="p-0026" num="0025">Accordingly, code knowledge system <b>102</b> may be configured to leverage knowledge of past changes made to source code, such as during code base migration, update, or maintenance events, in order to automate tasks such as composition and/or summarization of code change intents and/or comments, to predict code changes, etc. Many of these tasks may rely on the ability to accurately and quickly identify changes made to source code. Although it is possible to perform text comparisons to determine textual changes between different versions of source code, these textual changes may not convey structure relationships embodied in the source code, e.g., between different logical branches, statements, variables, etc.</p><p id="p-0027" num="0026">Source code&#x2014;and changes to source code&#x2014;can also be represented in graph form. For example, source code may be converted into an abstract syntax tree (AST) and/or control flow graph (CFG), either of which may maintain not only the syntax of the code, but also the underlying structure. A change graph can be generated based on graphs representing a source code snippet before (predecessor) and after (successor) the source code snippet is changed.</p><p id="p-0028" num="0027">Conventional heuristics for determining an alignment between matching/corresponding nodes of the predecessor and successor graphs in order to generate a change graph may have limited accuracy. Many of the code changes may be minor, may be relatively hard to discern in similar contexts, and/or may be incomplete and/or semantically incorrect. Accordingly, code knowledge system <b>102</b> is configured with selected aspects of the present disclosure to leverage machine learning to more accurately determine alignments between matching/corresponding nodes of predecessor and successor graphs representing predecessor and successor source code snippets.</p><p id="p-0029" num="0028">As noted above, alignments between matching nodes in general, and change graphs generated therefrom in particular, may have a variety of uses. As one example, with change graphs generated using techniques described herein, a machine learning model such as a GNN may be trained to predict code changes, e.g., to automate at least part of a widespread source code update and/or migration. As another example, change graphs generated using techniques described herein may be processed using a machine learning model such as a GNN to automatically predict and/or compose code change intents. Code change intents may be embodied in various forms, such as in change list entries that are sometimes required when an updated source code snippet is committed (e.g., installed, stored, incorporated) into a code base, in comments (e.g., delimited with symbols such as &#x201c;//&#x201d; or &#x201c;#&#x201d;) embedded in the source code, in change logs, or anywhere else where human-composed language indicating an intent behind a source code change might be found.</p><p id="p-0030" num="0029">In either case (predicting code changes or the intents behind them), labeled pairs of predecessor/successor source code snippets may be used to generate corresponding pairs of graphs (e.g., ASTs, CFGs). These graph pairs may be processed with a machine learning model such as a GNN to generate an embedding in vector space. Techniques such as triplet loss may then be used to train the machine learning model based on the embedding's relative proximity in the latent space to other embeddings having similar and dissimilar labels. Labels used for code change prediction and labels used for code change intent prediction may or may not be similar, identical, or entirely different from each other.</p><p id="p-0031" num="0030">Subsequently, to predict a code change, a source code snippet to-be-updated may be converted into graph form and embedded into the vector space using the trained machine learning model. Various nearest neighbor search algorithms may then be used to identify proximate embeddings that represent previous code changes made during previous migrations. These previous code changes may be considered as candidate edits for the source code snippet to-be-updated. Similarly, to predict a code change intent, predecessor and successor source code snippets may be converted into graph form and embedded into the same vector space or a different vector space using the trained machine model. Various nearest neighbor search algorithms may then be used to identify proximate embeddings that represent previous code changes made during previous migrations, as well as code change intents behind those changes.</p><p id="p-0032" num="0031">In various implementations, code knowledge system <b>102</b> may include a machine learning (&#x201c;ML&#x201d; in <figref idref="DRAWINGS">FIG. <b>1</b></figref>) database <b>104</b> that includes data indicative of one or more trained machine learning models <b>106</b><sub>1-N</sub>. These machine learning models <b>106</b><sub>1-N </sub>may take various forms that will be described in more detail below, including but not limited to a GNN, a graph matching network (GMN), a sequence-to-sequence model such as various flavors of a recurrent neural network (e.g., long short-term memory, or &#x201c;LSTM&#x201d;, gate recurrent units, or &#x201c;GRU&#x201d;, etc.) or an encoder-decoder, and any other type of machine learning model that may be applied to facilitate selected aspects of the present disclosure. In some implementations, code knowledge system <b>102</b> may also have access to one or more code base(s) <b>108</b>. In some implementations, the code bases <b>108</b> may be used, for instance, to train one or more of the machine learning models <b>106</b><sub>1-N</sub>.</p><p id="p-0033" num="0032">In various implementations, a client <b>110</b> that wishes to take advantage of techniques described herein to, for example, predict and/or implement code changes and/or code change intents when migrating, updating, or even maintaining its code base <b>112</b> may establish a relationship with an entity (not depicted in <figref idref="DRAWINGS">FIG. <b>1</b></figref>) that hosts code knowledge system <b>102</b>. In some implementations, code knowledge system <b>102</b> may then process all or parts of the client's source code base <b>112</b>, e.g., by interfacing with the client's software development version control system (not depicted) over one or more networks <b>114</b> such as the Internet. Based on this processing, code knowledge system <b>102</b> may perform various techniques described herein for predicting and/or utilizing code changes and/or the intents behind them. In other implementations, e.g., where the client's code base <b>112</b> is massive, one or more representatives of the entity that hosts code knowledge system <b>102</b> may travel to the client's site(s) to perform updates and/or make recommendations.</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram showing an example process flow <b>200</b> in which code knowledge system <b>102</b> may use one or more machine learning models <b>106</b><sub>1-N </sub>trained using techniques described herein to make inferences, in accordance with various implementations. Various components depicted in <figref idref="DRAWINGS">FIG. <b>2</b></figref> may be implemented by code knowledge system <b>102</b> or separately from code knowledge system <b>102</b>. These components may be implemented using any combination of hardware and computer-readable instructions.</p><p id="p-0035" num="0034">Beginning at the top left, in implementations, a predecessor source code snippet <b>205</b> and a successor source code snippet <b>210</b> may be processed by a &#x201c;code-to-AST&#x201d; component <b>215</b> to generate, respectively, first AST <b>220</b> and second AST <b>225</b>. In other implementations, source code snippets <b>205</b>, <b>210</b> may be converted into other types of graphs.</p><p id="p-0036" num="0035">In implementations, the first AST <b>220</b> and the second AST <b>225</b> may be processed by a mapping module <b>230</b> to generate an AST mapping <b>235</b>. The mapping module <b>230</b> may generate, as the AST mapping <b>235</b>, a mapping across the first AST <b>220</b> and the second AST <b>225</b> using, e.g., a tree-based code differencing algorithm. In implementations, the AST mapping <b>235</b> may be a mapping between pairs of similar nodes in the first AST <b>220</b> and the second AST <b>225</b>.</p><p id="p-0037" num="0036">In implementations, the predecessor source code snippet <b>205</b> and the successor source code snippet <b>210</b> may also be processed by a &#x201c;code-to-CFG&#x201d; component <b>240</b> to generate, respectively, first CFG <b>245</b> and second CFG <b>250</b>. In implementations, the code-to-CFG component <b>240</b> may use a parser to generate the first CFG <b>245</b> from the predecessor source code snippet <b>205</b> and the second CFG <b>250</b> from the successor source code snippet <b>210</b>.</p><p id="p-0038" num="0037">In implementations, the first CFG <b>245</b> and the second CFG <b>250</b> may be processed by a block alignment module <b>255</b> to generate aligned blocks <b>260</b>. The block alignment module <b>255</b> may align blocks in the first CFG <b>245</b> with blocks in the second CFG <b>250</b> based on the AST mapping <b>235</b>. In implementations, the block alignment module <b>255</b> may determine a mapping across the first CFG <b>245</b> and the second CFG <b>250</b> between pairs of similar blocks identified using the AST mapping <b>235</b> across the first AST <b>220</b> and the second AST <b>225</b>. The block alignment module <b>255</b> may then use the mapping across the first CFG <b>245</b> and the second CFG <b>250</b> to generate aligned blocks <b>260</b> in which the blocks in the first CFG <b>245</b> are aligned with the blocks in the second CFG <b>250</b>.</p><p id="p-0039" num="0038">In implementations, the first AST <b>220</b>, the second AST <b>225</b>, and the aligned blocks <b>260</b> may be applied by an alignment module <b>265</b> as inputs across a machine learning model <b>106</b><sub>1-N </sub>(e.g., a GNN) to generate a predicted node alignment <b>270</b> between the first AST <b>220</b> and the second AST <b>225</b>. In implementations, the alignment module <b>265</b> may constrain candidate node alignments based on nodes in the aligned blocks <b>260</b>. In this manner, the predicted node alignment <b>270</b> may be generated in a hierarchical manner, block by block. For example, the alignment module <b>265</b> may use the machine learning model <b>106</b><sub>1-N </sub>to generate a plurality of node similarity measures between individual nodes of the first AST <b>220</b> and nodes of the second AST <b>225</b> that are within aligned blocks and then generate the predicted node alignment <b>270</b> based on the node similarity measures. This may be true where, for instance, the machine learning model <b>106</b><sub>1-N </sub>is a GNN. With a GNN, each node similarity measure of the plurality of node similarity measures may be based on a cross-graph attention mechanism (e.g., an attention layer) employed by the GNN. The cross-graph attention mechanism (e.g., attention layer) may provide an attention weight (also referred to as an &#x201c;attention coefficient&#x201d;) for each possible pair of nodes that includes a node from the first AST <b>220</b> and a node from the second AST <b>225</b>. Thus, in some implementations, the machine learning model <b>106</b><sub>1-N </sub>takes the form of a cross-graph attention mechanism employed as part of a GNN.</p><p id="p-0040" num="0039">The predicted node alignment <b>270</b> may be provided to various downstream module(s) <b>275</b> for additional processing. For example, one downstream module <b>275</b> may generate a change graph <b>280</b>. As mentioned previously, change graph <b>280</b> may be used for a variety of purposes. For example, a prediction module (not shown) may be configured to process the change graph <b>280</b>, e.g., using a machine learning model <b>106</b><sub>1-N </sub>such as a GNN to make a prediction (not shown). These predictions may include, for instance, predicted code changes, predicted code change intents, etc.</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a block diagram showing an example process flow <b>300</b> in which code knowledge system <b>102</b> may generate a synthetic training dataset and use the synthetic training dataset to train one or more machine learning models <b>106</b><sub>1-N </sub>to make inferences, in accordance with various implementations. Various components depicted in <figref idref="DRAWINGS">FIG. <b>3</b></figref> may be implemented by code knowledge system <b>102</b> or separately from code knowledge system <b>102</b>. These components may be implemented using any combination of hardware and computer-readable instructions.</p><p id="p-0042" num="0041">Beginning at the top left, in implementations, a source code snippet <b>305</b> may be processed by a &#x201c;code-to-AST&#x201d; component <b>215</b> to generate first AST <b>310</b>. In implementations, the code-to-AST component <b>215</b> may also generate second AST <b>315</b>, e.g., from the first AST <b>310</b>. The second AST <b>315</b> may be a copy of the first AST <b>310</b>. In other implementations, source code snippet <b>305</b> may be converted into other types of graphs.</p><p id="p-0043" num="0042">In implementations, the second AST <b>315</b> may be processed by a modifier module <b>320</b> to generate, as a synthetic training dataset in conjunction with the first AST <b>310</b>, a modified second AST <b>325</b> and a node alignment <b>330</b>. The modifier module <b>320</b> may generate the modified second AST <b>325</b> by changing a field value of each node of a first set of nodes in the second AST <b>315</b>, deleting each node in a second set of nodes in the second AST <b>315</b>, and/or changing a parent node of each node in a third set of nodes in the second AST <b>315</b>. Additionally, in implementations, the modifier module <b>320</b> may delete at least one node in the first AST <b>315</b>. The node alignment <b>330</b> generated by the modifier module <b>320</b> may indicate a mapping across the first AST <b>310</b> and the modified second AST <b>325</b> between pairs of similar nodes (e.g., based on the modifications made by the modifier module <b>320</b>). In implementations, the first AST <b>310</b>, the modified second AST <b>325</b>, and the node alignment <b>330</b> may be provided to a training module <b>335</b>, which will be described shortly.</p><p id="p-0044" num="0043">In implementations, the first AST <b>310</b> and the modified second AST <b>325</b> may be applied by an alignment module <b>265</b> as inputs across a machine learning model <b>106</b><sub>1-N </sub>(e.g., a GNN). Additionally, in implementations, aligned blocks (not shown), e.g., generated in the manner described with respect to the aligned blocks <b>260</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, may also be applied by the alignment module <b>265</b> as inputs across the machine learning model <b>106</b><sub>1-N</sub>. The alignment module <b>265</b> may then use the machine learning model <b>106</b><sub>1-N </sub>to generate a predicted node alignment <b>340</b> between the first AST <b>310</b> and the modified second AST <b>325</b>, e.g., as described with respect to <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0045" num="0044">In implementations, the predicted node alignment <b>340</b> may be provided to the training module <b>335</b>. Training module <b>335</b> may then perform a comparison of the predicted node alignment <b>340</b> and the node alignment <b>330</b> (e.g., a ground truth node alignment) and may train the machine learning model <b>106</b><sub>1-N </sub>based on the comparison.</p><p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flowchart illustrating an example method <b>400</b> of using one or more machine learning models <b>106</b><sub>1-N </sub>trained using techniques described herein to make inferences, in accordance with implementations disclosed herein. For convenience, the operations of the flowchart are described with reference to a system that performs the operations. This system may include various components of various computer systems, such as one or more components of the code knowledge system <b>102</b>. Moreover, while operations of method <b>400</b> are shown in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted, or added.</p><p id="p-0047" num="0046">At block <b>410</b>, the system may obtain a first abstract syntax tree that represents a predecessor source code snippet and a second abstract syntax tree that represents a successor source code snippet. In implementations, at block <b>410</b>, the code knowledge system <b>102</b> may obtain the first abstract syntax tree (e.g., <b>220</b>) that represents the predecessor source code snippet (e.g., <b>205</b>) and the second abstract syntax tree (e.g., <b>225</b>) that represents the successor source code snippet (e.g., <b>210</b>).</p><p id="p-0048" num="0047">Still referring to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, at block <b>420</b>, the system may determine a mapping across the first abstract syntax tree and the second abstract syntax tree between pairs of matching nodes. In implementations, at block <b>420</b>, the code knowledge system <b>102</b> may determine the mapping (e.g., <b>235</b>) across the first abstract syntax tree and the second abstract syntax tree between pairs of matching nodes. In implementations, the code knowledge system <b>102</b> may use a tree-based code differencing algorithm to determine the mapping.</p><p id="p-0049" num="0048">Still referring to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, at block <b>430</b>, the system may obtain a first control-flow graph that represents the predecessor source code snippet and a second control-flow graph that represents the successor source code snippet. In implementations, at block <b>430</b>, the code knowledge system <b>102</b> may obtain the first control-flow graph (e.g., <b>245</b>) that represents the predecessor source code snippet and the second control-flow graph (e.g., <b>250</b>) that represents the successor source code snippet. In implementations, the code knowledge system <b>102</b> may use a parser to generate the first control-flow graph from the predecessor source code snippet and the second control-flow graph from the successor source code snippet.</p><p id="p-0050" num="0049">Still referring to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, at block <b>440</b>, the system may align blocks in the first control-flow graph with blocks in the second control-flow graph based on the mapping across the first abstract syntax tree and the second abstract syntax tree. In implementations, at block <b>440</b>, the code knowledge system <b>102</b> may align blocks in the first control-flow graph with blocks in the second control-flow graph based on the mapping across the first abstract syntax tree and the second abstract syntax tree to generate aligned blocks (e.g., <b>260</b>). In implementations, the code knowledge system <b>102</b> may determine a mapping across the first control-flow graph and the second control-flow graph between pairs of similar blocks identified using the mapping across the first abstract syntax tree and the second abstract syntax tree and then use the mapping across the first control-flow graph and the second control-flow graph to align the blocks in the first control-flow graph with the blocks in the second control-flow graph.</p><p id="p-0051" num="0050">Still referring to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, at block <b>450</b>, the system may apply the aligned blocks as inputs across a trained machine learning model to generate an alignment of nodes in the first abstract syntax tree with nodes in the second abstract syntax tree. In implementations, at block <b>450</b>, the code knowledge system <b>102</b> may apply the aligned blocks as inputs across the trained machine learning model (e.g., <b>106</b><sub>1-N</sub>) to generate an alignment of nodes (e.g., <b>270</b>) in the first abstract syntax tree with nodes in the second abstract syntax tree. In implementations, the machine learning model <b>106</b><sub>1-N </sub>may be a GNN. In other implementations, the machine learning model <b>106</b><sub>1-N </sub>may be a GMN or any other type of machine learning model. In implementations, the code knowledge system <b>102</b> may constrain candidate node alignments based on nodes in the aligned blocks.</p><p id="p-0052" num="0051">Still referring to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, at block <b>460</b>, the system may generate a change graph based on the alignment of the nodes in the first abstract syntax tree with the nodes in the second abstract syntax tree. In implementations, at block <b>460</b>, the code knowledge system <b>102</b> may generate the change graph (e.g., <b>280</b>) based on the alignment of the nodes in the first abstract syntax tree with the nodes in the second abstract syntax tree. The change graph may represent one or more edits made to the predecessor source code snippet to yield the successor source code snippet.</p><p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flowchart illustrating an example method <b>500</b> of generating a synthetic training dataset and using the synthetic training dataset to train one or more machine learning models <b>106</b><sub>1-N </sub>to make inferences, in accordance with implementations disclosed herein. For convenience, the operations of the flowchart are described with reference to a system that performs the operations. This system may include various components of various computer systems, such as one or more components of the code knowledge system <b>102</b>. Moreover, while operations of method <b>500</b> are shown in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted, or added.</p><p id="p-0054" num="0053">At block <b>510</b>, the system may obtain a first abstract syntax tree that represents a source code snippet. In implementations, at block <b>510</b>, the code knowledge system <b>102</b> may obtain the first abstract syntax tree (e.g., <b>310</b>) that represents a source code snippet (e.g., <b>305</b>).</p><p id="p-0055" num="0054">Still referring to <figref idref="DRAWINGS">FIG. <b>5</b></figref>, at block <b>520</b>, the system may generate a second abstract syntax tree from the first abstract syntax tree. In implementations, at block <b>520</b>, the code knowledge system <b>102</b> may generate the second abstract syntax tree (e.g., <b>315</b>) from the first abstract syntax tree.</p><p id="p-0056" num="0055">Still referring to <figref idref="DRAWINGS">FIG. <b>5</b></figref>, at block <b>530</b>, for each node in a first set of nodes in the second abstract syntax tree, the system may change a field value of the node. In implementations, at block <b>530</b>, the code knowledge system <b>102</b> may, for each node in the first set of nodes in the second abstract syntax tree, change a field value of the node to generate a modified second abstract syntax tree (e.g., <b>325</b>).</p><p id="p-0057" num="0056">Still referring to <figref idref="DRAWINGS">FIG. <b>5</b></figref>, at block <b>540</b>, the system may delete each node in a second set of nodes in the second abstract syntax tree. In implementations, at block <b>540</b>, the code knowledge system <b>102</b> may delete each node in the second set of nodes in the second abstract syntax tree to generate a modified second abstract syntax tree. In other implementations, at block <b>540</b>, the code knowledge system <b>102</b> may delete each node in the second set of nodes in the modified second abstract syntax tree to further modify the modified second abstract syntax tree.</p><p id="p-0058" num="0057">Still referring to <figref idref="DRAWINGS">FIG. <b>5</b></figref>, at block <b>550</b>, for each node in a third set of nodes in the second abstract syntax tree, the system may change a parent node of the node. In implementations, at block <b>550</b>, the code knowledge system <b>102</b> may, for each node in the third set of nodes in the second abstract syntax tree, change a parent node of the node to generate a modified second abstract syntax tree. In other implementations, at block <b>550</b>, the code knowledge system <b>102</b> may, for each node in the third set of nodes in the modified second abstract syntax tree, change a parent node of the node to further modify the modified second abstract syntax tree.</p><p id="p-0059" num="0058">Still referring to <figref idref="DRAWINGS">FIG. <b>5</b></figref>, at block <b>560</b>, the system may delete at least one node in the first abstract syntax tree. In implementations, at block <b>560</b>, the code knowledge system <b>102</b> may delete at least one node in the first abstract syntax tree.</p><p id="p-0060" num="0059">Still referring to <figref idref="DRAWINGS">FIG. <b>5</b></figref>, at block <b>570</b>, the system may train a machine learning model to generate an alignment of nodes based on the first abstract syntax tree and the second abstract syntax tree. In implementations, at block <b>570</b>, the code knowledge system <b>102</b> may train a machine learning model (e.g., <b>106</b><sub>1-N</sub>) to generate an alignment of nodes (e.g., <b>340</b>) based on the first abstract syntax tree and the modified second abstract syntax tree. In implementations, the machine learning model <b>106</b><sub>1-N </sub>may be a GNN. In other implementations, the machine learning model <b>106</b><sub>1-N </sub>may be a GMN or any other type of machine learning model.</p><p id="p-0061" num="0060"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a block diagram of an example computing device <b>610</b> that may optionally be utilized to perform one or more aspects of techniques described herein. Computing device <b>610</b> typically includes at least one processor <b>614</b> which communicates with a number of peripheral devices via bus subsystem <b>612</b>. These peripheral devices may include a storage subsystem <b>624</b>, including, for example, a memory subsystem <b>625</b> and a file storage subsystem <b>626</b>, user interface output devices <b>620</b>, user interface input devices <b>622</b>, and a network interface subsystem <b>616</b>. The input and output devices allow user interaction with computing device <b>610</b>. Network interface subsystem <b>616</b> provides an interface to outside networks and is coupled to corresponding interface devices in other computing devices.</p><p id="p-0062" num="0061">User interface input devices <b>622</b> may include a keyboard, pointing devices such as a mouse, trackball, touchpad, or graphics tablet, a scanner, a touchscreen incorporated into the display, audio input devices such as voice recognition systems, microphones, and/or other types of input devices. In general, use of the term &#x201c;input device&#x201d; is intended to include all possible types of devices and ways to input information into computing device <b>610</b> or onto a communication network.</p><p id="p-0063" num="0062">User interface output devices <b>620</b> may include a display subsystem, a printer, a fax machine, or non-visual displays such as audio output devices. The display subsystem may include a cathode ray tube (CRT), a flat-panel device such as a liquid crystal display (LCD), a projection device, or some other mechanism for creating a visible image. The display subsystem may also provide non-visual display such as via audio output devices. In general, use of the term &#x201c;output device&#x201d; is intended to include all possible types of devices and ways to output information from computing device <b>610</b> to the user or to another machine or computing device.</p><p id="p-0064" num="0063">Storage subsystem <b>624</b> stores programming and data constructs that provide the functionality of some or all of the modules described herein. For example, the storage subsystem <b>624</b> may include the logic to perform selected aspects of the process flows of <figref idref="DRAWINGS">FIGS. <b>2</b> and <b>3</b></figref> and the methods of <figref idref="DRAWINGS">FIGS. <b>4</b> and <b>5</b></figref>, as well as to implement various components depicted in <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0065" num="0064">These software modules are generally executed by processor <b>614</b> alone or in combination with other processors. The memory subsystem <b>625</b> included in the storage subsystem <b>624</b> can include a number of memories including a main random access memory (RAM) <b>630</b> for storage of instructions and data during program execution and a read only memory (ROM) <b>632</b> in which fixed instructions are stored. A file storage subsystem <b>626</b> can provide persistent storage for program and data files, and may include a hard disk drive, a floppy disk drive along with associated removable media, a CD-ROM drive, an optical drive, or removable media cartridges. The modules implementing the functionality of certain implementations may be stored by file storage subsystem <b>626</b> in the storage subsystem <b>624</b>, or in other machines accessible by the processor(s) <b>614</b>.</p><p id="p-0066" num="0065">Bus subsystem <b>612</b> provides a mechanism for letting the various components and subsystems of computing device <b>610</b> communicate with each other as intended. Although bus subsystem <b>612</b> is shown schematically as a single bus, alternative implementations of the bus subsystem may use multiple busses.</p><p id="p-0067" num="0066">Computing device <b>610</b> can be of varying types including a workstation, server, computing cluster, blade server, server farm, or any other data processing system or computing device. Due to the ever-changing nature of computers and networks, the description of computing device <b>610</b> depicted in <figref idref="DRAWINGS">FIG. <b>6</b></figref> is intended only as a specific example for purposes of illustrating some implementations. Many other configurations of computing device <b>610</b> are possible having more or fewer components than the computing device depicted in <figref idref="DRAWINGS">FIG. <b>6</b></figref>.</p><p id="p-0068" num="0067">In other implementations, a method implemented by one or more processors may include: obtaining a first abstract syntax tree that represents a source code snippet; generating a second abstract syntax tree from the first abstract syntax tree; for each node in a first set of nodes in the second abstract syntax tree, changing a field value of the node; deleting each node in a second set of nodes in the second abstract syntax tree; and for each node in a third set of nodes in the second abstract syntax tree, changing a parent node of the node.</p><p id="p-0069" num="0068">In some implementations, the method may further include deleting at least one node in the first abstract syntax tree. In some implementations, the method may further include training a machine learning model to generate an alignment of nodes based on the first abstract syntax tree and the second abstract syntax tree. In some implementations, the training the machine learning model may include: generating a predicted node alignment between the first abstract syntax tree and the second abstract syntax tree; comparing the predicted node alignment and a ground truth node alignment; and training the machine learning model based on the comparing.</p><p id="p-0070" num="0069">In other implementations, a computer program product may include one or more non-transitory computer-readable storage media having program instructions collectively stored on the one or more non-transitory computer-readable storage media. The program instructions may be executable to: obtain a first abstract syntax tree that represents a source code snippet; generate a second abstract syntax tree from the first abstract syntax tree; for each node in a first set of nodes in the second abstract syntax tree, change a field value of the node; delete each node in a second set of nodes in the second abstract syntax tree; and for each node in a third set of nodes in the second abstract syntax tree, change a parent node of the node.</p><p id="p-0071" num="0070">In some implementations, the program instructions may be further executable to delete at least one node in the first abstract syntax tree. In some implementations, the program instructions may be further executable to train a machine learning model to generate an alignment of nodes based on the first abstract syntax tree and the second abstract syntax tree. In some implementations, the training the machine learning model may include: generating a predicted node alignment between the first abstract syntax tree and the second abstract syntax tree; comparing the predicted node alignment and a ground truth node alignment; and training the machine learning model based on the comparing.</p><p id="p-0072" num="0071">In other implementations, a system may include: a processor, a computer-readable memory, one or more computer-readable storage media, and program instructions collectively stored on the one or more computer-readable storage media. The program instructions may be executable to: obtain a first abstract syntax tree that represents a source code snippet; generate a second abstract syntax tree from the first abstract syntax tree; for each node in a first set of nodes in the second abstract syntax tree, change a field value of the node; delete each node in a second set of nodes in the second abstract syntax tree; and for each node in a third set of nodes in the second abstract syntax tree, change a parent node of the node.</p><p id="p-0073" num="0072">In some implementations, the program instructions may be further executable to delete at least one node in the first abstract syntax tree. In some implementations, the program instructions may be further executable to train a machine learning model to generate an alignment of nodes based on the first abstract syntax tree and the second abstract syntax tree. In some implementations, the training the machine learning model may include: generating a predicted node alignment between the first abstract syntax tree and the second abstract syntax tree; comparing the predicted node alignment and a ground truth node alignment; and training the machine learning model based on the comparing.</p><p id="p-0074" num="0073">Implementations may address problems with the limited accuracy of conventional heuristics for determining an alignment between matching/corresponding nodes of predecessor and successor graphs in order to generate a change graph. In particular, some implementations may improve the functioning of a computer by providing methods and systems for training a machine learning model and using the trained machine learning model to more accurately determine alignments between matching/corresponding nodes of predecessor and successor graphs representing predecessor and successor source code snippets. Accordingly, through the use of rules that improve computer-related technology, implementations allow computer performance of functions not previously performable by a computer. Additionally, implementations use techniques that are, by definition, rooted in computer technology (e.g., machine learning models, GNNs, GMNs, etc.).</p><p id="p-0075" num="0074">While several implementations have been described and illustrated herein, a variety of other means and/or structures for performing the function and/or obtaining the results and/or one or more of the advantages described herein may be utilized, and each of such variations and/or modifications is deemed to be within the scope of the implementations described herein. More generally, all parameters, dimensions, materials, and configurations described herein are meant to be exemplary and that the actual parameters, dimensions, materials, and/or configurations will depend upon the specific application or applications for which the teachings is/are used. Those skilled in the art will recognize, or be able to ascertain using no more than routine experimentation, many equivalents to the specific implementations described herein. It is, therefore, to be understood that the foregoing implementations are presented by way of example only and that, within the scope of the appended claims and equivalents thereto, implementations may be practiced otherwise than as specifically described and claimed. Implementations of the present disclosure are directed to each individual feature, system, article, material, kit, and/or method described herein. In addition, any combination of two or more such features, systems, articles, materials, kits, and/or methods, if such features, systems, articles, materials, kits, and/or methods are not mutually inconsistent, is included within the scope of the present disclosure.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method implemented by one or more processors, the method comprising:<claim-text>obtaining a first abstract syntax tree that represents a source code snippet;</claim-text><claim-text>generating a second abstract syntax tree from the first abstract syntax tree;</claim-text><claim-text>for each node in a first set of nodes in the second abstract syntax tree, changing a field value of the node;</claim-text><claim-text>deleting each node in a second set of nodes in the second abstract syntax tree; and</claim-text><claim-text>for each node in a third set of nodes in the second abstract syntax tree, changing a parent node of the node.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising deleting at least one node in the first abstract syntax tree.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising training a machine learning model to generate an alignment of nodes based on the first abstract syntax tree and the second abstract syntax tree.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the machine learning model comprises a graph neural network.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, further comprising:<claim-text>obtaining a third abstract syntax tree that represents a predecessor source code snippet and a fourth abstract syntax tree that represents a successor source code snippet;</claim-text><claim-text>determining a mapping across the third abstract syntax tree and the fourth abstract syntax tree between pairs of matching nodes;</claim-text><claim-text>obtaining a first control-flow graph that represents the predecessor source code snippet and a second control-flow graph that represents the successor source code snippet;</claim-text><claim-text>aligning blocks in the first control-flow graph with blocks in the second control-flow graph based on the mapping across the third abstract syntax tree and the fourth abstract syntax tree; and</claim-text><claim-text>applying the aligned blocks as inputs across the machine learning model to generate an alignment of nodes in the third abstract syntax tree with nodes in the fourth abstract syntax tree.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, further comprising generating a change graph based on the alignment of the nodes in the third abstract syntax tree with the nodes in the fourth abstract syntax tree, wherein the change graph represents one or more edits made to the predecessor source code snippet to yield the successor source code snippet.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. A computer program product comprising one or more non-transitory computer-readable storage media having program instructions collectively stored on the one or more non-transitory computer-readable storage media, the program instructions executable to:<claim-text>obtain a first abstract syntax tree that represents a source code snippet;</claim-text><claim-text>generate a second abstract syntax tree from the first abstract syntax tree;</claim-text><claim-text>for each node in a first set of nodes in the second abstract syntax tree, change a field value of the node;</claim-text><claim-text>delete each node in a second set of nodes in the second abstract syntax tree; and</claim-text><claim-text>for each node in a third set of nodes in the second abstract syntax tree, change a parent node of the node.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The computer program product according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the program instructions are further executable to delete at least one node in the first abstract syntax tree.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The computer program product according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the program instructions are further executable to train a machine learning model to generate an alignment of nodes based on the first abstract syntax tree and the second abstract syntax tree.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The computer program product according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the machine learning model comprises a graph neural network.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The computer program product according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the program instructions are further executable to:<claim-text>obtain a third abstract syntax tree that represents a predecessor source code snippet and a fourth abstract syntax tree that represents a successor source code snippet;</claim-text><claim-text>determine a mapping across the third abstract syntax tree and the fourth abstract syntax tree between pairs of matching nodes;</claim-text><claim-text>obtain a first control-flow graph that represents the predecessor source code snippet and a second control-flow graph that represents the successor source code snippet;</claim-text><claim-text>align blocks in the first control-flow graph with blocks in the second control-flow graph based on the mapping across the third abstract syntax tree and the fourth abstract syntax tree; and</claim-text><claim-text>apply the aligned blocks as inputs across the machine learning model to generate an alignment of nodes in the third abstract syntax tree with nodes in the fourth abstract syntax tree.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The computer program product according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the program instructions are further executable to generate a change graph based on the alignment of the nodes in the third abstract syntax tree with the nodes in the fourth abstract syntax tree, wherein the change graph represents one or more edits made to the predecessor source code snippet to yield the successor source code snippet.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. A system comprising:<claim-text>a processor, a computer-readable memory, one or more computer-readable storage media, and program instructions collectively stored on the one or more computer-readable storage media, the program instructions executable to;</claim-text><claim-text>obtain a first abstract syntax tree that represents a source code snippet;</claim-text><claim-text>generate a second abstract syntax tree from the first abstract syntax tree;</claim-text><claim-text>for each node in a first set of nodes in the second abstract syntax tree, change a field value of the node;</claim-text><claim-text>delete each node in a second set of nodes in the second abstract syntax tree; and</claim-text><claim-text>for each node in a third set of nodes in the second abstract syntax tree, change a parent node of the node.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The system according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the program instructions are further executable to delete at least one node in the first abstract syntax tree.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The system according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the program instructions are further executable to train a machine learning model to generate an alignment of nodes based on the first abstract syntax tree and the second abstract syntax tree.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The system according to <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the machine learning model comprises a graph neural network.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The system according to <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the program instructions are further executable to:<claim-text>obtain a third abstract syntax tree that represents a predecessor source code snippet and a fourth abstract syntax tree that represents a successor source code snippet;</claim-text><claim-text>determine a mapping across the third abstract syntax tree and the fourth abstract syntax tree between pairs of matching nodes;</claim-text><claim-text>obtain a first control-flow graph that represents the predecessor source code snippet and a second control-flow graph that represents the successor source code snippet;</claim-text><claim-text>align blocks in the first control-flow graph with blocks in the second control-flow graph based on the mapping across the third abstract syntax tree and the fourth abstract syntax tree; and</claim-text><claim-text>apply the aligned blocks as inputs across the machine learning model to generate an alignment of nodes in the third abstract syntax tree with nodes in the fourth abstract syntax tree.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The system according to <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the program instructions are further executable to generate a change graph based on the alignment of the nodes in the third abstract syntax tree with the nodes in the fourth abstract syntax tree, wherein the change graph represents one or more edits made to the predecessor source code snippet to yield the successor source code snippet.</claim-text></claim></claims></us-patent-application>