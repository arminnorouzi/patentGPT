<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005176A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005176</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17945573</doc-number><date>20220915</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>CN</country><doc-number>202011110807.2</doc-number><date>20201016</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>70</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>60</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>70</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>60</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30232</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>10016</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30241</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>08</class><subclass>B</subclass><main-group>21</main-group><subgroup>02</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">THROWING POSITION ACQUISITION METHOD AND APPARATUS, COMPUTER DEVICE AND STORAGE MEDIUM</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>PCT/CN2021/120197</doc-number><date>20210924</date></document-id><parent-status>PENDING</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>17945573</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>TENCENT TECHNOLOGY (SHENZHEN) COMPANY LIMITED</orgname><address><city>Shenzhen</city><country>CN</country></address></addressbook><residence><country>CN</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>XIA</last-name><first-name>Maocai</first-name><address><city>Shenzhen</city><country>CN</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>TU</last-name><first-name>Si jia</first-name><address><city>Shenzhen</city><country>CN</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>LIU</last-name><first-name>Shusheng</first-name><address><city>Shenzhen</city><country>CN</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>TENCENT TECHNOLOGY (SHENZHEN) COMPANY LIMITED</orgname><role>03</role><address><city>Shenzhen</city><country>CN</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A throwing position acquisition method and apparatus, a computer device, and a storage medium. The method includes: acquiring image frames of a target video; acquiring a projectile position in each image frame; acquiring a trajectory starting point position of the target object based on the projectile position in each image frame; acquiring, based on projectile positions corresponding to at least one group of image frames in the image frames, a first height value corresponding to a case that the target object is thrown; and acquiring a throwing position of the target object based on the first height value and the trajectory starting point position of the target object.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="123.02mm" wi="123.61mm" file="US20230005176A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="224.62mm" wi="125.65mm" file="US20230005176A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="205.32mm" wi="128.35mm" file="US20230005176A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="172.64mm" wi="136.06mm" file="US20230005176A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="231.06mm" wi="154.26mm" file="US20230005176A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="209.63mm" wi="155.62mm" file="US20230005176A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="167.05mm" wi="154.69mm" file="US20230005176A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="161.63mm" wi="157.56mm" file="US20230005176A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION(S)</heading><p id="p-0002" num="0001">This application is a continuation application of International Application No. PCT/CN2021/120197, filed on Sep. 24, 2021, which claims priority to Chinese Patent Application No. 202011110807.2, filed with the China National Intellectual Property Administration on Oct. 16, 2020, the disclosures of which are incorporated by reference in their entireties.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">FIELD</heading><p id="p-0003" num="0002">The present disclosure relates to the field of image processing, and in particular, to a throwing position acquisition method and apparatus, a computer device, and a storage medium.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">High-altitude projectiles can easily cause harm to property and personnel safety, and it is difficult to determine locations at which high-altitude projectiles occur.</p><p id="p-0005" num="0004">With the continuous development of surveillance devices, buildings can be photographed through surveillance devices. A surveillance device acquires a video of a building within a period of time by photographing the facade of the building, then the building video within the period of time is manually screened to find a video clip in the building video corresponding to a case that an object is thrown, and subsequently the video clip is judged in a manual review manner to determine a throwing position of the object.</p><p id="p-0006" num="0005">In the related art, locations of the high-altitude projectile events are judged by manually screening camera videos of buildings, and the accuracy and efficiency of judging throwing positions are relatively low.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0007" num="0006">Embodiments of the disclosure provide a throwing position acquisition method and apparatus, a computer device, and a storage medium, which can improve the accuracy and judgment efficiency of throwing position acquisition while improving the efficiency of throwing position acquisition. The technical solution is as follows:</p><p id="p-0008" num="0007">According to an aspect, a throwing position acquisition method, performed by a computer device, may be provided, the method including: acquiring image frames of a target video, the target video being a video including a thrown target object; acquiring a projectile position in each image frame, the projectile position being a position of an image corresponding to the target object in each image frame; acquiring a trajectory starting point position of the target object based on the projectile position in each image frame, the trajectory starting point position being a starting point position of a projectile trajectory obtained based on the projectile position in each image frame; acquiring, based on projectile positions corresponding to at least one group of image frames in the image frames, a first height value corresponding to a case that the target object is thrown, the at least one group of image frames respectively including at least two image frames; and acquiring a throwing position of the target object based on the first height value and the trajectory starting point position of the target object.</p><p id="p-0009" num="0008">According to another aspect, a throwing position acquisition apparatus, applicable to a computer device, may be provided, the apparatus including: an image frame acquisition module, configured to acquire image frames of a target video, the target video being a video including a thrown target object; a projectile position acquisition module, configured to acquire a projectile position in each image frame, the projectile position being a position of an image corresponding to the target object in each image frame; a trajectory starting point acquisition module, configured to acquire a trajectory starting point position of the target object based on the projectile position in each image frame, the trajectory starting point position being a starting point position of a projectile trajectory obtained based on the projectile position in each image frame; a first height acquisition module, configured to acquire, based on projectile positions corresponding to at least one group of image frames in the image frames, a first height value corresponding to a case that the target object is thrown, the at least one group of image frames respectively including at least two image frames; and a throwing position acquisition module, configured to acquire a throwing position of the target object based on the first height value and the trajectory starting point position of the target object.</p><p id="p-0010" num="0009">In a possible implementation, the projectile position acquisition module includes:</p><p id="p-0011" num="0010">a candidate image acquisition unit, configured to acquire candidate object images in each image frame, the candidate object images being object images in a foreground image of each image frame; and</p><p id="p-0012" num="0011">a projectile position acquisition unit, configured to acquire the projectile positions in the image frames based on candidate positions in each image frame, the candidate positions being positions of the candidate object images in each image frame in the corresponding image frame.</p><p id="p-0013" num="0012">In a possible implementation, the projectile position acquisition unit includes:</p><p id="p-0014" num="0013">a foreground image acquisition subunit, configured to acquire the foreground image in each image frame based on each image frame in the target video; and</p><p id="p-0015" num="0014">a threshold judgment unit, configured to acquire object images of which image areas are greater than an area threshold in the foreground image in each image frame as the candidate object images in each image frame.</p><p id="p-0016" num="0015">In a possible implementation, the projectile position acquisition unit is configured to:</p><p id="p-0017" num="0016">connect, in chronological order, candidate positions between which target distances are less than a distance threshold in N consecutive image frames included in the image frames, to obtain at least one candidate trajectory, N&#x2265;2, N being an integer, and the target distance being a distance between candidate positions in two adjacent image frames sorted in chronological order in the N consecutive image frames;</p><p id="p-0018" num="0017">acquire a projectile trajectory of the target object in the N image frames according to the at least one candidate trajectory; and</p><p id="p-0019" num="0018">acquire candidate positions in the projectile trajectory of the target object in the N image frames as projectile positions in the N image frames.</p><p id="p-0020" num="0019">In a possible implementation, the projectile position acquisition unit is further configured to:</p><p id="p-0021" num="0020">acquire, based on a projectile position in an i<sup>th </sup>image frame in the image frames and candidate positions of candidate object images in an (i+1)<sup>th </sup>image frame in the image frames, a projectile position in the (i+1)<sup>th </sup>image frame, i&#x2265;1, and i being an integer.</p><p id="p-0022" num="0021">In a possible implementation, the projectile position acquisition unit is further configured to:</p><p id="p-0023" num="0022">acquire the candidate positions of the candidate object images in the (i+1)<sup>th </sup>image frame; and</p><p id="p-0024" num="0023">acquire a candidate position closest to the projectile position in the i<sup>th </sup>image frame in the candidate positions as the projectile position in the (i+1)<sup>th </sup>image frame.</p><p id="p-0025" num="0024">In a possible implementation, the projectile position acquisition unit is further configured to:</p><p id="p-0026" num="0025">acquiring a candidate position corresponding to a candidate object image of which an image area is the largest in each image frame as the projectile position in each image frame.</p><p id="p-0027" num="0026">In a possible implementation, the first height acquisition module is configured to:</p><p id="p-0028" num="0027">acquire the projectile positions corresponding to the at least two image frames in the at least one group of image frames and a time interval between the at least two image frames in the at least one group of image frames; and</p><p id="p-0029" num="0028">calculate, based on the projectile positions corresponding to the at least two image frames in the at least one group of image frames and the time interval between the at least two image frames in the at least one group of image frames, the first height value corresponding to a case that the target object is thrown.</p><p id="p-0030" num="0029">In a possible implementation, the throwing position acquisition module includes:</p><p id="p-0031" num="0030">a second height acquisition unit, configured to acquire a second height value of the trajectory starting point position, the second height value being a height value of the trajectory starting point position;</p><p id="p-0032" num="0031">a third height acquisition unit, configured to acquire, based on the first height value and the second height value, a third height value corresponding to a case that the target object is thrown; and</p><p id="p-0033" num="0032">a first throwing position acquisition unit, configured to acquire the throwing position of the target object based on the third height value and a horizontal position of the trajectory starting point position.</p><p id="p-0034" num="0033">In a possible implementation, the third height acquisition unit is configured to:</p><p id="p-0035" num="0034">perform weighted summation on the first height value and the second height value based on a weight parameter, to obtain the third height value corresponding to a case that the target object is thrown.</p><p id="p-0036" num="0035">In a possible implementation, the throwing position acquisition module includes:</p><p id="p-0037" num="0036">a second throwing position acquisition unit, configured to acquire the throwing position of the target object based on the first height value and a horizontal position of the trajectory starting point position.</p><p id="p-0038" num="0037">In a possible implementation, the apparatus further includes:</p><p id="p-0039" num="0038">a projectile trajectory acquisition module, configured to connect the projectile positions in the image frames in chronological order, to obtain the projectile trajectory of the target object; and</p><p id="p-0040" num="0039">a warning information generation module, configured to generate warning information in a case that the projectile trajectory of the target object meets a warning condition.</p><p id="p-0041" num="0040">In a possible implementation, the warning condition includes:</p><p id="p-0042" num="0041">the projectile trajectory of the target object being a vertically downward linear trajectory; or</p><p id="p-0043" num="0042">the projectile trajectory of the target object being a downward parabolic trajectory.</p><p id="p-0044" num="0043">According to still another aspect of some embodiments, a computer device may be provided, including a processor and a memory, the memory storing at least one instruction, at least one program, a code set, or an instruction set, the at least one instruction, the at least one program, the code set, or the instruction set being loaded and executed by the processor to implement the foregoing embodiments.</p><p id="p-0045" num="0044">According to still another aspect, a non-transitory computer-readable storage medium may be provided, storing at least one instruction, at least one program, a code set, or an instruction set, the at least one instruction, the at least one program, the code set, or the instruction set being loaded and executed by a processor to implement the foregoing embodiments.</p><p id="p-0046" num="0045">According to still another aspect, a computer program product or a computer program may be provided, the computer program product or the computer program including computer instructions, the computer instructions being stored in a computer-readable storage medium. A processor of a computer device reads the computer instructions from the computer-readable storage medium and executes the computer instructions to cause the computer device to perform the foregoing embodiments.</p><p id="p-0047" num="0046">The technical solutions provided in the disclosure may have the following advantageous effects:</p><p id="p-0048" num="0047">By acquiring image frames corresponding to a projectile video, and performing image processing on the image frames, a projectile position of a target object in each image frame is acquired, and a height value corresponding to a case that the target object is thrown in calculated according to at least one group of image frames in the image frames, then a throwing position of the target object is obtained with reference to a trajectory starting point position of a projectile trajectory of the target object. In the foregoing solution, the height value corresponding to a case that the target object is thrown is acquired based on at least one group of image frames in the image frames, and the throwing position of the target object is obtained according to the height value and the trajectory starting point position of the projectile trajectory, which reduces interference of external environmental factors on the acquired throwing position, and improves the accuracy of throwing position acquisition while improving the throwing position identification efficiency through the computer vision technology. In addition, in the process, influence of manual factors on the throwing position acquisition is reduced, thereby improving the efficiency of throwing position acquisition.</p><p id="p-0049" num="0048">It is to be understood that, the foregoing general descriptions and the following detailed descriptions are merely for illustration and explanation purposes and are not intended to limit the disclosure.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0050" num="0049">To describe the technical solutions of example embodiments of this disclosure more clearly, the following briefly introduces the accompanying drawings for describing the example embodiments. The accompanying drawings in the following description show only some embodiments of the disclosure, and a person of ordinary skill in the art may still derive other drawings from these accompanying drawings without creative efforts. In addition, one of ordinary skill would understand that aspects of example embodiments may be combined together or implemented alone.</p><p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic structural diagram of a throwing position acquisition system according to some embodiments.</p><p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a flowchart of a throwing position acquisition method according to some embodiments.</p><p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flowchart of a throwing position acquisition method according to some embodiments.</p><p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a schematic diagram of a projectile height calculating method according to the embodiment shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0055" num="0054"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a schematic diagram of a throwing position acquisition method according to the embodiment shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0056" num="0055"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a schematic diagram of application of projectile detection according to the embodiment shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a schematic framework diagram of a throwing position acquisition method according to some embodiments.</p><p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a structural block diagram of a throwing position acquisition apparatus according to some embodiments.</p><p id="p-0059" num="0058"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a structural block diagram of a computer device according to some embodiments.</p><p id="p-0060" num="0059"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a structural block diagram of a computer device according to some embodiments.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DESCRIPTION OF EMBODIMENTS</heading><p id="p-0061" num="0060">To make the objectives, technical solutions, and advantages of the present disclosure clearer, the following further describes the present disclosure in detail with reference to the accompanying drawings. The described embodiments are not to be construed as a limitation to the present disclosure. All other embodiments obtained by a person of ordinary skill in the art without creative efforts shall fall within the protection scope of the present disclosure.</p><p id="p-0062" num="0061">In the following descriptions, related &#x201c;some embodiments&#x201d; describe a subset of all possible embodiments. However, it may be understood that the &#x201c;some embodiments&#x201d; may be the same subset or different subsets of all the possible embodiments, and may be combined with each other without conflict.</p><p id="p-0063" num="0062"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic structural diagram of a throwing position acquisition system according to some embodiments. The system includes: a video acquisition device <b>120</b> and a video processing device <b>140</b>.</p><p id="p-0064" num="0063">The video processing device <b>140</b> may be a server. The server may be an independent physical server, or may be a server cluster or a distributed system including a plurality of physical servers, or may be a cloud server that provides basic cloud computing services such as a cloud service, a cloud database, cloud computing, a cloud function, cloud storage, a network service, cloud communication, a middleware service, a domain name service, a security service, a content delivery network (CDN), big data, and an artificial intelligence platform.</p><p id="p-0065" num="0064">The video acquisition device <b>120</b> may be a device having a video acquisition function. For example, the video acquisition device <b>120</b> may be a camera device. Alternatively, the video acquisition device <b>120</b> may be a terminal device having an image acquisition function. For example, the terminal device may be a mobile phone, a tablet computer, a laptop, a notebook computer, a desktop computer, a smart speaker, or a smart watch, but not limited thereto.</p><p id="p-0066" num="0065">In some embodiments, the video acquisition device <b>120</b> may send acquired video data to the video processing device <b>140</b>, and the video processing device <b>140</b> performs image processing on the video data.</p><p id="p-0067" num="0066">The video data may be video file data, or the video data may be video stream data.</p><p id="p-0068" num="0067">The video acquisition device <b>120</b> and the video processing device <b>140</b> are connected by a communication network. The communication network may be a wired network or a wireless network.</p><p id="p-0069" num="0068">The wireless network or the wired network uses a standard communication technology and/or protocol. The network is generally the Internet, but may be any network, including, but not limited to, any combination of a local area network (LAN), a metropolitan area network (MAN), a wide area network (WAN), a mobile, wired, or wireless network, and a dedicated network or a virtual private network. In some embodiments, technologies and/or formats such as hypertext markup language (HTML) and extensible markup language (XML) are used to represent data exchanged through a network. In addition, all or some links may be encrypted by using conventional encryption technologies such as a secure socket layer (SSL), transport layer security (TLS), a virtual private network (VPN), and internet protocol security (IPsec). In some other embodiments, custom and/or dedicated data communication technologies may also be used in place of or in addition to the foregoing data communication technologies.</p><p id="p-0070" num="0069"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a flowchart of a throwing position acquisition method according to some embodiments. The method may be performed by a computer device. The computer device may be a video processing device. The video processing device may be the video processing device <b>140</b> in the embodiment shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. As shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the process of the throwing position acquisition method may include the following operations:</p><p id="p-0071" num="0070">Operation <b>201</b>: Acquire image frames of a target video, the target video being a video including a thrown target object.</p><p id="p-0072" num="0071">In an example embodiment, the image frames in the target video are image frames including a process in which the target object is thrown; or, some of the image frames are image frames including a process in which the target object is thrown.</p><p id="p-0073" num="0072">The video processing device may first process the target video to obtain image frames in the target video that include the process in which the target object is thrown; or the video processing device may directly use all the image frames of the target video as image frames in the target video on which subsequent image processing needs to be performed.</p><p id="p-0074" num="0073">Operation <b>202</b>: Acquire a projectile position in each image frame, the projectile position being a position of an image corresponding to the target object in each image frame.</p><p id="p-0075" num="0074">The projectile position is a position obtained according to each image frame and at which the target object is located in the corresponding image frame.</p><p id="p-0076" num="0075">Operation <b>203</b>: Acquire a trajectory starting point position of the target object based on the projectile position in each image frame, the trajectory starting point position being a starting point position of a projectile trajectory obtained based on the projectile position in each image frame.</p><p id="p-0077" num="0076">Operation <b>204</b>: Acquire, based on projectile positions corresponding to at least one group of image frames in the image frames, a first height value corresponding to a case that the target object is thrown, the at least one group of image frames respectively including at least two image frames.</p><p id="p-0078" num="0077">Operation <b>205</b>: Acquire a throwing position of the target object based on the first height value and the trajectory starting point position of the target object.</p><p id="p-0079" num="0078">Based on above, in the solution shown in this embodiment, by acquiring image frames corresponding to a projectile video, and performing image processing on the image frames, a projectile position of a target object in each image frame is acquired, and a height value corresponding to a case that the target object is thrown in calculated according to at least one group of image frames in the image frames, then a throwing position of the target object is obtained with reference to a trajectory starting point position of a projectile trajectory of the target object. In the foregoing solution, the height value corresponding to a case that the target object is thrown is acquired based on at least one group of image frames in the image frames, and the throwing position of the target object is obtained according to the height value and the trajectory starting point position of the projectile trajectory, which reduces interference of external environmental factors on the acquired throwing position, and improves the accuracy of throwing position acquisition. In addition, in the process, influence of manual factors on the throwing position acquisition is reduced, thereby improving the efficiency of throwing position acquisition.</p><p id="p-0080" num="0079"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flowchart of a throwing position acquisition method according to some embodiments. The method may be performed by a computer device. The computer device may be a video processing device. The video processing device may be the video processing device <b>120</b> in the embodiment shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. As shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the throwing position acquisition method may include the following operations:</p><p id="p-0081" num="0080">Operation <b>301</b>: Acquire image frames of a target video, the target video being a video including a thrown target object.</p><p id="p-0082" num="0081">The image frames of the target video may be all or some of the image frames in the target video acquired by the video acquisition device based on an image capture component or a video acquisition component. After acquiring the image frames of the target video, the video acquisition device may upload the image frames of the target video to the video processing device, and correspondingly, the video processing device acquires the image frames of the target video.</p><p id="p-0083" num="0082">The image frames may be continuous image frames in the target video.</p><p id="p-0084" num="0083">Operation <b>302</b>: Acquire candidate object images in each image frame.</p><p id="p-0085" num="0084">There are a foreground image part and a background image part in each image frame. A foreground image is an image of an object relatively close to the video acquisition device, and a background image is an image of an object relatively far away from the video acquisition device. For example, in a high-altitude projectile scenario, the background image is usually a high-rise building, and the thrown target object usually exists in the foreground image. Therefore, the candidate object images may be object images in a foreground image of each image frame.</p><p id="p-0086" num="0085">In an example embodiment, the background image and the foreground image corresponding to each image frame of the target video may be acquired through the background modeling technology. For example, the background image and the foreground image corresponding to each image frame of the target video may be acquired by using a visual background extractor (VIBE) algorithm.</p><p id="p-0087" num="0086">The VIBE algorithm is a foreground detection algorithm based on background update, and the principle thereof is that: pixel values around a pixel point (x, y) and historical pixel values on the pixel point (x, y) are extracted to establish a sample set of the pixel point, then a pixel value at another frame (x, y) is compared with the pixel values in the sample set, if differences between the pixel value at the another frame (x, y) and the pixel values in the sample set are greater than a difference threshold, it is considered that the pixel point is a foreground pixel, otherwise it is considered that the pixel point is a background pixel point.</p><p id="p-0088" num="0087">For example, when each image frame of the target video is identified by using the VIBE algorithm, pixel values of a certain pixel point in previous several frames and pixel values of pixel points around the pixel point in the image frame are acquired. Because the background image usually does not change, and there is generally no big difference between pixel points of the background image and the surrounding pixels. Therefore, when differences between a pixel value of the pixel point in another image frame and the pixel values in the sample set of the pixel point are less than the difference threshold, the pixel point in the image frame is identified as a background pixel point, and the background pixel point may be marked as 0 in a binary function. Because the foreground image is usually an image of an object that is constantly moving based on the background image, and because the foreground image blocks (replaces) some pixel points in the background image, differences between pixel points corresponding to the foreground image and surrounding pixel points corresponding to the background image are relatively large. Therefore, when differences between a pixel value on the pixel point in the image frame and pixel values in the corresponding sample set are greater than the difference threshold, the pixel point in the image frame is identified as a pixel point corresponding to the foreground image. The foreground pixel point may be marked as 1 in the binary function. Additionally, the foreground pixel point may be highlighted to distinguish it from background pixel points.</p><p id="p-0089" num="0088">After pixel points in each image frame are identified by using the above method, each image frame may be divided into a foreground image and a background image.</p><p id="p-0090" num="0089">In an example embodiment, the foreground image in each image frame is acquired, and the foreground image is enlarged to obtain candidate object images in each image frame.</p><p id="p-0091" num="0090">For a surveillance video of the high-rise building acquired by the video acquisition device, images corresponding to projectile objects in the foreground images may be relatively small, which is not conducive to identification and subsequent image processing. Therefore, for the foreground image in each image frame, an image enhancement method may be used to enlarge the relatively small projectile objects in the foreground image into relatively large and easily detectable projectile objects. For example, an image expansion algorithm may be used to enlarge a relatively small projectile object in the foreground image, to obtain an image region larger than the original image of the projectile object.</p><p id="p-0092" num="0091">In the image expansion algorithm, a template B is used to expand an image A, where the template B is a convolution template or convolution kernel, and a shape thereof may be a square or a circle; convolution calculation is performed by using the template B and the image A, each pixel point in the image is scanned, and template elements and binary image elements are used for performing an &#x201c;AND&#x201d; operation, if all the results are 0, a target pixel point is 0, otherwise the target pixel point is 1. Therefore, the maximum value of pixel points in a region covered by the template B is calculated, and the value is used to replace a pixel value of a reference point, to realize image expansion. That is, after convolutional computation is performed on the foreground image through the image expansion algorithm by using the template B, the field of the foreground part is expanded, that is, the boundary region of the foreground image A is expanded by using the template B, to obtain a highlighted region larger than the original image.</p><p id="p-0093" num="0092">In an example embodiment, the candidate object images are obtained by using the connected domain algorithm.</p><p id="p-0094" num="0093">The foreground image obtained after processing by using the foregoing background modeling technology and the image expansion algorithm is a pixmap including a binary function (0,1). In this case, candidate object images in a pixmap corresponding to the foreground image of each image frame may be screened out by using the connected domain algorithm.</p><p id="p-0095" num="0094">The connected domain algorithm is an image segmentation method. After binarization, an image often includes a plurality of regions. For example, each image frame may include one or more highlighted regions (foreground images) marked by the pixel point <b>1</b>, and the foreground image is a target image of the projectile object that needs to be screened.</p><p id="p-0096" num="0095">One image frame is used as an example. During foreground image acquisition, the image frame may be scanned from left to right and from top to bottom. For a certain pixel point in the image frame, if the current pixel value is 0, move to the next scanning position; and if the current pixel value is 1, check its left adjacent pixel and upper adjacent pixel (the two adjacent pixels are scanned before this pixel point). By checking whether the adjacent pixels adjacent to the current pixel point are highlighted regions, whether the highlighted region with the current pixel value of 1 is a new highlighted region (foreground image) is determined. If the adjacent pixels adjacent to the current pixel point are a highlighted region, it is determined that the highlighted region with the current pixel value of 1 is not a new highlighted region; and if adjacent pixels adjacent to the current pixel point are a non-highlighted region, it is determined that the highlighted region with the current pixel value of 1 is a new highlighted region.</p><p id="p-0097" num="0096">Therefore, a foreground image region corresponding to the candidate object images in each image frame can be obtained by using the connected domain algorithm.</p><p id="p-0098" num="0097">Operation <b>303</b>: Acquire the projectile positions in the image frames based on candidate positions in each image frame.</p><p id="p-0099" num="0098">The candidate positions are positions of the candidate object images in each image frame in the corresponding image frame.</p><p id="p-0100" num="0099">In an example embodiment, the foreground image in each image frame is acquired based on each image frame in the target video; and object images of which image areas are greater than an area threshold in the foreground image in each image frame are acquired as the candidate object images in each image frame.</p><p id="p-0101" num="0100">In high-altitude projectile, more attention needs to be paid to relatively large objects, so that by screening the image areas of object images in the foreground images, more attention can be paid to the projectile process of the relatively large objects. In addition, during image processing, there may be a case in which some noise points in the image frames are misidentified as candidate object images. The noise points are usually relatively small, so that screening based on the image areas can avoid misidentification, thereby improving the efficiency of subsequent image identification.</p><p id="p-0102" num="0101">In an example embodiment, candidate positions between which target distances are less than a distance threshold in N consecutive image frames included in the image frames are connected in chronological order, to obtain at least one candidate trajectory, N&#x2265;2, N being an integer, and the target distance being a distance between candidate positions in two adjacent image frames sorted in chronological order in the N consecutive image frames;</p><p id="p-0103" num="0102">a projectile trajectory of the target object in the N image frames is acquired according to the at least one candidate trajectory; and candidate positions in the projectile trajectory of the target object in the N image frames are acquired as projectile positions in the N image frames.</p><p id="p-0104" num="0103">In the process of acquiring the target video by the video acquisition device, a foreign object may intrude into the video picture, and foreign object intrusion pictures are usually continuous, which forms a continuous trajectory in the target video, for example, a flying-by bird, or clouds photographed by the video acquisition device. In the process of identifying the image frames in the target video, the foreign object may have a relatively great impact on the subsequent identification results, and due to the relatively large volume of the foreign object, the foreign object may not be identified through the image area screening, and consequently causing impacts on the image frames of the target video.</p><p id="p-0105" num="0104">In this case, N consecutive image frames in the image frames in the target video may be selected, and the N consecutive image frames are processed, to obtain candidate object images in foreground images of the N image frames. Each candidate object image includes the target object to be detected and a possible foreign object. Considering the case of two consecutive frames, whether it is a foreign object or a target object in a projectile state, the moving distance will not be excessively long. Therefore, positions of the closest candidate object images in adjacent frames in the N consecutive image frames are connected in chronological order to obtain a candidate trajectory.</p><p id="p-0106" num="0105">Because the moving trajectory of the foreign object is usually a horizontal straight line or a straight line close to horizontal, when there is only one candidate trajectory and the candidate trajectory is a parabolic trajectory, it can be considered that the candidate trajectory is the projectile trajectory of the target object, and positions of the candidate object images of the image frames in the candidate trajectory is the projectile positions in the image frames.</p><p id="p-0107" num="0106">When there are two or more candidate trajectories, trajectory shapes of the candidate trajectories may be identified, and a parabolic trajectory or a straight falling trajectory may be acquired as the projectile trajectory of the target object.</p><p id="p-0108" num="0107">In an example embodiment, a projectile position in an (i+1)<sup>th </sup>image frame is acquired based on a projectile position in an i<sup>th </sup>image frame in the image frames and candidate positions of candidate object images in the (i+1)<sup>th </sup>image frame in the image frames, i&#x2265;1, and i being an integer.</p><p id="p-0109" num="0108">The image processing device may determine a projectile position in the next image frame based on a projectile position in the previous image frame.</p><p id="p-0110" num="0109">In a possible implementation, the candidate positions of the candidate object images in the (i+1)<sup>th </sup>image frame in the (i+1)<sup>th </sup>image frame are acquired; and a candidate position closest to the projectile position in the i<sup>th </sup>image frame in the candidate positions is acquired as the projectile position in the (i+1)<sup>th </sup>image frame.</p><p id="p-0111" num="0110">Because a moving distance of the target object in the projectile state will not be excessively long in the case of two consecutive frames, a candidate position closest to the projectile position of the target object in the previous frame in candidate positions in the next image frame may be selected as the projectile position of the target object in the next image frame; or a candidate position closest to the projectile position of the target object in the previous frame and located under the projectile position in the previous frame in the candidate positions of the next image frame may be used as the projectile position of the target object in the next image frame.</p><p id="p-0112" num="0111">In an example embodiment, areas of the candidate object images in each image frame in the corresponding image frame are acquired; and a candidate position corresponding to candidate object image of which an image area is the largest in each image frame is acquired as the projectile position in each image frame.</p><p id="p-0113" num="0112">Operation <b>304</b>: Acquire a trajectory starting point position of the target object based on the projectile position in each image frame.</p><p id="p-0114" num="0113">In an example embodiment, a projectile trajectory of the target object corresponding to each image frame is acquired based on the projectile position corresponding to each image frame, and a trajectory starting point position of the target object is acquired based on the projectile trajectory of the target object.</p><p id="p-0115" num="0114">The trajectory starting point position is a starting point position corresponding to the projectile trajectory obtained based on the image frames, that is, the trajectory starting point position is a starting position corresponding to the projectile trajectory of the target object detected based on the image frames. Because in the process of performing image processing on the image frames by the image processing device, an error may be caused by interference or noise in the image frames, the trajectory starting point position obtained based on image identification may have a certain error with the real throwing position of the target object.</p><p id="p-0116" num="0115">Operation <b>305</b>: Acquire the projectile positions corresponding to the at least two image frames in the at least one group of image frames and a time interval between the at least two image frames in the at least one group of image frames.</p><p id="p-0117" num="0116">In an example embodiment, the at least one group of image frames is at least two consecutive image frames.</p><p id="p-0118" num="0117">The target video includes several image frames, and a time interval between two adjacent image frames may be a certain value. For example, when the target video acquired by the video acquisition device is a video in a 24-frame video format, there are 24 frames of video within 1 second of the target video, and in this case, the time interval between every two adjacent image frames is 1/24 second.</p><p id="p-0119" num="0118">In an example embodiment, the at least one group of image frames is at least two image frames randomly extracted from the image frames of the target video.</p><p id="p-0120" num="0119">In some embodiments, the time interval between the at least two image frames in the at least one group of image frames is greater than a time interval threshold. In other words, the at least two image frames in the at least one group of image frames are at least two image frames with a relatively long time interval, and the corresponding projectile positions in the at least two image frames are also relatively far apart.</p><p id="p-0121" num="0120">Operation <b>306</b>: Calculate, based on the projectile positions corresponding to the at least two image frames in the at least one group of image frames and the time interval between the at least two image frames in the at least one group of image frames, the first height value corresponding to a case that the target object is thrown.</p><p id="p-0122" num="0121">When high-altitude projectile occurs in a high-rise building, it can be preliminarily considered that there are three cases: the projectile is thrown upward, the projectile is thrown horizontally, and the projectile is thrown downward:</p><p id="p-0123" num="0122">When the projectile is thrown upward, the throwing distance will not be excessively long, and the range is probably still near the floor on which it was initially thrown. When the projectile reaches the highest point, the vertical initial velocity of falling is 0, so that it can be approximately considered that the projectile is thrown with a vertical initial velocity of 0.</p><p id="p-0124" num="0123">When the projectile is thrown downward, cases in which the projectile is thrown downward with a relatively large force is rare, and in most cases, the initial velocity of the downward throwing is relatively low, which may be approximately 0.</p><p id="p-0125" num="0124">When the projectile is thrown horizontally, the case is quite common. There is an initial velocity in the horizontal direction, and the initial velocity in the vertical direction is 0.</p><p id="p-0126" num="0125">In general, because the initial velocity is relatively low in the case of a high-altitude projectile, the high-altitude projectile behavior may be approximately used as a horizontal projectile motion for analysis. Therefore, in some embodiments, the motion process of the object in the vertical direction in the horizontal projectile motion needs to be analyzed, and the high-altitude projectile model may be converted into a free fall motion model for analysis.</p><p id="p-0127" num="0126">In some embodiments, when projectile positions corresponding to two image frames in the at least one group of image frames and a time interval between the two image frames are acquired, the first height value of the target object can be calculated.</p><p id="p-0128" num="0127">A height difference between the projectile positions corresponding to the two image frames may be determined based on the positions of the projectile positions corresponding to the two image frames on background images. For example, when the background image is a tall building, the height difference between the projectile positions corresponding to the two image frames may be determined based on a floor difference between the projectile positions corresponding to the two image frames; or the height difference between the projectile positions corresponding to the two image frames may be determined based on a preset scale of the image in the target video and the tall building.</p><p id="p-0129" num="0128"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a schematic diagram of a projectile height calculating method according to some embodiments. As shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, a falling height corresponding to a first projectile position is h<sub>1</sub>, and a falling height corresponding to a second projectile position is h<sub>2</sub>, then a height difference between the first projectile position and the second projectile position is h<sub>1</sub>&#x2212;h<sub>2</sub>. Because the two image frames are two image frames in the image frames corresponding to the target video, a time interval between the two image frames is known. For example, t<sub>1 </sub>is a falling time corresponding to the first projectile position, and t<sub>2 </sub>is a falling time corresponding to the second projectile position, then the time interval between the two image frames is t<sub>2</sub>&#x2212;t<sub>1</sub>, and it can be learned from Newton's laws of motion that:</p><p id="p-0130" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mrow>  <mrow>   <msub>    <mi>h</mi>    <mn>2</mn>   </msub>   <mo>-</mo>   <msub>    <mi>h</mi>    <mn>1</mn>   </msub>  </mrow>  <mo>=</mo>  <mrow>   <mrow>    <mrow>     <mfrac>      <mn>1</mn>      <mn>2</mn>     </mfrac>     <mo>&#x2062;</mo>     <msubsup>      <mi>gt</mi>      <mn>2</mn>      <mn>2</mn>     </msubsup>    </mrow>    <mo>-</mo>    <mrow>     <mfrac>      <mn>1</mn>      <mn>2</mn>     </mfrac>     <mo>&#x2062;</mo>     <msubsup>      <mi>gt</mi>      <mn>1</mn>      <mn>2</mn>     </msubsup>    </mrow>   </mrow>   <mo>=</mo>   <mrow>    <mfrac>     <mn>1</mn>     <mn>2</mn>    </mfrac>    <mo>&#x2062;</mo>    <mrow>     <mi>g</mi>     <mo>&#x2061;</mo>     <mo>(</mo>     <mrow>      <msub>       <mi>t</mi>       <mn>2</mn>      </msub>      <mo>+</mo>      <msub>       <mi>t</mi>       <mn>1</mn>      </msub>     </mrow>     <mo>)</mo>    </mrow>    <mo>&#x2062;</mo>    <mrow>     <mo>(</mo>     <mrow>      <msub>       <mi>t</mi>       <mn>2</mn>      </msub>      <mo>-</mo>      <msub>       <mi>t</mi>       <mn>1</mn>      </msub>     </mrow>     <mo>)</mo>    </mrow>   </mrow>  </mrow> </mrow></math></maths></p><p id="p-0131" num="0129">where h<sub>2</sub>&#x2212;h<sub>1 </sub>and t<sub>2</sub>&#x2212;t<sub>1 </sub>are known, t<sub>2</sub>+t<sub>1 </sub>can be calculated, and in this way, t<sub>2 </sub>and t<sub>1 </sub>can be obtained, and thus h<sub>2 </sub>and h<sub>1 </sub>are obtained. That is, the falling height of the first projectile position and the falling height of the second projectile position can be obtained. In addition, because the first projectile position is a height value from a projectile starting point to the first projectile position, and the falling height of the second projectile position is a height value from the projectile starting point to the second projectile position, in a case that the height value of the first projectile position and the height value of the second projectile position are known, the first height value can be calculated by acquiring a time interval between the falling time corresponding to the throwing position point and the falling time corresponding to the first projectile position and a time interval between the throwing position point and the second projectile position.</p><p id="p-0132" num="0130">In some embodiments, projectile positions corresponding to at least two image frames in the at least one group of image frames and a time interval between the at least two image frames in the at least one group of image frames are acquired, and a pre-selected height value corresponding to each group of image frames is acquired according to the projectile positions corresponding to at least two image frames in each group of image frames and a time interval between at least two image frames in each group of image frames; and mean processing is performed on the pre-selected height values corresponding to the groups of image frames, to obtain the first height value.</p><p id="p-0133" num="0131">Operation <b>307</b>: Acquire a second height value of the trajectory starting point position, the second height value being a height value of the trajectory starting point position.</p><p id="p-0134" num="0132">In some embodiments, the second height value of the trajectory starting point position is acquired according to a correspondence between the trajectory starting point position and a background image corresponding to each image frame.</p><p id="p-0135" num="0133">The background image corresponding to each image frame is a fixed image, so that a height value corresponding to each pixel point can be acquired according to each pixel point in the background image.</p><p id="p-0136" num="0134">Operation <b>308</b>: Acquire, based on the first height value and the second height value, a third height value corresponding to a case that the target object is thrown.</p><p id="p-0137" num="0135">In some embodiments, weighted summation is performed on the first height value and the second height value based on a weight parameter, to obtain the third height value corresponding to a case that the target object is thrown.</p><p id="p-0138" num="0136">In some embodiments, mean processing is performed on the first height value and the second height value of the trajectory starting point position, to obtain the third height value corresponding to a case that the target object is thrown.</p><p id="p-0139" num="0137">Operation <b>309</b>: Acquire the throwing position of the target object based on the third height value and a horizontal position of the trajectory starting point position.</p><p id="p-0140" num="0138">In other words, the first height value corrected based on the second height value (or the second height value corrected based on the first height value), that is, the third height value, is acquired as the height value corresponding to the throwing position of the target object; and the throwing position of the target object can be located with reference to a horizontal position of the trajectory starting point position.</p><p id="p-0141" num="0139">In some embodiments, the first height value corresponding to a case that the target object is thrown may be directly determined as the height value corresponding to the throwing position of the target object; and the throwing position of the target object is acquired based on the first height value and the horizontal position of the trajectory starting point position.</p><p id="p-0142" num="0140">In some embodiments, the projectile positions corresponding to the image frames are connected in chronological order, to obtain the projectile trajectory of the target object; and warning information are generated in a case that the projectile trajectory of the target object meets a warning condition.</p><p id="p-0143" num="0141">The warning condition may include: the projectile trajectory of the target object being a vertically downward linear trajectory; or the projectile trajectory of the target object being a downward parabolic trajectory.</p><p id="p-0144" num="0142">When the image processing device detects that the projectile trajectory of the target object is a vertically downward linear trajectory, or the projectile trajectory of the target object is a downward parabolic trajectory, it means that the target object may be a relatively heavy object. In this case, warning information may be generated to remind that a relatively dangerous high-altitude projectile event has occurred.</p><p id="p-0145" num="0143"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a schematic diagram of a throwing position acquisition method according to some embodiments. As shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, for a high-rise building, a projectile video corresponding to the high-rise building may be acquired, and foreground image modeling (that is, acquiring the background image and the foreground image through the background modeling technology) is performed on image frames in the projectile video (target video), and then image filtering (that is, removing noise and interference through the connected domain algorithm and a screening process) is performed according to the modeled foreground image, and finally object tracking (acquisition of the projectile trajectory of the object) is implemented.</p><p id="p-0146" num="0144">Moreover, as shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, height values respectively corresponding to the first projectile position and the second projectile position can be obtained according to the first projectile position and the second projectile position in the projectile trajectory by using the free fall formula, then a first height value h of the projectile starting point is acquired, and tracing correction is performed on the projectile starting point based on the first height value h.</p><p id="p-0147" num="0145"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a schematic diagram of application of projectile detection according to some embodiments. As shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the computer device may be implemented as a high-altitude projectile real-time monitoring platform. The high-altitude projectile real-time monitoring platform is a high-altitude projectile real-time monitoring system <b>600</b> established based on the cloud technology. The video acquisition device <b>601</b> of each node monitors the facade of a building <b>602</b> in real time, and transmits real-time monitoring data to the high-altitude projectile real-time monitoring platform <b>600</b> deployed on a cloud server. The high-altitude projectile real-time monitoring platform processes an image video <b>603</b> corresponding to the monitoring data by using the method shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, and monitors and analyzes high-altitude projectile. When high-altitude projectile is detected, a warning module <b>604</b> in the high-altitude projectile real-time monitoring system <b>600</b> issues an alarm to the staff, so that the staff handles the high-altitude projectile event.</p><p id="p-0148" num="0146">The system may use an ordinary camera to monitor the facade of the building in real time, perform real-time analysis by using the AI algorithm based on the real-time monitoring data of the facade of the building, to implement high-altitude projectile abnormal event monitoring on the building, provide reverse tracing to the floor interval at which the projectile occurred based on a falling velocity and distance of a thrown object in the picture, to locate the location and time point of the event, help managers to efficiently collect evidence after the event, and acquire a complete high-altitude projectile event chain. Based on the extensive monitoring range, places in a region in which projectile events easily occur can be more effectively predicted, thereby achieving rapid warning and prevention.</p><p id="p-0149" num="0147">In addition, the system is a system for intelligent detection of high-altitude projectile based on a visual algorithm and physical tracing, in which modeling analysis is performed on video dynamic frames based on the computer vision technology, to separate background and foreground video frames (background image and foreground image), and initially locate the projectile position; and the falling process of the projectile is tracked by using the tracking algorithm, and the position of the object is located in real time with reference to the modeling algorithm. In addition, a falling velocity of the object can be calculated according to a falling time difference based on a physical model and a mathematical formula, thereby reversely tracing a falling distance of the projectile, and further correcting a falling region, to achieve the purpose of projectile monitoring and tracing. That is, background frame data and foreground frame data of the video data are firstly acquired through the background modeling technology to preliminarily locate the position of the projectile; the image enhancement method is used to increase a relatively small projectile in the picture into a relatively large and easily detectable projectile object, and the maximum connected domain algorithm is used to select the largest projectile in the picture, to filter out other relatively low image noises; the image tracking algorithm is used to track the falling process of the projectile, and the position of the projectile is located in real time with reference to the modeling algorithm; and the falling velocity of the object is calculated according to the time difference of falling based on modeling analysis and the principle of projectile motion, so as to reversely trace the fall distance of the projectile, thereby further correcting the falling region.</p><p id="p-0150" num="0148">Even though there is already a method for processing high-altitude projectile events currently, due to the rapid urban development and the increase in high-rise buildings, high-altitude projectile events will still occur, posing a threat to public safety. However, with the installation and application of high-altitude projectile intelligent detection systems, the existing loopholes can be plugged from the source, high-altitude projectiles can be prevented, and the probability of accidents can be reduced. With the increasing prosperity of technologies such as the Internet and big data, a more livable, convenient, and safe city has become a common demand of people, and the high-altitude projectile intelligent detection system is a part of the construction of a smart city.</p><p id="p-0151" num="0149">In addition, by using the high-altitude projectile intelligent detection system, the efficiency of community management can be improved, the risk of accidents can be reduced, and a plurality of system modules such as personnel management and vehicle management and control can be integrated in the future to achieve more refined data sharing, which not only deploys prevention for high-altitude projectile events, but also meets the upgrading needs of a smart city.</p><p id="p-0152" num="0150">Based on above, in the solution shown in this embodiment, by acquiring image frames corresponding to a projectile video, and performing image processing on the image frames, a projectile position of a target object in each image frame is acquired, a projectile starting point position of a projectile trajectory is acquired according to the projectile position in each image frame, a height value corresponding to a case that the target object is thrown in calculated according to at least one group of image frames in the image frames, and the projectile starting point position of the projectile trajectory is updated and corrected according to the height value. In the foregoing solution, the height value corresponding to a case that the target object is thrown is acquired based on at least one group of image frames in the image frames, and the projectile trajectory starting point obtained according to image processing is updated according to the height value, to obtain a throwing position, which reduces interference of external environmental factors on the acquired throwing position, and improves the accuracy of throwing position acquisition.</p><p id="p-0153" num="0151"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a schematic framework diagram of a throwing position acquisition method according to some embodiments. As shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>:</p><p id="p-0154" num="0152">a video acquisition device <b>701</b> monitors the facade of a building, acquires a target video <b>702</b> corresponding to high-altitude projectile of a target object, and saves and transmits the target video <b>702</b> to a video processing device <b>700</b> for high-altitude projectile detection.</p><p id="p-0155" num="0153">The video processing device <b>700</b> acquires the target video <b>702</b>, processes the target video <b>702</b> to acquire video frames corresponding to the target video <b>702</b>, and then acquires a background image and a foreground image <b>703</b> of video data through the background modeling technology, to preliminary locate the position of the projectile. For example, the VIBE algorithm may be used to randomly select samples of pixels that need to be replaced through pixel-level background modeling and the foreground detection algorithm, the domain pixels are updated, and the foreground image <b>703</b> is extracted for subsequent processing.</p><p id="p-0156" num="0154">For the extracted foreground image <b>703</b>, an image enhancement method may be used to enlarge a relatively small projectile in the foreground into a relatively large and easily detectable projectile object. For example, an image expansion algorithm may be used to perform domain expansion processing on the foreground image frame, to acquire a highlighted region larger than the original image, thereby obtaining a foreground image <b>704</b> with the enlarged projectile.</p><p id="p-0157" num="0155">A projectile trajectory of the projectile can be acquired based on the foreground image <b>704</b> with the enlarged projectile corresponding to each image frame of the target video.</p><p id="p-0158" num="0156">For example, the maximum connected domain algorithm may be used to screen the largest projectile region in the picture, that is, the largest image may correspond to the largest connected domain. Because the largest object is usually the object to which attention needs to be paid to the projectile process, the position corresponding to the largest image may be used as the projectile position corresponding to the image frame; and a projectile trajectory <b>705</b> of the projectile may be obtained by connecting the projectile positions corresponding to the largest images corresponding to the image frames in chronological order.</p><p id="p-0159" num="0157">Alternatively, when a foreign object appears in an image frame, the foreign object may be identified as the largest image, and the position of the foreign object is acquired as a projectile position corresponding to the image frame, resulting in that a trajectory formed through connection does not conform to a projectile trajectory. Therefore, two adjacent frames may be selected, and projectile positions identified from the previous frame of image may be compared with the relatively large candidate objects identified from this image frame. Due to a relatively short time interval between adjacent frames, a distance of the projectile between the two adjacent frames of images is not excessively large. Therefore, a position closest to the projectile position identified from the previous frame of image may be selected as the projectile position of this image frame, and projectile positions corresponding to the image frames may be connected to obtain the projectile trajectory <b>705</b> of the target object.</p><p id="p-0160" num="0158">Alternatively, when a foreign object appears in a series of image frames, such as a bird flying over a tall building photographed in the image, the bird may be identified as the largest image through the maximum connected domain algorithm, a position corresponding to the bird in each image frame is acquired as a projectile position corresponding to the image frame, and a trajectory of the bird flying over is identified as the projectile trajectory. In this case, a plurality of consecutive image frames may be taken, images in the plurality of image frames larger than a threshold may be screened as candidate objects, and positions of candidate objects in the plurality of image frames may be connected to positions of relatively close candidate objects in adjacent image frames, to obtain at least one candidate trajectory. When there are a plurality of candidate trajectories, it means that there are a plurality of objects moving simultaneously in the image video. In this case, the candidate trajectories may be screened to acquire a trajectory meeting a projectile feature in the candidate trajectories as the projectile trajectory <b>705</b> of the target object, and positions of candidate objects corresponding to the projectile trajectory are used as projectile positions in the image frames. When it is identified that the projectile trajectory <b>705</b> is a vertically downward straight line or a downward parabola, warning information (not shown in the figure) may further be generated.</p><p id="p-0161" num="0159">When projectile positions corresponding to the foreground images are obtained from the foreground images with the enlarged projectile by using any of the above methods, the foreground images may be grouped, and a height value (first height value) of the target object may be acquired through at least one group of foreground images (at least two images). For example, the method described in the embodiment corresponding to <figref idref="DRAWINGS">FIG. <b>3</b></figref> may be used, and details are not described herein again.</p><p id="p-0162" num="0160">The trajectory starting point of the projectile trajectory <b>705</b> is updated and corrected according to the height value <b>706</b> of the target object to obtain a throwing position <b>707</b> of the target object. Since the target video acquired by the video acquisition device is easily affected by the external environment such as light and foreign objects, there may be an error in determining the throwing position of the target object through the trajectory. Therefore, at least one group of foreground images in the foreground images may be taken, and a height value of the throwing position may be calculated based on a time difference and distance difference of foreground image photographing, and a height value of a projectile trajectory starting point position may be corrected according to the height value to obtain a throwing position of the target object, which improves accuracy of the identified projectile position. In the method provided in the embodiment shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, methods such as background modeling, image enhancement, and maximum connected domain processing are effectively used to perform preliminary cleaning and screening on video data, the tracking method is also used to track and locate the projectile in real time, and finally a physical quantity of projectile falling is calculated with reference to a physical model to reversely trace the falling position of the projectile, thereby further calibrating the throwing position of the projectile, greatly improving the accuracy, and meeting the needs of rapid identification, rapid warning, and traceability of responsibility.</p><p id="p-0163" num="0161"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a structural block diagram of a throwing position acquisition apparatus according to some embodiments. In some embodiments, the throwing position acquisition apparatus provided in the embodiments of the present disclosure may be implemented in the form of a combination of software and hardware. The throwing position acquisition apparatus may implement all or some of the operations in the method provided in the embodiment shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref> or <figref idref="DRAWINGS">FIG. <b>3</b></figref>. The throwing position acquisition apparatus may include:</p><p id="p-0164" num="0162">an image frame acquisition module <b>801</b>, configured to acquire image frames of a target video, the target video being a video including a thrown target object;</p><p id="p-0165" num="0163">a projectile position acquisition module <b>802</b>, configured to acquire a projectile position in each image frame, the projectile position being a position of an image corresponding to the target object in each image frame;</p><p id="p-0166" num="0164">a trajectory starting point acquisition module <b>803</b>, configured to acquire a trajectory starting point position of the target object based on the projectile position in each image frame, the trajectory starting point position being a starting point position of a projectile trajectory obtained based on the projectile position in each image frame;</p><p id="p-0167" num="0165">a first height acquisition module <b>804</b>, configured to acquire, based on projectile positions corresponding to at least one group of image frames in the image frames, a first height value corresponding to a case that the target object is thrown, the at least one group of image frames respectively including at least two image frames; and</p><p id="p-0168" num="0166">a throwing position acquisition module <b>805</b>, configured to acquire a throwing position of the target object based on the first height value and the trajectory starting point position of the target object.</p><p id="p-0169" num="0167">In some embodiments, the projectile position acquisition module <b>802</b> may include:</p><p id="p-0170" num="0168">a candidate image acquisition unit configured to acquire candidate object images in each image frame, the candidate object images being object images in a foreground image of each image frame; and</p><p id="p-0171" num="0169">a projectile position acquisition unit configured to acquire the projectile positions in the image frames based on candidate positions in each image frame, the candidate positions being positions of the candidate object images in each image frame in the corresponding image frame.</p><p id="p-0172" num="0170">In some embodiments, the projectile position acquisition unit may include:</p><p id="p-0173" num="0171">a foreground image acquisition subunit configured to acquire the foreground image in each image frame based on each image frame in the target video; and</p><p id="p-0174" num="0172">a threshold judgment unit configured to acquire object images of which image areas are greater than an area threshold in the foreground image in each image frame as the candidate object images in each image frame.</p><p id="p-0175" num="0173">In some embodiments, the projectile position acquisition unit may be configured to:</p><p id="p-0176" num="0174">connect, in chronological order, candidate positions between which target distances are less than a distance threshold in N consecutive image frames included in the image frames, to obtain at least one candidate trajectory, N&#x2265;2, N being an integer, and the target distance being a distance between candidate positions in two adjacent image frames sorted in chronological order in the N consecutive image frames;</p><p id="p-0177" num="0175">acquire a projectile trajectory of the target object in the N image frames according to the at least one candidate trajectory; and</p><p id="p-0178" num="0176">acquire candidate positions in the projectile trajectory of the target object in the N image frames as projectile positions in the N image frames.</p><p id="p-0179" num="0177">In some embodiments, the projectile position acquisition unit may be further configured to:</p><p id="p-0180" num="0178">acquire, based on a projectile position in an i<sup>th </sup>image frame in the image frames and candidate positions of candidate object images in an (i+1)<sup>th </sup>image frame in the image frames, a projectile position in the (i+1)<sup>th </sup>image frame, i&#x2265;1, and i being an integer.</p><p id="p-0181" num="0179">In some embodiments, the projectile position acquisition unit may be further configured to:</p><p id="p-0182" num="0180">acquire the candidate positions of the candidate object images in the (i+1)<sup>th </sup>image frame; and</p><p id="p-0183" num="0181">acquire a candidate position closest to the projectile position in the i<sup>th </sup>image frame in the candidate positions as the projectile position in the (i+1)<sup>th </sup>image frame.</p><p id="p-0184" num="0182">In some embodiments, the projectile position acquisition unit may be further configured to:</p><p id="p-0185" num="0183">acquire a candidate position corresponding to a candidate object image of which an image area is the largest in each image frame as the projectile position in each image frame.</p><p id="p-0186" num="0184">In some embodiments, the first height acquisition module <b>804</b> may be configured to:</p><p id="p-0187" num="0185">acquire the projectile positions corresponding to the at least two image frames in the at least one group of image frames and a time interval between the at least two image frames in the at least one group of image frames; and</p><p id="p-0188" num="0186">calculate, based on the projectile positions corresponding to the at least two image frames in the at least one group of image frames and the time interval between the at least two image frames in the at least one group of image frames, the first height value corresponding to a case that the target object is thrown.</p><p id="p-0189" num="0187">In some embodiments, the throwing position acquisition module <b>805</b> may include:</p><p id="p-0190" num="0188">a second height acquisition unit, configured to acquire a second height value of the trajectory starting point position, the second height value being a height value of the trajectory starting point position;</p><p id="p-0191" num="0189">a third height acquisition unit, configured to acquire, based on the first height value and the second height value, a third height value corresponding to a case that the target object is thrown; and</p><p id="p-0192" num="0190">a first throwing position acquisition unit, configured to acquire the throwing position of the target object based on the third height value and a horizontal position of the trajectory starting point position.</p><p id="p-0193" num="0191">In some embodiments, the third height acquisition unit may be configured to:</p><p id="p-0194" num="0192">perform weighted summation on the first height value and the second height value based on a weight parameter, to obtain the third height value corresponding to a case that the target object is thrown.</p><p id="p-0195" num="0193">In some embodiments, the throwing position acquisition module may include:</p><p id="p-0196" num="0194">a second throwing position acquisition unit, configured to acquire the throwing position of the target object based on the first height value and a horizontal position of the trajectory starting point position.</p><p id="p-0197" num="0195">In some embodiments, the apparatus may further include:</p><p id="p-0198" num="0196">a projectile trajectory acquisition module, configured to connect the projectile positions in the image frames in chronological order, to obtain the projectile trajectory of the target object; and</p><p id="p-0199" num="0197">a warning information generation module, configured to generate warning information in a case that the projectile trajectory of the target object meets a warning condition.</p><p id="p-0200" num="0198">In some embodiments, the warning condition may include:</p><p id="p-0201" num="0199">the projectile trajectory of the target object being a vertically downward linear trajectory; or</p><p id="p-0202" num="0200">the projectile trajectory of the target object being a downward parabolic trajectory.</p><p id="p-0203" num="0201">Based on above, by using the throwing position acquisition apparatus provided in the embodiments, by acquiring image frames corresponding to a projectile video, and performing image processing on the image frames, a projectile position of a target object in each image frame is acquired, and a height value corresponding to a case that the target object is thrown in calculated according to at least one group of image frames in the image frames, then a throwing position of the target object is obtained with reference to a trajectory starting point position of a projectile trajectory of the target object. In the foregoing solution, the height value corresponding to a case that the target object is thrown is acquired based on at least one group of image frames in the image frames, and the throwing position of the target object is obtained according to the height value and the trajectory starting point position of the projectile trajectory, which reduces interference of external environmental factors on the acquired throwing position, and improves the accuracy of throwing position acquisition. In addition, in the process, influence of manual factors on the throwing position acquisition is reduced, thereby improving the efficiency of throwing position acquisition.</p><p id="p-0204" num="0202">A person of ordinary skill would understand that these &#x201c;modules&#x201d; or &#x201c;units&#x201d; in the foregoing embodiments could be implemented by hardware logic, computer software code, or a combination of both.</p><p id="p-0205" num="0203"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a structural block diagram of a computer device <b>900</b> according to some embodiments. The computer device <b>900</b> may be the video acquisition device in the system shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0206" num="0204">Generally, the computer device <b>900</b> includes a processor <b>901</b> and a memory <b>902</b>.</p><p id="p-0207" num="0205">The processor <b>901</b> may include one or more processing cores, for example, a 4-core processor or an 8-core processor. The processor <b>901</b> may be implemented by using at least one hardware form of digital signal processing (DSP), a field-programmable gate array (FPGA), and a programmable logic array (PLA). The processor <b>901</b> may also include a main processor and a coprocessor. In some embodiments, the processor <b>901</b> may be integrated with a graphics processing unit (GPU). The processor <b>901</b> may further include an artificial intelligence (AI) processor. The AI processor is configured to process a computing operation related to machine learning.</p><p id="p-0208" num="0206">The memory <b>902</b> may include one or more computer-readable storage media. The computer-readable storage medium may be non-transient. The memory <b>902</b> may further include a high-speed random access memory (RAM), and a non-volatile memory such as one or more magnetic disk storage devices and a flash storage device. In some embodiments, the non-transitory computer-readable storage medium in the memory <b>902</b> is configured to store at least one instruction, the at least one instruction being executed by the processor <b>901</b> to implement all or some of the operations in the foregoing method embodiments.</p><p id="p-0209" num="0207">In some embodiments, when the computer device is implemented as a video acquisition device, the computer device <b>900</b> may further include a peripheral interface <b>903</b> and at least one peripheral. The processor <b>901</b>, the memory <b>902</b>, and the peripheral interface <b>903</b> may be connected to each other by a bus or a signal cable. Each peripheral may be connected to the peripheral interface <b>903</b> by a bus, a signal cable, or a circuit board. The peripheral may include: at least one of a radio frequency (RF) circuit <b>904</b>, a display screen <b>905</b>, a camera component <b>906</b>, an audio circuit <b>907</b>, a positioning component <b>908</b>, and a power supply <b>909</b>.</p><p id="p-0210" num="0208">In some embodiments, the computer device <b>900</b> further includes one or more sensors <b>910</b>. The one or more sensors <b>910</b> include, but not limited to, an acceleration sensor <b>911</b>, a gyroscope sensor <b>912</b>, a pressure sensor <b>913</b>, a fingerprint sensor <b>914</b>, an optical sensor <b>915</b>, and a proximity sensor <b>916</b>.</p><p id="p-0211" num="0209">A person skilled in the art may understand that the structure shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref> does not constitute any limitation on the computer device <b>900</b>, and the computer device may include more or fewer components than those shown in the figure, or some components may be combined, or a different component deployment may be used.</p><p id="p-0212" num="0210"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a structural block diagram of a computer device <b>1000</b> according to some embodiments. The computer device may be implemented as the video processing device in the foregoing solution. The computer device <b>1000</b> includes a central processing unit (CPU) <b>1001</b>, a system memory <b>1004</b> including a random access memory (RAM) <b>1002</b> and a read-only memory (ROM) <b>1003</b>, and a system bus <b>1005</b> connecting the system memory <b>1004</b> to the CPU <b>1001</b>. The computer device <b>1000</b> further includes a mass storage device <b>1006</b> configured to store an operating system <b>1009</b>, an application <b>1010</b>, and another program module <b>1011</b>.</p><p id="p-0213" num="0211">The mass storage device <b>1006</b> is connected to the CPU <b>1001</b> by a mass storage controller (not shown) connected to the system bus <b>1005</b>. The mass storage device <b>1006</b> and an associated computer-readable medium provide non-volatile storage for the computer device <b>1000</b>. That is, the mass storage device <b>1006</b> may include a computer-readable medium (not shown) such as a hard disk or a compact disc ROM (CD-ROM) drive.</p><p id="p-0214" num="0212">In general, the computer-readable medium may include a computer storage medium and a communication medium. The computer-storage medium includes volatile and non-volatile media, and removable and non-removable media implemented by using any method or technology used for storing information such as computer-readable instructions, data structures, program modules, or other data. The computer storage medium includes a RAM, a ROM, an erasable programmable ROM (EPROM), an electrically erasable programmable ROM (EEPROM), a flash memory or another solid-state memory technology, a CD-ROM, a digital versatile disc (DVD) or another optical memory, a magnetic cassette, a magnetic tape, a magnetic disk memory, or another magnetic storage device. Certainly, a person skilled in the art may know that the computer storage medium is not limited to the foregoing types. The system memory <b>1004</b> and the mass storage device <b>1006</b> may be collectively referred to as a memory.</p><p id="p-0215" num="0213">According to some embodiments, the computer device <b>1000</b> may be further connected, through a network such as the Internet, to a remote computer on the network, and run. That is, the computer device <b>1000</b> may be connected to a network <b>1008</b> by a network interface unit <b>1007</b> connected to the system bus <b>1005</b>, or may be connected to another type of network or a remote computer system (not shown) by a network interface unit <b>1007</b>.</p><p id="p-0216" num="0214">The memory may further include at least one instruction, at least one program, a code set, or an instruction set, the at least one instruction, the at least one program, the code set, or the instruction set being stored in the memory. The CPU <b>1001</b> executes the at least one instruction, the at least one program, the code set, or the instruction set to implement all or some of the operations in the throwing position acquisition method shown in the foregoing embodiments.</p><p id="p-0217" num="0215">In some embodiments, a non-transitory computer-readable storage medium including instructions, for example, a memory including a computer program (instructions), is further provided, and the program (the instructions) may be executed by a processor of a computer device to complete the methods shown in the embodiments. For example, the non-transitory computer-readable storage medium may be a ROM, a RAM, a compact disc read-only memory (CD-ROM), a magnetic tape, a floppy disk, an optical data storage device, and the like.</p><p id="p-0218" num="0216">In some embodiments, a computer program product or a computer program is further provided. The computer program product or the computer program includes computer instructions, and the computer instructions are stored in a computer-readable storage medium. A processor of a computer device reads the computer instructions from the computer-readable storage medium and executes the computer instructions to cause the computer device to perform the method shown in the foregoing embodiments.</p><p id="p-0219" num="0217">After considering the specification and practicing the present disclosure, a person skilled in the art may easily conceive of other implementations of the disclosure. The disclosure is intended to cover any variations, uses or adaptation of the disclosure following the general principles of the disclosure, and includes the well-known knowledge and conventional technical means in the art and undisclosed in the disclosure. The specification and the embodiments are merely considered as examples, and the actual scope and the spirit of the disclosure are stated in the claims.</p><p id="p-0220" num="0218">It is to be understood that the disclosure is not limited to the precise structures described above and shown in the accompanying drawings, and various modifications and changes can be made without departing from the scope of the disclosure. The scope is subject only to the appended claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230005176A1-20230105-M00001.NB"><img id="EMI-M00001" he="5.25mm" wi="76.20mm" file="US20230005176A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A throwing position acquisition method, performed by a computer device, the throwing position acquisition method comprising:<claim-text>acquiring image frames of a target video, the target video being a video comprising a thrown target object;</claim-text><claim-text>acquiring a projectile position in each image frame, the projectile position being a position of an image corresponding to the target object in each image frame;</claim-text><claim-text>acquiring a trajectory starting point position of the target object based on the projectile position in each image frame, the trajectory starting point position being a starting point position of a projectile trajectory obtained based on the projectile position in each image frame;</claim-text><claim-text>acquiring, based on projectile positions corresponding to at least one group of image frames in the image frames, a first height value corresponding to a case that the target object is thrown, the at least one group of image frames respectively comprising at least two image frames; and</claim-text><claim-text>acquiring a throwing position of the target object based on the first height value and the trajectory starting point position of the target object.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The throwing position acquisition method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the acquiring a projectile position in each image frame comprises:<claim-text>acquiring candidate object images in each image frame, the candidate object images being object images in a foreground image of each image frame; and</claim-text><claim-text>acquiring the projectile positions in the image frames based on candidate positions in each image frame, the candidate positions being positions of the candidate object images in each image frame in the corresponding image frame.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The throwing position acquisition method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the acquiring candidate object images in each image frame comprises:<claim-text>acquiring the foreground image in each image frame based on each image frame in the target video; and</claim-text><claim-text>acquiring object images of which image areas are greater than an area threshold in the foreground image in each image frame as the candidate object images in each image frame.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The throwing position acquisition method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the acquiring the projectile positions in the image frames based on candidate positions in each image frame comprises:<claim-text>connecting, in chronological order, candidate positions between which target distances are less than a distance threshold in N consecutive image frames comprised in the image frames, to obtain at least one candidate trajectory, N&#x2265;2, N being an integer, and the target distance being a distance between candidate positions in two adjacent image frames sorted in chronological order in the N consecutive image frames;</claim-text><claim-text>acquiring a projectile trajectory of the target object in the N image frames according to the at least one candidate trajectory; and</claim-text><claim-text>acquiring candidate positions in the projectile trajectory of the target object in the N image frames as projectile positions in the N image frames.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The throwing position acquisition method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the acquiring the projectile positions in the image frames based on candidate positions in each image frame comprises:<claim-text>acquiring, based on a projectile position in an i<sup>th </sup>image frame in the image frames and candidate positions of candidate object images in an (i+1)<sup>th </sup>image frame in the image frames, a projectile position in the (i+1)<sup>th </sup>image frame, i&#x2265;1, and i being an integer.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The throwing position acquisition method according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the acquiring, based on a projectile position in an i<sup>th </sup>image frame in the image frames and candidate positions of candidate object images in an (i+1)<sup>th </sup>image frame in the image frames, a projectile position in the (i+1)<sup>th </sup>image frame comprises:<claim-text>acquiring the candidate positions of the candidate object images in the (i+1)<sup>th </sup>image frame; and</claim-text><claim-text>acquiring a candidate position closest to the projectile position in the i<sup>th </sup>image frame in the candidate positions as the projectile position in the (i+1)<sup>th </sup>image frame.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The throwing position acquisition method according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the acquiring the projectile positions in the image frames based on candidate positions in each image frame comprises:<claim-text>acquiring a candidate position corresponding to a candidate object image of which an image area is the largest in each image frame as the projectile position in each image frame.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The throwing position acquisition method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the acquiring, based on projectile positions corresponding to at least one group of image frames in the image frames, a first height value corresponding to a case that the target object is thrown comprises:<claim-text>acquiring the projectile positions corresponding to the at least two image frames in the at least one group of image frames and a time interval between the at least two image frames in the at least one group of image frames; and</claim-text><claim-text>calculating, based on the projectile positions corresponding to the at least two image frames in the at least one group of image frames and the time interval between the at least two image frames in the at least one group of image frames, the first height value corresponding to a case that the target object is thrown.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The throwing position acquisition method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the acquiring a throwing position of the target object based on the first height value and the trajectory starting point position of the target object comprises:<claim-text>acquiring a second height value of the trajectory starting point position, the second height value being a height value of the trajectory starting point position;</claim-text><claim-text>acquiring, based on the first height value and the second height value, a third height value corresponding to a case that the target object is thrown; and</claim-text><claim-text>acquiring the throwing position of the target object based on the third height value and a horizontal position of the trajectory starting point position.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The throwing position acquisition method according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the acquiring, based on the first height value and the second height value, a third height value corresponding to a case that the target object is thrown comprises:<claim-text>performing weighted summation on the first height value and the second height value based on a weight parameter, to obtain the third height value corresponding to a case that the target object is thrown.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The throwing position acquisition method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the acquiring a throwing position of the target object based on the first height value and the trajectory starting point position of the target object comprises:<claim-text>acquiring the throwing position of the target object based on the first height value and a horizontal position of the trajectory starting point position.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The throwing position acquisition method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>connecting the projectile positions in the image frames in chronological order, to obtain the projectile trajectory of the target object; and</claim-text><claim-text>generating warning information in a case that the projectile trajectory of the target object meets a warning condition.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The throwing position acquisition method according to <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the warning condition comprises:<claim-text>the projectile trajectory of the target object being a vertically downward linear trajectory; or</claim-text><claim-text>the projectile trajectory of the target object being a downward parabolic trajectory.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. A throwing position acquisition apparatus, comprising:<claim-text>at least one memory configured to store program code; and</claim-text><claim-text>at least one processor configured to read the program code and operate as instructed by the program code, the program code comprising:</claim-text><claim-text>image frame acquisition code configured to cause the at least one processor to acquire image frames of a target video, the target video being a video comprising a thrown target object;</claim-text><claim-text>projectile position acquisition code configured to acquire a projectile position in each image frame, the projectile position being a position of an image corresponding to the target object in each image frame;</claim-text><claim-text>trajectory starting point acquisition code configured to cause the at least one processor to acquire a trajectory starting point position of the target object based on the projectile position in each image frame, the trajectory starting point position being a starting point position of a projectile trajectory obtained based on the projectile position in each image frame;</claim-text><claim-text>first height acquisition code configured to cause the at least one processor to acquire, based on projectile positions corresponding to at least one group of image frames in the image frames, a first height value corresponding to a case that the target object is thrown, the at least one group of image frames respectively comprising at least two image frames; and</claim-text><claim-text>throwing position acquisition code configured to cause the at least one processor to acquire a throwing position of the target object based on the first height value and the trajectory starting point position of the target object.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The throwing position acquisition apparatus according to <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the projectile position acquisition code comprises:<claim-text>candidate image acquisition subcode configured to cause the at least one processor to acquire candidate object images in each image frame, the candidate object images being object images in a foreground image of each image frame; and</claim-text><claim-text>projectile position acquisition subcode configured to cause the at least one processor to acquire the projectile positions in the image frames based on candidate positions in each image frame, the candidate positions being positions of the candidate object images in each image frame in the corresponding image frame.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The throwing position acquisition apparatus according to <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the projectile position acquisition code comprises:<claim-text>foreground image acquisition subcode configured to cause the at least one processor to acquire the foreground image in each image frame based on each image frame in the target video; and</claim-text><claim-text>threshold judgment subcode configured to cause the at least one processor to acquire object images of which image areas are greater than an area threshold in the foreground image in each image frame as the candidate object images in each image frame.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The throwing position acquisition apparatus according to <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the projectile position acquisition code is further configured to cause the at least one processor to:<claim-text>connect, in chronological order, candidate positions between which target distances are less than a distance threshold in N consecutive image frames comprised in the image frames, to obtain at least one candidate trajectory, N&#x2265;2, N being an integer, and the target distance being a distance between candidate positions in two adjacent image frames sorted in chronological order in the N consecutive image frames;</claim-text><claim-text>acquire a projectile trajectory of the target object in the N image frames according to the at least one candidate trajectory; and</claim-text><claim-text>acquire candidate positions in the projectile trajectory of the target object in the N image frames as projectile positions in the N image frames.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. A non-transitory computer-readable storage medium, storing computer code that when executed by at least one processor causes the at least one processor to:<claim-text>acquire image frames of a target video, the target video being a video comprising a thrown target object;</claim-text><claim-text>acquire a projectile position in each image frame, the projectile position being a position of an image corresponding to the target object in each image frame;</claim-text><claim-text>acquire a trajectory starting point position of the target object based on the projectile position in each image frame, the trajectory starting point position being a starting point position of a projectile trajectory obtained based on the projectile position in each image frame;</claim-text><claim-text>acquire, based on projectile positions corresponding to at least one group of image frames in the image frames, a first height value corresponding to a case that the target object is thrown, the at least one group of image frames respectively comprising at least two image frames; and</claim-text><claim-text>acquire a throwing position of the target object based on the first height value and the trajectory starting point position of the target object.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The non-transitory computer-readable storage medium according to <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the acquire a projectile position in each image frame comprises:<claim-text>acquiring candidate object images in each image frame, the candidate object images being object images in a foreground image of each image frame; and</claim-text><claim-text>acquiring the projectile positions in the image frames based on candidate positions in each image frame, the candidate positions being positions of the candidate object images in each image frame in the corresponding image frame.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The non-transitory computer-readable storage medium according to <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein the acquiring candidate object images in each image frame comprises:<claim-text>acquiring the foreground image in each image frame based on each image frame in the target video; and</claim-text><claim-text>acquiring object images of which image areas are greater than an area threshold in the foreground image in each image frame as the candidate object images in each image frame.</claim-text></claim-text></claim></claims></us-patent-application>