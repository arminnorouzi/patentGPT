<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005161A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005161</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17793993</doc-number><date>20200910</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>JP</country><doc-number>2020-027693</doc-number><date>20200221</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>246</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>70</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>74</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>248</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>70</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>761</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30196</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">INFORMATION PROCESSING APPARATUS AND DETERMINATION RESULT OUTPUT METHOD</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>HITACHI HIGH-TECH CORPORATION</orgname><address><city>Tokyo</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>TOYOMURA</last-name><first-name>Takashi</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>TANAKA</last-name><first-name>Takeshi</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>FUKUI</last-name><first-name>Daisuke</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>NAKAGAWA</last-name><first-name>Hiromitsu</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor></inventors></us-parties><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/JP2020/034224</doc-number><date>20200910</date></document-id><us-371c12-date><date>20220720</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">In a motion analysis apparatus <b>101,</b> a data input unit <b>205</b> acquires a first imaging result and a second imaging result. in the motion analysis apparatus <b>101,</b> a skeleton recognition unit <b>206</b> recognizes skeleton positions of a subject using the first imaging. result acquired by the data input unit <b>205,</b> and recognizes skeleton positions of the subject using the second imaging result acquired by the data input unit <b>205.</b> A motion period extraction unit <b>403</b> extracts a period from a start of a motion to an end of the motion as a range of data for comparing skeleton feature points recognized by the skeleton recognition unit <b>206.</b> The similarity calculation unit <b>401</b> compares skeleton feature points recognized for an input from a depth camera with skeleton feature points recognized for an input from an RGB camera to calculate similarities, and outputs a determination result based on the similarities.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="73.41mm" wi="137.24mm" file="US20230005161A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="82.55mm" wi="150.11mm" file="US20230005161A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="147.15mm" wi="162.22mm" file="US20230005161A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="116.59mm" wi="159.09mm" file="US20230005161A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="146.90mm" wi="162.14mm" file="US20230005161A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="106.60mm" wi="155.11mm" file="US20230005161A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="212.51mm" wi="133.69mm" file="US20230005161A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="99.65mm" wi="162.22mm" file="US20230005161A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="82.89mm" wi="152.82mm" file="US20230005161A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="116.59mm" wi="159.00mm" file="US20230005161A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="152.74mm" wi="162.05mm" file="US20230005161A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="126.15mm" wi="147.49mm" file="US20230005161A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="210.90mm" wi="151.30mm" file="US20230005161A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="191.85mm" wi="131.06mm" file="US20230005161A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="146.98mm" wi="162.05mm" file="US20230005161A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="146.22mm" wi="126.15mm" file="US20230005161A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0002" num="0001">The present invention relates to an information processing apparatus and a determination result output method.</p><heading id="h-0002" level="1">BACKGROUND ART</heading><p id="p-0003" num="0002">An average life of Japanese people continues to grow and exceeds 80 years for both men and women, and a health life indicating a period in which there is no limitation in daily life is 72 years old for men and 75 years old for women, which are different from the average life.</p><p id="p-0004" num="0003">This is because an elderly person is in a nursing-care-required state such as bedridden. A most frequent cause of a need for nursing care is movement organ disorder. A state in which a movement function is deteriorated due to the movement organ disorder is called locomotive syndrome, and in recent years, attention has been paid to prevention of the locomotive syndrome.</p><p id="p-0005" num="0004">It is known that a sign of the locomotive syndrome appears in walking, and an expert such as a doctor diagnoses the walking of a patient from. various angles.</p><p id="p-0006" num="0005">Meanwhile, with progress of wearable acceleration sensors and improvements in camera image analysis technology, an environment in which a motion of a person can be sensed and digitally analyzed has been established. With regard to walking analysis, attempts have been made to improve an efficiency of the diagnosis performed by the expert spending time in the related art by quantifying a walking pattern by attaching the acceleration sensor to hands or feet, or by tracking behaviors of the hands or feet by camera image recognition.</p><p id="p-0007" num="0006">For example, PTL 1 discloses a method of evaluating suitability of swings by skeleton-recognizing a motion such as golf swing using a depth camera such as &#x201c;Kinect (registered trademark)&#x201d; and comparing an index extracted from movements of skeleton feature points with reference data.</p><p id="p-0008" num="0007">In addition, there is disclosed a method in which a walking identification model is constructed in advance based on three-dimensional skeleton coordinates, and conversion is added such that two-dimensional skeleton coordinates captured at any angle can be input at a time of identification.</p><p id="p-0009" num="0008">PTL 2 discloses an apparatus related to key point data capable of obtaining information related to a predetermined posture in a three-dimensional space even when the key point data such as skeleton joint data is given as two-dimensional data.</p><heading id="h-0003" level="1">CITATION LIST</heading><heading id="h-0004" level="1">Patent Literature</heading><p id="p-0010" num="0009">PTL 1: JP-A7-2014-188146</p><p id="p-0011" num="0010">PTL 2: JP A 2019-96113</p><heading id="h-0005" level="1">SUMMARY OF INVENTION</heading><heading id="h-0006" level="1">Technical Problem</heading><p id="p-0012" num="0011">In recent years, as a sensing device has been commoditized, there has been an increasing need for flexibly constructing a system with a low-cost device having the same function as the acceleration sensor or a camera for cost reduction.</p><p id="p-0013" num="0012">In PTL 2, when a device is changed, conversion is performed so as to be close to an original condition, and thus the device can be used as an input as it is, but when an input condition is changed, it is originally necessary to reconstruct the model, and the method of PTL 2 has a limit in identification accuracy.</p><p id="p-0014" num="0013">That is, in order to ensure sufficient accuracy, it is necessary to analyze new input data and reconstruct the model, and thus there is a problem that data analysis cannot be efficiently performed.</p><p id="p-0015" num="0014">Therefore, an object of the invention is to enable efficient data analysis.</p><heading id="h-0007" level="1">Solution to Problem</heading><p id="p-0016" num="0015">An information processing apparatus for processing information acquired from a device capable of capturing an image of a user according to a representative embodiment of the invention includes: a first imaging result acquisition unit configured to acquire a first imaging result that is a result of imaging performed by a first device configured to capture an image of a motion state of the user; a second imaging result acquisition unit configured to acquire a second imaging result that is a result of imaging performed by a second device configured to capture an image of the motion state of the user; a skeleton recognition unit configured to recognize a skeleton position of the user by using the first imaging result acquired by the first imaging result acquisition unit and recognize a skeleton position of the user by using the second imaging result acquired by the second imaging result acquisition unit; a motion period specifying unit. configured to specify a predetermined motion period of the user based on a result obtained by the recognition performed by the skeleton recognition unit; a similarity calculation unit configured to calculate a similarity between a change in the skeleton position of the user recognized. by the skeleton recognition unit using the first imaging result and a change in the skeleton position of the user recognized by the skeleton recognition unit using the second imaging result in the motion period specified by the motion period specifying unit; and a determination result output. unit configured to output. a determination result based on the similarity calculated by the similarity calculation unit.</p><heading id="h-0008" level="1">Advantageous Effects of Invention</heading><p id="p-0017" num="0016">According to the invention, data analysis can be efficiently performed.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0009" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram showing an outline of a system configuration including a motion analysis apparatus.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a diagram showing an example of a configuration of the motion analysis apparatus according to the present embodiment.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a schematic diagram of a system including the motion analysis apparatus that compares imaging results obtained by a plurality of skeleton measurement units.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram showing an example of the configuration of the motion analysis apparatus according to the present embodiment.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram showing an example of an input screen for inputting a correspondence relationship between skeleton feature points.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flowchart showing a processing procedure for calculating a similarity of the skeleton feature points in a similarity calculation unit.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a diagram showing an example of a screen configuration for presenting the similarity to a user.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a diagram showing an example of a screen example showing a list of correspondences between the skeleton feature points and the similarities.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a diagram showing a configuration of a system that performs measurement using a motion analysis apparatus according to a second embodiment.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a diagram showing an example of a configuration of the motion analysis apparatus according to the present embodiment according to a second embodiment.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a diagram showing an input screen of walking types.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a diagram showing an example of a screen of a determination result.</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a flowchart in which the motion analysis apparatus estimates a correspondence relationship between skeleton feature points of a first measurement unit and a second measurement unit, and generates a skeleton correspondence table.</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a diagram showing a configuration of a motion analysis apparatus according to a fourth embodiment.</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a flowchart of specifying similar skeleton feature points of a first measurement unit with respect to skeleton feature points (target feature points) of the first measurement unit which do not exist in a second measurement unit.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0010" level="1">DESCRIPTION OF EMBODIMENTS</heading><p id="p-0033" num="0032">Hereinafter, embodiments of the invention will described with reference to the drawings.</p><heading id="h-0011" level="1">First Embodiment</heading><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram showing an outline of a system configuration including a motion analysis apparatus <b>101</b> according the present embodiment. The motion analysis apparatus <b>101</b> is an information processing apparatus that processes information acquired from a device (for example, a depth camera <b>102</b>), and is a server apparatus or the like. The motion analysis apparatus <b>101</b> is connected to the depth camera <b>102</b> (first device) by a USB cable or the like. According the motion analysis apparatus <b>101</b> can acquire the information from the depth camera <b>102</b>.</p><p id="p-0035" num="0034">The motion analysis apparatus <b>101</b> may transmit and receive information to and from the depth camera <b>102</b> via a wireless or wired network.</p><p id="p-0036" num="0035">The depth camera <b>102</b> is a known depth camera, generates information (depth information) in an X-axis direction, a Y-axis direction, and a Z-axis direction of an imaging target as a result of imaging, and stores the depth information.</p><p id="p-0037" num="0036">The depth camera <b>102</b> can capture an image of a state in which a subject <b>103</b> (user) is walking (for example, a state in which the subject <b>103</b> is walking toward the depth camera <b>102</b>). That is, the depth camera <b>102</b> is a device capable of capturing an image of the user, and is a device capable of capturing an image of a motion state of the user.</p><p id="p-0038" num="0037">The motion analysis apparatus <b>101</b> is an apparatus that acquires information from a device (such as the depth camera <b>102</b>) capable of capturing an image of the user and processes the information. Specifically, the motion analysis apparatus <b>101</b> acquires the depth information based on the result of imaging the state in which the subject <b>103</b> is walking. The motion analysis apparatus <b>101</b> specifies skeleton positions (for example, a skeleton position of a left elbow) of the subject <b>103</b> from the depth information, specifies changes in the specified skeleton positions, and determines whether the subject <b>103</b> is in locomotive syndrome based on the changes. In this way, the motion analysis apparatus <b>101</b> measures how the subject <b>103</b> walks.</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a diagram showing an example of a configuration of the motion analysis apparatus <b>101</b> according to the present embodiment. The motion analysis apparatus <b>101</b> includes an operation input unit <b>201</b>, a display unit <b>202</b>, a control unit <b>203</b>, a memory <b>204</b>, a data input unit <b>205</b>, a skeleton recognition unit <b>206</b>, a feature generation unit <b>207</b>, a model <b>208</b>, and an identification unit <b>209</b>, and the units are connected to one another via a common bus (including a data bus and an address bus).</p><p id="p-0040" num="0039">The operation input unit <b>201</b> is a part that receives an instruction from a user (for example, an administrator of the motion analysis apparatus <b>101</b>) by a mouse operation, a touch panel operation, or the like.</p><p id="p-0041" num="0040">The display unit <b>202</b> is a display or the like, and is a part that displays and outputs various types of information. The control unit <b>203</b> is a central processing unit (CPU) or the like, and is a part that controls operations of each of the units.</p><p id="p-0042" num="0041">The memory <b>204</b> is a part that stores various types of information, such as a memory. The memory <b>204</b> temporarily stores, for example, data related to the operation control performed by the control unit <b>203</b>.</p><p id="p-0043" num="0042">The data input unit <b>205</b> is a part that acquires, from the depth camera <b>102</b>, the depth information or image data that is the result of imaging performed by the depth camera <b>102</b> that captures the image of the motion state of the subject <b>103</b>.</p><p id="p-0044" num="0043">The skeleton recognition unit <b>206</b> is a part that recognizes skeleton feature point positions of a human body (subject <b>103</b>) from the depth information. That is, the skeleton recognition unit <b>206</b> is a part that recognizes the skeleton positions of the subject <b>103</b> using the depth information acquired by the data input unit <b>205</b>.</p><p id="p-0045" num="0044">The skeleton recognition unit <b>206</b> extracts position information on each of the skeleton positions (information on the skeleton feature point positions) specified from the depth information.</p><p id="p-0046" num="0045">The feature generation unit <b>207</b> is a part that extracts indexes (features) designed in advance from temporal transition of the skeleton feature point positions (skeleton feature point coordinates). That is, the feature generation unit <b>207</b> is a part that generates the features (for example, a left stride) of the skeleton based on the skeleton positions recognized by the skeleton recognition unit <b>206</b>.</p><p id="p-0047" num="0046">The model <b>208</b> is a part that stores a model in which a correspondence relationship is learned in advance in order to identify walking types based on a condition of the feature. That is, the model <b>208</b> is a database that stores model information in which a feature of a motion of a skeleton portion and a walking state corresponding to the feature are defined, and functions as a storage unit.</p><p id="p-0048" num="0047">The identification unit <b>209</b> is a part that identifies the walking type of the subject <b>103</b> using the feature generated by the feature generation unit <b>207</b> and the model stored in the model <b>208</b>.</p><p id="p-0049" num="0048">In this way, the motion analysis apparatus <b>101</b> generates the feature such as a stride and a walking speed based on coordinate changes in skeletons when the subject. <b>103</b> walks, and identifies the walking type by combining a plurality of features.</p><p id="p-0050" num="0049">Here, the walking type corresponds to a classification of walking that is clinically regarded as abnormal walking such as antalgic gait and steppage gait, a classification of walking for each age such as young people and elderly people, and the like.</p><p id="p-0051" num="0050">These walking types are determined by an expert such as a doctor looking at features of a way of walking, such as the stride and the walking speed.</p><p id="p-0052" num="0051">The motion analysis apparatus <b>101</b> quantifies these features based on measurement data of the depth camera <b>102</b>, and performs identification using a model in which relationships with the walking types are learned by machine learning or the like.</p><p id="p-0053" num="0052">Next, a system that compares depth information obtained from a result of a plurality of devices (skeleton measurement units) capturing images of the same user will be described with reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a schematic diagram of the system including the motion analysis apparatus that compares imaging results obtained by the plurality of skeleton measurement units (devices).</p><p id="p-0055" num="0054">As shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, in addition to the motion analysis apparatus <b>101</b> and the depth camera <b>102</b> (first device) described. with. reference to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the system further includes an RGB camera <b>301</b> and an RGB camera <b>302</b> (second device). The depth camera <b>102</b> functions as a first skeleton measurement unit, and the RGB camera <b>301</b> and the RGB camera <b>302</b> function as a second skeleton measurement unit.</p><p id="p-0056" num="0055">The motion analysis apparatus <b>101</b> is connected to the RGB camera <b>301</b> and the RGB camera <b>302</b> by a USB cable or the like. Further, the motion analysis apparatus <b>101</b> may transmit and receive information to and from the RGB camera <b>301</b> and the RGB camera <b>302</b> via a wireless or wired network.</p><p id="p-0057" num="0056">The RGB camera <b>301</b> and the RGB camera <b>302</b> are stereo cameras, and can capture the image of the motion state of the user.</p><p id="p-0058" num="0057">A method of estimating a depth by using a stereo camera including the RGB camera <b>301</b> and the RGB camera <b>302</b> is a method of estimating a depth of an object by using a parallax of two-dimensional images captured from two different directions, which is a known method.</p><p id="p-0059" num="0058">In the system configuration shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, skeleton coordinate recognition by the depth camera <b>102</b> and skeleton coordinate recognition by the RGB camera <b>301</b> and the RGB camera <b>302</b> are performed for the same walking of the same subject <b>103</b>, so that an accurate comparison can be made.</p><p id="p-0060" num="0059">In motions such as walking, a movement differs depending on the subject <b>103</b>, and even for the same subject <b>103</b>, movements of the skeletons are slightly different every time the subject <b>103</b> walks, and therefore, it is desirable to simultaneously measure the same walking of the same subject <b>103</b> to perform the comparison and evaluation.</p><p id="p-0061" num="0060">The motion analysis apparatus <b>101</b> acquires the depth information from the depth camera <b>102</b>, and acquires the image data from the RGB camera <b>301</b> and the RGB camera <b>302</b>. The motion analysis apparatus <b>101</b> specifies a moving state of the skeletons of the subject <b>103</b> based on the depth information acquired from the depth camera <b>102</b>.</p><p id="p-0062" num="0061">Further, the motion analysis apparatus <b>101</b> specifies a moving state (position change) of the skeletons of the subject <b>103</b> based on the image data acquired from the RGB camera <b>301</b> and the RGB camera <b>302</b>. Then, the motion analysis apparatus <b>101</b> determines whether the respective moving states of the skeletons are similar to each other. Accordingly, the motion analysis apparatus <b>101</b> can clarify whether the motion state may be determined using the same model for the results of the imaging performed by two skeleton coordinate acquisition units (the first skeleton measurement unit and the second skeleton measurement unit).</p><p id="p-0063" num="0062">Next, functional details of the motion analysis apparatus <b>101</b> will. be described with reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>. <figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram showing an example or the configuration of ;ho motion analysis apparatus <b>101</b> according to the present embodiment.</p><p id="p-0064" num="0063">As shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, in addition to the configuration of the motion analysis apparatus <b>101</b> shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the motion analysis apparatus <b>101</b> includes a similarity calculation unit <b>401</b>, a skeleton correspondence table <b>402</b>, and a motion period extraction unit <b>403</b>.</p><p id="p-0065" num="0064">The data input unit <b>205</b> acquires the image data from the RGB camera <b>301</b> and the RGB camera <b>302</b>. That is, the data input unit <b>205</b> acquires a second imaging result that is a result of imaging performed by the second device.</p><p id="p-0066" num="0065">The skeleton recognition unit <b>206</b> recognizes the skeleton positions of the subject <b>103</b> using the second imaging result (image data acquired from the RGB camera <b>301</b> and the RGB camera <b>302</b>) acquired by the data input unit <b>205</b>. Specifically, the skeleton recognition unit <b>206</b> extracts position information on each of the skeleton positions (information on skeleton feature point positions) specified from the image data acquired from the RGB camera <b>301</b> and the RGB camera <b>302</b>.</p><p id="p-0067" num="0066">The similarity calculation unit <b>401</b> compares skeleton feature points recognized for an input from the depth. camera <b>102</b> with skeleton feature points recognized for an input from the RGB cameras <b>301</b> and <b>302</b> to calculate similarities.</p><p id="p-0068" num="0067">In addition, the similarity calculation unit <b>401</b> outputs a determination result based on the similarities calculated itself to the display unit <b>202</b>.</p><p id="p-0069" num="0068">In order to compare the skeleton feature points of first measurement unit (depth camera <b>102</b>) and a second measurement unit (RGB camera <b>301</b> and RGB camera <b>302</b>), it is necessary to know in advance a correspondence relationship between the skeleton feature points recognized by the first and second measurement units.</p><p id="p-0070" num="0069"><figref idref="DRAWINGS">FIG. <b>5</b></figref> shows an input screen for allowing the user to designate this correspondence relationship. <figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram showing an example of the input screen for inputting the correspondence relationship between the skeleton feature points. In an image <b>501</b>, positions of the recognized skeleton feature points are superimposed on a subject image acquired by the first measurement unit and displayed by black circles.</p><p id="p-0071" num="0070">Similarly, in an image <b>502</b>, positions of the recognized skeleton feature points are superimposed on a subject image acquired by the second measurement unit.</p><p id="p-0072" num="0071">The data input unit <b>205</b> acquires the image data and the depth data from the depth camera <b>102</b>, the RGB camera <b>301</b>, and the RGB camera <b>302</b>, and the skeleton recognition unit <b>206</b> extracts the position information on each of the skeleton positions and then outputs the screen shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref> to the display unit <b>202</b>.</p><p id="p-0073" num="0072">When the user selects one skeleton feature point from each the image <b>501</b> and the image <b>502</b>, the skeleton recognition unit <b>206</b> associates and stores the skeleton feature points. At this time, a name <b>503</b> of the skeleton feature points such as a &#x201c;left elbow&#x201d; may be given. All the skeleton feature points are stored in association with each other to form the skeleton correspondence table <b>402</b>.</p><p id="p-0074" num="0073">As described above, the skeleton recognition unit <b>206</b> specifies the correspondence relationships between the skeleton positions of the user recognized from the image data or the depth information of the depth camera <b>102</b> acquired by the data input unit <b>205</b> and the skeleton positions of the user recognized from the image data of the RGB camera <b>301</b> and the RGB camera <b>302</b> acquired by the data input unit <b>205</b>.</p><p id="p-0075" num="0074">Accordingly, the motion analysis apparatus <b>101</b> can compare the same or corresponding skeleton portion by associating the skeleton positions specified from the information acquired from both devices.</p><p id="p-0076" num="0075">The skeleton recognition unit <b>206</b> can recognize the skeleton positions more accurately by recognizing the skeleton positions according to a user operation on the screen as shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0077" num="0076">The motion period extraction unit <b>403</b> extracts period from a start of a motion to an end of the motion as a range of data for comparing the skeleton feature points recognized by the skeleton recognition unit <b>206</b>. In a case of walking, for example, one cycle of the walking from landing of a right foot to next landing of the right foot again is set as a comparison range. In order to extract one walking cycle, for example, a Y-axis coordinate value of a right ankle may be observed, and the start and the end may be determined by regarding a timing at which the Y-axis coordinate value becomes a minimum value as landing.</p><p id="p-0078" num="0077">Specifically, the motion period extraction unit <b>403</b> specifies one walking cycle from the landing of the right foot to the landing of the right foot based on changes in the skeleton feature point positions based on the depth information of the depth camera <b>102</b> and recognized by the skeleton recognition unit <b>206</b>. The motion period extraction unit <b>403</b> specifies one walking cycle from the landing of the right foot to the landing of the right foot based on changes in the skeleton feature point positions based on the image data of the RGB camera <b>301</b> and the RGB camera <b>302</b> and recognized by the skeleton recognition unit <b>206</b>.</p><p id="p-0079" num="0078">In this way, the motion period extraction unit <b>403</b> specifies a predetermined motion period of the user based on results recognized by the skeleton recognition unit <b>206</b>. Further, the motion period extraction unit <b>403</b> specifies the walking cycle of the subject <b>103</b> based on the results recognized by the skeleton recognition unit <b>206</b>. Accordingly, the motion analysis apparatus <b>101</b> can perform the comparison during the same period of the motion.</p><p id="p-0080" num="0079">The similarity calculation unit <b>401</b> calculates the similarity between the change in the skeleton positions of the subject <b>103</b> recognized by the skeleton recognition unit <b>206</b> using a first imaging result and the change in the skeleton positions of the subject <b>103</b> recognized by the skeleton recognition unit <b>206</b> using a second imaging result in the motion period specified by the motion period extraction unit <b>403</b>.</p><p id="p-0081" num="0080">Here, a processing procedure in which the similarity calculation unit <b>401</b> calculates the similar the skeleton feature points will be described with reference to a flowchart shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>.</p><p id="p-0082" num="0081"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flowchart of the processing procedure for calculating the similarity of the skeleton feature points in the similarity calculation unit <b>401</b>.</p><p id="p-0083" num="0082">In step S<b>01</b>, the similarity calculation unit <b>401</b> performs processing of selecting any skeleton. For example, the similarity calculation unit <b>401</b> selects one skeleton to be subjected to the calculation of the similarity from a left shoulder, a right shoulder, a left ankle, right ankle, and the like. Here, it is assumed that the left ankle is selected, and the following processing will be described.</p><p id="p-0084" num="0083">In step S<b>02</b>, the similarity calculation unit <b>401</b> acquires coordinate data of the skeleton feature point selected in step S<b>01</b> from each of the first measurement unit and the second measurement unit. The coordinate data of the skeleton feature point (data of the first measurement unit and the second measurement unit) referred to here is data extracted and outputted by the skeleton recognition unit <b>206</b>.</p><p id="p-0085" num="0084">A skeleton coordinate is recognized three-dimensionally, and includes data acquired at specific sampling intervals for each of an X axis, a Y axis, and a Z axis. Here, for example, the following processing will be described on an assumption that the X axis is selected.</p><p id="p-0086" num="0085">Step S<b>03</b> is processing of acquiring data of one specific coordinate axis from the skeleton coordinate data acquired in step S<b>02</b>. Specifically, the similarity calculation unit <b>401</b> selects one of the X axis, the Y axis, and the Z axis, and acquires skeleton data of the coordinate axis from the data that is extracted and outputted by the skeleton recognition unit <b>206</b> and corresponds to the first measurement unit and the second measurement unit.</p><p id="p-0087" num="0086">Step S<b>04</b> is processing of calculating a cross-correlation function by using the data of the first measurement unit and the second measurement unit for the axis selected in step S<b>03</b>. Here, the cross-correlation function is calculation used to confirm a similarity between two signals, and is obtained by multiplying a first signal by a second signal shifted by a time t and integrating the first signal and the second signal. The shift time t is plotted on a horizontal axis, and an integral value is plotted on a vertical axis to obtain the cross-correlation function.</p><p id="p-0088" num="0087">Step S<b>05</b> is processing of determining whether the calculation of the cross-correlation function is ended for all the coordinate axes. When the calculation is not ended, a process returns to step S<b>03</b>, and a new axis is selected. Specifically, the similarity calculation unit <b>401</b> selects one of the Y axis and the Z axis except for the X axis for which the cross-correlation function is calculated earlier. When the calculation is ended for all the axes, the process proceeds to step <b>306</b>.</p><p id="p-0089" num="0088">Step S<b>06</b> is processing of calculating a similarity of the left ankle by using the cross-correlation function calculated for each of the three axes of the left ankle. The similarity is obtained by adding the three cross-correlation functions on the same time axis and taking a maximum value thereof . When the subject <b>103</b> walks toward the depth camera <b>102</b>, a Z-axis coordinate value of the left ankle changes in a large range, and values that can be taken by coordinate values of the X axis and the Y axis are smaller than that the Z axis. When the cross-correlation function calculated and added as it is, an influence of the Z axis increases depending on a magnitude of an original value.</p><p id="p-0090" num="0089">In order to prevent this, it is desirable to normalize a coordinate value obtained bar the first measurement unit and a coordinate value obtained by the second measurement unit so as to have a minimum value of <b>0</b> and a maximum value of 1, respectively.</p><p id="p-0091" num="0090">Step S<b>07</b> is processing of determining the similarity calculated in step S<b>06</b> with a specific threshold. When the similarity is greater than the specific threshold, the similarity calculation unit <b>401</b> determines that characteristics of the first measurement unit and the second measurement unit are the same for the left ankle.</p><p id="p-0092" num="0091">Conversely, when the similarity is smaller than the threshold (or equal to or smaller than the threshold), the similarity calculation unit <b>401</b> determines that the characteristics of the first measurement unit and the second measurement unit are different from each other. The threshold may be determined in advance by a designer as an initial setting, or may be freely set to a value by the user.</p><p id="p-0093" num="0092">Step S<b>08</b> is processing of determining whether the similarity calculation is ended for all the skeleton feature points. When the similarity calculation is not ended, the process returns to step S<b>01</b> and the similarity calculation unit <b>401</b> selects a new skeleton feature point. Specifically, the skeleton feature points such as the right ankle are selected except for the left ankle for which the similarity is calculated earlier. When the calculation, performed by the similarity calculation unit <b>401</b>, for all the skeleton feature points is ended, the process proceeds to step S<b>09</b>. It is not always necessary to calculate the similarity for all the skeletons, and the skeleton feature points to be subjected to the calculation of the similarity may be selected by designation by the user.</p><p id="p-0094" num="0093">Step S<b>09</b> is processing of presenting a result of the similarity determined for each of the skeleton feature points to the user by outputting the result to the display unit <b>202</b>.</p><p id="p-0095" num="0094"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a diagram showing an example of a screen configuration for presenting the similarity to the user. skeleton model <b>701</b> (human body portion) schematically showing the skeleton positions of the human body is di-played on a left side of the screen.</p><p id="p-0096" num="0095">In the skeleton model <b>701</b>, each circle (for example, a skeleton portion C<b>1</b> and a skeleton portion C<b>2</b>) indicates a position of the skeleton feature point such as a head, a neck, a left shoulder, or a right shoulder. Each circle indicating the skeleton feature point. is highlighted and displayed according to the similarity for each of the skeleton feature points calculated in step <b>306</b>. For example, the similarity calculation unit <b>401</b> displays and outputs a left wrist (skeleton portion C<b>1</b>) which is determined to have a small similarity and to have different characteristics between the first measurement unit and the second measurement unit with a black circle to urge the user to pay attention.</p><p id="p-0097" num="0096">Meanwhile, the similarity calculation unit <b>401</b> displays and outputs the left wrist (skeleton portion C<b>2</b>), which is determined to have a high similarity and to have the same characteristics between the first measurement unit and the second measurement unit, in a display color (for example, gray) different from black.</p><p id="p-0098" num="0097">In this way, the similarity calculation unit <b>401</b> changes the display color according to the similarity and outputs the skeleton portion, thereby allowing the user to easily understand the similarity of each skeleton portion.</p><p id="p-0099" num="0098">Further, the similarity calculation unit <b>401</b> may change a color or a size of the circle to be emphasized in a stepwise way according to a magnitude of a value of the similarity of the similarity calculation unit.</p><p id="p-0100" num="0099">When the similarity calculation unit <b>401</b> detects that a circle of a specific skeleton feature point (for example, the skeleton portion C<b>2</b>) is selected and instructed as a result of the user operating a pointer or the like, the similarity calculation unit <b>401</b> displays measurement. data of the skeleton feature point on a right side of the screen.</p><p id="p-0101" num="0100">In a measurement data plot <b>702</b> of the first measurement unit, a measurement data plot <b>703</b> of the second measurement unit, and a cross-correlation function plot <b>704</b>, the similarity calculation unit <b>401</b> displays the measurement data of the X axis, the Y axis, and the Z axis, respectively.</p><p id="p-0102" num="0101">In addition, the similarity calculation unit <b>401</b> also displays a plot <b>705</b> of the similarity obtained by adding the cross-correlation functions. In this way, the similarity calculation unit <b>401</b> outputs the skeleton model <b>701</b> and information indicating the similarity of the skeleton portion (for example, the skeleton portion C<b>2</b>) of the skeleton model <b>701</b> as a determination result of the similarity.</p><p id="p-0103" num="0102">By viewing this screen, the user can easily confirm the characteristics of the first measurement unit and the second measurement unit, and an efficiency of a model reconstruction operation can be improved. Here, the similarity is displayed as a graph with respect to a time lag, and the maximum value may be directly displayed as a numerical value.</p><p id="p-0104" num="0103">As a result, the user does not have to read the graph, and can grasp the similarity more simply.</p><p id="p-0105" num="0104">Further, the similarity calculation unit <b>401</b> may display and output a table <b>801</b> in which correspondences between the skeleton feature points and the similarities are listed as shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>. <figref idref="DRAWINGS">FIG. <b>8</b></figref> is a diagram showing an example of a screen example indicating a table in which the correspondences between the skeleton feature points and the similarities are listed.</p><p id="p-0106" num="0105">The table <b>801</b> shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref> includes an &#x201c;order of contribution to identification&#x201d; indicating an order of a ratio of contribution when identifying the walking types, a &#x201c;feature name&#x201d; indicating a content of the feature, a &#x201c;skeleton feature point name&#x201d; indicating a name of the skeleton used to calculate the feature, and a &#x201c;similarity&#x201d; indicating the similarity. The order of contribution to identification is determined in advance by the model constructed by the first measurement unit.</p><p id="p-0107" num="0106">The feature contributing to the identification when the model is constructed by the first measurement unit is clarified, and when this feature has a large difference in the characteristics between the first measurement unit and the second measurement unit, the model needs to be reconstructed.</p><p id="p-0108" num="0107">In order to make it easier for the user to confirm the walking type, feature names are listed in descending order of the order of contribution to identification. In addition, skeleton feature point names of original data used for generating the features and the similarities thereof are displayed.</p><p id="p-0109" num="0108">For example, a right stride most contributing to the identification is generated based on data of the right ankle, and a similarity of the right ankle is as high as 0.9. Therefore, it can be determined that the data of the right ankle does not need to be reanalyzed, and there is no problem to use the model constructed by the first measurement unit as it is.</p><p id="p-0110" num="0109">On the other hand, a swing width of a left hand that second contributes to the identification is generated based on data of the left wrist, and a similarity of the left wrist is as low as 0.3. Therefore, the model constructed by the first measurement unit cannot be used as it is for the data of the left wrist, and it is necessary to perform reanalysis. As described above, by presenting the table <b>801</b>, the user can easily grasp necessity of the reanalysis, and can quickly determine which skeleton feature point data is to be reanalyzed.</p><p id="p-0111" num="0110">According to the above-described embodiment, in the motion analysis apparatus <b>101</b>, the data input unit <b>205</b> acquires the first imaging result and the second imaging result. In the motion analysis apparatus <b>101</b>, the skeleton recognition unit <b>206</b> recognizes the skeleton positions of the subject <b>103</b> using the first imaging result acquired by the data input unit <b>205</b>, and recognizes the skeleton positions of the subject <b>103</b> using the second imaging result acquired by the data input unit <b>205</b>.</p><p id="p-0112" num="0111">The motion period extraction unit <b>403</b> extracts the period from the start of the motion to the end of the motion as the range of the data for comparing the skeleton feature points recognized by the skeleton recognition unit <b>206</b>. The similarity calculation unit <b>401</b> compares the skeleton feature points recognized for the input from the depth camera <b>102</b> with the skeleton feature points recognized for the input from the RGB cameras <b>301</b> and <b>302</b> to calculate the similarities, and outputs the determination result based on the similarities.</p><p id="p-0113" num="0112">The motion analysis apparatus <b>101</b> can determine whether the model for the first device can be applied to the second device by outputting the result of calculating the similarities of the changes in the skeleton positions of the user based on the imaging results of the different devices as described above, and can efficiently perform data analysis.</p><heading id="h-0012" level="1">Second Embodiment</heading><p id="p-0114" num="0113">The first embodiment discloses the example in which when the second measurement unit is introduced for the first time, the efficiency of the model reconstruction operation is improved by visualizing a difference in characteristics, and in an actual environment where measurement is performed, the measurement. may be performed only by the second measurement unit and final adjustment of a model may be performed.</p><p id="p-0115" num="0114"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a diagram showing a configuration of a system that performs measurement using a motion analysis apparatus according to a second embodiment. The RGB cameras <b>301</b> and <b>302</b> are connected to the motion analysis apparatus <b>101</b>, and measure a location where the subject <b>103</b> walks toward them. In the present embodiment, it is assumed that the motion analysis apparatus <b>101</b> stores in advance a model based on an imaging result acquired from the first device (depth camera <b>102</b>) described in the first embodiment.</p><p id="p-0116" num="0115"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a diagram showing an internal configuration of the motion analysis apparatus <b>101</b> that performs the final adjustment of the model in a local measurement environment (an environment where the RGB camera <b>301</b> and the RGB camera <b>302</b> are actually operated).</p><p id="p-0117" num="0116">Blocks denoted by the same reference numerals as those in <figref idref="DRAWINGS">FIG. <b>2</b></figref> have the same functions as those in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. An identification result determination unit. <b>1001</b> is a block that determines whether a result of identification using the model matches a known walking type when walking of a subject that is the known walking type is measured.</p><p id="p-0118" num="0117">The known walking types are input by a user in advance on a screen as shown in <figref idref="DRAWINGS">FIG. <b>11</b></figref>. <figref idref="DRAWINGS">FIG. <b>11</b></figref> is a diagram showing an input screen of walking types (walking type input screen). It is desirable that a walking type input screen <b>1101</b> displays the walking types that can be identified by the model as candidates and allows the user to select the walking types.</p><p id="p-0119" num="0118">For example, when the user selects normal walking on the screen, the motion analysis apparatus <b>101</b> determines whether a result obtained by measuring the walking and. identifying the walking by the model is the normal walking.</p><p id="p-0120" num="0119">That when the above selection operation is performed by the operation input unit <b>201</b>, the data input unit <b>205</b> receives an input of the walking type. Further, the data input unit <b>205</b> acquires an imaging result for verification (verification imaging result) from the RGB camera <b>301</b> and the RGB camera <b>302</b> separately from image data from the RGB camera <b>301</b> and the RGB camera <b>302</b> described in the first embodiment.</p><p id="p-0121" num="0120">The skeleton recognition unit <b>206</b> recognizes skeleton positions of the subject <b>103</b> using the verification imaging result. Then, the feature generation unit <b>207</b> generates features from the skeleton positions of the user based on the verification imaging result recognized by the skeleton recognition unit <b>206</b>. The identification result determination unit <b>1001</b> verifies, using the features generated by the feature generation unit <b>207</b> and model information corresponding to the walking type received by the data input unit <b>205</b>, whether the result corresponds to the walking type received by the data input unit <b>205</b>. The identification result determination unit <b>1001</b> display and outputs a verification result on the display unit <b>202</b>.</p><p id="p-0122" num="0121">The walking type input screen <b>1101</b> includes a start button <b>1102</b> and a cancel button <b>1103</b>.</p><p id="p-0123" num="0122">When the user selects the start button <b>1102</b>, the measurement is started, and a determination result is output. When the cancel button <b>1103</b> is selected, the screen returns to a previous screen.</p><p id="p-0124" num="0123">Next, an example a screen of the determination result (verification result) will be described with reference to <figref idref="DRAWINGS">FIG. <b>12</b></figref>. <figref idref="DRAWINGS">FIG. <b>12</b></figref> is a diagram showing the example of the screen of the determination result.</p><p id="p-0125" num="0124">When an identification result (determination result) matches the walking type, the user is notified that an identification model and a measurement device are consistent on a screen <b>1201</b> as shown in (a) of <figref idref="DRAWINGS">FIG. <b>12</b></figref>. Accordingly, the user can determine that it is not necessary to adjust the measurement device and reconstruct the model. An end button <b>1202</b> and a re-measurement button <b>1203</b> are displayed on the screen <b>1201</b>. When the user selects the end button <b>1202</b>, processing of the present embodiment ends, and when the user selects the re-measurement button <b>1203</b>, processing of walking measurement is performed again.</p><p id="p-0126" num="0125">When the identification result does not match the walking type, a screen <b>1204</b> as shown in (b) of <figref idref="DRAWINGS">FIG. <b>12</b></figref> is presented to present the user that it is necessary to adjust the measurement device or the identification model. That is, when it is shown that the verification result by the identification result determination unit <b>1001</b> does not correspond to the walking type received by the data input unit <b>205</b>, the identification result determination unit <b>1001</b> outputs information indicating a warning. Accordingly, the user can determine that it is necessary to change an installation environment of the measurement device or to reconstruct the model, and the processing is repeated until the identification result matches the walking type by measuring the walking again.</p><p id="p-0127" num="0126">Further, when the identification result does not match the walking type, coordinate data of the skeleton feature points obtained from the measurement device may be corrected such that the identification result match the walking type. In a correcting method, representative data of the known walking types is stored in advance, and a range in which the coordinate data changes is scaled such that a similarity between the representative data and the coordinate data of the skeleton feature points increases. When the identification results do not match the walking type after the correction, processing of confirming the identification results by further correcting the identification result may be repeated.</p><heading id="h-0013" level="1">Third Embodiment</heading><p id="p-0128" num="0127">The first embodiment discloses that the correspondence relationship between the skeleton feature points obtained by the first measurement unit and the second measurement unit is designated by a user, and the corresponding skeleton feature points may be estimated based on the acquired skeleton coordinate data.</p><p id="p-0129" num="0128">Here, a description will be given with reference to a flowchart shown in <figref idref="DRAWINGS">FIG. <b>13</b></figref>. <figref idref="DRAWINGS">FIG. <b>13</b></figref> is a flowchart in which the motion analysis apparatus <b>101</b> estimates the correspondence relationship between the skeleton feature points of the first measurement unit and the second measurement unit and generates a skeleton correspondence table.</p><p id="p-0130" num="0129">Step S<b>21</b> is processing of selecting one skeleton feature point from depth data of the first measurement unit. Here, it is assumed that the skeleton recognition unit <b>206</b> selects any skeleton feature point (also referred to as a skeleton feature point <b>1</b>-<b>1</b> for convenience) extracted from the depth data.</p><p id="p-0131" num="0130">Step S<b>22</b> is processing of selecting a skeleton feature point specified based on image data obtained by imaging performed by the second measurement unit from a plurality of directions. Here, it is assumed that the skeleton recognition unit <b>206</b> selects any skeleton feature point specified based on the image data obtained by the imaging performed by the second measurement unit from the plurality of directions.</p><p id="p-0132" num="0131">Step S<b>23</b> is processing of calculating the similarity for a combination of the skeleton feature points selected in steps S<b>21</b> and S<b>22</b>. The similarity, is obtained by calculating cross-correlation functions of an X axis, a axis, and a Z axis for two skeleton feature points as described in the first embodiment, and adding the cross-correlation functions to obtain a maximum value of a similarity function.</p><p id="p-0133" num="0132">Step S<b>24</b> is processing of determining whether calculation of the similarity is ended using all the skeleton feature points of the second measurement unit. When the calculation is not ended, a process returns to step S<b>22</b>, and another skeleton feature point is selected. When the calculation is ended, the process proceeds to step S<b>25</b>.</p><p id="p-0134" num="0133">In step S<b>25</b>, a combination of the skeleton feature points having the maximum value of the similarity is extracted from the similarities comprehensively calculated for all the skeleton feature points extracted from the depth data. For example, when a similarity between the skeleton feature point <b>1</b>-<b>1</b> and a skeleton feature point <b>2</b>-<b>1</b> is maximum, it is determined that the combination has the correspondence relationship, and the combination is stored in the skeleton correspondence table <b>402</b>.</p><p id="p-0135" num="0134">In step S<b>26</b>, it is determined whether processing of finding the combination having the correspondence relationship for all the skeleton feature points of the first measurement unit is ended. When the processing is not ended, the process returns to step S<b>21</b> to select another skeleton feature point <b>1</b>-<b>2</b>. When the processing is ended, the correspondence relationships are generated for all the skeleton feature points, and the process of the present. embodiment is ended.</p><p id="p-0136" num="0135">When the correspondence relationship between the skeleton feature points estimated by this method are displayed on the screen shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref> and the user is caused to perform final confirmation and correction using the correspondence relationship as an initial value, input work of the user is reduced and efficiency is improved.</p><heading id="h-0014" level="1">Fourth Embodiment</heading><p id="p-0137" num="0136">The first and third embodiments disclose the case in which types of skeleton feature points obtained by the first measurement unit and the second measurement unit are the same, and when the skeleton feature points of the first measurement unit and the second measurement unit are different from each other, the skeleton feature points having similar coordinate changes may be substituted.</p><p id="p-0138" num="0137">In the present embodiment, a method will be described in which, when the number of types of the skeleton feature points obtained by the second measurement unit is smaller than that of the first measurement unit, the skeleton feature points having the similar coordinate changes are specified and substituted with data thereof.</p><p id="p-0139" num="0138"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a diagram showing a configuration of a motion analysis apparatus according to a fourth embodiment. The motion analysis apparatus <b>101</b> includes a skeleton feature point interpolation unit <b>1401</b> (skeleton position interpolation unit) in addition to the configuration of the first embodiment. The skeleton feature point interpolation unit <b>1401</b> interpolates data of a skeleton feature point that is present in the skeleton feature points recognized by the first measurement unit and is not present in skeleton feature points recognized by the second measurement. unit. That is, the skeleton feature point interpolation unit <b>1401</b> is a part that specifies, when skeletons of a user recognized by the skeleton recognition unit <b>206</b> using a first imaging result are not present in skeletons of the user recognized using a second imaging result, positions corresponding to the positions of the skeletons of the user recognized by the skeleton recognition unit <b>206</b> using the first imaging result.</p><p id="p-0140" num="0139"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a flowchart of specifying a similar skeleton feature point of the first measurement unit with respect to a skeleton feature point (target feature point) of the first measurement unit that is not present in the skeleton feature points of the second measurement unit.</p><p id="p-0141" num="0140">Step S<b>31</b> is processing of selecting one skeleton feature point of a first measurement method adjacent to the target feature point. Since the adjacent skeleton feature points indicate similar coordinate changes due to characteristics of a human body, when the target feature point is, for example, a right elbow, a right wrist and a right shoulder are served as the adjacent skeleton feature points, and the skeleton feature point interpolation unit <b>1401</b> selects one of the right wrist and the right shoulder.</p><p id="p-0142" num="0141">Step S<b>32</b> is processing of calculating a similarity using the target feature point and the skeleton feature point selected in step S<b>31</b>. The similarity is obtained by calculating cross-correlation functions of an X axis, a axis, and a Z axis for two skeleton feature points as described in the first embodiment, and adding the cross-correlation functions to obtain a maximum value of a similarity function.</p><p id="p-0143" num="0142">Step S<b>33</b> is processing determining whether calculation of the similarity is ended for all the skeleton feature points adjacent to the target feature point. When the calculation is not ended, a process returns to step S<b>31</b>, and another skeleton feature point is selected. When the calculation is ended, the process proceeds to step S<b>34</b>.</p><p id="p-0144" num="0143">Step S<b>34</b> is processing of extracting, by the skeleton feature point interpolation unit <b>1401</b>, a combination having the largest similarity from the similarities calculated for the target feature point. For example, when the target feature point is the right elbow and the similarity to the right wrist is maximum, it is determined that the coordinate change in the right elbow can be substituted by the that of the right wrist, and data of the right elbow is substituted by that of the right wrist in the second measurement unit.</p><p id="p-0145" num="0144">By presenting the user that it is determined that the data of the right elbow can be substituted with that of the right wrist by this method and allowing the user to confirm this substitution, is possible to prevent unintended substitution by the user.</p><p id="p-0146" num="0145">Instead of substituting with one adjacent skeleton feature point, the data of the target feature point may be generated by interpolation or extrapolation from a plurality of adjacent skeleton feature points. In a case of walking, since an arm often swings while an elbow is extended, when the right elbow is the target feature point, the coordinate change can be obtained by the interpolation from the right wrist and the right shoulder.</p><p id="p-0147" num="0146">Then, when the skeletons of the user recognized by the skeleton recognition unit <b>206</b> using the first imaging result are not present in the skeletons of the user recognized using the second imaging result, the similarity calculation unit <b>401</b> calculates the similarities between changes in the skeleton positions of the user recognized by the skeleton recognition unit <b>206</b> using the first imaging result and changes in skeleton positions at a position that is specified by the skeleton feature point interpolation unit <b>1401</b> and corresponds to the skeleton position of the user recognized using the first imaging result.</p><p id="p-0148" num="0147">Accordingly, the motion analysis apparatus <b>101</b> can calculate the similarities even if there is no information on the same skeleton position from the first imaging result and the second imaging result.</p><p id="p-0149" num="0148">The invention is not limited to the above-mentioned embodiments, and includes various modifications. For example, the above-described embodiments are described in detail for easy understanding of the invention, and the invention is not necessarily limited to an embodiment including all the configurations described above. A part of a configuration of a certain embodiment can be substituted with a configuration of another embodiment, and can be added to, deleted from, or substituted with the configuration of a certain embodiment by using another configuration. The configurations, functions, processing units, processing methods, or the like may be implemented by hardware by designing a part or all of them with, for example, an integrated circuit. Further, the above-described configurations, functions, and the like may implemented by software by means of a processor interpreting and executing a program for implementing respective functions. Information such as a program, a table, and a file for implementing each of the functions can be stored in a recording device such as a memory, a hard disk, or a solid state drive (SSD), or in a recording medium such as an IC card, an SD card, or a DVD.</p><p id="p-0150" num="0149">Further, in the above-described embodiments, the walking is taken as an example of the motion to be analyzed, and the invention may be applied to any motion.</p><heading id="h-0015" level="1">INDUSTRIAL APPLICABILITY</heading><p id="p-0151" num="0150">The invention is applicable to a system that processes information acquired from a device capable of capturing an image of a user.</p><heading id="h-0016" level="1">REFERENCE SIGNS LIST</heading><p id="p-0152" num="0151"><b>101</b> motion analysis apparatus</p><p id="p-0153" num="0152"><b>201</b> operation input unit</p><p id="p-0154" num="0153"><b>202</b> display unit</p><p id="p-0155" num="0154"><b>203</b> control unit</p><p id="p-0156" num="0155"><b>204</b> memory</p><p id="p-0157" num="0156"><b>205</b> data input unit</p><p id="p-0158" num="0157"><b>206</b> skeleton recognition unit</p><p id="p-0159" num="0158"><b>207</b> feature generation unit</p><p id="p-0160" num="0159"><b>208</b> model</p><p id="p-0161" num="0160"><b>401</b> similarity calculation unit</p><p id="p-0162" num="0161"><b>402</b> skeleton correspondence table</p><p id="p-0163" num="0162"><b>403</b> motion period extraction unit</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An information processing apparatus for processing information acquired from a device capable of capturing an image of a user, the information processing apparatus comprising:<claim-text>a first imaging result acquisition unit configured to acquire a first imaging result that is a result of imaging performed by a first device configured to capture an image of a motion state of the user;</claim-text><claim-text>a second imaging result acquisition unit configured to acquire a second imaging result that is a result of imaging performed by a second device configured to capture an image of the motion state of the user;</claim-text><claim-text>a skeleton recognition unit configured to recognize a skeleton position of the user by using the first imaging result acquired by the first imaging result acquisition unit and recognize a skeleton position of the user by using the second imaging result acquired by the second imaging result acquisition unit;</claim-text><claim-text>a motion period specifying unit configured to specify a predetermined motion period of the user based on a result obtained by the recognition performed by the skeleton recognition unit;</claim-text><claim-text>a similarity calculation unit configured to calculate a similarity between a change in the skeleton position of the user recognized by the skeleton recognition unit using the first imaging result and a change in the skeleton position of the user recognized by the skeleton recognition unit using the second imaging result in the motion period specified by the motion period specifying unit; and</claim-text><claim-text>a determination result output unit configured to output a determination result based on the similarity calculated by the similarity calculation unit.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The information processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the motion period specifying unit specifies a walking cycle of the user based on the result obtained the recognition performed by the skeleton recognition unit.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The information processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the determination result output unit outputs information indicating that the first device and the second device are similar to each other when the similarity calculated by the similarity calculation unit is greater than a predetermined threshold, and outputs information indicating that characteristics of the first device and the second device are different from each other when the similarity calculated by the similarity calculation unit is equal to or smaller than the predetermined threshold.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The information processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the determination result output unit outputs, as the determination result, a human body portion and information indicating a similarity of a skeleton portion of the human body portion.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The information processing apparatus according to claim wherein<claim-text>the skeleton recognition unit further acquires information indicating a skeleton position designated by the user, and recognizes the skeleton position of the user.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The information processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>a storage unit configured to store, for each walking type, model information in which a feature of a motion of a skeleton portion and a walking state corresponding to the feature are defined;</claim-text><claim-text>a reception unit configured to receive an input of the walking type;</claim-text><claim-text>a verification imaging result acquisition unit configured to acquire a verification imaging result that is a result of imaging performed by the second device for verification;</claim-text><claim-text>a feature generation unit configured to generate a feature of the skeleton based on the skeleton position recognized by the skeleton recognition unit;</claim-text><claim-text>a verification unit configured to verify, using the feature generated by the feature generation unit and model information corresponding to the walking type received by the reception unit, whether a verification result corresponds to the walking type received by the reception unit; and</claim-text><claim-text>a verification result output unit configured to output the verification result by the verification unit, wherein</claim-text><claim-text>the skeleton recognition unit recognizes the skeleton position of the user from the verification imaging result acquired by the verification imaging result acquisition unit, and</claim-text><claim-text>the feature generation unit generates the feature from the skeleton position of the user based on the verification imaging result recognized by the skeleton recognition unit.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The information processing apparatus according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein<claim-text>the verification result output unit outputs information indicating a warning when the verification result by the verification unit does not correspond to the walking type received by the reception unit.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The information processing apparatus according to claim wherein<claim-text>the skeleton recognition unit specifies a correspondence relationship between the skeleton position of the user recognized from the first imaging result acquired by the first imaging result acquisition unit and the skeleton position of the user recognized from the second imaging result acquired by the second imaging result acquisition unit.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The information processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>a skeleton position interpolation unit configured to specify a position corresponding to the skeleton position of the user recognized by using the first imaging result when a skeleton of the user recognized by the skeleton recognition unit using the first imaging result is not present in a skeleton of the user recognized by the skeleton recognition unit using the second imaging result, wherein</claim-text><claim-text>when the skeleton of the user recognized by the skeleton recognition unit using the first imaging result is not present in the skeleton of the user recognized by the skeleton recognition unit using the second imaging result, the similarity calculation unit calculates a similarity between the change in the skeleton position of the user recognized by the skeleton recognition unit using the first imaging result and a change in a skeleton position at the position that specified by the skeleton position interpolation unit and corresponds to the skeleton position of the user recognized by using the first imaging result.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. A determination result output method executed by an information processing apparatus for processing information acquired from a device capable of capturing an image of a user, the determination result output method comprising:<claim-text>a first imaging result acquisition step of acquiring a first imaging result that is a result of imaging performed by a first device that captures an image of a motion state of the user;</claim-text><claim-text>a second imaging result acquisition step of acquiring a second imaging result that is a result of imaging performed by a second device configured to capture an image of the motion state of the user;</claim-text><claim-text>a skeleton recognition step of recognizing a skeleton position of the user by using the first imaging result acquired in the first imaging result acquisition step, and recognizing a skeleton position of the user by using the second imaging result acquired in the second imaging result acquisition step;</claim-text><claim-text>a motion period specifying step of specifying a predetermined motion period of the user based on a result obtained by the recognition performed in the skeleton recognition step;</claim-text><claim-text>a similarity calculation step of calculating a similarity between a change in the skeleton position of the user recognized in the skeleton recognition step using the first imaging result and a change in the skeleton position of the user recognized in the skeleton recognition step using the second imaging result in the motion period specified in the motion period specifying step; and</claim-text><claim-text>a determination result output step of outputting a determination result based on the similarity calculated in the similarity calculation step.</claim-text></claim-text></claim></claims></us-patent-application>