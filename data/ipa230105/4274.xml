<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004275A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004275</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17930612</doc-number><date>20220908</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>04815</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>04845</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>0482</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>04815</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>04845</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>0482</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">SYSTEM AND USER INTERFACE FOR VIEWING AND INTERACTING WITH THREE-DIMENSIONAL SCENES</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>17301874</doc-number><date>20210416</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11455074</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17930612</doc-number></document-id></child-doc></relation></continuation><us-provisional-application><document-id><country>US</country><doc-number>63011432</doc-number><date>20200417</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Occipital, Inc.</orgname><address><city>Boulder</city><state>CO</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Osipov</last-name><first-name>Kirill</first-name><address><city>Kaluga</city><country>RU</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Romanov</last-name><first-name>Anton</first-name><address><city>Bashkortostan</city><country>RU</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Yakubenko</last-name><first-name>Anton</first-name><address><city>Boulder</city><state>CO</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Krivovyaz</last-name><first-name>Gleb</first-name><address><city>Moscow</city><country>RU</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>Powers</last-name><first-name>Jeffrey Roger</first-name><address><city>San Francisco</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="05" designation="us-only"><addressbook><last-name>Reddy</last-name><first-name>Vikas Muppidi</first-name><address><city>Boulder</city><state>CO</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A system configured to generate a three-dimensional scene of a physical environment. In some cases, the system may present the three-dimensional scene such that the user is able to view the model in various different types. The system may also allow the user to capture various measurements associated with the scanned physical environment by selecting different portions of the scene.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="101.09mm" wi="158.75mm" file="US20230004275A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="258.23mm" wi="167.47mm" file="US20230004275A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="237.32mm" wi="167.05mm" orientation="landscape" file="US20230004275A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="236.90mm" wi="167.05mm" orientation="landscape" file="US20230004275A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="236.81mm" wi="167.05mm" orientation="landscape" file="US20230004275A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="238.34mm" wi="167.05mm" orientation="landscape" file="US20230004275A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="233.60mm" wi="167.05mm" orientation="landscape" file="US20230004275A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="238.34mm" wi="167.05mm" orientation="landscape" file="US20230004275A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="207.52mm" wi="168.49mm" orientation="landscape" file="US20230004275A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="236.90mm" wi="155.79mm" file="US20230004275A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="237.15mm" wi="167.05mm" orientation="landscape" file="US20230004275A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="241.64mm" wi="167.05mm" orientation="landscape" file="US20230004275A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="239.86mm" wi="167.22mm" orientation="landscape" file="US20230004275A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="246.21mm" wi="167.05mm" orientation="landscape" file="US20230004275A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="242.40mm" wi="167.89mm" orientation="landscape" file="US20230004275A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="241.13mm" wi="167.05mm" orientation="landscape" file="US20230004275A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="244.09mm" wi="167.05mm" orientation="landscape" file="US20230004275A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="233.68mm" wi="167.05mm" orientation="landscape" file="US20230004275A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="242.74mm" wi="167.05mm" orientation="landscape" file="US20230004275A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00019" num="00019"><img id="EMI-D00019" he="248.24mm" wi="167.05mm" orientation="landscape" file="US20230004275A1-20230105-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00020" num="00020"><img id="EMI-D00020" he="218.44mm" wi="167.05mm" orientation="landscape" file="US20230004275A1-20230105-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00021" num="00021"><img id="EMI-D00021" he="236.30mm" wi="167.05mm" orientation="landscape" file="US20230004275A1-20230105-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00022" num="00022"><img id="EMI-D00022" he="241.72mm" wi="167.05mm" orientation="landscape" file="US20230004275A1-20230105-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00023" num="00023"><img id="EMI-D00023" he="236.90mm" wi="167.05mm" orientation="landscape" file="US20230004275A1-20230105-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00024" num="00024"><img id="EMI-D00024" he="257.30mm" wi="151.05mm" file="US20230004275A1-20230105-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00025" num="00025"><img id="EMI-D00025" he="248.33mm" wi="154.77mm" file="US20230004275A1-20230105-D00025.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00026" num="00026"><img id="EMI-D00026" he="257.47mm" wi="145.46mm" file="US20230004275A1-20230105-D00026.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00027" num="00027"><img id="EMI-D00027" he="254.34mm" wi="162.90mm" file="US20230004275A1-20230105-D00027.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION(S)</heading><p id="p-0002" num="0001">This application is a continuation of and claims priority to U.S. application Ser. No. 17/301,874, filed on Apr. 16, 2021 and entitled &#x201c;System And User Interface For Viewing And Interacting With Three-Dimensional Scenes,&#x201d; which claims priority to U.S. Provisional Application No. 63/011,432 filed on Apr. 17, 2020 and entitled &#x201c;System and User Interface for Viewing and Interacting with Three-Dimensional Spaces,&#x201d; the entirety of which is incorporated herein by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">The presence of three-dimensional (3D) imaging and virtual reality systems is becoming more and more common. In some cases, the imaging system or virtual reality system may be configured to allow a user to interact with a three-dimensional virtual scene of a physical environment. However, interacting with the three-dimensional environment often requires a three-dimensional viewing device and/or complicated setups that are not suitable for all users.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0004" num="0003">The detailed description is described with reference to the accompanying figures. In the figures, the left-most digit(s) of a reference number identifies the figure in which the reference number first appears. The use of the same reference numbers in different figures indicates similar or identical components or features.</p><p id="p-0005" num="0004"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates an example of a user viewing a three-dimensional scene or model via a user device according to some implementations.</p><p id="p-0006" num="0005"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is an example user interface for viewing a three-dimensional scene of a physical environment according to some implementations.</p><p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is another example user interface for viewing a three-dimensional scene of a physical environment according to some implementations.</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is another example user interface for viewing a three-dimensional scene of a physical environment according to some implementations.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is another example user interface for viewing a three-dimensional scene of a physical environment according to some implementations.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is another example user interface for viewing a three-dimensional scene of a physical environment according to some implementations.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is another example user interface for viewing a three-dimensional scene of a physical environment according to some implementations.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is another example user interface for viewing a three-dimensional scene of a physical environment according to some implementations.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is an example of multiple views of a user interface for interacting with a three-dimensional scene of a physical environment according to some implementations.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is another example user interface for viewing a three-dimensional scene of a physical environment according to some implementations.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is another example user interface for viewing a three-dimensional scene of a physical environment according to some implementations.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is another example user interface for viewing a three-dimensional scene of a physical environment according to some implementations.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is another example user interface for viewing a three-dimensional scene of a physical environment according to some implementations.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is another example user interface for viewing a three-dimensional scene of a physical environment according to some implementations.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is another example user interface for viewing a three-dimensional scene of a physical environment according to some implementations.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>16</b></figref> is another example user interface for viewing a three-dimensional scene of a physical environment according to some implementations.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is another example user interface for viewing a three-dimensional scene of a physical environment according to some implementations.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>18</b></figref> is another example user interface for viewing a three-dimensional scene of a physical environment according to some implementations.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>19</b></figref> is another example user interface for viewing a three-dimensional scene of a physical environment according to some implementations.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>20</b></figref> is another example user interface for viewing a three-dimensional scene of a physical environment according to some implementations.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>21</b></figref> is another example user interface for viewing a three-dimensional scene of a physical environment according to some implementations.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>22</b></figref> is another example user interface for viewing a three-dimensional scene of a physical environment according to some implementations</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>23</b></figref> is another example user interface for viewing a three-dimensional scene of a physical environment according to some implementations.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>24</b></figref> is an example flow diagram showing an illustrative process for generating measurements associated with a three-dimensional scene of a physical environment according to some implementations.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>25</b></figref> is another example flow diagram showing an illustrative process for displaying measurements associated with a region of a three-dimensional scene of a physical environment according to some implementations.</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>26</b></figref> is another example flow diagram showing an illustrative process for receiving user inputs with respect to a three-dimensional scene according to some implementations.</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>27</b></figref> is an example device associated with consuming a three-dimensional scene according to some implementations.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0004" level="1">DETAILED DESCRIPTION</heading><p id="p-0032" num="0031">This disclosure includes techniques and implementations for presenting and viewing stored three-dimensional scene. In some examples, the system may generate three-dimensional models, computer-aided design (CAD) models, point clouds, meshes, semantic scenes, reconstructions, or other models representing real physical environments and locations as well as virtual environments and locations. In some cases, individual physical environments may be viewable as various different types of three-dimensional scene or models, as discussed in more detail below.</p><p id="p-0033" num="0032">In some cases, the system may present a model or reconstruction that may be viewable as a top-down model, bird's eye view, a side view model, or a user navigable reconstruction. In various implementations, the user may switch between views and types of models in substantially real-time. The user may also utilize the models to generate measurements, metrics, and other data associated with the represented physical environment. In some cases, the user may also adjust the models via the viewing system.</p><p id="p-0034" num="0033">In some instances, the models may be viewable via a web-browser, downloadable and/or device hosted application, or shareable link. In this manner, the system does not require specialized software, such as CAD software, to view, edit, and traverse the models and/or scenes. The system also allows the sharing of the models with friends, family, customers, clients, and the like without the need to ensure the receiving party has any particular user equipment or software.</p><p id="p-0035" num="0034">In some cases, the model may be traversable or navigable by the user. For example, the user may move through the model by panning horizontally at a predetermined height or altitude above the floor. In some cases, the height may be selected based on an average height of a human, a known height of the viewing user, a user input, or based on the height of the capture device when the physical environment was scanned. In some cases, a slope may be determined when the user is moving about a terrain or model having variable floor height (such as up or down stairs). The user may then be maintained at the predetermined height above the determined slope.</p><p id="p-0036" num="0035">In some cases, the system may limit the movement of the users to an area defined by a bounding box determined by objects and/or a predetermined distance from a point of capture of the image data used to generate the model. In some cases, the boundary may be configured such that the boundary does not include corners, right angles, or other types of straight intersections. In this manner, the boundary may be a three-dimensional shape that includes only sloped surfaces.</p><p id="p-0037" num="0036">In some examples, the user within the model may be represented as a target point at the predetermined height or altitude above the floor. As the user moves, the user may collide or otherwise intersect with the bounding box. In the event of a collision, the target point may continue to move but the movement may be upward or downward along the bounding box opposed to the horizontal movement representing the user traversing the interior of the model or scene. In other cases, the target point may be moved horizontally along the surface of the bounding box.</p><p id="p-0038" num="0037">In some cases, the system may be configured to provide various types of measurements (such as distances, areas, heights, slopes, angles, and the like). In some examples, the system may allow the user to select a point, line, and/or plane and the system may provide the user with various options based on the normals of the plane or point selected, type of surface selected (e.g., wall, furniture, window, door, or the like) and corresponding objects, planes, and surfaces (such as parallel walls, intersecting planes, nearby corners, and the like) to present the user with various selectable lines and/or points to generate measurements. For example, if a user selects a point on a first wall the system may present lines representing a distance or height from the point to the floor, from the point to the ceiling, from the floor to ceiling, from the first wall to a parallel wall across the environment, as well as an area of the wall or the like. For example, the system may generate selectable measurements based on global normals or grids associated with the environment and/or based on local normals or grids associated with the selected surface or plane.</p><p id="p-0039" num="0038">In some cases, the system may also generate measurements based at least in part on a viewpoint of the user when viewing the model or scene. For example, if a user is viewing an object, such as a door, window, or article of furniture, the system may display the measurements associated with the object (e.g., a window's height, length, depth, area, and the like). In some cases, the measurements generated without the user input may be updated as the user moves around the model or scene. However, in some instances, the measurements may be displayed for a user defined or predetermined period of time before the system generates new viewpoint dependent measurements. In another example, if the user is viewing a bird's eye view or overhead view of the model or scene the system may display the lengths of walls, the total areas, lengths and widths of a room, and the like.</p><p id="p-0040" num="0039">In some implementations, the viewpoint dependent measurements may also be generated based on stored parameters. For example, if a user is viewing a light switch or an electrical outlet, the system may generate the distance between the floor and the outlet and/or a distance between a nearest wall and the outlet as these types of measurements are most likely of interest to the user. In some specific implementations, the parameters used to generate the viewpoint-based measurements may be determined by one or more machine learned models and/or networks.</p><p id="p-0041" num="0040">In some cases, the system may also generate a per environment or per room measurements report, including, for example, a list of various dimensions. For example, the per room report may present the user with total area or square footage, number of windows, number of doors, wall dimensions, ceiling height, area of walls with and without openings, room perimeter, dimensions of doors and windows, number of electrical fixtures (such as switches and power outlets), presence of certain objects in the scene (such as fire extinguishers), and the like. In some cases, the per room report may also include a description of materials, such as trim, flooring, lighting and the like.</p><p id="p-0042" num="0041">In some cases, the system may provide assistance in point selection for generation of measurements. For example, the system may allow the user selection or input to snap or align with axis points (e.g., corners, surface midpoints, lines, and the like) that are in proximity to the position of the user input. For instance, if a user selects a point within a threshold distance of a corner, it is likely the user intended to select the corner and the system may assist by presenting the user with the option to select the corner. In some cases, the system may also present a magnifier about or around the selection point to assist the user in placing more precise user inputs. In some cases, the magnifier may be positioned about the selection point for a predetermined radius or distance. The user may then move the magnifier by adjusting a position of the user input and/or intersecting with (e.g., pushing) the edge of the magnifier.</p><p id="p-0043" num="0042">In some examples, the system may allow the user to draw, outline, or otherwise define an area to determine the lengths of the boundary of the defined area. The system may also determine an area of the bounded region. In some examples, the user may modify the scene or model by adjusting the boundary or measurement inputs. In some cases, the system may, for each position of the user, determine a plurality of circles within the bounded region, select the largest circle, and maintain the measurement of the area at the center of the largest circle as the user traverses the model or scene. The system may also adjust in substantially real-time the position of each displayed measurement to ensure that, from a current viewpoint, each of the displayed measurements is visible and not occluded by other measurements.</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates an example of a user <b>102</b> viewing a three-dimensional scene or model <b>104</b> via a user device <b>106</b> according to some implementations. In the current example, the system, such as the cloud-based services <b>108</b> and/or an application hosted on the user device <b>106</b>, may have generated a three-dimensional scene or model <b>104</b> of the physical environment <b>110</b> using sensor and/or image data captured during a scanning session. For instance, the user <b>102</b> may initialize a three-dimensional scanning or capture session to generate a three-dimensional scene or model <b>104</b>. The image and/or sensor data capture may be processed by the application on the device <b>104</b> and/or by a cloud-based service <b>106</b>. The user <b>102</b> may then view and/or interact with the scenes <b>104</b> using, for instance, the device <b>106</b>.</p><p id="p-0045" num="0044">In some implementations, the user <b>102</b> may view and/or interact with the scene or model <b>104</b> using the device <b>106</b>, another device having a two-dimensional display, and/or via a three-dimensional viewing system, such as an immersive wearable three-dimensional display system or glasses. In various instances, the user <b>102</b> may traverse the environment such as if the user <b>102</b> was physically located within the scene or model <b>104</b>. The user <b>102</b> may also view a top down or bird's eye view of the environment or from various perspectives.</p><p id="p-0046" num="0045">In each different type of view the user <b>102</b> may interact or provide user inputs <b>112</b>, generally indicated by user input data <b>112</b>, associated with the scene or model <b>104</b>. Based on the user input data <b>112</b>, the system (e.g., the cloud-based service <b>108</b> and/or the application hosted on the device <b>106</b>) may generate various measurements <b>114</b> and/or adjustments to the model or scene <b>104</b>. For instance, the user input data <b>112</b> may include a user selection, such as a tap, mouse click, or the like on a portion of the model <b>104</b>. In other cases, the user input data <b>112</b> may include a visible or viewed portion of the model <b>104</b>. In still other cases, such as when an immersive three-dimensional display is used, the user input data <b>112</b> may include eye tracking and/or area of interest determinations by the system.</p><p id="p-0047" num="0046">In the cases of user selections (e.g., taps, mouse clicks, and the like), the system may receive the selection of a point, plane, surface, or the like and provide the user with various measurement options based on the normals of the plane or point selected, type of surface selected (e.g., wall, furniture, window, door, or the like) and corresponding objects, planes, and surfaces (such as parallel walls, intersecting planes, nearby corners, and the like) to present the user with various selectable lines and/or points to generate measurements. For example, if the user <b>102</b> selects a point on a first wall the system may present lines representing a distance or height from the point to the floor, from the point to the ceiling, from the floor to ceiling, from the first wall to a parallel wall across the model <b>104</b>, as well as an area of the wall or the like. In example, the system may generate selectable measurements <b>114</b> based on global normals or grids associated with the environment and/or based on local normals or grids associated with the selected surface or plane. Once the user selects the types and/or measurement points, the system may display the measurements <b>114</b> via the user interface on the device <b>106</b>, as discussed in more detail below.</p><p id="p-0048" num="0047">In some cases, the system may also generate measurements <b>114</b> based at least in part on a viewpoint of the user <b>102</b> when viewing the model or scene <b>104</b>. For example, if a user <b>102</b> is viewing an object, such as a door, window, or article of furniture, the system may display the measurements <b>114</b> associated with the object (e.g., a window's height, length, depth, area, distances from the object to other objects, distances from edges of the object to edges of the wall or window, and the like). In some cases, the measurements <b>114</b> generated without the user input may be updated as the user <b>102</b> moves about the model or scene <b>104</b>. However, in some instance, the measurements <b>114</b> may be displayed for a user defined or predetermined period of time before the system generates new viewpoint dependent measurements <b>114</b>. In another example, if the user <b>102</b> is viewing a bird's eye view or overhead view of the model or scene <b>104</b>, the system may display via the device <b>106</b> the lengths of walls, the total areas, lengths and widths of a room, and the like.</p><p id="p-0049" num="0048">In some implementations, as discussed above, the viewpoint dependent measurements <b>104</b> may also be generated based on stored parameters. For example, if a user <b>102</b> is viewing a light switch or an electrical outlet, the system may generate the distance between the floor and the outlet and/or a distance between a nearest wall and the outlet as these types of measurements are most likely of interest to the user <b>102</b>. In some specific implementations, the parameters used to generate the viewpoint-based measurements <b>114</b> may be determined by one or more machine learned models and/or networks. For example, a machine learned model or neural network may be a biologically inspired technique which passes input data (e.g., the frames or other image/sensor data) through a series of connected layers to produce an output or learned inference. Each layer in a neural network can also comprise another neural network or can comprise any number of layers (whether convolutional or not). As can be understood in the context of this disclosure, a neural network can utilize machine learning, which can refer to a broad class of such techniques in which an output is generated based on learned parameters.</p><p id="p-0050" num="0049">As an illustrative example, one or more neural network(s) may generate any number of learned inferences or heads from the user input data <b>112</b>. In some cases, the neural network may be a trained network architecture that is end-to-end. Although discussed in the context of neural networks, any type of machine learning can be used consistent with this disclosure. For example, machine learning algorithms can include, but are not limited to, regression algorithms (e.g., ordinary least squares regression (OLSR), linear regression, logistic regression, stepwise regression, multivariate adaptive regression splines (MARS), locally estimated scatterplot smoothing (LOESS)), instance-based algorithms (e.g., ridge regression, least absolute shrinkage and selection operator (LASSO), elastic net, least-angle regression (LARS)), decisions tree algorithms (e.g., classification and regression tree (CART), iterative dichotomiser 3 (ID3), Chi-squared automatic interaction detection (CHAID), decision stump, conditional decision trees), Bayesian algorithms (e.g., na&#xef;ve Bayes, Gaussian na&#xef;ve Bayes, multinomial na&#xef;ve Bayes, average one-dependence estimators (AODE), Bayesian belief network (BNN), Bayesian networks), clustering algorithms (e.g., k-means, k-medians, expectation maximization (EM), hierarchical clustering), association rule learning algorithms (e.g., perceptron, back-propagation, hopfield network, Radial Basis Function Network (RBFN)), deep learning algorithms (e.g., Deep Boltzmann Machine (DBM), Deep Belief Networks (DBN), Convolutional Neural Network (CNN), Stacked Auto-Encoders), Dimensionality Reduction Algorithms (e.g., Principal Component Analysis (PCA), Principal Component Regression (PCR), Partial Least Squares Regression (PLSR), Sammon Mapping, Multidimensional Scaling (MDS), Projection Pursuit, Linear Discriminant Analysis (LDA), Mixture Discriminant Analysis (MDA), Quadratic Discriminant Analysis (QDA), Flexible Discriminant Analysis (FDA)), Ensemble Algorithms (e.g., Boosting, Bootstrapped Aggregation (Bagging), AdaBoost, Stacked Generalization (blending), Gradient Boosting Machines (GBM), Gradient Boosted Regression Trees (GBRT), Random Forest), SVM (support vector machine), supervised learning, unsupervised learning, semi-supervised learning, etc. Additional examples of architectures include neural networks such as ResNet50, ResNet101, VGG, DenseNet, PointNet, and the like. In some cases, the system may also apply Gaussian blurs, Bayes Functions, color analyzing or processing techniques and/or a combination thereof.</p><p id="p-0051" num="0050">In some cases, the system may provide assistance in generating the user inputs <b>112</b>. For instance, the system may assist with a point selection. As an illustrative example, the system may allow the user input <b>112</b> to snap or align with axis points (e.g., corners, surface midpoints, lines, and the like) that are in proximity to the position of the user input <b>112</b>. For example, the system may allow snapping to axes (e.g., either local or global), elements or objects within the model (such as corners and lines of a model), derivatives of an element or object (such as intersections, midpoints, etc.), visual features in the scene, and the like. In some specific cases, the snapping may be in association with other measurements, such as endpoints of other measurements, midpoints of measurement, and the like. As an illustrative example, if a user <b>102</b> selects a point within a threshold distance of a corner, it is likely the user <b>102</b> intended to select the corner and the system may assist by presenting the user <b>102</b> with the option to select the corner. In some cases, the system may also present a magnifier about or around the point of the user inputs <b>112</b> to assist the user <b>102</b> in placing more precise user inputs <b>112</b>. In some examples, the user input data <b>112</b> may include an outline or defined an area. In these examples, the system may determine the lengths of the boundary of the defined area and/or an area within the defined area.</p><p id="p-0052" num="0051">In some examples, the user <b>102</b> may modify the scene or model <b>104</b> based on the user input data <b>112</b>. For example, the user <b>102</b> may pull, adjust, or otherwise alter the model <b>104</b> while viewing in various different types of views. For example, the user <b>102</b> may pull a wall, enter a width of a hallway or the like. In one particular example, the user <b>102</b> may input reference dimensions for a measurement, and the system <b>100</b> may in response use the input dimensions to adjust the model or scene accordingly.</p><p id="p-0053" num="0052">In some cases, the user <b>102</b> may share a link (such as via email, text message, and the like) to the model or scene using the cloud-based service <b>108</b>. For example, the cloud-based service <b>108</b> may host the model and allow a user to view the model or scene using a browser or hosted application. In this manner, the user <b>102</b> may share the model or scene with family and friends that do not have special software installed. In some cases, the user <b>102</b> may select specific tools that may be accessed or apply restrictions with respect to the shared model.</p><p id="p-0054" num="0053"><figref idref="DRAWINGS">FIGS. <b>2</b>-<b>5</b></figref> are example user interfaces <b>200</b>-<b>500</b> for viewing a three-dimensional scene <b>202</b> of a physical environment according to some implementations. In these examples, the user may be viewing the three-dimensional scene <b>202</b> as if the user was physically present within the scene <b>202</b>. As shown, the user may traverse the scene <b>202</b> by moving within the scene <b>202</b>, such as from the viewpoint shown by interface <b>200</b> to the viewpoint shown by interface <b>300</b>. In various examples, the user may rotate, move forward, backward as well as look up and down within the scene <b>202</b>. In particular, the example user interfaces <b>200</b> and <b>300</b> illustrate a rotation around a viewpoint and the user interface <b>400</b> and <b>500</b> illustrate horizontal movements (such as walking) about the scene <b>202</b>.</p><p id="p-0055" num="0054">In some cases, the user may move through or traverse the scene <b>202</b> by panning horizontally at a predetermined height or altitude above the floor. In some cases, the height may be selected based on an average height of a human, a known height of the viewing user, a user input, or based on the height of the capture device when the physical environment was scanned. In some cases, a slope may be determined when the user is moving about a terrain or model having variable floor height (such as up or down stairs). The user may then be maintained at the predetermined height above the determined slope.</p><p id="p-0056" num="0055">For example, the system may limit the movement of the users to an area defined by a bounding box determined by objects and/or a predetermined distance from a point of capture of the image data used to generate the model, such as a predefined viewpoint. In some cases, the boundary may be configured such that the boundary does not include corners, right angles, or other types of straight intersections. In this manner, the boundary may be a three-dimensional shape that includes only sloped surfaces. In some cases, the bounding box may be formed proximate to the edges of the scene <b>202</b>, such as the walls of a room.</p><p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is another example user interface <b>600</b> for viewing a three-dimensional scene of a physical environment according to some implementations. In this example, the user may be viewing a CAD model or bird's eye view <b>602</b> of the physical environment, such as the environment of the scene <b>202</b> discussed above with respect to <figref idref="DRAWINGS">FIGS. <b>2</b>-<b>5</b></figref>. In the current example, the bird's eye view model <b>602</b> is shown with the ceiling in place but in other examples, such as those illustrated below, the ceiling may be removed to allow a user to view the interior of the model <b>602</b>. The model <b>602</b> is shown as a CAD model but in other cases, the model may be represented as a virtual model including color, textures, and the like. In some examples, the user may be able to transition between the scene views of <figref idref="DRAWINGS">FIGS. <b>2</b>-<b>5</b></figref>, the CAD model <b>602</b> of <figref idref="DRAWINGS">FIG. <b>6</b></figref>, and other views, such as the virtual model of <figref idref="DRAWINGS">FIG. <b>7</b></figref> discussed below or a point cloud model.</p><p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is another example user interface for viewing a three-dimensional scene of a physical environment according to some implementations. In the current view, the user may be viewing a virtual bird's eye view model <b>702</b> as a three-dimensional scene. The virtual bird's eye view model may be similar to the CAD model of <figref idref="DRAWINGS">FIG. <b>6</b></figref>, however representing the colors, textures, and surfaces within the scene.</p><p id="p-0059" num="0058">In the current example, a vertical clipping plane <b>704</b> may be displayed around the bird's eye view model <b>702</b>. The vertical clipping plane <b>704</b> may be manipulated by the user to present a cross section of the model <b>702</b>. For example, as the user moves the vertical clipping plane <b>704</b> up and down, the system may display only the portion of the bird's eye view model <b>702</b> below the vertical clipping plane <b>704</b>. In this manner the user may remove obstructions, such as ceilings, hanging lights, furniture, and the like from view. For instance, by moving the vertical clipping plane <b>704</b> down to the floor, the system may reveal the floor without any furniture or other objects obstructing the user's view.</p><p id="p-0060" num="0059">In some examples, the system may automatically position the clipping plane <b>704</b> based on the model (e.g., size, position, orientation, and the like). For example, the system may recognize the ceiling (either from a scan using computer vision or by knowing types of element the model) and different floors and align the clipping plane <b>704</b> based on the detected ceiling and floors.</p><p id="p-0061" num="0060">In the current example, the clipping plane <b>704</b> is positioned in a vertical manner. However, it should be understood, that in other implementations, the clipping plane <b>704</b> or an additional clipping plane may be positioned about the bird's eye view model <b>702</b> in a horizontal manner, such that the user may move the additional clipping plane to the left and right and/or back and forth to generate additional side view cross sections of the model <b>702</b>. In this example, the system may also present the user with navigation controls or icons, generally indicated by <b>706</b>. The navigation icons <b>706</b> may allow the user to switch between various types of views including, but not limited to, top-down view, perspective view, and/or interior view.</p><p id="p-0062" num="0061"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is another example user interface <b>800</b> for viewing a three-dimensional scene <b>802</b> of a physical environment according to some implementations. In the current example, the user may view the model <b>802</b> from the top down or bird's eye perspective. In this view, the system may allow the user to quickly determine dimensions such as length and width of the environment represented by the scene <b>802</b>. In this example, the system may also present the user with model controls or icons, generally indicated by <b>804</b>. The model controls <b>804</b> may allow the user to switch between various types of models including, but not limited to, top-down CAD models, three-dimensional bird's eye view models, virtual environment or scene, point cloud models, and the like.</p><p id="p-0063" num="0062"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is an example of multiple views <b>902</b>, <b>904</b>, and <b>906</b> of a user interface <b>900</b> for interacting with a three-dimensional scene <b>908</b> of a physical environment according to some implementations. In the current example, the user may first be viewing the scene <b>908</b> as a virtual bird's eye view model, such as shown in view <b>902</b>. The user may then utilize the model controls <b>910</b> to transition to a CAD model, shown in view <b>904</b>. The user may then utilize the vertical clipping plane <b>912</b> to reveal that the main room <b>914</b> of the scene <b>908</b> has a soffit or higher ceiling than the hallway <b>916</b>. The user may then continue to adjust the vertical reveal of the model using the vertical clipping plane <b>912</b> and/or the type of model displayed using the model controls <b>910</b>.</p><p id="p-0064" num="0063"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is another example user interface <b>1000</b> for viewing a three-dimensional scene <b>1002</b> of a physical environment according to some implementations. In this example, the user may again be adjusting the vertical reveal of the scene <b>1002</b> using the vertical clipping plane <b>1004</b>. In this example, the user interface <b>1000</b> may also include visual indicators, generally indicated by <b>1006</b>, to assist the user in determining a height or level at which the cross section is currently positioned.</p><p id="p-0065" num="0064"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is another example user interface <b>1100</b> for viewing a three-dimensional scene <b>1102</b> of a physical environment according to some implementations. In the current example, the user is viewing the scene <b>1102</b> from within the virtual representation. The user is also attempting to select the corner <b>1106</b> of the soffit and the adjacent wall as a starting point for a measurement. In this case, to assist the user with making a more precise selection of the corner point, the user interface <b>1100</b> is presenting an area or region about the user's initial selection in a magnified view, generally indicated as magnifier <b>1104</b>. The magnifier <b>1104</b> may allow the user to input a second selection or to snap or align the selection with the axis points representing the corner <b>1106</b>. In some cases, the magnifier <b>1104</b> may be positioned about the selection point for a predetermined radius or distance. The user may then move the magnifier <b>1104</b> by adjusting a position of the user input device.</p><p id="p-0066" num="0065"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is another example user interface <b>1200</b> for viewing a three-dimensional scene <b>1202</b> of a physical environment according to some implementations. In the current example, the user interface <b>1200</b> is again displaying a magnifier <b>1204</b>. In some cases, the user may be viewing the scene <b>1202</b> via a touch screen or touch enabled display. In these cases, the magnifier <b>1204</b> may be displayed above (or otherwise adjacent to, e.g., beside, next to, or the like) the user input (e.g., above the finger <b>1208</b> of the user) to allow the user to more easily view the magnified region, as the magnified region may otherwise be occluded by the finger <b>1208</b>. In other cases, when the selection point is too close to the edge of the displayed portion of the model (e.g., adjacent to the edge of the display of the device), the user interface <b>1200</b> may display the magnifier <b>1204</b> on the display to the right or left of the selection point such that the entire magnified region is visible within the magnifier <b>1204</b>.</p><p id="p-0067" num="0066"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is another example user interface <b>1300</b> for viewing a three-dimensional scene <b>1302</b> of a physical environment according to some implementations. In the current example, the user is viewing the model from within the three-dimensional scene <b>1302</b>. As discussed above, with respect to <figref idref="DRAWINGS">FIG. <b>11</b></figref>, the user may have selected the corner <b>1304</b> as a starting point for a measurement. In this example, the system may have presented the dashed line <b>1306</b> to the parallel wall as a potential desired measurement. However, in this example, the user may be measuring the length of the soffit from the corner <b>1304</b> to the corner <b>1308</b>. In this example, as the user moves the selection (e.g., the mouse <b>1310</b>) towards the corner <b>1308</b>, the user interface <b>1300</b> may display the distances, as measurement <b>1312</b>. In this manner, the measurement <b>1312</b> may adjust until the user enters a final selection point, such as via a magnifier as discussed above with respect to <figref idref="DRAWINGS">FIGS. <b>11</b> and <b>12</b></figref>.</p><p id="p-0068" num="0067"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is another example user interface <b>1400</b> for viewing a three-dimensional scene <b>1402</b> of a physical environment according to some implementations. In the current example, the user may have input a final selection point with respect to the measurement <b>1312</b> of <figref idref="DRAWINGS">FIG. <b>13</b></figref>. In this example, once the user enters the final selection point (e.g., corner <b>1308</b>), the measurement <b>1312</b> may be fixed and no longer adjust as the user moves the selection point or mouse <b>1310</b>. In some cases, the line <b>1404</b> representing the measurement <b>1312</b> may change styles, colors, or the like to indicate the measurement <b>1312</b> is complete. For example, the line <b>1404</b> may transition from a dashed line to a solid line or the line <b>1404</b> may change from blue to yellow.</p><p id="p-0069" num="0068"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is another example user interface <b>1500</b> for viewing a three-dimensional scene <b>1502</b> of a physical environment according to some implementations. In this example, the user has completed the measurement <b>1312</b> as discussed above and has initiated a capture of a second measurement <b>1504</b>. The measurement <b>1312</b> was discussed as a point to point measurement (e.g., the corner <b>1304</b> to the corner <b>1308</b>). In this example, the second measurement <b>1504</b> may represent a plane to plane measurement between parallel walls, as shown.</p><p id="p-0070" num="0069">In some examples, a color or style of a measurement line may represent a type of measurement. For instance, a height may be represented as a red line while a length or width may be represented as a blue line. In another example, a point to point measurement may be represented as yellow and the plane to plane measurement may be represented as green. In this example, the plane to plane measurement <b>1504</b> also includes circles along each plane whereas the point to point measurement <b>1312</b> does not.</p><p id="p-0071" num="0070">Additionally, the system may determine other measurements that may be of interest to the user based on the selected planes, points, currently displayed measurements, and/or current pointer position. For example, the system may generate the lines <b>1506</b>-<b>1512</b>. The user may then select the displayed lines <b>1506</b>-<b>1512</b> to cause the system to display a corresponding measurement.</p><p id="p-0072" num="0071"><figref idref="DRAWINGS">FIG. <b>16</b></figref> is another example user interface <b>1600</b> for viewing a three-dimensional scene <b>1602</b> of a physical environment according to some implementations. In the current example, the user may be interested in determining a measurement with respect to an object <b>1604</b> (e.g., the chair) positioned within the physical environment and represented within the scene <b>1602</b>. In this example, the user interface <b>1600</b> may display various lines <b>1606</b>-<b>1618</b> illustrating various potential measurements based on a local axis, plane, and/or normals associated with the selected surface of the object <b>1604</b> within the scene <b>1602</b>. The user interface <b>1600</b> may also display multiple measurement selections along the same line. For example, the line <b>1606</b> may interest a second object <b>1620</b> (e.g., the bed). The line <b>1606</b> may be shown in two styles, the first for the portion <b>1622</b> that represents the distance between the selected position and the second object and the second for the portion <b>1624</b> representing the distance from the intersection of the line <b>1606</b> with the bed to the wall. In this example, local axes, planes, and normals are used to recommend measurements. However, it should be understood that in other examples, the user interface <b>1600</b> may also utilize global axes, planes, and/or normals when determining the suggested or recommended measurements.</p><p id="p-0073" num="0072">In some examples, when the user taps (or otherwise inputs) the first point of a measurement, the system may determine local axes (e.g. one axis that extends upward, another axis that is the normal to the surface of this first point, and third axis that is orthogonal to the first two axes in a manner in which each axes intersects the first measurement point). In some cases, the system may illustrate the axes, for instance, in response to the user selecting or causing a snap to the illustrated axis liens <b>1606</b>-<b>1618</b>. For instance, when the user places and moves the second point (not shown), the system may determine or confirm that the second point lies within a threshold distance of an axis (either in 2D and/or 3D), and if the second point is not within the threshold distance, the system may project or move the second point to this axis. In this manner, the user is able to accurately place the second point along the axis without a precise clicks. In other examples, the second point is greater than the threshold (e.g., the second point is not close enough to the axis), then the second point does not snap. In some cases, an axis may intersect a portion of the model or scene that include a hole (e.g., an area of the scene lacking input data). In these cases, the system may automatically adjust the position of the axis to measure a distance between the two surfaces at which the scene is complete on both ends of the measurements.</p><p id="p-0074" num="0073"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is another example user interface <b>1700</b> for viewing a three-dimensional scene <b>1702</b> of a physical environment according to some implementations. In the current example, the user has generated a plane to plane measurement <b>1704</b>. In this example, a measurement control window <b>1706</b> may be displayed with respect to the measurement <b>1704</b> and/or other measurements associated with the scene <b>1702</b>. In the illustrated example, the user interface <b>1700</b> may allow the user to select the type of units (e.g., metric, imperial, and the like), activate and deactivate the magnifier discussed above with respect to <figref idref="DRAWINGS">FIGS. <b>11</b> and <b>12</b></figref>, activate and deactivate automatic plane and/or point selection, and the like.</p><p id="p-0075" num="0074"><figref idref="DRAWINGS">FIG. <b>18</b></figref> is another example user interface <b>1800</b> for viewing a three-dimensional scene <b>1802</b> of a physical environment according to some implementations. In this example, the user is viewing the scene <b>1802</b> as a CAD model. The user interface <b>1800</b> may allow the user to set a transparency associated with the CAD model. For example, the user may view an outline of the structure of the objects, walls, and elements of the scene <b>1802</b> through the exteriors of the CAD model as shown. As another example, if the user desires to view both a scan and a CAD model concurrently, the user may set transparency for either or both of models.</p><p id="p-0076" num="0075"><figref idref="DRAWINGS">FIG. <b>19</b></figref> is another example user interface <b>1900</b> for viewing a three-dimensional scene <b>1902</b> of a physical environment according to some implementations. In this example, the user may be viewing a top-down view of a three-dimensional model. As illustrated, the user may trace a region, generally indicated by <b>1904</b>, and defined by user generated lines <b>1906</b>-<b>1910</b> and system generated line <b>1912</b>. In this example, the user interface <b>1900</b> may display measurements <b>1914</b>-<b>1920</b> associated with the user defined lines <b>1906</b>-<b>1910</b> as well as the measurement <b>1914</b> associated with the area of the defined region <b>1904</b>.</p><p id="p-0077" num="0076"><figref idref="DRAWINGS">FIG. <b>20</b></figref> is another example user interface <b>2000</b> for viewing a three-dimensional scene <b>2002</b> of a physical environment according to some implementations. In this example, the user has defined the region <b>2004</b> by drawing and connecting lines <b>2006</b>-<b>2012</b>. The user may then grab and drag the boundary of the region <b>2004</b> as shown. As the user adjusts the boundary of the region <b>2004</b>, the user interface <b>2000</b> may determine a plurality of circles within the bounded region <b>2004</b>, select the largest circle, and maintain the measurement <b>2014</b> within the center of the largest circle as the user adjusts the viewpoint. In this manner, the user interface <b>2000</b> may adjust in substantially real-time the position of each displayed measurement to ensure that, from a current viewpoint, each of the displayed measurements, including measurements <b>2014</b>, are visible and not occluded by each other.</p><p id="p-0078" num="0077"><figref idref="DRAWINGS">FIG. <b>21</b></figref> is another example user interface <b>2100</b> for viewing a three-dimensional scene <b>2102</b> of a physical environment according to some implementations. In this example, the user may adjust the structure, surfaces, objects, and planes within the scene <b>2102</b> in a manner similar to adjusting the bounded region <b>2004</b> discussed above with respect to <figref idref="DRAWINGS">FIG. <b>20</b></figref>. As shown, the user has used the magnifier <b>2104</b> to select the region <b>2106</b> associated with the floor of the room and to adjust the position of the corner <b>2108</b>, as shown. The user has selected the corner of the room and pulled the corner to the location shown. In this manner, the user may adjust the actual dimensions of the scene <b>2102</b> such as when the system's generated dimensions include some amount of error.</p><p id="p-0079" num="0078"><figref idref="DRAWINGS">FIG. <b>22</b></figref> is another example user interface <b>2200</b> for viewing a three-dimensional scene <b>2202</b> of a physical environment according to some implementations. In this example, the user may generate measurements, such as measurement <b>2204</b>, by selecting or drawing lines atop the model, as shown. In this manner, the user interface <b>2200</b> may display the measurement <b>2204</b> in a manner that is easily visible to the user based on the current viewpoint, does not occlude the model, and provides a visual indication of measured distance, area, or the like.</p><p id="p-0080" num="0079"><figref idref="DRAWINGS">FIG. <b>23</b></figref> is another example user interface <b>2300</b> for viewing a three-dimensional scene <b>2302</b> of a physical environment according to some implementations. In this example, a background <b>2304</b> surrounding the scene <b>2302</b> is shown. In some cases, the background <b>2304</b> may be formed by a color gradient that is darker proximate to the floor of the scene <b>2302</b> and lighter proximate to the top of the scene <b>2302</b>. In this manner, the background <b>2302</b> may assist the user in understanding the dimensions, model position, and the like. In some cases, the system may apply a non-linear correction (such as dithering or adding noise) to the color gradient to present a smooth transition between the intensity of the color in the user interface <b>2300</b>.</p><p id="p-0081" num="0080"><figref idref="DRAWINGS">FIGS. <b>24</b>-<b>26</b></figref> are flow diagrams illustrating example processes associated with generating a three-dimensional scene according to some implementations. The processes are illustrated as a collection of blocks in a logical flow diagram, which represent a sequence of operations, some or all of which can be implemented in hardware, software or a combination thereof. In the context of software, the blocks represent computer-executable instructions stored on one or more computer-readable media that, which when executed by one or more processors, perform the recited operations. Generally, computer-executable instructions include routines, programs, objects, components, encryption, deciphering, compressing, recording, data structures and the like that perform particular functions or implement particular abstract data types.</p><p id="p-0082" num="0081">The order in which the operations are described should not be construed as a limitation. Any number of the described blocks can be combined in any order and/or in parallel to implement the process, or alternative processes, and not all of the blocks need be executed. For discussion purposes, the processes herein are described with reference to the frameworks, architectures and environments described in the examples herein, although the processes may be implemented in a wide variety of other frameworks, architectures or environments.</p><p id="p-0083" num="0082">The processes discussed below with respect to <figref idref="DRAWINGS">FIGS. <b>24</b>-<b>26</b></figref> are discussed with respect to a device such as <figref idref="DRAWINGS">FIG. <b>27</b></figref>. Further, it should be understood that the processes of <figref idref="DRAWINGS">FIGS. <b>24</b>-<b>26</b></figref> may be used together or in conjunction with the examples of <figref idref="DRAWINGS">FIGS. <b>1</b>-<b>23</b></figref> discussed above.</p><p id="p-0084" num="0083"><figref idref="DRAWINGS">FIG. <b>24</b></figref> is an example flow diagram showing an illustrative process <b>2400</b> for generating measurements associated with a three-dimensional scene of a physical environment according to some implementations. As discussed above, a user may utilize the three-dimensional scene to generate accurate and/or precise measurements with respect to the actual physical environment represented by the scene.</p><p id="p-0085" num="0084">At <b>2402</b>, the system may determine if a surface or object is of interest based at least in part on a viewpoint of a user with respect to the three-dimensional scene. For example, the system may identify a door, window, wall, outlet, object (e.g., furniture), or the like is of interest to the user based on a visible portion of the three-dimensional scene. In some cases, the system may input the visible portion of the three-dimensional scene into one or more machine learned models or networks and receive the surface or objects of interest as the output.</p><p id="p-0086" num="0085">At <b>2404</b>, the system may select a measurement type based on a type of the surface or object. For example, if the object was a window, the system may select an area, height of the window, width of the window, height of the window from the floor or ground plane, distance of the window to a nearest perpendicular wall or corner, and the like. In another example, if the object is a chair, the system may select a height, width, length or the like of the chair as the measurement. In this example, the system may also select a distance to each wall or other nearby objects. In yet another example, if the object is an outlet the system may select a height from the floor or ground plane, an area of the outlet, length or width of the outlet, distance of the outlet from a nearest wall, and the like. In some cases, the measurement or measurements selected may be learned via one or more machine learned models or networks.</p><p id="p-0087" num="0086">At <b>2406</b>, the system may determine a first measurement value based at least in part on the measurement type and features of the surface or object. For example, the system may measure a distance to an object, surface, wall or the like and/or an area of the surface or object based on the object of interest and the measurement type selected at <b>2404</b>. In some cases, when determining a distance, the system may determine a position on the current surface and a position on the corresponding surface. For example, if a surface has variation associated with the depth or uniformity and/or the two surfaces associated with the measurement are not parallel with each other, the system may generate two perpendicular lines, one starting with an endpoint on the first plane and the other starting with an endpoint on the second plane. The system may then average the distance between the two distances to generate the distance presented to the user. In other cases, the system may find the midpoint of the two perpendicularly cast lines and compute the distance between the first plane and the midpoint and the second plane and the midpoint and use the two distances to generate the distance between the first plane and the second plane. In still other cases, the system may compute a center of mass for a segmented region of each plane in order to determine a position of each of the corresponding endpoints. In other cases, the system may cast a perpendicular from the first endpoint to the second endpoint</p><p id="p-0088" num="0087">At <b>2408</b>, the system may display the first measurement value at a first position based at least in part on the viewpoint. For example, the system may select a position on the display for the measurement proximate to the measurement and/or the surface that is not occluded by or occluding other measurement values.</p><p id="p-0089" num="0088">At <b>2410</b>, the system may detect a change in position of the viewpoint of the user. For example, the user may move or otherwise traverse the three-dimensional scene construction, thereby causing a change in the viewpoint position. For example, the user may move horizontally, rotate about a point, and/or adjust a tilt of the viewpoint.</p><p id="p-0090" num="0089">At <b>2412</b>, the system may display the first measurement values at a second position based at least in part on the change in position of the viewpoint. For example, the system may select a second position on the display for the measurement proximate to the measurement based on the change in position of the viewpoint and/or the surface and at a location that is not occluded by or occluding other measurement values. As an example, as the viewpoint is moved, various displayed measurement values may occlude each other. As such the system may adjust the position of the displayed measurements values to, for instance, maintain a first distance between the measurement value and the visual representation of the measurement (e.g., the line, lines, boundary, region, or the like) below or equal to a first threshold and maintain a second distance between the measurement value and other displayed measurements above or equal to a second threshold.</p><p id="p-0091" num="0090">At <b>2414</b>, the system may determine that the first measurement value occludes a second measurement value at the second position and, at <b>2416</b>, the system may display the first measurement value at a third position based at least in part on a position of the second measurement value. For instance, in the specific example above, the system may adjust the position to maintain the second threshold between the two measurement values.</p><p id="p-0092" num="0091"><figref idref="DRAWINGS">FIG. <b>25</b></figref> is another example flow diagram showing an illustrative process <b>2500</b> for displaying measurements associated with a region of a three-dimensional scene of a physical environment according to some implementations. For example, the system may present multiple measurements associated with the three-dimensional scene. In some cases, the system may define areas as measurement values and desire to maintain the area's displayed within the associated bounded region.</p><p id="p-0093" num="0092">At <b>2502</b>, the system may define a region based at least in part on a user input. For example, the user may completely or partially define a region on the display. For instance, the user may trace lines around an object or surface and the system may present a boundary to the region based on the object or surface dimensions and the portion of the region defined by the user. For instance, if the user selects two sides of a window, the system may define the region as the area associated with the window.</p><p id="p-0094" num="0093">At <b>2504</b>, the system may determine a plurality of circles that fit within the region. For instance, the system may compute circles within a bounded region based on the current viewpoint. As the viewpoint is changed the system may update and compute additional circles that fit within the region to assist with adjusting the position of the measurement value displayed with respect to the region.</p><p id="p-0095" num="0094">At <b>2506</b>, the system may select the largest circle for the plurality of circles to use as the circle for displaying the corresponding measurement value and, at <b>2508</b>, the system may display the measurement evaluation at the center of the largest circle. For instance, the system may then select the largest circle that is not occluded by another measurement value. The system may then display the measurement value within the center of the largest nonoccluded circle. In the current case that the largest circle is occluded, the system may select a second largest circle within the bounded region and display the measurement value at the center of the second largest circle such that each desired measurement is fully visible to the user.</p><p id="p-0096" num="0095">At <b>2510</b>, the system may receive an additional user input to adjust the region. For instance, the system may define the region as the window, as discussed above, but the user may adjust or move the boundary of the region to define an area associated with the wall including the window. In this case, the process <b>2500</b> may return to <b>2502</b> and redisplay the measurement value at a new position. In other cases, the user may adjust the viewpoint and again the process <b>2500</b> may return to <b>2502</b> and redisplay the measurement value.</p><p id="p-0097" num="0096"><figref idref="DRAWINGS">FIG. <b>26</b></figref> is another example flow diagram showing an illustrative process for receiving user inputs with respect to a three-dimensional scene according to some implementations. In some cases, to assist the user in selecting points or providing user inputs, the system may display a magnifier, as discussed above, that allows for more precise user inputs for more precise measurements, such as when the measurement are being used for construction, reconstruction, repair, modification or the like with respect to the physical environment.</p><p id="p-0098" num="0097">At <b>2602</b>, the system may detect a user input associated with a three-dimensional scene. For instance, the user may be viewing the scene as a bird's eye view, from within the scene, a point cloud, or the like and the user may be entering a selection of a particular point within the scene. In this case, the selection of the particular point may be precise, such as to determine a measurement such as a length, width, or the like.</p><p id="p-0099" num="0098">At <b>2604</b>, the system may display a magnified region. The magnified region may be about an initial selection point. For instance, the system may display a magnified region at a predefined magnification and with a predefined radius about the initial selection point. In some cases, the system may display the magnified region with respect to the selection point. In other cases, the system may display the magnified region with respect to the user input device. For instance, if the user is engaged with the scene via a touch-enabled display, the user's input device may be a finger or stylus that may be occluding the magnifier if the magnifier was positioned about the initial selection point. In this case, the system may display the magnifier above or to the right of the position if the user input device with respect to the display. In this manner, the magnifier may be positioned such that the magnifier is not occluded by the user's finger or the stylus.</p><p id="p-0100" num="0099">At <b>2606</b>, the system may adjust the position of the magnified region in response to detecting a change in position of the user input. For instance, the system may detect an intersection between an edge of the magnified region and the user input (e.g., the user has moved the input device&#x2014;mouse, stylus, finger, etc.) to the edge of the magnified region. In this example, the system may move the magnified region with the user input device until the user selects a final position for the selection point.</p><p id="p-0101" num="0100">At <b>2608</b>, the system may maintain the magnified region on the display in response to detecting an intersection of an edge of the magnified region with an edge of the displayed portion of the scene. For example, if the user moves the input position to the edge of the display the system may pan or move the display as well as the magnified region. In other cases, if the user moves the input position to the edge of the model or scene the system may move the magnified region, such that the magnified region remains entirely on the display (e.g., that no portion of the magnified region is outside of the display). In some cases, the magnified region may be adjusted based on the edge of the display and the position of the input, such that the magnified region is not off the display and not occluded by the input device (e.g., the stylus, finger, or the like).</p><p id="p-0102" num="0101"><figref idref="DRAWINGS">FIG. <b>27</b></figref> is an example device <b>2700</b> associated with consuming a three-dimensional scene according to some implementations. As described above, the device <b>2700</b> may be used by a user to scan, generate, and/or consume three-dimensional models or scenes associated with physical environments. In the current example, the device <b>2700</b> may include image components <b>2702</b> for capturing visual data, such as image data, video data, depth data, color data, infrared data, or the like from a physical environment surrounding the device <b>2700</b>. For example, the image components <b>2702</b> may be positioned to capture multiple images from substantially the same perspective (e.g., a position proximate to each other on the device <b>2700</b>). The image components <b>2702</b> may be of various sizes and quality, for instance, the image components <b>2702</b> may include one or more wide screen cameras, three-dimensional cameras, high definition cameras, video cameras, infrared camera, depth sensors, monocular cameras, among other types of sensors. In general, the image components <b>2702</b> may each include various components and/or attributes.</p><p id="p-0103" num="0102">In some cases, the device <b>2700</b> may include one or more position sensors <b>2704</b> to determine the orientation and motion data of the device <b>2700</b> (e.g., acceleration, angular momentum, pitch, roll, yaw, etc.). The position sensors <b>2704</b> may include one or more IMUs, one or more accelerometers, one or more gyroscopes, one or more magnetometers, and/or one or more pressure sensors, as well as other sensors. In one particular example, the position sensors <b>2704</b> may include three accelerometers placed orthogonal to each other, three rate gyroscopes placed orthogonal to each other, three magnetometers placed orthogonal to each other, and a barometric pressure sensor.</p><p id="p-0104" num="0103">The device <b>2700</b> may also include one or more communication interfaces <b>2706</b> configured to facilitate communication between one or more networks and/or one or more cloud-based services, such as a cloud-based services <b>108</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. The communication interfaces <b>2706</b> may also facilitate communication between one or more wireless access points, a master device, and/or one or more other computing devices as part of an ad-hoc or home network system. The communication interfaces <b>2706</b> may support both wired and wireless connection to various networks, such as cellular networks, radio, WiFi networks, short-range or near-field networks (e.g., Bluetooth&#xae;), infrared signals, local area networks, wide area networks, the Internet, and so forth.</p><p id="p-0105" num="0104">The device <b>2700</b> may also include one or more displays <b>2708</b>. The displays <b>2708</b> may include a virtual environment display or a traditional two-dimensional display, such as a liquid crystal display or a light emitting diode display. The device <b>2700</b> may also include one or more input components <b>2710</b> for receiving feedback from the user. In some cases, the input components <b>2710</b> may include tactile input components, audio input components, or other natural language processing components. In one specific example, the displays <b>2708</b> and the input components <b>2710</b> may be combined into a touch enabled display.</p><p id="p-0106" num="0105">The device <b>2700</b> may also include one or more processors <b>2712</b>, such as at least one or more access components, control logic circuits, central processing units, or processors, as well as one or more computer-readable media <b>2714</b> to perform the function associated with the virtual environment. Additionally, each of the processors <b>2712</b> may itself comprise one or more processors or processing cores.</p><p id="p-0107" num="0106">Depending on the configuration, the computer-readable media <b>2714</b> may be an example of tangible non-transitory computer storage media and may include volatile and nonvolatile memory and/or removable and non-removable media implemented in any type of technology for storage of information such as computer-readable instructions or modules, data structures, program modules or other data. Such computer-readable media may include, but is not limited to, RAM, ROM, EEPROM, flash memory or other computer-readable media technology, CD-ROM, digital versatile disks (DVD) or other optical storage, magnetic cassettes, magnetic tape, solid state storage, magnetic disk storage, RAID storage systems, storage arrays, network attached storage, storage area networks, cloud storage, or any other medium that can be used to store information and which can be accessed by the processors <b>2712</b>.</p><p id="p-0108" num="0107">Several modules such as instruction, data stores, and so forth may be stored within the computer-readable media <b>2714</b> and configured to execute on the processors <b>2712</b>. For example, as illustrated, the computer-readable media <b>2714</b> store scene viewing instructions <b>2716</b>, axis selection instructions <b>2718</b>, measurement type selection instructions <b>2720</b>, measurement generation instructions <b>2722</b>, point selection instructions <b>2724</b>, region selection instructions <b>2726</b>, magnifier instructions <b>2728</b>, measurement display instructions <b>2730</b> as well as other instructions <b>2732</b>, such as operating instructions. The computer-readable media <b>2714</b> may also store data usable by the instructions <b>2716</b>-<b>2732</b> to perform operations. The data may include image data <b>2734</b> such as frames of a physical environment, measurement data <b>2736</b>, machine learned model data <b>2738</b>, machine learned model or network data <b>2740</b>, surface normal data <b>2742</b>, as discussed above.</p><p id="p-0109" num="0108">The scene viewing instructions <b>2716</b> may be configured to allow the user to view the scene or model via various views. For instance, the scene may be viewed as a traversable model (e.g., the user is within the scene), as a three-dimensional bird's eye view model, a CAD model, a top-down layout, a point cloud, and the like. The scene viewing instructions <b>2716</b> may also allow the user to transition between viewing and/or model types by selecting various options.</p><p id="p-0110" num="0109">In some cases, the scene viewing instructions <b>2716</b> may allow the user to apply vertical and/or horizontal cross sections of the model or scene, remove ceilings, floor, and/or walls from the scene, add or remove furniture or objects from the scene, and the like. In some specific instances, such as when the user is traversing the scene as a virtual environment, the scene viewing instructions <b>2716</b> may allow the user to move within a defined boundary, from viewpoint to viewpoint, and the like. In the case of the boundary, the boundary area may be defined without corners or edges to allow a smooth horizontal panning throughout the scene.</p><p id="p-0111" num="0110">In another specific example, if the user is viewing the model or scene, the scene viewing instructions <b>2716</b> may render corner lines and/or features in front of walls and an actual physical position to provide a more realistic viewing experience and prevent occluding one wall by the other as the user traverses the scene.</p><p id="p-0112" num="0111">The scene viewing instructions <b>2716</b> may also determine a physical area, jurisdiction, or region (e.g., the United States or Canada) associated with the physical environment based at least in part on the style, content, measurement units, objects, and the like of the scene. In some cases, the scene viewing instructions <b>2716</b> may change an appearance, background color, theme, or the like of the scene or model based on the determined physical area jurisdiction, or region. In other cases, the scene viewing instructions <b>2716</b> may set a time zone associated with the model or scene based on the determined physical area, jurisdiction, or region.</p><p id="p-0113" num="0112">The axis selection instructions <b>2718</b> may be configured to select or display various global and local axes based on a selected surface, plane, object, point or the like. For example, the if the user selects the side of a chair, the system may display local axes based on the normal of the selected surface (e.g., the side of the chair) as well as global axes based on the orientation and/or normals of the room or environment. In some cases, the axis selection instructions <b>2718</b> may display each of the axes as lines within the scene using in various colors, shading, thickness, textures, types and the like to distinguish each axis. For example, global axes may be in solid lines and local axes may be in dashed lines. In other cases, the axes may be shown as solid until the axes intersect an object in the environment and subsequently (e.g., after the intersection) shown as dashed or broken. In some cases, the user may select an axis to obtain a measurement associated therewith.</p><p id="p-0114" num="0113">The measurement type selection instructions <b>2720</b> may be configured to select a type of measurement intended by a user selection. For instance, the measurement type selection instructions <b>2720</b> may select a type of measurement (e.g., area, volume, distance, and the like) based on an area or object of interest of the user, a user selection or input (e.g., a selection of a point, surface, and/or object), and the type or class associated with the area or object. As an illustrative example, if the user is viewing a painting on the wall, the measurement type selection instructions <b>2720</b> may select a distance to the floor as a first measurement type and an area of the painting as a second measurement type. In some cases, the measurement type selection instructions <b>2720</b> may select a predetermined number of measurement types, such as two, three, four, five, etc.</p><p id="p-0115" num="0114">The measurement generation instructions <b>2722</b> may be configured to determine the value of the selected measurements and/or measurement types. For example, if the user selects two points, planes, surfaces, or the like, the measurement generation instructions <b>2722</b> may determine a distance between them. In some cases, the measurement generation instructions <b>2722</b> may also determine a total distance associated with a chain of lines or measurements selected by the user, such as if the user defines a region about a doorway or the like. In this manner, the measurement generation instructions <b>2722</b> may combine measurements to generate both individual distances such as height and width of the doorway as well as a total exterior length of the doorway and/or area within the defined region.</p><p id="p-0116" num="0115">The point selection instructions <b>2724</b> may be configured to assist the user in selecting a desired point and generating accurate measurements. For instance, if the user selects a position within a predefined distance (such as a pixel distance) from a corner or edge, the point selection instructions <b>2724</b> may cause the user's selection to snap or otherwise align with the corner or edge. In some cases, the point selection instructions <b>2724</b> may allow the user to enable and disable the snap to axis, corner, intersection and the like.</p><p id="p-0117" num="0116">The region selection instructions <b>2726</b> may be configured to select a region of the scene for the user. For example, if a user selects a wall, the region selection instructions <b>2726</b> may outline the wall and/or features, such as light switches, outlets, paintings, and the like associated with the wall. In this manner, the user may select with a single click the desired region, thereby preventing the user from having to outline or define the entire area.</p><p id="p-0118" num="0117">The magnifier instructions <b>2728</b> may be configured to present a magnified region about an initial selection point for the user to assist with more precise point selection and more accurate measurements. For instance, the magnifier instructions <b>2728</b> may display a magnified region at a predefined magnification and with a predefined radius about the initial selection point. In some cases, the magnifier instructions <b>2728</b> may display the magnified region with respect to the selection point. In other cases, the magnifier instructions <b>2728</b> may display the magnified region with respect to the user input device. For instance, if the user is engaged with the scene via a touch-enabled display, the users input device may be a finger or stylus that may be occluding the magnifier if the magnifier was positioned about the initial selection point. In this case, the magnifier instructions <b>2728</b> may display the magnifier above or to the right of the position if the user input device with respect to the display. In this manner, the magnifier may be positioned such that the magnifier is not occluded by the user's finger or the stylus.</p><p id="p-0119" num="0118">The magnifier instructions <b>2728</b> may also adjust the position of the magnified region in response to detecting a change in position of the user input. For instance, the magnifier instructions <b>2728</b> may detect an intersection between an edge of the magnified region and the user input (e.g., the user has moved the input device&#x2014;mouse, stylus, finger, etc.) to the edge of the magnified region. In this example, the system may move the magnified region with the user input device until the user selects a final position for the selection point.</p><p id="p-0120" num="0119">The magnifier instructions <b>2728</b> may maintain the magnified region on the display in response to detecting an intersection of an edge of the magnified region with an edge of the displayed portion of the scene. For example, if the user moves the input position to the edge of the display or viewing window the system may pan or move the display as well as the magnified region. In other cases, if the user moves the input position to the edge of the model or scene, the magnifier instructions <b>2728</b> may move the magnified region, such that the magnified region remains entirely on the display (e.g., that no portion of the magnified region is outside of the display). In some cases, the magnifier instructions <b>2728</b> may adjust the magnified region based on the edge of the display and the position of the input, such that the magnified region is not off the display and not occluded by the input device (e.g., the stylus, finger, or the like).</p><p id="p-0121" num="0120">The measurement display instructions <b>2730</b> may be configured to display the measurement values determined by the measurement generation instructions <b>2722</b>. In some cases, the measurement generation instructions <b>2722</b> may be configured to adjust the position of the displayed measurements as the user either traverses the scene or adjusts a viewpoint of the model, as discussed above. In some cases, the measurement generation instructions <b>2722</b> may limit the number of displayed measurements to below a predetermined threshold (such as 3, 5, 7, 10, or the like). In one example, the number of measurements displayed may be based at least in part on a distance between the measurements, zoom and/or proximity of the viewpoint to the displayed measurements, region of interest, or the like. For example, the further the user is from the model, the fewer measurements may be displayed.</p><p id="p-0122" num="0121">Although the subject matter has been described in language specific to structural features, it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features described. Rather, the specific features are disclosed as illustrative forms of implementing the claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method comprising:<claim-text>presenting, on a display, a scene representative of a physical environment as a three-dimensional environment, the scene presented from a first viewpoint;</claim-text><claim-text>receiving an input to transition the first viewpoint to a second viewpoint;</claim-text><claim-text>determining that the second viewpoint exceeds a predefined three-dimensional boundary, the predefined three-dimensional boundary being substantially smooth; and</claim-text><claim-text>causing a trajectory associated the transition to change based at least in part on a position of the predefined three-dimensional boundary with respect to the scene.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>the scene is a first scene;</claim-text><claim-text>the predefined three-dimensional boundary is a first predefined three-dimensional boundary;</claim-text><claim-text>transitioning the first viewpoint to the second viewpoint includes transition from the first scene to a second scene, the second scene having a second predefined three-dimensional boundary; and</claim-text><claim-text>causing the trajectory associated the transition to change further comprises smoothly transitioning to a position within the second predefined three-dimensional boundary.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein causing the trajectory associated the transition to change includes at least one of the following:<claim-text>causing a direction of travel along the predefined three-dimensional boundary; or</claim-text><claim-text>causing the trajectory associated the transition to change includes causing a user to bounce off the predefined three-dimensional boundary.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising rotating the scene in response to receiving a signal from a motion sensor associated with the display.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>receiving a first user input of a first position within the scene;</claim-text><claim-text>presenting, on the display, a magnified region about the first position;</claim-text><claim-text>receiving a second user input of a second position associated with the scene, the second position within the magnified region; and</claim-text><claim-text>utilizing the second user input as an input to cause one or more processors perform an operation.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>determining a position associated with the scene, the position being a variable distance from the first viewpoint; and</claim-text><claim-text>in response to receiving a user input to rotate the scene, causing the scene to rotate about the position.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>receiving an input to transition from the second viewpoint to a third viewpoint;</claim-text><claim-text>determining that the third viewpoint exceeds a second predefined three-dimensional boundary associated with the second viewpoint; and</claim-text><claim-text>causing a second trajectory associated the transition from the second viewpoint to the third viewpoint to follow the second predefined three-dimensional boundary, the second predefined three-dimensional boundary having rounded edges and corners to form a substantially smooth boundary.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. A method comprising:<claim-text>presenting, on a display, a scene representative of a physical environment from a first viewpoint;</claim-text><claim-text>receiving a first user input of a first surface within the scene;</claim-text><claim-text>determining a measurement type for a first measurement based at least in part on a characteristic of the first surface; and</claim-text><claim-text>presenting, on the display, a first measurement of the measurement type at a first position.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method as recited in <claim-ref idref="CLM-00008">claim 8</claim-ref>, further comprising:<claim-text>receiving an input to transition the first viewpoint to a second viewpoint;</claim-text><claim-text>presenting, on the display, the scene representative of the physical environment from a second viewpoint; and</claim-text><claim-text>presenting, on the display, the first measurement at a second position based at least in part on the first surface and the second viewpoint;</claim-text><claim-text>receiving an input to transition from the first viewpoint to the second viewpoint;</claim-text><claim-text>determining that the second viewpoint exceeds a predefined three-dimensional boundary, the predefined three-dimensional boundary being substantially smooth; and</claim-text><claim-text>causing a trajectory associated the transition to change based at least in part on a position of the predefined three-dimensional boundary with respect to the scene;</claim-text><claim-text>determining, based at least in part on the second viewpoint, that the first measurement is occluded; and</claim-text><claim-text>responsive to determining the first measurement is occluded, presenting, on the display, the first measurement of the measurement type at a second position.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method as recited in <claim-ref idref="CLM-00008">claim 8</claim-ref>, further comprising:<claim-text>receiving an input to transition the first viewpoint to a second viewpoint;</claim-text><claim-text>presenting, on the display, the scene representative of the physical environment from a second viewpoint;</claim-text><claim-text>presenting, on the display, the first measurement at a second position based at least in part on the first surface and the second viewpoint;</claim-text><claim-text>receiving an input to transition from the first viewpoint to the second viewpoint;</claim-text><claim-text>determining, based at least in part on the second viewpoint, that the first measurement is occluded; and</claim-text><claim-text>responsive to determining the first measurement is occluded, removing, from the display, the first measurement of the measurement type.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method as recited in <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the first viewpoint is a bird's eye view of the physical environment and the method further comprises:<claim-text>presenting a vertical clipping plane with respect to the physical environment, the vertical clipping plane manipulable by a user to cause the display to present various sections of the physical environment.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method as recited in <claim-ref idref="CLM-00008">claim 8</claim-ref>, further comprising:<claim-text>responsive to determining, based at least in part on the first position, that the first measurement is occluded, presenting, on the display, the first measurement of the measurement type at a second position, the second position closer to the first viewpoint than the first position.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method as recited in <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein determining the measurement type for the first measurement further comprises:<claim-text>detecting a second surface associated with the first surface; and</claim-text><claim-text>the measurement type is based at least in part on a relationship of the first surface to the second surface.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method as recited in <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein determining the measurement type for the first measurement further comprises:<claim-text>displaying a plurality of lines associated with the first surface based at least in part on a local axis associated with the first surface;</claim-text><claim-text>receiving a selection of a selected line from the plurality of lines; and</claim-text><claim-text>wherein the measurement type is based at least in part on the selected line.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The method as recited in <claim-ref idref="CLM-00014">claim 14</claim-ref>, further comprising:<claim-text>displaying a plurality of lines associated with the first surface based at least in part on a local axis associated with the first surface;</claim-text><claim-text>receiving a selection of a selected line from the plurality of lines;</claim-text><claim-text>determining the selected line intersects a portion of the scene that is missing at least some data; and</claim-text><claim-text>adjusting a position of at least one endpoint of the selected line based at least in part on the hole.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The method as recited in <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein determining the measurement type for the first measurement further comprises:<claim-text>determining a first line segment associated with the user input;</claim-text><claim-text>determining at least a second line segment based at least in part on the user input and the first line segment; and</claim-text><claim-text>determining the measurement type based at least in part on the first line segment and the second line segment.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The method as recited in <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the measurement type is at least one of:<claim-text>a length of the first line segment and the second line segment;</claim-text><claim-text>an area at least partially defined by the first line segment and the second line segment; or</claim-text><claim-text>a measurement associated with an object defined by the first line segment and the second line segment.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. One or more non-transitory computer-readable media having computer-executable instructions that, when executed by one or more processors, cause the one or more processors to perform operations comprising:<claim-text>presenting, on a display of a user device, a scene representative of a physical environment from a first viewpoint;</claim-text><claim-text>receiving a first user input of a first surface within the scene;</claim-text><claim-text>determining a measurement type for a first measurement based at least in part on a characteristic of the first surface; and</claim-text><claim-text>presenting, on the display, a first measurement of the measurement type at a first position.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The one or more computer-readable media as recited in <claim-ref idref="CLM-00018">claim 18</claim-ref>, further comprising receiving a physical location of the user device and wherein:<claim-text>determining the measurement type further comprises selecting a measurement unit based at least in part on the physical location.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The one or more computer-readable media as recited in <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein determining the measurement type for the first measurement further comprises:<claim-text>displaying a plurality of lines associated with the first surface based at least in part on a plane or normal associated with the first surface; and</claim-text><claim-text>receiving a selection of a selected line from the plurality of lines; and</claim-text><claim-text>wherein the measurement type is based at least in part on the selected line.</claim-text></claim-text></claim></claims></us-patent-application>