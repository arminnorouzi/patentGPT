<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005348A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005348</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17782435</doc-number><date>20201203</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>FR</country><doc-number>FR1913824</doc-number><date>20191205</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>08</class><subclass>B</subclass><main-group>13</main-group><subgroup>22</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>08</class><subclass>B</subclass><main-group>13</main-group><subgroup>22</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc></classifications-cpc><invention-title id="d2e61">FRAUD DETECTION SYSTEM AND METHOD</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>KNAP</orgname><address><city>Biot</city><country>FR</country></address></addressbook><residence><country>FR</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>LETIERCE</last-name><first-name>Dylan</first-name><address><city>Peymeinade</city><country>FR</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>MALGOGNE</last-name><first-name>Jonathan</first-name><address><city>Auribeau</city><country>FR</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>CHALOIN</last-name><first-name>Christophe</first-name><address><city>Le Cannet</city><country>FR</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>MANDRIOLI</last-name><first-name>Damien</first-name><address><city>Villeneuve Loubet</city><country>FR</country></address></addressbook></inventor></inventors></us-parties><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/EP2020/084359</doc-number><date>20201203</date></document-id><us-371c12-date><date>20220705</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A method for detecting fraud in the event of the purchase of at least one item by at least one user, the method including at least a step of capturing a plurality of data, including at least the following steps, a step of processing, by the computer processing unit, the plurality of data, including at least the following steps, a step of determining a probability of fraud.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="151.21mm" wi="158.75mm" file="US20230005348A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="138.26mm" wi="138.43mm" file="US20230005348A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="166.20mm" wi="164.51mm" file="US20230005348A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="170.60mm" wi="160.78mm" file="US20230005348A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="141.82mm" wi="153.08mm" file="US20230005348A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="168.57mm" wi="163.15mm" file="US20230005348A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD OF THE INVENTION</heading><p id="p-0002" num="0001">The present invention relates to the field of fraud detection when purchasing items. It finds a particularly advantageous application in the field of mass distribution and carts, smart shopping baskets or cash register devices.</p><heading id="h-0002" level="1">PRIOR ART</heading><p id="p-0003" num="0002">There are several ways to check up whether an item is not stolen. Currently, the most exploited technology concerns the so-called RFID technology. There are also systems for validating shopping carts by analysing the weight of each item added to the cart.</p><p id="p-0004" num="0003">However, this type of solution is on the one hand very expensive and/or unreliable. Indeed, fraudsters are constantly imagining new ways to circumvent the anti-fraud systems put in place in supermarkets, for example.</p><p id="p-0005" num="0004">Hence, an object of the present invention is to propose a solution to at least some of these problems.</p><p id="p-0006" num="0005">The other objects, features and advantages of the present invention will become apparent from a review of the following description and the appended drawings. It should be understood that other benefits may be incorporated.</p><heading id="h-0003" level="1">SUMMARY OF THE INVENTION</heading><p id="p-0007" num="0006">The present invention relates to a method for detecting fraud in the event of the purchase by at least one user of at least one item comprising at least:<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0000">    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0007">A capturing step, performed by at least one user terminal, of a plurality of data from at least one sensor, and preferably from a plurality of sensors, comprising at least the following steps:        <ul id="ul0003" list-style="none">            <li id="ul0003-0001" num="0008">i. Obtainment of an identifier of the item by at least one identification device;</li>            <li id="ul0003-0002" num="0009">ii. Determination by at least one optical device of at least one trajectory of the item manually moved by the user in a three-dimensional space, said three-dimensional space comprising at least:            <ul id="ul0004" list-style="none">                <li id="ul0004-0001" num="0010">1. An identification area corresponding to a volume of the three-dimensional space in which at least one portion of the item is disposed by the user to obtain the identifier of the item;</li>                <li id="ul0004-0002" num="0011">2. An entrance area corresponding to a volume of the three-dimensional space crossed by the item when the user deposits the item in at least one container associated with the user terminal;</li>            </ul>            </li>            <li id="ul0003-0003" num="0012">iii. Sending by the user terminal to at least one computer processing unit of:            <ul id="ul0005" list-style="none">                <li id="ul0005-0001" num="0013">1. the identifier of the item from the identification device;</li>                <li id="ul0005-0002" num="0014">2. the trajectory of the item;</li>            </ul>            </li>        </ul>        </li>        <li id="ul0002-0002" num="0015">A processing step performed by the computer processing unit, of the plurality of data comprising at least the following steps:        <ul id="ul0006" list-style="none">            <li id="ul0006-0001" num="0016">i. Generation of at least one behaviour of said item from at least the trajectory of the item in the three-dimensional space;</li>            <li id="ul0006-0002" num="0017">ii. Comparison of the behaviour of said item with a plurality of predetermined behaviour models so as to identify a handling anomaly;</li>        </ul>        </li>        <li id="ul0002-0003" num="0018">A step of determining a probability of fraud as a function of said behaviour comparison.</li>    </ul>    </li></ul></p><p id="p-0008" num="0019">The present invention cleverly uses a plurality of sensors to cross-check a plurality of data so as to identify a fraud situation.</p><p id="p-0009" num="0020">For example, the proposed process allows identifying a manipulation made by the user consisting in adding an item without identifying and therefore counting it at first. In this case, the item is not identified by the user before reaching the entrance area and being placed in the container.</p><p id="p-0010" num="0021">Advantageously, the present invention allows determining the behaviour of an item so as to identify whether or not this behaviour is consistent with a behaviour considered as standard, i.e. non-fraudulent. In a simple and reliable manner, the present invention allows classifying a behaviour as potentially fraudulent behaviour as long as it deviates beyond a predetermined threshold from one or several standard behaviour model(s). The present invention cleverly uses a plurality of predetermined behaviour models comprising one or several standard behaviour model(s).</p><p id="p-0011" num="0022">The present invention allows detecting a plurality of frauds when purchasing an item in a store, for example using automatic checkout systems for example, or else so-called smart carts.</p><p id="p-0012" num="0023">The present invention solves most, if not all, fraud situations.</p><p id="p-0013" num="0024">The present invention allows guiding the customer during his purchase process and identifying fraud or errors without the user automatically receiving notification. Since the contents of the cart are checked almost in real-time, payment without passing through a checkout or terminal and without direct control of all of the contents of a cart is therefore possible thanks to the present invention.</p><p id="p-0014" num="0025">Since the calculation of the probability of fraud is done on items and/or actions, if one or several fraud(s) are suspected, the customers will be checked only on one or several item(s) and not on the entire cart.</p><p id="p-0015" num="0026">This could allow the user to have more information on his purchases, whether this information relates to the items but also the price of his basket.</p><p id="p-0016" num="0027">In a particularly advantageous manner, the step of capturing a plurality of data comprises at least one measurement, by at least one measuring device, of the weight of the item, and a step of sending by the user terminal to the computer processing unit the measured weight of the item.</p><p id="p-0017" num="0028">Preferably, the processing step comprises, preferably before the step of generating the behaviour of the item, at least the following steps:<ul id="ul0007" list-style="none">    <li id="ul0007-0001" num="0000">    <ul id="ul0008" list-style="none">        <li id="ul0008-0001" num="0029">a. Identification in at least one database of the item from the identifier, the database comprising at least the identifier of the item associated with a predetermined weight of the item;</li>        <li id="ul0008-0002" num="0030">b. Obtainment of the predetermined weight of the item from the database:        <ul id="ul0009" list-style="none">            <li id="ul0009-0001" num="0031">i. In the event that the predetermined weight is equal to zero or is not input, the computer processing unit assigns the measured weight of the item as the predetermined weight associated with said identifier in the database;</li>            <li id="ul0009-0002" num="0032">ii. In the case where the predetermined weight is different from zero and input, the computer processing unit performs a comparison of the predetermined weight and the measured weight so as to identify a weight anomaly if the weight difference is greater than a predetermined threshold.</li>        </ul>        </li>    </ul>    </li></ul></p><p id="p-0018" num="0033">Advantageously, the determination of a probability of fraud is carried out according to said comparison of the predetermined weight with the measured weight, this probability being non-zero if a weight anomaly has been identified.</p><p id="p-0019" num="0034">By smartly coupling an analysis of the weight and the image, the present invention allows reducing, and possibly avoiding, any fraud.</p><p id="p-0020" num="0035">The present invention also relates to a system for detecting at least one fraud in the event of the purchase by a user of at least one item in a store, comprising at least:<ul id="ul0010" list-style="none">    <li id="ul0010-0001" num="0000">    <ul id="ul0011" list-style="none">        <li id="ul0011-0001" num="0036">A user terminal comprising at least:        <ul id="ul0012" list-style="none">            <li id="ul0012-0001" num="0037">i. An identification device configured to identify the item when a user passes the item in the proximity, preferably within one meter, of the identification device;</li>            <li id="ul0012-0002" num="0038">ii. A measuring device configured to measure the weight of the item;</li>            <li id="ul0012-0003" num="0039">iii. An optical device configured at least to determine at least one trajectory of the item manually moved by the user in the three-dimensional space;</li>        </ul>        </li>        <li id="ul0011-0002" num="0040">A computer processing unit in communication with at least the user terminal, the computer processing unit being remote or not from the user terminal and being configured to:        <ul id="ul0013" list-style="none">            <li id="ul0013-0001" num="0041">i. Generate at least one behaviour of said item at least from the trajectory of the item in the three-dimensional space;</li>            <li id="ul0013-0002" num="0042">ii. Compare the behaviour of said item with a plurality of predetermined behaviour models so as to identify a handling anomaly;</li>        </ul>        </li>    </ul>    </li></ul></p><p id="p-0021" num="0043">So as to determine a probability of fraud as a function of said behaviour comparison, this probability being non-zero if a handling anomaly has been identified.</p><p id="p-0022" num="0044">Advantageously, the computer processing unit is further in communication with a database comprising the identifier of the item associated with a predetermined weight of the item.</p><p id="p-0023" num="0045">Preferably, the computer processing unit is further in communication with a data comparison module configured to compare the measured weight with the weight indicated in the database according to the identified item, the comparison module being configured to identify a weighing anomaly.</p><p id="p-0024" num="0046">Advantageously, the optical device is configured to further collect a plurality of images of the item, and the computer processing unit is further in communication with a module for analysing the images collected by said optical device configured to identify a handling anomaly.</p><p id="p-0025" num="0047">Preferably, the computer processing unit is further configured to:<ul id="ul0014" list-style="none">    <li id="ul0014-0001" num="0000">    <ul id="ul0015" list-style="none">        <li id="ul0015-0001" num="0048">a. Compare the predetermined weight of the item obtained from the database with the measured weight so as to identify a weight anomaly if the weight difference is greater than a predetermined threshold;</li>        <li id="ul0015-0002" num="0049">b. Determine a probability of fraud according to said weight comparison, this probability being non-zero if a weight anomaly has been identified.</li>    </ul>    </li></ul></p><p id="p-0026" num="0050">Advantageously, the computer processing unit is further configured to analyse the plurality of collected images so as to identify a handling anomaly.</p><p id="p-0027" num="0051">The present invention also relates to a computer program product comprising instructions which, when performed by at least one processor, executes at least the steps of the method according to the present invention.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE FIGURES</heading><p id="p-0028" num="0052">The aims, objects, as well as the features and advantages of the invention will appear better from the detailed description of an embodiment of the latter which is illustrated by the following appended drawings wherein:</p><p id="p-0029" num="0053"><figref idref="DRAWINGS">FIG. <b>1</b></figref> represents a fraud detection system according to an embodiment of the present invention.</p><p id="p-0030" num="0054"><figref idref="DRAWINGS">FIG. <b>2</b></figref> represents a diagram of the positioning of the identification device, of the optical device and of their observation areas according to an embodiment of the present invention.</p><p id="p-0031" num="0055"><figref idref="DRAWINGS">FIG. <b>3</b></figref> represents a cart integrating at least one portion of the system according to an embodiment of the present invention.</p><p id="p-0032" num="0056"><figref idref="DRAWINGS">FIG. <b>4</b></figref> represents a graphical interface of a mobile analysis device according to an embodiment of the present invention.</p><p id="p-0033" num="0057"><figref idref="DRAWINGS">FIG. <b>5</b></figref> represents an algorithm for recording data and analysing said record according to an embodiment of the present invention.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0034" num="0058">The drawings are given as examples and do not limit the invention. They form schematic representations of principle intended to facilitate understanding of the invention and are not necessarily scaled to practical applications. In particular, the dimensions are not representative of reality.</p><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0035" num="0059">Before starting a detailed review of embodiments of the invention, optional features are set out hereinafter which could possibly be used in combination or alternatively:</p><p id="p-0036" num="0060">According to one example, the optical device is configured to allow depth to be taken into account in the capture of three-dimensional images.</p><p id="p-0037" num="0061">According to one example, the optical device is configured to allow considering a so-called depth spatial dimension extending along an axis orthogonal to the two axes forming the plane of a dioptre of the optical device.</p><p id="p-0038" num="0062">According to one example, the trajectory of the item in the three-dimensional space comprises at least one plurality of points, each point of said plurality of points comprising at least three spatial coordinates, preferably in an orthonormal three-dimensional space.</p><p id="p-0039" num="0063">According to one example, the optical device is configured to allow taking into account the depth in the determination of said trajectory of the item.</p><p id="p-0040" num="0064">According to one example, the optical device is configured to allow considering a so-called depth spatial dimension extending along an axis orthogonal to the two axes forming the plane of a dioptre of the optical device in the determination of said trajectory of the item.</p><p id="p-0041" num="0065">According to one example, the trajectory of the item in the three-dimensional space, as determined by the optical device, comprises at least one plurality of points, each point of said plurality of points comprising at least three spatial coordinates, possibly each evolving along the trajectory, preferably in an orthogonal three-dimensional space.</p><p id="p-0042" num="0066">According to one example, the optical device comprises a stereoscopic optical device, preferably is a stereoscopic optical device.</p><p id="p-0043" num="0067">According to one example, the probability is non-zero if a handling anomaly is identified.</p><p id="p-0044" num="0068">According to one example, the predetermined weight of the item contained in the database comprises a range of weights, preferably a minimum predetermined weight and a maximum predetermined weight.</p><p id="p-0045" num="0069">According to one example, the step of determining the trajectory of the item in the three-dimensional space comprises tracking of the item in at least one area selected from at least the identification area, the entrance area, at least one external area, at least one internal area corresponding at least to the entrance of at least one container, the entrance area separating the external area from the internal area.</p><p id="p-0046" num="0070">Dividing the space into several areas allows for better tracking of the item and for functionalisation of the space.</p><p id="p-0047" num="0071">According to one example, the determination of the trajectory of the item in the three-dimensional space comprises at least the passages, and preferably only the passages, of the item from one area of the three-dimensional space to another area of the three-dimensional space.</p><p id="p-0048" num="0072">According to one example, the step of determining the trajectory of the item comprises at least the determination of the trajectory of an object other than the item moving in the three-dimensional space, said object preferably being selected from: a hand, an arm, another item, a bag, an accessory worn by the user, a garment worn by the user.</p><p id="p-0049" num="0073">According to one example, the step of generating the behaviour of the item comprises the mention of any approach of said object to the item beyond a predetermined threshold.</p><p id="p-0050" num="0074">According to one example, the generated behaviour of said item comprises at least one sequence of events detected by the plurality of sensors, these events being selected from at least: the identification of the item, the passage from an area of the three-dimensional space to another area of the three-dimensional space, the measurement of the weight of the item, the approach of the item by another object.</p><p id="p-0051" num="0075">According to one example, the step of capturing the plurality of data comprises the collection by the optical device of a plurality of images at least of the item and at least of one hand of the user carrying the item.</p><p id="p-0052" num="0076">According to one example, the processing step comprises a step of analysing the plurality of collected images in order to record at least one two-dimensional representation of the item and to identify whether the user's hand is empty or full.</p><p id="p-0053" num="0077">According to one example, the processing step comprises at least one comparison of an image of the item present in the database and one or more images of the plurality of collected images so as to identify an anomaly between the image of the item from the database and the collected image(s) of the item.</p><p id="p-0054" num="0078">This allows comparing the scanned item not only according to its label, but also according to an optical comparison.</p><p id="p-0055" num="0079">According to one example, the step of comparing an image of the item comprises at least one step of optical recognition of the item by the computer processing unit, preferably by a trained neural network.</p><p id="p-0056" num="0080">According to one example, the step of collecting a plurality of images comprises at least one step of recording by the optical device a video, advantageously temporally compressed, preferably from the plurality of collected images.</p><p id="p-0057" num="0081">This allows an event to be visualised easily, intuitively and quickly.</p><p id="p-0058" num="0082">According to one example, the step of recording the video, preferably temporally compressed, comprises insetting data collected by at least one sensor at the time of collection of said data, said sensor being selected from at least: the identification device, the optical device, the measuring device, a spatial orientation sensor, a motion sensor.</p><p id="p-0059" num="0083">This allows an event to be visualised easily, intuitively and quickly.</p><p id="p-0060" num="0084">According to one example, the step of determining the trajectory of the item comprises at least:<ul id="ul0016" list-style="none">    <li id="ul0016-0001" num="0000">    <ul id="ul0017" list-style="none">        <li id="ul0017-0001" num="0085">a. The collection of a plurality of two-dimensional images, preferably in colour;</li>        <li id="ul0017-0002" num="0086">b. The collection of a plurality of three-dimensional images.</li>    </ul>    </li></ul></p><p id="p-0061" num="0087">This allows identifying the geometric shape of the item and following it spatially.</p><p id="p-0062" num="0088">According to one example, the collection of the plurality of two-dimensional images is carried out by at least one camera and by at least one additional camera, and the collection of the plurality of three-dimensional images is carried out by at least one stereoscopic camera.</p><p id="p-0063" num="0089">According to one example, the stereoscopic camera is configured to spatially track the item in the three-dimensional space, and the additional camera is configured to transmit a plurality of two-dimensional images to at least one neural network so as to train said neural network to recognise the geometric shape of the item, preferably the database could also provide corrective data to refine the model generated by the neural network, the spatial position of the item and its geometric shape are then used to track the item by the two-dimensional camera when the item leaves the field of view of the stereoscopic camera.</p><p id="p-0064" num="0090">The collaboration of the two cameras allows for better tracking of the item as well as better identification, thus reducing the number of possible frauds.</p><p id="p-0065" num="0091">According to one example, the two-dimensional camera comprises an objective lens having an angle larger than 100 degrees, preferably called &#x201c;wide-angle&#x201d;, and is configured to ensure tracking of the spatial position of the item outside the field of view of the stereoscopic camera and to collect images of the geometric shape of the item, the spatial position of the item and its geometric shape are then used to track the item by the stereoscopic camera and by the additional two-dimensional camera when the item falls within the field of view of the stereoscopic camera.</p><p id="p-0066" num="0092">The collaboration of the two cameras allows for better tracking of the item as well as for better identification, thus reducing the number of possible frauds.</p><p id="p-0067" num="0093">According to one example, the method comprises, before the step of identifying the item, a step of identifying the user followed by a step of reading a user profile specific to the user from a user profile database.</p><p id="p-0068" num="0094">This allows taking into account the user's history as a weighting parameter in the assessment of the probability of fraud.</p><p id="p-0069" num="0095">According to one example, the predetermined behaviour models comprise at least one standard behaviour model comprising at least the following sequence of events:<ul id="ul0018" list-style="none">    <li id="ul0018-0001" num="0000">    <ul id="ul0019" list-style="none">        <li id="ul0019-0001" num="0096">a. Identification of the item;</li>        <li id="ul0019-0002" num="0097">b. Tracking the item from the identification area to the entrance area;</li>        <li id="ul0019-0003" num="0098">c. Tracking the item from the entrance area to the internal area;</li>        <li id="ul0019-0004" num="0099">d. Preferably, tracking an empty hand of the user from the internal area to the external area before or after measuring the weight of the item.</li>    </ul>    </li></ul></p><p id="p-0070" num="0100">According to one example, a handling anomaly comprises at least one of the following situations: exchange of the item with another item, addition of another item in a container at the same time as said item, removal of another item from said container when depositing said item in said container, exchange of an identified item with another unidentified item, identification of an item with a fraudulent identifier.</p><p id="p-0071" num="0101">This allows identifying anomalies other than weight-related ones, and mainly those due to handling of the item.</p><p id="p-0072" num="0102">According to one example, the method comprises, if a weight anomaly is detected, the following steps:<ul id="ul0020" list-style="none">    <li id="ul0020-0001" num="0000">    <ul id="ul0021" list-style="none">        <li id="ul0021-0001" num="0103">a. Formulation by message, preferably visual and/or audio, to the user of a request to remove the item, this formulation being carried out by a user interface, the user interface being for example the computer processing unit;</li>        <li id="ul0021-0002" num="0104">b. Formulation by message, preferably visual and/or audio, to the user of a request to weigh the item again so as to obtain a new weight, this formulation being carried out by a user interface, the user interface being for example the computer processing unit;</li>        <li id="ul0021-0003" num="0105">c. Sending by the user terminal to the computer processing unit of the new weight of the item;</li>        <li id="ul0021-0004" num="0106">d. Processing, carried out by the computer processing unit, of the new identifier of the item, of the new weight of the item, and preferably of the collected images, comprising at least the comparison of the predetermined weight with the new measured weight so as to identify a weight anomaly.</li>    </ul>    </li></ul></p><p id="p-0073" num="0107">This enables a verification of the weight of the item and thus this could reduce the interventions of the supervisors.</p><p id="p-0074" num="0108">According to one example, the method comprises, if an anomaly is detected, the following steps:<ul id="ul0022" list-style="none">    <li id="ul0022-0001" num="0000">    <ul id="ul0023" list-style="none">        <li id="ul0023-0001" num="0109">a. Formulation by message, preferably visual and/or audio, to the user of a request to identify the item again, this formulation being carried out by a user interface, the user interface being for example the computer processing unit;</li>        <li id="ul0023-0002" num="0110">b. Sending by the user terminal to the computer processing unit of the new identifier of the item;</li>        <li id="ul0023-0003" num="0111">c. Formulation by message, preferably visual and/or audio, to the user of a request to weigh the item again so as to obtain a new weight, this formulation being carried out by a user interface, the user interface being for example the computer processing unit;</li>        <li id="ul0023-0004" num="0112">d. Sending by the user terminal to the computer processing unit of the new weight of the item;</li>        <li id="ul0023-0005" num="0113">e. Processing, by the computer processing unit, of the new identifier of the item, of the new weight of the item, and preferably of the collected images, comprising at least comparing the predetermined weight with the new measured weight so as to identify a weight anomaly.</li>    </ul>    </li></ul></p><p id="p-0075" num="0114">This enables a double verification and thus it could reduce the interventions of the supervisors.</p><p id="p-0076" num="0115">According to one example, the method comprises a continuous step of recording an initial video with a predetermined duration by the optical device, said initial video being erased at the end of said predetermined duration unless an event is detected by at least one sensor selected from at least: the identification device, the measuring device, the optical device, a motion sensor, a spatial orientation sensor.</p><p id="p-0077" num="0116">This allows having images prior to the event triggering the recording and therefore to the relevant event.</p><p id="p-0078" num="0117">According to one example, the processing step is carried out only when the step of capturing the at least one plurality of data is complete.</p><p id="p-0079" num="0118">This allows saving system resources and energy. Indeed, advantageously, the collection and analysis phases are distinct so that the system could operate efficiently with little system resource, little energy and therefore at low cost.</p><p id="p-0080" num="0119">According to one example, the method comprises, when the probability of fraud is greater than a predetermined threshold, sending from the computer processing unit of a plurality of secondary data based on said plurality of data to at least one management station so that a first supervisor analyses said plurality of secondary data.</p><p id="p-0081" num="0120">This allows having a first automated anti-fraud filter, and a second anti-fraud filter involving one or several human operator(s).</p><p id="p-0082" num="0121">According to one example, if a fraud situation is validated by the first supervisor, said plurality of secondary data is transmitted to at least one mobile analysis device, preferably located in the same building as the user terminal, so that a second supervisor analyses said plurality of secondary data and moves to the user.</p><p id="p-0083" num="0122">This allows having a mobile supervisor to go on site and visually check the presence of fraud or not.</p><p id="p-0084" num="0123">According to one example, said plurality of secondary data comprises at least one of the following data: the identifier of the item, the weight of the item, an original image of the item, one or more images of the plurality of collected images, a video, preferably temporally compressed.</p><p id="p-0085" num="0124">This allows for a simple and intuitive presentation of information.</p><p id="p-0086" num="0125">According to one example, the user terminal is a mobile cart.</p><p id="p-0087" num="0126">This allows having a smart cart enabling the user to pay for his purchases easily at the end of the session, scanning of the items being done during the purchase session.</p><p id="p-0088" num="0127">According to one example, at least one portion of the computer processing unit is embedded in the mobile cart.</p><p id="p-0089" num="0128">According to one example, the system comprises at least one management station, preferably remote, configured to receive at least a plurality of data from the computer processing unit so as to be analysed by at least one first supervisor.</p><p id="p-0090" num="0129">According to one example, the system comprises at least one mobile analysis device configured to receive a plurality of data from the management station so as to enable a second supervisor to analyse said plurality of data and to move to the user.</p><p id="p-0091" num="0130">According to one example, the computer processing unit is in communication with another database comprising at least the history of detected frauds of the user.</p><p id="p-0092" num="0131">According to one example, in which the user terminal is a fixed terminal, typically intended to be placed in a store, for example close to the exit of the store.</p><p id="p-0093" num="0132">According to one example, the computer processing unit is in communication with at least one classification module comprising at least one neural network trained to detect a situation of fraud from data transmitted to the computer processing unit.</p><p id="p-0094" num="0133">According to one example, the user terminal comprises at least one display device configured to display at least the identifier and/or the weight of the item.</p><p id="p-0095" num="0134">According to one example, the system comprises at least one electric battery.</p><p id="p-0096" num="0135">In the present description, the term &#x201c;a three-dimensional space&#x201d; means a space comprising at least three spatial dimensions, at least part of this space being captured by an optical device, preferably stereoscopic, configured to consider these three spatial dimensions, i.e. it is possible to determine the spatial position of one or several object(s) present in this three-dimensional space via this optical device. In particular, this optical device is configured to take into account, in addition, the depth with respect to said optical device, i.e. it is possible to assess the distance of one or several object(s) present in this three-dimensional space with respect to said optical device. Thus, in this three-dimensional space, an object could describe a trajectory and this object therefore comprises three spatial coordinates at each point of this trajectory, because the optical device is capable of assessing the evolution of said object in the three dimensions of space. This also allows for an advantageously much more flexible placement of the optical device while preserving understanding of the actions carried out in the three-dimensional space. Unlike the prior art which considers only two spatial dimensions and does not measure the depth, the optical device according to the present invention is not necessarily arranged vertically to the two-dimensional area to be assessed.</p><p id="p-0097" num="0136">The present invention relates to a system, as well as a method for detecting fraud during the purchase of an item by a user in a store, for example.</p><p id="p-0098" num="0137">The present invention cleverly allows the detection of fraud during the purchase of an item. Indeed, via a clever method based on an advantageous system, the present invention allows detecting fraud in the case of automatic collection, and possibly automatic payment, systems also called automatic checkouts or else automatic payment carts, for example without limitation.</p><p id="p-0099" num="0138">We will first present the fraud detection system according to an embodiment of the present invention. Then, we will present the fraud detection method according to an embodiment of the present invention.</p><p id="p-0100" num="0139"><figref idref="DRAWINGS">FIGS. <b>1</b> to <b>3</b></figref> illustrate a fraud detection system according to an embodiment of the present invention.</p><p id="p-0101" num="0140"><figref idref="DRAWINGS">FIG. <b>1</b></figref> schematically illustrates such a system <b>1000</b>.</p><p id="p-0102" num="0141">Advantageously, the fraud detection system <b>1000</b> comprises at least:<ul id="ul0024" list-style="none">    <li id="ul0024-0001" num="0000">    <ul id="ul0025" list-style="none">        <li id="ul0025-0001" num="0142">a. A user terminal <b>10</b> comprising at least:        <ul id="ul0026" list-style="none">            <li id="ul0026-0001" num="0143">i. An identification device <b>1100</b> configured to obtain the identifier of an item <b>20</b>;</li>            <li id="ul0026-0002" num="0144">ii. A measuring device <b>1200</b> configured to measure the weight of said item <b>20</b>;</li>            <li id="ul0026-0003" num="0145">iii. An optical device <b>1300</b> configured at least to detect and follow said item <b>20</b> in space;</li>            <li id="ul0026-0004" num="0146">iv. Preferably, a motion sensor and/or a spatial displacement sensor, such as a gyroscope for example.</li>        </ul>        </li>        <li id="ul0025-0002" num="0147">b. A computer processing unit <b>1400</b> configured to process a plurality of data and determine a probability of fraud, preferably to determine whether there is fraud or not.</li>    </ul>    </li></ul></p><p id="p-0103" num="0148">According to one embodiment, the user terminal <b>10</b> comprises part or all of the computer processing unit <b>1400</b>.</p><p id="p-0104" num="0149">According to a preferred embodiment, the user terminal <b>10</b> is a mobile cart <b>10</b>, as illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref> for example.</p><p id="p-0105" num="0150">According to another embodiment, the user terminal is a terminal, for example a payment terminal or an automatic pay machine.</p><p id="p-0106" num="0151">According to one embodiment, the user terminal <b>10</b> may comprise a container <b>11</b> intended to receive the item <b>20</b> after the user has identified said item <b>20</b>. According to a preferred embodiment, at least the identification device <b>1100</b>, the measuring device <b>1200</b> and the optical device <b>1300</b> are mounted on the same device, preferably mobile, such as for example a cart <b>10</b> as described later on in <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0107" num="0152">According to one embodiment, the identification device <b>1100</b> is configured to determine the identifier of the item <b>20</b>. This determination may be in any form. For example, it may comprise the fact of having the identification device <b>1100</b> read the barcode of the item <b>20</b>. It may be a radiofrequency technology of the RFID type or else a visual recognition of the item <b>20</b>, or even a touch interface enabling the user to indicate to the system <b>1000</b> the considered item so that the identifier of the item <b>20</b> is determined. In the case of visual recognition of the item <b>20</b>, the identification device <b>1100</b> may comprise the optical device <b>1300</b> and/or vice versa.</p><p id="p-0108" num="0153">According to one embodiment, the identification device <b>1100</b> may comprise a mobile device, for example belonging to the user. In this case the identification device <b>1100</b> could use at least one camera of this mobile device to identify the item <b>20</b>. For example, this mobile device may be a digital tablet or a smartphone.</p><p id="p-0109" num="0154">Preferably, the user presents the item <b>20</b> to the identification device <b>1100</b> of the barcode reader type, for example, the identifier is obtained by the identification device <b>1100</b> then transmitted to the computer processing unit <b>1400</b>. Afterwards, the user moves the item <b>20</b> into the container <b>11</b>. The container <b>11</b> advantageously comprises the measuring device <b>1200</b>.</p><p id="p-0110" num="0155">According to one embodiment, the measuring device <b>1200</b> is configured to measure the weight of the item <b>20</b>. Advantageously, the measuring device <b>1200</b> comprises a force sensor from which hangs the container <b>11</b> configured to receive said item <b>20</b> once it has been identified. According to one embodiment, the container <b>11</b> may be placed on the force sensor. According to another embodiment, the measuring device <b>1200</b> comprises a scale on which the item <b>20</b> is placed to measure its weight. Once the weight has been measured, this data is transmitted from the measuring device <b>1200</b> to the computer processing unit <b>1400</b>.</p><p id="p-0111" num="0156">According to one embodiment, the optical device <b>1300</b> comprises a so-called two-dimensional camera <b>1310</b> configured to collect two-dimensional images of a predetermined two-dimensional scene, and preferably a stereoscopic camera also called a three-dimensional camera <b>1320</b>. This stereoscopic camera, or more generally this three-dimensional sensor <b>1320</b>, is configured to collect three-dimensional images of a predetermined three-dimensional scene. We will further describe the optical device <b>1300</b> later on as well as the different areas that form this predetermined three-dimensional scene, through <figref idref="DRAWINGS">FIG. <b>2</b></figref>. Preferably, the optical device <b>1300</b> is configured to transmit said collected images to the computer processing unit <b>1400</b>.</p><p id="p-0112" num="0157">According to one embodiment, the optical device <b>1300</b> comprises a camera.</p><p id="p-0113" num="0158">According to one embodiment, the system <b>1000</b> may comprise a plurality of sensors, including the identification device <b>1100</b>, the measuring device <b>1200</b> and the optical device <b>1300</b>, but also a motion sensor for example, or else an accelerometer, or a gyroscope, or any other sensor that could be used to collect one or several data useful for identifying a potential fraud situation. As presented later on, the present invention advantageously takes advantage of the cross-checking of data collected by a plurality of sensors. This cross-checking of data is advantageously carried out by an artificial intelligence module <b>1420</b>, preferably comprising at least one trained neural network, advantageously automatically.</p><p id="p-0114" num="0159">According to one embodiment, the computer processing unit <b>1400</b> is configured to process the obtained data, collected by the identification device <b>1100</b>, the measuring device <b>1200</b>, the optical device <b>1300</b>, and preferably by any other sensor. Indeed, preferably, the computer processing unit <b>1400</b> is configured to receive:<ul id="ul0027" list-style="none">    <li id="ul0027-0001" num="0000">    <ul id="ul0028" list-style="none">        <li id="ul0028-0001" num="0160">a. At least one identifier of said item <b>20</b> from the identification device <b>1100</b>;</li>        <li id="ul0028-0002" num="0161">b. At least one measurement of the weight of said item <b>20</b> from the measuring device <b>1200</b>;</li>        <li id="ul0028-0003" num="0162">c. At least a plurality of images collected by the optical device <b>1300</b>.</li>    </ul>    </li></ul></p><p id="p-0115" num="0163">Advantageously, the computer processing unit <b>1400</b> is in communication with at least one database <b>1410</b> comprising for each identifier at least one series of data comprising the predetermined weight of said item <b>20</b>, and preferably an image or a graphical representation of said item <b>20</b>. To carry out this comparison, the computer processing unit <b>1400</b> may comprise a weight comparison module for example.</p><p id="p-0116" num="0164">According to one embodiment, the predetermined weight of the item <b>20</b> corresponds to a weight interval. Indeed, the database may comprise a weight interval and not a specific value. In particular, this avoids many situations where the weight does not accurately correspond. Indeed, it is hard that all items <b>20</b> have the same weight. On the other hand, it is perfectly possible to define a weight range in which the item <b>20</b> must be. For example, this weight range may correspond to the weight of the item <b>20</b> more or less 2%, preferably 5% and advantageously 10%. According to a preferred example, this range has a minimum value and a maximum value, preferably pre-recorded or acquired by learning during the operating time of the invention.</p><p id="p-0117" num="0165">According to a preferred embodiment, the predetermined weight recorded in the database <b>1410</b>, at least before scanning of the item <b>20</b>, is zero, i.e. it is equal to zero or is not input. According to this embodiment, the system <b>1000</b> is self-learning, i.e. it will feed its database <b>1410</b> from the measured weight. For example, the user scans an item <b>20</b>, the system <b>1000</b> identifies the item <b>20</b> and accesses the database <b>1410</b> of items <b>20</b> to compare the weight of said scanned item <b>20</b> with that of the database <b>1410</b>. If the database returns a zero weight value or if the weight value is not input in the database <b>1410</b>, then the system <b>1000</b> switches into self-learning mode and replaces this zero weight or not input value with the value of the measured weight. In this self-learning phase, the system <b>1000</b> captures images of the item <b>20</b> so that it could subsequently associate a two-dimensional image of the item <b>20</b> with the identifier of the item <b>20</b> and the weight of the item <b>20</b>. If during the purchase session, the user handles said item <b>20</b>, its weight, its identifier and its visual recognition will be used to prevent a situation of fraud. It should also be noted that during the first scan, the system <b>1000</b> is designed to reason logically, i.e. if the user tries to place a fruit and vegetable label on an item <b>20</b> other than fruit and vegetables, the visual analysis, described later on, allows triggering a notification of a potential situation of fraud even though the weight is not listed in the database <b>1410</b>.</p><p id="p-0118" num="0166">Preferably, as long as the weight of the item <b>20</b> is greater than a predetermined threshold, this weight may be used as a predetermined weight if, before weighing, the predetermined weight of said item in the database was zero. Advantageously, this predetermined threshold is less than 100 gr, preferably 50 gr and advantageously 25 gr.</p><p id="p-0119" num="0167">Preferably, the computer processing unit <b>1400</b> is configured to obtain from said database <b>1410</b> at least the predetermined weight of said item <b>20</b> and to compare this predetermined weight with the measured weight transmitted by the measuring device <b>1200</b>.</p><p id="p-0120" num="0168">Advantageously, the computer processing unit <b>1400</b> is configured to process the plurality of collected images. This processing may comprise the identification and/or spatial location of the item <b>20</b>. In the case of an identification, this may be used to compare the identifier of the item <b>20</b> with the optical identification carried out by the computer processing unit <b>1400</b> from the plurality of collected images.</p><p id="p-0121" num="0169">According to one embodiment, the spatial location of the item <b>20</b> is used in order to verify that the identified item <b>20</b> is actually the weighed item <b>20</b> and that the user has not exchanged the identified item <b>20</b> with another item <b>20</b> of the same weight.</p><p id="p-0122" num="0170">According to one embodiment, the optical device <b>1300</b> only comprises a single camera capable of capturing two-dimensional images and three-dimensional images.</p><p id="p-0123" num="0171">Preferably, the optical device <b>1300</b> is configured to capture points in a three-dimensional space, thus allowing depth to be taken into account in the capture of three-dimensional images.</p><p id="p-0124" num="0172">Preferably, the optical device <b>1300</b> is configured to capture two-dimensional colour data.</p><p id="p-0125" num="0173">Advantageously, the optical device <b>1300</b> is configured to follow an object, preferably the item <b>20</b> or one or more hands of a user for example, in a space. This space is compartmentalised into various virtual areas. These virtual areas are defined by the computer processing unit <b>1400</b> and are used for the analysis of the collected images, or for triggering actions.</p><p id="p-0126" num="0174">Thus, according to an embodiment illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the considered analysed three-dimensional space comprises at least four areas:<ul id="ul0029" list-style="none">    <li id="ul0029-0001" num="0000">    <ul id="ul0030" list-style="none">        <li id="ul0030-0001" num="0175">a. A scan area <b>1321</b>, located at the level of the identification device, for example in front of a barcode scanner;</li>        <li id="ul0030-0002" num="0176">b. An external area <b>1322</b>, located above the container <b>11</b>, preferably above the cart, or outside a deposit area for an automatic scan, for example;</li>        <li id="ul0030-0003" num="0177">c. An internal area <b>1323</b>, located inside the container, preferably in a so-called deposit area, advantageously in the cart;</li>        <li id="ul0030-0004" num="0178">d. An entrance area <b>1234</b>, located between the external <b>1322</b> and internal <b>1323</b> areas.</li>    </ul>    </li></ul></p><p id="p-0127" num="0179">The use of these areas will be described more specifically later on, as well as the clever process of processing the collected images.</p><p id="p-0128" num="0180">According to one embodiment, the system <b>1000</b> also comprises at least one mobile fraud analysis device <b>1700</b>. This device <b>1700</b> is configured to be used by a user called a supervisor, his role being to supervise some situations of possible fraud. Indeed, in a clever way, and as described later on, in case of doubt concerning a situation of fraud, a supervisor having a fraud analysis device <b>1700</b> receives thereon a plurality of information enabling him to assess whether or not there is fraud. This analysis step will be described later on, in particular its advantageous presentation allowing for a very high and reliable responsiveness from the supervisor.</p><p id="p-0129" num="0181">According to one embodiment, the processing unit <b>1400</b> may be in communication with a management station <b>1600</b>. This management station <b>1600</b> allows supervising a plurality of fraud detection systems <b>1000</b>. This management station <b>1600</b> will also be described more specifically later on.</p><p id="p-0130" num="0182"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates a fraud detection system <b>1000</b> according to a preferred embodiment. In this figure, a cart <b>10</b> comprises a gripping device <b>13</b> and a frame <b>15</b> supported by wheels <b>14</b> thus making the cart <b>10</b> mobile.</p><p id="p-0131" num="0183">Advantageously, the cart <b>10</b> further comprises the identification device <b>1100</b>, the optical device <b>1300</b>, the measuring device <b>1200</b> and at least one container <b>11</b>.</p><p id="p-0132" num="0184">Advantageously, the cart <b>10</b> may comprise at least one display device <b>12</b> enabling the user to be informed where necessary, and possibly a touch interface service for managing the user's virtual basket, for example.</p><p id="p-0133" num="0185">According to one embodiment, the computer processing unit <b>1400</b> may be embedded in the cart <b>10</b> and/or be partially or totally shifted and be in communication with the elements embedded in the cart <b>10</b>.</p><p id="p-0134" num="0186">In this figure, the cart <b>10</b> comprises a container <b>11</b>, preferably hanging from at least one force sensor thus serving as a device <b>1200</b> for measuring the weight of the item <b>20</b>. Advantageously, the identification device <b>1100</b> is a barcode scanner. Preferably, the cart <b>10</b> comprises the optical device <b>1300</b> adapted to collect two-dimensional images, preferably in colour, and three-dimensional images.</p><p id="p-0135" num="0187">According to one embodiment, the cart <b>10</b> may comprise a plurality of sensors such as, for example, a sensor of spatial position, movement, direction of movement, presence, NFC (Near Field Communication) sensor, RFID sensor (standing for radio frequency identification), LI-FI sensor (standing for Light Fidelity), Bluetooth sensor, or else WI-FI&#x2122; type radio communication sensor, etc. . . . .</p><p id="p-0136" num="0188">According to one embodiment, the cart <b>10</b> comprises one or several Bluetooth, WI-FI&#x2122; or Lora (Long Range) type communication modules.</p><p id="p-0137" num="0189">According to a preferred embodiment, the cart <b>10</b> comprises different sensors linked to an artificial intelligence whose purpose is to understand each action performed on the cart <b>10</b> by the user and to detect fraudulent actions. For example, this intelligence may be in the form of a data processing module comprising at least one neural network, preferably trained. This neural network may be embedded in the cart <b>10</b>. Preferably, the cart <b>10</b> comprises an electric power source <b>16</b> for example to power the different elements indicated before.</p><p id="p-0138" num="0190">We will now simply illustrate the clever operation of the present invention, for example when a user is about to add an item <b>20</b> to his virtual basket, i.e. when the user adds in the container <b>11</b> an item <b>20</b> for its subsequent purchase in a store equipped with the present invention.</p><p id="p-0139" num="0191">In the following example and for clarity, the fraud detection system <b>1000</b> is partly at least mobile and partly at least on board a cart <b>10</b> as described before.</p><p id="p-0140" num="0192">According to one embodiment, the system <b>1000</b> comprises an interface <b>12</b> that could either be placed on the cart <b>10</b> itself in the form of a touch interface <b>12</b>, or be virtualised in the form of a mobile application that the user will have downloaded beforehand, for example, on his smartphone.</p><p id="p-0141" num="0193">The user, after having selected the item <b>20</b> to be purchased, scans it with the identification device <b>1100</b>. Preferably, the barcode of the item <b>20</b> is scanned by the identifier of the device <b>1100</b>. Once the item <b>20</b> has been scanned, the user has a predetermined time, for example 10 seconds, to deposit the scanned item <b>20</b>, i.e. identified, on or in the container <b>11</b>. Advantageously, the container <b>11</b> is configured to cooperate with the measuring device <b>1200</b> so that the weight of the item <b>20</b> is measured by the measuring device <b>1200</b>.</p><p id="p-0142" num="0194">According to a preferred embodiment, the measuring device <b>1200</b> is embedded in the cart. Thus, the user must have the scanned item <b>20</b> in the cart <b>10</b> in less than 10 seconds, for example without limitation.</p><p id="p-0143" num="0195">According to another embodiment, the measuring device <b>1200</b> may be externalised relative to the cart <b>10</b> so that the user, after having scanned the item <b>20</b>, places the latter on or in the measuring device <b>1200</b> so that its weight is measured there, before placing the item <b>20</b> in the container <b>11</b>.</p><p id="p-0144" num="0196">Once the item <b>20</b> is placed, the measuring device <b>1200</b> determines the weight of the item <b>20</b>.</p><p id="p-0145" num="0197">According to one embodiment, before weighing, the identifier is transmitted to the computer processing unit <b>1400</b>. According to another embodiment, the identifier is transmitted to the computer processing unit <b>1400</b> after weighing, and preferably at the same time as the weight is measured.</p><p id="p-0146" num="0198">After weighing, the item <b>20</b> is added to a virtual basket allowing the system <b>1000</b> and the user to have a follow-up of the purchases of the user.</p><p id="p-0147" num="0199">According to one embodiment, only one action is possible at a time, i.e. it is not possible to scan, or to identify, another item <b>20</b> as long as the previously scanned item <b>20</b> is not deposited and its weight has not been assessed.</p><p id="p-0148" num="0200">Advantageously, the present invention enables the user to cancel his scan to potentially scan another item <b>20</b>. In this case, either the user cancels the previous scan via the control interface <b>12</b>, or he waits for the predetermined time indicated previously, for example 10 seconds.</p><p id="p-0149" num="0201">The present invention also takes into account the situation where the user would like to remove an item <b>20</b> from the cart <b>10</b>. In this case, the user uses the control interface <b>12</b> to indicate to it that he wishes to remove an item <b>20</b> from the cart <b>10</b>. Afterwards, the user can remove as many items <b>20</b> as he wishes, but must preferably scan them one by one, advantageously waiting each time between each scan for the system <b>1000</b> to detect that the weight of the container <b>11</b> has varied.</p><p id="p-0150" num="0202">In the case where an item <b>20</b> is placed or removed without a scan step, the weight variation would be detected by the system <b>1000</b>, preferably by the measuring device <b>1200</b>, and would be mentioned to the user, preferably via the control interface <b>12</b>, also called display device <b>12</b>. The same applies if the assessed weight is inconsistent with the identifier of the item <b>20</b> obtained after scanning it. The same is also true for the removal of an item <b>20</b> whose weight does not correspond to the identifier of the scanned item <b>20</b> supposed to have been removed.</p><p id="p-0151" num="0203">Thus, the present invention is specially designed to secure the purchase of an item <b>20</b> and thus significantly reduce fraud while allowing for a better fluidity at checkout since payment is ensured directly by means of the present invention, directly via the cart <b>10</b> for example, preferably through the display device <b>12</b> which could be used as a control, and preference payment, interface <b>12</b>.</p><p id="p-0152" num="0204">We will now describe the fraud detection method according to the present invention.</p><p id="p-0153" num="0205">According to one embodiment, the fraud detection method comprises at least:<ul id="ul0031" list-style="none">    <li id="ul0031-0001" num="0000">    <ul id="ul0032" list-style="none">        <li id="ul0032-0001" num="0206">a. A step of capturing a plurality of data. These data are at least those previously indicated. This capture step is advantageously carried out by the user terminal <b>10</b>. This capture step comprises at least the following steps:        <ul id="ul0033" list-style="none">            <li id="ul0033-0001" num="0207">i. Obtainment of the identifier of the item <b>20</b> by the identification device <b>1100</b>; this step is for example carried out by scanning the item <b>20</b> by means of the identification device <b>1100</b>; The user is invited to scan any item <b>20</b> that he wishes to place in the cart <b>10</b> for example.</li>            <li id="ul0033-0002" num="0208">ii. Determination by the optical device <b>1300</b> of at least one trajectory of the item <b>20</b> in a three-dimensional space, the item <b>20</b> being manually moved in said three-dimensional space by a user, this trajectory is preferably manually imposed on the item <b>20</b> by a user, said three-dimensional space comprising at least:            <ul id="ul0034" list-style="none">                <li id="ul0034-0001" num="0209">1. An identification area <b>1321</b> corresponding to a volume of the three-dimensional space in which at least one portion of the item <b>20</b> is placed by the user to obtain of the identifier of the item <b>20</b>;</li>                <li id="ul0034-0002" num="0210">2. An entrance area <b>1324</b> corresponding to a volume of the three-dimensional space through which the item <b>20</b> passes when the user places the item <b>20</b> in at least one container <b>11</b>, preferably associated with the user terminal <b>10</b>;</li>                <li id="ul0034-0003" num="0211">3. Preferably, an internal area <b>1323</b> corresponding to the entrance of a container <b>11</b>;</li>                <li id="ul0034-0004" num="0212">4. Preferably, an external area <b>1322</b>, the entrance area <b>1324</b> separating the external area <b>1322</b> from the internal area <b>1323</b>. The external area <b>1322</b> advantageously corresponds to the three-dimensional space surrounding the entrance area <b>1324</b>, itself surrounding the internal area <b>1323</b>.                <ul id="ul0035" list-style="none">                    <li id="ul0035-0001" num="0213">The determination of the trajectory consists in tracking the item <b>20</b> from one area to another area and recording either the entire trajectory, or only the sequence of passage from one area to another.</li>                    <li id="ul0035-0002" num="0214">Preferably, any object within the field of view of the optical device <b>1300</b> is tracked in the three-dimensional space.</li>                </ul>                </li>                <li id="ul0034-0005" num="0215">As discussed later on, if the trajectory of an object approaches beyond a predetermined threshold the trajectory of the item <b>20</b>, in other words if an object approaches the item <b>20</b> beyond a predetermined threshold, this may correspond to a situation of fraud, so the system <b>1000</b> is designed to mention during the analysis of the data later on.</li>            </ul>            </li>            <li id="ul0033-0003" num="0216">iii. Preferably, the optical device <b>1300</b> collects a plurality of images of said item <b>20</b> and/or of at least one hand of the user carrying said item <b>20</b>. This collection of images lasts until the item <b>20</b> is set so that the measuring device <b>1200</b> could measure its weight; once the item <b>20</b> has been scanned, the user has a predetermined time to place the item <b>20</b> in the container <b>11</b> and thus weigh it; moreover, scanning the item <b>20</b> triggers the capture of the plurality of two-dimensional and preferably three-dimensional images; this step of collecting the plurality of images is intended to track the item <b>20</b> visually from the scan area <b>1321</b> to its deposit place in the internal area <b>1323</b>; this allows, inter alia, verifying that the scanned item <b>20</b> is not exchanged with another item before being deposited in the container <b>11</b> for example.</li>            <li id="ul0033-0004" num="0217">iv. Sending from the identification device <b>1100</b> to the computer processing unit <b>1400</b> of the identifier of the item <b>20</b>;</li>            <li id="ul0033-0005" num="0218">v. Sending from the optical device <b>1300</b> to the computer processing unit <b>1400</b> of the plurality of collected images;</li>            <li id="ul0033-0006" num="0219">vi. Preferably, measurement, by the measuring device <b>1200</b>, of the weight of the item <b>20</b>, advantageously once the latter has been placed in the container <b>11</b> by the user;</li>            <li id="ul0033-0007" num="0220">vii. Preferably, sending from the measuring device <b>1200</b> to the computer processing unit <b>1400</b> the measured weight of said item <b>20</b> from said measuring device <b>1200</b>;</li>        </ul>        </li>        <li id="ul0032-0002" num="0221">b. A processing step, carried out by the computer processing unit <b>1400</b>, of the plurality of data, preferably of the identifier of the item <b>20</b>, of the measured weight of the item <b>20</b> and of the collected images, comprising at least the following steps:        <ul id="ul0036" list-style="none">            <li id="ul0036-0001" num="0222">i. Preferably, identification in the database <b>1410</b> of the item <b>20</b> from said identifier;</li>            <li id="ul0036-0002" num="0223">ii. Preferably, obtainment of the predetermined weight of the item <b>20</b> from the database <b>1410</b>; according to one embodiment, the predetermined weight of the item <b>20</b> contained in the database during the first scan of the item <b>20</b> could be equal to zero or not be input;</li>            <li id="ul0036-0003" num="0224">iii. Preferably, comparison of the predetermined weight with the measured weight so as to identify a weight anomaly; preferably, a weight anomaly corresponds to a measured weight different from the predetermined weight found in the database <b>1410</b> with the exception of the situation where the predetermined weight is equal to zero or is not input; otherwise, beyond a weight difference greater than a predetermined threshold, a weight anomaly is considered; this weight anomaly may occur when the user exchanges the scanned item <b>20</b> with another item whose weight is different, or when he modifies the barcode for example in order to scan an item with a weight different from the deposited actual item. In the case of a predetermined weight equal to zero or not input, the present invention is configured to replace this value with the value of the measured weight, this measured weight value then becoming the value of the predetermined weight during, at least, the remainder of the user's purchase session.</li>            <li id="ul0036-0004" num="0225">iv. Generation of at least one behaviour of said item <b>20</b> from at least the trajectory of the item <b>20</b> in the three-dimensional space; This step consists in aggregating various measurements from various sensors so as to recreate a behaviour of the item <b>20</b> evolving in a three-dimensional space but also in a sensor space. If the sequence of measurements is not consistent with at least one model among a plurality of standard behaviour models, then there is a suspicion of fraud, and a handling anomaly is detected; Preferably, a behaviour is not consistent with a standard behaviour model from the time point it presents a deviation from this model by more than 2%, preferably 5% and advantageously 10%; Advantageously, a behaviour is not consistent with a standard behaviour model from the time point some key events of the model are not present in the generated behaviour, such key events may for example be the fact that the item <b>20</b> has not been identified, that the item <b>20</b> has not been deposited, that the item <b>20</b> has not crossed the entrance area <b>1324</b>, etc . . . ; Preferably, a behaviour is not consistent with a standard behaviour model from the time point that some suspicious events are present in the generated behaviour, such suspicious events may for example be the fact that the optical device is temporarily obstructed, or else an object has approached the item <b>20</b>, etc. . . .</li>            <li id="ul0036-0005" num="0226">v. Comparison of the behaviour of said item with a plurality of predetermined behaviour models, said generated behaviour comprising at least the trajectory of said item in the three-dimensional space, if the behaviour is different from each model of predetermined behaviours, then a handling anomaly is identified; It should be noted that advantageously, the system is configured to learn from each situation and thus add and/or modify its standard behaviour models;</li>            <li id="ul0036-0006" num="0227">vi. Preferably, analysis of the plurality of collected images so as to identify a handling anomaly; a handling anomaly consists, for example, in scanning an item and depositing another of the same weight, or else in scanning an item with a label that does not correspond to said item even though the weight is correct; visual and preferably automated analysis is necessary for this type of situation, this analysis is provided by the present invention; advantageously, the computer processing unit <b>1400</b> comprises an artificial intelligence module <b>1420</b> comprising at least one neural network, advantageously trained to determine handling anomalies; In a particularly advantageous manner, the analysis of the plurality of collected images consists of an analysis of a three-dimensional scene and in particular of the displacement of a plurality of points associated with the item <b>20</b> in a three-dimensional space split into different areas; these areas will be described later on. The principle of this analysis of the plurality of images is to determine whether the movement of the item <b>20</b> in space corresponds to a predetermined model selected from among a plurality of models deemed to be non-fraudulent which will be described subsequently; In the case where the movement of the item <b>20</b> through these different areas does not correspond to a non-fraudulent model, then there is potentially a situation of fraud. Preferably, in addition to considering the movement of the item <b>20</b> in this compartmentalised virtual space, the present invention also considers the interactions between the item <b>20</b> and any other foreign element; advantageously, if a cloud of points, i.e. a hand or another object approaches and interacts with the cloud of points corresponding to the item <b>20</b>, the suspicion of fraud increases; Preferably, if the foreign element is a hand identified as empty, then the suspicion of fraud could be reduced.</li>        </ul>        </li>        <li id="ul0032-0003" num="0228">c. A step for assessing a probability of fraud, this probability being non-zero if:        <ul id="ul0037" list-style="none">            <li id="ul0037-0001" num="0229">i. A handling anomaly is identified; and/or</li>            <li id="ul0037-0002" num="0230">ii. Preferably, a weight anomaly is identified.</li>        </ul>        </li>    </ul>    </li></ul></p><p id="p-0154" num="0231">It should be noted that a probability of fraud could correspond to a binary piece of data such as for example 1 or 0, 1 corresponding to the fact that the fraud is certain and 0 corresponding to the fact that there is no fraud. According to another embodiment, a probability of fraud could correspond to a percentage of fraud, for example an absence of fraud is equivalent to 0% and a certainty of fraud to 100%.</p><p id="p-0155" num="0232">Thus, a fraud probability could be a numerical value between 0 and 100 and/or be a binary value equal to 0 or 1.</p><p id="p-0156" num="0233">This fraud assessment step consists in cross-checking a plurality of data so as to assess a probability of fraud, in particular if a weight and/or handling anomaly is detected. Advantageously, this cross-checking of data is carried out by an artificial intelligence module <b>1420</b> preferably comprising a trained neural network, preferably automatically.</p><p id="p-0157" num="0234">Some situations could be easily identified as fraud, nevertheless, other situations could sometimes be too complex for fully automated processing at low cost. Also, in order to reduce the costs of a highly automated analysis system, but the cost of which would be very high, the present invention proposes a hybrid solution in which a portion of the analysis is carried out automatically and another portion is carried out via the intervention of supervisors where necessary.</p><p id="p-0158" num="0235">Thus, cleverly, and as indicated before, the present invention may comprise at least one mobile analysis device <b>1700</b> intended to be used by at least one supervisor.</p><p id="p-0159" num="0236">According to one embodiment, the mobile analysis device <b>1700</b> is configured to receive a plurality of data from the computer processing unit <b>1400</b> and/or from a management station <b>1600</b> which will be described later on.</p><p id="p-0160" num="0237">Cleverly, the mobile analysis device <b>1700</b> is configured to display at least part of these data in a form enabling quick decision-making, for example in less than 10 seconds, preferably in less than 5 seconds and advantageously in less than 2 seconds, from the supervisor.</p><p id="p-0161" num="0238">Thus, the objective is to send the most qualitative information to the supervisors, preferably for remote control.</p><p id="p-0162" num="0239">For this purpose, the computer processing unit <b>1400</b> selects a selection of images from the plurality of collected images and transmits this selection to the mobile analysis device <b>1700</b>. This selection is advantageously carried out by considering particular time points, for example the time point of the scan, of the weighing, of the movement of the item <b>20</b>, of the entry or exit of an area, etc. . . . .</p><p id="p-0163" num="0240">According to one embodiment, the computer processing unit <b>1400</b> makes a video, preferably temporally compressed, which it also transmits to the mobile analysis device <b>1700</b>. A temporally compressed video should be understood as a video whose number of images per second is greater than 24 for example, and possibly a video whose playback time from start to end is less than the duration of the illustrated action, we also speak of time lapse video and possibly accelerated video. Advantageously, this video also comprises, preferably over its timeframe, the notification of the particular time points mentioned before, for example, in the form of markers. This enables the supervisor to select, if he wishes, a specific passage of the video relating to a particular event which is located there. This makes it easy, intuitive and quick to select an event and access the passage of the video and preferably other data related to this event.</p><p id="p-0164" num="0241">Finally, the computer processing unit <b>1400</b> transmits to the mobile analysis device <b>1700</b> the information related to the scanned item <b>20</b> and/or a text explaining the detected anomaly or anomalies, and possibly the type of fraud that is suspected and/or detected.</p><p id="p-0165" num="0242">Preferably, the computer processing unit <b>1400</b> transmits this data either directly to the mobile analysis device <b>1700</b>, or via a computer server <b>1600</b>. This computer server <b>1600</b> is advantageously configured to conform the data to be transmitted so as, for example, to prioritise them according to various prioritisation parameters and/or to sort them, for example.</p><p id="p-0166" num="0243">According to one embodiment, this computer server is an integral part of a management station <b>1600</b>.</p><p id="p-0167" num="0244">According to one embodiment, when fraud is suspected, the computer processing unit <b>1400</b> transmits said data to at least one management station <b>1600</b> via a computer server for example, then an employee, called super-supervisor for example, is then in charge of analysing whether there is fraud or not.</p><p id="p-0168" num="0245">In the case where there is no fraud, a validation command is transmitted to the computer processing unit <b>1400</b> validating the action of the user. In the case where a certainty of fraud or a doubt remains, the super-supervisor transmits the considered data to the analysis device <b>1700</b> of the supervisor. This supervisor is advantageously mobile and could thus approach the user whose action seems to be fraudulent. Thus, the supervisor is intended to take charge of the situation, on the one hand by analysing said data and on the other hand by moving to the place of the possible fraud.</p><p id="p-0169" num="0246">According to one embodiment, the mobile analysis device <b>1700</b> may for example comprise a tablet, a computer, a smartphone and possibly any medium allowing the display of data and preferably comprising an advantageously tactile interface.</p><p id="p-0170" num="0247">According to an advantageous embodiment, the data presented on the mobile analysis device <b>1700</b> is formatted to be easily understood and analysed. In a particularly advantageous manner, the present invention proposes a clear, simple and intuitive presentation of the data enabling the supervisor to decide very quickly, preferably in less than 10 seconds, whether the situation is a situation of fraud or not.</p><p id="p-0171" num="0248">Thus, for example, when the probability of fraud exceeds a predetermined threshold, the computer processing unit <b>1400</b> transmits the data necessary for the super-supervisor located at the management station <b>1600</b> to be able to filter out potential situations of fraud. If according to his analysis, there is no fraud, he sends a validation command to the user so that he could continue his purchases or his payment.</p><p id="p-0172" num="0249">If according to his analysis, there is a possibility of fraud, he transfers the data to the mobile analysis device <b>1700</b> of a supervisor, preferably the one closest to the user for example.</p><p id="p-0173" num="0250">Thus, for example, a summary of all &#x201c;suspicious&#x201d; actions, i.e. potentially fraudulent actions, is presented on the management station <b>1600</b> of a super-supervisor and/or on the mobile analysis device <b>1700</b> of the supervisor, for example the supervisor located at the exit of the store, so that he could interact with the user during the payment phase, for example.</p><p id="p-0174" num="0251">Advantageously, when an action is interpreted as potentially fraudulent by the computer processing unit <b>1400</b>, all the data necessary for the remote control of this situation are sent to the management station <b>1600</b>, i.e. to a supervisor. This person could be a security guard, a cashier or be totally decentralised in another country where labour is less expensive, for example.</p><p id="p-0175" num="0252">As indicated before, the super-supervisor has all the information necessary to control the action on a graphical interface. This graphical interface is advantageously configured to display the image and the title of the concerned item <b>20</b>, a short description of the type of fraud detected, a sequence of images of the action, such as a comic strip for example in the form of thumbnails, and advantageously a video, preferably accelerated; the objective being that the supervisor and/or the super-supervisor could determine whether the action is fraudulent in a very short time, generally in less than 10 seconds, preferably 5 seconds and advantageously in 2 seconds.</p><p id="p-0176" num="0253">Very cleverly, the interface and/or the conformation of the data are configured to simplify the work of the supervisor and of the super-supervisor.</p><p id="p-0177" num="0254">Very cleverly, the present invention first uses a first automated filter, represented by the computer processing unit <b>1400</b>, preferably based on the use of an artificial intelligence comprising at least one neural network, to filter the potentially fraudulent situations from the other ones, then a second filter is applied. This second filter, according to one embodiment, comprises the mobile supervisors using a mobile analysis device <b>1700</b>. According to another embodiment, this second filter comprises the super-supervisors at the management station <b>1600</b>, therefore the mobile supervisors using a mobile analysis device <b>1700</b> represent a third filter. The combination of these different filters makes the work of each filter increasingly easier and quicker.</p><p id="p-0178" num="0255">It should be noted that quite advantageously, the present invention analyses the possibility of fraud on the basis of an analysis of three-dimensional scenes. In particular, the three-dimensional scenes, also called plurality of images, are collected by the stereoscopic camera <b>1320</b>. These preferably dynamic 3D scenes comprise one or several pluralities of moving points. A first plurality of points corresponds to the item <b>20</b> which is then tracked in space. A second plurality of points may correspond to a user's hand or to another item. Any plurality of points which interacts, i.e. which approaches at a distance less than a predetermined threshold from the first cloud of points, is considered as a potential source of fraud.</p><p id="p-0179" num="0256">In a particularly advantageous manner, and as specified later on, the displacement of the first plurality of points among the various areas is recorded and compared with a plurality of non-fraudulent displacement models. Should a sequence of actions do not correspond with a sequence of actions belonging to a predetermined model among the non-fraudulent models, then the probability of fraud increases.</p><p id="p-0180" num="0257">We will now describe a plurality of standard behaviour models that could be used by the present invention to classify a behaviour.</p><p id="p-0181" num="0258">Standard behaviour model corresponding to the addition of an item <b>20</b>:<ul id="ul0038" list-style="none">    <li id="ul0038-0001" num="0000">    <ul id="ul0039" list-style="none">        <li id="ul0039-0001" num="0259">a. Identification of the item <b>20</b>;</li>        <li id="ul0039-0002" num="0260">b. Definition of the geometric shape of the validated item, called &#x201c;globe&#x201d; hereinafter, in the scan area <b>1321</b>;</li>        <li id="ul0039-0003" num="0261">c. Validated comparison of at least one two-dimensional image of the item <b>20</b> contained in the database <b>1410</b> with at least one two-dimensional image of the item <b>20</b> taken during its identification;</li>        <li id="ul0039-0004" num="0262">d. The validated item <b>20</b> leaves the scan area <b>1321</b>;</li>        <li id="ul0039-0005" num="0263">e. The validated item <b>20</b> passes or not by the external area <b>1322</b>;</li>        <li id="ul0039-0006" num="0264">f. The validated item <b>20</b> enters the entrance area <b>1324</b>;</li>        <li id="ul0039-0007" num="0265">g. Validated comparison of the two-dimensional image of the item <b>20</b> taken during the identification of the item <b>20</b> with the two-dimensional image of the validated item <b>20</b> during passage through the entrance area <b>1324</b>;</li>        <li id="ul0039-0008" num="0266">h. The validated item <b>20</b> enters the internal area <b>1323</b>;</li>        <li id="ul0039-0009" num="0267">i. Measurement of the resulting increase in the weight of the container <b>11</b>, i.e. a measurement of the starting weight increased by the predetermined weight of the identified item <b>20</b>, this increase in weight could occur before or after the two-dimensional identification with an empty hand leaving the internal area <b>1323</b> through the entrance area <b>1324</b>.</li>    </ul>    </li></ul></p><p id="p-0182" num="0268">Standard behaviour model corresponding to the identification of an empty hand:<ul id="ul0040" list-style="none">    <li id="ul0040-0001" num="0000">    <ul id="ul0041" list-style="none">        <li id="ul0041-0001" num="0269">a. An empty hand enters the external area <b>1322</b>;</li>        <li id="ul0041-0002" num="0270">b. An empty hand enters the entrance area <b>1324</b>;</li>        <li id="ul0041-0003" num="0271">c. An empty hand in the internal area <b>1323</b>;</li>        <li id="ul0041-0004" num="0272">d. Weight change or not, this weight change could occur before or after the following two events;</li>        <li id="ul0041-0005" num="0273">e. An empty hand enters the entrance area <b>1324</b>;</li>        <li id="ul0041-0006" num="0274">f. An empty hand enters the external area <b>1322</b>.</li>    </ul>    </li></ul></p><p id="p-0183" num="0275">Standard behaviour model corresponding to the user taking, for example to look at it, an item <b>20</b> already validated and present in the container:<ul id="ul0042" list-style="none">    <li id="ul0042-0001" num="0000">    <ul id="ul0043" list-style="none">        <li id="ul0043-0001" num="0276">a. An empty hand enters the external area <b>1322</b>;</li>        <li id="ul0043-0002" num="0277">b. An empty hand enters the entrance area <b>1324</b>;</li>        <li id="ul0043-0003" num="0278">c. An empty hand in the internal area <b>1323</b>;</li>        <li id="ul0043-0004" num="0279">d. The weight decreases, this decrease in weight possibly occurring at this time point or during the following 5 events;</li>        <li id="ul0043-0005" num="0280">e. A full hand enters the entrance area <b>1324</b>, becomes the object followed by the optical device <b>1300</b>;</li>        <li id="ul0043-0006" num="0281">f. The tracked object enters the external area <b>1322</b>;</li>        <li id="ul0043-0007" num="0282">g. The tracked object enters the entrance area <b>1324</b>;</li>        <li id="ul0043-0008" num="0283">h. Validated two-dimensional comparison between the two-dimensional image of the tracked object during the first pass through the entrance area <b>1324</b> with the two-dimensional image of the tracked object during the second pass through the entrance area <b>1324</b>;</li>        <li id="ul0043-0009" num="0284">i. The tracked object enters the internal area <b>1323</b>;</li>        <li id="ul0043-0010" num="0285">j. The weight increases accordingly, i.e. it returns to the starting weight, this increase in weight possibly occurring between this time point and the end of the model;</li>        <li id="ul0043-0011" num="0286">k. An empty hand enters the entrance area <b>1324</b>;</li>        <li id="ul0043-0012" num="0287">l. An empty hand enters the external area <b>1322</b>.</li>    </ul>    </li></ul></p><p id="p-0184" num="0288">Standard behaviour model corresponding to the user setting an item <b>20</b>, and forgetting to identify it:<ul id="ul0044" list-style="none">    <li id="ul0044-0001" num="0000">    <ul id="ul0045" list-style="none">        <li id="ul0045-0001" num="0289">a. A full hand enters the entrance area <b>1324</b>, becomes the tracked object;</li>        <li id="ul0045-0002" num="0290">b. The measured weight increases, this increase in weight possibly occurring at this time point or during the following 3 events;</li>        <li id="ul0045-0003" num="0291">c. An empty hand enters the entrance area <b>1324</b>;</li>        <li id="ul0045-0004" num="0292">d. An empty hand enters the external area <b>1322</b>;</li>        <li id="ul0045-0005" num="0293">e. An empty hand enters the entrance area <b>1324</b>;</li>        <li id="ul0045-0006" num="0294">f. An empty hand enters the internal area <b>1323</b>;</li>        <li id="ul0045-0007" num="0295">g. The weight decreases, as a result, i.e. it returns to the starting weight of the model, this decrease in weight possibly occurring between this time point and the end of the model;</li>        <li id="ul0045-0008" num="0296">h. The tracked object enters the entrance area <b>1324</b>,</li>        <li id="ul0045-0009" num="0297">i. Validated two-dimensional comparison between the two-dimensional image of the object tracked during the first pass through the entrance area with the two-dimensional image of the object tracked during the second pass through the entrance area <b>1324</b>;</li>        <li id="ul0045-0010" num="0298">j. The tracked object enters the external area <b>1322</b>.</li>    </ul>    </li></ul></p><p id="p-0185" num="0299">Standard behaviour model corresponding to the removal of an item <b>20</b>:<ul id="ul0046" list-style="none">    <li id="ul0046-0001" num="0000">    <ul id="ul0047" list-style="none">        <li id="ul0047-0001" num="0300">a. An empty hand enters the external area <b>1322</b>;</li>        <li id="ul0047-0002" num="0301">b. An empty hand enters the entrance area <b>1324</b>;</li>        <li id="ul0047-0003" num="0302">c. An empty hand enters the internal area <b>1323</b>;</li>        <li id="ul0047-0004" num="0303">d. The weight decreases, this decrease in weight possibly occurring between this time point and the end of the model;</li>        <li id="ul0047-0005" num="0304">e. A full hand enters the entrance area <b>1324</b>, becomes the tracked object;</li>        <li id="ul0047-0006" num="0305">f. The tracked object enters the external area <b>1322</b>;</li>        <li id="ul0047-0007" num="0306">g. The tracked object enters the scan area <b>1321</b>;</li>        <li id="ul0047-0008" num="0307">h. Identification of an item <b>20</b> of the virtual basket selected as being an item <b>20</b> to be removed by the user;</li>        <li id="ul0047-0009" num="0308">i. Validated two-dimensional comparison between the image of the item <b>20</b> during its identification and the image of the object tracked during its passage through the entrance area <b>1324</b>.</li>    </ul>    </li></ul></p><p id="p-0186" num="0309">Finally, it should be noted that from the time point when an element external to the item <b>20</b> and preferably to an empty hand comes into contact with the validated item <b>20</b> or the tracked object or enters the internal area <b>1323</b>, during the steps of a model, fraud might then be suspected.</p><p id="p-0187" num="0310">Similarly, if during a sequence of events, the item <b>20</b> or the tracked object leaves the field of view of the optical device <b>1300</b>, fraud might be suspected.</p><p id="p-0188" num="0311">The present invention advantageously takes advantage of these standard behaviour models. Indeed, instead of trying to classify a sequence of events as fraudulent, it is simpler and faster to compare a sequence of events to a series of models considered as non-fraudulent. Whenever there is a difference above a predetermined threshold between the assessed behaviour and a standard behaviour model, fraud is suspected. If so, it is upon one or several super-supervisor(s) or supervisor(s) to intervene.</p><p id="p-0189" num="0312"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates, according to an embodiment of the present invention, an interface of a management station <b>1600</b> and/or a mobile analysis device <b>1700</b>. This interface is advantageously tactile. This interface comprises a smart graphical interface.</p><p id="p-0190" num="0313">This graphical interface comprises a graphical representation <b>21</b> of the item <b>20</b>, as well as optionally a description <b>22</b>, preferably short and concise. This graphical interface comprises a simple and synthetic description of the potential type of fraud <b>23</b>. This graphical interface may comprise a plurality of images in the form of thumbnails <b>24</b> which could for example represent specific and relevant actions of the user taking into account the type of estimated fraud. This graphical interface preferably comprises a video, advantageously temporally compressed, as described before.</p><p id="p-0191" num="0314">Advantageously, the graphical interface comprises at least a first actuator <b>26</b> and at least a second actuator <b>27</b>. The first actuator <b>26</b> may for example be configured to enable the supervisor or the super-supervisor to indicate that there is no fraud. The second actuator <b>27</b> may for example be configured to enable the supervisor or the super-supervisor to validate that there is a situation of fraud. According to one embodiment, the graphical interface of the management station <b>1600</b> may comprise a third actuator, not illustrated in this figure, configured to transmit the analysis of the data to the mobile supervisor through a mobile analysis device <b>1700</b> so that he could go on site and validate or not a situation of fraud.</p><p id="p-0192" num="0315">Advantageously, if the user has no action reported as potentially fraudulent by the computer processing unit <b>1400</b> and/or no action reported as fraudulent by the supervisor and/or the super-supervisor, then he could pay without any interruption, the purpose being that a user who does not cheat is absolutely not disturbed during his purchase session.</p><p id="p-0193" num="0316">Advantageously, if a fraud is reported by a supervisor and/or a super-supervisor, then:<ul id="ul0048" list-style="none">    <li id="ul0048-0001" num="0000">    <ul id="ul0049" list-style="none">        <li id="ul0049-0001" num="0317">a. The user is notified and waits for the arrival of a supervisor; and/or</li>        <li id="ul0049-0002" num="0318">b. The payment phase is interrupted pending the arrival of a supervisor;</li>    </ul>    </li></ul></p><p id="p-0194" num="0319">In any situation, in case of doubt or validated fraud, a supervisor is in charge of moving to the user and checking the item(s) to which the probability of fraud relates. In this way, the check-up of the supervisor is quick and directly oriented towards one or several item(s) among several others.</p><p id="p-0195" num="0320">Finally, once the user initiates the payment phase, if no action is reported as fraudulent or potentially fraudulent by the supervisors and/or the super-supervisors preferably located remotely, the payment is validated.</p><p id="p-0196" num="0321">In addition to a clever presentation, and as mentioned before, according to one embodiment, the present invention also proposes a clever way for hierarchising the data and the situations of potential fraud to be processed.</p><p id="p-0197" num="0322">We will now list non-limiting examples of priorities taken into account in the presentation of information to the supervisors and super-supervisors:<ul id="ul0050" list-style="none">    <li id="ul0050-0001" num="0000">    <ul id="ul0051" list-style="none">        <li id="ul0051-0001" num="0323">a. The actions of a user in the payment mode should be shown in absolute priority, i.e. a potential fraud situation involving a user in the payment phase has priority;</li>        <li id="ul0051-0002" num="0324">b. The longer a user's purchase session lasts, the higher their actions are prioritised, because their chances of finishing their purchases increase;</li>        <li id="ul0051-0003" num="0325">c. The lower the probability of fraud of an action, the higher it is prioritised, because non-fraudulent users don't have to wait or be slowed down to pay;</li>        <li id="ul0051-0004" num="0326">d. Similarly, a user who has very few suspicious actions will be checked first.</li>    </ul>    </li></ul></p><p id="p-0198" num="0327">Thus, the present invention cleverly crosses several data to assess a probability of fraud, then this data is cleverly conformed and each situation prioritised to allow for fluidity to the user experience and a high responsiveness of the supervisors and/or super-supervisors.</p><p id="p-0199" num="0328">We will now specify a point of the analysis of the data that the present invention implements. Indeed, we have previously indicated that the plurality of collected images is analysed.</p><p id="p-0200" num="0329">Thus, according to a preferred embodiment, the processing of the plurality of data comprises processing of a plurality of collected images, which may comprise two-dimensional images, preferably in colour, and three-dimensional images. This processing is advantageously carried out by the computer processing unit <b>1400</b> which is preferably embedded in a mobile element such as the cart <b>10</b> described before.</p><p id="p-0201" num="0330">According to one embodiment, the cart <b>10</b>, at least the computer processing unit <b>1400</b>, should analyse scenes acquired by several sensors; a so-called two-dimensional camera <b>1310</b>, advantageously a wide-angle one; a so-called stereoscopic 3D camera <b>1320</b>; a gyroscope; a measuring device <b>1200</b>; an identification device <b>1100</b>; etc. . . . .</p><p id="p-0202" num="0331">The analysis of these scenes generally requires a lot of system resources, therefore computing power, and therefore energy. Nonetheless, the system <b>1000</b> according to the present invention is cleverly designed to do this type of processing with little energy, few system resources and quickly.</p><p id="p-0203" num="0332">Indeed, according to one embodiment, this processing could be shifted to a computer server in order to reduce the electrical consumption, but also the system resources used by the cart <b>10</b>.</p><p id="p-0204" num="0333">According to another embodiment, and in particular when the cart <b>10</b> is not connected to a computer server, the processing should be done directly with the system resources and the energy available in the cart <b>10</b>.</p><p id="p-0205" num="0334">The present invention is designed so as to limit the costs and energy of an anti-fraud solution. To this end, the analysis of the scenes is not necessarily a priority in terms of time, i.e. this analysis does not need to be carried out in real-time. This is, inter alia, how the present invention offers a clever solution.</p><p id="p-0206" num="0335">According to one embodiment, the method of the present invention comprises a step of recording the scenes by all sensors on a video, in order to analyse them a posteriori.</p><p id="p-0207" num="0336">Preferably, under some conditions, the two-dimensional and three-dimensional video recording begins, i.e. the two-dimensional and three-dimensional image collection, when there is an object in an area of the previously defined space, for example in the entrance <b>1324</b> or scan <b>1321</b> area, and possibly in the external area <b>1322</b>.</p><p id="p-0208" num="0337">According to one embodiment, the data measured or collected by the other sensors are recorded at the accurate time point of each event.</p><p id="p-0209" num="0338">Advantageously, each event is temporally inset, for example, via metadata in the video. Thus, for example, every scan and every resulting weight change is recorded and noted in the video.</p><p id="p-0210" num="0339">Advantageously, the present invention is configured to generate a timeframe comprising events that could be selected from among: 2D images, 3D images, identification, weight variation, and more generally any measurement by one of the sensors. Thus, this timeframe allows representing the events that have occurred chronologically.</p><p id="p-0211" num="0340">Thus, this enriched timeframe saves time in the analysis of a potential situation of fraud.</p><p id="p-0212" num="0341">Advantageously, the recording of this video is defined by the capture of points in a given space.</p><p id="p-0213" num="0342">According to one embodiment, when the recording starts, it takes into account the previous X seconds in order to have information related to the scene before the event that triggered the recording, i.e. the video record, also known as temporally compressed video, begins with the action that triggered its recording. For this purpose, and as presented before, the system permanently records a predetermined duration, for example 5 seconds, which it gradually deletes. Thus, it records 5 seconds of data for example and erases them after 5 seconds unless an event is detected involving the start of recording for analysis a posteriori, the images recorded before this event are then taken into account in the generation of the temporally compressed video.</p><p id="p-0214" num="0343">According to one embodiment, the start of this recording is subject to a change of state of at least one sensor selected from among all the sensors of the system. As a reminder, the sensors of the system are selected from at least: the identification device <b>1100</b>, the measuring device <b>1200</b>, the optical device <b>1300</b>, a motion sensor, a gyroscope, a spatial positioning sensor, an accelerometer, etc.</p><p id="p-0215" num="0344">Advantageously, the sensor may be a virtual sensor, i.e. a virtual event such as the passage of a cloud of points from one spatial area to another spatial area. For example, when the item <b>20</b> crosses the entrance area <b>1324</b>, this crossing could be considered as a change of state, the analysis of the 3D scene therefore serving as a virtual sensor.</p><p id="p-0216" num="0345">Thus, when an event is detected, captured and possibly measured by one of the sensors of the system, said recording is carried out, preferably via the collection of a plurality of images and data from the various sensors. It should be noted that preferably all of the measurements of each sensor are recorded.</p><p id="p-0217" num="0346">For example, when a scan is in progress, an increase in the weight of the container is expected as a result or when a scan is cancelled, or when the weight has varied and the system is waiting to return to a stable state, these are examples of events leading to the start of data recording.</p><p id="p-0218" num="0347">According to one embodiment, a first recording could be launched when the previously listed conditions are present, then, if there is an absence of user actions, for example after a predetermined time period, then the first recording stops. And a second recording starts as soon as the user performs a new action. Nonetheless, the final analysis comprises the analysis of the first record and of the second record even if this analysis is done on a timeframe comprising one or several time gap(s), i.e. one or several period(s) not recorded as there were no actions.</p><p id="p-0219" num="0348">For example, when a user places an item <b>20</b> without scanning it, the recording will start, but if the user leaves and does not take any action after 10 seconds for example, the recording will stop, and a new recording will start as soon as an action is detected. However, the analysis will be done while considering the two records, because the analysis is done only when the cart <b>10</b> becomes stable again, it will however have a gap in the data record.</p><p id="p-0220" num="0349">Thus, advantageously, an analysis could cover several records.</p><p id="p-0221" num="0350">And preferably, the same record could be used for several different analyses.</p><p id="p-0222" num="0351">The start of the recording could also be launched by the three-dimensional capture of the crossing of the entrance area <b>1324</b> by the cart <b>10</b> for example.</p><p id="p-0223" num="0352">A stable state is defined when all of the sensors do not detect a measurement variation greater than a predetermined threshold, this threshold could depend on each sensor. Hence, an unstable situation is defined as corresponding to the detection of a measurement variation by at least one of the sensors greater than said predetermined threshold, preferably specific to said sensor. It should be noted that the scan of an item is considered as an unstable state by the present invention.</p><p id="p-0224" num="0353">From the time point there has been a switch into an unstable state, all of the video records, as well as the acquisitions of the sensors included in the temporally compressed video of this unstable state are analysed a posteriori. Indeed, when an unstable state is detected, the system resources are primarily dedicated to data collection. Once this unstable state is over, the collected data are processed, i.e. the temporally compressed video, comprising the acquisitions of the different sensors, is analysed by the computer processing unit <b>1400</b>. This allows smartly allocating the limited system resources between data collection and analysis thereof. This allows keeping production costs and energy consumption low.</p><p id="p-0225" num="0354">We will now describe an example of implementation of the optical analysis proposed by the present invention.</p><p id="p-0226" num="0355">In a particularly clever way, and as mentioned before, tracking of the item <b>20</b> and/or of the hand or hands of the user is triggered following the scan of said item <b>20</b>. Similarly, the tracking of an item <b>20</b> could be triggered when the user takes an item out of the container <b>11</b> given the detection of the change in weight by the measuring device <b>1200</b>.</p><p id="p-0227" num="0356">Preferably, after the scan of an item <b>20</b>, the three-dimensional shape of the item <b>20</b>, also called an object, is rebuilt, preferably in two portions, this three-dimensional shape will be called &#x201c;validated shape&#x201d;. The first portion of this validated shape is the end of the shape that we will call the &#x201c;globe&#x201d; which represents the item and the hand. The second portion of this shape is the arm and potentially a portion of the body of the user.</p><p id="p-0228" num="0357">We will describe the operation of this optical analysis using the example of a person buying an item <b>20</b>. This optical analysis enables the identification of what we have called a handling anomaly. Once the scan has been performed, the shape present in the scan area <b>1321</b> becomes the validated shape and the globe is the end thereof. The globe should move from the scan area <b>1321</b> to the external area <b>1322</b>, then pass through the entrance area <b>1324</b> and disappear into the internal area <b>1323</b>. Afterwards, the item is supposed to be deposited in the container <b>11</b>, and therefore a variation in weight should be measured, finally the globe comes out through the entrance area. The globe could also pass directly from the scan area <b>1321</b> to the entrance area <b>1324</b>.</p><p id="p-0229" num="0358">Afterwards, a two-dimensional analysis of the images of the 2D camera <b>1310</b> through a neural network is carried out in order to verify that the globe which comes out of the container after the deposit of the item <b>20</b> in the container <b>11</b> actually corresponds to an empty hand. If the analysis detects an empty hand passing through the entrance area <b>1324</b> and towards the external area <b>1322</b>, then there is no fraud. The same situation applies if the analysis detects an empty hand after measuring an increase in weight consistent with the identifier of the item <b>20</b>, then there is no fraud.</p><p id="p-0230" num="0359">On the other hand, upon the two-dimensional analysis of the images from the 2D camera <b>1310</b>, if the leaving globe is detected as being a &#x201c;full hand&#x201d; by the neural network, this means that the hand comes out full, so that there is a potential fraud.</p><p id="p-0231" num="0360">Let us remember here that several actions could suggest that there has been a fraud after a scan to add an item <b>20</b>, such as, for example, if another unknown shape comes too close to the validated shape (to exchange the item for example), or if a shape obstructs the camera or if an unknown shape enters the entrance area <b>1324</b>.</p><p id="p-0232" num="0361">Advantageously, during the two-dimensional analysis, if the unknown shape is identified as an empty hand, the probability of fraud could be nuanced, and possibly zero.</p><p id="p-0233" num="0362">Cleverly, if the measuring device <b>1200</b> detects a deposition action, i.e. an increase in the weight of the container <b>11</b>, while the validated shape is still in the external area <b>1322</b>, one could deduce a strong probability of fraud via the detection of a handling anomaly.</p><p id="p-0234" num="0363">In the case of a removal, the scenario without fraud is the same, but in the other direction, i.e. a hand identified as empty recovers an item <b>20</b> whose weight is subtracted from that of the container <b>11</b> and this item <b>20</b> is then scanned, the correspondence between the predetermined weight and the less measured weight confirms the absence of fraud for example. Conversely, if a weight is removed without a subsequent scan or if the weight of the scanned item <b>20</b> does not correspond to the removed weight, the probability of fraud increases.</p><p id="p-0235" num="0364">In the case where an unscanned item <b>20</b> is deposited in the container <b>11</b>, the system <b>1000</b> will detect a full hand via the two-dimensional analysis, this hand crossing the entrance area <b>1324</b>, and possibly the internal area <b>1323</b>, and the measuring device <b>1200</b> will detect an increase in the weight of the container <b>11</b> and its contents. Thus, no item <b>20</b> having been scanned prior to this weight increase, the probability of a weight anomaly, i.e. fraud, is high. In the case where a full hand is detected crossing the entrance area <b>1324</b> for example, and possibly the internal area <b>1323</b> without prior scanning, a handling anomaly is detected, and the probability of fraud increases.</p><p id="p-0236" num="0365">Similarly, if for example the measuring device <b>1200</b> detects an increase in weight, this means that a deposit action has been performed, and if no scan has been performed, the probability of fraud increases.</p><p id="p-0237" num="0366">In the case where an empty hand enters, then an item <b>20</b> is removed and a full hand leaves the internal area <b>1323</b>, the leaving shape becomes what we will call a tracked shape, i.e. the shape followed by the optical device <b>1300</b>.</p><p id="p-0238" num="0367">In the case where the tracked shape does not leave the field of view of the optical device <b>1300</b> and re-enters the internal area <b>1323</b> without an unknown shape approaching it, without entering the entrance area <b>1324</b> or without the optical device <b>1300</b> being obstructed, there is no handling anomaly and the probability of fraud is low.</p><p id="p-0239" num="0368">Preferably, this action being all the same suspicious, the present invention provides for a two-dimensional comparison of the taken out item <b>20</b> and the returned item <b>20</b>.</p><p id="p-0240" num="0369">In the case where a tracked shape, and possibly a validated shape, leaves the field of the optical device <b>1300</b>, the function of the system <b>1000</b> is to find this shape when the shape enters again in the field of view of the optical device <b>1300</b>.</p><p id="p-0241" num="0370">Advantageously, the system <b>1000</b> comprises a so-called &#x201c;wide-angle&#x201d; two-dimensional camera <b>1310</b>, i.e. having an optical angle larger than 100 degrees. This 2D camera <b>1310</b> is configured to also ensure this tracking function.</p><p id="p-0242" num="0371">Advantageously, the optical device comprises an additional 2D camera configured to cooperate with the 3D camera. Indeed, the additional 2D camera is configured to collect two-dimensional images of the three-dimensional scene.</p><p id="p-0243" num="0372">According to one embodiment, the optical device <b>1300</b> comprises a plurality of 3D cameras <b>1320</b> and 2D cameras <b>1310</b>, and possibly additional 2D cameras.</p><p id="p-0244" num="0373">Thus, when a shape is tracked, for example via the stereoscopic camera <b>1320</b>, its two-dimensional aspect observed via the additional 2D camera is &#x201c;learned&#x201d; by automatic training of a neural network via a technique of the &#x201c;machine learning&#x201d; type, a term indicating automatic training. Simultaneously, its position on the three-dimensional camera <b>1320</b> is synchronised on the two-dimensional camera <b>1310</b>.</p><p id="p-0245" num="0374">The objective being that when the object or the item <b>20</b> leaves the field of the 3D camera <b>1320</b>, the 2D camera <b>1310</b> &#x201c;knows&#x201d; its appearance, its geometric shape, and its position at the exit in order to continue to track the object on the 2D camera <b>1310</b>.</p><p id="p-0246" num="0375">Thus, the three-dimensional camera <b>1320</b> enables the system <b>1000</b> to learn the shape of the tracked item and track its position in space, this learned shape and this known position are then transmitted to the two-dimensional camera <b>1310</b> for tracking over a larger area, as soon as the item <b>20</b> leaves the monitoring area of the three-dimensional camera <b>1320</b>.</p><p id="p-0247" num="0376">The goal being that when the item <b>20</b>, or more generally the tracked object, re-enters the field of the three-dimensional camera <b>1320</b>, the 2D camera <b>1310</b> could communicate its position thereto as well as its aspect in return, so that the 3D camera <b>1320</b> could resume its monitoring, and possibly improve its learning, for example.</p><p id="p-0248" num="0377">According to one embodiment, an analysis could be done on the 2D camera <b>1310</b> in order to know whether a full hand or an empty hand has approached the tracked item <b>20</b>, or the object. The term item or object is used independently to define item <b>20</b>.</p><p id="p-0249" num="0378">In the case where a hand has approached the tracked item <b>20</b>, the probability of fraud increases. According to one embodiment, the present invention comprises a double check mode. This mode is to be set up when there is a doubt concerning a fraud. This mode consists in transmitting a request to the user to scan again an item <b>20</b> that is supposed to be in the container <b>11</b>, a few minutes after he has inserted it or during his payment.</p><p id="p-0250" num="0379">In the case where there is no fraud, the normal course should be that an empty hand enters the container <b>11</b>, that a weight corresponding to the requested item <b>20</b> should be removed by the full hand which should remove the item <b>20</b>, the scanner; then the item <b>20</b> is then put back in place, the measured weight of the container <b>11</b> and its contents should therefore rise and finally an empty hand should come out. If this does not happen, fraud might be suspected, the probability of fraud could then increase.</p><p id="p-0251" num="0380">There is a technique to counter the anti-fraud systems of the prior art based solely on the measurement of weight. This consists in replacing the label, more generally the barcode of the item <b>20</b>. For this purpose, for example, the item <b>20</b> is weighed in the store on a fruit and vegetable scale, then the label corresponding to a piece of fruit, for example, is stuck on the item <b>20</b>. This label indicates the correct weight, but not the correct item <b>20</b>, at the level of the automatic checkout for example, the user will scan the item <b>20</b>, and place it on a scale, the system of the prior art has no means for detecting the fraud that is taking place.</p><p id="p-0252" num="0381">The present invention provides an effective solution to this type of fraud. Indeed, to defeat this type of fraud, the present invention suggests taking photos in the direction of the item <b>20</b> from different angles. During a scan, these photos have a double use:<ul id="ul0052" list-style="none">    <li id="ul0052-0001" num="0000">    <ul id="ul0053" list-style="none">        <li id="ul0053-0001" num="0382">a. Firstly, these photos are used to feed a neural network model linked to the identifier of the item <b>20</b>, for example to its barcode. Afterwards, this neural network is configured to indicate whether the item <b>20</b> is present on an image taken by the optical device <b>1300</b> or not. This is done by comparing the photo just taken and all of the other photos taken during a scan for the same item <b>20</b> for example.</li>        <li id="ul0053-0002" num="0383">b. Secondly, the photos pass through the neural network model and the output of said network is a probability of conformity between the scanned item <b>20</b> and the expected item <b>20</b> with regards to its identifier, in order to estimate whether the scanned barcode actually corresponds to the item <b>20</b> of the photo.</li>    </ul>    </li></ul></p><p id="p-0253" num="0384">According to one embodiment, the label exchange being done most often with fruits and vegetables, the neural network is trained to identify a bag of fruits and/or vegetables, and if during a scan of a &#x201c;fruits and vegetables&#x201d; barcode, the optical device <b>1300</b> does not recognise a bag of this type, then fraud is suspected.</p><p id="p-0254" num="0385">In a particularly clever and counter-intuitive way, the database may comprise a score per item corresponding to the fact that it is a cheap item and therefore regularly used to carry out fraud, either by using the label of such an item, or its packaging, for example without limitation. Also, preferably, these inexpensive items have a higher fraud score than luxury items.</p><p id="p-0255" num="0386">According to another embodiment, luxury items have a higher fraud score than other items.</p><p id="p-0256" num="0387">It should be noted that when the probability of fraud exceeds a predetermined threshold, fraud is considered, and it is then up to the super-supervisor and/or the supervisor to intervene by confirming the fraud, or by invalidating it and/or by moving to the user.</p><p id="p-0257" num="0388">We will now describe <figref idref="DRAWINGS">FIG. <b>5</b></figref> which schematically represents the data recording and processing process.</p><p id="p-0258" num="0389">This figure illustrates two portions of a fraud detection algorithm according to an embodiment of the present invention.</p><p id="p-0259" num="0390">We will describe it. The recording <b>110</b> of the data begins <b>120</b> as soon as an object is detected by the optical device, preferably by the stereoscopic camera and advantageously when the detected object is located in one of the areas of the three-dimensional space. If no object is detected <b>122</b>, the recording remains on standby.</p><p id="p-0260" num="0391">If there is detection <b>121</b>, then the previous X seconds are stored in memory <b>130</b>, <b>131</b> and recording continues after them. If an object is still present in one of the areas <b>140</b>, then <b>142</b>, recording continues <b>143</b>.</p><p id="p-0261" num="0392">If there is no more object in the three-dimensional space <b>141</b>, X seconds are counted <b>150</b> and added <b>151</b> to the end of the recording upon completion thereof <b>146</b>. Recording then stops <b>160</b>.</p><p id="p-0262" num="0393">Once the record is created, it is transmitted <b>147</b> for analysis.</p><p id="p-0263" num="0394">The analysis <b>210</b> is in standby as long as a recording is in progress. Thus, the system monitors whether an identification is in progress <b>220</b>: yes <b>221</b>, no <b>222</b>, whether a weight measurement is in progress <b>225</b>: yes <b>223</b>, no <b>222</b>.</p><p id="p-0264" num="0395">When an identification is in progress and/or the weight is unstable, the system prepares <b>230</b> to analyse a record.</p><p id="p-0265" num="0396">If the measurements are complete, i.e. if the state of the system is again stable <b>240</b>, then we carry on <b>241</b> with the definition <b>250</b> of the end of the record to be analysed, otherwise <b>242</b> we remain on standby for a stable situation to carry out the analysis. As a reminder, a situation is considered unstable as long as variations in the measurements of the sensors are detected.</p><p id="p-0266" num="0397">Once the system is stable, the analysis <b>260</b> of the record begins. This allows using the little system resource only when the data collection phase is complete.</p><p id="p-0267" num="0398">Afterwards, the algorithm finishes its analysis <b>270</b> and returns to its initial state of waiting for a new analysis to be carried out.</p><p id="p-0268" num="0399">According to one embodiment, in the event of an unstable situation, while an analysis is in progress, part of the system resources allocated to the analysis is redistributed for the collection of data.</p><p id="p-0269" num="0400">In a particularly advantageous manner, the present invention uses few system resources and little energy by separating into two distinct phases, the collection of data and the analysis of these collected data.</p><p id="p-0270" num="0401">Thus, the present invention allows obtaining high-quality fraud detection while proposing a low-cost technical solution, the solution being optimised for a large-scale and inexpensive application.</p><p id="p-0271" num="0402">Thus, the present invention allows solving at least the following fraud situations:<ul id="ul0054" list-style="none">    <li id="ul0054-0001" num="0000">    <ul id="ul0055" list-style="none">        <li id="ul0055-0001" num="0403">a. The user scans an item <b>20</b> and places two;</li>        <li id="ul0055-0002" num="0404">b. The user discreetly places an item <b>20</b> in the container <b>11</b> without scanning it;</li>        <li id="ul0055-0003" num="0405">c. The user scans a bottle of wine at <b>5</b>&#x20ac; and deposits one at <b>50</b>&#x20ac; of equivalent weight, and possibly one that resembles it morphologically;</li>        <li id="ul0055-0004" num="0406">d. The user swaps an unscanned item <b>20</b> with an already scanned item <b>20</b> in the container <b>11</b>,</li>        <li id="ul0055-0005" num="0407">e. The user scans an item <b>20</b> with a fruit or vegetable barcode label;</li>        <li id="ul0055-0006" num="0408">f. The user puts a fragrance bottle in a fruit and vegetable bag and scans it with a fruit and vegetable barcode label.</li>    </ul>    </li></ul></p><p id="p-0272" num="0409">Hence, the present invention uses the merger of several data from several sensors to determine a probability of fraud.</p><p id="p-0273" num="0410">In a particularly advantageous manner, the present invention comprises a so-called self-learning analysis of its data, i.e. the computer processing unit is configured to automatically learn the elements forming a fraud. For example, the system is configured to learn that generally a series of actions, or that some values of the collected data lead to a situation of fraud. For this purpose, the processing unit receives as input a plurality of data and as output the situation is judged as fraud or not by the supervisors and/or the super-supervisors.</p><p id="p-0274" num="0411">The invention is not limited to the previously-described embodiments and extends to all of the embodiments covered by the claims.</p><heading id="h-0006" level="1">REFERENCES</heading><p id="p-0275" num="0000"><ul id="ul0056" list-style="none">    <li id="ul0056-0001" num="0000">    <ul id="ul0057" list-style="none">        <li id="ul0057-0001" num="0412"><b>10</b> Mobile cart</li>        <li id="ul0057-0002" num="0413"><b>11</b> container</li>        <li id="ul0057-0003" num="0414"><b>12</b> Display device</li>        <li id="ul0057-0004" num="0415"><b>13</b> Gripping device</li>        <li id="ul0057-0005" num="0416"><b>14</b> Wheel</li>        <li id="ul0057-0006" num="0417"><b>15</b> Frame</li>        <li id="ul0057-0007" num="0418"><b>16</b> Electric battery</li>        <li id="ul0057-0008" num="0419"><b>20</b> Item</li>        <li id="ul0057-0009" num="0420"><b>21</b> Graphical representation of the item</li>        <li id="ul0057-0010" num="0421"><b>22</b> Description of the item</li>        <li id="ul0057-0011" num="0422"><b>23</b> Description of the potential fraud situation</li>        <li id="ul0057-0012" num="0423"><b>24</b> Thumbnails</li>        <li id="ul0057-0013" num="0424"><b>25</b> Temporally compressed video</li>        <li id="ul0057-0014" num="0425"><b>26</b> First actuator</li>        <li id="ul0057-0015" num="0426"><b>27</b> Second actuator</li>        <li id="ul0057-0016" num="0427"><b>1000</b> Fraud detection system</li>        <li id="ul0057-0017" num="0428"><b>1100</b> Identification device</li>        <li id="ul0057-0018" num="0429"><b>1200</b> Measuring device</li>        <li id="ul0057-0019" num="0430"><b>1300</b> Optical device</li>        <li id="ul0057-0020" num="0431"><b>1310</b> Camera</li>        <li id="ul0057-0021" num="0432"><b>1320</b> Stereoscopic camera</li>        <li id="ul0057-0022" num="0433"><b>1321</b> Scan area</li>        <li id="ul0057-0023" num="0434"><b>1322</b> External area</li>        <li id="ul0057-0024" num="0435"><b>1323</b> Internal area</li>        <li id="ul0057-0025" num="0436"><b>1324</b> Entrance area</li>        <li id="ul0057-0026" num="0437"><b>1400</b> Computer processing unit</li>        <li id="ul0057-0027" num="0438"><b>1410</b> Database</li>        <li id="ul0057-0028" num="0439"><b>1420</b> Artificial intelligence module</li>        <li id="ul0057-0029" num="0440"><b>1500</b> User interface</li>        <li id="ul0057-0030" num="0441"><b>1600</b> Management station</li>        <li id="ul0057-0031" num="0442"><b>1700</b> Mobile analysis device</li>    </ul>    </li></ul></p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method for detecting fraud in the event of the purchase by at least one user of at least one item comprising at least:<claim-text>a. a capturing step, performed by at least one user terminal, of a plurality of data from at least one sensor, the capturing step comprising at least the following steps:<claim-text>i. obtainment of an identifier of the item by at least one identification device;</claim-text><claim-text>ii. determination by at least one optical device of at least one trajectory of the item manually moved by the user in a three-dimensional space, said three-dimensional space comprising at least:</claim-text><claim-text>1. an identification area corresponding to a volume of the three-dimensional space in which at least one portion of the item is intended to be disposed by the user to achieve the obtainment of the identifier of the item;</claim-text><claim-text>2. an entrance area corresponding to a volume of the three-dimensional space crossed by the item when the user deposits the item in at least one container associated with the user terminal);</claim-text><claim-text>iii. sending by the user terminal to at least one computer processing unit of:<claim-text>1. the identifier of the item from the identification device;</claim-text><claim-text>2. the trajectory of the item;</claim-text></claim-text></claim-text><claim-text>b. a processing step performed by the computer processing unit, of the plurality of data comprising at least the following steps:<claim-text>i. generation of at least one behaviour of said item from at least the trajectory of the item in the three-dimensional space;</claim-text><claim-text>ii. comparison of the behaviour of said item with a plurality of predetermined behaviour models so as to identify a handling anomaly by the user;</claim-text></claim-text><claim-text>c. a step of determining a probability of fraud as a function of said behaviour comparison, this probability being non-zero if a handling anomaly has been identified.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the optical device is configured to enable depth to be taken into account in determining said trajectory of the item.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the trajectory of the item in the three-dimensional space, as determined by the optical device, comprises at least one plurality of points, each point of said plurality of points comprising at least three spatial coordinates.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the optical device (<b>1300</b>) comprises a stereoscopic optical device.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the step of capturing a plurality of data comprises at least one measurement, by at least one measuring device, of the weight of the item, and a step of sending by the user terminal to the computer processing unit the measured weight of the item.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the processing step comprises, at least the following steps:<claim-text>a. identification in at least one database of the item from the identifier, the database comprising at least the identifier of the item associated with a predetermined weight of the item;</claim-text><claim-text>b. obtainment of the predetermined weight of the item from the database:<claim-text>i. In in the event that the predetermined weight is equal to zero or is not input, the computer processing unit assigns the measured weight of the item as the predetermined weight associated with said identifier in the database;</claim-text><claim-text>ii. In in the case where the predetermined weight is different from zero and is input, the computer processing unit performs a comparison of the predetermined weight and the measured weight so as to identify a weight anomaly if the weight difference is greater than a predetermined threshold.</claim-text></claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the determination of a probability of fraud is carried out according to said comparison of the predetermined weight with the measured weight, this probability being non-zero if a weight anomaly has been identified.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the predetermined weight of the item contained in the database comprises a range of weights.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the step of determining the trajectory of the item in the three-dimensional space comprises tracking the item in at least one area selected from among at least the identification area, the entrance area, at least one external area, at least one internal area corresponding at least to the entrance of at least one container, the entrance area separating the external area from the internal area.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the determination of the trajectory of the item in the three-dimensional space comprises at least the passages of the item from one area of the three-dimensional space to another area of the three-dimensional space.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the step of determining the trajectory of the item comprises at least the determination of the trajectory of an object other than the item moving in the three-dimensional space.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. (canceled)</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the behaviour generated by said item comprises at least one sequence of events detected by the plurality of sensors, these events being selected from among at least: the identification of the item, the passage from one area of the three-dimensional space to another area of the three-dimensional space, the measurement of the weight of the item, the approach of the item by another object.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the step of capturing the plurality of data comprises the collection by the optical device of a plurality of images at least of the item and at least of a hand of the user carrying the item.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. (canceled)</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The method according to <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the processing step comprises at least one comparison of an image of the item present in the database and one or more images of the plurality of collected images so as to identify an anomaly between the image of the item of the database and the collected image(s) of the item.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. (canceled)</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. (canceled)</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. (canceled)</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the step of determining the trajectory of the item comprises at least:<claim-text>The collection of a plurality of two-dimensional images carried out by at least one camera and by at least one additional camera;</claim-text><claim-text>The collection of a plurality of three-dimensional images carried out by at least one stereoscopic camera.</claim-text></claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. (canceled)</claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. The method according to <claim-ref idref="CLM-00020">claim 20</claim-ref>, wherein the stereoscopic camera is configured to spatially track the item in the three-dimensional space, and wherein the additional camera is configured to transmit a plurality of two-dimensional images to at least one neural network so as to train said neural network to recognise the geometric shape of the item, the spatial position of the item and its geometric shape are then used for tracking the item by the two-dimensional camera when the item leaves the field of view of the stereoscopic camera.</claim-text></claim><claim id="CLM-00023" num="00023"><claim-text><b>23</b>. (canceled)</claim-text></claim><claim id="CLM-00024" num="00024"><claim-text><b>24</b>. (canceled)</claim-text></claim><claim id="CLM-00025" num="00025"><claim-text><b>25</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein a handling anomaly comprises at least one of the following situations: exchange of the item with another item, addition of another item in a container together with said item, removal of another item from said container upon deposition of said item in said container, exchange of an identified item with another unidentified item, identification of an item with a fraudulent identifier.</claim-text></claim><claim id="CLM-00026" num="00026"><claim-text><b>26</b>. (canceled)</claim-text></claim><claim id="CLM-00027" num="00027"><claim-text><b>27</b>. (canceled)</claim-text></claim><claim id="CLM-00028" num="00028"><claim-text><b>28</b>. (canceled)</claim-text></claim><claim id="CLM-00029" num="00029"><claim-text><b>29</b>. (canceled)</claim-text></claim><claim id="CLM-00030" num="00030"><claim-text><b>30</b>. (canceled)</claim-text></claim><claim id="CLM-00031" num="00031"><claim-text><b>31</b>. (canceled)</claim-text></claim><claim id="CLM-00032" num="00032"><claim-text><b>32</b>. (canceled)</claim-text></claim><claim id="CLM-00033" num="00033"><claim-text><b>33</b>. A system for detecting at least one fraud in the event of the purchase by a user of at least one item in a store, comprising at least:<claim-text>a user terminal comprising at least:<claim-text>i. an identification device configured to identify the item when a user passes the item in the proximity of the identification device;</claim-text><claim-text>ii. a measuring device configured to measure the weight of the item;</claim-text><claim-text>iii. an optical device configured at least to determine at least one trajectory of the item manually moved by the user in the three-dimensional space;</claim-text></claim-text><claim-text>a computer processing unit in communication with at least the user terminal, the computer processing unit being remote or not from the user terminal and being configured to:<claim-text>i. generate at least one behaviour of said item at least from the trajectory of the item in the three-dimensional space;</claim-text><claim-text>ii. compare the behaviour of said item with a plurality of predetermined behaviour models so as to identify a handling anomaly;</claim-text></claim-text></claim-text><claim-text>so as to determine a probability of fraud as a function of said behaviour comparison, this probability being non-zero if a handling anomaly has been identified.</claim-text></claim><claim id="CLM-00034" num="00034"><claim-text><b>34</b>. The system according to <claim-ref idref="CLM-00033">claim 33</claim-ref>, wherein the computer processing unit is further in communication with a database comprising the identifier of the item associated with a predetermined weight of the item.</claim-text></claim><claim id="CLM-00035" num="00035"><claim-text><b>35</b>. The system according to <claim-ref idref="CLM-00034">claim 34</claim-ref>, wherein the computer processing unit is further configured to:<claim-text>compare the predetermined weight of the item obtained from the database with the measured weight so as to identify a weight anomaly if the weight difference is greater than a predetermined threshold;</claim-text><claim-text>determine a probability of fraud according to said weight comparison, this probability being non-zero if a weight anomaly has been identified.</claim-text></claim-text></claim><claim id="CLM-00036" num="00036"><claim-text><b>36</b>. The system according to <claim-ref idref="CLM-00033">claim 33</claim-ref>, wherein the user terminal is a mobile cart.</claim-text></claim><claim id="CLM-00037" num="00037"><claim-text><b>37</b>. The system according to <claim-ref idref="CLM-00036">claim 36</claim-ref>, wherein at least one portion of the computer processing unit is embedded in the mobile cart.</claim-text></claim><claim id="CLM-00038" num="00038"><claim-text><b>38</b>. The system according to <claim-ref idref="CLM-00033">claim 33</claim-ref>, wherein the user terminal is a fixed terminal.</claim-text></claim><claim id="CLM-00039" num="00039"><claim-text><b>39</b>. The system according to <claim-ref idref="CLM-00033">claim 33</claim-ref>, wherein the computer processing unit is in communication with at least one classification module comprising at least one neural network trained to detect a fraud situation based on data transmitted to the computer processing unit.</claim-text></claim><claim id="CLM-00040" num="00040"><claim-text><b>40</b>. The system according to <claim-ref idref="CLM-00033">claim 33</claim-ref>, wherein the user terminal comprises at least one display device configured to display at least the identifier and/or the weight of the item.</claim-text></claim><claim id="CLM-00041" num="00041"><claim-text><b>41</b>. A computer program product comprising instructions which, when performed by at least one processor, executes at least the steps of the method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>.</claim-text></claim></claims></us-patent-application>