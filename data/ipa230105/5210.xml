<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005211A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005211</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17942542</doc-number><date>20220912</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>15</main-group><subgroup>06</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>5</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>20</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>15</main-group><subgroup>50</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>5</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>15</main-group><subgroup>06</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>5</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20190101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>20</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>15</main-group><subgroup>506</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>5</main-group><subgroup>002</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">DENOISING TECHNIQUES SUITABLE FOR RECURRENT BLURS</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>17203267</doc-number><date>20210316</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11508113</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17942542</doc-number></document-id></child-doc></relation></continuation><us-provisional-application><document-id><country>US</country><doc-number>62991000</doc-number><date>20200317</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>NVIDIA Corporation</orgname><address><city>Santa Clara</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Zhdan</last-name><first-name>Dmitriy</first-name><address><city>Korolev</city><country>RU</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Makarov</last-name><first-name>Evgeny</first-name><address><city>Moscow</city><country>RU</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Recurrent blurring may be used to render frames of a virtual environment, where the radius of a filter for a pixel is based on a number of successfully accumulated frames that correspond to that pixel. To account for rejections of accumulated samples for the pixel, ray-traced samples from a lower resolution version of a ray-traced render may be used to increase the effective sample count for the pixel. Parallax may be used to control the accumulation speed along with an angle between a view vector that corresponds to the pixel. A magnitude of one or more dimensions of a filter applied to the pixel may be based on an angle of a view vector that corresponds to the pixel to cause reflections to elongate along an axis under glancing angles. The dimension(s) may be based on a direction of a reflected specular lobe associated with the pixel.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="99.06mm" wi="158.75mm" file="US20230005211A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="205.23mm" wi="149.69mm" orientation="landscape" file="US20230005211A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="197.10mm" wi="141.48mm" orientation="landscape" file="US20230005211A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="229.19mm" wi="112.86mm" file="US20230005211A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="229.19mm" wi="125.39mm" file="US20230005211A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="229.19mm" wi="112.86mm" file="US20230005211A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="184.49mm" wi="150.03mm" orientation="landscape" file="US20230005211A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="175.51mm" wi="152.23mm" orientation="landscape" file="US20230005211A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="203.28mm" wi="151.21mm" orientation="landscape" file="US20230005211A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="217.34mm" wi="112.86mm" file="US20230005211A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="217.34mm" wi="113.62mm" file="US20230005211A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="217.34mm" wi="114.47mm" file="US20230005211A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="196.17mm" wi="99.65mm" file="US20230005211A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="221.83mm" wi="101.26mm" orientation="landscape" file="US20230005211A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="123.70mm" wi="120.40mm" orientation="landscape" file="US20230005211A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="146.98mm" wi="126.32mm" orientation="landscape" file="US20230005211A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="217.34mm" wi="113.62mm" file="US20230005211A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="217.34mm" wi="113.62mm" file="US20230005211A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="217.34mm" wi="113.62mm" file="US20230005211A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00019" num="00019"><img id="EMI-D00019" he="227.08mm" wi="146.13mm" file="US20230005211A1-20230105-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00020" num="00020"><img id="EMI-D00020" he="217.34mm" wi="113.62mm" file="US20230005211A1-20230105-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00021" num="00021"><img id="EMI-D00021" he="217.34mm" wi="113.96mm" file="US20230005211A1-20230105-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00022" num="00022"><img id="EMI-D00022" he="217.34mm" wi="113.96mm" file="US20230005211A1-20230105-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00023" num="00023"><img id="EMI-D00023" he="221.91mm" wi="156.55mm" orientation="landscape" file="US20230005211A1-20230105-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00024" num="00024"><img id="EMI-D00024" he="220.98mm" wi="123.95mm" file="US20230005211A1-20230105-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00025" num="00025"><img id="EMI-D00025" he="238.59mm" wi="129.62mm" file="US20230005211A1-20230105-D00025.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application is a Continuation of U.S. application Ser. No. 17/203,267, filed Mar. 16, 2021, which claims the benefit of U.S. Provisional Application No. 62/991,000, filed on Mar. 17, 2020, both of which are hereby incorporated by reference in their entirety.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">Ray-tracing may be used to render images by tracing a path of light in a virtual environment and simulating the effects of the light's encounters with virtual objects. Various applications of ray-tracing technology may simulate a variety of optical effects&#x2014;such as shadows, reflections and refractions, scattering phenomenon, ambient occlusions, global illuminations, and dispersion phenomenon (such as chromatic aberration). Ray-tracing typically involves generating ray-traced samples by casting rays in a virtual environment to sample lighting conditions for pixels. The ray-traced samples may be combined and used to determine pixel colors for an image. To conserve computing resources, the lighting conditions may be sparsely sampled, resulting in noisy render data. To produce a final render that approximates a render of a fully-sampled scene, a denoising filter may be applied to the noisy render data to reduce noise and temporal accumulation may be used to increase the effective sample count by leveraging information from previous frames.</p><p id="p-0004" num="0003">Denoising strategies that involve temporal accumulation typically use either post-accumulation or pre-accumulation. In post-accumulation, a frame of noisy render data is first blurred, then temporally accumulated with similar blurred frames that were previously blurred. In pre-accumulation, the frames of noisy render data are first temporally accumulated, then the temporally accumulated data is blurred. In either case artifacts may result, as low and high frequency inputs are not mixed in blurring. Recurrent blur may be used to mix low and high frequency inputs by iteratively accumulating a frame of noisy render data with temporally accumulated data, then blurring the accumulated data to generate an output that is fed back to update the temporally accumulated data for a subsequent iteration. However, as recurrent blur feeds back a blurred output that is then used for blurring, this approach may be prone to over-blurring.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0005" num="0004">This disclosure relates to denoising techniques for recurrent blurs in ray-tracing applications. More specifically, the current disclosure relates to various approaches which may be used to improve denoising of ray-traced renders, such as denoising that employs recurrent blurs. Various disclosed concepts may be incorporated into blurring that leverages temporal accumulation.</p><p id="p-0006" num="0005">In contrast to traditional systems, the disclosure may provide for recurrent blurring in order to render frames of a virtual environment, where the radius (or more generally dimension or size) of at least one denoising filter for a pixel is based at least on a number of successfully accumulated frames or samples that correspond to that pixel. A hierarchical approach may be used to account for rejections of temporally accumulated samples with respect to pixels where rather than using only the ray-traced sample(s) from a ray-traced render for that pixel, ray-traced samples from a lower resolution version of the ray-traced render may be used to increase the effective sample count. Parallax may be used to control the accumulation speed of temporal accumulation for a pixel along with an angle between a view vector that corresponds to the pixel to account for how parallax is perceived visually at different viewing angles.</p><p id="p-0007" num="0006">Rather than applying a denoising filter to the pixel that is effectively isotropic in world space, a magnitude of one or more dimensions of the filter may be based at least on an angle of a view vector that corresponds to the pixel to cause reflections to elongate along an axis under glancing angles, resulting in a more realistic render. The dimension(s) may be based at least on a direction (e.g., dominant direction) of a reflected specular lobe associated with the view vector and a corresponding surface normal.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0008" num="0007">The present systems and methods for denoising techniques suitable for recurrent blurs are described in detail below with reference to the attached drawing figures, wherein:</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a data flow diagram illustrating an example process for determining one or more filter parameters based at least on accumulated samples, in accordance with at least one embodiment of the present disclosure;</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>2</b>A</figref> shows an example render of a virtual environment which may be generated using the example process of <figref idref="DRAWINGS">FIG. <b>1</b></figref> after two frames of accumulation, in accordance with at least one embodiment of the present disclosure;</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>2</b>B</figref> shows an example render of a virtual environment which may be generated using the example process of <figref idref="DRAWINGS">FIG. <b>1</b></figref> after thirty-two frames of accumulation, in accordance with at least one embodiment of the present disclosure;</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flow diagram showing an example of a method for determining one or more parameters of a filter based at least on a quantity of rendered frames of a virtual environment, in accordance with at least one embodiment of the present disclosure;</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flow diagram showing an example of a method for iteratively determining one or more parameters of a filter based at least on a quantity of renders of a virtual environment that correspond to an accumulated sample, in accordance with at least one embodiment of the present disclosure;</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flow diagram showing an example of a method for determining one or more parameters of a filter based at least on a quantity of renders of a virtual environment that correspond to at least one sample of an accumulated render, in accordance with at least one embodiment of the present disclosure;</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>6</b>A</figref> is a flow diagram showing an example process for denoising using recurrent blur that includes generating an augmented sample based at least on a quantity of renders of a virtual environment that correspond to a ray-traced sample in an accumulated render, in accordance with at least one embodiment of the present disclosure;</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>6</b>B</figref> is a flow diagram showing an example process for denoising using pre-accumulation that includes generating an augmented sample based at least on a quantity of renders of a virtual environment that correspond to a ray-traced sample in an accumulated render, in accordance with at least one embodiment of the present disclosure;</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>7</b>A</figref> shows an example of a render of a virtual environment at different mip levels, in accordance with at least one embodiment of the present disclosure;</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>7</b>B</figref> shows examples of mipmap levels and an alignment between neighboring mipmap levels, in accordance with at least one embodiment of the present disclosure;</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>8</b>A</figref> shows an example render of a virtual environment which may be generated without augmenting accumulated samples, in accordance with at least one embodiment of the present disclosure;</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>8</b>B</figref> shows an example render of a virtual environment which may be generated with augmenting accumulated samples, in accordance with at least one embodiment of the present disclosure;</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a flow diagram showing an example of a method for augmenting an accumulated render based at least on a quantity of renders that correspond to a ray-traced sample of a virtual environment in an accumulated render of the virtual environment, in accordance with at least one embodiment of the present disclosure;</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a flow diagram showing an example of a method for iteratively augmenting a set of accumulated samples based at least on a quantity of renders that correspond to at least one ray-traced sample of a virtual environment, in accordance with at least one embodiment of the present disclosure;</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a flow diagram showing an example of a method for augmenting an accumulated render based at least on a quantity of renders that correspond to a pixel in an accumulated render of the virtual environment, in accordance with at least one embodiment of the present disclosure;</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a data flow diagram illustrating an example process for denoising a render of a virtual environment, in accordance with at least one embodiment of the present disclosure;</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>13</b>A</figref> is a diagram illustrating an example of determining a blur plane for mirror reflections, in accordance with at least one embodiment of the present disclosure;</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>13</b>B</figref> is a diagram illustrating an example of determining a blur plane for diffuse signals, in accordance with at least one embodiment of the present disclosure;</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>14</b>A</figref> is a diagram illustrating an example of determining a blur plane for denoising ray-traced samples, in accordance with at least one embodiment of the present disclosure;</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>14</b>B</figref> is a diagram illustrating an example of determining an anisotropic filter for denoising ray-traced samples, in accordance with at least one embodiment of the present disclosure;</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a flow diagram showing an example of a method for computing a dimension of a filter based at least on a direction of a reflected specular lobe, a normal, and an angle of a view vector, in accordance with at least one embodiment of the present disclosure;</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>16</b></figref> is a flow diagram showing an example of a method for determining an anisotropic filter based at least on elongating an isotropic filter basis based at least on an angle of a view vector, in accordance with at least one embodiment of the present disclosure;</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is a flow diagram showing an example of a method for computing a dimension of a filter based at least on a direction of a reflected specular lobe, a normal, and an angle of a ray associated with an interaction, in accordance with at least one embodiment of the present disclosure;</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>18</b></figref> shows an example render of a virtual environment with view vectors that may be used to compute parallax, in accordance with at least one embodiment of the present disclosure;</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>19</b></figref> is a flow diagram showing an example of a method for computing an accumulation speed based at least on an angle between view vectors and a viewing angle associated with a view vector, in accordance with at least one embodiment of the present disclosure;</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>20</b></figref> is a flow diagram showing an example of a method for accumulating ray-traced renders using accumulation speeds based at least on parallax between views and viewing angles captured by the ray-traced renders, in accordance with at least one embodiment of the present disclosure;</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>21</b></figref> is a flow diagram showing an example of a method for computing an accumulation speed based at least on parallax between renders and a viewing angle associated with one or more of the renders, in accordance with at least one embodiment of the present disclosure;</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>22</b></figref> is a diagram used to illustrate examples of ray-tracing techniques which may be used to generate ray-traced renders, in accordance with some embodiments of the present disclosure;</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>23</b></figref> is a block diagram of an example computing device suitable for use in implementing some embodiments of the present disclosure; and</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>24</b></figref> is a block diagram of an example data center suitable for use in implementing some embodiments of the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0039" num="0038">Systems and methods are disclosed related to denoising techniques suitable for recurrent blurs. The disclosure provides various denoising techniques which may be used separately or in any combination thereof. Various disclosed concepts may be incorporated into blurring that leverages temporal accumulation.</p><p id="p-0040" num="0039">Aspects of the disclosure may provide for recurrent blurring in order to render frames of a virtual environment, where the radius (or more generally dimension or size) of at least one denoising filter for a pixel is based at least on a number of successfully accumulated frames that correspond to that pixel (e.g., temporally accumulated samples in a history buffer). For example, the number of successfully accumulated frames may be tracked per-pixel and may be used to control or adapt the blur dimensions of denoising filters applied to those pixels (e.g., in one or more spatial filtering passes). These approaches may be used to reduce sizes of denoising filters for pixels as the number of accumulated samples or frames associated with those pixels increases, thereby reducing or preventing over-blurring.</p><p id="p-0041" num="0040">Aspects of the disclosure may also provide for a hierarchical approach to accounting for rejections of temporally accumulated samples with respect to pixels. Typically when a temporally accumulated sample for a pixel gets rejected&#x2014;for example, due to a disocclusion of the pixel in ray-tracing&#x2014;temporal accumulation for that pixel is reset. Thus, the effective sample count for the pixel may be reduced for a number of frames as temporal data is re-accumulated. This may result in artifacts in rendered frames, particularly when the number of rays per-pixel used to sample a virtual environment for the pixel is low.</p><p id="p-0042" num="0041">Disclosed approaches may supplement the effective number of rays per-pixel used to sample a virtual environment for a pixel based at least on a quantity of successfully accumulated frames, renders, and/or samples that correspond to that pixel. For example, in a denoising pipeline, rather than using only the ray-traced sample(s) from a ray-traced render for that pixel, ray-traced samples from a lower resolution version of the ray-traced render may be used. In various embodiments, the resolution level (e.g., a mipmap level) may be based at least on the quantity of successfully accumulated frames and may increase with the quantity of successfully accumulated frames (e.g., until a base resolution of the ray-traced render is reached). For example, the quantity of successfully accumulated frames may be tracked per-pixel and may be used to control, or adapt, the resolution of the ray-traced render that is used in combination with or in place of temporally accumulated data for those pixels (e.g., before or after one or more spatial filtering passes). This approach may be used in combination with recurrent blurring, which may or may not adapt denoising filter size based at least on the numbers of successfully accumulated frames. It is also contemplated that these approaches may be implemented using other denoising strategies, such as post-accumulation or pre-accumulation.</p><p id="p-0043" num="0042">Aspects of the disclosure may also provide for approaches to compute one or more dimensions of an anisotropic denoising filter (e.g., filter kernel) applied to a pixel when denoising a ray-traced render. For example, rather than applying a denoising filter to a pixel that is effectively isotropic in world space, a magnitude of one or more dimensions of the filter may be based at least on an angle of a view vector that corresponds to the pixel. This may be used to cause reflections to elongate along an axis under glancing angles, resulting in a more realistic render. The disclosure provides for computing an anisotropic filter kernel that may be elongated so that a size of the filter kernel extends farther along a first axis than along a second axis to more closely reflect the footprint of a BRDF lobe in a virtual environment. In various examples, the dimension(s) may also be based at least on a direction (e.g., dominant direction) of a reflected specular lobe associated with the view vector and a corresponding surface normal. These approaches may be used in combination with recurrent blurring, post-accumulation, pre-accumulation, and/or other denoising strategies which may not necessarily include temporal accumulation.</p><p id="p-0044" num="0043">Aspects of the disclosure may also provide for using parallax between frames or renders to control the accumulation speed of temporal accumulation (e.g., for specular accumulation). For example, surface motion may be used to reproject the specular history texture from a previous frame or render to a subsequent (e.g., current) frame or render. However, when light reflects off a surface, the motion of reflections may be different that surface motion, and therefore using only surface motion may result in artifacts in rendered frames. The accuracy of surface motion for a pixel may be based, at least in part on parallax between frames, and therefore may be used to control the impact of the surface motion in the temporal accumulation.</p><p id="p-0045" num="0044">In various embodiments, parallax used to compute accumulation speed for a pixel may be based at least on an angle between view vectors that corresponds to the pixel. For example, parallax may correspond to an angle between previous and current view vectors for the same surface point(s). In at least one embodiment, parallax for a pixel may be computed as a ratio between a camera movement projection for the point(s) between renders or frames to the screen plane and a distance to the point(s). The accumulation speed may also be based at least on a viewing angle associated with the view vector of the ray-traced render, which may account for how parallax is perceived visually at different viewing angles. These approaches may be used in combination with recurrent blurring, post-accumulation, pre-accumulation, and/or other denoising strategies.</p><p id="p-0046" num="0045">Disclosed approaches may be implemented, for example, using denoisers that accept radiance (e.g., only radiance and/or radiance in addition to other information). Diffuse and specular signals may or may not be separated, in accordance with various embodiments. For example, radiance, or energy coming from a particular direction, may be separated out from materials. In some embodiments, the final color for a pixel may be based at least on a multiplication or other combination of radiance (e.g., after denoising) applied to a physically-based light model that includes light properties. As an example, for diffuse radiance, the final color may be based at least on radiance multiplied by an albedo, which may essentially represent a base color. While disclosed approaches may be compatible with both specular and diffuse radiance, some embodiments may be adapted such that they only accommodate specular or diffuse radiance.</p><p id="p-0047" num="0046">Disclosed approaches may generate ray-traced samples (e.g., of radiance) using any suitable approach to ray-tracing, such as stochastic ray-tracing. Examples of stochastic ray-tracing techniques that may be used include those that employ Monte Carlo or quasi-Monte Carlo sampling strategies. Disclosed approaches may account for such properties as surface reflections cast by various types of surfaces, such as glossy surfaces. To do so, a normal distribution function (NDF) may be employed. A non-limiting example of an NDF described herein is a GGX NDF, but other types of NDFs may be employed.</p><p id="p-0048" num="0047">Although the disclosure refers to a roughness value of a surface at a point, surface roughness may be defined using any suitable approach, which may include using one or more roughness values. For example, where the NDF captures a GGX distribution, a single roughness value may be used. However, the examples provided herein may be adapted to other microfacet distribution models as needed. In various examples, the disclosure may be used with GGX, Beckman, Blinn-Phong, or other types of NDF distributions, which may also include non-standard distributions. Where multiple roughness values are defined, a single roughness value may be determined and used as described herein by combining and/or analyzing the roughness values, one of the roughness values may be used, or the approaches may otherwise be suitably adapted.</p><p id="p-0049" num="0048">Disclosed systems may comprise or be included in one or more of a system for performing simulation operations, a system for performing simulation operations to test or validate autonomous machine applications, a system for performing deep learning operations, a system implemented using an edge device, a system incorporating one or more Virtual Machines (VMs), a system implemented at least partially in a data center, or a system implemented at least partially using cloud computing resources.</p><p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a data flow diagram illustrating an example process <b>100</b> for determining one or more filter parameters based at least on accumulated samples, in accordance with at least one embodiment of the present disclosure. The process <b>100</b> may be performed using, for example, an image renderer <b>102</b>, a sample accumulator <b>104</b>, a filter determiner <b>106</b>, and a denoiser <b>108</b>. It should be understood that this and other arrangements described herein are set forth only as examples. Other arrangements and elements (e.g., machines, interfaces, functions, orders, groupings of functions, etc.) may be used in addition to or instead of those shown, and some elements may be omitted altogether. Further, many of the elements described herein are functional entities that may be implemented as discrete or distributed components or in conjunction with other components, and in any suitable combination and location. Various functions described herein as being performed by entities may be carried out by hardware, firmware, and/or software.</p><p id="p-0051" num="0050">The image renderer <b>102</b> may be configured to render images of virtual environments (e.g., a virtual environment <b>2200</b> of <figref idref="DRAWINGS">FIG. <b>22</b></figref>). To render an image of a virtual environment, the image renderer <b>102</b> may employ the sample accumulator <b>104</b>, the filter determiner <b>106</b>, and the denoiser <b>108</b>. The sample accumulator <b>104</b> may be configured to accumulate (e.g., temporally with respect to pixels) ray-traced samples (i.e., samples generated based on ray-tracing the virtual environment). The samples accumulated by the sample accumulator <b>104</b> may be used in various ways, for example, depending on whether post-accumulation, pre-accumulation, or recurrent blurring is used to render an image. The filter determiner <b>106</b> may be configured to determine&#x2014;for a point in the virtual environment or corresponding pixel&#x2014;at least one dimension (e.g., geometry) of a filter (e.g., a spatial filter) and corresponding filter weights of the filter. The denoiser <b>108</b> may be configured to apply the filter to a pixel (e.g., at an initial pixel position of the filter) that corresponds to the point in the virtual environment to denoise an image that is representative of the virtual environment (e.g., a ray-traced render, or frame).</p><p id="p-0052" num="0051">The image renderer <b>102</b> may similarly use the sample accumulator <b>104</b>, the filter determiner <b>106</b>, and the denoiser <b>108</b> to determine filters for other points in the virtual environment and corresponding pixels in the image to produce a resultant image. Thus, multiple filters may be used to denoise lighting condition data (e.g., radiance) associated with the image to produce a resultant image (e.g., at least one filter per pixel), and those filters may all be applied in a single draw call, or pass (although multiple passes may be used for additional filters). Images described herein need not be fully rendered. As an example, one or more portions of visual content of an image and/or components thereof may be rendered (e.g., radiance). The denoiser <b>108</b> may apply filters to image data representative of the one or more portions of visual content of an image and/or components thereof to produce image data representative of a rendered frame (e.g., using any number of denoising passes). Images may be shown to illustrate noise that may be filtered in some examples of the disclosure.</p><p id="p-0053" num="0052">The image renderer <b>102</b> may use the lighting condition data for each pixel to render one or more corresponding pixels of an image. Generally, the accuracy of the lighting conditions that the image renderer computes for a pixel with respect to a light source may increase with the quantity of primary and/or secondary rays used to sample the lighting conditions. However, the computing resources used to determine the lighting conditions also may increase with the quantity of rays, which may increase render times.</p><p id="p-0054" num="0053">To preserve computing resources and to reduce render times, the quantity of rays used to sample lighting conditions may be below what is needed for reflection quality to converge to an ideal ray-traced result. This may result in the image renderer <b>102</b> generating lighting condition data that includes noisy image data, as indicated in various images throughout. For example, the lighting conditions of each pixel of a screen may be based on a single ray-traced sample (e.g., one ray-per pixel sampling) or other limited number of samples of a state of a virtual environment (e.g., comprising a single camera or eye ray and a single reflected ray per incident ray).</p><p id="p-0055" num="0054">To reduce noise in the lighting condition data for the pixels, the denoiser <b>108</b> may filter the lighting condition data (ray-traced samples, accumulated samples, etc.) any number of times in order to generate a final rendered frame. In some examples, the filter determiner <b>106</b> may determine a filter for each pixel of a screen and/or render. The image renderer <b>102</b> may apply the filters to the lighting condition data at image (e.g., pixel) locations corresponding to the associated pixels to render a resultant image. In various examples, one or more of the filters may be applied in parallel. Further, each filter may be applied in one or more multiple passes (e.g., as a separable filter). Additionally, while examples of information used to determine properties of a filter are provided, additional information may be used that results in corresponding adjustments to the properties (e.g., dimensions and/or filter weights) of the filter.</p><p id="p-0056" num="0055">Each filter described herein may comprise a filter kernel and may or may not include one or more filter directions. The filter kernel of a filter may refer to a matrix (e.g., rectangular array) that defines one or more convolutions for processing image data of an image to alter one or more characteristics of the image, such as shades and colors of the pixels of the image (e.g., through application to radiance). In some examples, a filter kernel may be applied as a separable filter in which the matrix may be represented using multiple sub-matrices, or filters, that may be applied to an image in multiple passes. When determining or computing a filter kernel for a separable filter, the disclosure contemplates that the sub-matrices may be directly computed, or may be derived from another matrix.</p><p id="p-0057" num="0056">Each element of a matrix of a filter kernel may represent a respective pixel position. One of the pixel positions of a matrix may represent an initial pixel position that corresponds to a pixel to which the filter is applied and is typically located at the center of the matrix. For example, in various figures described herein, point p may correspond to an initial pixel position (e.g., also the point <b>2216</b> and/or the pixel <b>2212</b> of <figref idref="DRAWINGS">FIG. <b>22</b></figref>). A filter direction may define the alignment of the matrix relative to the image and/or pixel to which the filter is applied. Thus, when applying a filter to a pixel, other pixels for other pixel positions of a matrix of a filter kernel may be relative to the initial pixel position based on the filter direction.</p><p id="p-0058" num="0057">Each element of a matrix of a filter kernel may comprise a filter weight for the pixel position (which may be a combination, such as a multiplication, of one or more sub-weights described herein). The matrix may be applied to an image using convolution, in which a data value for each pixel of the image that corresponds to a pixel position of the matrix may be added to data values for pixels that correspond to the local neighbors in the matrix, weighted by the filter values (also referred to as filter weights). The filter values may be configured to blur the pixels, such as by fitting a distribution(s) to a size of the filter kernel (e.g., to a width and a height), or otherwise modeling the filter weights, or values, using light-based models.</p><p id="p-0059" num="0058">The data values to which a filter is applied may correspond to lighting condition data of the pixels (e.g., before or after prior denoising). Thus, the denoiser <b>108</b> applying a matrix of a filter kernel to a pixel may cause the lighting condition data to be at least partially shared amongst the pixels that correspond to the pixel positions of the filter kernel. The sharing of the lighting condition data may mitigate noise due to sparsely sampling lighting conditions in ray-tracing when the filter kernel accurately defines which pixels may share lighting condition data (e.g., via the size of the matrix and filter direction(s)) and how much lighting condition data may be shared (e.g., via the filter weights). As such, where the size, filter direction(s), and/or filter weights of the filter kernel do not accurately reflect lighting conditions of a virtual environment, the filter kernel may cause over-blurring and/or unrealistic blurring, resulting in unrealistic lighting conditions being represented in a rendered image.</p><p id="p-0060" num="0059">The sample accumulator <b>104</b> may be configured to accumulate (e.g., temporally with respect to pixels) ray-traced samples (i.e., samples generated based on ray-tracing the virtual environment). Denoising approaches herein may employ the sample accumulator <b>104</b> to store accumulated samples in a history buffer and/or a texture (e.g., with respect to particular pixels). For example, the image renderer <b>102</b> may iteratively generate ray-traced renders and in each iteration, the sample accumulator <b>104</b> may accumulate one or more samples to a history buffer. At times, the sample accumulator <b>104</b> may reject one or more accumulated samples for use in a denoising iteration for a rendered frame (and/or subsequent iterations). For example, accumulated samples may be rejected for pixels and/or ray-traced samples from an input signal (e.g., a ray-traced render) that the system determines are not present in the history buffer (e.g., using projection to map ray-traced samples to accumulated samples).</p><p id="p-0061" num="0060">By way of example and not limitation, when one or more pixels and/or ray-traced samples correspond to an occlusion event or a disocclusion event (e.g., as determined by the sample accumulator <b>104</b>), accumulated samples being maintained by the sample accumulator <b>104</b> in the history buffer may be rejected and not used for those pixels and/or ray-traced samples. Rejections may occur as a camera or perspective used to render a virtual environment moves and/or an object in the virtual environment moves, causing occlusion and/or disocclusion events. In various embodiments, the sample accumulator <b>104</b> may analyze render data from ray tracing to determine such events, and if an event is identified (e.g., a disocclusion is found) for a pixel(s) and/or accumulated sample(s), a quantity of successfully accumulated frames may be reset (e.g., to zero) and accumulation may resume from scratch (e.g., for that accumulated sample(s)).</p><p id="p-0062" num="0061">As some examples, the sample accumulator <b>104</b> may determine a disocclusion for a ray-traced sample or pixel based at least on a comparison between a corresponding current depth value (e.g., a z-value) and a corresponding previous depth value (e.g., determined using reprojection). Additionally or alternatively, the disocclusion may be determined for a ray-traced sample or pixel based at least on comparing plane distances. In various embodiments other suitable approaches may be employed.</p><p id="p-0063" num="0062">The image renderer <b>102</b>, the sample accumulator <b>104</b>, the filter determiner <b>106</b>, and the denoiser <b>108</b>, may be configured differently depending upon the denoising strategy or strategies being employed. For post-accumulation, the sample accumulator <b>104</b> may accumulate samples after the denoiser <b>108</b> executes a blur or denoising pass on the sample. For pre-accumulation the sample accumulator <b>104</b> may accumulate samples before a blur or denoising pass is performed on the accumulated samples. An input signal to post-accumulation or pre-accumulation may include a ray-traced render and/or ray traced samples of a virtual environment (optionally after pre-processing, such as pre-blur). The input signal may have a very high frequency, particularly where it is sparsely sampled (e.g., with only one ray per-pixel). The input may also be referred to as ray-tracing output and/or denoising input. After blurring, the input signal may be converted into a lower frequency signal. In post-accumulation, the high frequency input may be blurred, then used to accumulate to a low frequency texture, and in pre-accumulation, the opposite may occur.</p><p id="p-0064" num="0063">Recurrent blur or blurring may mix signals with wider frequencies. After the denoiser <b>108</b> performs blurring, the signal may have a lower frequency and may be put into a feedback loop (optionally after additional processing and/or blurring) as an input to the next frame and/or render to be accumulated by the sample accumulator <b>104</b> with a subsequent high frequency noisy input. It may also be referred to as a blur of a ray-traced render with a background, and that background may be accumulated with the currently incoming input signal from a ray-tracing process performed by the image renderer <b>102</b>. As recurrent blur feeds back a blurred output (e.g., the blurred accumulated samples, or background) that is then used for blurring, this approach may be prone to over-blurring. The process <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> may be employed to reduce or prevent over-blurring, which may result in clearer output frames.</p><p id="p-0065" num="0064">Adaptive Blur Radius</p><p id="p-0066" num="0065">At a high level, the process <b>100</b> may include the image renderer <b>102</b> generating one or more outputs, such as a ray-traced render of a virtual environment (e.g., a virtual environment <b>2200</b> of <figref idref="DRAWINGS">FIG. <b>22</b></figref>). The process <b>100</b> may also include the sample accumulator <b>104</b> receiving one or more inputs, such as the ray-traced render and a denoised accumulated render from a prior iteration of the process <b>100</b>, accumulating the input(s), and generating one or more outputs, such as an accumulated render. The process <b>100</b> may further include the filter determiner <b>106</b> receiving one or more inputs, such as counts of quantities of samples accumulated to pixels of the accumulated render, and generating one or more outputs, such as one or more dimensions of one or more filters. The process <b>100</b> may further include the denoiser <b>108</b> receiving one or more inputs, such as the one or more dimensions, and generating one or more outputs, such as the accumulated render denoised using the one or more filters.</p><p id="p-0067" num="0066">As indicated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the process <b>100</b> may leverage recurrent blurring to effectively redistribute spatial sampling in time due to its recursive nature. For example, after the denoiser <b>108</b> performs blurring of accumulated samples, the signal may have a lower frequency and may be put into a feedback loop <b>140</b> (optionally after additional processing and/or blurring) as an input to the sample accumulator <b>104</b> for the next frame and/or render to be accumulated by the sample accumulator <b>104</b> with a subsequent high frequency noisy input from the image renderer <b>102</b>. The process <b>100</b> may leverage the sample accumulator <b>104</b> to increase the effective sample count of the ray-traced render generated by the image renderer <b>102</b>. As a non-limiting example, if the Frames Per Second (FPS) of frames rendered by the image renderer <b>102</b> is at 30, and 8 ray-traced samples per pixel are employed to ray-trace a virtual environment, accumulation may effectively result in 240 ray-traced samples per second as a result of blurring the same texture (e.g., in the history buffer) repeatedly.</p><p id="p-0068" num="0067">To reduce or eliminate over-blurring, which may result from recurrent blurs, aspects of the disclosure may provide for recurrent blurring in order to render frames of a virtual environment where the radius (or more generally dimension or size) of at least one denoising filter for a pixel(s) is determined by the filter determiner <b>106</b> based at least on a quantity of successfully accumulated frames that correspond to that pixel (e.g., temporally accumulated samples in the history buffer). For example, the sample accumulator <b>104</b> may track the quantity of successfully accumulated frames per-pixel and may be used to control, or adapt, the blur dimensions of denoising filters applied by the denoiser <b>108</b> to those pixels (e.g., in one or more spatial filtering passes).</p><p id="p-0069" num="0068">Disclosed approaches may reduce the size of a blur filter for a pixel based on the contribution of accumulation to solving the final lighting integral for that pixel. This may allow filter sizes to be reduced as the contribution (e.g., quantity of samples accumulated to the history buffer) increases over a number of denoising iterations. If the history buffer gets rejected for a pixel, the contribution of accumulation for that pixel may be eliminated for at least one iteration and the size for a corresponding blur filter may be increased (e.g., when not considering other potential factors that may be used to impact the size of the filter).</p><p id="p-0070" num="0069">In at least one embodiment, the sample accumulator <b>104</b> may increment one or more counters to track the quantity of successfully accumulated frames and/or samples (e.g., per-pixel). For example, a counter may be incremented per-iteration of the denoising iteration, per-accumulation, per-frame or render (e.g., once per successful accumulation for a pixel). If the sample accumulator <b>104</b> rejects one or more accumulated samples for use in a denoising iteration, the count may be reset (e.g., to zero) and accumulation may resume from scratch (e.g., for that accumulated sample(s)). For example, the count may be reset for pixels and/or ray-traced samples from an input signal (e.g., a ray-traced render) that the sample accumulator <b>104</b> determines are not present in the history buffer (e.g., using projection to map ray-traced samples to accumulated samples).</p><p id="p-0071" num="0070">In <figref idref="DRAWINGS">FIG. <b>1</b></figref>, image <b>150</b> provides a visualization of quantities of successfully accumulated frames for pixels. A darker pixel may correspond to fewer accumulated samples or frames for that pixel, and lighter pixels may correspond to more accumulated samples or frames for that pixel. For example, black pixels may have a minimum count indicating no successfully accumulated frames (e.g., due to an occlusion event or a disocclusion event) and white pixels may have a maximum count indicating a maximum number of accumulated frames. By way of example and not limitation, the count may range from 0 to 32.</p><p id="p-0072" num="0071">In at least one embodiment, the count may be used to compute an adjustment factor that is then used to compute one or more dimensions of a filter (e.g., blur radius). The adjustment factor may correspond to, for example, a computed result of an equation or portion thereof that uses the count as a variable. The adjustment factor may be proportional to or otherwise related to the quantity of accumulated frames and/or samples and may be computed per-pixel. In some embodiments, the quantity of accumulated frames that correspond to a sample(s) may be constrained to a maximum quantity of accumulated frames (e.g., 32). This number may be controlled by an end user and/or software and/or may be dynamic. Reducing the quantity of accumulated frames may reduce temporal lag. In at least one embodiment, the radius of a filter for a pixel may be largest upon a reset (e.g., count=0) and smallest upon reaching a maximum quantity of accumulated frames (e.g., count=32). By way of example, the radius scale for a pixel may be computed using Equation (1):</p><p id="p-0073" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>1/(1+newCount),&#x2003;&#x2003;(1)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0074" num="0072">where newCount may refer to the updated count for the pixel in a denoising iteration after accumulation.</p><p id="p-0075" num="0073">Referring now to <figref idref="DRAWINGS">FIGS. <b>2</b>A and <b>2</b>B</figref>, <figref idref="DRAWINGS">FIG. <b>2</b>A</figref> shows a render <b>200</b>A of a virtual environment which may be generated using the example process <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> after two frames of accumulation, in accordance with at least one embodiment of the present disclosure. <figref idref="DRAWINGS">FIG. <b>2</b>B</figref> shows a render <b>200</b>B of a virtual environment which may be generated using the example process <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> after thirty-two frames of accumulation, in accordance with at least one embodiment of the present disclosure. While counts and filter sizes may vary per-pixel, the renders <b>200</b>A and <b>200</b>B are examples where each pixel has the same count. As can be seen, with 2 frames of accumulation, the render <b>200</b>A has less detail than the render <b>200</b>B due to the denoiser <b>108</b> using filters with larger blur radii when blurring render data to generate the render <b>200</b>A.</p><p id="p-0076" num="0074">Now referring to <figref idref="DRAWINGS">FIGS. <b>3</b>-<b>5</b></figref>, each block of methods <b>300</b>, <b>400</b>, and <b>500</b>, and other methods described herein, comprises a computing process that may be performed using any combination of hardware, firmware, and/or software. For instance, various functions may be carried out by a processor executing instructions stored in memory. The methods may also be embodied as computer-usable instructions stored on computer storage media. The methods may be provided by a standalone application, a service or hosted service (standalone or in combination with another hosted service), or a plug-in to another product, to name a few. In addition, the methods are described, by way of example, with respect to the systems components of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. However, these methods may additionally or alternatively be executed by any one system, or any combination of systems, including, but not limited to, those described herein.</p><p id="p-0077" num="0075"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flow diagram showing an example of a method <b>300</b> for determining one or more parameters of a filter based at least on a quantity of rendered frames of a virtual environment, in accordance with at least one embodiment of the present disclosure. The method <b>300</b>, at block B<b>302</b>, includes accumulating a ray-traced sample with at least one sample generated based at least on denoising a first accumulated sample to generate a second accumulated sample. For example, the sample accumulator <b>104</b> may accumulate a ray-traced sample of a virtual environment from the image renderer <b>102</b> with at least one sample generated based at least on the denoiser <b>108</b> denoising a first accumulated sample of the virtual environment. The at least one sample may be provided by the feedback loop <b>140</b> from a prior iteration of the process <b>100</b> and the accumulating may generate a second accumulated sample to be provided to the denoiser <b>108</b> in the current iteration of the process <b>100</b>.</p><p id="p-0078" num="0076">At block B<b>304</b>, the method <b>300</b> includes determining one or more parameters of a filter based at least on a quantity of rendered frames that correspond to the second accumulated sample. For example, the filter determiner <b>106</b> may determine a blur radius of a filter based at least on a quantity of rendered frames (or samples) that correspond to the second accumulated sample.</p><p id="p-0079" num="0077">At block B<b>306</b>, the method <b>300</b> includes generating a rendered frame based at least on blurring the second accumulated samples using the filter. For example, a rendered frame may be generated based at least on the denoiser <b>108</b> blurring the second accumulated sample using the filter having the blur radius.</p><p id="p-0080" num="0078">Referring now to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, <figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flow diagram showing an example of a method <b>400</b> for iteratively determining one or more parameters of a filter based at least on a quantity of renders of a virtual environment that correspond to an accumulated sample, in accordance with at least one embodiment of the present disclosure.</p><p id="p-0081" num="0079">The method <b>400</b>, at block B<b>402</b>, includes accumulating ray-traced samples with samples corresponding to samples of a previous iteration to generate accumulated samples. For example, the sample accumulator <b>104</b> may accumulate ray-traced samples of the virtual environment from the image renderer <b>102</b> with samples of a previous iteration of the process <b>100</b> (provided by the feedback loop <b>140</b>) to generate accumulated samples for the current iteration of the process <b>100</b>.</p><p id="p-0082" num="0080">The method <b>400</b>, at block B<b>404</b> includes determining one or more parameters of a filter based at least on a quantity of renders that correspond to at least one sample of the accumulated samples. For example, the filter determiner <b>106</b> may determine a blur radius of a filter based at least on a quantity of the renders that correspond to at least one sample of the accumulated samples.</p><p id="p-0083" num="0081">The method <b>400</b>, at block B<b>406</b> includes generating the samples for a subsequent iteration based at least on blurring the accumulated samples using the filter. For example, the samples for a subsequent iteration may be generated based at least on the denoiser <b>108</b> blurring the accumulated samples using the filter having the blur radius. In some examples, the samples may comprise an output of the denoiser <b>108</b>. However, in at least one embodiment further processing may be performed after the blurring by the denoiser <b>108</b> to generate the samples.</p><p id="p-0084" num="0082">Referring now to <figref idref="DRAWINGS">FIG. <b>5</b></figref>, <figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flow diagram showing an example of a method <b>500</b> for determining one or more parameters of a filter based at least on a quantity of renders of a virtual environment that correspond to at least one sample of an accumulated render, in accordance with at least one embodiment of the present disclosure.</p><p id="p-0085" num="0083">The method <b>500</b>, at block B<b>502</b> includes generating an accumulated render that corresponds to a ray-traced render accumulated with a denoised accumulated render. For example, the sample accumulator <b>104</b> may generate an accumulated render that corresponds to a ray-traced render of a virtual environment from the image renderer <b>102</b> accumulated with a denoised accumulated render of the virtual environment generated using the denoiser <b>108</b> from a previous iteration of the process <b>100</b>.</p><p id="p-0086" num="0084">At block B<b>504</b>, the method <b>500</b> includes determining one or more parameters of a filter based at least on a quantity of renders that correspond to at least one sample of the accumulated render. For example, the filter determiner <b>106</b> may determine a blur radius of a filter based at least on a quantity of renders that correspond to at least one sample of the accumulated render.</p><p id="p-0087" num="0085">At block B<b>506</b>, the method <b>500</b> includes generating a rendered frame based at least on blurring the accumulated render using the filter. For example, a rendered frame may be generated based at least on the denoiser <b>108</b> blurring the accumulated render using the filter having the blur radius.</p><p id="p-0088" num="0086">Hierarchical History Reconstruction</p><p id="p-0089" num="0087">Aspects of the disclosure may also provide for a hierarchical approach to account for rejections of accumulated samples with respect to pixels. Typically, when a temporally accumulated sample for a pixel gets rejected by the sample accumulator <b>104</b>, &#x2014;due to a disocclusion of the pixel in ray-tracing performed by the image renderer <b>102</b>, for example&#x2014;temporal accumulation for that pixel is reset. Thus, the effective sample count for the pixel may be reduced for a number of frames as temporal data is re-accumulated by the sample accumulator <b>104</b>. This may result in artifacts in rendered frames, particularly when the number of rays per-pixel used by the image renderer <b>102</b> to sample a virtual environment for the pixel is low.</p><p id="p-0090" num="0088">Disclosed approaches may supplement the effective number of rays per-pixel used to sample a virtual environment for a pixel based at least on a quantity of successfully accumulated frames that correspond to that pixel. Referring now to <figref idref="DRAWINGS">FIGS. <b>6</b>A and <b>6</b>B</figref>, <figref idref="DRAWINGS">FIG. <b>6</b>B</figref> is a flow diagram showing a process <b>600</b>A for denoising using recurrent blur that includes generating an augmented sample based at least on a quantity of renders of a virtual environment that correspond to a ray-traced sample in an accumulated render, in accordance with at least one embodiment of the present disclosure. <figref idref="DRAWINGS">FIG. <b>6</b>B</figref> is a flow diagram showing a process <b>600</b>B for denoising using pre-accumulation that includes generating an augmented sample based at least on a quantity of renders of a virtual environment that correspond to a ray-traced sample in an accumulated render, in accordance with at least one embodiment of the present disclosure.</p><p id="p-0091" num="0089">In the process <b>600</b>A and the process <b>600</b>B, rather than the sample accumulator <b>104</b> using only the ray-traced sample(s) from a ray-traced render for accumulating for a pixel, ray-traced samples from a lower resolution version(s) of the ray-traced render may be used. At a high level, the process <b>600</b>A may include the image renderer <b>102</b> generating one or more outputs, such as a ray-traced render of a virtual environment (e.g., a virtual environment <b>2200</b> of <figref idref="DRAWINGS">FIG. <b>22</b></figref>). The process <b>100</b> may also include the sample accumulator <b>104</b> receiving one or more inputs, such as the ray-traced render and a denoised accumulated render from a prior iteration of the process <b>600</b>A, accumulating the input(s), and generating one or more outputs, such as an accumulated render.</p><p id="p-0092" num="0090">The process <b>600</b>A may further include a resolution manager receiving one or more inputs, such as the accumulated render and counts of quantities of samples accumulated to pixels of the accumulated render, and generating one or more outputs, such as one or more samples augmented with at least one sample that corresponds to a resolution level of the ray-traced render. The process <b>100</b> may further include the filter determiner <b>106</b> receiving one or more inputs, and generating one or more outputs, such as one or more parameters of one or more filters. The process <b>100</b> may further include the denoiser <b>108</b> receiving one or more inputs, such as the one or more parameters, and generating one or more outputs, such as the accumulated render denoised using the one or more filters.</p><p id="p-0093" num="0091">The process <b>600</b>B may be similar to the process <b>600</b>A, except that the sample accumulator <b>104</b> may accumulate samples before a blur or denoising pass is performed on the accumulated samples.</p><p id="p-0094" num="0092">Due to a rejection of accumulated data, a history buffer may not include any usable data for a pixel. Using disclosed approaches, the resolution manager <b>602</b> may use a coarse resolution (e.g., highest mipmap level) for the pixel in an iteration of the process <b>600</b>A or <b>600</b>B. The resolution manager <b>602</b> may incrementally increase the resolution each iteration in correspondence with the quantity of successfully accumulated frames and/or samples for the pixel (e.g., until a base level resolution is reached). Initially, the resolution manager <b>602</b> may use the corresponding composite sample for the pixel (e.g., along with the non-rejected accumulated samples for other pixels) in the history buffer so as to augment the accumulated data in a denoising iteration. After successful accumulation for the pixel, the resolution manager <b>602</b> may use the corresponding composite sample for the pixel in combination corresponding accumulated data in the history buffer for that pixel, so as to augment the accumulated data in a denoising iteration.</p><p id="p-0095" num="0093">In various embodiments, the resolution manager <b>602</b> may compute the resolution level (e.g., a mipmap level) for one or more samples and/or pixels may be based at least on the quantity of successfully accumulated frames and may increase with the quantity of successfully accumulated frames or samples (e.g., until a base resolution of the ray-traced render is reached). For example, as described herein, the sample accumulator <b>104</b> may track the quantity of successfully accumulated frames or samples (e.g., per-pixel), which the resolution manager <b>602</b> may use to control, or adapt, the resolution of the ray-traced render from the image renderer <b>102</b>. In at least one embodiment, the resolution manager <b>602</b> may additionally or alternatively use the surface roughness of a pixel to determine the resolution level for the pixel. For example, the resolution level may be computed such that the resolution decreases with increased surface roughness.</p><p id="p-0096" num="0094">The resolution manager <b>602</b> may use one or more portions of a version of the ray-traced render at the resolution in combination with or in place of temporally accumulated data from the sample accumulator <b>104</b> (e.g., before or after one or more spatial filtering passes). This approach may be used in the recurrent blurring approach of the process <b>600</b>A, which may or may not adapt denoising filter size based at least on the numbers of successfully accumulated frames (e.g., according to the process <b>100</b>). It is also contemplated that these approaches may be implemented using other denoising strategies, such as post-accumulation or pre-accumulation. For example, the process <b>600</b>B shows these approaches implemented for pre-accumulation.</p><p id="p-0097" num="0095">In at least one embodiment, versions of a render at different resolution levels may be implemented using one or more mipmaps, for example, as described with respect to <figref idref="DRAWINGS">FIGS. <b>7</b>A and <b>7</b>B</figref>. Referring now to <figref idref="DRAWINGS">FIGS. <b>7</b>A and <b>7</b>B</figref>, <figref idref="DRAWINGS">FIG. <b>7</b>A</figref> shows an example of a render of a virtual environment at different mipmap levels, in accordance with at least one embodiment of the present disclosure. <figref idref="DRAWINGS">FIG. <b>7</b>B</figref> shows examples of mipmap levels MIP 0 through MIP 4 and an alignment between neighboring mipmap levels, in accordance with at least one embodiment of the present disclosure.</p><p id="p-0098" num="0096">For example, In <figref idref="DRAWINGS">FIGS. <b>7</b>A and <b>7</b>B</figref>, MIP 0 may refer to a base resolution of a ray-traced render, which may be output by the image renderer <b>102</b>. As the mip level increases, the resolution of the render decreases. Stack <b>700</b> of <figref idref="DRAWINGS">FIG. <b>7</b>B</figref> illustrates how the corresponding mipmaps can be visualized as a stack of overlaid images. An example of an alignment <b>702</b> between neighboring resolutions/mip levels is shown on the right. Any number of mipmap and/or resolution levels may be used in accordance with various embodiments of the disclosure. In at least one embodiment, mipmaps may be generated for radiance and viewZ (which may capture Z-coordinates of pixels in world-space), where a viewZ delta may be used to compute weights during upsampling.</p><p id="p-0099" num="0097">In various embodiments, the resolution manager <b>602</b> may use a count of successfully accumulated frames or samples for a pixel(s) to compute an adjustment factor that is then used to compute the resolution level for that pixel(s). The count may be the same count described with respect to the process <b>100</b> and tracked by the sample accumulator <b>104</b>, or may be a different count. The adjustment factor may correspond to, for example, a computed result of an equation or portion thereof that uses the count as a variable. The adjustment factor may be proportional to or otherwise related to the quantity of accumulated frames and/or samples and may be computed per-pixel.</p><p id="p-0100" num="0098">By way of example, the mip level for a pixel may be computed using Equation (2):</p><p id="p-0101" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>MaxLevel*roughness*max(1&#x2212;newCount/<i>N,</i>0),&#x2003;&#x2003;(2)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0102" num="0099">where roughness may refer to a surface roughness value for a pixel, MaxLevel may refer to the mipmap level of the lowest resolution of the render (e.g., MIP 4 in <figref idref="DRAWINGS">FIG. <b>7</b>A</figref>), and N may refer to a number of frames with active history reconstruction.</p><p id="p-0103" num="0100">Using disclosed approaches, the resolution manager <b>602</b> may trade off resolution for rays per-pixel to increase the effective sample count for a pixel(s). Given, for example and without limitation, a base resolution at MIP or resolution level 0 that corresponds to one Ray Per-Pixel (RPP), at MIP 1 the resolution manager <b>602</b> may use a 2&#xd7;2 pixel region of the render for a pixel to increase the effective sample count to 4 RPP. At MIP 2 the resolution manager <b>602</b> may use a 4&#xd7;4 pixel region of the render for the pixel to increase the effective sample count to 16 RPP. At MIP 3 the resolution manager <b>602</b> may use an 8&#xd7;8 pixel region of the render for the pixel to increase the effective sample count to 64 RPP. Further, at MIP 4 the resolution manager <b>602</b> may use a 16&#xd7;16 pixel region of the render for the pixel to increase the effective sample count to 256 RPP.</p><p id="p-0104" num="0101">The resolution manager <b>602</b> may downsample the ray-traced render for an iteration (e.g., by shrinking it in all directions dividing it by 2) to generate any number of different resolutions and corresponding samples may be applied to a base resolution pixel in the denoising iteration. In some embodiments, those samples may be upsampled (e.g., using bilinear, bilateral, or high order upsampling) or otherwise combined to generate a corresponding composite sample for the base resolution sample.</p><p id="p-0105" num="0102">Referring now to <figref idref="DRAWINGS">FIGS. <b>8</b>A and <b>8</b>B</figref>, <figref idref="DRAWINGS">FIG. <b>8</b>A</figref> shows a render <b>800</b>A of a virtual environment which may be generated without augmenting accumulated samples, in accordance with at least one embodiment of the present disclosure. <figref idref="DRAWINGS">FIG. <b>8</b>B</figref> shows a render <b>800</b>B of the virtual environment which may be generated with augmenting accumulated samples, in accordance with at least one embodiment of the present disclosure. For example, the render <b>800</b>B may be generated using the process <b>600</b>A or <b>600</b>B.</p><p id="p-0106" num="0103">Referring now to <figref idref="DRAWINGS">FIG. <b>9</b></figref>, <figref idref="DRAWINGS">FIG. <b>9</b></figref> is a flow diagram showing an example of a method <b>900</b> for augmenting an accumulated render based at least on a quantity of renders that correspond to a ray-traced sample of a virtual environment in an accumulated render of the virtual environment, in accordance with at least one embodiment of the present disclosure. The method <b>900</b>, at block B<b>902</b> includes computing a resolution level based at least on a quantity of renders that correspond to a pixel of a ray-traced render in an accumulated render. For example, the resolution manager <b>602</b> may compute a resolution level (e.g., a mipmap level) based at least on a quantity of renders that correspond to a ray-traced sample of a virtual environment in an accumulated render of the virtual environment. The accumulated render may be generated using the sample accumulator <b>104</b>.</p><p id="p-0107" num="0104">At block B<b>904</b>, the method <b>900</b> includes generating a rendered frame based at least on augmenting the accumulated render with at least one sample that corresponds to the resolution level. For example, the resolution manager <b>602</b> may generate an augmented sample based at least on augmenting the accumulated render with at least one sample that corresponds to the resolution level and the ray-traced sample.</p><p id="p-0108" num="0105">At block B<b>906</b>, the method <b>900</b> includes generating a rendered frame based at least on blurring data corresponding to the augmented sample using one or more filters. For example, a rendered frame may be generated based at least on the denoiser <b>108</b> blurring data corresponding to the augmented sample using one or more filters.</p><p id="p-0109" num="0106">Referring now to <figref idref="DRAWINGS">FIG. <b>10</b></figref>, <figref idref="DRAWINGS">FIG. <b>10</b></figref> is a flow diagram showing an example of a method <b>1000</b> for iteratively augmenting a set of accumulated samples based at least on a quantity of renders that correspond to at least one ray-traced sample of a virtual environment, in accordance with at least one embodiment of the present disclosure. The method <b>1000</b>, at block B<b>1002</b>, includes accumulating ray-traced samples with samples of a previous iteration to generate a set of accumulated samples. For example, the sample accumulator <b>104</b> may accumulate ray-traced samples of the virtual environment from the image renderer <b>102</b> with samples of a previous iteration of a plurality of iterations, where the accumulating is to generate a set of accumulated samples.</p><p id="p-0110" num="0107">At block B<b>1004</b>, the method <b>1000</b> includes computing a resolution level based at least on a quantity of renders that correspond to at least one sample of the ray-traced samples. For example, the resolution manager <b>602</b> may compute a resolution level based at least on a quantity of the renders that correspond to at least one sample of the ray-traced samples in the set of accumulated samples.</p><p id="p-0111" num="0108">At block B<b>1006</b>, the method <b>1000</b> includes augmenting the set of accumulated samples with one or more samples that correspond to the resolution level. For example, the resolution manager <b>602</b> may augment the set of accumulated samples with one or more samples that correspond to the resolution level.</p><p id="p-0112" num="0109">At block B<b>1008</b>, the method <b>1000</b> includes generating a render based at least on blurring data corresponding to the set of augmented samples using one or more filters. For example, a render may be generated based at least on the denoiser <b>108</b> blurring data corresponding to the set of accumulated samples that are augmented with the one or more samples using one or more filters.</p><p id="p-0113" num="0110">Referring now to <figref idref="DRAWINGS">FIG. <b>11</b></figref>, <figref idref="DRAWINGS">FIG. <b>11</b></figref> is a flow diagram showing an example of a method <b>1100</b> for augmenting an accumulated render based at least on a quantity of renders that correspond to a pixel in an accumulated render of the virtual environment, in accordance with at least one embodiment of the present disclosure. The method <b>1100</b>, at block B<b>1102</b>, includes computing a resolution level based at least on a quantity of renders that correspond to a pixel of a ray-traced render in an accumulated render. For example, the resolution manager <b>602</b> may compute a resolution level based at least on a quantity of renders that correspond to a pixel of a ray-traced render of a virtual environment generated using the image renderer <b>102</b> in an accumulated render of the virtual environment generated using the sample accumulator <b>104</b>.</p><p id="p-0114" num="0111">At block B<b>1104</b>, the method <b>1100</b> includes augmenting the accumulated render with at least one sample that corresponds to the pixel and the resolution level. For example, the resolution manager <b>602</b> may augment the accumulated render with at least one sample that corresponds to the pixel and the resolution level to generate an augmented accumulated render.</p><p id="p-0115" num="0112">At block B<b>1106</b>, the method <b>1100</b> includes generating a rendered frame based at least on blurring data corresponding to the augmented accumulated render using one or more filters. For example, a rendered frame may be generated based at least on the denoiser <b>108</b> blurring data corresponding to the accumulated render that is augmented with the at least one sample using one or more filters.</p><p id="p-0116" num="0113">Referring now to <figref idref="DRAWINGS">FIG. <b>12</b></figref>, <figref idref="DRAWINGS">FIG. <b>12</b></figref> is a data flow diagram illustrating an example process <b>1200</b> for denoising a render of a virtual environment, in accordance with at least one embodiment of the present disclosure. The process <b>1200</b> may implement an adaptive blur radius in accordance with the process <b>100</b> and history reconstruction in accordance with the process <b>600</b>A.</p><p id="p-0117" num="0114">The process <b>1200</b> may be used for diffuse or specular input signals and uses recurrent blurring, by way of example. Further, as shown, pre blur <b>1202</b> and post blur <b>1210</b> may be included in the process <b>1200</b>. By way of example, the pre blur <b>1202</b>, the blur <b>1208</b>, and the post blur <b>1210</b> may be based on Poisson-disk sampling, although other types may be used. The pre blur <b>1202</b> may use a constant filter size and the post blur <b>1210</b> may be adaptive. The post blur <b>1210</b> may also factor is the intensity of input signal after accumulation and the intensity of the signal after blurring. The filter determiner <b>106</b> may compare these values and the size (e.g., radius) of the filters may be based at least on the difference between these values. For example, the size may increase as a function of the difference, as a larger difference may indicate a lack of convergence.</p><p id="p-0118" num="0115">The process <b>1200</b> may include one or more passes pre blur <b>1202</b> being applied to a ray-traced render of a virtual environment (e.g., from the image renderer <b>102</b>). The pre blur may, for example, use a constant or adaptive radius filter and may be used to address outliers in the samples of the ray-traced render. The process <b>1200</b> may also include accumulation <b>1204</b> (e.g., using the sample accumulator <b>104</b>) of the pre blurred ray-traced render with a denoised accumulated render from a feedback loop <b>1240</b>. The accumulation <b>1204</b> may be performed, by way of example, using linear weights and may accumulate up to 32 or some other maximum number of frames.</p><p id="p-0119" num="0116">The process <b>1200</b> may further include history reconstruction (e.g., using the resolution manager <b>602</b>) and radius determination <b>1206</b> (e.g., using the filter determiner <b>106</b>) from the accumulated data. This may include generation of the mipmaps and using the mipmaps to augment the accumulated samples (e.g., in the history buffer used for accumulation). In at least one embodiment, the mipmaps may be generated in a single pass in shared memory, for example, using averaging. The history reconstruction may be performed for each pixel that has discarded or otherwise insufficient accumulated sample data.</p><p id="p-0120" num="0117">The process may also include blur <b>1202</b> of the augmented accumulated samples (e.g., using the denoiser <b>108</b>). The blur <b>1202</b> may be based at least on the radius determination using one or more filters having corresponding radii. The process <b>1200</b> may additionally include post blur <b>1210</b> of the blurred and augmented accumulated samples (e.g., using the denoiser <b>108</b>). The post blur <b>1210</b> may use the same radii determined for the blur <b>1202</b> or different radii. In at least one embodiment, a radius for a filter used by the post blur <b>1210</b> may be based at least on a quantity of rendered frames that correspond to a pixel and an intensity delta between the pixel and reprojected history from the history reconstruction. The process may also include temporal stabilization <b>1212</b> of the post blurred render, which may leverage Temporal Anti-Aliasing (TAA) with potentially wider variance clamping.</p><p id="p-0121" num="0118">Denoising Filter Dimensions</p><p id="p-0122" num="0119">Referring not to <figref idref="DRAWINGS">FIGS. <b>13</b>A and <b>13</b>B</figref>, <figref idref="DRAWINGS">FIG. <b>13</b>A</figref> is a diagram illustrating an example of determining a blur plane <b>1302</b>A for mirror reflections, in accordance with at least one embodiment of the present disclosure. <figref idref="DRAWINGS">FIG. <b>13</b>B</figref> is a diagram illustrating an example of determining a blur plane <b>1302</b>B for diffuse signals, in accordance with at least one embodiment of the present disclosure.</p><p id="p-0123" num="0120">The approaches illustrated in <figref idref="DRAWINGS">FIGS. <b>13</b>A and <b>13</b>B</figref> may be used by the filter determiner <b>106</b> in at least one embodiment to determine blur planes for filters in world space. However, other approaches may be employed. As diffuse signals may cover the entire hemisphere of the sampling space, the blur plane <b>1302</b>B for a point p may be in a tangent plane constructed around the plane normal N. For specular signals, at least for almost mirror reflections, the blur plane <b>1302</b>A may be constructed in a plane perpendicular to the view vector V, where vector R in <figref idref="DRAWINGS">FIG. <b>13</b>A</figref> may be a reflection of the inverse of the view vector V across the normal vector N of a surface <b>1306</b>. This may in practice be very close to screen space, as illustrated by a screen plane <b>1304</b> in <figref idref="DRAWINGS">FIG. <b>13</b>A</figref>.</p><p id="p-0124" num="0121">In some embodiments, the blur plane may be based, at least in part, on a direction (e.g., dominant direction) of a specular lobe associated with the view vector and a corresponding surface normal, as further described with respect to <figref idref="DRAWINGS">FIG. <b>14</b>A</figref>. <figref idref="DRAWINGS">FIG. <b>14</b>A</figref> is a diagram illustrating an example of determining a blur plane <b>1402</b> for denoising ray-traced samples, in accordance with at least one embodiment of the present disclosure. The approach of <figref idref="DRAWINGS">FIG. <b>14</b>A</figref> may, for example, for specular and/or diffuse signals.</p><p id="p-0125" num="0122"><figref idref="DRAWINGS">FIG. <b>14</b>A</figref> illustrates that the blur plane <b>1402</b> for a filter for point p (e.g., the kernel center in world space) may be based, at least in part, on a GGX NDF dominant direction <b>1404</b>, although other types of NDFs may be employed in various embodiments. As an example, a direction of a specular lobe may be computed by trimming out values (e.g., random values) in NDF importance sampling, such as a Visible Normal Distribution Function (VNDF). The direction may be used, for example, to provide a generalized approach (e.g., compatible with both specular and diffuse signals) for defining the blur plane <b>1402</b> perpendicular to the view vector, and more particularly perpendicular to the reflected specular direction (e.g., dominant direction) <b>1406</b>. In the case of a diffuse signal, the reflected specular direction <b>1406</b> may be the normal vector N if surface roughness of the surface <b>1306</b> is 0, and the reflected specular direction <b>1406</b> (a vector) around the normal is still the same normal vector N, so constructing a tangent plane for this case corresponds to a diffuse signal. The same approach may be used for almost mirror reflections.</p><p id="p-0126" num="0123">Aspects of the disclosure may provide approaches to compute one or more dimensions of an anisotropic denoising filter (e.g., filter kernel) applied to a pixel when denoising a ray-traced render. For example, rather than applying a denoising filter to a pixel that is effectively isotropic in world space, the filter determiner <b>106</b> may determine a magnitude of one or more dimensions of the filter based at least on an angle of a view vector that corresponds to the pixel. This may be used to cause reflections to elongate along an axis under glancing angles, resulting in a more realistic render. The disclosure provides for computing an anisotropic filter kernel that may be elongated so that a size of the filter kernel extends farther along a first axis than along a second axis to more closely reflect the footprint of a BRDF lobe in a virtual environment. In various examples, the dimension(s) may also be based at least on the direction (e.g., dominant direction) of a reflected specular lobe associated with the view vector and a corresponding surface normal.</p><p id="p-0127" num="0124">Referring now to <figref idref="DRAWINGS">FIG. <b>14</b>B</figref>, <figref idref="DRAWINGS">FIG. <b>14</b>B</figref> is a diagram illustrating an example of determining an anisotropic filter for denoising ray-traced samples, in accordance with at least one embodiment of the present disclosure. <figref idref="DRAWINGS">FIG. <b>14</b>B</figref> illustrates an example in which an anisotropic filter may be based, at least in part, on the GGX NDF dominant direction <b>1404</b>, although other types of NDFs may be employed in various embodiments. Under glancing angles, reflections elongate along some axis. Disclosed approaches may be used to determine how much and how long the effect should be and how to find such an axis.</p><p id="p-0128" num="0125">As described herein, sampling may be defined on the blur plane <b>1402</b>, which is perpendicular to the reflected specular direction (e.g., dominant direction) <b>1406</b>. A resultant filter may be anisotropic in screen space and isotropic in world space. The filter determiner <b>106</b> may construct the basis of the filter around the reflected specular direction <b>1406</b> using tangent vector T and basis vector B to define filter directions and/or radii of the filter. Vectors T and B may have the same unit length when a generalized basis is employed. The tangent vector T may be computed from the normal vector N and the reflected specular direction <b>1406</b> (e.g., based at least on a normalized cross-product of the vectors). The basis vector B may be computed from the reflected specular direction <b>1406</b> and the tangent vector T (e.g., based at least on a cross-product of the vectors). The vectors T and B may define an isotropic filter basis. In order to simulate the effect of reflections under glancing angles, the tangent vector T may be scaled based at least on the viewing angle of the view vector V. In some embodiments, surface roughness of the point p may also be used as a scaling factor, as elongation does not happen for diffuse signals or specular signals as they approach diffuse signals due to the expansion of the hemisphere of the sampling space. In at least one embodiment, the tangent vector may be scaled and may be perpendicular to the surface normal such that the elongation happens along the surface normal.</p><p id="p-0129" num="0126">Referring now to <figref idref="DRAWINGS">FIG. <b>15</b></figref>, <figref idref="DRAWINGS">FIG. <b>15</b></figref> is a flow diagram showing an example of a method <b>1500</b> for computing a dimension of a filter based at least on a direction of a reflected specular lobe, a normal, and an angle of a view vector, in accordance with at least one embodiment of the present disclosure. The method <b>1500</b> at block B<b>1502</b> includes generating a ray-traced sample based at least on an interaction in a virtual environment. For example, the image renderer <b>102</b> may generate at least one ray-traced sample based at least on an interaction between the view vector V and the point p in a virtual environment.</p><p id="p-0130" num="0127">At block B<b>1504</b>, the method <b>1500</b> includes computing a dimension of a filter based at least on a direction of a reflected specular lobe of the interaction and a normal associated with the interaction, where a magnitude of the dimension is based at least on an angle of a view vector associated with the interaction. For example, the filter determiner <b>106</b> may compute a dimension of an anisotropic denoising filter (e.g., along the tangent vector T) based at least on the reflected specular direction of the interaction and the normal vector N that corresponds to the point p in the virtual environment, where a magnitude of the dimension is based at least on an angle of the view vector V.</p><p id="p-0131" num="0128">At block B<b>1506</b>, the method <b>1500</b> includes generating a rendered frame using the filter. For example, a rendered frame may be generated based at least on the denoiser <b>108</b> applying the anisotropic denoising filter to data corresponding to the at least one ray-traced sample.</p><p id="p-0132" num="0129">Referring now to <figref idref="DRAWINGS">FIG. <b>16</b></figref>, <figref idref="DRAWINGS">FIG. <b>16</b></figref> is a flow diagram showing an example of a method <b>1600</b> for determining an anisotropic filter based at least on elongating an isotropic filter basis based at least on an angle of a view vector, in accordance with at least one embodiment of the present disclosure. The method <b>1600</b>, at block B<b>1602</b>, includes computing an isotropic filter basis based at least on a direction of a reflected specular lobe and a normal of an interaction of a view vector in a virtual environment. For example, the filter determiner <b>106</b> may compute an isotropic filter basis (e.g., comprising the tangent vector T and the basis vector B) based at least on the reflected specular direction <b>1406</b> and the normal vector N of an interaction of the view vector V in a virtual environment.</p><p id="p-0133" num="0130">At block B<b>1604</b>, the method <b>1600</b> includes determining an anisotropic filter based at least on elongating the isotropic filter basis based at least on an angle of the view vector associated with the interaction. For example, the filter determiner <b>106</b> may determine an anisotropic filter based at least on elongating the isotropic filter basis (e.g., the tangent vector T) based at least on an angle of the view vector V.</p><p id="p-0134" num="0131">At block B<b>1606</b>, the method <b>1600</b> includes generating a rendered frame using the anisotropic filter. For example, a rendered frame of the virtual environment may be generated based at least on the denoiser <b>108</b> filtering render data corresponding to the interaction using the anisotropic filter.</p><p id="p-0135" num="0132">Referring now to <figref idref="DRAWINGS">FIG. <b>17</b></figref>, <figref idref="DRAWINGS">FIG. <b>17</b></figref> is a flow diagram showing an example of a method <b>1700</b> for computing a dimension of a filter based at least on a direction of a reflected specular lobe, a normal, and an angle of a ray associated with an interaction, in accordance with at least one embodiment of the present disclosure. The method <b>1700</b>, at block B<b>1702</b>, includes determining a dimension of a filter based at least on a direction of a reflected specular lobe of an interaction of a ray in a virtual environment, a normal of the interaction, and an angle of the ray. For example, the filter determiner <b>106</b> may determine a dimension of a filter for the point p based at least on the reflected specular direction <b>1406</b> of an interaction of the view vector V in a virtual environment at the point p.</p><p id="p-0136" num="0133">At block B<b>1704</b>, the method <b>1700</b> includes generating a rendered frame using the filter. For example, a rendered frame of the virtual environment may be generated based at least on the denoiser <b>108</b> blurring render data that corresponds to the interaction using the filter.</p><p id="p-0137" num="0134">Accumulation Speed Based on Parallax</p><p id="p-0138" num="0135">Aspects of the disclosure may provide for using parallax between frames or renders to control the accumulation speed of temporal accumulation (e.g., for specular accumulation). For example, surface motion may be used to reproject the specular history texture from a previous frame or render to a subsequent (e.g., current) frame or render. However, when light reflects off a surface, the motion of reflections may be different that surface motion, and therefore using only surface motion may result in artifacts in rendered frames. The accuracy of surface motion for a pixel may be based, at least in part on parallax between frames, and therefore may be used to control the impact of the surface motion in the temporal accumulation.</p><p id="p-0139" num="0136">In various embodiments, parallax used to compute accumulation speed for a pixel may be based at least on an angle between view vectors that corresponds to the pixel. For example, parallax may correspond to an angle between previous and current view vectors for the same surface point(s). In at least one embodiment, parallax for a pixel may be computed as a ratio between a camera movement projection for the point(s) between renders or frames to the screen plane and a distance to the point(s). The accumulation speed may also be based at least on a viewing angle associated with the view vector of the ray-traced render, which may account for how parallax is perceived visually at different viewing angles. These approaches may be used in combination with recurrent blurring, post-accumulation, pre-accumulation, and/or other denoising strategies.</p><p id="p-0140" num="0137">Referring now to <figref idref="DRAWINGS">FIG. <b>18</b></figref>, <figref idref="DRAWINGS">FIG. <b>18</b></figref> shows a render <b>1800</b> of a virtual environment with view vectors <b>1802</b>A, <b>1802</b>B, <b>1804</b>A, and <b>1804</b>B that may be used to compute parallax, in accordance with at least one embodiment of the present disclosure. In <figref idref="DRAWINGS">FIG. <b>18</b></figref>, the view vectors <b>1802</b>A and <b>1802</b>B are examples of view vectors for a current frame and/or rendered state of a virtual environment and the view vectors <b>1804</b>A and <b>1804</b>B are examples of view vectors for a prior frame and/or rendered state of the virtual environment. The view vectors <b>1802</b>A and <b>1804</b>A may be used to compute parallax for a corresponding pixel and the view vectors <b>1802</b>B and <b>1804</b>B may be used to compute parallax for a different corresponding pixel. In at least one embodiment, the parallax may be computed based at least on an angle between the current and previous view vectors (e.g., the view vectors <b>1802</b>A and <b>1804</b>A or the view vectors <b>1802</b>B and <b>1804</b>B). For example, parallax for a pixel may be computed at least in part as a ratio between a camera movement projection for point(s) between renders or frames to the screen plane and a distance to the point(s). Camera movement for the view vectors <b>1802</b>A and <b>1804</b>A may comprise a world-space vector between the corresponding camera positions <b>1810</b>A and <b>1810</b>B (as shown), which may be projected to the screen plane to determine the camera movement projection. In at least one embodiment, the viewZ coordinate of the pixel for the current frame may be used for the distance to the point(s).</p><p id="p-0141" num="0138">In various embodiments, the sample accumulator <b>104</b> may compute the parallax in world space or in view space and the parallax may be applied to one or more corresponding pixels for determining accumulation speed. Parallax may be large for pixels close to the camera. For example, parallax may be larger for the pixel corresponding to the view vectors <b>1802</b>A and <b>1804</b>A, then for the pixel corresponding to the view vectors <b>1802</b>B and <b>1804</b>B. If parallax is small, the actual accumulation determined by the sample accumulator <b>104</b> is more or less correct based on surface motion. Thus, parallax may be used by the sample accumulator <b>104</b> to determine accumulation speeds used to accumulate for pixels.</p><p id="p-0142" num="0139">In at least one embodiment, the sample accumulator <b>104</b> may determine the accumulation speed based at least on parallax and on a dot product between the current and previous view vectors (e.g., the view vectors <b>1802</b>A and <b>1804</b>A or the view vectors <b>1802</b>B and <b>1804</b>B). Surface roughness at the current point may also be a factor. If roughness is close to 1, reflections may get resolved into a diffuse signal, and the diffuse signal may always lie on the surface such that accumulation can be close to 100% with the history buffer. With respect to viewing angle, when a surface is viewed at a glancing angle, a view is most sensitive to parallax. When viewed from above at the surface, the view vector is perpendicular to the surface and the effect of parallax will be reduced. As such, viewing angle may be used as measure of sensitivity for the parallax.</p><p id="p-0143" num="0140">Referring now to <figref idref="DRAWINGS">FIG. <b>19</b></figref>, <figref idref="DRAWINGS">FIG. <b>19</b></figref> is a flow diagram showing an example of a method <b>1900</b> for computing an accumulation speed based at least on an angle between view vectors and a viewing angle associated with a view vector, in accordance with at least one embodiment of the present disclosure. The method <b>1900</b>, at block B<b>1902</b>, includes computing an accumulation based at least on an angle between a first view vector and a second view vector, and based at least on a viewing angle associated with the first view vector. For example, the sample accumulator <b>104</b> may compute an accumulation speed for at least one pixel based at least on an angle between the view vector <b>1802</b>A that corresponds to the at least one pixel and the view vector <b>1804</b>A that corresponds to the at least one pixel, and based at least on a viewing angle associated with the view vector <b>1802</b>A. The view vector <b>1802</b>A may be used the image renderer <b>102</b> to generate a first ray-traced render of a virtual environment and the view vector <b>1804</b>A may be used by the image renderer <b>102</b> to generate a second ray-traced render of the virtual environment.</p><p id="p-0144" num="0141">At block B<b>1904</b>, the method <b>1900</b> includes accumulating one or more samples using the accumulation speed to generate an accumulated render of the virtual environment. For example, the sample accumulator <b>104</b> may accumulate one or more samples with a first accumulated render of the virtual environment using the accumulation speed for the at least one pixel to generate a second accumulated render of the virtual environment.</p><p id="p-0145" num="0142">At block B<b>1906</b>, the method <b>1900</b> includes generating a rendered frame using the accumulated render. For example, a rendered frame may be generated using the second accumulated render.</p><p id="p-0146" num="0143">Referring now to <figref idref="DRAWINGS">FIG. <b>20</b></figref>, <figref idref="DRAWINGS">FIG. <b>20</b></figref> is a flow diagram showing an example of a method <b>2000</b> for accumulating ray-traced renders using accumulation speeds based at least on parallax between views and viewing angles captured by the ray-traced renders, in accordance with at least one embodiment of the present disclosure. The method <b>2000</b>, at block B<b>2002</b>, includes computing an accumulation speed based at least on parallax between ray-traced renders of a virtual environment, and based at least on a viewing angle associated with one or more of the ray-traced renders. For example, the sample accumulator <b>104</b> may compute an accumulation speed based at least on parallax between ray-traced renders of a virtual environment, and based at least on a viewing angle (e.g., of the view vector <b>1802</b>A and/or <b>1802</b>B) associated with one or more of the ray-traced renders.</p><p id="p-0147" num="0144">At block B<b>2004</b>, the method <b>2000</b> includes accumulating one or more samples using the accumulation speed to generate an accumulated render of the virtual environment. For example, the sample accumulator <b>104</b> may accumulate one or more samples to a first accumulated render of the virtual environment using the accumulation speed to generate a second accumulated render of the virtual environment.</p><p id="p-0148" num="0145">At block B<b>2006</b>, the method <b>2000</b> includes generating a rendered frame using the second accumulated render.</p><p id="p-0149" num="0146">Referring now to <figref idref="DRAWINGS">FIG. <b>21</b></figref>, <figref idref="DRAWINGS">FIG. <b>21</b></figref> is a flow diagram showing an example of a method <b>2100</b> for computing an accumulation speed based at least on parallax between renders and a viewing angle associated with one or more of the renders, in accordance with at least one embodiment of the present disclosure. The method <b>2100</b>, at block B<b>2102</b>, includes accumulating ray-traced renders using accumulation speeds based at least on parallax between views and viewing angles captured by the ray-traced renders. For example, the sample accumulator <b>104</b> may accumulate ray-traced renders of a virtual environment using accumulation speeds for pixels based at least on parallax between views of the virtual environment represented by the ray-traced renders and viewing angles of the virtual environment captured by the ray-traced renders.</p><p id="p-0150" num="0147">At block B<b>2104</b>, the method <b>2100</b> includes generating one or more renderer frames using data generated by the accumulating.</p><heading id="h-0006" level="1">Examples of Ray-Tracing Techniques</heading><p id="p-0151" num="0148">Referring now to <figref idref="DRAWINGS">FIG. <b>22</b></figref>, <figref idref="DRAWINGS">FIG. <b>22</b></figref> is a diagram used to illustrate examples of ray-tracing techniques which the image renderer <b>102</b> may use to generate ray-traced renders, in accordance with some embodiments of the present disclosure. <figref idref="DRAWINGS">FIG. <b>22</b></figref> shows a virtual environment <b>2200</b> including a camera <b>2202</b>, a screen <b>2204</b>, an object <b>2206</b>, and a surface <b>2208</b>.</p><p id="p-0152" num="0149">The screen <b>2204</b> may be a virtual representation of a screen which may or more not be the same resolution as rendered frames or intermediate images or renders described herein. The screen <b>2204</b> may include a matrix of virtual pixels or regions, of which a pixel <b>2212</b> is individually labeled. To determine at least some lighting condition data for the pixel <b>2212</b>, the image renderer <b>102</b> may cast any number of rays (e.g., one or more) &#x2014;such as a ray <b>2214</b>&#x2014;through the pixel <b>2212</b> of the screen <b>2204</b> to sample lighting conditions for the pixel <b>2212</b>. These rays may be referred to as camera rays, eye rays, incident rays, view vectors, or primary rays, as examples.</p><p id="p-0153" num="0150">The image renderer <b>102</b> may use the camera rays to determine visible points in the environment which may be affected by the object <b>2206</b> and/or at least one light source. For example, the image renderer <b>102</b> may use the ray <b>2214</b> to determine a point <b>2216</b> (e.g., the point p) on or near the surface <b>2208</b>. This may include the image renderer determining the point <b>2216</b> as the location where the ray <b>2214</b> intersects with the surface <b>2208</b> (or the point <b>2216</b> may otherwise be based at least on that location). Although the ray <b>2214</b> intersects with the surface <b>2208</b>, in examples where more than one ray is cast, not all rays may intersect with a surface.</p><p id="p-0154" num="0151">From each point in the virtual environment <b>2200</b> that the image renderer <b>102</b> determines using a ray cast through the pixel <b>2212</b>, any number of rays (e.g., one or more) &#x2014;such as a ray <b>2218</b>&#x2014;may be cast to model a reflection of the ray <b>2214</b> at the point <b>2216</b>. The image renderer <b>102</b> may determine the direction of the ray <b>2218</b> based at least on a normal of the surface <b>2208</b> at the point <b>2216</b>. For example, the image renderer may define the NDF range <b>2220</b> for the point <b>2216</b> based at least on the normal of the surface <b>2208</b> at the point <b>2216</b>. The image renderer <b>102</b> may use the NDF, the ray <b>2214</b>, and a roughness value of the surface <b>2208</b> that is associated with the point <b>2216</b> to define the BRDF (e.g., using a glossy microfacet BRDF model). For example, the NDF range <b>2220</b> may be defined using a function (the NDF) that defines a likelihood of a microfacet being aligned in a particular direction.</p><p id="p-0155" num="0152">The BRDF lobe may be defined using a function (the BRDF) that uses the NDF as a weighting function to scale the brightness of reflections. The image renderer <b>102</b> may sample the BRDF lobe (e.g., stochastically using the BRDF or using another sampling strategy) to determine the ray <b>2218</b>. These rays may be referred to as reflected rays, or secondary rays, as examples. Although the ray <b>2218</b> intersects with the object <b>2206</b>, in examples where more than one ray is cast, not all rays may intersect with the object <b>2206</b>. For example, a ray may intersect with a different object or may not intersect with any object. In some embodiments, one or more additional rays may be cast to determine lighting conditions for the pixel <b>2212</b>, such as based on an interaction of the ray <b>2218</b> with the object <b>2206</b>, and similar operations may be performed with respect to a point of that interaction.</p><p id="p-0156" num="0153">The various rays (e.g., the ray <b>2214</b>, the ray <b>2218</b>, etc.) may be used by the image renderer <b>102</b> to determine ray-traced samples of lighting conditions for the pixel <b>2212</b>. For example, the rays may form any number of virtual light paths between the pixel <b>2212</b> and one or more light sources, any number of examples of which may include the ray <b>2214</b> and the ray <b>2218</b>. The object <b>2206</b> may be an example of such a light source, or the object <b>2206</b> may be another type of object, such as a non-light emitting reflective object. Where the object <b>2206</b> is not a light source, the virtual light path may further include one or more rays to a light source. The image renderer may determine at least some lighting condition data for the pixel <b>2212</b> by combining (e.g., averaging) the lighting condition data derived from the various ray-traced samples. The image renderer may similarly determine at least some lighting condition data for each pixel or region of the screen <b>2204</b> (e.g., using any number of shadow rays, camera rays, and/or other ray types) in order to generate a ray-traced render of the virtual environment <b>2200</b>.</p><p id="p-0157" num="0154">As described herein, aspects of the disclosure provide approaches for the filter determiner <b>108</b> determining the size, shape, orientation, filter directions, and/or filter weights of a filter kernel that is applied to a pixel(s) to denoise the ray-traced samples of a graphically-rendered image. A size, orientation, and/or shape of a filter kernel and corresponding filter weights of the filter may be determined and/or adjusted based on various information described herein, such as temporal ray-traced sample information from preceding frames and/or image render data used by the image renderer to render an output and/or final frame or render. Examples include radiance data (e.g., specular or diffuse), hit distance data, scene depth data, and world normals. Various types of filters are contemplated as being within the scope of the disclosure including Poisson disk-based filters, Poisson-based filters, uniform jitter-based filters, bilateral filters, cross-bilateral filters, spatiotemporal filters, temporal filters, spatial filters, etc. Various numbers of samples may be used for a filter, an example of which includes eight. Suitable footprints for filters may include rectangular, square, round, elliptical, etc.</p><p id="p-0158" num="0155">In various examples, the filter determiner <b>108</b> may determine one or more filter dimensions of a filter based at least on the ray <b>2214</b>, which may or may not have been used by the image renderer to sample lighting conditions for the pixel <b>2212</b> (e.g., to determine corresponding image data). In some examples, the image renderer <b>102</b> uses at least one ray-traced sample of at least one previous state of the virtual environment <b>2200</b> (e.g., each state may correspond to a rendered frame representative of the virtual environment <b>2200</b>) to determine lighting conditions for the pixel <b>2212</b> at a subsequent state(s) of the virtual environment. For example, any number of temporal ray-traced samples may be used to determine lighting conditions for the pixel <b>2212</b> in addition to the one or more spatial or current ray-traced samples used to determine the lighting conditions for the pixel <b>2212</b> (e.g., using an accumulation or history buffer). Where a number of temporal ray-traced samples are used, those samples may be counted (e.g., per-pixel) or may otherwise be used to reduce the size and/or one or more geometries of the filter.</p><p id="p-0159" num="0156">Using some approaches described herein, at grazing angles the filter may be stretched along with the reflection itself, and in a region where the ray <b>2214</b> is more perpendicular to the surface <b>2208</b>, the filter may become less elongated because it may more closely correspond to the BRDF lobe at the point <b>2216</b>.</p><p id="p-0160" num="0157">Example Computing Device</p><p id="p-0161" num="0158"><figref idref="DRAWINGS">FIG. <b>23</b></figref> is a block diagram of an example computing device(s) <b>2300</b> suitable for use in implementing some embodiments of the present disclosure. Computing device <b>2300</b> may include an interconnect system <b>2302</b> that directly or indirectly couples the following devices: memory <b>2304</b>, one or more central processing units (CPUs) <b>2306</b>, one or more graphics processing units (GPUs) <b>2308</b>, a communication interface <b>2310</b>, input/output (I/O) ports <b>2312</b>, input/output components <b>2314</b>, a power supply <b>2316</b>, one or more presentation components <b>2318</b> (e.g., display(s)), and one or more logic units <b>2320</b>. In at least one embodiment, the computing device(s) <b>2300</b> may comprise one or more virtual machines (VMs), and/or any of the components thereof may comprise virtual components (e.g., virtual hardware components). For non-limiting examples, one or more of the GPUs <b>2308</b> may comprise one or more vGPUs, one or more of the CPUs <b>2306</b> may comprise one or more vCPUs, and/or one or more of the logic units <b>2320</b> may comprise one or more virtual logic units. As such, a computing device(s) <b>2300</b> may include discrete components (e.g., a full GPU dedicated to the computing device <b>2300</b>), virtual components (e.g., a portion of a GPU dedicated to the computing device <b>2300</b>), or a combination thereof.</p><p id="p-0162" num="0159">Although the various blocks of <figref idref="DRAWINGS">FIG. <b>23</b></figref> are shown as connected via the interconnect system <b>2302</b> with lines, this is not intended to be limiting and is for clarity only. For example, in some embodiments, a presentation component <b>2318</b>, such as a display device, may be considered an I/O component <b>2314</b> (e.g., if the display is a touch screen). As another example, the CPUs <b>2306</b> and/or GPUs <b>2308</b> may include memory (e.g., the memory <b>2304</b> may be representative of a storage device in addition to the memory of the GPUs <b>2308</b>, the CPUs <b>2306</b>, and/or other components). In other words, the computing device of <figref idref="DRAWINGS">FIG. <b>23</b></figref> is merely illustrative. Distinction is not made between such categories as &#x201c;workstation,&#x201d; &#x201c;server,&#x201d; &#x201c;laptop,&#x201d; &#x201c;desktop,&#x201d; &#x201c;tablet,&#x201d; &#x201c;client device,&#x201d; &#x201c;mobile device,&#x201d; &#x201c;hand-held device,&#x201d; &#x201c;game console,&#x201d; &#x201c;electronic control unit (ECU),&#x201d; &#x201c;virtual reality system,&#x201d; and/or other device or system types, as all are contemplated within the scope of the computing device of <figref idref="DRAWINGS">FIG. <b>23</b></figref>.</p><p id="p-0163" num="0160">The interconnect system <b>2302</b> may represent one or more links or busses, such as an address bus, a data bus, a control bus, or a combination thereof. The interconnect system <b>2302</b> may include one or more bus or link types, such as an industry standard architecture (ISA) bus, an extended industry standard architecture (EISA) bus, a video electronics standards association (VESA) bus, a peripheral component interconnect (PCI) bus, a peripheral component interconnect express (PCIe) bus, and/or another type of bus or link. In some embodiments, there are direct connections between components. As an example, the CPU <b>2306</b> may be directly connected to the memory <b>2304</b>. Further, the CPU <b>2306</b> may be directly connected to the GPU <b>2308</b>. Where there is direct, or point-to-point connection between components, the interconnect system <b>2302</b> may include a PCIe link to carry out the connection. In these examples, a PCI bus need not be included in the computing device <b>2300</b>.</p><p id="p-0164" num="0161">The memory <b>2304</b> may include any of a variety of computer-readable media. The computer-readable media may be any available media that may be accessed by the computing device <b>2300</b>. The computer-readable media may include both volatile and nonvolatile media, and removable and non-removable media. By way of example, and not limitation, the computer-readable media may comprise computer-storage media and communication media.</p><p id="p-0165" num="0162">The computer-storage media may include both volatile and nonvolatile media and/or removable and non-removable media implemented in any method or technology for storage of information such as computer-readable instructions, data structures, program modules, and/or other data types. For example, the memory <b>2304</b> may store computer-readable instructions (e.g., that represent a program(s) and/or a program element(s), such as an operating system. Computer-storage media may include, but is not limited to, RAM, ROM, EEPROM, flash memory or other memory technology, CD-ROM, digital versatile disks (DVD) or other optical disk storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium which may be used to store the desired information and which may be accessed by computing device <b>2300</b>. As used herein, computer storage media does not comprise signals per se.</p><p id="p-0166" num="0163">The computer storage media may embody computer-readable instructions, data structures, program modules, and/or other data types in a modulated data signal such as a carrier wave or other transport mechanism and includes any information delivery media. The term &#x201c;modulated data signal&#x201d; may refer to a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal. By way of example, and not limitation, the computer storage media may include wired media such as a wired network or direct-wired connection, and wireless media such as acoustic, RF, infrared and other wireless media. Combinations of any of the above should also be included within the scope of computer-readable media.</p><p id="p-0167" num="0164">The CPU(s) <b>2306</b> may be configured to execute at least some of the computer-readable instructions to control one or more components of the computing device <b>2300</b> to perform one or more of the methods and/or processes described herein. The CPU(s) <b>2306</b> may each include one or more cores (e.g., one, two, four, eight, twenty-eight, seventy-two, etc.) that are capable of handling a multitude of software threads simultaneously. The CPU(s) <b>2306</b> may include any type of processor, and may include different types of processors depending on the type of computing device <b>2300</b> implemented (e.g., processors with fewer cores for mobile devices and processors with more cores for servers). For example, depending on the type of computing device <b>2300</b>, the processor may be an Advanced RISC Machines (ARM) processor implemented using Reduced Instruction Set Computing (RISC) or an x86 processor implemented using Complex Instruction Set Computing (CISC). The computing device <b>2300</b> may include one or more CPUs <b>2306</b> in addition to one or more microprocessors or supplementary co-processors, such as math co-processors.</p><p id="p-0168" num="0165">In addition to or alternatively from the CPU(s) <b>2306</b>, the GPU(s) <b>2308</b> may be configured to execute at least some of the computer-readable instructions to control one or more components of the computing device <b>2300</b> to perform one or more of the methods and/or processes described herein. One or more of the GPU(s) <b>2308</b> may be an integrated GPU (e.g., with one or more of the CPU(s) <b>2306</b> and/or one or more of the GPU(s) <b>2308</b> may be a discrete GPU. In embodiments, one or more of the GPU(s) <b>2308</b> may be a coprocessor of one or more of the CPU(s) <b>2306</b>. The GPU(s) <b>2308</b> may be used by the computing device <b>2300</b> to render graphics (e.g., 3D graphics) or perform general purpose computations. For example, the GPU(s) <b>2308</b> may be used for General-Purpose computing on GPUs (GPGPU). The GPU(s) <b>2308</b> may include hundreds or thousands of cores that are capable of handling hundreds or thousands of software threads simultaneously. The GPU(s) <b>2308</b> may generate pixel data for output images in response to rendering commands (e.g., rendering commands from the CPU(s) <b>2306</b> received via a host interface). The GPU(s) <b>2308</b> may include graphics memory, such as display memory, for storing pixel data or any other suitable data, such as GPGPU data. The display memory may be included as part of the memory <b>2304</b>. The GPU(s) <b>2308</b> may include two or more GPUs operating in parallel (e.g., via a link). The link may directly connect the GPUs (e.g., using NVLINK) or may connect the GPUs through a switch (e.g., using NVSwitch). When combined together, each GPU <b>2308</b> may generate pixel data or GPGPU data for different portions of an output or for different outputs (e.g., a first GPU for a first image and a second GPU for a second image). Each GPU may include its own memory, or may share memory with other GPUs.</p><p id="p-0169" num="0166">In addition to or alternatively from the CPU(s) <b>2306</b> and/or the GPU(s) <b>2308</b>, the logic unit(s) <b>2320</b> may be configured to execute at least some of the computer-readable instructions to control one or more components of the computing device <b>2300</b> to perform one or more of the methods and/or processes described herein. In embodiments, the CPU(s) <b>2306</b>, the GPU(s) <b>2308</b>, and/or the logic unit(s) <b>2320</b> may discretely or jointly perform any combination of the methods, processes and/or portions thereof. One or more of the logic units <b>2320</b> may be part of and/or integrated in one or more of the CPU(s) <b>2306</b> and/or the GPU(s) <b>2308</b> and/or one or more of the logic units <b>2320</b> may be discrete components or otherwise external to the CPU(s) <b>2306</b> and/or the GPU(s) <b>2308</b>. In embodiments, one or more of the logic units <b>2320</b> may be a coprocessor of one or more of the CPU(s) <b>2306</b> and/or one or more of the GPU(s) <b>2308</b>.</p><p id="p-0170" num="0167">Examples of the logic unit(s) <b>2320</b> include one or more processing cores and/or components thereof, such as Tensor Cores (TCs), Tensor Processing Units (TPUs), Pixel Visual Cores (PVCs), Vision Processing Units (VPUs), Graphics Processing Clusters (GPCs), Texture Processing Clusters (TPCs), Streaming Multiprocessors (SMs), Tree Traversal Units (TTUs), Artificial Intelligence Accelerators (AIAs), Deep Learning Accelerators (DLAs), Arithmetic-Logic Units (ALUs), Application-Specific Integrated Circuits (ASICs), Floating Point Units (FPUs), input/output (I/O) elements, peripheral component interconnect (PCI) or peripheral component interconnect express (PCIe) elements, and/or the like.</p><p id="p-0171" num="0168">The communication interface <b>2310</b> may include one or more receivers, transmitters, and/or transceivers that enable the computing device <b>2300</b> to communicate with other computing devices via an electronic communication network, included wired and/or wireless communications. The communication interface <b>2310</b> may include components and functionality to enable communication over any of a number of different networks, such as wireless networks (e.g., Wi-Fi, Z-Wave, Bluetooth, Bluetooth LE, ZigBee, etc.), wired networks (e.g., communicating over Ethernet or InfiniBand), low-power wide-area networks (e.g., LoRaWAN, SigFox, etc.), and/or the Internet.</p><p id="p-0172" num="0169">The I/O ports <b>2312</b> may enable the computing device <b>2300</b> to be logically coupled to other devices including the I/O components <b>2314</b>, the presentation component(s) <b>2318</b>, and/or other components, some of which may be built in to (e.g., integrated in) the computing device <b>2300</b>. Illustrative I/O components <b>2314</b> include a microphone, mouse, keyboard, joystick, game pad, game controller, satellite dish, scanner, printer, wireless device, etc. The I/O components <b>2314</b> may provide a natural user interface (NUI) that processes air gestures, voice, or other physiological inputs generated by a user. In some instances, inputs may be transmitted to an appropriate network element for further processing. An NUI may implement any combination of speech recognition, stylus recognition, facial recognition, biometric recognition, gesture recognition both on screen and adjacent to the screen, air gestures, head and eye tracking, and touch recognition (as described in more detail below) associated with a display of the computing device <b>2300</b>. The computing device <b>2300</b> may be include depth cameras, such as stereoscopic camera systems, infrared camera systems, RGB camera systems, touchscreen technology, and combinations of these, for gesture detection and recognition. Additionally, the computing device <b>2300</b> may include accelerometers or gyroscopes (e.g., as part of an inertia measurement unit (IMU)) that enable detection of motion. In some examples, the output of the accelerometers or gyroscopes may be used by the computing device <b>2300</b> to render immersive augmented reality or virtual reality.</p><p id="p-0173" num="0170">The power supply <b>2316</b> may include a hard-wired power supply, a battery power supply, or a combination thereof. The power supply <b>2316</b> may provide power to the computing device <b>2300</b> to enable the components of the computing device <b>2300</b> to operate.</p><p id="p-0174" num="0171">The presentation component(s) <b>2318</b> may include a display (e.g., a monitor, a touch screen, a television screen, a heads-up-display (HUD), other display types, or a combination thereof), speakers, and/or other presentation components. The presentation component(s) <b>2318</b> may receive data from other components (e.g., the GPU(s) <b>2308</b>, the CPU(s) <b>2306</b>, etc.), and output the data (e.g., as an image, video, sound, etc.).</p><p id="p-0175" num="0172">Example Data Center</p><p id="p-0176" num="0173"><figref idref="DRAWINGS">FIG. <b>24</b></figref> illustrates an example data center <b>2400</b> that may be used in at least one embodiments of the present disclosure. The data center <b>2400</b> may include a data center infrastructure layer <b>2410</b>, a framework layer <b>2420</b>, a software layer <b>2430</b>, and/or an application layer <b>2440</b>.</p><p id="p-0177" num="0174">As shown in <figref idref="DRAWINGS">FIG. <b>24</b></figref>, the data center infrastructure layer <b>2410</b> may include a resource orchestrator <b>2412</b>, grouped computing resources <b>2414</b>, and node computing resources (&#x201c;node C.R.s&#x201d;) <b>2416</b>(<b>1</b>)-<b>2416</b>(N), where &#x201c;N&#x201d; represents any whole, positive integer. In at least one embodiment, node C.R.s <b>2416</b>(<b>1</b>)-<b>2416</b>(N) may include, but are not limited to, any number of central processing units (CPUs) or other processors (including accelerators, field programmable gate arrays (FPGAs), graphics processors or graphics processing units (GPUs), etc.), memory devices (e.g., dynamic read-only memory), storage devices (e.g., solid state or disk drives), network input/output (NW I/O) devices, network switches, virtual machines (VMs), power modules, and/or cooling modules, etc. In some embodiments, one or more node C.R.s from among node C.R.s <b>2416</b>(<b>1</b>)-<b>2416</b>(N) may correspond to a server having one or more of the above-mentioned computing resources. In addition, in some embodiments, the node C.R.s <b>2416</b>(<b>1</b>)-<b>24161</b>(N) may include one or more virtual components, such as vGPUs, vCPUs, and/or the like, and/or one or more of the node C.R.s <b>2416</b>(<b>1</b>)-<b>2416</b>(N) may correspond to a virtual machine (VM).</p><p id="p-0178" num="0175">In at least one embodiment, grouped computing resources <b>2414</b> may include separate groupings of node C.R.s <b>2416</b> housed within one or more racks (not shown), or many racks housed in data centers at various geographical locations (also not shown). Separate groupings of node C.R.s <b>2416</b> within grouped computing resources <b>2414</b> may include grouped compute, network, memory or storage resources that may be configured or allocated to support one or more workloads. In at least one embodiment, several node C.R.s <b>2416</b> including CPUs, GPUs, and/or other processors may be grouped within one or more racks to provide compute resources to support one or more workloads. The one or more racks may also include any number of power modules, cooling modules, and/or network switches, in any combination.</p><p id="p-0179" num="0176">The resource orchestrator <b>2422</b> may configure or otherwise control one or more node C.R.s <b>2416</b>(<b>1</b>)-<b>2416</b>(N) and/or grouped computing resources <b>2414</b>. In at least one embodiment, resource orchestrator <b>2422</b> may include a software design infrastructure (SDI) management entity for the data center <b>2400</b>. The resource orchestrator <b>2422</b> may include hardware, software, or some combination thereof.</p><p id="p-0180" num="0177">In at least one embodiment, as shown in <figref idref="DRAWINGS">FIG. <b>24</b></figref>, framework layer <b>2420</b> may include a job scheduler <b>2418</b>, a configuration manager <b>2434</b>, a resource manager <b>2436</b>, and/or a distributed file system <b>2438</b>. The framework layer <b>2420</b> may include a framework to support software <b>2432</b> of software layer <b>2430</b> and/or one or more application(s) <b>2442</b> of application layer <b>2440</b>. The software <b>2432</b> or application(s) <b>2442</b> may respectively include web-based service software or applications, such as those provided by Amazon Web Services, Google Cloud and Microsoft Azure. The framework layer <b>2420</b> may be, but is not limited to, a type of free and open-source software web application framework such as Apache Spark&#x2122; (hereinafter &#x201c;Spark&#x201d;) that may utilize distributed file system <b>2438</b> for large-scale data processing (e.g., &#x201c;big data&#x201d;). In at least one embodiment, job scheduler <b>2418</b> may include a Spark driver to facilitate scheduling of workloads supported by various layers of data center <b>2400</b>. The configuration manager <b>2434</b> may be capable of configuring different layers such as software layer <b>2430</b> and framework layer <b>2420</b> including Spark and distributed file system <b>2438</b> for supporting large-scale data processing. The resource manager <b>2436</b> may be capable of managing clustered or grouped computing resources mapped to or allocated for support of distributed file system <b>2438</b> and job scheduler <b>2418</b>. In at least one embodiment, clustered or grouped computing resources may include grouped computing resource <b>2414</b> at data center infrastructure layer <b>2410</b>. The resource manager <b>1036</b> may coordinate with resource orchestrator <b>2412</b> to manage these mapped or allocated computing resources.</p><p id="p-0181" num="0178">In at least one embodiment, software <b>2432</b> included in software layer <b>2430</b> may include software used by at least portions of node C.R.s <b>2416</b>(<b>1</b>)-<b>2416</b>(N), grouped computing resources <b>2414</b>, and/or distributed file system <b>2438</b> of framework layer <b>2420</b>. One or more types of software may include, but are not limited to, Internet web page search software, e-mail virus scan software, database software, and streaming video content software.</p><p id="p-0182" num="0179">In at least one embodiment, application(s) <b>2442</b> included in application layer <b>2440</b> may include one or more types of applications used by at least portions of node C.R.s <b>2416</b>(<b>1</b>)-<b>2416</b>(N), grouped computing resources <b>2414</b>, and/or distributed file system <b>2438</b> of framework layer <b>2420</b>. One or more types of applications may include, but are not limited to, any number of a genomics application, a cognitive compute, and a machine learning application, including training or inferencing software, machine learning framework software (e.g., PyTorch, TensorFlow, Caffe, etc.), and/or other machine learning applications used in conjunction with one or more embodiments.</p><p id="p-0183" num="0180">In at least one embodiment, any of configuration manager <b>2434</b>, resource manager <b>2436</b>, and resource orchestrator <b>2412</b> may implement any number and type of self-modifying actions based on any amount and type of data acquired in any technically feasible fashion. Self-modifying actions may relieve a data center operator of data center <b>2400</b> from making possibly bad configuration decisions and possibly avoiding underutilized and/or poor performing portions of a data center.</p><p id="p-0184" num="0181">The data center <b>2400</b> may include tools, services, software or other resources to train one or more machine learning models or predict or infer information using one or more machine learning models according to one or more embodiments described herein. For example, a machine learning model(s) may be trained by calculating weight parameters according to a neural network architecture using software and/or computing resources described above with respect to the data center <b>2400</b>. In at least one embodiment, trained or deployed machine learning models corresponding to one or more neural networks may be used to infer or predict information using resources described above with respect to the data center <b>2400</b> by using weight parameters calculated through one or more training techniques, such as but not limited to those described herein.</p><p id="p-0185" num="0182">In at least one embodiment, the data center <b>2400</b> may use CPUs, application-specific integrated circuits (ASICs), GPUs, FPGAs, and/or other hardware (or virtual compute resources corresponding thereto) to perform training and/or inferencing using above-described resources. Moreover, one or more software and/or hardware resources described above may be configured as a service to allow users to train or performing inferencing of information, such as image recognition, speech recognition, or other artificial intelligence services.</p><p id="p-0186" num="0183">Example Network Environments</p><p id="p-0187" num="0184">Network environments suitable for use in implementing embodiments of the disclosure may include one or more client devices, servers, network attached storage (NAS), other backend devices, and/or other device types. The client devices, servers, and/or other device types (e.g., each device) may be implemented on one or more instances of the computing device(s) <b>2300</b> of <figref idref="DRAWINGS">FIG. <b>23</b></figref>&#x2014;e.g., each device may include similar components, features, and/or functionality of the computing device(s) <b>2300</b>. In addition, where backend devices (e.g., servers, NAS, etc.) are implemented, the backend devices may be included as part of a data center <b>2400</b>, an example of which is described in more detail herein with respect to <figref idref="DRAWINGS">FIG. <b>24</b></figref>.</p><p id="p-0188" num="0185">Components of a network environment may communicate with each other via a network(s), which may be wired, wireless, or both. The network may include multiple networks, or a network of networks. By way of example, the network may include one or more Wide Area Networks (WANs), one or more Local Area Networks (LANs), one or more public networks such as the Internet and/or a public switched telephone network (PSTN), and/or one or more private networks. Where the network includes a wireless telecommunications network, components such as a base station, a communications tower, or even access points (as well as other components) may provide wireless connectivity.</p><p id="p-0189" num="0186">Compatible network environments may include one or more peer-to-peer network environments&#x2014;in which case a server may not be included in a network environment&#x2014;and one or more client-server network environments&#x2014;in which case one or more servers may be included in a network environment. In peer-to-peer network environments, functionality described herein with respect to a server(s) may be implemented on any number of client devices.</p><p id="p-0190" num="0187">In at least one embodiment, a network environment may include one or more cloud-based network environments, a distributed computing environment, a combination thereof, etc. A cloud-based network environment may include a framework layer, a job scheduler, a resource manager, and a distributed file system implemented on one or more of servers, which may include one or more core network servers and/or edge servers. A framework layer may include a framework to support software of a software layer and/or one or more application(s) of an application layer. The software or application(s) may respectively include web-based service software or applications. In embodiments, one or more of the client devices may use the web-based service software or applications (e.g., by accessing the service software and/or applications via one or more application programming interfaces (APIs)). The framework layer may be, but is not limited to, a type of free and open-source software web application framework such as that may use a distributed file system for large-scale data processing (e.g., &#x201c;big data&#x201d;).</p><p id="p-0191" num="0188">A cloud-based network environment may provide cloud computing and/or cloud storage that carries out any combination of computing and/or data storage functions described herein (or one or more portions thereof). Any of these various functions may be distributed over multiple locations from central or core servers (e.g., of one or more data centers that may be distributed across a state, a region, a country, the globe, etc.). If a connection to a user (e.g., a client device) is relatively close to an edge server(s), a core server(s) may designate at least a portion of the functionality to the edge server(s). A cloud-based network environment may be private (e.g., limited to a single organization), may be public (e.g., available to many organizations), and/or a combination thereof (e.g., a hybrid cloud environment).</p><p id="p-0192" num="0189">The client device(s) may include at least some of the components, features, and functionality of the example computing device(s) <b>2300</b> described herein with respect to <figref idref="DRAWINGS">FIG. <b>23</b></figref>. By way of example and not limitation, a client device may be embodied as a Personal Computer (PC), a laptop computer, a mobile device, a smartphone, a tablet computer, a smart watch, a wearable computer, a Personal Digital Assistant (PDA), an MP3 player, a virtual reality headset, a Global Positioning System (GPS) or device, a video player, a video camera, a surveillance device or system, a vehicle, a boat, a flying vessel, a virtual machine, a drone, a robot, a handheld communications device, a hospital device, a gaming device or system, an entertainment system, a vehicle computer system, an embedded system controller, a remote control, an appliance, a consumer electronic device, a workstation, an edge device, any combination of these delineated devices, or any other suitable device.</p><p id="p-0193" num="0190">The disclosure may be described in the general context of computer code or machine-useable instructions, including computer-executable instructions such as program modules, being executed by a computer or other machine, such as a personal data assistant or other handheld device. Generally, program modules including routines, programs, objects, components, data structures, etc., refer to code that perform particular tasks or implement particular abstract data types. The disclosure may be practiced in a variety of system configurations, including hand-held devices, consumer electronics, general-purpose computers, more specialty computing devices, etc. The disclosure may also be practiced in distributed computing environments where tasks are performed by remote-processing devices that are linked through a communications network.</p><p id="p-0194" num="0191">As used herein, a recitation of &#x201c;and/or&#x201d; with respect to two or more elements should be interpreted to mean only one element, or a combination of elements. For example, &#x201c;element A, element B, and/or element C&#x201d; may include only element A, only element B, only element C, element A and element B, element A and element C, element B and element C, or elements A, B, and C. In addition, &#x201c;at least one of element A or element B&#x201d; may include at least one of element A, at least one of element B, or at least one of element A and at least one of element B. Further, &#x201c;at least one of element A and element B&#x201d; may include at least one of element A, at least one of element B, or at least one of element A and at least one of element B.</p><p id="p-0195" num="0192">The subject matter of the present disclosure is described with specificity herein to meet statutory requirements. However, the description itself is not intended to limit the scope of this disclosure. Rather, the inventors have contemplated that the claimed subject matter might also be embodied in other ways, to include different steps or combinations of steps similar to the ones described in this document, in conjunction with other present or future technologies. Moreover, although the terms &#x201c;step&#x201d; and/or &#x201c;block&#x201d; may be used herein to connote different elements of methods employed, the terms should not be interpreted as implying any particular order among or between various steps herein disclosed unless and except when the order of individual steps is explicitly described.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method comprising:<claim-text>computing an accumulation speed for at least one pixel based at least on an angle between a first view vector that corresponds to the at least one pixel and a second view vector that corresponds to the at least one pixel;</claim-text><claim-text>accumulating one or more samples with a first accumulated render of a virtual environment using the accumulation speed for the at least one pixel to generate a second accumulated render of a virtual environment; and</claim-text><claim-text>generating a rendered frame based at least on denoising at least one sample corresponding to the virtual environment using the second accumulated render.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the accumulation speed is further based on one or more of a surface roughness that corresponds to an interaction of the first view vector or a viewing angle associated with the first view vector.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the denoising includes one or more of:<claim-text>applying one or more denoising filters to the second accumulated render;</claim-text><claim-text>accumulating at least a portion of the second accumulated render with a noisy render of the virtual environment to generate a third accumulated render of the virtual environment, and applying one or more denoising filters to the third accumulated render; or</claim-text><claim-text>determining, using the second accumulated render, one or more dimensions of one or more denoising filters used to generate the rendered frame.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the computing of the accumulation speed uses an adjustment factor that increases a sensitivity of the accumulation speed to the angle between the first view vector and the second view vector as a viewing angle associated with the first view vector decreases.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the computing of the accumulation speed uses an adjustment factor that increases the accumulation speed as the angle between the first view vector and the second view vector increases.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first view vector is used to generate a first ray-traced render of a virtual environment and the second view vector is used to generate a second ray-traced render of the virtual environment, and the first accumulated render corresponds to the second ray-traced render.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one or more samples are from a ray-traced render of the virtual environment and correspond to the at least one pixel.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein computing of the accumulation speed is further based on a surface normal associated with the first view vector.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. A processor comprising:<claim-text>one or more circuits to:</claim-text><claim-text>compute an accumulation speed based at least on parallax between ray-traced renders of a virtual environment;</claim-text><claim-text>accumulate one or more samples to a first accumulated render of the virtual environment using the accumulation speed to generate a second accumulated render of the virtual environment; and</claim-text><claim-text>generate a rendered frame based at least on denoising at least one sample corresponding to the virtual environment using the second accumulated render.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The processor of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the accumulation speed is further based on one or more of a surface roughness that corresponds to an interaction of a view vector associated with a viewing angle corresponding to one or more of the ray-traced renders.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The processor of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein a sensitivity of the accumulation speed to the parallax increases as a viewing angle corresponding to one or more of the ray-traced renders decreases.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The processor of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the accumulation speed increases as the parallax increases.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The processor of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the one or more circuits are further to compute the parallax based at least on an angle between a first view vector associated with a first of the ray-traced renders and a second view vector associated with a second of the ray-traced renders.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The processor of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the accumulation speed is computed based on determining the parallax of at least one pixel and the one or more samples correspond to the at least one pixel.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The processor of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the processor is comprised in at least one of:<claim-text>a system for performing simulation operations;</claim-text><claim-text>a system for performing deep learning operations;</claim-text><claim-text>a system implemented using an edge device;</claim-text><claim-text>a system incorporating one or more virtual machines (VMs);</claim-text><claim-text>a system implemented at least partially in a data center; or</claim-text><claim-text>a system implemented at least partially using cloud computing resources.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. A system comprising:<claim-text>one or more processing units to execute operations comprising:<claim-text>accumulating ray-traced renders of a virtual environment using accumulation speeds for pixels based at least on parallax between views of the virtual environment represented by the ray-traced renders; and</claim-text><claim-text>generating one or more rendered frames using data that is generated by the accumulating.</claim-text></claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The system of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the accumulation speeds are further based on one or more of surface roughness values that corresponds to interactions of view vectors associated with viewing angles of the virtual environment captured by the ray-traced renders.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The system of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein a sensitivity of the accumulation speeds to the parallax increases as the viewing angles decrease.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The system of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the accumulation speeds increase as the parallax increases.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The system of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the operations further comprise determining the accumulation speeds based at least on computing the parallax using angles between view vectors associated with the pixels.</claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. The system of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the accumulation speeds are further based on surface normals associated with view vectors that have the viewing angles.</claim-text></claim></claims></us-patent-application>