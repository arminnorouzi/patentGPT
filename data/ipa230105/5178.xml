<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005179A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005179</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17941859</doc-number><date>20220909</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>73</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>593</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>80</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>13</main-group><subgroup>296</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>13</main-group><subgroup>344</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>13</main-group><subgroup>239</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>73</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>593</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>85</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20180501</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>13</main-group><subgroup>296</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20180501</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>13</main-group><subgroup>344</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20180501</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>13</main-group><subgroup>239</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>10028</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30244</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">USING 6DOF POSE INFORMATION TO ALIGN IMAGES FROM SEPARATED CAMERAS</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>17340455</doc-number><date>20210607</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11475586</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17941859</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16932415</doc-number><date>20200717</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11049277</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17340455</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Microsoft Technology Licensing, LLC</orgname><address><city>Redmond</city><state>WA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>PRICE</last-name><first-name>Raymond Kirk</first-name><address><city>Carnation</city><state>WA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>BLEYER</last-name><first-name>Michael</first-name><address><city>Seattle</city><state>WA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>EDMONDS</last-name><first-name>Christopher Douglas</first-name><address><city>Carnation</city><state>WA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Techniques for aligning images generated by an integrated camera physically mounted to an HMD with images generated by a detached camera physically unmounted from the HMD are disclosed. A 3D feature map is generated and shared with the detached camera. Both the integrated camera and the detached camera use the 3D feature map to relocalize themselves and to determine their respective <b>6</b> DOF poses. The HMD receives the detached camera's image of the environment and the <b>6</b> DOF pose of the detached camera. A depth map of the environment is accessed. An overlaid image is generated by reprojecting a perspective of the detached camera's image to align with a perspective of the integrated camera and by overlaying the reprojected detached camera's image onto the integrated camera's image.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="134.62mm" wi="158.75mm" file="US20230005179A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="192.45mm" wi="184.23mm" orientation="landscape" file="US20230005179A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="216.58mm" wi="181.86mm" orientation="landscape" file="US20230005179A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="193.97mm" wi="152.82mm" orientation="landscape" file="US20230005179A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="234.95mm" wi="178.82mm" orientation="landscape" file="US20230005179A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="154.26mm" wi="178.82mm" orientation="landscape" file="US20230005179A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="193.46mm" wi="181.44mm" orientation="landscape" file="US20230005179A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="140.80mm" wi="165.78mm" orientation="landscape" file="US20230005179A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="105.24mm" wi="164.34mm" orientation="landscape" file="US20230005179A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="193.46mm" wi="181.44mm" orientation="landscape" file="US20230005179A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="193.89mm" wi="127.51mm" orientation="landscape" file="US20230005179A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="105.24mm" wi="171.20mm" orientation="landscape" file="US20230005179A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="208.36mm" wi="181.95mm" orientation="landscape" file="US20230005179A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="122.26mm" wi="145.20mm" orientation="landscape" file="US20230005179A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="194.90mm" wi="182.63mm" orientation="landscape" file="US20230005179A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="167.39mm" wi="136.91mm" orientation="landscape" file="US20230005179A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="187.54mm" wi="147.24mm" orientation="landscape" file="US20230005179A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="215.90mm" wi="180.85mm" orientation="landscape" file="US20230005179A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="206.59mm" wi="171.37mm" orientation="landscape" file="US20230005179A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application is a continuation of U.S. patent application Ser. No. 17/340,455 filed on Jun. 7, 2021, entitled &#x201c;USING 6DOF POSE INFORMATION TO ALIGN IMAGES FROM SEPARATED CAMERAS&#x201d;, which is a continuation of U.S. patent application Ser. No. 16/932,415 filed on Jul. 17, 2020, entitled &#x201c;USING 6DOF POSE INFORMATION TO ALIGN IMAGES FROM SEPARATED CAMERAS,&#x201d; which applications are expressly incorporated herein by reference in their entirety.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">Mixed-reality (MR) systems, including virtual-reality (VR) and augmented-reality (AR) systems, have received significant attention because of their ability to create truly unique experiences for their users. For reference, conventional VR systems create completely immersive experiences by restricting their users' views to only virtual environments. This is often achieved through the use of a head-mounted device (HMD) that completely blocks any view of the real world. As a result, a user is entirely immersed within the virtual environment. In contrast, conventional AR systems create an augmented-reality experience by visually presenting virtual objects that are placed in or that interact with the real world.</p><p id="p-0004" num="0003">As used herein, VR and AR systems are described and referenced interchangeably. Unless stated otherwise, the descriptions herein apply equally to all types of MR systems, which (as detailed above) include AR systems, VR reality systems, and/or any other similar system capable of displaying virtual content.</p><p id="p-0005" num="0004">A MR system may also employ different types of cameras in order to display content to users, such as in the form of a passthrough image. A passthrough image or view can aid users in avoiding disorientation and/or safety hazards when transitioning into and/or navigating within a MR environment. A MR system can present views captured by cameras in a variety of ways. The process of using images captured by world-facing cameras to provide views of a real-world environment creates many challenges, however.</p><p id="p-0006" num="0005">Some of these challenges occur when attempting to align image content from multiple cameras. Often, this alignment process requires detailed timestamp information in order to perform the alignment processes. Sometimes, however, timestamp data is not available because different cameras may be operating in different time domains such that they have a temporal offset. Furthermore, sometimes the timestamp data is simply not available because the cameras may be operating remotely from one another, and the timestamp data is not transmitted. Another problem occurs as a result of having both a left and a right HMD camera (i.e. a dual camera system) but only a single detached camera. Aligning image content between the detached camera's image and the left camera's image in addition to aligning image content between the detached camera's image and the right camera's image causes many problems in compute efficiency and image alignment. That said, aligning image content provides substantial benefits, especially in terms of hologram placement and generation, so these problems present serious obstacles to the technical field. Accordingly, there is a substantial need in the field to improve how images are aligned with one another.</p><p id="p-0007" num="0006">The subject matter claimed herein is not limited to embodiments that solve any disadvantages or that operate only in environments such as those described above. Rather, this background is only provided to illustrate one exemplary technology area where some embodiments described herein may be practiced.</p><heading id="h-0003" level="1">BRIEF SUMMARY</heading><p id="p-0008" num="0007">Embodiments disclosed herein relate to systems, devices (e.g., hardware storage devices, wearable devices, etc.), and methods for aligning and stabilizing images generated by an integrated camera that is physically mounted to a head-mounted device (HMD) with images generated by a detached camera that is physically unmounted from the HMD.</p><p id="p-0009" num="0008">In some embodiments, a three-dimensional (3D) feature map of an environment in which both the HMD and the detached camera are operating in is generated. The 3D feature map is then shared with the detached camera. The 3D feature map is used to relocalize a positional framework of the integrated camera based on a first image generated by the integrated camera. As a consequence, a 6 degree of freedom (<b>6</b> DOF) pose of the integrated camera is determined. Furthermore, the detached camera uses the 3D feature map to relocalize a positional framework of the detached camera based on a second image generated by the detached camera. Consequently, a <b>6</b> DOF pose of the detached camera is also determined. The embodiments then receive (i) the second image of the environment and (ii) the <b>6</b> DOF pose of the detached camera from the detached camera. A depth map of the environment is accessed. Additionally, an overlaid image is generated by reprojecting a perspective of the second image to align or match with a perspective of the first image and then by overlaying at least a portion of the reprojected second image onto the first image. Notably, (i) the <b>6</b> DOF pose of the integrated camera, (ii) the <b>6</b> DOF pose of the detached camera, and (iii) the depth map are used to perform the reprojection process.</p><p id="p-0010" num="0009">Optionally, some embodiments additionally perform parallax correction on the overlaid image to modify a perspective of the overlaid image to correspond to a novel perspective. In some cases, the novel perspective is a perspective of a pupil of a user wearing the HMD. An additional option is to display the overlaid image for the user to view.</p><p id="p-0011" num="0010">This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter, nor is it intended to be used as an aid in determining the scope of the claimed subject matter.</p><p id="p-0012" num="0011">Additional features and advantages will be set forth in the description which follows, and in part will be obvious from the description, or may be learned by the practice of the teachings herein. Features and advantages of the invention may be realized and obtained by means of the instruments and combinations particularly pointed out in the appended claims. Features of the present invention will become more fully apparent from the following description and appended claims or may be learned by the practice of the invention as set forth hereinafter.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0013" num="0012">In order to describe the manner in which the above-recited and other advantages and features can be obtained, a more particular description of the subject matter briefly described above will be rendered by reference to specific embodiments which are illustrated in the appended drawings. Understanding that these drawings depict only typical embodiments and are not therefore to be considered to be limiting in scope, embodiments will be described and explained with additional specificity and detail through the use of the accompanying drawings in which:</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates an example scenario involving an integrated camera and a detached camera.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates an example head-mounted device (HMD).</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates an example implementation or configuration of an HMD.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIGS. <b>4</b>A and <b>4</b>B</figref> illustrate flowcharts of an example method for aligning images from a detached camera with images from an integrated camera using 6 DOF pose information from both of those cameras.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates an example scenario in which an integrated camera is generating an image of an environment.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates a resulting 3D feature map that may be generated, where the 3D feature map identifies feature points located in the environment.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates how the HMD is able to share or transmit the 3D feature map with the detached camera.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates an example scenario in which the integrated camera and the detached camera are generating images of the environment.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>9</b></figref> illustrates a relocalization process that may be performed by the integrated camera and the detached camera in order to enable both of those cameras to use the same coordinate system in the HMD's physical space.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>10</b></figref> illustrates how the detached camera is able to transmit its <b>6</b> DOF pose information and its generated image to the HMD.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>11</b></figref> illustrates how the HMD maintains information regarding the integrated camera's images and <b>6</b> DOF pose; information regarding the detached camera's images and <b>6</b> DOF pose; and information regarding a depth map of the environment. The HMD is also able to update the <b>6</b> DOF pose information based on inertial measurement unit (IMU) data obtained from IMUS associated with the integrated camera and the detached camera.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>12</b></figref> illustrates how the depth map may be generated based on different source information.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>13</b></figref> illustrates an example reprojection operation that may be performed to reproject a perspective of the detached camera's image to a new perspective matching the perspective of the integrated camera's image in order to enable that reprojected image to then be subsequently overlaid onto the integrated camera's image.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>14</b></figref> illustrates inputs that may be used to perform the reprojection operation.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>15</b></figref> illustrates an example overlay operation that may be performed to overlay the reprojected image (i.e. the detached camera's image) onto the integrated camera's image to generate an overlaid image.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>16</b></figref> illustrates how a parallax correction operation may be performed on the overlaid image to correct for parallax.</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>17</b></figref> illustrates an example computer system capable of performing any of the disclosed operations.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0031" num="0030">Embodiments disclosed herein relate to systems, devices (e.g., hardware storage devices, wearable devices, etc.), and methods for aligning and stabilizing images generated by an integrated camera that is physically mounted to a head-mounted device (HMD) with images generated by a detached camera that is physically unmounted from the HMD.</p><p id="p-0032" num="0031">In some embodiments, a 3D feature map of an environment is generated and then shared with the detached camera. The 3D feature map is used to relocalize the integrated camera such that a <b>6</b> DOF pose of the integrated camera is determined. The detached camera also relocalizes itself based on the 3D feature map such that its <b>6</b> DOF pose is also determined. The embodiments then receive (i) the detached camera's image of the environment and (ii) the <b>6</b> DOF pose of the detached camera. A depth map of the environment is accessed. An overlaid image is generated by reprojecting a perspective of the detached camera's image to align with a perspective of the integrated camera's image and by overlaying at least a portion of the reprojected detached camera's image onto the integrated camera's image. Notably, (i) the <b>6</b> DOF pose of the integrated camera, (ii) the <b>6</b> DOF pose of the detached camera, and (iii) the depth map are used to perform the reprojection process. Optionally, some embodiments additionally perform parallax correction on the overlaid image and then display the overlaid image.</p><heading id="h-0006" level="1">Examples Of Technical Benefits, Improvements, And Practical Applications</heading><p id="p-0033" num="0032">The following section outlines some example improvements and practical applications provided by the disclosed embodiments. It will be appreciated, however, that these are just examples only and that the embodiments are not limited to only these improvements.</p><p id="p-0034" num="0033">The disclosed embodiments provide substantial improvements, benefits, and practical applications to the technical field. By way of example, the disclosed embodiments improve how images are generated and displayed and improve how image content is aligned.</p><p id="p-0035" num="0034">That is, the embodiments solve the problem of aligning image content from a remote or detached camera image with image content from an integrated camera image to create a single composite or overlaid image. Notably, the overlaid image is generated without requiring the use of timestamp data, but rather is generated by using a 3D feature map to determine the <b>6</b> DOF poses of both camera systems. By having 6 DOF poses from both the remote camera system and the HMD, and with the understanding of the scene geometry, the disclosed embodiments are able to provide precise image overlay between the remote camera system and the HMD, taking into account the physical separation and different orientation. Once the poses are determined, the embodiments are able to beneficially reproject the detached camera's image in a manner so as to align its perspective with the perspective of the integrated camera's image. After the reprojection occurs, the detached camera's image can then be overlaid onto the integrated camera's image to form the overlaid image. In this regard, the disclosed embodiments solve problems related to image alignment when images are generated by separated cameras and when both a left and a right passthrough image are desired despite only a single detached camera image being generated. By performing the disclosed operations, the embodiments are able to significantly improve image quality and image display.</p><p id="p-0036" num="0035">Integrated Cameras And Detached Cameras</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows an example environment <b>100</b> in which an HMD <b>105</b> is operating. HMD <b>105</b> may be configured in various different ways, as illustrated in <figref idref="DRAWINGS">FIGS. <b>2</b> and <b>3</b></figref>.</p><p id="p-0038" num="0037">By way of example, HMD <b>105</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> may be configured as the HMD <b>200</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>. HMD <b>200</b> can be any type of MR system <b>200</b>A, including a VR system <b>200</b>B or an AR system <b>200</b>C. It should be noted that while a substantial portion of this disclosure is focused on the use of an HMD, the embodiments are not limited to being practiced using only an HMD. That is, any type of scanning system can be used, even systems entirely removed or separate from an HMD. As such, the disclosed principles should be interpreted broadly to encompass any type of scanning scenario or device. Some embodiments may even refrain from actively using a scanning device themselves and may simply use the data generated by the scanning device. For instance, some embodiments may at least be partially practiced in a cloud computing environment.</p><p id="p-0039" num="0038">HMD <b>200</b> is shown as including scanning sensor(s) <b>205</b> (i.e. a type of scanning or camera system), and HMD <b>200</b> can use the scanning sensor(s) <b>205</b> to scan environments, map environments, capture environmental data, and/or generate any kind of images of the environment (e.g., by generating a 3D representation of the environment or by generating a &#x201c;passthrough&#x201d; visualization). Scanning sensor(s) <b>205</b> may comprise any number or any type of scanning devices, without limit.</p><p id="p-0040" num="0039">In accordance with the disclosed embodiments, the HMD <b>200</b> may be used to generate a parallax-corrected passthrough visualization of the user's environment. In some cases, a &#x201c;passthrough&#x201d; visualization refers to a visualization that reflects what the user would see if the user were not wearing the HMD <b>200</b>, regardless of whether the HMD <b>200</b> is included as a part of an AR system or a VR system. In other cases, the passthrough visualization reflects a different or novel perspective.</p><p id="p-0041" num="0040">To generate this passthrough visualization, the HMD <b>200</b> may use its scanning sensor(s) <b>205</b> to scan, map, or otherwise record its surrounding environment, including any objects in the environment, and to pass that data on to the user to view. In many cases, the passed-through data is modified to reflect or to correspond to a perspective of the user's pupils, though other perspectives may be reflected by the image as well. The perspective may be determined by any type of eye tracking technique or other data.</p><p id="p-0042" num="0041">To convert a raw image into a passthrough image, the scanning sensor(s) <b>205</b> typically rely on its cameras (e.g., head tracking cameras, hand tracking cameras, depth cameras, or any other type of camera) to obtain one or more raw images (aka texture images) of the environment. In addition to generating passthrough images, these raw images may also be used to determine depth data detailing the distance from the sensor to any objects captured by the raw images (e.g., a z-axis range or measurement). Once these raw images are obtained, then a depth map can be computed from the depth data embedded or included within the raw images (e.g., based on pixel disparities), and passthrough images can be generated (e.g., one for each pupil) using the depth map for any reprojections. In some cases, the depth map can be assessed by 3D sensing systems, including time of flight, stereo, active stereo, or structured light systems. Furthermore, an assessment of the visual map of the surrounding environment may be performed with head tracking cameras, and these head tracking cameras typically have a stereo overlap region to assess 3D geometry and generate a map of the environment. Also worthwhile to note, it is often the case that the remote camera system has similar &#x201c;head tracking cameras&#x201d; for identifying its location in the 3D space.</p><p id="p-0043" num="0042">As used herein, a &#x201c;depth map&#x201d; details the positional relationship and depths relative to objects in the environment. Consequently, the positional arrangement, location, geometries, contours, and depths of objects relative to one another can be determined. From the depth maps, a 3D representation of the environment can be generated.</p><p id="p-0044" num="0043">Relatedly, from the passthrough visualizations, a user will be able to perceive what is currently in his/her environment without having to remove or reposition the HMD <b>200</b>. Furthermore, as will be described in more detail later, the disclosed passthrough visualizations will also enhance the user's ability to view objects within his/her environment (e.g., by displaying additional environmental conditions or image data that may not have been detectable by a human eye).</p><p id="p-0045" num="0044">It should be noted that while the majority of this disclosure focuses on generating &#x201c;a&#x201d; passthrough (or overlaid) image, the embodiments may generate a separate passthrough image for each one of the user's eyes. That is, two passthrough images are typically generated concurrently with one another. Therefore, while frequent reference is made to generating what seems to be a single passthrough image, the embodiments are actually able to simultaneously generate multiple passthrough images.</p><p id="p-0046" num="0045">In some embodiments, scanning sensor(s) <b>205</b> include visible light camera(s) <b>210</b>, low light camera(s) <b>215</b>, thermal imaging camera(s) <b>220</b>, potentially (though not necessarily, as represented by the dotted box in <figref idref="DRAWINGS">FIG. <b>2</b></figref>) ultraviolet (UV) camera(s) <b>225</b>, and potentially (though not necessarily) a dot illuminator (not shown). The ellipsis <b>230</b> demonstrates how any other type of camera or camera system (e.g., depth cameras, time of flight cameras, virtual cameras, depth lasers, etc.) may be included among the scanning sensor(s) <b>205</b>.</p><p id="p-0047" num="0046">As an example, a camera structured to detect mid-infrared wavelengths may be included within the scanning sensor(s) <b>205</b>. As another example, any number of virtual cameras that are reprojected from an actual camera may be included among the scanning sensor(s) <b>205</b> and may be used to generate a stereo pair of images. In this manner and as will be discussed in more detail later, the scanning sensor(s) <b>205</b> may be used to generate the stereo pair of images. In some cases, the stereo pair of images may be obtained or generated as a result of performing any one or more of the following operations: active stereo image generation via use of two cameras and one dot illuminator; passive stereo image generation via use of two cameras; image generation using structured light via use of one actual camera, one virtual camera, and one dot illuminator; or image generation using a time of flight (TOF) sensor in which a baseline is present between a depth laser and a corresponding camera and in which a field of view (FOV) of the corresponding camera is offset relative to a field of illumination of the depth laser.</p><p id="p-0048" num="0047">Generally, a human eye is able to perceive light within the so-called &#x201c;visible spectrum,&#x201d; which includes light (or rather, electromagnetic radiation) having wavelengths ranging from about 380 nanometers (nm) up to about 740 nm. As used herein, the visible light camera(s) <b>210</b> include two or more monochrome cameras structured to capture light photons within the visible spectrum. Often, these monochrome cameras are complementary metal-oxide-semiconductor (CMOS) type cameras, though other camera types may be used as well (e.g., charge coupled devices, CCD). These monochrome cameras can also extend into the NIR regime (up to 1100 nm).</p><p id="p-0049" num="0048">The monochrome cameras are typically stereoscopic cameras, meaning that the fields of view of the two or more monochrome cameras at least partially overlap with one another. With this overlapping region, images generated by the visible light camera(s) <b>210</b> can be used to identify disparities between certain pixels that commonly represent an object captured by both images. Based on these pixel disparities, the embodiments are able to determine depths for objects located within the overlapping region (i.e. &#x201c;stereoscopic depth matching&#x201d; or &#x201c;stereo depth matching&#x201d;). As such, the visible light camera(s) <b>210</b> can be used to not only generate passthrough visualizations, but they can also be used to determine object depth. In some embodiments, the visible light camera(s) <b>210</b> can capture both visible light and IR light.</p><p id="p-0050" num="0049">The low light camera(s) <b>215</b> are structured to capture visible light and IR light. IR light is often segmented into three different classifications, including near-IR, mid-IR, and far-IR (e.g., thermal-IR). The classifications are determined based on the energy of the IR light. By way of example, near-IR has relatively higher energy as a result of having relatively shorter wavelengths (e.g., between about 750 nm and about 1,100 nm). In contrast, far-IR has relatively less energy as a result of having relatively longer wavelengths (e.g., up to about 30,000 nm). Mid-IR has energy values in between or in the middle of the near-IR and far-IR ranges. The low light camera(s) <b>215</b> are structured to detect or be sensitive to IR light in at least the near-IR range.</p><p id="p-0051" num="0050">In some embodiments, the visible light camera(s) <b>210</b> and the low light camera(s) <b>215</b> (aka low light night vision cameras) operate in approximately the same overlapping wavelength range. In some cases, this overlapping wavelength range is between about 400 nanometers and about 1,100 nanometers. Additionally, in some embodiments these two types of cameras are both silicon detectors.</p><p id="p-0052" num="0051">One distinguishing feature between these two types of cameras is related to the illuminance conditions or illuminance range(s) in which they actively operate. In some cases, the visible light camera(s) <b>210</b> are low power cameras and operate in environments where the illuminance is between about a dusk illuminance (e.g., about 10 lux) and a bright noonday sun illuminance (e.g., about 100,000 lux), or rather, the illuminance range begins at about 10 lux and increases beyond 10 lux. In contrast, the low light camera(s) <b>215</b> consume more power and operate in environments where the illuminance range is between about a starlight illumination (e.g., about 1 milli lux) and a dusk illumination (e.g., about 10 lux).</p><p id="p-0053" num="0052">The thermal imaging camera(s) <b>220</b>, on the other hand, are structured to detect electromagnetic radiation or IR light in the far-IR (i.e. thermal-IR) range, though some embodiments also enable the thermal imaging camera(s) <b>220</b> to detect radiation in the mid-IR range. To clarify, the thermal imaging camera(s) <b>220</b> may be a long wave infrared imaging camera structured to detect electromagnetic radiation by measuring long wave infrared wavelengths. Often, the thermal imaging camera(s) <b>220</b> detect IR radiation having wavelengths between about 8 microns and 14 microns to detect blackbody radiation from the environment and people in the camera field of view. Because the thermal imaging camera(s) <b>220</b> detect far-IR radiation, the thermal imaging camera(s) <b>220</b> can operate in any illuminance condition, without restriction.</p><p id="p-0054" num="0053">In some cases (though not all), the thermal imaging camera(s) <b>220</b> include an uncooled thermal imaging sensor. An uncooled thermal imaging sensor uses a specific type of detector design that is based on an array of microbolometers, which is a device that measures the magnitude or power of an incident electromagnetic wave/radiation. To measure the radiation, the microbolometer uses a thin layer of absorptive material (e.g., metal) connected to a thermal reservoir through a thermal link. The incident wave strikes and heats the material. In response to the material being heated, the microbolometer detects a temperature-dependent electrical resistance. Changes to environmental temperature cause changes to the bolometer's temperature, and these changes can be converted into an electrical signal to thereby produce a thermal image of the environment. In accordance with at least some of the disclosed embodiments, the uncooled thermal imaging sensor is used to generate any number of thermal images. The bolometer of the uncooled thermal imaging sensor can detect electromagnetic radiation across a wide spectrum, spanning the mid-IR spectrum, the far-IR spectrum, and even up to millimeter-sized waves.</p><p id="p-0055" num="0054">The UV camera(s) <b>225</b> are structured to capture light in the UV range. The UV range includes electromagnetic radiation having wavelengths between about 150 nm and about 400 nm. The disclosed UV camera(s) <b>225</b> should be interpreted broadly and may be operated in a manner that includes both reflected UV photography and UV induced fluorescence photography.</p><p id="p-0056" num="0055">Accordingly, as used herein, reference to &#x201c;visible light cameras&#x201d; (including &#x201c;head tracking cameras&#x201d;), are cameras that are primarily used for computer vision to perform head tracking. These cameras can detect visible light, or even a combination of visible and IR light (e.g., a range of IR light, including IR light having a wavelength of about 850 nm). In some cases, these cameras are global shutter devices with pixels being about 3 &#x3bc;m in size. Low light cameras, on the other hand, are cameras that are sensitive to visible light and near-IR. These cameras are larger and may have pixels that are about 8 &#x3bc;m in size or larger. These cameras are also sensitive to wavelengths that silicon sensors are sensitive to, which wavelengths are between about 350 nm to 1100 nm. These sensors can also be fabricated with III-V materials to be optically sensitive to NIR wavelengths. Thermal/long wavelength IR devices (i.e. thermal imaging cameras) have pixel sizes that are about 10 &#x3bc;m or larger and detect heat radiated from the environment. These cameras are sensitive to wavelengths in the 8 &#x3bc;m to 14 &#x3bc;m range. Some embodiments also include mid-IR cameras configured to detect at least mid-IR light. These cameras often comprise non-silicon materials (e.g., InP or InGaAs) that detect light in the 800 nm to 2 &#x3bc;m wavelength range.</p><p id="p-0057" num="0056">Accordingly, the disclosed embodiments may be structured to utilize numerous different camera types. The different camera types include, but are not limited to, visible light cameras, low light cameras, thermal imaging cameras, and UV cameras. Stereo depth matching may be performed using images generated from any one type or combination of types of the above listed camera types.</p><p id="p-0058" num="0057">Generally, the low light camera(s) <b>215</b>, the thermal imaging camera(s) <b>220</b>, and the UV camera(s) <b>225</b> (if present) consume relatively more power than the visible light camera(s) <b>210</b>. Therefore, when not in use, the low light camera(s) <b>215</b>, the thermal imaging camera(s) <b>220</b>, and the UV camera(s) <b>225</b> are typically in the powered-down state in which those cameras are either turned off (and thus consuming no power) or in a reduced operability mode (and thus consuming substantially less power than if those cameras were fully operational). In contrast, the visible light camera(s) <b>210</b> are typically in the powered-up state in which those cameras are by default fully operational.</p><p id="p-0059" num="0058">It should be noted that any number of cameras may be provided on the HMD <b>200</b> for each of the different camera types. That is, the visible light camera(s) <b>210</b> may include 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, or more than 10 cameras. Often, however, the number of cameras is at least 2 so the HMD <b>200</b> can perform stereoscopic depth matching, as described earlier. Similarly, the low light camera(s) <b>215</b>, the thermal imaging camera(s) <b>220</b>, and the UV camera(s) <b>225</b> may each respectively include 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, or more than 10 corresponding cameras.</p><p id="p-0060" num="0059"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates an example HMD <b>300</b>, which is representative of the HMD <b>200</b> from <figref idref="DRAWINGS">FIG. <b>2</b></figref>. HMD <b>300</b> is shown as including multiple different cameras, including cameras <b>305</b>, <b>310</b>, <b>315</b>, <b>320</b>, and <b>325</b>. Cameras <b>305</b>-<b>325</b> are representative of any number or combination of the visible light camera(s) <b>210</b>, the low light camera(s) <b>215</b>, the thermal imaging camera(s) <b>220</b>, and the UV camera(s) <b>225</b> from <figref idref="DRAWINGS">FIG. <b>2</b></figref>. While only 5 cameras are illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, HMD <b>300</b> may include more or less than 5 cameras.</p><p id="p-0061" num="0060">In some cases, the cameras can be located at specific positions on the HMD <b>300</b>. For instance, in some cases a first camera (e.g., perhaps camera <b>320</b>) is disposed on the HMD <b>300</b> at a position above a designated left eye position of any users who wear the HMD <b>300</b> relative to a height direction of the HMD. For instance, the camera <b>320</b> is positioned above the pupil <b>330</b>. As another example, the first camera (e.g., camera <b>320</b>) is additionally positioned above the designated left eye position relative to a width direction of the HMD. That is, the camera <b>320</b> is positioned not only above the pupil <b>330</b> but also in-line relative to the pupil <b>330</b>. When a VR system is used, a camera may be placed directly in front of the designated left eye position. For example, with reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, a camera may be physically disposed on the HMD <b>300</b> at a position in front of the pupil <b>330</b> in the z-axis direction.</p><p id="p-0062" num="0061">When a second camera is provided (e.g., perhaps camera <b>310</b>), the second camera may be disposed on the HMD at a position above a designated right eye position of any users who wear the HMD relative to the height direction of the HMD. For instance, the camera <b>310</b> is above the pupil <b>335</b>. In some cases, the second camera is additionally positioned above the designated right eye position relative to the width direction of the HMD. When a VR system is used, a camera may be placed directly in front of the designated right eye position. For example, with reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, a camera may be physically disposed on the HMD <b>300</b> at a position in front of the pupil <b>335</b> in the z-axis direction.</p><p id="p-0063" num="0062">When a user wears HMD <b>300</b>, HMD <b>300</b> fits over the user's head and the HMD <b>300</b>'s display is positioned in front of the user's pupils, such as pupil <b>330</b> and pupil <b>335</b>. Often, the cameras <b>305</b>-<b>325</b> will be physically offset some distance from the user's pupils <b>330</b> and <b>335</b>. For instance, there may be a vertical offset in the HMD height direction (i.e. the &#x201c;Y&#x201d; axis), as shown by offset <b>340</b>. Similarly, there may be a horizontal offset in the HMD width direction (i.e. the &#x201c;X&#x201d; axis), as shown by offset <b>345</b>.</p><p id="p-0064" num="0063">As described earlier, HMD <b>300</b> is configured to provide passthrough image(s) for the user of HMD <b>300</b> to view. In doing so, HMD <b>300</b> is able to provide a visualization of the real world without requiring the user to remove or reposition HMD <b>300</b>. These passthrough image(s) effectively represent the same view the user would see if the user were not wearing HMD <b>300</b>. Cameras <b>305</b>-<b>325</b> are used to provide these passthrough image(s).</p><p id="p-0065" num="0064">None of the cameras <b>305</b>-<b>325</b>, however, are telecentrically aligned with the pupils <b>330</b> and <b>335</b>. The offsets <b>340</b> and <b>345</b> actually introduce differences in perspective as between the cameras <b>305</b>-<b>325</b> and the pupils <b>330</b> and <b>335</b>. These perspective differences are referred to as &#x201c;parallax.&#x201d;</p><p id="p-0066" num="0065">Because of the parallax occurring as a result of the offsets <b>340</b> and <b>345</b>, raw images (aka texture images) produced by the cameras <b>305</b>-<b>325</b> may not be available for immediate use as passthrough images. Instead, it is beneficial to perform a parallax correction (aka an image synthesis) on the raw images to transform the perspectives embodied within those raw images to correspond to perspectives of the user's pupils <b>330</b> and <b>335</b>. The parallax correction includes any number of corrections, which will be discussed in more detail later.</p><p id="p-0067" num="0066">Returning to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, HMD <b>105</b> is shown as including an integrated stereo camera pair <b>110</b> comprising a first camera <b>115</b> and a second camera <b>120</b>, which cameras are representative of the cameras mentioned in <figref idref="DRAWINGS">FIGS. <b>2</b> and <b>3</b></figref>. Additionally, the first camera <b>115</b> and the second camera <b>120</b> are both integrated parts of the HMD <b>105</b>, thus the first camera <b>115</b> may be considered as an integrated camera, and the second camera <b>120</b> may also be considered as an integrated camera.</p><p id="p-0068" num="0067"><figref idref="DRAWINGS">FIG. <b>1</b></figref> also shows a detached camera <b>125</b>. Notice, the detached camera <b>125</b> is physically unmounted from the HMD <b>105</b> such that it is able to move independently of any motion of the HMD <b>105</b>. Furthermore, the detached camera <b>125</b> is separated from the HMD <b>105</b> by a distance <b>130</b>. This distance <b>130</b> may be any distance, but typically it is less than 1.5 meters (i.e. the distance <b>130</b> is at most 1.5 meters).</p><p id="p-0069" num="0068">In this example, the various different cameras are being used in a scenario where objects in the environment <b>100</b> are relatively far away from the HMD <b>105</b>, as shown by the distance <b>135</b>. The relationship between the distance <b>135</b> and the distance <b>130</b> will be discussed in more detail later. Often, however, the distance <b>135</b> is at least 3 meters.</p><p id="p-0070" num="0069">In any event, the first camera <b>115</b> is capturing images of the environment <b>100</b> from a first perspective <b>140</b>. Similarly, the second camera <b>120</b> is capturing images of the environment <b>100</b> from a second perspective <b>145</b>, and the detached camera <b>125</b> is capturing images of the environment <b>100</b> from a third perspective <b>150</b>.</p><p id="p-0071" num="0070">In situations involving the use of an integrated camera and a detached camera, it is beneficial to be able to overlay the detached camera's image onto the integrated camera's image in order to generate an overlaid image. In order to provide a highly accurate overlay between those two images, it is beneficial to first determine the 6 degrees of freedom (<b>6</b> DOF) poses of the respective cameras and then use that pose information (along with depth information) to reproject the detached camera's image to a perspective that matches or coincides with the integrated camera's perspective. After the perspectives are aligned with one another, then the detached camera's image (or at least a portion thereof) can be overlaid onto the integrated camera's image to generate the overlaid passthrough image. Accordingly, the remaining portion of this disclosure will present various techniques for aligning and stabilizing image content between two separate cameras using 6 DOF pose information.</p><heading id="h-0007" level="1">Example Methods</heading><p id="p-0072" num="0071">The following discussion now refers to a number of methods and method acts that may be performed. Although the method acts may be discussed in a certain order or illustrated in a flow chart as occurring in a particular order, no particular ordering is required unless specifically stated, or required because an act is dependent on another act being completed prior to the act being performed.</p><p id="p-0073" num="0072">Attention will now be directed to <figref idref="DRAWINGS">FIGS. <b>4</b>A and <b>4</b>B</figref>, which illustrate flowcharts of an example method <b>400</b> for aligning and stabilizing images generated by an integrated camera that is physically mounted to a head-mounted device (HMD) with images generated by a detached camera that is physically unmounted from the HMD. For instance, the HMD in method <b>400</b> may be any of the HMDs discussed thus far (e.g., HMD <b>105</b> from <figref idref="DRAWINGS">FIG. <b>1</b></figref>) such that the method <b>400</b> may be performed by the HMD <b>105</b>. Similarly, the so-called integrated camera may be either one of the first camera <b>115</b> from <figref idref="DRAWINGS">FIG. <b>1</b></figref> or the second camera <b>120</b>, and the detached camera may be the detached camera <b>125</b>.</p><p id="p-0074" num="0073">In some cases, the integrated camera is one camera selected from a group of cameras comprising a visible light camera, a low light camera, or a thermal imaging camera, and the detached camera is also one camera selected from the group of cameras. In some cases, both the detached camera and the integrated camera are of the same modality (e.g., both are thermal imaging cameras, or both are low light cameras, etc.).</p><p id="p-0075" num="0074">Initially, method <b>400</b> includes an act (act <b>405</b>) of generating a three-dimensional (3D) feature map of an environment in which both the HMD and the detached camera are operating in. The environment <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> may, for instance, be the environment mentioned in act <b>405</b>. In order to generate the 3D feature map mentioned in act <b>405</b>, the embodiments first perform a scan of the environment, as shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0076" num="0075"><figref idref="DRAWINGS">FIG. <b>5</b></figref> shows an environment <b>500</b>, which is representative of the environment <b>100</b> from <figref idref="DRAWINGS">FIG. <b>1</b></figref>. <figref idref="DRAWINGS">FIG. <b>5</b></figref> also shows an HMD <b>505</b> which is representative of the HMDs discuss thus far and is particularly representative of the HMD mentioned in act <b>405</b>. In this example scenario, the HMD <b>505</b> is performing a scan of the environment <b>500</b> using its cameras (e.g., perhaps the integrated camera or perhaps any one or more other cameras included on the HMD <b>505</b>), as shown by scan <b>510</b>, scan <b>515</b>, and scan <b>520</b> (e.g., the HMD <b>505</b> is being aimed at different areas of the environment <b>500</b>). By way of example, the HMD <b>505</b> may be utilizing its head tracking cameras in order to perform the scans.</p><p id="p-0077" num="0076">As a result of performing the scan, the HMD <b>505</b> is able to generate a 3D feature map of the environment <b>500</b>, as shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. Specifically, <figref idref="DRAWINGS">FIG. <b>6</b></figref> shows a 3D feature map <b>600</b>, which is representative of the 3D feature map mentioned in act <b>405</b> of <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>. Each of the dark circles illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref> represents a feature point, such as feature point <b>605</b>, feature point <b>610</b>, and feature point <b>615</b>.</p><p id="p-0078" num="0077">Generally, a &#x201c;feature point&#x201d; (e.g., any of feature points <b>605</b>-<b>615</b>) refers to a discrete and identifiable point included within an object or image. Examples of feature points include corners, edges, or other geometric contours having a stark contrast with other areas of the environment. The dark circles shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref> correspond to the corners where walls meet and where table corners are formed and are considered to be feature points. While only a few feature points are illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, one will appreciate how the embodiments are able to identify any number of feature points in an image.</p><p id="p-0079" num="0078">Identifying feature points may be performed using any type of image analysis, image segmentation, or perhaps even machine learning (ML). Any type of ML algorithm, model, or machine learning may be used to identify feature points. As used herein, reference to &#x201c;machine learning&#x201d; or to a ML model may include any type of machine learning algorithm or device, neural network (e.g., convolutional neural network(s), multilayer neural network(s), recursive neural network(s), deep neural network(s), dynamic neural network(s), etc.), decision tree model(s) (e.g., decision trees, random forests, and gradient boosted trees), linear regression model(s) or logistic regression model(s), support vector machine(s) (&#x201c;SVM&#x201d;), artificial intelligence device(s), or any other type of intelligent computing system. Any amount of training data may be used (and perhaps later refined) to train the machine learning algorithm to dynamically perform the disclosed operations.</p><p id="p-0080" num="0079">In a general sense, the 3D feature map <b>600</b> is a compilation of a set of fused sparse depth maps that have been acquired over time. These depth maps identify 3D depth information as well as the features points. The collection or fusing of these depth maps constitute the 3D feature map <b>600</b>.</p><heading id="h-0008" level="1">Sharing And Using The 3D Feature Map</heading><p id="p-0081" num="0080">Returning to <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, after the 3D feature map has been generated, method <b>400</b> then includes an act (act <b>410</b>) of sharing the 3D feature map with the detached camera. <figref idref="DRAWINGS">FIG. <b>7</b></figref> is representative of this method act <b>410</b>.</p><p id="p-0082" num="0081">Specifically, <figref idref="DRAWINGS">FIG. <b>7</b></figref> shows an HMD <b>700</b> and a detached camera <b>705</b>. HMD <b>700</b> is representative of the HMD <b>105</b> from <figref idref="DRAWINGS">FIG. <b>1</b></figref>, and the detached camera <b>705</b> is representative of the detached camera <b>125</b>. A broadband radio connection <b>710</b> exists between the HMD <b>700</b> and the detached camera <b>705</b> to enable information to be quickly transmitted back and forth between the HMD <b>700</b> and the detached camera <b>705</b>. The broadband radio connection <b>710</b> is a high-speed connection with a high bandwidth availability.</p><p id="p-0083" num="0082">As described in method act <b>410</b>, the HMD <b>700</b> is able to use the broadband radio connection <b>710</b> to transmit the 3D feature map <b>715</b>, which is representative of the 3D feature map <b>600</b> of <figref idref="DRAWINGS">FIG. <b>6</b></figref>, to the detached camera <b>705</b>. In this regard, the detached camera <b>705</b> receives the 3D feature map <b>715</b> from the HMD <b>700</b>. That is, the process of sharing the 3D feature map with the detached camera may be performed by transmitting the 3D feature map to the detached camera via the broadband radio connection <b>710</b>.</p><p id="p-0084" num="0083">Before, during, or possibly even after the HMD shares the 3D feature map with the detached camera, the integrated camera and the detached camera both generate images of the environment, as shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>. Specifically, <figref idref="DRAWINGS">FIG. <b>8</b></figref> shows an environment <b>800</b>, an integrated camera <b>805</b>, and a detached camera <b>810</b>, all of which are representative of the environments, integrated cameras, and detached cameras discussed herein, respectively.</p><p id="p-0085" num="0084"><figref idref="DRAWINGS">FIG. <b>8</b></figref> shows how the integrated camera <b>805</b> has a field of view (FOV) <b>815</b> and is performing an image capture in order to generate a first image <b>820</b>. Similarly, the detached camera <b>810</b> has a FOV <b>825</b> and is performing an image capture in order to generate a second image <b>830</b>. The FOV of a camera generally refers to the area that is observable by the camera. Here, the size of the FOV <b>815</b> is different from the size of the FOV <b>825</b>. In some cases, the FOVs may be the same. In this case, the FOV <b>815</b> is larger than the FOV <b>825</b>. In other cases, the FOV <b>825</b> may be larger than the FOV <b>815</b>. Despite differences in the sizes of the FOVs, it may be the case that the resulting images may have the same resolution. This aspect will be discussed in more detail later.</p><p id="p-0086" num="0085">In some implementations, the overall architecture includes computer vision (CV) visible light (VL) cameras on the detached system. These CV VL cameras are used to identify markers in the scene and to relocalize the position of the device in the shared map from the HMD. The FOV of the detached camera CV VL cameras is typically much larger than the main imaging camera used in the remote camera system.</p><p id="p-0087" num="0086">The two image capture processes may be performed simultaneously with one another or, alternatively, there may be no time correlation. In some instances, the integrated camera's image capture process may at least partially overlap in time with the detached camera's image capture process while in other instances there may be no overlap in time. Regardless, the integrated camera <b>805</b> generates the first image <b>820</b>, and the detached camera <b>810</b> generates the second image <b>830</b>. Notably, at least a portion of the FOV of the two cameras overlaps such that at least a portion of the second image <b>830</b> overlaps with at least a portion of the first image <b>820</b>.</p><p id="p-0088" num="0087">By way of additional clarification, the dotted circle illustrated in <figref idref="DRAWINGS">FIG. <b>8</b></figref> corresponds to the detached camera's FOV <b>825</b> and the rounded corner dotted rectangle corresponds to the integrated camera's FOV <b>815</b>. In this example scenario, the integrated camera's FOV <b>815</b> entirely consumes or envelopes the detached camera's FOV <b>825</b>.</p><p id="p-0089" num="0088">Returning to <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, method <b>400</b> includes an act (act <b>415</b>) of using the 3D feature map to relocalize a positional framework of the integrated camera based on a first image (e.g., first image <b>820</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>) generated by the integrated camera such that a 6 degree of freedom (<b>6</b> DOF) pose of the integrated camera is determined. Act <b>415</b> may be performed before, during, or even after act <b>410</b> (i.e. the act of sharing the 3D feature map).</p><p id="p-0090" num="0089">Additionally, method <b>400</b> includes an act (act <b>420</b>) of causing the detached camera to use the 3D feature map to relocalize a positional framework of the detached camera based on a second image (e.g., second image <b>830</b> of <figref idref="DRAWINGS">FIG. <b>8</b></figref>) generated by the detached camera such that a <b>6</b> DOF pose of the detached camera is determined. Method act <b>420</b> is performed subsequent to act <b>410</b>, but act <b>420</b> may be performed before, during, or even aft act <b>415</b>. That is, in some cases, the detached camera and the integrated camera may perform a relocalization process (i) at the same, (ii) during overlapping time periods, or (iii) during nonoverlapping time periods. <figref idref="DRAWINGS">FIG. <b>9</b></figref> more fully clarifies what is meant by relocalization.</p><p id="p-0091" num="0090">Generally, relocalization refers to the process of determining a camera's <b>6</b> DOF pose relative to an environment in order to enable the camera to rely on a baseline coordinate system used for that environment. In the context of the detached and integrated cameras, the detached camera is able to receive the 3D feature map from the HMD. Based on the detached camera's image (i.e. the second image <b>830</b> from <figref idref="DRAWINGS">FIG. <b>8</b></figref>), the detached camera can identify feature points within the second image and correlate those feature points with the feature points identified in the 3D feature map. Once those correlations are identified, then the detached camera obtains or generates an understanding of the scene or environment geometry. The detached camera then determines or computes a geometric transform (e.g., a rotational transform) to determine where the detached camera is physically located relative to the detected feature points (e.g., by determining a full 6 degree of freedom (<b>6</b> DOF) pose).</p><p id="p-0092" num="0091">Stated differently, relocalization refers to the process of matching feature points between the 3D feature map and an image and then computing a geometric translation or transform to determine where that camera physically is relative to the environment based on the 3D feature map and the current image. Performing the relocalization enables both the detached camera and the integrated camera to rely on the same coordinate system. <figref idref="DRAWINGS">FIG. <b>9</b></figref> shows a relocalization process that is performed by both the integrated camera and the detached camera.</p><p id="p-0093" num="0092">Specifically, <figref idref="DRAWINGS">FIG. <b>9</b></figref> shows a 3D feature map <b>900</b> and an image frame <b>905</b>. The 3D feature map <b>900</b> is representative of the 3D feature map <b>600</b> of <figref idref="DRAWINGS">FIG. <b>6</b></figref> and the other 3D feature maps discussed thus far. If the integrated camera is performing the relocalization process, then the image frame <b>905</b> corresponds to the first image <b>820</b> from <figref idref="DRAWINGS">FIG. <b>8</b></figref>. On the other hand, if the detached camera is performing the relocalization process, then the image frame <b>905</b> corresponds to the second image <b>830</b>. Notably, the integrated camera and the detached camera independently perform their own respective relocalization processes, which processes are generally the same and which are outlined in <figref idref="DRAWINGS">FIG. <b>9</b></figref>.</p><p id="p-0094" num="0093">The 3D feature map <b>900</b> and the image frame <b>905</b> are fed as inputs into the relocalize <b>910</b> operation. The relocalize <b>910</b> operation relocalizes the positional framework <b>915</b> of the camera (e.g., either the integrated camera or the detached camera) based on correspondences between feature points detected in the image frame <b>905</b> and feature points included in the 3D feature map <b>900</b>. Simultaneous Location and Mapping (SLAM) (e.g., SLAM <b>920</b>) techniques may also be used to relocalize the camera systems within the same physical space (i.e. the HMD space). The SLAM techniques use imagery from cameras to make maps that act as the frame of reference for the physical system.</p><p id="p-0095" num="0094">Historically, SLAM techniques have been used to allow multiple users to visualize holographic content in a scene. The disclosed embodiments may be configured to use SLAM to relocalize the position of a remote camera (i.e. the detached camera) with respect to an HMD-mounted camera (i.e. the integrated camera). By using SLAM from the remote camera system and the HMD-mounted system, the embodiments are able to determine the relative and absolute positions of the two camera systems. Consequently, the result of the relocalize <b>910</b> operation is a 6 DOF pose <b>925</b> of the camera (e.g., the detached camera and separately the integrated camera). By determining the <b>6</b> DOF pose <b>925</b>, the embodiments enable the two camera systems to effectively operate using the same coordinate system <b>930</b>. By <b>6</b> DOF pose <b>925</b>, it is meant that the embodiments are able to determine the camera's angular placement (e.g., yaw, pitch, roll), and translational placement (e.g., forward/backward, left/right, and up/down) in the environment.</p><p id="p-0096" num="0095">Accordingly, the embodiments are able to use the 3D feature map to relocalize the positional framework of the integrated camera into an HMD physical space. This relocalization process is performed by identifying feature points in the first image and feature points in the 3D feature map. The embodiments then attempt to make correlations or matches between those two sets of feature points. Once a sufficient number of matches are made, then the embodiments are able to use that information to determine the <b>6</b> DOF pose of the integrated camera.</p><p id="p-0097" num="0096">Similarly, the detached camera is able to use the 3D feature map to relocalize its positional framework into the HMD space. This relocalization process is performed in the same manner. That is, the detached camera identifies feature points in the second image and feature points in the 3D feature map. The detached camera then attempts to make correlations or matches between those two sets of feature points. Because the FOV of the detached camera at least partially overlaps the FOV of the integrated camera, the second image should include at least a few of the same feature points as are included in the first image. Consequently, the detached camera is able to identify matches between feature points (some of which are the same as were detected in the first image), thereby enabling it to also determine its <b>6</b> DOF pose. In this regard, the detached camera is able to determine its <b>6</b> DOF pose based at least partially on some of the same identified feature points that were used by the integrated camera to determine its <b>6</b> DOF pose. As a consequence of causing the detached camera to use the 3D feature map to relocalize the positional framework of the detached camera (e.g., into the HMD space), the detached camera will then be able to use the same coordinate system as the integrated camera.</p><p id="p-0098" num="0097">Stated differently, the detached camera and the integrated camera compute rotation base matrices detailing the angular and translational differences between the perspectives embodied in the respective images relative to the environment (e.g., the feature points detected in the environment) and relative to one another. In this regard, the rotation base matrices provide a mapping on the translational or angular movement to map the feature points detected in the images to the feature points included in the 3D feature map. The mapping enables the system to determine which translational and angular translations are needed to transition from the perspective of the first image to the perspective of the second image, and vice versa. The process of causing the detached and integrated cameras to use the 3D feature map to relocalize their positional frameworks (e.g., into the HMD space) may include performing a simultaneous location and mapping (SLAM) operation to determine a relative position between the detached camera and the integrated camera.</p><p id="p-0099" num="0098">Returning to <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, method <b>400</b> then includes an act (act <b>425</b>) where the HMD receives (i) the second image of the environment from the detached camera and (ii) the <b>6</b> DOF pose of the detached camera from the detached camera. As a consequence, the HMD now includes data detailing the <b>6</b> DOF pose of the detached camera, the detached camera's image (i.e. the second image), the <b>6</b> DOF pose of the integrated camera, and the integrated camera's image (i.e. the first image). <figref idref="DRAWINGS">FIG. <b>10</b></figref> is illustrative of method act <b>425</b>.</p><p id="p-0100" num="0099"><figref idref="DRAWINGS">FIG. <b>10</b></figref> shows an HMD <b>1000</b> and a detached camera <b>1005</b>, each of which is representative of its counterparts mentioned herein. There is a broadband radio connection <b>1010</b> between the HMD <b>1000</b> and the detached camera <b>1005</b>, as was described in <figref idref="DRAWINGS">FIG. <b>7</b></figref>. In this case, the detached camera <b>1005</b> is transmitting a <b>6</b> DOF pose <b>1015</b> and a second image <b>1020</b> to the HMD <b>1000</b>. Here, the <b>6</b> DOF pose <b>1015</b> corresponds to the <b>6</b> DOF pose <b>925</b> (when computed for the detached camera), and the second image <b>1020</b> corresponds to the second image <b>830</b> from <figref idref="DRAWINGS">FIG. <b>8</b></figref>. In some cases, the <b>6</b> DOF pose <b>1015</b> and the second image <b>1020</b> may be transmitted using the same transmission bursts while in other cases the two pieces of information may be transmitted in separate and independent transmission bursts.</p><p id="p-0101" num="0100">Depth Maps</p><p id="p-0102" num="0101">As a result of performing the method acts <b>405</b> through <b>425</b>, the HMD now includes the information detailed in <figref idref="DRAWINGS">FIG. <b>11</b></figref>. Specifically, the HMD <b>1100</b> includes a first image <b>1105</b>, a <b>6</b> DOF pose <b>1110</b> of the integrated camera, a second image <b>1115</b>, and a <b>6</b> DOF pose <b>1120</b> of the detached camera. Each of these elements corresponds to its respective element discussed herein. Additionally, the HMD <b>1100</b> is able to generate, access, or acquire a depth map <b>1125</b> of the environment. To clarify, as recited in method act <b>430</b> illustrated in <figref idref="DRAWINGS">FIG. <b>4</b>B</figref>, method <b>400</b> includes an act (act <b>430</b>) of accessing a depth map (e.g., depth map <b>1125</b>) of the environment.</p><p id="p-0103" num="0102">As used herein, a &#x201c;depth map&#x201d; details the positional relationship and depths relative to objects in the environment. Consequently, the positional arrangement, location, geometries, contours, and depths of objects relative to one another can be determined. As shown in <figref idref="DRAWINGS">FIG. <b>12</b></figref>, the depth map <b>1125</b> may be computed in different ways.</p><p id="p-0104" num="0103">Specifically, the depth map <b>1200</b> of <figref idref="DRAWINGS">FIG. <b>12</b></figref> is representative of the depth map <b>1125</b>. In some cases, the depth map <b>1200</b> may be computed using a range finder <b>1205</b>. In some cases, the depth map <b>1200</b> may be computed by performing stereoscopic depth matching <b>1210</b>. The ellipsis <b>1215</b> shows how the depth map <b>1200</b> may be computed using other techniques and is not limited to the two illustrated in <figref idref="DRAWINGS">FIG. <b>12</b></figref>. In some implementations, the depth map <b>1200</b> may be a full and complete depth map in which a corresponding depth value is assigned for every pixel in the depth map. In some implementations, the depth map <b>1200</b> may be a single pixel depth map. In some implementations, the depth map may be a planar depth map where every pixel in the depth map is assigned the same depth value. In any event, the depth map <b>1125</b> of <figref idref="DRAWINGS">FIG. <b>11</b></figref> represents one or more depths of objects located in the environment. Notably, the depth of the center of the secondary camera can also be determined by a rangefinder/single pixel measurement system. The embodiments are able to overlay the two camera images based on the <b>6</b> DOF pose plus the single pixel depth information.</p><p id="p-0105" num="0104">Returning to <figref idref="DRAWINGS">FIG. <b>11</b></figref>, if the first image <b>1105</b>, the <b>6</b> DOF pose <b>1110</b>, the second image <b>1115</b>, and the <b>6</b> DOF pose <b>1120</b> were computed prior to a subsequent movement of either the integrated camera and/or the detached camera, then the embodiments are able to update those pieces of data using inertial measurement unit (IMU) data <b>1130</b> obtained from an IMU <b>1135</b>. To clarify, the integrated camera may be associated with its own corresponding IMU, and the detached camera may be associated with its own corresponding IMU. These two IMUs are able to generate IMU data, as represented by the IMU data <b>1130</b>. The detached camera is able to transmit its IMU data to the HMD.</p><p id="p-0106" num="0105">If the previously described rotational/rotation base matrices (computed during the relocalization process) were calculated prior to a subsequent movement of any of the integrated or detached cameras, the embodiments are able to utilize the IMU data <b>1130</b> to update the respective rotational base matrices to account for the new movement. For instance, by multiplying the integrated camera's rotational base matrix against matrix data generated based on the IMU data <b>1130</b>, the embodiments are able to undo the effects of movement of the integrated camera. Similarly, by multiplying the detached camera's rotational base matrix against matrix data generated based on its corresponding IMU data, the embodiments are able to undo the effects of movement of the detached camera. Accordingly, the <b>6</b> DOF pose <b>1110</b> and the <b>6</b> DOF pose <b>1120</b> may be updated based on subsequently obtained IMU data. Stated differently, the embodiments are able to update the <b>6</b> DOF pose of the integrated camera (or detached camera) based on a detected movement of the integrated camera (or detached camera). The detected movement may be detected based on IMU data obtained from an IMU of the integrated camera (or detached camera).</p><heading id="h-0009" level="1">Generating An Overlaid Image</heading><p id="p-0107" num="0106">Returning to <figref idref="DRAWINGS">FIG. <b>4</b>B</figref>, method <b>400</b> then includes an act (act <b>435</b>) of generating an overlaid image by reprojecting a perspective of the second image to align with a perspective of the first image (e.g., the reprojection occurs using the two 6 DOF poses and the depth map previously discussed). After the perspectives are aligned, the embodiments overlay at least a portion (and potentially all) of the reprojected second image onto the first image. To clarify, the <b>6</b> DOF pose of the integrated camera, the <b>6</b> DOF pose of the detached camera, and the depth map are used to perform the reprojection operation. Of course, the detached camera's image (i.e. the second image) is also used to perform the reprojection operation. <figref idref="DRAWINGS">FIG. <b>13</b></figref> is illustrative of the reprojection operation in which a perspective of the second image is reprojected so as to align, match, or coincide with the perspective of the first image. By making this alignment, the embodiments can then selectively overlay portions of the second image onto the first image while ensuring accurate alignment between the two images' contents.</p><p id="p-0108" num="0107"><figref idref="DRAWINGS">FIG. <b>13</b></figref> shows a second image <b>1300</b>, which is representative of the second images discussed thus far. The second image includes a 2D keypoint <b>1305</b>A and a corresponding 3D point <b>1310</b> for that 2D keypoint <b>1305</b>A. After determining the intrinsic camera parameters <b>1315</b> (e.g., the camera's focal length, the principle point, and the lens distortion) and the extrinsic camera parameters <b>1320</b>A (e.g., the position and orientation of the camera, or rather the <b>6</b> DOF pose of the camera), the embodiments are able to perform a reprojection <b>1325</b> operation on the second image <b>1300</b> to reproject a perspective embodied by that image to a new perspective, where the new perspective matches the perspective of the first image (so the second image can then be accurately overlaid onto the first image).</p><p id="p-0109" num="0108">For instance, as a result of performing the reprojection <b>1325</b> operation, the reprojected image <b>1330</b> is generated, where the reprojected image <b>1330</b> includes a 2D keypoint <b>1305</b>B corresponding to the 2D keypoint <b>1305</b>A. In effect, the reprojection <b>1325</b> operation produces a synthetic camera having new extrinsic camera parameters <b>1320</b>B so as to give the illusion that the reprojected image <b>1330</b> was captured by the synthetic camera at the new perspective (e.g., at the same location as the integrated camera). In this regard, reprojecting the second image (or at least a portion of the second image) compensates for a distance separating the detached camera from the integrated camera (e.g., distance <b>130</b> from <figref idref="DRAWINGS">FIG. <b>1</b></figref>) and also compensates for pose or perspective differences between the two cameras.</p><p id="p-0110" num="0109">Accordingly, the embodiments reproject the second image to a new perspective in order to align the perspective of the second image with the perspective of the first image. Further details are illustrated in <figref idref="DRAWINGS">FIG. <b>14</b></figref>.</p><p id="p-0111" num="0110"><figref idref="DRAWINGS">FIG. <b>14</b></figref> shows how the <b>6</b> DOF pose <b>1400</b> of the detached camera, the second image <b>1405</b>, the <b>6</b> DOF pose <b>1410</b> of the integrated camera, and the depth map <b>1415</b> (i.e. the depth map <b>1125</b>) are fed as inputs into the reprojection <b>1420</b> operation (i.e. reprojection <b>1325</b> from <figref idref="DRAWINGS">FIG. <b>13</b></figref>) in order to produce the reprojected image <b>1425</b> (i.e. the reprojected image <b>1330</b> from <figref idref="DRAWINGS">FIG. <b>13</b></figref>). As a result of performing the reprojection <b>1420</b> operation, the perspective embodied by the reprojected image <b>1425</b> matches the perspective of the integrated camera (e.g., either the first camera <b>115</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> or the second camera <b>120</b>). In some cases, the disclosed operations are performed twice, with one operation being performed for the first camera <b>115</b> and the second operation being performed for the second camera <b>120</b> so as to produce two separate passthrough images.</p><p id="p-0112" num="0111">The reprojected image <b>1425</b> of <figref idref="DRAWINGS">FIG. <b>14</b></figref> is further illustrated as reprojected image <b>1500</b> in <figref idref="DRAWINGS">FIG. <b>15</b></figref>. Now that the reprojected image <b>1500</b> has a perspective corresponding to the perspective of the first image, the embodiments are able to perform an overlay <b>1505</b> operation to generate an overlaid image <b>1510</b>. To clarify, the embodiments generate the overlaid image <b>1510</b> by merging or fusing pixels from the first image (i.e. the first image pixels <b>1515</b>) with pixels from the reprojected image <b>1500</b> (i.e. the second image pixels <b>1520</b>). Stated differently, one or more portions from the reprojected image <b>1500</b> are overlaid onto the first image to form the overlaid image <b>1510</b>. The second image pixels <b>1520</b> are properly aligned with the underlying first image pixels <b>1515</b> as a result of performing the earlier reprojection operation on the second image.</p><p id="p-0113" num="0112">For instance, the reprojected image <b>1500</b> shows a man with a baseball cap and the back of a woman. The first image (e.g., see the first image <b>1105</b> illustrated in <figref idref="DRAWINGS">FIG. <b>11</b></figref>) also included the same content. It is beneficial to overlay the second image content onto the first image content for a number of reasons.</p><p id="p-0114" num="0113">For instance, because the sizes of the FOVs of the different cameras may be different, the size of the resulting images may also be different. Despite the sizes being different, the resolutions may still be the same. For instance, <figref idref="DRAWINGS">FIG. <b>11</b></figref> shows how the second image <b>1115</b> is smaller than the first image <b>1105</b>. Notwithstanding this difference in size, the resolutions may all be the same. Consequently, each pixel included in the second image <b>1115</b> is smaller and provides a heightened level of detail as compared to each pixel in the first image <b>1105</b>.</p><p id="p-0115" num="0114">Accordingly, in some embodiments, the resolution of the second image <b>1115</b> may be the same as the resolution of the first image <b>1105</b> such that, as a result of the FOV of the second image <b>1115</b> being smaller than the FOV of the first image <b>1105</b>, each pixel in the second image <b>1115</b> is smaller than each pixel in the first image <b>1105</b>. Consequently, the pixels of the second image <b>1115</b> will give content a sharper, clearer, or more crisp visualization as compared to pixels of the first image <b>1105</b>. Therefore, by overlaying the second image content onto the first image content, the section included within the boundary <b>1525</b> of <figref idref="DRAWINGS">FIG. <b>15</b></figref> (corresponding to the second image content) may appear to be clearer or of higher detail than other portions of the overlaid image <b>1510</b> (e.g., those pixels corresponding to the first image content). Therefore, by overlaying content, enhanced images may be generated.</p><heading id="h-0010" level="1">Parallax Correction</heading><p id="p-0116" num="0115">Returning to <figref idref="DRAWINGS">FIG. <b>4</b>B</figref>, method <b>400</b> includes an optional (as indicated by the dotted box) act (act <b>440</b>) of performing parallax correction on the overlaid image to modify a perspective of the overlaid image to correspond to a novel perspective. In some implementations (though not all), the novel perspective is a perspective of a pupil of a user wearing the HMD (e.g., pupil <b>330</b> or pupil <b>335</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>). Method <b>400</b> includes another optional act (act <b>445</b>) of displaying the overlaid image for the user to view.</p><p id="p-0117" num="0116">The computer system implementing the disclosed operations (including method <b>400</b>) may be a head-mounted device (HMD) worn by a user. The new perspective may correspond to one of a left eye pupil or a right eye pupil. If a second overlaid image is generated, then the second overlaid image may also be parallax corrected to a second new perspective, where the second new perspective may correspond to the other one of the left eye pupil or the right eye pupil. <figref idref="DRAWINGS">FIG. <b>16</b></figref> provides some additional clarification regarding the parallax correction operation.</p><p id="p-0118" num="0117"><figref idref="DRAWINGS">FIG. <b>16</b></figref> shows an overlaid image <b>1600</b>, which may be the overlaid image <b>1510</b> from <figref idref="DRAWINGS">FIG. <b>15</b></figref> and which may be the overlaid images discussed in method <b>400</b>. Here, the overlaid image <b>1600</b> is shown as having an original perspective <b>1605</b>. In accordance with the disclosed principles, the embodiments are able to perform a parallax correction <b>1610</b> to transform the original perspective <b>1605</b> of the overlaid image <b>1600</b> into a new or novel perspective. It should be noted how the pixels that were taken from the detached camera image are then subjected to two separate reprojection operations, one involving modifying the perspective of the detached camera image to coincide with the perspective of the integrated camera and one involving modifying the perspective of the overlaid image to coincide with the perspective of the user's pupil.</p><p id="p-0119" num="0118">Performing the parallax correction <b>1610</b> involves the use of a depth map in order to reproject the image content to a new perspective. This depth map may be the same or may be different from the depth maps mentioned earlier. In some cases, the depth map is an updated version of the previous depth map to reflect the current positioning and pose of the HMD. In some cases, the depth map is a new depth map generated for the purpose of performing the parallax correction.</p><p id="p-0120" num="0119">The parallax correction <b>1610</b> is shown as including any one or more of a number of different operations. For instance, the parallax correction <b>1610</b> may involve distortion corrections <b>1615</b> (e.g., to correct for concave or convex wide or narrow angled camera lenses), epipolar transforms <b>1620</b> (e.g., to parallelize the optical axes of the cameras), and/or reprojection transforms <b>1625</b> (e.g., to reposition the optical axes so as to be essentially in front of or in-line with the user's pupils). The parallax correction <b>1610</b> includes performing depth computations to determine the depth of the environment and then reprojecting images to a determined location or as having a determined perspective. As used herein, the phrases &#x201c;parallax correction&#x201d; and &#x201c;image synthesis&#x201d; may be interchanged with one another and may include performing stereo passthrough parallax correction and/or image reprojection parallax correction.</p><p id="p-0121" num="0120">The reprojections are based on the original perspective <b>1605</b> of the overlaid image <b>1600</b> relative to the surrounding environment. Based on the original perspective <b>1605</b> and the depth maps that are generated, the embodiments are able to correct parallax by reprojecting a perspective embodied by the overlaid images to coincide with a new perspective, as shown by the parallax-corrected image <b>1630</b> and the new perspective <b>1635</b>. In some embodiments, the new perspective <b>1635</b> may be one of the user's pupils <b>330</b> and <b>335</b> from <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0122" num="0121">Some embodiments perform three-dimensional (3D) geometric transforms on the overlaid images to transform the perspectives of the overlaid images in a manner so as to correlate with the perspectives of the user's pupils <b>330</b> and <b>335</b>. Additionally, the 3D geometric transforms rely on depth computations in which the objects in the HMD's environment are mapped out to determine their depths as well as the perspective. Based on these depth computations and perspective, the embodiments are able to three-dimensionally reproject or three-dimensionally warp the overlaid images in such a way so as to preserve the appearance of object depth in the parallax-corrected image <b>1630</b> (i.e. a type of passthrough image), where the preserved object depth substantially matches, corresponds, or visualizes the actual depths of objects in the real world. Accordingly, the degree or amount of the parallax correction <b>1610</b> is at least partially dependent on the degree or amount of the offsets <b>340</b> and <b>345</b> from <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0123" num="0122">By performing the parallax correction <b>1610</b>, the embodiments effectively create &#x201c;virtual&#x201d; cameras having positions that are in front of the user's pupils <b>330</b> and <b>335</b>. By way of additional clarification, consider the position of camera <b>305</b> from <figref idref="DRAWINGS">FIG. <b>3</b></figref>, which is currently above and to the left of the pupil <b>335</b>. By performing the parallax correction, the embodiments programmatically transform images generated by camera <b>305</b>, or rather the perspectives of those images, so the perspectives appear as though camera <b>305</b> were actually positioned immediately in front of pupil <b>335</b>. That is, even though camera <b>305</b> does not actually move, the embodiments are able to transform images generated by camera <b>305</b> so those images have the appearance as if camera <b>305</b> were positioned in front of pupil <b>335</b>.</p><p id="p-0124" num="0123">In some cases, the parallax correction <b>1610</b> relies on a full depth map to perform the reprojections while in other cases the parallax correction <b>1610</b> relies on a planar depth map to perform the reprojections. In some embodiments, the parallax correction <b>1610</b> relies on a one-pixel depth map (e.g., a one-pixel depth measurement for each camera frame), such as a depth map that is generated by a one-pixel range finder.</p><p id="p-0125" num="0124">When performing a reprojection using a full depth map on the overlaid image, it is sometimes beneficial to attribute a single depth to all of the pixels bounded by the dotted circle in the parallax-corrected image <b>1630</b>. Not doing so may result in skewing or warping of the parallax corrected region corresponding to the bounded pixels. For instance, instead of resulting in a circle of pixels, not using a single common depth for the pixels in the circle may result in an oval or other skewing effects. Accordingly, some embodiments determine a depth corresponding to the depth of a particular pixel (e.g., perhaps the center pixel of the circle) and then attribute that single depth to all of the pixels bounded by the circle. To clarify, all of the pixels bounded by the circle may be given the same depth value.</p><p id="p-0126" num="0125">The full depth map is then used to perform the reprojections involved in the parallax correction operations discussed earlier. By attributing the same depth to all of the pixels bounded by the circle in the overlaid image, the embodiments prevent skewing from occurring on that image content as a result of performing parallax correction.</p><p id="p-0127" num="0126">While most embodiments select the depth corresponding to the center pixel, some embodiments may be configured to select a depth of a different pixel bounded by the circle. As such, using the depth of the center pixel is simply one example implementation, but it is not the only implementation. Some embodiments select a number of pixels that are centrally located and then use the average depth of those pixels. Some embodiments select an off-center pixel or group of pixel's depth.</p><p id="p-0128" num="0127">Instead of using a full depth map to perform reprojections, some embodiments use a fixed depth map to perform a fixed depth map reprojection. In this case, the embodiments select the depth of a particular pixel from the pixels bounded by the circle (e.g., perhaps again the center pixel). Based on the selected depth, the embodiments then attribute that single depth to all of the pixels of a depth map to generate the fixed depth map. To clarify, all of the depth pixels in the fixed depth map are assigned or attributed the same depth, which is the depth of the selected pixel (e.g., perhaps the center pixel or perhaps some other selected pixel).</p><p id="p-0129" num="0128">Once the fixed depth map is generated, this depth map may then be used to perform a reprojection (e.g., a planar reprojection) on the overlaid image using the fixed depth map. In this regard, reprojecting the overlaid image (e.g., overlaid image <b>1600</b> from <figref idref="DRAWINGS">FIG. <b>16</b></figref>) to generate parallax-corrected image <b>1630</b> may be performed using a full depth map or a fixed depth map.</p><p id="p-0130" num="0129">Accordingly, the disclosed embodiments are able to align images by using 6 DOF poses to perform a reprojection in order to align the images to have matching perspectives. The embodiments then perform parallax correction on the aligned overlaid images in order to generate passthrough images having new perspectives. Such operations significantly enhance the quality of images by enabling new and dynamic image content to be displayed.</p><heading id="h-0011" level="1">Example Computer/Computer Systems</heading><p id="p-0131" num="0130">Attention will now be directed to <figref idref="DRAWINGS">FIG. <b>17</b></figref> which illustrates an example computer system <b>1700</b> that may include and/or be used to perform any of the operations described herein. Computer system <b>1700</b> may take various different forms. For example, computer system <b>1700</b> may be embodied as a tablet <b>1700</b>A, a desktop or laptop <b>1700</b>B, a wearable device <b>1700</b>C (e.g., such as any of the disclosed HMDs), a mobile device, a standalone device, or any other embodiment as shown by the ellipsis <b>1700</b>D. Computer system <b>1700</b> may also be a distributed system that includes one or more connected computing components/devices that are in communication with computer system <b>1700</b>.</p><p id="p-0132" num="0131">In its most basic configuration, computer system <b>1700</b> includes various different components. <figref idref="DRAWINGS">FIG. <b>17</b></figref> shows that computer system <b>1700</b> includes one or more processor(s) <b>1705</b> (aka a &#x201c;hardware processing unit&#x201d;), scanning sensor(s) <b>1710</b> (e.g., such as the scanning sensor(s) <b>205</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>), an image processing engine <b>1715</b>, and storage <b>1720</b>.</p><p id="p-0133" num="0132">Regarding the processor(s) <b>1705</b>, it will be appreciated that the functionality described herein can be performed, at least in part, by one or more hardware logic components (e.g., the processor(s) <b>1705</b>). For example, and without limitation, illustrative types of hardware logic components/processors that can be used include Field-Programmable Gate Arrays (&#x201c;FPGA&#x201d;), Program-Specific or Application-Specific Integrated Circuits (&#x201c;ASIC&#x201d;), Program-Specific Standard Products (&#x201c;ASSP&#x201d;), System-On-A-Chip Systems (&#x201c;SOC&#x201d;), Complex Programmable Logic Devices (&#x201c;CPLD&#x201d;), Central Processing Units (&#x201c;CPU&#x201d;), Graphical Processing Units (&#x201c;GPU&#x201d;), or any other type of programmable hardware.</p><p id="p-0134" num="0133">Any type of depth detection may be utilized by the computer system <b>1700</b> and by the scanning sensor(s) <b>1710</b>. Examples include, but are not limited to, stereoscopic depth detection (both active illumination (e.g., using a dot illuminator), structured light illumination (e.g., <b>1</b> actual camera, <b>1</b> virtual camera, and 1 dot illuminator), and passive (i.e. no illumination)), time of flight depth detection (with a baseline between the laser and the camera, where the field of view of the camera does not perfectly overlap the field of illumination of the laser), range finder depth detection, or any other type of range or depth detection.</p><p id="p-0135" num="0134">The image processing engine <b>1715</b> may be configured to perform any of the method acts discussed in connection with method <b>400</b> of <figref idref="DRAWINGS">FIGS. <b>4</b>A and <b>4</b>B</figref>. In some instances, the image processing engine <b>1715</b> includes a ML algorithm. That is, ML may also be utilized by the disclosed embodiments, as discussed earlier. ML may be implemented as a specific processing unit (e.g., a dedicated processing unit as described earlier) configured to perform one or more specialized operations for the computer system <b>1700</b>. As used herein, the terms &#x201c;executable module,&#x201d; &#x201c;executable component,&#x201d; &#x201c;component,&#x201d; &#x201c;module,&#x201d; &#x201c;model,&#x201d; or &#x201c;engine&#x201d; can refer to hardware processing units or to software objects, routines, or methods that may be executed on computer system <b>1700</b>. The different components, modules, engines, models, and services described herein may be implemented as objects or processors that execute on computer system <b>1700</b> (e.g. as separate threads). ML models and/or the processor(s) <b>1705</b> can be configured to perform one or more of the disclosed method acts or other functionalities.</p><p id="p-0136" num="0135">Storage <b>1720</b> may be physical system memory, which may be volatile, non-volatile, or some combination of the two. The term &#x201c;memory&#x201d; may also be used herein to refer to non-volatile mass storage such as physical storage media. If computer system <b>1700</b> is distributed, the processing, memory, and/or storage capability may be distributed as well.</p><p id="p-0137" num="0136">Storage <b>1720</b> is shown as including executable instructions (i.e. code <b>1725</b>). The executable instructions represent instructions that are executable by the processor(s) <b>1705</b> (or perhaps even the image processing engine <b>1715</b>) of computer system <b>1700</b> to perform the disclosed operations, such as those described in the various methods.</p><p id="p-0138" num="0137">The disclosed embodiments may comprise or utilize a special-purpose or general-purpose computer including computer hardware, such as, for example, one or more processors (such as processor(s) <b>1705</b>) and system memory (such as storage <b>1720</b>), as discussed in greater detail below. Embodiments also include physical and other computer-readable media for carrying or storing computer-executable instructions and/or data structures. Such computer-readable media can be any available media that can be accessed by a general-purpose or special-purpose computer system. Computer-readable media that store computer-executable instructions in the form of data are &#x201c;physical computer storage media&#x201d; or a &#x201c;hardware storage device.&#x201d; Computer-readable media that carry computer-executable instructions are &#x201c;transmission media.&#x201d; Thus, by way of example and not limitation, the current embodiments can comprise at least two distinctly different kinds of computer-readable media: computer storage media and transmission media.</p><p id="p-0139" num="0138">Computer storage media (aka &#x201c;hardware storage device&#x201d;) are computer-readable hardware storage devices, such as RAM, ROM, EEPROM, CD-ROM, solid state drives (&#x201c;SSD&#x201d;) that are based on RAM, Flash memory, phase-change memory (&#x201c;PCM&#x201d;), or other types of memory, or other optical disk storage, magnetic disk storage or other magnetic storage devices, or any other medium that can be used to store desired program code means in the form of computer-executable instructions, data, or data structures and that can be accessed by a general-purpose or special-purpose computer.</p><p id="p-0140" num="0139">Computer system <b>1700</b> may also be connected (via a wired or wireless connection) to external sensors (e.g., one or more remote cameras) or devices via a network <b>1730</b>. For example, computer system <b>1700</b> can communicate with any number devices or cloud services to obtain or process data. In some cases, network <b>1730</b> may itself be a cloud network. Furthermore, computer system <b>1700</b> may also be connected through one or more wired or wireless networks <b>1730</b> to remote/separate computer systems(s) that are configured to perform any of the processing described with regard to computer system <b>1700</b>.</p><p id="p-0141" num="0140">A &#x201c;network,&#x201d; like network <b>1730</b>, is defined as one or more data links and/or data switches that enable the transport of electronic data between computer systems, modules, and/or other electronic devices. When information is transferred, or provided, over a network (either hardwired, wireless, or a combination of hardwired and wireless) to a computer, the computer properly views the connection as a transmission medium. Computer system <b>1700</b> will include one or more communication channels that are used to communicate with the network <b>1730</b>. Transmissions media include a network that can be used to carry data or desired program code means in the form of computer-executable instructions or in the form of data structures. Further, these computer-executable instructions can be accessed by a general-purpose or special-purpose computer. Combinations of the above should also be included within the scope of computer-readable media.</p><p id="p-0142" num="0141">Upon reaching various computer system components, program code means in the form of computer-executable instructions or data structures can be transferred automatically from transmission media to computer storage media (or vice versa). For example, computer-executable instructions or data structures received over a network or data link can be buffered in RAM within a network interface module (e.g., a network interface card or &#x201c;NIC&#x201d;) and then eventually transferred to computer system RAM and/or to less volatile computer storage media at a computer system. Thus, it should be understood that computer storage media can be included in computer system components that also (or even primarily) utilize transmission media.</p><p id="p-0143" num="0142">Computer-executable (or computer-interpretable) instructions comprise, for example, instructions that cause a general-purpose computer, special-purpose computer, or special-purpose processing device to perform a certain function or group of functions. The computer-executable instructions may be, for example, binaries, intermediate format instructions such as assembly language, or even source code. Although the subject matter has been described in language specific to structural features and/or methodological acts, it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the described features or acts described above. Rather, the described features and acts are disclosed as example forms of implementing the claims.</p><p id="p-0144" num="0143">Those skilled in the art will appreciate that the embodiments may be practiced in network computing environments with many types of computer system configurations, including personal computers, desktop computers, laptop computers, message processors, hand-held devices, multi-processor systems, microprocessor-based or programmable consumer electronics, network PCs, minicomputers, mainframe computers, mobile telephones, PDAs, pagers, routers, switches, and the like. The embodiments may also be practiced in distributed system environments where local and remote computer systems that are linked (either by hardwired data links, wireless data links, or by a combination of hardwired and wireless data links) through a network each perform tasks (e.g. cloud computing, cloud services and the like). In a distributed system environment, program modules may be located in both local and remote memory storage devices.</p><p id="p-0145" num="0144">The present invention may be embodied in other specific forms without departing from its spirit or characteristics. The described embodiments are to be considered in all respects only as illustrative and not restrictive. The scope of the invention is, therefore, indicated by the appended claims rather than by the foregoing description. All changes which come within the meaning and range of equivalency of the claims are to be embraced within their scope.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method for aligning an image generated by a first camera with an image generated by a second camera, said method comprising:<claim-text>accessing a feature map of an environment in which both the first camera and the second camera are operating;</claim-text><claim-text>obtaining a first image generated by the first camera;</claim-text><claim-text>obtaining a second image generated by the second camera;</claim-text><claim-text>using the feature map to determine a first pose of the first camera as represented within the first image;</claim-text><claim-text>obtaining a second pose of the second camera as represented within the second image; and</claim-text><claim-text>providing an overlaid image by reprojecting a perspective of the second image to align with a perspective of the first image and by overlaying at least a portion of the reprojected second image onto the first image, wherein the first pose of the first camera and the second pose of the second camera are used to perform said reprojecting.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first camera is one of a visible light camera, a low light camera, or a thermal imaging camera.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the second camera is one of a visible light camera, a low light camera, or a thermal imaging camera.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first camera and the second camera are of a same modality.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the feature map is a three dimensional (3D) feature map.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the feature map includes a feature point, and wherein the feature point is identified using machine learning.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein a field of view of the first camera is larger than a field of view of the second camera.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the first image has a same resolution as the second image.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the second image is obtained over a broadband radio connection.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the second pose and the second image are obtained using a same transmission burst.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. A computer system that aligns an image generated by a first camera with an image generated by a second camera, said computer system comprising:<claim-text>at least one processor; and</claim-text><claim-text>at least one hardware storage device that stores instructions that are executable by the at least one processor to cause the computer system to:<claim-text>access a feature map of an environment in which both the first camera and the second camera are operating;</claim-text><claim-text>obtain a first image generated by the first camera;</claim-text><claim-text>obtain a second image generated by the second camera;</claim-text><claim-text>use the feature map to determine a first pose of the first camera as represented within the first image;</claim-text><claim-text>obtain a second pose of the second camera as represented within the second image; and</claim-text><claim-text>provide an overlaid image by reprojecting a perspective of the second image to align with a perspective of the first image and by overlaying at least a portion of the reprojected second image onto the first image, wherein the first pose of the first camera and the second pose of the second camera are used to perform said reprojecting.</claim-text></claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The computer system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the second pose is determined using the feature map.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The computer system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein said reprojecting is performed using a depth map.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The computer system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein all of the reprojected second image is overlaid onto the first image.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The computer system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein said reprojecting compensates for a distance that separates the first camera and the second camera.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The computer system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein said reprojecting compensates for a difference that exists between the first pose and the second pose.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The computer system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein a size of the first image is different than a size of the second image.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The computer system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein a perspective of the overlaid image is subsequently modified.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. A method for aligning an image generated by a first camera with an image generated by a second camera, said method comprising:<claim-text>accessing a feature map of an environment in which both the first camera and the second camera are operating;</claim-text><claim-text>obtaining a first image generated by the first camera;</claim-text><claim-text>obtaining a second image generated by the second camera;</claim-text><claim-text>using the feature map to determine a first pose of the first camera as represented within the first image;</claim-text><claim-text>obtaining a second pose of the second camera as represented within the second image;</claim-text><claim-text>providing an overlaid image by reprojecting a perspective of the second image to align with a perspective of the first image and by overlaying at least a portion of the reprojected second image onto the first image, wherein the first pose of the first camera and the second pose of the second camera are used to perform said reprojecting;</claim-text><claim-text>performing parallax correction on the overlaid image; and</claim-text><claim-text>causing the overlaid image to be displayed on a display.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The method of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein the parallax correction is performed using a depth map.</claim-text></claim></claims></us-patent-application>