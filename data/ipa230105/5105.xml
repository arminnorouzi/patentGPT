<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005106A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005106</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17725108</doc-number><date>20220420</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>5</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>5</main-group><subgroup>002</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>10016</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30168</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20081</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30252</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>10048</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">Automated high speed image enhancement algorithm selection and application for infrared videos</invention-title><us-related-documents><us-provisional-application><document-id><country>US</country><doc-number>63177281</doc-number><date>20210420</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only"><addressbook><last-name>Buurma</last-name><first-name>Christopher Frank</first-name><address><city>Columbus</city><state>OH</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant><us-applicant sequence="01" app-type="applicant" designation="us-only"><addressbook><last-name>Grein</last-name><first-name>Christoph</first-name><address><city>Wheaton</city><state>IL</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Buurma</last-name><first-name>Christopher Frank</first-name><address><city>Columbus</city><state>OH</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Grein</last-name><first-name>Christoph</first-name><address><city>Wheaton</city><state>IL</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A method of substantially real-time image restoration of an infrared camera includes the steps of: analyzing the last X number of video frames; classifying the last X number of video frames as to the source of noise in the last X number of video frames; selecting a noise suppression transform based on the source of the noise; receiving real time video frames; correcting the real time video frames using the selected noise suppression transform.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="88.22mm" wi="158.75mm" file="US20230005106A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="176.61mm" wi="103.55mm" orientation="landscape" file="US20230005106A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><p id="p-0002" num="0001">This application claims the benefit of U.S. Provisional Application No. 63/177,281 filed Apr. 20, 2021.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">BACKGROUND OF THE INVENTION</heading><p id="p-0003" num="0002">Driving ground vehicles in heavily degraded visual environments brings significant risk to the driver and cargo since course corrections cannot be made safely without visual cues. Infrared imagery can assist in driving vehicles under such conditions since the obscuring effect of many noise sources are reduced at longer wavelengths, namely the primary loss due to scattering is suppressed and replaced by the emission of the obscurant and its opacity or extinction coefficient. However, this method alone is insufficient when the scene is too heavily degraded with a signal-to-noise ratio below 1 since even imagery in the long-wavelength infrared (LWIR) suffers from significant loss in a heavily degraded visual environment.</p><heading id="h-0002" level="1">SUMMARY</heading><p id="p-0004" num="0003">An exemplary method of substantially real-time image restoration of an infrared camera includes the steps of:</p><p id="p-0005" num="0000">analyzing noise sources in video frames of degraded imagery;<br/>using modeling and machine learning methods, developing metrics to categorize these noise sources present in the video frames of degraded imagery;<br/>using the metrics to rapidly identify an optimal noise removal method;<br/>using the optimal noise removal method, restoring the degraded imagery in subsequent video frames of degraded imagery to repair damage done by the noise sources;<br/>limiting the scope of repair to ensure proper algorithm complexity for high-speed restoring of the degraded imagery.</p><p id="p-0006" num="0004">The step of analyzing noise sources can be done on a preceding number of 120 video frames and the noise removal is done on the succeeding number of video frames. In other words, frame number 121 is enhanced based on the analysis of the preceding 120 video frames.</p><p id="p-0007" num="0005">The exemplary method of the invention automatically selects from a range of image processing algorithms in real time based on goals set by humans. This method uses weakly supervised learning to auto-label a training set based on the quality of the image to achieve a goal. Using less than 1% human labels, the rest of the data is auto-labeled and used to train a decision making classification algorithm for image enhancement selection. The entire workflow including application of the image enhancement occurs in less than 80 ms.</p><p id="p-0008" num="0006">The method extracts imagery in low signal-to-noise environments. Many of the noise sources and obscurants contributing to image degradation can be rapidly categorized and suppressed in near real-time by applying image processing techniques coupled with a machine learning-based classifier. The classifier is first trained through the use of supervised learning techniques applied to labeled datasets containing both human labels and a weakly supervised learning technique. The incoming imagery is initially analyzed to determine noise sources present and their loss signatures. Next, the machine learning classifier uses this data to choose the optimum image restoration technique from its available set of methods. The chosen method is then applied in near real time to incoming video frames and sub-regions of those video frames.</p><p id="p-0009" num="0007">Detailed analysis of the noise sources present in a set of video frames reveals a variety of characteristic noise sources, each with possible suppression transforms. First, one classifies the noise type present in a short sequence of video frames using a classification technique in a supervised machine learning approach (e.g. neural networks). Each noise category can then be analyzed, and methods are adapted to replicate the representative noise in the scene for comparison to the un-degraded scene. With successful noise replication, an associated noise suppression transform can be applied to the video frames and reduce this noise significantly, letting salient parts of the image dramatically increase in contrast. With a careful implementation, language, compiler choice, and proper hardware, this entire process can be done at high speed (&#x3c;80 ms). The major limitation to speed will only be in applying the noise correction transform since the classification method can be trained offline. During operation, periodic noise re-classification can be done in asynchronous with the incoming video frames, in parallel, and over longer timescales to ensure proper image enhancement transforms are applied. Between classifications, the correction transform is still applied in near real-time to the incoming video frames and the enhanced imagery displayed to the user.</p><p id="p-0010" num="0008">Numerous other advantages and features of the present invention will be become readily apparent from the following detailed description of the invention and the embodiments thereof, and from the accompanying drawings.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0011" num="0009"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a flowchart of one exemplary image enhancement method according to the invention.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0004" level="1">DETAILED DESCRIPTION</heading><p id="p-0012" num="0010">While this invention is susceptible of embodiment in many different forms, there are shown in the drawings, and will be described herein in detail, specific embodiments thereof with the understanding that the present disclosure is to be considered as an exemplification of the principles of the invention and is not intended to limit the invention to the specific embodiments illustrated.</p><p id="p-0013" num="0011">This application incorporates by reference U.S. Provisional Application No. 63/177,281 filed Apr. 20, 2021.</p><p id="p-0014" num="0012">A method includes using an image restoration algorithm and classification artificial intelligence (AI) on imagery and within the required execution time to display an image. First, noise sources are analyzed in degraded imagery with special attention paid to the temporal component of the noise. Next, modeling and machine learning methods use these developed metrics to categorize these noise sources present in imagery, allowing for rapid identification of the optimal noise removal method. Next, image restoration methods are implemented so they can repair damage done by such identified noise sources. Lastly, a limited scope implementation is done to ensure proper algorithm complexity for high-speed application of these image enhancements to remain suitable for use when driving a vehicle.</p><p id="p-0015" num="0013"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates this method. In step <b>10</b>, the last X number of video frames, such as 120 video frames, are analyzed and classified in step <b>12</b> as to the source of noise. In step <b>14</b>, an effective noise suppression transform A, B, C or D is selected based on the results of step <b>12</b>. <figref idref="DRAWINGS">FIG. <b>1</b></figref> shows transform B selected. The selection of the transform occurs within a short time interval <b>16</b>, such as within 80 ms. In step <b>18</b>, real-time incoming video frames are received and the set of video frames <b>20</b> is corrected using the selected transform in step <b>22</b>, within a short interval of time <b>24</b>, such as within 80 ms, resulting in an enhanced set of video frames <b>26</b>.</p><p id="p-0016" num="0014">The development of a weakly supervised learning method allowed for 0.1% of the sparsely labeled data to be used to automatically label the other 99.9% of the data based on perceived image quality and improvement after image processing for a given task. The successful training of a neural-network classifier has reached a predictive accuracy of 83% over a validation set when attempting to choose the &#x2018;best&#x2019; image processing routine to match expected subjective human-labeled image quality. Incorrect predictions still selected highly similar methods, showing a robustness in the AI.</p><p id="p-0017" num="0015">The invention includes wider suites of image processing functions and scenes including separate analysis of sub-regions of the image rather than only full-frame restoration methods.</p><p id="p-0018" num="0016">Use of an exemplary embodiment of the invention can restore a full 1920&#xd7;1200 14-bit frame using a 3&#xd7;3 sub-region grid in 76 ms when only using 4 processing cores without GPU acceleration. This total restoration time includes the time to perform AI prediction inferences, a first restoration method, a contrast-enhancement method, and outputting the result. Images and full video sequences can be rendered using this methodology.</p><p id="p-0019" num="0017">The method can utilize a blend of PYTHON, C, FORTRAN, and specialized compilers to achieve a balance between high extensibility and computation speed.</p><p id="p-0020" num="0018">From the foregoing, it will be observed that numerous variations and modifications may be effected without departing from the spirit and scope of the invention. It is to be understood that no limitation with respect to the specific apparatus illustrated herein is intended or should be inferred.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>The invention claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method of image restoration comprising the steps of:<claim-text>analyzing noise sources in video frames of degraded imagery;</claim-text><claim-text>using modeling and machine learning methods, developing metrics to categorize these noise sources present in the video frames of degraded imagery;</claim-text><claim-text>using the metrics to rapidly identify an optimal noise removal method;</claim-text><claim-text>using the optimal noise removal method, restoring the degraded imagery in subsequent video frames of degraded imagery to repair damage done by the noise sources;</claim-text><claim-text>limiting the scope of repair to ensure proper algorithm complexity for high-speed restoring of the degraded imagery to remain suitable for use when driving a vehicle.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the step of analyzing noise sources is done on a preceding number of 120 video frames and the noise removal is done on succeeding number of video frames.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. A method of substantially real-time image restoration of an infrared camera includes the steps of:<claim-text>analyzing the last X number of video frames;</claim-text><claim-text>classifying the last X number of video frames as to the source of noise in the last X number of video frames;</claim-text><claim-text>selecting a noise suppression transform based on the source of the noise;</claim-text><claim-text>receiving real time video frames;</claim-text><claim-text>correcting the real time video frames using the selected noise suppression transform.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the selection of the noise suppression transform occurs within a first short time interval.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the short time interval is about 80 ms.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the real-time video frames are received and the real-time video frames are corrected using the selected noise suppression transform, within a second short time interval.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the second short time interval is about 80 ms.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. A method of substantially real-time image restoration of an infrared camera includes the steps of:<claim-text>analyzing noise sources on a preceding select number of video frames and based on that analysis, removing noise on a succeeding number of video frames.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the preceding select number of video frames comprises 120 video frames.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the analyzing noise sources classifies the noise source from the preceding select number of video frames and based on the classification, automatically selects from a range of image processing algorithms in real time to remove noise on the succeeding number of video frames.</claim-text></claim></claims></us-patent-application>