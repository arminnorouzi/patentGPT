<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005182A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005182</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17783601</doc-number><date>20191210</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>73</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>77</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>766</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>75</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>7715</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>766</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">ESTIMATION METHOD, ESTIMATION APPARATUS AND PROGRAM</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>NIPPON TELEGRAPH AND TELEPHONE CORPORATION</orgname><address><city>Tokyo</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>MIZUTANI</last-name><first-name>Shin</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>NIPPON TELEGRAPH AND TELEPHONE CORPORATION</orgname><role>03</role><address><city>Tokyo</city><country>JP</country></address></addressbook></assignee></assignees><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/JP2019/048228</doc-number><date>20191210</date></document-id><us-371c12-date><date>20220608</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">An estimation step according to an embodiment causes a computer to execute: a calculation step of using a plurality of images obtained by a plurality of imaging devices imaging a three-dimensional space in which a plurality of objects reside, to calculate representative points of pixel regions representing the objects among pixel regions of the images; a position estimation step of estimating positions of the objects in the three-dimensional space, based on the representative points calculated by the calculation step; an extraction step of extracting predetermined feature amounts from image regions representing the objects; and an attitude estimation step of estimating attitudes of the objects in the three-dimensional space, through a preliminarily learned regression model, using the positions estimated by the position estimation step, and the feature amounts extracted by the extraction step.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="66.89mm" wi="148.93mm" file="US20230005182A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="212.77mm" wi="150.96mm" file="US20230005182A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="193.12mm" wi="141.73mm" file="US20230005182A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="164.68mm" wi="109.98mm" file="US20230005182A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="217.17mm" wi="154.69mm" file="US20230005182A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="222.93mm" wi="122.34mm" orientation="landscape" file="US20230005182A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="227.50mm" wi="122.77mm" orientation="landscape" file="US20230005182A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="132.50mm" wi="150.62mm" file="US20230005182A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0002" num="0001">The present invention relates to an estimation method, an estimation device, and a program.</p><heading id="h-0002" level="1">BACKGROUND ART</heading><p id="p-0003" num="0002">In the field of computer vision (CV), a method has been known that calculates the attitude of an object in a three-dimensional space from two-dimensional images. For example, the parametric eigenspace method (e.g., Non-Patent Literature 1), a method of estimating the attitude using spherical functions (e.g., Non-Patent Literature 2) and the like have been known. Besides these methods, a method has also been known that captures points on the surface of a three-dimensional object, and calculates an attitude matrix representing the attitude of the three-dimensional object using the positions of the points (e.g., Non-Patent Literature 3). Moreover, it can be imagined that a regression model is learned on the basis of pair data on images and physical quantities representing the attitude, and the attitude is estimated.</p><heading id="h-0003" level="1">CITATION LIST</heading><heading id="h-0004" level="1">Non-Patent Literature</heading><p id="p-0004" num="0003">Non-Patent Literature 1: Hiroshi MURASE, S. K. Nayar, &#x201c;3D Object Recognition from Appearance&#x2014;Parametric Eigenspace Method&#x201d;, The transactions of the Institute of Electronics, Information and Communication Engineers, D-II, vol. 7, no. 11, pp. 2179-2187, November 1994.</p><p id="p-0005" num="0004">Non-Patent Literature 2: Toru TAMAKI, Toshiyuki AMANO, Kazufumi KANEDA, &#x201c;Representing pose by spherical functions for pose estimation&#x201d;, Meeting on Image Recognition and Understanding (MIRU2008), pp. 1134-1141, 2008.</p><p id="p-0006" num="0005">Non-Patent Literature 3: Kunihiro NISHIMURA, Masamichi SHIMOSAKA, Tomohiro TANIKAWA, Mamoru NAKAMURA, Masayuki TANAKA, Yoshiyuki NAKAGAKI, &#x201c;Department of Mechano-Informatics, Seminar, Media Interface (1), Camera Interface&#x201d;, Internet &#x3c;URL: http://www.cyber.t.u-tokyo.ac.jp/-kuni/enshu2010/enshu2010mi1.pdf&#x3e;</p><heading id="h-0005" level="1">SUMMARY OF THE INVENTION</heading><heading id="h-0006" level="1">Technical Problem</heading><p id="p-0007" num="0006">Unfortunately, since the methods described in Non-Patent Literature 1 and Non-Patent Literature 2 use, for example, images themselves for attitude estimation, the estimation accuracy is sometimes reduced by a certain change in illumination condition and the like. For example, the method described in Non-Patent Literature 3 requires capturing points on a surface of a three-dimensional object. Accordingly, in case these points cannot be observed, for example, the attitude cannot be estimated.</p><p id="p-0008" num="0007">For example, the method of using a regression model is accompanied by specific selection of input and output data for learning, a specific structure and mechanism of a regression model and the like. Accordingly, it is difficult to achieve this method. For example, in case part of input data includes a defect, it is impossible to estimate the attitude by a single regression model. In order to achieve attitude estimation even with a defect at part of input data, many regression models in accordance with defect situations are required to be prepared.</p><p id="p-0009" num="0008">Furthermore, even if the methods described in Non-Patent Literature 1 and Non-Patent Literature 2 and the method of using a regression model are combined, it is impossible to estimate the attitudes of multiple objects that cannot be discriminated from each other.</p><p id="p-0010" num="0009">An embodiment of the present invention has been made in view of the points described above, and has an object to accurately estimate the positions and attitudes of objects in a three-dimensional space.</p><heading id="h-0007" level="1">Means for Solving the Problem</heading><p id="p-0011" num="0010">To achieve the above object, an estimation step according to an embodiment causes a computer to execute: a calculation step of using a plurality of images obtained by a plurality of imaging devices imaging a three-dimensional space in which a plurality of objects reside, to calculate representative points of pixel regions representing the objects among pixel regions of the images; a position estimation step of estimating positions of the objects in the three-dimensional space, based on the representative points calculated by the calculation step; an extraction step of extracting predetermined feature amounts from image regions representing the objects; and an attitude estimation step of estimating attitudes of the objects in the three-dimensional space, through a preliminarily learned regression model, using the positions estimated by the position estimation step, and the feature amounts extracted by the extraction step.</p><heading id="h-0008" level="1">Effects of the Invention</heading><p id="p-0012" num="0011">The positions and attitudes of objects in the three-dimensional space can be accurately estimated.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0009" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows an example of an overall configuration of an estimation device according to this embodiment.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a diagram for illustrating an example of imaging through multiple imaging devices.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram for illustrating an example of a process of 3D position projecting.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram for illustrating an example of an object image of an ellipse.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram for illustrating an example of a neural network included in a regression model.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flowchart showing an example of a flow of a position estimation process according to this embodiment.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart showing an example of a flow of an attitude estimation process according to this embodiment.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>8</b></figref> shows an example of an estimation result of 3D positions and attitudes (I).</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>9</b></figref> shows an example of an estimation result of 3D positions and attitudes (II).</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>10</b></figref> shows an example of a hardware configuration of the estimation device according to this embodiment.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0010" level="1">DESCRIPTION OF EMBODIMENTS</heading><p id="p-0023" num="0022">Hereinafter, an embodiment of the present invention is described. In this embodiment, an estimation device <b>10</b> is described that can accurately estimate the positions and attitudes of three-dimensional objects using two-dimensional images obtained by imaging the three-dimensional objects. Note that hereinafter, &#x201c;two-dimensional&#x201d; is also represented as &#x201c;2D&#x201d; and &#x201c;three-dimensional&#x201d; is also represented as &#x201c;3D&#x201d;.</p><p id="p-0024" num="0023">Here, in this embodiment, it is assumed that multiple 2D images (hereinafter also simply represented as &#x201c;images&#x201d;) obtained by imaging multiple 3D objects (hereinafter also simply represented as &#x201c;objects&#x201d;) of an identical type in a closed space are used to estimate the positions and attitudes of these objects. It is herein assumed that the total number of objects can be easily estimated from the images. That is, it is assumed that the maximum number among the numbers of object images in the respective images can be estimated as the total number of objects. If each object is imaged in an environment where the object is less likely to mask the other objects, this estimation is correct. Note that the object image is a combined pixel region that represents an object projected in an image.</p><p id="p-0025" num="0024">The object is assumed as, for example, a rigid body that is an ellipsoid having three axes different in length from each other. For example, a living thing such as a fish in an aquarium, an insect flying around in a room, a drone or the like is assumed as such an object. Note that, for example, a living thing is not a rigid body in a strict sense. It is however assumed that a living thing can be approximated as a rigid body. It is assumed that the three-dimensional shape can also be approximated as an ellipsoid having three axes different in length from each other.</p><p id="p-0026" num="0025">The 3D position P of an object is represented by 3D absolute coordinates (X, Y, Z). A representative point, such as the 3D barycenter, of the object is assumed as the 3D position P. The 3D attitude of the object is represented by an attitude matrix (or rotation matrix) R&#x2208;R<sup>3&#xd7;3</sup>. Note that, according to this embodiment, the representative point of the object is the 3D barycenter. The object is assumed to have an equal density, and the 3D barycenter is assumed as the volume center point.</p><p id="p-0027" num="0026">&#x3c;Overall Configuration&#x3e;</p><p id="p-0028" num="0027">First, an overall configuration of the estimation device <b>10</b> according to this embodiment is described with reference to <figref idref="DRAWINGS">FIG. <b>1</b></figref>. <figref idref="DRAWINGS">FIG. <b>1</b></figref> shows an example of the overall configuration of the estimation device <b>10</b> according to this embodiment.</p><p id="p-0029" num="0028">As shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the estimation device <b>10</b> according to this embodiment includes a position estimation unit <b>101</b>, a feature amount extraction unit <b>102</b>, an attitude estimation unit <b>103</b>, and a storage unit <b>104</b>.</p><p id="p-0030" num="0029">The storage unit <b>104</b> stores multiple images obtained by imaging multiple objects at individual time points. Here, according to this embodiment, for example, it is assumed that images obtained by imaging multiple objects in a closed space represented by &#x2212;1&#x2264;X, Y, Z&#x2264;1 by three imaging devices at individual time points are stored. For example, as shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, it is assumed that at each time point, imaging is performed by an imaging device <b>20</b>A residing in front of an object O in the closed space, an imaging device <b>20</b>B residing above the object O, and an imaging device <b>20</b>C residing laterally to the object O. Note that in the example shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, only one object O resides in the closed space. However, multiple objects reside in the closed space. Note that it is assumed that camera parameters (i.e., intrinsic parameters, and extrinsic parameters) of the imaging devices <b>20</b>A to <b>20</b>C are known.</p><p id="p-0031" num="0030">As with the example shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, if the visual axes of the respective imaging devices <b>20</b> are different from each other, the probability of obtaining two or more images with no overlap of object images increases even in a case where the number of objects is large and the objects are three-dimensionally close to each other, for example. Accordingly, the number of cases where the 3D position can be calculated by the position estimation unit <b>101</b>, described later, increases. Note that the case where the number of imaging devices <b>20</b> is three is only an example. The number of imaging devices <b>20</b> may be four or more.</p><p id="p-0032" num="0031">Here, as shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, this embodiment assumes that the surface of each object is divided into four regions D<sub>1 </sub>to D<sub>4</sub>, and each imaging device <b>20</b> can image (observe) at least one of these four regions D<sub>1 </sub>to D<sub>4</sub>. The four regions D<sub>1 </sub>to D<sub>4 </sub>are assumed to be projected as pixel regions having luminances different from each other on an image. Here, for example, three axes of the object (ellipsoid) are called a major axis, a median axis and a minor axis in a descending order of length. In this case, as a method of division into the four regions D<sub>1 </sub>to D<sub>4</sub>, a method is conceivable where regions at least including the proximity of respective intersections (two points) with the major axis and the ellipsoidal surface are D<sub>1 </sub>and D<sub>2</sub>, and regions at least including the proximity of respective intersections (two points) with the median axis and the ellipsoidal surface are D<sub>4 </sub>and D<sub>4</sub>. Note that the regions D<sub>1 </sub>to D<sub>4 </sub>are obtained by dividing the surface of the ellipsoid with no overlap. The sum of the areas of the regions D<sub>1 </sub>to D<sub>4 </sub>is equal to the surface area of the ellipsoid.</p><p id="p-0033" num="0032">The position estimation unit <b>101</b> uses the images stored in the storage unit <b>104</b> to estimate the 3D position from the 2D positions of the object on the images at the same time point. Here, this embodiment assumes that the 3D barycenter of the object is projected on the 2D barycenter of an object image (a two-dimensional combined pixel region) in the image, and the 2D barycenter is assumed as the 2D position of the object.</p><p id="p-0034" num="0033">For example, the position of the imaging device <b>20</b>A is assumed as U<sub>A</sub>, and the position of the imaging device <b>20</b>B is assumed as U<sub>B</sub>. A projection process of the 3D position P of a certain object viewed from these imaging devices <b>20</b>A and <b>20</b>B is shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>. In the example shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, a projection surface (image) of the imaging device <b>20</b>A is represented as GA, and a projection surface (image) of the imaging device <b>20</b>B is represented as G<sub>B</sub>. In this case, when the number of objects in the closed space is one, the position U<sub>A </sub>and the position U<sub>B</sub>, the direction of the visual axes of the imaging device <b>20</b>A and the imaging device <b>20</b>B and the like are known (i.e., camera parameters are known). Accordingly, when the 2D positions Q<sub>A </sub>and Q<sub>B </sub>of the object are obtained, the 3D position P of the object can be calculated.</p><p id="p-0035" num="0034">According to this embodiment, there are multiple objects that are of the identical type and cannot be discriminated from each other in the closed space. Consequently, by solving a correspondence problem of 2D positions between the multiple images at the same time point under an epipolar constraint condition, the correspondence of the 2D positions of the images of the identical object between the images is obtained. Here, an epipolar line L<sub>A </sub>is a line on the image G<sub>A </sub>that connects the 2D position Q<sub>A </sub>and a point (an epipole e<sub>A</sub>) where a viewpoint (i.e., U<sub>S</sub>) of the imaging device <b>20</b>B is projected. Likewise, an epipolar line L<sub>B </sub>is a line on the image G<sub>B </sub>that connects the 2D position Q<sub>B </sub>and a point (an epipole es) where a viewpoint (i.e., U<sub>A</sub>) of the imaging device <b>20</b>A is projected. That is, the epipolar line is obtained by projecting, on the imaging device, the visual line when the object is viewed from the other imaging device. Accordingly, ideally, the 2D position should be on the epipolar line. However, because of an error or the like, the 2D position is not necessarily on the epipolar line in some cases. Even when the 2D position of the object is on the epipolar line but when the 3D position of another object is on an epipolar plane T, multiple 2D positions are on the same epipolar line. Accordingly, even under the epipolar constraint condition, the correspondence between the 2D positions of the images of the identical object cannot be obtained. Note that for meanings of terms and the like of the epipolar geometry, such as the epipole and the epipolar line, see Non-Patent Literature 3 described above and the like.</p><p id="p-0036" num="0035">Accordingly, in this embodiment, it is assumed that the correspondence problem is solved by the position estimation unit <b>101</b> achieving a correspondence, as an identical object, between 2D positions having the minimum distance between a certain epipolar line and a certain 2D position on a certain image. Note that when the intrinsic parameters that are camera optical characteristics are the same between the imaging devices <b>20</b>, the same result can be achieved in an ideal sense, with a correspondence based on the distance of any image.</p><p id="p-0037" num="0036">That is, for instance, in the example shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, two distances are conceivable that are the distance of the 2D position Q<sub>A </sub>from the epipolar line L<sub>A </sub>on the image G<sub>A</sub>, and the distance of the 2D position Q<sub>B </sub>from the epipolar line L<sub>B </sub>on the image G<sub>B</sub>. Meanwhile, when the two imaging devices <b>20</b>A and <b>20</b>B have the same camera intrinsic parameters, the 2D positions are the same between the images in an ideal sense. Accordingly, it is sufficient to consider any one of the images. The pair of the epipolar line and the 2D position having the minimum distance is assumed as the correspondence between 2D positions of the identical object. For instance, in the case of the example shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, any one of the distance of the 2D position Q<sub>A </sub>from the epipolar line L<sub>A </sub>on the image G<sub>A</sub>, and the distance of the 2D position Q<sub>B </sub>from the epipolar line L<sub>B </sub>on the image GB is considered. The pair of the 2D position of the 2D point on which the 3D point is projected, and the 2D position having the minimum distance from the corresponding epipolar line is assumed as the correspondence between 2D positions of the identical object. Accordingly, from each of images (at least two images) at the same time point, the 2D position of each object can be obtained. Consequently, the position estimation unit <b>101</b> can estimate the 3D position of the object from these 2D positions. Note that, for example, a known method, such as triangulation, may be used to estimate the 3D position.</p><p id="p-0038" num="0037">Here, in the example shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, for simplicity's sake, only one 3D point (P) is shown. When there are multiple objects, multiple 3D points (P<sub>1</sub>, P<b>2</b>, . . . ) reside. In this case, there are 2D positions (Q<sub>A1</sub>, Q<sub>A2</sub>, . . . , or Q<sub>B1</sub>, QB<b>2</b><sub>,</sub>. . . ) of multiple 2D points where these 3D points are projected on corresponding images. For example, on the image G<sub>A </sub>where the 2D position Q<sub>A </sub>resides, there are as many epipolar lines as the number of 3D points (L<sub>A1</sub>, L<sub>A2</sub>, . . . ); the epipolar lines include the epipolar line L<sub>A1 </sub>on which a 3D line connecting the viewpoint of the other imaging device <b>20</b>B and the 3D point P<sub>1 </sub>is projected, the epipolar line L<sub>A2 </sub>on which a 3D line connecting the viewpoint of the other imaging device <b>20</b>B and the 3D point P<sub>2 </sub>is projected, in a similar manner, and the like. Consequently, the pair of the 2D position Q<sub>A </sub>and the epipolar line having the minimum distance is assumed as the correspondence of 2D positions of the identical object.</p><p id="p-0039" num="0038">As described above, in order to estimate the 3D position of a certain object, 2D positions of the object are required to be obtained on at least two images at the same time point. Meanwhile, when multiple object images (combined pixel regions) overlap with each other on the image, the 2D barycenter of each object cannot be calculated. Accordingly, as shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, it is preferable to image each object through multiple imaging devices <b>20</b> having different visual axes. Accordingly, the probability of obtaining two or more images with no overlap of object images increases, and the number of cases of allowing 3D position calculation can increase.</p><p id="p-0040" num="0039">The feature amount extraction unit <b>102</b> extracts feature amounts from each object image on multiple images at the same time point. According to this embodiment, an attitude matrix is estimated using the feature amounts through a regression model. This embodiment herein assumes that the ellipsoid having three axes different in length from each other is projected approximately as an ellipse on an image. Accordingly, for example, as shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, an elliptic region is included, on the image, as an object image O&#x2032; on which a certain object O is projected.</p><p id="p-0041" num="0040">In the example shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, Q is the 2D barycenter of the object image O&#x2032;, and is a point on which the 3D barycenter of the ellipsoid (i.e., the object O) is projected. The angle &#x3b8; between the positive sense of a major axis A<sub>1 </sub>of the object image O&#x2032; and the positive sense of the horizontal direction of the image, the angle &#x3d5; between the positive sense of a minor axis A<sub>2 </sub>of the object image O&#x2032; and the positive sense of the horizontal direction of the image, the ratio r=l<sub>2</sub>/l<sub>1 </sub>between a length l<sub>1 </sub>of the major axis A<sub>1 </sub>and a length l<sub>2 </sub>of the minor axis A<sub>2</sub>, and the product S=l<sub>2</sub>&#xd7;l<sub>1 </sub>of l<sub>2 </sub>and l<sub>1</sub>, are used as the feature amounts for estimating the attitude matrix. That is, the feature amount extraction unit <b>102</b> extracts the feature amounts (&#x3b8;, &#x3d5;, r, S) for each object image O&#x2032;.</p><p id="p-0042" num="0041">Here, in order to extract the angles &#x3b8; and &#x3d5; among the feature amounts (&#x3b8;, &#x3d5;, r, S) described above, the positive senses of the major axis A<sub>1 </sub>and the minor axis A<sub>2 </sub>are required to be determined. As described above, each object (ellipsoid) is divided into the four regions D<sub>1 </sub>to D<sub>4</sub>. These regions D<sub>1 </sub>to D<sub>4 </sub>are projected on the image as pixel regions having luminances different from each other. Accordingly, among the pixels included in the outer periphery of the object image O&#x2032;, the axis having the maximum distance between two pixels, which are freely selected, is assumed as the major axis A<sub>1</sub>, and the positive sense may be determined on the basis of the difference in luminance between the end points of the major axis A<sub>1</sub>. Likewise, the axis that passes through a 2D barycenter Q of the object image O&#x2032; and is perpendicular to the major axis A<sub>1 </sub>is assumed as the minor axis A<sub>2</sub>, and the positive sense may be determined on the basis of the difference in luminance between the end points of the minor axis A<sub>2</sub>.</p><p id="p-0043" num="0042">For example, among intersections between the major axis and the ellipsoidal surface, the intersection included in the region D<sub>1 </sub>is assumed as a first intersection, and the intersection included in the region D<sub>2 </sub>is assumed as a second intersection, and the direction from the second intersection toward the first intersection can be determined as the positive direction of the major axis. For example, it is assumed that the pixel region where the region D<sub>1 </sub>is projected has a higher luminance than the pixel region where the region D<sub>2 </sub>is projected has. In this case, the positive direction of the major axis A<sub>1 </sub>is determined as the direction from the end point having a lower luminance to the end point having a higher luminance. Likewise, for the direction of the minor axis A<sub>2</sub>, the positive direction may be determined.</p><p id="p-0044" num="0043">Note that, for example, if the minor axis A<sub>2 </sub>corresponds to the minor axis of the ellipsoid, there can be no difference in luminance between the end points. In this case, a predetermined direction may be assumed as the positive direction of the minor axis A<sub>2</sub>.</p><p id="p-0045" num="0044">At a certain 3D position and attitude, the angles &#x3b8; and &#x3d5; of the major axis and the minor axis of the elliptic region on the image are defined to be always in specified directions, thereby achieving a one-to-one relationship between the feature amounts (&#x3b8;, &#x3d5;, r, S) and the attitude matrix. Accordingly, as described later, a regression model that calculates the attitude matrix using the 3D position and the feature amounts (more correctly, feature amounts obtained from the feature amounts) can be constructed.</p><p id="p-0046" num="0045">Note that the method of determining the positive directions of the major axis A<sub>1 </sub>and the minor axis A<sub>2 </sub>described above is only one example. The positive directions of the major axis A<sub>1 </sub>and the minor axis A<sub>2 </sub>may be determined by another method. For example, in a case where unusual 3D points reside at the intersections between the surface of the ellipsoid and the positive directions of the three axes, the positive directions of the major axis A<sub>1 </sub>and the minor axis A<sub>2 </sub>can be determined using 2D points projected on the image (i.e., points where the unusual 3D points are projected on the image).</p><p id="p-0047" num="0046">The attitude estimation unit <b>103</b> calculates the attitude matrix, through the regression model, using the feature amounts extracted by the feature amount extraction unit <b>102</b>, and the 3D position estimated by the position estimation unit <b>101</b>. Accordingly, the attitude of the three-dimensional object is estimated.</p><p id="p-0048" num="0047">Here, the feature amounts extracted by the feature amount extraction unit <b>102</b> depend not only on the attitude of the object but also on the 3D position. Accordingly, not only the feature amounts extracted from multiple images at the same time point (three images in this embodiment) but also the 3D position estimated by the position estimation unit <b>101</b> is used as input data to be input into the regression model. The &#x3b8; and &#x3d5; included in the feature amounts are not input into the regression model as they are. The cosine and sine values are used instead. Consequently, provided that the feature amounts extracted from the images G<sub>A</sub>, G<sub>B </sub>and G<sub>C </sub>are (&#x3b8;<sub>A</sub>, &#x3d5;<sub>A</sub>, r<sub>A</sub>, S<sub>A</sub>) , (&#x3b8;<sub>B</sub>, &#x3d5;<sub>B</sub>, r<sub>B</sub>, S<sub>B</sub>) and (&#x3b8;<sub>C</sub>, &#x3d5;<sub>C</sub>, r<sub>C</sub>, S<sub>C</sub>), respectively, and the 3D position is P=(X<sub>P</sub>, Y<sub>P</sub>, Z<sub>P</sub>), the input data to be input into the regression model is 21-dimensional data represented as follows. That is, the 21-dimensional data is represented as (cos &#x3b8;<sub>A</sub>, cos &#x3d5;<sub>A</sub>, sin &#x3b8;<sub>A</sub>, sin &#x3d5;<sub>A</sub>, r<sub>A</sub>, S<sub>A</sub>, cos &#x3b8;<sub>B</sub>, cos &#x3d5;<sub>B</sub>, sin &#x3b8;<sub>B</sub>, sin &#x3d5;<sub>B</sub>, r<sub>B</sub>, S<sub>B</sub>, cos &#x3b8;<sub>C</sub>, cos &#x3d5;<sub>C</sub>, sin &#x3b8;<sub>C</sub>, sin &#x3d5;<sub>C</sub>, r<sub>C</sub>, S<sub>C</sub>, X<sub>P</sub>, Y<sub>P</sub>, Z<sub>P</sub>). Note that output data from the regression model is the attitude matrix R&#x2208;P<sup>3&#xd7;3</sup>.</p><p id="p-0049" num="0048">Here, the regression model includes a neural network that receives 21-dimensional data as input, and an orthonormalization processing unit that orthonormalizes the output of the neural network. The orthonormalization processing unit is required because the output of the neural network sometimes does not satisfy the condition of the attitude matrix (i.e., orthogonal matrix, and det|R|=1). <figref idref="DRAWINGS">FIG. <b>5</b></figref> shows an example of the neural network included in the regression model. As shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the neural network included in the regression model includes a 21-dimensional input layer (i.e., an input layer having 21 units), a 32-dimensional intermediate layer, a 16-dimensional intermediate layer, and a nine-dimensional output layer. The first to fourth layers are fully connected layers. Linear transformation is used for the activating functions of the input layer and the output layer. ReLU (Rectified Linear Unit) is used for the activating functions of the intermediate layers. Note that the neural network shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref> is only one example. The dimensions (the number of units) of the intermediate layers, the number of intermediate layers, the activating functions and the like can be appropriately changed.</p><p id="p-0050" num="0049">The regression model described above may be learned by a supervised learning method, for example. However, according to this embodiment, the learning method is devised. Here, for example, if it is configured such that the feature amounts from all the images at the same time point are input into the regression model, the object images overlap with each other, part of input data into the regression model includes a defect in case no feature amount is obtained from a certain image, and the attitude matrix cannot be estimated. Typically, the 3D position of the object can be calculated when there are two or more images with object images not overlapping with each other. Likewise, it is accordingly conceivable that the 3D attitude can be calculated when there are two or more images with object images not overlapping with each other. In order to support a case where the object images overlap with each other, typically, all regression models into which the feature amounts of only any two images have been input and which support every overlapping case are required. In this embodiment, for saving the effort, in order to allow the attitude matrix to be calculated using a single regression model even with occurrence of an overlap, the following devices are applied to a dataset for the neural network learning.</p><p id="p-0051" num="0050">Typically, the dataset for learning is created, with feature amounts obtained from each of images obtained by the imaging devices imaging each individual object at various 3D positions and attitudes, and the 3D positions being adopted as input data, and attitude matrices representing the attitudes being adopted as training data. In this case, the feature amounts of the image where the object images overlap with each other cannot be obtained. Accordingly, this embodiment assumes that the values of the feature amounts when the object images overlap with each other are fixed to predetermined values out of the range of the values of feature amounts obtained from the image where the object images do not overlap with each other (i.e., out of the range of the upper limit value and the lower limit value). That is, according to this embodiment, learning is performed using a dataset that includes not only typical learning data but also learning data assuming a case where the object images overlap with each other. Here, the typical learning data is learning data that adopts, as input data, the feature amounts and the 3D position obtained from each of the images, and adopts the attitude matrix as training data. The learning data assuming the case where the object images overlap with each other is learning data that adopts, as input data, the feature amounts and the 3D position that are fixed to predetermined values, and adopts the attitude matrix as training data. The functions obtainable by the neural network through learning can be many-to-one mapping. Accordingly, the regression model can be constructed even by learning with such a dataset for learning. Note that in a case where the object images of the images overlap with each other when the attitude is actually estimated (i.e., when the regression model including a learned neural network is used), the values of the feature amounts of the images are input as the predetermined values described above into the regression model.</p><p id="p-0052" num="0051">&#x3c;Estimation of 3D Position and Attitude&#x3e;</p><p id="p-0053" num="0052">Next, a flow of processes in a case where the position and attitude of each object are estimated by the estimation device <b>10</b> according to this embodiment is described. Note that a case of estimating the position and attitude of the object using three images G<sub>A </sub>to G<sub>C </sub>(i.e., images taken by the imaging devices <b>20</b>A to <b>20</b>C) at a certain time point is hereinafter described.</p><p id="p-0054" num="0053">&#x3c;&#x3c;Position Estimation Process&#x3e;&#x3e;</p><p id="p-0055" num="0054">Hereinafter, the flow of a position estimation process for estimating the 3D position of each object is described with reference to <figref idref="DRAWINGS">FIG. <b>6</b></figref>. <figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flowchart showing an example of the flow of the position estimation process according to this embodiment.</p><p id="p-0056" num="0055">First, the position estimation unit <b>101</b> obtains the images G<sub>A </sub>to G<sub>C </sub>at the time point concerned from the storage unit <b>104</b> (step S<b>101</b>).</p><p id="p-0057" num="0056">Next, the position estimation unit <b>101</b> extracts foreground portions representing object images on which the objects are projected from each of the images G<sub>A </sub>to G<sub>C </sub>(step S<b>102</b>). That is, the position estimation unit <b>101</b> cuts out the foreground portions from the images G<sub>A </sub>to G<sub>C</sub>. These foreground portions are also passed to the feature amount extraction unit <b>102</b>. Note that if the object images do not overlap with each other, these foreground portions are elliptic regions (however, if the object images overlap with each other, the detection is possible by the fact that the number of combined regions in the image is smaller than that in another image having no overlap; in such a case, the correspondence problem may be solved between images that do not overlap with each other). If the object images do not overlap with each other, it is assumed which foreground portion is of which object image is known or can be easily estimated.</p><p id="p-0058" num="0057">Next, the position estimation unit <b>101</b> calculates the 2D positions of the respective objects from the foreground portions (object images) extracted in step S<b>102</b> described above (step S<b>103</b>). As described above, the position estimation unit <b>101</b> calculates the 2D barycenters of the respective object images, assumes these 2D barycenters as the 2D positions, and associates the 2D positions having minimum distance between the epipolar line and the 2D position between two images with each other, as the 2D positions of the images of the identical object. Note that as described above, when the object images overlap with each other, the 2D barycenter cannot be calculated. It is assumed that in this step, for individual object images, the object images do not overlap with each other in at least two images, and the 2D barycenters (i.e., 2D positions) can be calculated between at least two images.</p><p id="p-0059" num="0058">Lastly, the position estimation unit <b>101</b> estimates the 3D position of each object from the 2D positions of the corresponding object obtained in step S<b>103</b> described above (step S<b>104</b>). As described above, for example, a known method, such as triangulation, may be used to estimate the 3D position. These 3D positions are passed to the attitude estimation unit <b>103</b>.</p><p id="p-0060" num="0059">&#x3c;&#x3c;Attitude Estimation Process&#x3e;&#x3e;</p><p id="p-0061" num="0060">Hereinafter, the flow of an attitude estimation process for estimating the attitude of each object is described with reference to <figref idref="DRAWINGS">FIG. <b>7</b></figref>. <figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart showing an example of a flow of the attitude estimation process according to this embodiment.</p><p id="p-0062" num="0061">First, the feature amount extraction unit <b>102</b> receives the foreground portions passed from the position estimation unit <b>101</b> as input (step S<b>201</b>).</p><p id="p-0063" num="0062">Next, for each foreground portion input in step S<b>201</b> described above, on an object-by-object basis, when the foreground portions corresponding to the respective objects are elliptic regions (i.e., when the object images do not overlap with each other), the feature amount extraction unit <b>102</b> extracts the feature amounts from the elliptic regions (step S<b>202</b>). Accordingly, on an object-by-object basis, first feature amounts extracted from the foreground portion of the image G<sub>A</sub>, second feature amounts extracted from the foreground portion of the image G<sub>B</sub>, and third feature amounts extracted from the foreground portion of the image G<sub>C </sub>are obtained. Note that as described above, if the object images overlap with each other, the feature amount extraction unit <b>102</b> sets the values of the feature amounts to predetermined values. For example, when the overlap occurs between object images at the foreground portion of the image G<sub>A</sub>, the feature amount extraction unit <b>102</b> sets the values of the first feature amounts respectively to predetermined values.</p><p id="p-0064" num="0063">Lastly, on an object-by-object basis, the attitude estimation unit <b>103</b> calculates the attitude matrix that represents the attitude of the object through the regression model, using the 3D position (the 3D position of the object concerned) passed from the position estimation unit <b>101</b>, and the feature amounts (the first feature amounts to third feature amounts of the object concerned) extracted in step S<b>202</b> described above (step S<b>203</b>). Note that as described above, for example, the attitude estimation unit <b>103</b> creates 21-dimensional input data using the 3D position, and the first feature amounts to the third feature amounts, and inputs the 21-dimensional input data into the regression model, thereby obtaining the attitude matrix R. Accordingly, the attitude represented by the attitude matrix R is estimated as the attitude of the object concerned.</p><p id="p-0065" num="0064">&#x3c;Evaluation of Estimation Result&#x3e;</p><p id="p-0066" num="0065">Hereinafter, evaluation of a result of estimation of a 3D position and an attitude of each object at a certain time point by the estimation device <b>10</b> according to this embodiment is described. Note that the neural network shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref> is used as the neural network included in the regression model used for this evaluation. In the neural network learning, mean squared error (MSE) is used for the loss function, and Adam is used for the optimization method. The number of mini batches is 90, and the number of epochs is 100.</p><p id="p-0067" num="0066">Here, a result of estimation of the 3D positions and attitudes of the objects O<sub>1 </sub>to O<sub>3 </sub>at a certain time point by the estimation device <b>10</b> according to this embodiment is shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>. In <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the upper row indicates true values, and the lower row indicates estimation results. From the left, the position and attitude of each object in a projection surface of the imaging device <b>20</b>A, the position and attitude of each object in a projection surface of the imaging device <b>20</b>B, the position and attitude of each object in a projection surface of the imaging device <b>20</b>C, and the position and attitude of each object in the 3D space are shown. As shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, by comparing the true values with the estimation results, it is shown that both the 3D positions and the attitudes can be accurately estimated even with slight errors.</p><p id="p-0068" num="0067">A result in a case where parts of object images overlap with each other when the 3D positions and attitudes of the objects O<sub>1 </sub>to O<sub>3 </sub>at a certain time point by the estimation device <b>10</b> according to this embodiment are estimated is shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref>. Also in <figref idref="DRAWINGS">FIG. <b>9</b></figref>, the upper row indicates true values, and the lower row indicates estimation results. From the left, the position and attitude of each object in a projection surface of the imaging device <b>20</b>A, the position and attitude of each object in a projection surface of the imaging device <b>20</b>B, the position and attitude of each object in a projection surface of the imaging device <b>20</b>C, and the position and attitude of each object in the 3D space are shown. As shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref>, the objects O<sub>1 </sub>and O<sub>3 </sub>partially overlap with each other in the projection surface of the imaging device <b>20</b>C. Note that, for example, in this case, when two imaging devices <b>20</b> including the imaging device <b>20</b>C are used, the 2D positions of the objects O<sub>1 </sub>and O<sub>3 </sub>cannot be calculated. However, use of the three imaging devices <b>20</b> as in this embodiment can calculate the 2D positions of the objects O<sub>1 </sub>and O<sub>3</sub>.</p><p id="p-0069" num="0068">As shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref>, even when the object images overlap with each other, it is shown that both the 3D positions and the attitudes can be accurately estimated even with slight errors.</p><p id="p-0070" num="0069">Note that according to this evaluation, images at 1000 time points (total 3000 images) are taken by each of the three imaging devices <b>20</b>A to <b>20</b>C. Although the object images overlap with each other in one image, images at 71 time points where no object images overlap between the remaining two images are obtained. Consequently, it can be said that, in this case, in comparison with use of two imaging devices <b>20</b>, use of the three imaging devices <b>20</b> can calculate new 2D positions which are 0.071 of the entirety. However, the extent to which the new 2D positions can be calculated depends on moving images used for evaluation.</p><p id="p-0071" num="0070">As described above, use of the estimation device <b>10</b> according to this embodiment can accurately estimate the 3D positions and attitudes of multiple 3D objects that are of the identical type and are incapable of being discriminated from each other in the closed space, using multiple images. Note that this embodiment assumes that the shape of the object is the ellipsoid. Shapes whose 3D attitudes can be calculated using the feature amounts obtained from images of silhouettes of objects can be similarly used as approximated 3D shapes other than ellipsoids.</p><p id="p-0072" num="0071">&#x3c;Hardware Configuration&#x3e;</p><p id="p-0073" num="0072">Lastly, a hardware configuration of the estimation device <b>10</b> according to this embodiment is described with reference to <figref idref="DRAWINGS">FIG. <b>10</b></figref>. <figref idref="DRAWINGS">FIG. <b>10</b></figref> shows an example of the hardware configuration of the estimation device <b>10</b> according to this embodiment.</p><p id="p-0074" num="0073">As shown in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, the estimation device <b>10</b> according to this embodiment is achieved by a typical computer or computer system, and includes an input device <b>201</b>, a display device <b>202</b>, an external I/F <b>203</b>, a communication I/F <b>204</b>, a processor <b>205</b>, and a memory device <b>206</b>. These pieces of hardware are communicably connected to each other via a bus <b>207</b>.</p><p id="p-0075" num="0074">The input device <b>201</b> is, for example, a keyboard, a mouse, a touch panel, or the like. The display device <b>202</b>, for example, is a display or the like. Note that it may be configured such that the estimation device <b>10</b> does not include at least one of the input device <b>201</b> and the display device <b>202</b>.</p><p id="p-0076" num="0075">The external I/F <b>203</b> is an interface with an external device. The external device may be a recording medium <b>203</b><i>a </i>or the like. The estimation device <b>10</b> can perform, for example, reading from and writing to the recording medium <b>203</b><i>a </i>via the external I/F <b>203</b>. The recording medium <b>203</b><i>a </i>may store, for example, one or more programs that achieve the respective function units (the position estimation unit <b>101</b>, the feature amount extraction unit <b>102</b>, and the attitude estimation unit <b>103</b>) that the estimation device <b>10</b> includes.</p><p id="p-0077" num="0076">Note that the recording medium <b>203</b><i>a </i>is, for example, a CD (Compact Disc), a DVD (Digital Versatile Disk), an SD (Secure Digital) memory card, a USB (Universal Serial Bus) memory card or the like.</p><p id="p-0078" num="0077">The communication I/F <b>204</b> is an interface for connecting the estimation device <b>10</b> to the communication network. Note that the one or more programs that achieve the respective function units included in the estimation device <b>10</b> may be obtained (downloaded) from a predetermined server device or the like via the communication I/F <b>204</b>.</p><p id="p-0079" num="0078">The processor <b>205</b> is various types of operation devices, for example, a CPU (Central Processing Unit), a GPU (Graphics Processing Unit), etc. The function units that the estimation device <b>10</b> includes are achieved by, for example, processes that the one or more programs stored in the memory device <b>206</b> or the like cause the processor <b>205</b> to execute.</p><p id="p-0080" num="0079">The memory device <b>206</b> is various types of storage devices, such as an HDD (Hard Disk Drive), an SSD (Solid State Drive), a RAM (Random Access Memory), a ROM (Read Only Memory), and a flash memory, for example. The storage unit <b>104</b> that the estimation device <b>10</b> includes can be achieved using the memory device <b>206</b>, for example. Note that for example, the storage unit <b>104</b> may be achieved using the storage device (e.g., a database server or the like) connected to the estimation device <b>10</b> via the communication network.</p><p id="p-0081" num="0080">The estimation device <b>10</b> according to this embodiment includes the hardware configuration shown in <figref idref="DRAWINGS">FIG. <b>10</b></figref>. Accordingly, the position estimation process and the attitude estimation process described above can be achieved. Note that the hardware configuration shown in <figref idref="DRAWINGS">FIG. <b>10</b></figref> is only an example. The estimation device <b>10</b> may have another hardware configuration. For example, the estimation device <b>10</b> may include multiple processors <b>205</b>, and include multiple memory devices <b>206</b>.</p><p id="p-0082" num="0081">The present invention is not limited to the specifically disclosed embodiment described above. Various modifications, changes, combination with known techniques and the like are allowed without departing from the description of Claims.</p><heading id="h-0011" level="1">REFERENCE SIGNS LIST</heading><p id="p-0083" num="0082"><b>10</b> Estimation device</p><p id="p-0084" num="0083"><b>101</b> Position estimation unit</p><p id="p-0085" num="0084"><b>102</b> Feature amount extraction unit</p><p id="p-0086" num="0085"><b>103</b> Attitude estimation unit</p><p id="p-0087" num="0086"><b>104</b> Storage unit</p><p id="p-0088" num="0087"><b>201</b> Input device</p><p id="p-0089" num="0088"><b>202</b> Display device</p><p id="p-0090" num="0089"><b>203</b> External I/F</p><p id="p-0091" num="0090"><b>203</b><i>a </i>Recording medium</p><p id="p-0092" num="0091"><b>204</b> Communication I/F</p><p id="p-0093" num="0092"><b>205</b> Processor</p><p id="p-0094" num="0093"><b>206</b> Memory device</p><p id="p-0095" num="0094"><b>207</b> Bus</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An estimation method causing a computer to execute:<claim-text>calculating, using a plurality of images obtained by a plurality of imaging devices imaging a three-dimensional space in which a plurality of objects reside, representative points of pixel regions representing the plurality of objects among pixel regions of the images;</claim-text><claim-text>estimating positions of the plurality of objects in the three-dimensional space, based on the representative points;</claim-text><claim-text>extracting predetermined feature amounts from image regions representing the plurality of objects; and</claim-text><claim-text>estimating attitudes of the plurality of objects in the three-dimensional space, through a preliminarily learned regression model, using the positions of the plurality of objects and the predetermined feature amounts.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The estimation method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein the regression model is learned using a dataset that includes:</claim-text><claim-text>first learning data that adopts the predetermined feature amounts extracted from the images obtained by the plurality of imaging devices imaging the three-dimensional space in which the plurality of objects reside, and the positions of the plurality of objects in the three-dimensional space, as input data, and the first learning data that adopts attitude matrices representing the attitudes of the plurality of objects, as training data; and</claim-text><claim-text>second learning data that adopts feature amounts set to be predetermined values as the predetermined feature amounts when the image regions overlap with each other among two or more objects of the plurality of objects, and the positions of the plurality of objects in the three-dimensional space, as input data, and adopts the attitude matrices representing the attitudes of the plurality of objects as training data, and<claim-text>when at least parts of the image regions representing the plurality of objects among two or more objects of the plurality of objects overlap with each other, the extracting further comprises adopting the predetermined values as the feature amounts.</claim-text></claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The estimation method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the plurality of objects represent objects that are of an identical type and cannot be discriminated from each other, and<claim-text>the estimating further comprises solving, between two images among the images, a correspondence problem of the representative points of the image regions representing the respective objects of the plurality of objects, under an epipolar constraint condition, to identify a pair of the representative points corresponding to an identical object between the two images, and estimating a position of the identical object from the identified pair of the representative points.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The estimation method according to <claim-ref idref="CLM-00003">claim 3</claim-ref>,<claim-text>wherein the estimating comprises identifying representative points having a minimum sum of distances from an epipolar line between the two images, as the pair of the representative points corresponding to the identical object.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The estimation method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein when the image region representing an object of the plurality of objects can be approximated as an elliptic region, the extracting further comprises extracting a first angle between a positive sense of a major axis of an ellipse represented by the elliptic region and a positive sense of a horizontal direction of an image associated with the object, a second angle between a positive sense of a minor axis of the ellipse and the positive sense of the horizontal direction of the image, a ratio between a length of the major axis and a length of the minor axis, and a product of the length of the major axis and the length of the minor axis, as the predetermined feature amounts.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The estimation method according to <claim-ref idref="CLM-00005">claim 5</claim-ref>,<claim-text>wherein the extracting further comprises determining a positive direction of the major axis and a positive direction of the minor axis, using differences between luminances of pixel regions representing respective four regions that divide a surface of the object among the pixel regions of the image.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. An estimation device comprising a processor configured to execute a method comprising:<claim-text>calculating, using a plurality of images obtained by a plurality of imaging devices imaging a three-dimensional space in which a plurality of objects reside, representative points of pixel regions representing the plurality of objects among pixel regions of the images;</claim-text><claim-text>estimating positions of the plurality of objects in the three-dimensional space, based on the representative points;</claim-text><claim-text>extracting predetermined feature amounts from image regions representing the plurality of objects; and</claim-text><claim-text>estimating attitudes of the plurality of objects in the three-dimensional space, through a preliminarily learned regression model, using the positions estimated by the position estimation means, and the predetermined feature amounts.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. A computer-readable non-transitory recording medium storing computer-executable program instructions that when executed by a processor cause a computer to execute a method comprising:<claim-text>calculating, using a plurality of images obtained by a plurality of imaging devices imaging a three-dimensional space in which a plurality of objects resides, representative points of pixel regions representing the plurality of objects among pixel regions of the images;</claim-text><claim-text>estimating positions of the plurality of objects in the three-dimensional space, based on the representative points;</claim-text><claim-text>extracting predetermined feature amounts from image regions representing the plurality of objects; and</claim-text><claim-text>estimating attitudes of the plurality of objects in the three-dimensional space, through a preliminarily learned regression model, using the positions estimated by the position estimation means, and the predetermined feature amounts.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The estimation method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>,<claim-text>wherein the plurality of objects represents objects that are of an identical type and cannot be discriminated from each other, and</claim-text><claim-text>the estimating further comprises solving, between two images among the images, a correspondence problem of the representative points of the image regions representing the respective objects, under an epipolar constraint condition, to identify a pair of the representative points corresponding to an identical object between the two images, and estimating a position of the identical object from the identified pair of the representative points.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The estimation device according to <claim-ref idref="CLM-00007">claim 7</claim-ref>,<claim-text>wherein the regression model is learned using a dataset that includes:</claim-text><claim-text>first learning data that adopts the predetermined feature amounts extracted from the images obtained by the plurality of imaging devices imaging the three-dimensional space in which the plurality of objects reside, and the positions of the plurality of objects in the three-dimensional space, as input data, and the first learning data that adopts attitude matrices representing the attitudes of the plurality of objects, as training data; and</claim-text><claim-text>second learning data that adopts feature amounts set to be predetermined values as the predetermined feature amounts when the image regions overlap with each other among two or more objects of the plurality of objects, and the positions of the plurality of objects in the three-dimensional space, as input data, and adopts the attitude matrices representing the attitudes of the plurality of objects as training data, and<claim-text>when at least parts of the image regions representing the plurality of objects among two or more objects of the plurality of objects overlap with each other, the extracting further comprises adopting the predetermined values as the predetermined feature amounts.</claim-text></claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The estimation device according to <claim-ref idref="CLM-00007">claim 7</claim-ref>,<claim-text>wherein the plurality of objects represents objects that are of an identical type and cannot be discriminated from each other, and</claim-text><claim-text>the estimating further comprises solving, between two images among the images, a correspondence problem of the representative points of the image regions representing the respective objects, under an epipolar constraint condition, to identify a pair of the representative points corresponding to an identical object between the two images, and estimating a position of the identical object from the identified pair of the representative points.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The estimation device according to <claim-ref idref="CLM-00007">claim 7</claim-ref>,<claim-text>wherein when the image region representing an object of the plurality of objects can be approximated as an elliptic region, the extracting further comprises extracting a first angle between a positive sense of a major axis of an ellipse represented by the elliptic region and a positive sense of a horizontal direction of an image associated with the object, a second angle between a positive sense of a minor axis of the ellipse and the positive sense of the horizontal direction of the image, a ratio between a length of the major axis and a length of the minor axis, and a product of the length of the major axis and the length of the minor axis, as the predetermined feature amounts.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The computer-readable non-transitory recording medium according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the regression model is learned using a dataset that includes:<claim-text>first learning data that adopts predetermined the predetermined feature amounts extracted from the images obtained by the plurality of imaging devices imaging the three-dimensional space in which the plurality of objects reside, and the positions of the plurality of objects in the three-dimensional space, as input data, and the first learning data that adopts attitude matrices representing the attitudes of the plurality of objects, as training data; and</claim-text><claim-text>second learning data that adopts feature amounts set to be predetermined values as the predetermined feature amounts when the image regions overlap with each other among two or more objects of the plurality of objects, and the positions of the plurality of objects in the three-dimensional space, as input data, and adopts the attitude matrices representing the attitudes of the plurality of objects as training data, and</claim-text><claim-text>when at least parts of the image regions representing the plurality of objects among two or more objects of the plurality of objects overlap with each other, the extracting further comprises adopting the predetermined values as the predetermined feature amounts.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The computer-readable non-transitory recording medium according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the plurality of objects represents objects that are of an identical type and cannot be discriminated from each other, and<claim-text>the estimating further comprises solving, between two images among the images, a correspondence problem of the representative points of the image regions representing the respective objects, under an epipolar constraint condition, to identify a pair of the representative points corresponding to an identical object between the two images, and estimating a position of the identical object from the identified pair of the representative points.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The computer-readable non-transitory recording medium according to <claim-ref idref="CLM-00008">claim 8</claim-ref>,<claim-text>wherein when the image region representing an object of the plurality of objects can be approximated as an elliptic region, the extracting further comprises extracting a first angle between a positive sense of a major axis of an ellipse represented by the elliptic region and a positive sense of a horizontal direction of an image associated with the object, a second angle between a positive sense of a minor axis of the ellipse and the positive sense of the horizontal direction of the image, a ratio between a length of the major axis and a length of the minor axis, and a product of the length of the major axis and the length of the minor axis, as the predetermined feature amounts.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The estimation device according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the regression model is learned using a dataset that includes:<claim-text>first learning data that adopts predetermined feature amounts extracted from the images obtained by the plurality of imaging devices imaging the three-dimensional space in which the plurality of objects reside, and the positions of the plurality of objects in the three-dimensional space, as input data, and the first learning data that adopts attitude matrices representing the attitudes of the plurality of objects, as training data; and</claim-text><claim-text>second learning data that adopts the predetermined feature amounts set to be predetermined values as the predetermined feature amounts when the image regions overlap with each other among two or more objects of the plurality of objects, and the positions of the plurality of objects in the three-dimensional space, as input data, and adopts the attitude matrices representing the attitudes of the plurality of objects as training data, and<claim-text>when at least parts of the image regions representing objects among two or more objects of the plurality of objects overlap with each other, the extracting further comprises adopting the predetermined values as the predetermined feature amounts.</claim-text></claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The estimation device according to <claim-ref idref="CLM-00011">claim 11</claim-ref>,<claim-text>wherein the estimating comprises identifying representative points having a minimum sum of distances from an epipolar line between the two images, as the pair of the representative points corresponding to the identical object.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The estimation device according to <claim-ref idref="CLM-00012">claim 12</claim-ref>,<claim-text>wherein the extracting further comprises determining a positive direction of the major axis and a positive direction of the minor axis, using differences between luminances of pixel regions representing respective four regions that divide a surface of the object among the pixel regions of the image.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The computer-readable non-transitory recording medium according to <claim-ref idref="CLM-00013">claim 13</claim-ref>,<claim-text>wherein the plurality of objects represents objects that are of an identical type and cannot be discriminated from each other, and</claim-text><claim-text>the estimating further comprises solving, between two images among the images, a correspondence problem of the representative points of the image regions representing the respective objects, under an epipolar constraint condition, to identify a pair of the representative points corresponding to an identical object between the two images, and estimating a position of the identical object from the identified pair of the representative points.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The computer-readable non-transitory recording medium according to <claim-ref idref="CLM-00014">claim 14</claim-ref>,<claim-text>wherein the estimating comprises identifying representative points having a minimum sum of distances from an epipolar line between the two images, as the pair of the representative points corresponding to the identical object.</claim-text></claim-text></claim></claims></us-patent-application>