<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005155A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005155</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17856847</doc-number><date>20220701</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>KR</country><doc-number>10-2021-0086859</doc-number><date>20210702</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>11</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>593</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>11</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>593</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20081</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30024</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>10012</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">METHOD AND APPARATUS FOR GENERATING  HIGH DEPTH OF FIELD IMAGE, AND APPARATUS FOR TRAINING HIGH DEPTH OF FIELD IMAGE GENERATION MODEL USING STEREO IMAGE</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>VIEWORKS CO., LTD.</orgname><address><city>Anyang-si</city><country>KR</country></address></addressbook><residence><country>KR</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>KANG</last-name><first-name>Yu Jung</first-name><address><city>Anyang-si</city><country>KR</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>JIN</last-name><first-name>Min Gyu</first-name><address><city>Seoul</city><country>KR</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>VIEWORKS CO., LTD.</orgname><role>03</role><address><city>Anyang-si</city><country>KR</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A high depth of field image generating apparatus according to the present disclosure includes a region segmentation unit which segments a region for a stereo image to generate region data, a depth estimating unit which estimates depths for the stereo image to generate depth data, and a high depth of field image generating unit which generates a high depth of field image from the stereo image, the region data, and the depth data.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="48.18mm" wi="158.75mm" file="US20230005155A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="199.22mm" wi="72.05mm" orientation="landscape" file="US20230005155A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="91.69mm" wi="140.97mm" file="US20230005155A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="163.07mm" wi="149.44mm" file="US20230005155A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="163.07mm" wi="149.18mm" file="US20230005155A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="197.87mm" wi="142.24mm" file="US20230005155A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="96.27mm" wi="92.03mm" file="US20230005155A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="202.61mm" wi="86.44mm" orientation="landscape" file="US20230005155A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="197.87mm" wi="119.63mm" orientation="landscape" file="US20230005155A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="189.65mm" wi="139.19mm" orientation="landscape" file="US20230005155A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application claims priority to and the benefit of Korean Patent Application No. 10-2021-0086859 filed in the Korean Intellectual Property Office on Jul. 2, 2021, the entire contents of which are incorporated herein by reference.</p><heading id="h-0002" level="1">BACKGROUND</heading><heading id="h-0003" level="1">Field</heading><p id="p-0003" num="0002">The present disclosure relates to a method and an apparatus for generating a high depth of field image, and an apparatus for training a high depth of field image generation model.</p><heading id="h-0004" level="1">Description of the Related Art</heading><p id="p-0004" num="0003">A microscope is an instrument which magnifies and observes microscopic objects or microorganisms which are difficult to be observed with the human eye. A slide scanner which interworks with the microscope is a device which automatically scans one or a plurality of slides to store, observe, and analyze an image. In general, since a microscope uses a high magnification lens for capturing tissues or cells, it is difficult to simultaneously capture cells distributed at various depths due to a low depth of field. For example, when a tissue specimen with a thickness of 4 um is used for pathological examination, if a depth of focus of a 40&#xd7; objective lens is up to 1 um and two or more cells are distributed with a height difference of 1 um or larger in a single capturing area, it is difficult to capture all the cells to be in focus in one image. Further, an object which has a three-dimensional shape rather than a planar shape needs to be captured so that it is necessary to focus on an uneven surface. In general, many cells in the image are located in different positions so that it is difficult to obtain a focused image as a whole.</p><p id="p-0005" num="0004">Accordingly, in order to obtain a high depth of field image in a microscope or a slide scanner, a z-stacking (or focus stacking) technique is used that captures a plurality of images in a fixed position of the x and y-axis while changing a focal plane through z-axis, and then combines the images.</p><p id="p-0006" num="0005">However, the z-stacking technique has many problems in that in order to change the focal plane of the z-axis, an optical structure for multiple capturing at different depths of field needs to be provided and a very larger number of focal planes are repeatedly determined, changed, captured, and combined so that it takes a long time to capture. Further, a method of determining a focusing distance (depth) using a technology such as laser is not appropriate to capture an object which requires image-based focus determination.</p><heading id="h-0005" level="1">SUMMARY</heading><p id="p-0007" num="0006">A technical object to be achieved by the present disclosure is to provide a method and an apparatus for generating a high depth of field image to generate a high depth of field image from a captured image without requiring an optical structure for multiple capturing at different depths of field and an apparatus for training a high depth of field image generation model therefor.</p><p id="p-0008" num="0007">In order to achieve the above-described technical objects, according to an aspect of the present disclosure, a high depth of field image generating apparatus includes: a region segmentation unit which segments a region for a stereo image to generate region data; a depth estimating unit which estimates depths for the stereo image to generate depth data; and a high depth of field image generating unit which generates a high depth of field image from the stereo image, the region data, and the depth data.</p><p id="p-0009" num="0008">The high depth of field image generating unit may generate the high depth of field image using a trained deep learning model.</p><p id="p-0010" num="0009">The region segmentation unit or the depth estimating unit may generate the region data or the depth data using the trained deep learning model, respectively.</p><p id="p-0011" num="0010">The region segmentation unit may segment the region for each image which configures the stereo image to generate region data and the depth estimating unit may generate depth data for each segmented region.</p><p id="p-0012" num="0011">The stereo image may be obtained by capturing a tissue or a cell.</p><p id="p-0013" num="0012">The trained deep learning model may be implemented to simulate blind deconvolution using a point-spread function.</p><p id="p-0014" num="0013">In order to achieve the above-described technical objects, according to another aspect of the present disclosure, a high depth of field image generating method includes: a region segmentation step of segmenting a region for a stereo image to generate region data; a depth estimating step of estimating depths for the stereo image to generate depth data; and a high depth of field image generating step of generating a high depth of field image from the stereo image, the region data, and the depth data.</p><p id="p-0015" num="0014">The high depth of field image generating step may use the trained deep learning model.</p><p id="p-0016" num="0015">The region segmentation step and the depth estimating step may use a trained deep learning model.</p><p id="p-0017" num="0016">The region segmentation unit may segment the region for each image which configures the stereo image to generate region data and the depth estimating unit generates depth data for each segmented region.</p><p id="p-0018" num="0017">The stereo image may be obtained by capturing a tissue or a cell.</p><p id="p-0019" num="0018">The trained deep learning model may be implemented to simulate blind deconvolution using a point-spread function.</p><p id="p-0020" num="0019">In order to achieve the above-described technical objects, according to another aspect of the present disclosure, an apparatus for training a high depth of field image generation model includes: as a learning model implemented to output a high depth of field image from an input stereo image, a region segmentation unit which segments a region for a stereo image to generate region data; a depth estimating unit which estimates depths for the stereo image to generate depth data; and a high depth of field image generating unit which generates a high depth of field image from the stereo image, the region data, and the depth data; and a training unit which trains the learning model with learning data including stereo image and a high depth of field image corresponding thereto.</p><p id="p-0021" num="0020">The training unit may calculate a cost function from a high depth of field image output from the learning model and the high depth of field reference image and may train the learning model using the cost function.</p><p id="p-0022" num="0021">The high depth of field image generating unit may generate the high depth of field image using a deep learning model.</p><p id="p-0023" num="0022">The region segmentation unit or the depth estimating unit may generate the region data or the depth data using the deep learning model.</p><p id="p-0024" num="0023">The region segmentation unit may segment the region for each image which configures the stereo image to generate region data and the depth estimating unit may generate depth data for each segmented region.</p><p id="p-0025" num="0024">The stereo image may be obtained by capturing a tissue or a cell.</p><p id="p-0026" num="0025">The deep learning model may be implemented to simulate blind deconvolution using a point-spread function.</p><p id="p-0027" num="0026">The apparatus for training a high depth of field image generation model may further include: a preprocessing unit which preprocesses the stereo image to input the preprocessed image to the learning model.</p><p id="p-0028" num="0027">According to the present disclosure, an optical structure for multiple capturing at different depths of field is not required and a high depth of field image can be generated from the stereo image so that a capturing time is significantly shortened and the high depth of field image can be effectively acquired from the stereo image.</p><p id="p-0029" num="0028">Basic information required to change a focal plane is a depth (z-axis) at which the object is located so that the present disclosure utilizes a stereo technique like the human eye to identify a depth at which the cell is located. Further, in order to remove a time-consuming repeated capturing process, a deconvolution algorithm is performed by means of the stereo image to increase a depth of field. Further, an image which is focused for every object area may be generated by deconvolution restoration based on information obtained by distinguishing an object and estimating a depth of each object from the stereo image.</p><p id="p-0030" num="0029">According to the present disclosure, an all-in-focus high depth of field image may be generally generated from objects (cells, etc.) located in various positions even from a slide which is thicker than that in the related art.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0006" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates an apparatus for training a high depth of field image generation model according to an exemplary embodiment of the present disclosure;</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a view for explaining an example of a stereo image;</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIGS. <b>3</b>A and <b>3</b>B</figref> are views for explaining an example of determining an angle between cameras for the cytopathology and the histopathology;</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a view for explaining an example of a high depth of field reference image;</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a view for explaining a process of generating region data;</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a view for explaining a process of estimating a depth by means of a size, a center position and an observation position of a cell;</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates an example of depth data obtained for a stereo image of <figref idref="DRAWINGS">FIG. <b>2</b></figref>;</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates a high depth of field image generating apparatus according to an exemplary embodiment of the present disclosure;</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>9</b></figref> illustrates an example of a cell region segmentation image, region data, and depth data obtained from a stereo image according to an exemplary embodiment of the present disclosure; and</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>10</b></figref> illustrates an example of a high depth of field image obtained from a stereo image, region data, and depth data according to an exemplary embodiment of the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0007" level="1">DETAILED DESCRIPTION OF THE EMBODIMENT</heading><p id="p-0041" num="0040">Hereinafter, exemplary embodiments of the present invention will be described in detail with reference to the drawings. Substantially same components in the following description and the accompanying drawings may be denoted by the same reference numerals and redundant description will be omitted. Further, in the description of the exemplary embodiment, if it is considered that specific description of related known configuration or function may cloud the gist of the present invention, the detailed description thereof will be omitted.</p><p id="p-0042" num="0041">The inventor of the present application believed that a stereo image captured by a stereo camera has depth information and the depth information is closely related to the focus so that the high depth of field image can be generated from the stereo image by means of a deep learning model to conceive the present invention. In the exemplary embodiment of the present disclosure, even though the stereo image is explained using an image obtained by capturing the cell as an example, there may be various objects to be captured to include not only living tissues such as cells, but also tissues, materials, or products. Further, in the following description, the high depth of field image is an image having a range of a focused depth broader than that of the single captured image, and for example, refers to an image having a range of a focused depth broader than that of each image which configures the stereo image.</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates an apparatus for training a high depth of field image generation model according to an exemplary embodiment of the present disclosure. The apparatus for training a high depth of field image generation model according to an exemplary embodiment includes an image preprocessing unit which preprocesses a stereo image, a learning model <b>120</b> implemented to output a high depth of field image from the stereo image, and a training unit <b>130</b> which trains the learning model <b>120</b> with learning data.</p><p id="p-0044" num="0043">The learning data is formed of a data set of a stereo image and a high depth of field reference image corresponding to the stereo image. <figref idref="DRAWINGS">FIG. <b>2</b></figref> is a view for explaining an example of a stereo image. The stereo image includes a left image and a right image captured by a left camera and a right camera. Referring to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the left camera and the right camera capture different focal planes (a depth of field range) which are inclined at a predetermined angle from each other. For example, when there are three cells C<b>1</b>, C<b>2</b>, C<b>3</b>, only the cells C<b>2</b> and C<b>3</b> may be captured from the left image and only the cells C<b>1</b> and C<b>2</b> may be captured from the right image. Alternatively, the identical cell is in focus to be clear in one image, but is out of focus to be blurry in the other image.</p><p id="p-0045" num="0044">The inclined angle of the left camera and the right camera may be determined in consideration of a target depth of field, a size of a camera sensor to be used, an optical magnification, etc. <figref idref="DRAWINGS">FIGS. <b>3</b>A and <b>3</b>B</figref> are views for explaining an example of determining an angle between cameras for the cytopathology and the histopathology. For example, when it is assumed that a camera sensor of a size of 22.5&#xd7;16.9 mm is used and only 20 mm (w=20) is used as a cropped image after removing an outer distortion, if a depth of field with a thickness of 50 um with a 40&#xd7; magnification is observed (m is 40 and d is 0.05), &#x3b8;&#x2248;0.01 rad&#x2248;5.74&#xb0; is derived by Equation w/m&#xd7;sin &#x3b8;=d. Since the angle between two camera sensors is 20, the angle is approximately 12&#xb0; in the cytopathology as illustrated in <figref idref="DRAWINGS">FIG. <b>3</b>A</figref> and is approximately 1.2&#xb0; in the histopathology as illustrated in <figref idref="DRAWINGS">FIG. <b>3</b>B</figref>. As described above, an appropriate angle is derived according to a depth and a camera configuration desired for various objects and targets to obtain a stereo image.</p><p id="p-0046" num="0045">In the meantime, the stereo image may be obtained using two or more cameras having different optical paths or may be obtained using a structure in which one optical path is split into two or more optical paths using a light splitting unit (for example, a beam splitter, a prism, or a mirror) to obtain an image having a focal plane inclined at a predetermined angle by a plurality of cameras.</p><p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a view for explaining an example of a high depth of field reference image. The high depth of field reference image is a high depth of field image obtained for the same target of the stereo image and is obtained using the existing z-stack technique. That is, the high depth of field reference image may be obtained by combining a plurality of images which is captured while changing a z-axis focal plane in the same (x,y) offset position as the stereo image. Referring to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, in the high depth of field reference image, all three cells C<b>1</b>, C<b>2</b>, and C<b>3</b> are represented.</p><p id="p-0048" num="0047">The image preprocessing unit <b>110</b> performs image augmentation and image normalization as preprocesses for the stereo image.</p><p id="p-0049" num="0048">Training data is increased by means of the image augmentation to ensure the robustness against a noise and the model is trained for various capturing conditions. The image preprocessing unit <b>110</b> increases an amount of training data by arbitrarily adjusting a brightness, a contrast, and an RGB value of the image. For example, the image preprocessing unit <b>110</b> adjusts the brightness, the contrast, the RGB value or a distribution thereof according to an average or a standard deviation or adjusts a staining strength by arbitrarily adjusting absorbance coefficients obtained according to Beer-Lambert Law of Light Absorption.</p><p id="p-0050" num="0049">The image normalization may improve a performance of the learning model and increase a learning convergence speed. In general, the learning data of the stereo image is an image obtained by capturing various cells to be observed with various equipment at various points of view so that capturing conditions are different. Even with the same cell to be observed, different images may be obtained by various environment variables simulated by a capturing condition or augmentation. Therefore, the image normalization may be performed to minimize various variations and match a color space of the image. For example, staining normalization may use light absorbance coefficients of H&#x26;E staining according to the Beer-Lambert law.</p><p id="p-0051" num="0050">Depending on the exemplary embodiment, both the image augmentation and the image normalization are performed or only any one of them is performed or both are omitted according to the restriction of the learning resource. Referring to <figref idref="DRAWINGS">FIG. <b>1</b></figref> again, the learning model <b>120</b> according to the exemplary embodiment of the present disclosure is built to include a region segmentation unit <b>121</b> which segments a cell region for an input stereo image to generate region data, a depth estimating unit <b>122</b> which estimates depths for the input stereo image to generate depth data, and a high depth of field image generating unit <b>123</b> which generates a high depth of field image from the region data output from the region segmentation unit <b>121</b> and the depth data output from the depth estimating unit <b>122</b>.</p><p id="p-0052" num="0051">The region segmentation unit <b>121</b> segments a cell region for a left image and a right image and combines the left image and the right image in which the cell region is segmented to generate region data. When the cell region is segmented, the image segmentation unit <b>121</b> separates a cell region and other region as a foreground and a background and separates each cell as a separate object.</p><p id="p-0053" num="0052">A process of segmenting a cell region, for example, may be performed by means of a deep learning model such as DeepLab v3 or U-Net. When the deep learning model is used, a segmentation speed is fast, it is robust to various capturing conditions such as a variation of an image, an accuracy for a complex image is high, and a fine tuning is possible according to the purpose. When there is a restriction in a resource for learning, the process of segmenting a cell region may be performed using the existing region segmentation algorithm such as Otsu Thresholding, Region Growing, Watershed algorithm, Graph cut, Active contour model, or Active shape model.</p><p id="p-0054" num="0053">The left image and the right image which configure the stereo image have a low depth of field so that all cells may not be observed in two images in the same manner. Accordingly, the left image and the right image may be combined to show cells present at different depths. In the process of combining the left image and the right image, positions of the cells are combined and naturally stitched by means of the deep learning model. <figref idref="DRAWINGS">FIG. <b>5</b></figref> is a view for explaining a process of generating region data. Even though the stereo disparity is considered, if the left and right images are different in the same position, a background color estimated from the entire image is considered to select a foreground color far from the background color from the left and right images. A color having the largest distribution in the edge region of the image may be selected as the background color. As illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the region data may have a value which separates a background and each cell region. For example, as illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, 0 is assigned to the background and different pixel values such as 1, 2, 3 may be assigned to the cell regions.</p><p id="p-0055" num="0054">In general, the stereo image aims to generate a three-dimensional image by estimating a distance or a depth of an object using a visual difference between two images. In contrast, according to the exemplary embodiment of the present disclosure, the region is segmented using the stereo image. Specifically, in the high magnification images, a depth of focus is very low so that very different objects are frequently captured in the stereo image obtained by simultaneously capturing a single capturing region. For example, if a specific cell is captured only in one image of two images which configure the stereo image, this cell may be very blurred or removed from the image when a general stereo depth estimation method is used. In the exemplary embodiment of the present disclosure, detailed regions are segmented based on shapes and forms of the captured object from two stereo images. At this time, when the segmented positions of the object of two stereo images do not match, depth information on the region is used to clearly express the object whose region is segmented in a final image. By doing this, a high depth of field image with a depth of field beyond a physically limited depth of focus of the objective lens is generated and a depth of field is determined according to a disparity angle of the stereo image.</p><p id="p-0056" num="0055">Generally, even though the image analysis is performed on a high magnification image to determine a position of a lens for an optimal focus, in the exemplary embodiment of the present disclosure, not only an image analyzing method, but also a method of measuring a tissue specimen slide height (position) by a laser sensor is available. According to the method of estimating a slide height by a laser sensor, a precision is lower than that of a lens depth of focus. Further, even though the height of the slide is identified, which height (position) in the thickness of the specimen on the slide where the cell is located at is not known. Therefore, the method is not used for a general high-resolution optical imaging system. However, according to the exemplary embodiment of the present disclosure, the depth of focus may be extended to a thickness similar to a thickness of the specimen so that if only the slide height is determined, an optimized focused image can be ensured. Accordingly, it has an advantage of enabling a laser sensor which was difficult to be biologically used in the related art to be applicable to a lens height adjusting method. When the laser sensor is used, as compared with the method of determining a focal height position by means of the image analysis, the focal position may be quickly determined, so that it is possible to increase the capturing speed. Further, according to the present disclosure, a depth of focus of the image is longer than a depth of focus of the lens so that it is not necessary to analyze an optimal focal position of the image while adjusting a height position of the lens in units similar to or shorter than the depth of focus of the lens. Accordingly, there is no need for a nanometer level ultra-precise lens positioning mechanism for lens focal height adjustment.</p><p id="p-0057" num="0056">The depth estimating unit <b>122</b> extracts a feature map for a left image and a right image and estimate a depth by means of a size, a center position, or an observation position of the cell from the feature map. In the center part of the image, there are many regions where objects (cells) are observed identically, but in an outer peripheral portion of the image, there are relatively few regions in which the object is observed identically, due to a depth (z) difference. As the angle of the camera sensor increases for a high depth of field, this phenomenon is more significant and it may affect the image segmentation and depth estimation performance. Accordingly, in order to distinguish the object (cell), a shape, a color, a center position, and an observation position may be considered. A process of extracting a feature map from the left image and the right image may be performed by a convolutional neural network (CNN) model such as VGG, ResNet, or Inception. When there is a restriction on resources for learning, the feature map may be extracted using an existing stereo matching algorithm.</p><p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a view for explaining a process of estimating a depth by means of a size, a center position, and an observation position of a cell. When the identical cell is observed from both the left and right images according to the disparity, the depth may be estimated by comparing a range (size) and a center position of the cell. For example, a cell C<b>2</b> was observed on a right side from a center in an x-axis, a size was smaller in the right image than in the left image, and the center was observed to be biased so that it is estimated that the cell is located above (that is, a shallow depth) a reference focal point (a point where two focal planes intersect, see <figref idref="DRAWINGS">FIG. <b>6</b></figref>). When the cell is observed in only any one of the left and right images, the depth may be estimated in consideration of an angle of the camera which captures the image. For example, the cell C<b>1</b> was observed in the right image and the center thereof was located at the left side from the x-axis center so that it is estimated that the cell C<b>1</b> was located above (that is, a shallow depth) the reference focus. Further, a cell C<b>3</b> was observed in the left image and a center thereof was located at the left side from the x-axis center so that it is estimated that the cell C<b>3</b> is located below (that is, deep depth) the reference focus. At this time, the depth value may be estimated in consideration of the size of the cell and a distance between the center of the cell and the x-axis center. However, the depth value may also be obtained from the feature map of the left and right images by means of a CNN model such as DeepFocus. <figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates an example of depth data obtained for a stereo image of <figref idref="DRAWINGS">FIG. <b>2</b></figref>. For example, the depth data may have a depth value of 0 for the background and a depth value of 1 to 255 for the cell region depending on the depth.</p><p id="p-0059" num="0058">The results of the image segmentation unit <b>121</b> and the depth estimating unit <b>122</b> influence each other to generate a final high depth of field image and increase a level of the image.</p><p id="p-0060" num="0059">When a depth of an object to be captured is estimated, the depth estimating unit <b>122</b> independently estimates the depths of the segmented regions of the left and right images segmented by the region segmentation unit <b>121</b> and then generates final depth data. A depth estimation result of every region may influence the region segmentation result of a high depth of field image to be finally generated. As illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, in the case of the cell C<b>2</b> which is simultaneously captured in the left image and the right image, in the left and right images, the cell C<b>2</b> is segmented in different regions and it is determined which region between regions differently segmented in the same position of the left and right images reflects a size of an actual cell more than the other as follows. The depth estimating unit <b>122</b> determines that an image which is relatively large and has a clear contrast, between the left and right images, is captured at a focal height close to the real one to estimate the depth of the object and have information of the segmented region from the image. Accordingly, the range of the region of the cell C<b>2</b> in the final high depth of field image may be generated to be similar to the size represented in the left image.</p><p id="p-0061" num="0060">Referring to <figref idref="DRAWINGS">FIG. <b>1</b></figref> again, a stereo image, region data, and depth data may be input to the high depth of field image generating unit <b>123</b>. When the stereo image is an RGB image, 8 channels of data including three channels of RGB of each of the left image and the right image, one channel of region data, and one channel of depth data may be input to the high depth of field image generating unit <b>123</b>. The high depth of image generating unit <b>123</b> generates three channels of RGB of high depth of field image from 8 channels of input data by means of the deep learning model.</p><p id="p-0062" num="0061">The high depth of field image generating unit <b>123</b> performs deconvolution in consideration of the depth for each region for the stereo image, from the region data, the depth data, and the stereo image, by means of the deep learning model. The deep learning model may be implemented to determine a degree of the focus of a region or a sub region of the image, that is, an in-focus or out-focus level, from the input data and apply the trained point-spread function. The point-spread function describes a shape of light scattering when a point light source is captured as a function. When the point-spread function is inversely applied, a clear image may be obtained from the blurred image. The deep learning model is a CNN model and may be implemented to simulate blind deconvolution which estimates and applies an inverse function of the point-spread function. In order to improve the learning performance of the deconvolution model, input data may be preprocessed by an algorithm such as the Jansson-Van Cittert algorithm, the Agard's Modified algorithm, Regularized least squares minimization method, maximum likelihood estimation (MLE), and expectation maximization (EM).</p><p id="p-0063" num="0062">The training unit <b>130</b> trains the learning model <b>120</b> with learning data including a stereo image and a high depth of field reference image corresponding to the stereo image. At this time, the training unit <b>130</b> trains the learning model <b>120</b> by an end-to-end learning.</p><p id="p-0064" num="0063">The training unit <b>130</b> calculates a cost function from a high depth of field image output from the high depth of field image generating unit <b>123</b> and the high depth of field reference image and updates parameters (weights) of the learning model <b>120</b> using the cost function. When all the region segmentation unit <b>121</b>, the depth estimating unit <b>122</b>, and the high depth of field image generating unit <b>123</b> which configure the learning model <b>120</b> are implemented by the deep learning model, the parameters those of may be updated by the learning process. When some of the units is implemented by the deep learning model, parameters of the corresponding deep learning model may be updated. The cost function may be configured by a sum or a weighted sum of the loss function such as residual (a difference between the output high depth of field image and the high depth of field reference image), a peak signal-to-noise ratio (PSNR), a mean squared error (MSE), a structural similarity (SSIM), or a perceptual loss. The Residual, PSNR, and MSE may be used to reduce an absolute error between the output high depth of field image and the high depth of field reference image. SSIM may be used to improve the learning performance by reflecting a structural feature such as a luminance or a contrast. The perceptual loss is used to improve a learning performance for a detailed part and a feature perceived by the human. In order to improve the performance of the region segmentation unit <b>121</b>, as a loss function, a segmentation loss may be additionally used. The segmentation loss uses a Dice coefficient equation which compares region data output from the region segmentation unit <b>121</b> and a region segmentation label of a high depth of field reference image.</p><p id="p-0065" num="0064">The training unit <b>130</b> updates the parameter of the learning model <b>120</b> using an error back propagation method. The back propagation value may be adjusted by means of an optimization algorithm. For example, a searching direction, a learning strength (learning rate), decay, and momentum may be adjusted based on the pervious state (a back propagation value and direction). By doing this, the learning direction may be optimized to be robust against the noise and increase the speed. As the optimization algorithm, Adam optimizer or stochastic gradient descent (SGD), AdaGrad, and RMSProp may be used. Further, batch normalization may be used to improve the learning speed and the robustness.</p><p id="p-0066" num="0065">The training unit <b>130</b> may train the learning model <b>120</b> until the value of the cost function is reduced below a predetermined level or reaches a set epoch, by means of the learning process.</p><p id="p-0067" num="0066"><figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates a high depth of field image generating apparatus according to an exemplary embodiment of the present disclosure. The high depth of field image generating apparatus according to the exemplary embodiment includes an image preprocessing unit <b>110</b>&#x2032; which preprocesses a stereo image and a learning model <b>120</b> which is trained to output a high depth of field image from the stereo image, by means of the apparatus for training a high depth of field image generation model.</p><p id="p-0068" num="0067">The image preprocessing unit <b>110</b>&#x2032; performs image normalization as preprocesses for the stereo image. The image normalization may be the same as the image normalization performed by the image preprocessing unit <b>110</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0069" num="0068">The learning model <b>120</b> includes a region segmentation unit <b>121</b> which segments a cell region for an input stereo image to generate region data, a depth estimating unit <b>122</b> which estimates depths for the input stereo image to generate depth data, and a high depth of field image generating unit <b>123</b> which generates a high depth of field image from the region data output from the region segmentation unit <b>121</b> and the depth data output from the depth estimating unit <b>122</b>. The high depth of field image generating unit <b>123</b> is implemented by a deep learning model trained by the above-described apparatus for training a high depth of field image generation model. The image segmentation unit <b>121</b> or the depth estimating unit <b>122</b> may also be implemented by the deep learning model trained by the above-described apparatus for training a high depth of field image generation model.</p><p id="p-0070" num="0069"><figref idref="DRAWINGS">FIG. <b>9</b></figref> illustrates an example of a cell region segmentation image, region data, and depth data obtained from a stereo image according to an exemplary embodiment of the present disclosure. Referring to <figref idref="DRAWINGS">FIG. <b>9</b></figref>, region data c obtained by combining an image b<b>1</b> obtained by segmenting the cell region from the left image a<b>1</b> and an image b<b>2</b> obtained by segmenting the cell region from the right image a<b>2</b> and depth data d generated from the left image a<b>1</b> and the right image a<b>2</b> are illustrated.</p><p id="p-0071" num="0070"><figref idref="DRAWINGS">FIG. <b>10</b></figref> illustrates an example of a high depth of field image obtained from a stereo image, region data, and depth data according to an exemplary embodiment of the present disclosure. Referring to <figref idref="DRAWINGS">FIG. <b>10</b></figref>, when the left image (three channels), the right image (three channels), image data (one channel), and the depth data (<b>1</b> channel) are input to the high depth of field image generating unit <b>123</b>, the high depth of field image generating unit <b>123</b> outputs the high depth of field image (three channels) by means of the deep learning model.</p><p id="p-0072" num="0071">The apparatus according to the exemplary embodiments of the present disclosure includes a processor, a permanent storage which stores and executes program data such as a memory or a disk driver, a communication port which communicates with the external device, and a user interface such as a touch panel, a key or a button. Methods which are implemented by a software module or an algorithm may be computer readable codes or program instructions which are executable on the processor and stored on a computer readable recording medium. Here, the computer readable recording medium may include a magnetic storage medium such as a read only memory (ROM), a random access memory (RAM), a floppy disk, and hard disk and an optical reading medium such as CD-ROM or digital versatile disc (DVD). The computer readable recording medium is distributed in computer systems connected through a network so that computer readable code is stored therein and executed in a distributed manner. The medium is readable by the computer, is stored in the memory, and is executed in the processor.</p><p id="p-0073" num="0072">Exemplary embodiments of the present disclosure may be represented with functional block configurations and various processing steps. The functional blocks may be implemented by various numbers of hardware and/or software configurations which execute specific functions. For example, the exemplary embodiment may employ integrated circuit configurations such as a memory, a processing, a logic, or a look-up table in which various functions are executable by the control of one or more microprocessors or the other control devices. Similar to execution of the components of the present disclosure with software programming or software elements, the exemplary embodiment may be implemented by programming or scripting languages such as C, C++, Java, assembler including various algorithms implemented by a combination of data structures, processes, routines, or other program configurations. The functional aspects may be implemented by an algorithm executed in one or more processors. Further, the exemplary embodiment may employ the related art for the electronic environment setting, signal processing and/or data processing. The terms such as &#x201c;mechanism&#x201d;, &#x201c;element&#x201d;, &#x201c;means&#x201d;, and &#x201c;configuration&#x201d; are broadly used and are not limited to mechanical and physical configurations. The terms may include meaning of a series of routines of a software in association with the processor.</p><p id="p-0074" num="0073">Specific executions described in the exemplary embodiments are examples, so that the range of the exemplary embodiment is not limited by any way. For simplicity of the specification, the description of another functional aspects of the electronic configurations, control systems, software, and the systems of the related art may be omitted. Further, connections of components illustrated in the drawing with lines or connection members illustrate functional connection and/or physical or circuit connections. Therefore, in the actual device, it is replaceable or represented as additional various functional connections, physical connections, or circuit connections. Unless specifically stated as &#x201c;essential&#x201d;, &#x201c;importantly&#x201d;, it may not be an essential configuration to apply the present disclosure.</p><p id="p-0075" num="0074">For now, the present disclosure has been described with reference to the exemplary embodiments. It is understood to those skilled in the art that the present disclosure may be implemented as a modified form without departing from an essential characteristic of the present disclosure. Therefore, the disclosed exemplary embodiments may be considered by way of illustration rather than limitation. The scope of the present disclosure is presented not in the above description but in the claims and it may be interpreted that all differences within an equivalent range thereto may be included in the present disclosure.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A high depth of field image generating apparatus, comprising:<claim-text>a region segmentation unit which segments a region for a stereo image to generate region data;</claim-text><claim-text>a depth estimating unit which estimates depths for the stereo image to generate depth data; and</claim-text><claim-text>a high depth of field image generating unit which generates a high depth of field image from the stereo image, the region data, and the depth data.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The high depth of field image generating apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the high depth of field image generating unit generates the high depth of field image using a trained deep learning model.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The high depth of field image generating apparatus according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the region segmentation unit or the depth estimating unit generates the region data or the depth data using the trained deep learning model, respectively.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The high depth of field image generating apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the region segmentation unit segments the region for each image which configures the stereo image to generate region data and the depth estimating unit generates depth data for each segmented region.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The high depth of field image generating apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the stereo image is obtained by capturing a tissue or a cell.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The high depth of field image generating apparatus according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the trained deep learning model is implemented to simulate blind deconvolution using a point-spread function.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. A high depth of field image generating method, comprising:<claim-text>a region segmentation step of segmenting a region for a stereo image to generate region data;</claim-text><claim-text>a depth estimating step of estimating depths for the stereo image to generate depth data; and</claim-text><claim-text>a high depth of field image generating step of generating a high depth of field image from the stereo image, the region data, and the depth data.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The high depth of field image generating method according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein in the high depth of field image generating step, a trained deep learning model is used.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The high depth of field image generating method according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the region segmentation step and the depth estimating step use a trained deep learning model.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The high depth of field image generating method according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the region segmentation unit segments the region for each image which configures the stereo image to generate region data and the depth estimating unit generates depth data for each segmented region.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The high depth of field image generating method according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the stereo image is obtained by capturing a tissue or a cell.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The high depth of field image generating method according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the trained deep learning model is implemented to simulate blind deconvolution using a point-spread function.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. An apparatus for training a high depth of field image generation model, comprising:<claim-text>a learning model implemented to output a high depth of field image from an input stereo image, wherein the learning model comprises a region segmentation unit which segments a region for the stereo image to generate region data; a depth estimating unit which estimates depths for the stereo image to generate depth data; a high depth of field image generating unit which generates a high depth of field image from the stereo image, the region data, and the depth data; and</claim-text><claim-text>a training unit which trains the learning model with learning data including stereo image and a high depth of field image corresponding thereto.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The apparatus according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the training unit calculates a cost function from a high depth of field image output from the learning model and the high depth of field reference image and trains the learning model using the cost function.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The apparatus according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the high depth of field image generating unit generates the high depth of field image using a deep learning model.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The apparatus according to <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the region segmentation unit or the depth estimating unit generates the region data or the depth data using the deep learning model.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The apparatus according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the region segmentation unit segments the region for each image which configures the stereo image to generate region data and the depth estimating unit generates depth data for each segmented region.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The apparatus according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the stereo image is obtained by capturing a tissue or a cell.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The apparatus according to <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the deep learning model is implemented to simulate blind deconvolution using a point-spread function.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The apparatus according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, further comprising:<claim-text>an image preprocessing unit which preprocesses the stereo image to input the preprocessed image to the learning model.</claim-text></claim-text></claim></claims></us-patent-application>