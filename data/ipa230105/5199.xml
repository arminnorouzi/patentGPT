<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005200A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005200</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17893015</doc-number><date>20220822</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>11</main-group><subgroup>60</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>20</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>109</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>166</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>274</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>44</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>11</main-group><subgroup>60</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20190101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>20</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20200101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>109</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20200101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>166</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20200101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>274</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>44</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2200</main-group><subgroup>24</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>0482</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">SYSTEM AND METHOD FOR GENERATING EMOJI MASHUPS WITH MACHINE LEARNING</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16821891</doc-number><date>20200317</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11423596</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17893015</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>15790799</doc-number><date>20171023</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>10593087</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>16821891</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Paypal, Inc.</orgname><address><city>San Jose</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Woo</last-name><first-name>Titus</first-name><address><city>San Francisco</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Kuo</last-name><first-name>Wei Ting</first-name><address><city>San Jose</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Lu</last-name><first-name>Xueguang</first-name><address><city>San Jose</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Komma</last-name><first-name>Venkatesh</first-name><address><city>Charlotte</city><state>NC</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Aspects of the present disclosure involve systems, methods, devices, and the like for emoji mashup generation. The system and method introduce a method and model that can generate emoji mashups representative of contextual information received by a user at an application. The emoji mashup may come in the form of two or more emojis coherently combined to represent the contextual idea or emotion being conveyed.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="82.47mm" wi="158.75mm" file="US20230005200A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="180.85mm" wi="169.08mm" file="US20230005200A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="184.49mm" wi="186.69mm" file="US20230005200A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="177.80mm" wi="170.26mm" file="US20230005200A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="205.57mm" wi="156.72mm" file="US20230005200A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="203.37mm" wi="171.28mm" file="US20230005200A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="196.26mm" wi="180.17mm" file="US20230005200A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="200.32mm" wi="187.37mm" file="US20230005200A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="193.46mm" wi="181.78mm" orientation="landscape" file="US20230005200A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="213.53mm" wi="187.45mm" orientation="landscape" file="US20230005200A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="205.66mm" wi="183.81mm" file="US20230005200A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application is a continuation of U.S. patent application Ser. No. 16/821,891, filed Mar. 17, 2020, which is a continuation of U.S. patent application Ser. No. 15/790,799, filed Oct. 23, 2017, issued as U.S. Pat. No. 10,593,087, all of which are incorporated by reference herein in their entirety.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">The present disclosure generally relates to user device communication, and more specifically, to user device communication using emojis.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">Nowadays with the evolution and proliferation of devices, users are constantly connected to the internet and social media as a means for communication. Oftentimes, in the communication the users resort to the use of emojis to express an emotion, an idea, place, event, etc. The emojis are often available for selection from the application in use and may be selected by the user. In some instances however, the emoji may appear in response to the word or group of words typed by the user. These emojis are often restricted to the emojis available to the application and/or constraint by the one or more words identified by the application that relate to an emoji. This however, may lead to an incorrect emoji being presented, as the emoji may not fit the occasion. In other words, the emojis presented are constrained to the one or more words matched to the emoji. Thus, the sentiment or occasion as described by a sentence typed is not understood and the user instead resorts to a sticker or gif for the emotion. Therefore, it would be beneficial to create a system that can generate emojis that are tailored for the conversation.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE FIGURES</heading><p id="p-0005" num="0004"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a graphical diagram of a user communicating with a user device.</p><p id="p-0006" num="0005"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates a graphical diagram of user device communication using emoji mashup.</p><p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates a block diagram of a system for the overall process for generating emoji mashups with machine learning.</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates a block diagram exemplifying a training and predicting method for generating emoji mashups with machine learning.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIGS. <b>5</b>A-<b>5</b>B</figref> illustrate graphical diagrams demonstrating exemplary emoji mashups.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIGS. <b>6</b>A-<b>6</b>B</figref> illustrate graphical diagrams of training and retrieval processes for generating emoji mashups.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIGS. <b>7</b>A-<b>7</b>B</figref> illustrate other graphical diagrams of training and retrieval processes for generating emoji mashups.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates a flow diagram illustrating operations for generating emoji mashups.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>9</b></figref> illustrates a block diagram of a system for generating emoji mashups with machine learning.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>10</b></figref> illustrates an example block diagram of a computer system suitable for implementing one or more devices of the communication systems of <figref idref="DRAWINGS">FIGS. <b>1</b>-<b>9</b></figref>.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0015" num="0014">Embodiments of the present disclosure and their advantages are best understood by referring to the detailed description that follows. It should be appreciated that like reference numerals are used to identify like elements illustrated in one or more of the figures, whereas showings therein are for purposes of illustrating embodiments of the present disclosure and not for purposes of limiting the same.</p><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0016" num="0015">In the following description, specific details are set forth describing some embodiments consistent with the present disclosure. It will be apparent, however, to one skilled in the art that some embodiments may be practiced without some or all of these specific details. The specific embodiments disclosed herein are meant to be illustrative but not limiting. One skilled in the art may realize other elements that, although not specifically described here, are within the scope and the spirit of this disclosure. In addition, to avoid unnecessary repetition, one or more features shown and described in association with one embodiment may be incorporated into other embodiments unless specifically described otherwise or if the one or more features would make an embodiment non-functional.</p><p id="p-0017" num="0016">Aspects of the present disclosure involve systems, methods, devices, and the like for emoji mashup generation. In one embodiment, a system is introduced that generates emoji mashups representative of contextual information received by a user at an application. The emoji mashup may come in the form of two or more emojis coherently combined to represent the contextual idea or emotion being conveyed.</p><p id="p-0018" num="0017">Conventionally, device users have depended on predefined emojis for use in expressing emotions, ideas, or places. However, oftentimes, the user may be limited to those emojis available on the application. In some instances, the emoji available is further limited and presented in response to the recognition of one or more words being input/typed by the user. The emojis presented however, may be out of context, as the presented emojis are limited to those on the application and/or the correlation between the one or more words detected.</p><p id="p-0019" num="0018">An example, <figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a graphical diagram <b>100</b> of a user <b>104</b> communicating with a user device <b>102</b>. In particular, <figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a user <b>104</b> interacting with an application <b>106</b> on a user device <b>102</b>. The user device <b>102</b> may include a smart phone, tablet, computer, laptop, wearable device, tablet, virtual reality system, etc. The user device <b>102</b> may be capable of communicating with one or more devices over one or more networks. In some instances, the user device <b>102</b> may include the interaction with an application for communicating over social media, communicating with another individual, transferring funds, processing transactions or the like. As illustrated, the user <b>104</b> may be interacting with the application (e.g., Venmo) over user device <b>102</b>. The application (e.g., Venmo) <b>106</b> may include one that presents emojis in response to detecting a word(s) (e.g., dinner) that is correlated with the words detected. As an example, <figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates user <b>104</b> interacting with Venmo with the indication that a payment will be made to a person (e.g., Robert Kuo) in response to a dinner outing. In the message, the user is thanking Robert for dinner and in response to the detection of the word dinner <b>108</b>, emojis <b>110</b> that have been identified to correlate with the word dinner <b>108</b> appear. In the example, the application recognizes the word dinner <b>108</b> and in response silverware and dinner plate emojis <b>110</b> are presented. A disadvantage however, as illustrated, is that word-to-emoji type matching is limited by the correlations that the system has and corresponding emojis <b>110</b> available for selection.</p><p id="p-0020" num="0019">In one embodiment, a system and method is introduced that enables emoji mashups with machine learning. That is to say, a system and method are introduced that enable the ability to combine two or more emojis to generate at least a single emoji that represents more than a single word or words, but instead a sentence and/or the context involved in a communication.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates a graphical diagram of user device communication using emoji mashup. In particular <figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates a flow diagram <b>200</b> for generating emoji mashups that may then be presented on a user interface of an application <b>106</b> on a user device <b>102</b>.</p><p id="p-0022" num="0021">As indicated above, a large limitation exists in current emojis used, based in part, on the partial library that may be available on the application as well as the strict emoji-word designation based on predefined correlations. Flow diagram <b>200</b> is presented as an exemplary communication that can occur between various systems that can enable the generation of emoji mashups that are more closely related to the contextual information on the message at the time.</p><p id="p-0023" num="0022">For example, as illustrated in flow diagram <b>200</b>, an emoji repository/system <b>202</b> that may be in communication with external networks <b>210</b> including social networks <b>208</b> (e.g., Twitter, Facebook) may be used. These networks can communicate and share emojis available that may be used by the user device <b>102</b> during an interaction with another user (e.g., a Venmo transaction). The flow diagram also illustrates a coordinate system <b>204</b> that may be in communication with at least the user device <b>104</b> and external networks <b>210</b>. The coordinate system <b>204</b> can be a system that uses the emoji details gathered from the user device <b>102</b>, social networks <b>208</b>, and other external networks <b>210</b> to determine how to best locate the two or more emojis that become the emoji mashup <b>212</b> and/or to extract coordinate information from two or more emojis to determine how to best place the emojis with respect to each other in response to the contextual information gathered during the user device interaction with the another user.</p><p id="p-0024" num="0023">The coordination, repository and use of intelligent algorithms work jointly to generate the new emoji mashup. Feedback from user input is also used to learn and identify the best matches as well as identify new emoji mashups <b>212</b>. To illustrate the idea of emoji mashup <b>212</b>, <figref idref="DRAWINGS">FIG. <b>2</b></figref> includes a conversation on a display <b>216</b> on user device <b>102</b>. As an example, on the display <b>216</b>, user <b>104</b> is communicating with another user regarding a concert that was attended. The other user comments on the concert and how well it went while user <b>104</b> responds with, &#x201c;IKR?! Dude, Dan was a friggin rock star . . . &#x201d; In response the user's comments <b>214</b>, emoji mashups <b>214</b> are presented which are suggested in response to the context in the conversation.</p><p id="p-0025" num="0024">Notice that unlike conventional systems where the word &#x201c;rock&#x201d; or &#x201c;star&#x201d; would be recognized and a rock or star would be suggested, here instead more relevant emoji mashups <b>214</b> are suggested. For example, a guitar and a star are combined, a rock&#x26;roll emoji and stars are combined, and a trophy and music notes are combined. Thus, the emoji mashup <b>214</b> presents an emoji that represents the idea and/or emotion in a more comprehensive manner.</p><p id="p-0026" num="0025">Turning to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, a more detailed description of the overall process for generating the emoji mashup <b>214</b> is presented. In particular, <figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates a block diagram of a methodology <b>300</b> for generating emoji mashups. Methodology <b>300</b> presents the model that may be used for retrieving, analyzing, and training information received for suggesting an emoji mashups <b>212</b>. Methodology <b>300</b> can begin with first analyzing the text <b>302</b> received by the application. As indicated above, an aspect of the current embodiment includes retrieving mashup emojis <b>212</b> that closely correlate with the contextual information received. Analyzing the text can include taking the message transmitted and evaluating the contextual information to identify the words transmitted. In addition, the information can also be run through a system and method of evaluating the context which includes word vectorization <b>304</b>. Word vectorization <b>304</b> is a method that uses computational intelligence including neural networks and natural language processing for generating dictionaries from words into real-valued vectors. The process which includes converting a bitmap image into a vector representation such that terms are given a corresponding number such that the closer the words are in relationship to each other the closer the numerical number between the words. As an example, the words can be vectorized to find similar meanings using language models to extract word functions and structure. <figref idref="DRAWINGS">FIG. <b>4</b></figref> and its corresponding description below provides an example of word vectorization. In addition to vectorization, the information may be sent through a sentiment analyzer where clues about the message tone, purpose, and context may be used in the analysis.</p><p id="p-0027" num="0026">After the words are vectorized, the words can be filtered <b>306</b>. The filtered vectors may be converted into matrices which can be used to programmatically generate new emojis <b>308</b>. The new emojis generated can be emojis identified from social networks, system repositories, other networks, etc. Once the new emojis are generated and/or retrieved, the emojis are combined. Combining the emojis can occur by using matchmaking logic <b>310</b>. The matchmaking logic can include coordinates, image recognition systems, as well as machine learning algorithm which can be used to learn and/or determine how to combine the emojis coherently <b>312</b>. For example, coordinates from each set of emojis retrieved or generated <b>308</b> can be analyzed to determine their corresponding center to determine how to best combine. Once one or more emojis are combined to generate emoji mashups <b>212</b>, the emojis can be presented to the user <b>104</b> for selection.</p><p id="p-0028" num="0027">To illustrate methodology <b>300</b>, consider a user <b>104</b> whose input includes &#x201c;Dude, you're on fire!&#x201d; For such input, methodology can use word vectorization and sentiment analysis to determine that the communication includes a positive sentiment and smiley face and flame emoji can be retrieved. Once these two emojis are retrieved, the matchmaking algorithm can be used to determine how to scale and place the emojis relative to each other. In one embodiment, the face emoji may be placed prominently in front of the flame emoji which can sit behind on by the head portion of the face. Additionally, a refresh button may be available which can be used to generate a new emoji mashup. Thus, with the use of the external networks, user feedback, and system userbase (e.g., Paypal/Venmo userbase), machine learning and neural networks may be used to generate new and improved emoji mashups <b>212</b> over time.</p><p id="p-0029" num="0028">To illustrate some of the processes involved in methodology <b>300</b> for generating the emoji mashups <b>212</b>, <figref idref="DRAWINGS">FIGS. <b>4</b>-<b>5</b></figref> are included. Turning to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, word training and prediction is illustrated. In particular, <figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates a block diagram <b>400</b> exemplifying a training and predicting method for generating emoji mashups with machine learning.</p><p id="p-0030" num="0029">As previously indicated, word vectorization <b>304</b> is a technique that may be used in the text analysis for identifying and predicting emojis based on the written text. As an example of how word vectorization may occur, word2vec training model is presented in <figref idref="DRAWINGS">FIG. <b>4</b></figref>. Word2vec is a model that may be used to illustrate a model that may be used to learn to map discrete words into a low-dimensional vector. Generally, word2vec may be an algorithm that may comprise at least two models that are trained to predict relationships. In particular, vector space models such as word2vec enable words to be represented in a continuous vector space such that similar words are maps to nearby data points through the use of machine learning. As an example, consider the word &#x201c;animal&#x201d;, the data point assigned to it will be very similar to that which would be assigned to a word like &#x201c;cat&#x201d; or &#x201c;dog.&#x201d; As another example, the word &#x201c;man&#x201d; would have a closer value to &#x201c;king&#x201d; than the word &#x201c;queen.&#x201d; In some instances, an open source library may be used to obtain these values.</p><p id="p-0031" num="0030">Returning to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, an illustration of the implementation of the word2vec model is illustrated. Note that the word2vec model in the current embodiment is used to train the entire sentence. Unlike conventional systems which simply map to a single word, the current system learns the entire sentence <b>402</b> in order to predict the two or more emojis that best represent the idea or emotion being communicated. As an example, training diagram <b>420</b> illustrates the sentence &#x201c;you are a rock star&#x201d; such that the system presents the guitar and star as a suggestion. This training process continues for commonly used phrases and statements and builds as the communication, suggestion and feedback is received from the user. Again, the training and emojis are suggested based using machine learning algorithms, libraries, databases, and models including but not limited to word2vec, open source libraries and databases, convolutional neural networks, supervised learning, clustering, dimensionality reduction, structured prediction, decision tree learning and the like. As the phrases/sentences are learned, the system may begin making predictions as illustrated in predicting diagram <b>440</b>. Notice that now the system recognizes the trained sentence <b>408</b>, &#x201c;You are a rock star!&#x201d; and produces various emoji suggestions with corresponding data points <b>410</b> with the suggestion of a guitar and a star receiving the highest data points of 0.86 and 0.77 respectively. Once the emojis are identified or new emojis generated <b>308</b>, as indicated above, the method continues to combine the emojis using matchmaking logic.</p><p id="p-0032" num="0031">A large part of the matchmaking logic includes determining how to merge the emojis. In one embodiment, to determine how to merge the emojis, object recognition is used to determine what the image is and an optimal location to merge. For example, if the two emojis identified include a smiley face and fire in response to &#x201c;Dude you are on fire!&#x201d; understanding how to merge the smiley face and the fire is determined using object recognition. To accomplish this, in one embodiment, the system can perform a center of mass like analysis to determine where the center of the object is. In another embodiment, the system can recognize the object (e.g., smiley face and fire images) and extract their coordinates. In one example, the dimensions may be pulled from the object while in other instances, the images may be turn a determined amount such that depth is provided to the image and coordinates can be extracted. The coordinates can then be provided to the matchmaking logic which can in-turn and suggest various ways to merge the two images detected (e.g., smiley face and fire emojis).</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIGS. <b>5</b>A-<b>5</b>B</figref> illustrate graphical diagrams demonstrating exemplary emoji mashups. In particular, <figref idref="DRAWINGS">FIG. <b>5</b>A</figref> illustrates three sample smiley face emojis that are merged with the fire emoji. Referring to middle smiley fire emoji <b>502</b>, note that coordinates for the smiley face and fire emoji are presented and referenced for the placement of the two to generate the emoji mashup <b>212</b>. <figref idref="DRAWINGS">FIG. <b>5</b>B</figref> illustrates a guitar emoji <b>504</b> and a star emoji <b>506</b> and the generation of the rockstar guitar emoji <b>508</b>. Notice that the guitar emoji <b>504</b> and star emoji <b>506</b> were presented in response to high data points that may have been determined from the initial training by the machine learning system. Also notice that in this instance, the emojis resulted in a newly generated emoji (e.g., rockstar guitar emoji <b>508</b>). Note that in some instances, the final emoji maybe a product of the combination of the two or more emojis suggested by the learning system while in other instances, the emojis may be a newly generated emoji defined by the two or more emojis initially identified. The newly generated emoji may be a merger of the two or a previously defined emoji located internally or on an external network. Also note that the emojis may have been obtained from an external library, repository, database or the like.</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIGS. <b>6</b>A-<b>7</b>B</figref> illustrate graphical diagrams of training and retrieval processes for generating emoji mashups. <figref idref="DRAWINGS">FIGS. <b>6</b>A-<b>6</b>B</figref> illustrate early processes where the system may include minimal or no user feedback data that can help train and tailor the emojis that are suggested. <figref idref="DRAWINGS">FIGS. <b>7</b>A-<b>7</b>B</figref> illustrate a more mature process where the system is not only using external information (e.g., Twitter&#xae; feeds, Google libraries, etc.), but also using internal user feedback to train and present the user <b>104</b> with emoji mashup suggestions.</p><p id="p-0035" num="0034">Turning to <figref idref="DRAWINGS">FIG. <b>6</b>A</figref>, system training is presented, in particular text training process <b>600</b> begins with training text sequences (e.g., a text message on a messaging application) for emoji representation. To begin the emoji training process, external sources are used for the retrieval of information. For example, social media feeds <b>602</b> (e.g., Twitter feeds) may be collected for the use in training the system. The social media feeds may then go through some data processing <b>604</b>, where the feeds and other information collected is in preparation for the word2vec conversion <b>606</b>. As indicated above, word2vec <b>606</b> is a training model that may be used to convert words into vectors where part of word2vec <b>606</b> entails turning the text into vectors and extracting the text and the emojis. Thus, input <b>608</b> word vectors produces the emojis which are used as the labels. This process is performed for the sequences such that a text2emoji sequence is identified in the translation model <b>610</b>.</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>6</b>A</figref> then continues the training process with emoji training process <b>620</b> where the emoji images are trained using object detection. In emoji training process <b>620</b>, the emoji images identified are then processed <b>614</b> and resized using the object detection model <b>618</b>. Note that emoji images are used for exemplary purposes and are not so restricted, as other forms and images may be contemplated.</p><p id="p-0037" num="0036">At <figref idref="DRAWINGS">FIG. <b>6</b>B</figref>, the runtime process <b>650</b> is presented. As illustrated in runtime process <b>650</b>, begins with the receipt of user input <b>622</b>. User input can come in the form of a phrase, a sentence, an abbreviation, text, or the like. Oftentimes, the user input will be received at the display of a user device <b>102</b> while on an application <b>216</b> in communication with one or more other users.</p><p id="p-0038" num="0037">In runtime process <b>650</b>, like text training process <b>600</b>, the text is processes and converted using word2vec <b>626</b>. Next, the information is run against the pre-trained Text2Emoji model <b>630</b> which can then output the emoji sequences <b>632</b> that are correlated to the input text from the user <b>104</b>. In some embodiments, the emoji sequences identified can then be presented to the user <b>104</b> on the user device UI <b>634</b> for user observation and selection <b>646</b>. Additionally or alternatively, the emoji sequences <b>632</b> obtained can are received <b>636</b>, processed <b>638</b>, and run against the pre-trained object detection model <b>640</b>. After the pre-trained object detection model <b>640</b>, emoji coordinates may be evaluated <b>642</b> and extracted and sent to the user UI for user selection. Note that in some instances, the two or more emoji sequences may be presented to the user <b>104</b>, while in other instances, multiple mashup emojis may already be generated based on the coordinates <b>642</b> extracted and presented to the user for user selection <b>646</b>. After the user <b>104</b> has made a selection as to a preferred emoji arrangement, emoji pair, emoji overlay and/or emoji mashup user feedback is stored <b>648</b>.</p><p id="p-0039" num="0038">As more user preferences and selections are acquired by the system, then user feedback may be used to generate the emoji mashups. For example, in training and runtime processes of <figref idref="DRAWINGS">FIGS. <b>7</b>A-<b>7</b>B</figref>, a system which includes user feedback is presented. Notice that text training process <b>700</b> and emoji training process <b>720</b> are very similar to training process <b>600</b> and <b>620</b> corresponding to <figref idref="DRAWINGS">FIGS. <b>6</b>A-<b>6</b>B</figref>. As such, the processes descriptions are similar with the exception of user feedback <b>712</b>, <b>714</b>, which are now used to train both the text model and the positioning model. Similarly, runtime process <b>750</b> of <figref idref="DRAWINGS">FIG. <b>7</b>B</figref> imitates runtime process <b>650</b> of <figref idref="DRAWINGS">FIG. <b>6</b>B</figref>. Distinct from runtime process <b>650</b> however, is the use of positioning model <b>736</b> of <figref idref="DRAWINGS">FIG. <b>7</b>B</figref>. The positioning model was training during emoji training process <b>720</b> with the use of user feedback, therefore, at runtime process <b>750</b>, the pre-trained object detection model <b>640</b> is no longer used and instead replaced by the positioning model <b>736</b>. Once coordinate information <b>738</b> is extracted from the emoji sequences again the emojis and coordinates can be presented to the user for user selection <b>742</b> and feedback is stored <b>744</b>.</p><p id="p-0040" num="0039">Note that the process presented is for exemplary purposes and other processes, modeling, and training may be contemplated. In addition, a previously indicated, the system is not restricted to the use of the word2vec model as other machine learning models may be used. Additionally, the system is not restricted to the use of emojis and other forms and types of images may be contemplated.</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates an example process for generating emoji mashups that may be implemented in on a system, such as system <b>900</b> and device <b>102</b> in <figref idref="DRAWINGS">FIGS. <b>9</b> and <b>1</b></figref> respectively. In particular, <figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates a flow diagram illustrating operations for receiving a user input and in response presenting the user with emoji mashup suggestions to represent the emotion or idea conveyed in the user input. According to some embodiments, process <b>800</b> may include one or more of operations <b>802</b>-<b>818</b>, which may be implemented, at least in part, in the form of executable code stored on a non-transitory, tangible, machine readable media that, when run on one or more hardware processors, may cause a system to perform one or more of the operations <b>802</b>-<b>818</b>.</p><p id="p-0042" num="0041">Process <b>800</b> may begin with operation <b>802</b>, where user input is received. The user input may be in the form of a sequence, statement, sentence, phrase or other text. The user input may be input into an application used for transacting, communicating, or interacting with another user. For example, the user input may be at an application like Venmo where a user may send or request a payment to another user with a brief statement and/or emoji regarding the transaction involved.</p><p id="p-0043" num="0042">The user input received is then analyzed for identifying a relevant emoji(s) to present. To analyze the text, at operation <b>804</b>, the input text is vectorized. Word vectorization may occur using a model such as but not limited to word2vec, where word2vec may be an algorithm that may comprise at least two models that are trained to predict relationships. Thus, in this instance, the user sequence is vectorized corresponding emoji(s) are allocated based on the word. As the entire sequence (user input) is vectorized, the model is trained to extract a series of emoji sequences, in operation <b>806</b>, that correspond to the context of the sequence input. In some instances, social media networks may be used to identify the sequences and/or emojis for use, while in other instances user selection feedback may exist such that the emojis extracted are retrieved from an existing database or other storage unit. Note that in addition to vectorization, the information may be sent through a sentiment analyzer where clues about the message tone, purpose, and context may be used in the analysis and in identifying the emoji sequences.</p><p id="p-0044" num="0043">If the system model used for extracting the emoji model is mature and sufficient user feedback exists, then a positioning model exists with user preferences and emoji details. As such, at operation <b>808</b>, a determination is made as to whether a positioning model is available. If the system is still underdeveloped or if a new sequence is identified, the emoji processing may continue further using an object detection model at operation <b>810</b>. Object detection model may be a model used to detect the emoji sequences extracted from the text input such that coordinate information may be extract from the emojis at operation <b>814</b>. Alternatively, at operation <b>812</b> if the input received is recognized and/or sufficient user feedback exists such that emoji training is not needed, then emoji sequences may be processed through a emoji positing model at operation <b>812</b> so that coordinate information may be extracted at operation <b>814</b>. Once the coordinate information is known at operation <b>814</b>, then the two or more emojis identified (emoji sequences) may be coherently combined to generate an emoji mashup representative of the input received. The emoji mashup(s) coherently combined may occur at operation <b>816</b> where the output mashup emoji(s) may be presented to the user for selection. Note, however, that in some instances, the emoji sequence at operation <b>806</b> may additionally or alternatively be presented to the user for the opportunity to combine the emojis by the user.</p><p id="p-0045" num="0044">Note that more or fewer operations may exist in performing method <b>800</b>. In addition, an operation may exist for determining new emoji or other media object mashup. In addition, the operations are not limited to the training models identified. Further, user selection may be stored for later use by the user and/or another user.</p><p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. <b>9</b></figref> illustrates, in block diagram format, an example embodiment of a computing environment adapted for implementing a system for generating emoji mashups. As shown, a computing environment <b>900</b> may comprise or implement a plurality of servers and/or software components that operate to perform various methodologies in accordance with the described embodiments. Severs may include, for example, stand-alone and enterprise-class servers operating a server operating system (OS) such as a MICROSOFT&#xae; OS, a UNIX&#xae; OS, a LINUX&#xae; OS, or other suitable server-based OS. It may be appreciated that the servers illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref> may be deployed in other ways and that the operations performed and/or the services provided by such servers may be combined, distributed, and/or separated for a given implementation and may be performed by a greater number or fewer number of servers. One or more servers may be operated and/or maintained by the same or different entities.</p><p id="p-0047" num="0046">Computing environment <b>900</b> may include, among various devices, servers, databases and other elements, one or more clients <b>902</b> that may comprise or employ one or more client devices <b>904</b>, such as a laptop, a mobile computing device, a tablet, a PC, a wearable device, and/or any other computing device having computing and/or communications capabilities in accordance with the described embodiments. Client devices <b>904</b> may include a cellular telephone, smart phone, electronic wearable device (e.g., smart watch, virtual reality headset), or other similar mobile devices that a user may carry on or about his or her person and access readily.</p><p id="p-0048" num="0047">Client devices <b>904</b> generally may provide one or more client programs <b>906</b>, such as system programs and application programs to perform various computing and/or communications operations. Some example system programs may include, without limitation, an operating system (e.g., MICROSOFT&#xae; OS, UNIX&#xae; OS, LINUX&#xae; OS, Symbian OS&#x2122;, Embedix OS, Binary Run-time Environment for Wireless (BREW) OS, JavaOS, a Wireless Application Protocol (WAP) OS, and others), device drivers, programming tools, utility programs, software libraries, application programming interfaces (APIs), and so forth. Some example application programs may include, without limitation, a web browser application, messaging applications (e.g., e-mail, IM, SMS, MIMS, telephone, voicemail, VoIP, video messaging, internet relay chat (IRC)), contacts application, calendar application, electronic document application, database application, media application (e.g., music, video, television), location-based services (LBS) applications (e.g., GPS, mapping, directions, positioning systems, geolocation, point-of-interest, locator) that may utilize hardware components such as an antenna, and so forth. One or more of client programs <b>906</b> may display various graphical user interfaces (GUIs) to present information to and/or receive information from one or more users of client devices <b>904</b>. In some embodiments, client programs <b>906</b> may include one or more applications configured to conduct some or all of the functionalities and/or processes discussed above and in conjunction <figref idref="DRAWINGS">FIGS. <b>1</b>-<b>8</b></figref>.</p><p id="p-0049" num="0048">As shown, client devices <b>904</b> may be communicatively coupled via one or more networks <b>908</b> to a network-based system <b>910</b>. Network-based system <b>910</b> may be structured, arranged, and/or configured to allow client <b>902</b> to establish one or more communications sessions between network-based system <b>910</b> and various computing devices <b>904</b> and/or client programs <b>906</b>. Accordingly, a communications session between client devices <b>904</b> and network-based system <b>910</b> may involve the unidirectional and/or bidirectional exchange of information and may occur over one or more types of networks <b>908</b> depending on the mode of communication. While the embodiment of <figref idref="DRAWINGS">FIG. <b>9</b></figref> illustrates a computing environment <b>900</b> deployed in a client-server operating relationship, it is to be understood that other suitable operating environments, relationships, and/or architectures may be used in accordance with the described embodiments.</p><p id="p-0050" num="0049">Data communications between client devices <b>904</b> and the network-based system <b>910</b> may be sent and received over one or more networks <b>508</b> such as the Internet, a WAN, a WWAN, a WLAN, a mobile telephone network, a landline telephone network, personal area network, as well as other suitable networks. For example, client devices <b>904</b> may communicate with network-based system <b>910</b> over the Internet or other suitable WAN by sending and or receiving information via interaction with a web site, e-mail, IM session, and/or video messaging session. Any of a wide variety of suitable communication types between client devices <b>904</b> and system <b>910</b> may take place, as will be readily appreciated. In particular, wireless communications of any suitable form may take place between client device <b>904</b> and system <b>910</b>, such as that which often occurs in the case of mobile phones or other personal and/or mobile devices.</p><p id="p-0051" num="0050">In various embodiments, computing environment <b>900</b> may include, among other elements, a third party <b>912</b>, which may comprise or employ third-party devices <b>914</b> hosting third-party applications <b>516</b>. In various implementations, third-party devices <b>514</b> and/or third-party applications <b>916</b> may host applications associated with or employed by a third party <b>912</b>. For example, third-party devices <b>914</b> and/or third-party applications <b>916</b> may enable network-based system <b>910</b> to provide client <b>902</b> and/or system <b>910</b> with additional services and/or information, such as merchant information, data communications, payment services, security functions, customer support, and/or other services, some of which will be discussed in greater detail below. Third-party devices <b>914</b> and/or third-party applications <b>916</b> may also provide system <b>910</b> and/or client <b>902</b> with other information and/or services, such as email services and/or information, property transfer and/or handling, purchase services and/or information, and/or other online services and/or information.</p><p id="p-0052" num="0051">In one embodiment, third-party devices <b>914</b> may include one or more servers, such as a transaction server that manages and archives transactions. In some embodiments, the third-party devices may include a purchase database that can provide information regarding purchases of different items and/or products. In yet another embodiment, third-party severs <b>914</b> may include one or more servers for aggregating consumer data, purchase data, and other statistics.</p><p id="p-0053" num="0052">Network-based system <b>910</b> may comprise one or more communications servers <b>920</b> to provide suitable interfaces that enable communication using various modes of communication and/or via one or more networks <b>908</b>. Communications servers <b>920</b> may include a web server <b>922</b>, an API server <b>924</b>, and/or a messaging server <b>926</b> to provide interfaces to one or more application servers <b>930</b>. Application servers <b>930</b> of network-based system <b>910</b> may be structured, arranged, and/or configured to provide various online services, merchant identification services, merchant information services, purchasing services, monetary transfers, checkout processing, data gathering, data analysis, and other services to users that access network-based system <b>910</b>. In various embodiments, client devices <b>904</b> and/or third-party devices <b>914</b> may communicate with application servers <b>930</b> of network-based system <b>910</b> via one or more of a web interface provided by web server <b>922</b>, a programmatic interface provided by API server <b>924</b>, and/or a messaging interface provided by messaging server <b>926</b>. It may be appreciated that web server <b>922</b>, API server <b>924</b>, and messaging server <b>526</b> may be structured, arranged, and/or configured to communicate with various types of client devices <b>904</b>, third-party devices <b>914</b>, third-party applications <b>916</b>, and/or client programs <b>906</b> and may interoperate with each other in some implementations.</p><p id="p-0054" num="0053">Web server <b>922</b> may be arranged to communicate with web clients and/or applications such as a web browser, web browser toolbar, desktop widget, mobile widget, web-based application, web-based interpreter, virtual machine, mobile applications, and so forth. API server <b>924</b> may be arranged to communicate with various client programs <b>906</b> and/or a third-party application <b>916</b> comprising an implementation of API for network-based system <b>910</b>.</p><p id="p-0055" num="0054">Messaging server <b>926</b> may be arranged to communicate with various messaging clients and/or applications such as e-mail, IM, SMS, MMS, telephone, VoIP, video messaging, IRC, and so forth, and messaging server <b>926</b> may provide a messaging interface to enable access by client <b>902</b> and/or third party <b>912</b> to the various services and functions provided by application servers <b>930</b>.</p><p id="p-0056" num="0055">Application servers <b>930</b> of network-based system <b>910</b> may be a server that provides various services to clients including, but not limited to, data analysis, geofence management, order processing, checkout processing, and/or the like. Application server <b>930</b> of network-based system <b>910</b> may provide services to a third party merchants such as real time consumer metric visualizations, real time purchase information, and/or the like. Application servers <b>930</b> may include an account server <b>932</b>, device identification server <b>934</b>, payment server <b>936</b>, content selection server <b>938</b>, profile merging server <b>940</b>, user ID server <b>942</b>, feedback server <b>954</b>, and/or content statistics server <b>946</b>. Note that any one or more of the serves <b>932</b>-<b>946</b> may be used in storing and/or retrieving emojis, user feedback, coordinates, emoji positioning, etc. For example, user selections may be stored in feedback server <b>944</b>. These servers, which may be in addition to other servers, may be structured and arranged to configure the system for monitoring queues and identifying ways for reducing queue times.</p><p id="p-0057" num="0056">Application servers <b>930</b>, in turn, may be coupled to and capable of accessing one or more databases <b>950</b> including a profile database <b>952</b>, an account database <b>954</b>, geofence database <b>956</b>, and/or the like. Databases <b>950</b> generally may store and maintain various types of information for use by application servers <b>930</b> and may comprise or be implemented by various types of computer storage devices (e.g., servers, memory) and/or database structures (e.g., relational, object-oriented, hierarchical, dimensional, network) in accordance with the described embodiments.</p><p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. <b>10</b></figref> illustrates an example computer system <b>1000</b> in block diagram format suitable for implementing on one or more devices of the system in <figref idref="DRAWINGS">FIGS. <b>1</b>-<b>9</b></figref>. In various implementations, a device that includes computer system <b>1000</b> may comprise a personal computing device (e.g., a smart or mobile device, a computing tablet, a personal computer, laptop, wearable device, PDA, user device <b>102</b>, etc.) that is capable of communicating with a network <b>926</b> (e.g., networks <b>208</b>,<b>210</b>). A service provider and/or a content provider may utilize a network computing device (e.g., a network server) capable of communicating with the network. It should be appreciated that each of the devices utilized by users, service providers, and content providers may be implemented as computer system <b>1000</b> in a manner as follows.</p><p id="p-0059" num="0058">Additionally, as more and more devices become communication capable, such as new smart devices using wireless communication to report, track, message, relay information and so forth, these devices may be part of computer system <b>1000</b>. For example, windows, walls, and other objects may double as touch screen devices for users to interact with. Such devices may be incorporated with the systems discussed herein.</p><p id="p-0060" num="0059">Computer system <b>1000</b> may include a bus <b>1010</b> or other communication mechanisms for communicating information data, signals, and information between various components of computer system <b>1000</b>. Components include an input/output (I/O) component <b>1004</b> that processes a user action, such as selecting keys from a keypad/keyboard, selecting one or more buttons, links, actuatable elements, etc., and sending a corresponding signal to bus <b>1010</b>. I/O component <b>1004</b> may also include an output component, such as a display <b>1002</b> and a cursor control <b>1008</b> (such as a keyboard, keypad, mouse, touchscreen, etc.). In some examples, I/O component <b>1004</b> other devices, such as another user device, a merchant server, an email server, application service provider, web server, a payment provider server, and/or other servers via a network. In various embodiments, such as for many cellular telephone and other mobile device embodiments, this transmission may be wireless, although other transmission mediums and methods may also be suitable. A processor <b>1018</b>, which may be a micro-controller, digital signal processor (DSP), or other processing component, that processes these various signals, such as for display on computer system <b>1000</b> or transmission to other devices over a network <b>1026</b> via a communication link <b>1024</b>. Again, communication link <b>1024</b> may be a wireless communication in some embodiments. Processor <b>1018</b> may also control transmission of information, such as cookies, IP addresses, images, and/or the like to other devices.</p><p id="p-0061" num="0060">Components of computer system <b>1000</b> also include a system memory component <b>1012</b> (e.g., RAM), a static storage component <b>1014</b> (e.g., ROM), and/or a disk drive <b>1016</b>. Computer system <b>1000</b> performs specific operations by processor <b>1018</b> and other components by executing one or more sequences of instructions contained in system memory component <b>1012</b> (e.g., text processing and emoji processing). Logic may be encoded in a computer readable medium, which may refer to any medium that participates in providing instructions to processor <b>1018</b> for execution. Such a medium may take many forms, including but not limited to, non-volatile media, volatile media, and/or transmission media. In various implementations, non-volatile media includes optical or magnetic disks, volatile media includes dynamic memory such as system memory component <b>1012</b>, and transmission media includes coaxial cables, copper wire, and fiber optics, including wires that comprise bus <b>1010</b>. In one embodiment, the logic is encoded in a non-transitory machine-readable medium. In one example, transmission media may take the form of acoustic or light waves, such as those generated during radio wave, optical, and infrared data communications.</p><p id="p-0062" num="0061">Some common forms of computer readable media include, for example, hard disk, magnetic tape, any other magnetic medium, CD-ROM, any other optical medium, RAM, PROM, EPROM, FLASH-EPROM, any other memory chip or cartridge, or any other medium from which a computer is adapted to read.</p><p id="p-0063" num="0062">Components of computer system <b>1000</b> may also include a short range communications interface <b>1020</b>. Short range communications interface <b>1020</b>, in various embodiments, may include transceiver circuitry, an antenna, and/or waveguide. Short range communications interface <b>1020</b> may use one or more short-range wireless communication technologies, protocols, and/or standards (e.g., WiFi, Bluetooth&#xae;, Bluetooth Low Energy (BLE), infrared, NFC, etc.).</p><p id="p-0064" num="0063">Short range communications interface <b>1020</b>, in various embodiments, may be configured to detect other devices (e.g., device <b>102</b>, secondary user device, etc.) with short range communications technology near computer system <b>1000</b>. Short range communications interface <b>1020</b> may create a communication area for detecting other devices with short range communication capabilities. When other devices with short range communications capabilities are placed in the communication area of short range communications interface <b>1020</b>, short range communications interface <b>1020</b> may detect the other devices and exchange data with the other devices. Short range communications interface <b>1020</b> may receive identifier data packets from the other devices when in sufficiently close proximity. The identifier data packets may include one or more identifiers, which may be operating system registry entries, cookies associated with an application, identifiers associated with hardware of the other device, and/or various other appropriate identifiers.</p><p id="p-0065" num="0064">In some embodiments, short range communications interface <b>1020</b> may identify a local area network using a short range communications protocol, such as WiFi, and join the local area network. In some examples, computer system <b>1000</b> may discover and/or communicate with other devices that are a part of the local area network using short range communications interface <b>1020</b>. In some embodiments, short range communications interface <b>1020</b> may further exchange data and information with the other devices that are communicatively coupled with short range communications interface <b>1020</b>.</p><p id="p-0066" num="0065">In various embodiments of the present disclosure, execution of instruction sequences to practice the present disclosure may be performed by computer system <b>1000</b>. In various other embodiments of the present disclosure, a plurality of computer systems <b>1000</b> coupled by communication link <b>1024</b> to the network (e.g., such as a LAN, WLAN, PTSN, and/or various other wired or wireless networks, including telecommunications, mobile, and cellular phone networks) may perform instruction sequences to practice the present disclosure in coordination with one another. Modules described herein may be embodied in one or more computer readable media or be in communication with one or more processors to execute or process the techniques and algorithms described herein.</p><p id="p-0067" num="0066">A computer system may transmit and receive messages, data, information and instructions, including one or more programs (i.e., application code) through a communication link <b>1024</b> and a communication interface. Received program code may be executed by a processor as received and/or stored in a disk drive component or some other non-volatile storage component for execution.</p><p id="p-0068" num="0067">Where applicable, various embodiments provided by the present disclosure may be implemented using hardware, software, or combinations of hardware and software. Also, where applicable, the various hardware components and/or software components set forth herein may be combined into composite components comprising software, hardware, and/or both without departing from the spirit of the present disclosure. Where applicable, the various hardware components and/or software components set forth herein may be separated into sub-components comprising software, hardware, or both without departing from the scope of the present disclosure. In addition, where applicable, it is contemplated that software components may be implemented as hardware components and vice-versa.</p><p id="p-0069" num="0068">Software, in accordance with the present disclosure, such as program code and/or data, may be stored on one or more computer readable media. It is also contemplated that software identified herein may be implemented using one or more computers and/or computer systems, networked and/or otherwise. Where applicable, the ordering of various steps described herein may be changed, combined into composite steps, and/or separated into sub-steps to provide features described herein.</p><p id="p-0070" num="0069">The foregoing disclosure is not intended to limit the present disclosure to the precise forms or particular fields of use disclosed. As such, it is contemplated that various alternate embodiments and/or modifications to the present disclosure, whether explicitly described or implied herein, are possible in light of the disclosure. For example, the above embodiments have focused on the user and user device, however, a customer, a merchant, a service or payment provider may otherwise presented with tailored information. Thus, &#x201c;user&#x201d; as used herein can also include charities, individuals, and any other entity or person receiving information. Having thus described embodiments of the present disclosure, persons of ordinary skill in the art will recognize that changes may be made in form and detail without departing from the scope of the present disclosure. Thus, the present disclosure is limited only by the claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. (canceled)</claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. A system comprising:<claim-text>a non-transitory memory storing instructions; and</claim-text><claim-text>one or more hardware processors coupled to the non-transitory memory and configured to read the instructions from the non-transitory memory to cause the system to perform operations comprising:<claim-text>determining two emojis in a digital message transmitted by a computing device of a user, wherein the two emojis are received via a user interface of a mobile application on the computing device;</claim-text><claim-text>identifying emoji image information for each of the two emojis based on emoji images of the two emojis;</claim-text><claim-text>determining emoji context information for the two emojis used in the digital image based on at least one of a use of the two emojis in the digital message or text data in the digital message;</claim-text><claim-text>determining, using a machine learning model, a combination of the two emojis based on the emoji image information and the emoji context information;</claim-text><claim-text>merging, based the combination, the two emojis into an emoji mashup, wherein the emoji mashup comprises a single emoji image having the combination of the two emojis determined using the machine learning model; and</claim-text><claim-text>presenting, via the user interface in the mobile application, the emoji mashup.</claim-text></claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The system of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the emoji context information comprises a payment context associated with at least the use of the two emojis in the digital message of a payment processed for the user.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The system of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the payment comprises a user-to-user payment to another user processed via the mobile application, and wherein the digital image is transmittable to the other user in conjunction with the user-to-user payment.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The system of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the payment comprises a merchant payment to a merchant or at a merchant location, and wherein the two emojis or the emoji mashup are specific to the merchant or the merchant location for the merchant payment having an item provided to the user.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The system of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the determining emoji context information comprises identifying a user selection parameter associated with the user selecting the two emojis for the digital message, and wherein the user selection parameter is associated with at least one of a location of the computing device, a time the digital message was entered to the mobile application or sent by the mobile application, or another user receiving the digital message.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The system of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the merging the two emojis comprises:<claim-text>determining at least one of a relative position of the two emojis within the digital message based on the emoji image information or a scale for the two emojis within the digital message; and</claim-text><claim-text>adjusting a relative size based on the scale or the relative position of one or more of the two emojis,</claim-text><claim-text>wherein the merging is based on the adjusted relative size or the adjusted relative position.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The system of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the merging is further based on matchmaking logic that merges the two emojis based on a selection of the two emojis from a plurality of emojis in the digital message using the machine learning model.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. A method comprising:<claim-text>receiving, by one or more hardware processors from a first user, a string of text comprising two emojis, wherein the at least two emojis replace a portion of text in the string of text;</claim-text><claim-text>obtaining, by the one or more hardware processors, emoji presentation data for at least two emojis;</claim-text><claim-text>determining, by the one or more hardware processors, a payment context associated with a use of the at least two emojis in the string of text based at least on the string of text and the at least two emojis;</claim-text><claim-text>determining, by the one or more hardware processors using a machine learning model, an emoji combination of the at least two emojis based on the emoji presentation information and the payment context, wherein the emoji combination comprises a single emoji image combining the at least two emojis determined using the machine learning model;</claim-text><claim-text>generating, by the one or more hardware processors, the emoji combination using the at least two emojis; and</claim-text><claim-text>transmitting, by the one or more hardware processors, the emoji combination to the user.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the payment context is associated with a user payment transaction to a second user that is generated or processed for the first user, and wherein the user payment transaction comprises a message field having the string of text with the at least two emojis.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the payment context is associated with a merchant payment transaction to a merchant that is generated by or processed for the first user for an item, and wherein the merchant payment transaction comprises a message field having the string of text with the at least two emojis.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the payment context comprises at least one of user data of the user or message data for a message having the string of text with the at least two emojis, wherein the user data comprises device data associated with a device of the user, a geo-location of the device, a transaction history of the user, or past messages having a plurality of emojis sent by the user, and wherein the message data comprises a recipient of the message, a time of the message, or additional content in the message.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the emoji presentation data comprises graphical data for a graphical layout of each of the at least two emojis when entered to an application with the string of text.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the generating the emoji combination comprises merging the at least two emojis based on the graphical data.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, further comprising:<claim-text>selecting the at least two emojis from a plurality of emojis in the string of text based on matchmaking logic that identifies different emoji combinations from the plurality of emojis.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. A non-transitory machine-readable medium having stored thereon machine-readable instructions executable to cause a machine to perform operations comprising:<claim-text>detecting a message entered via a mobile application on a mobile device of a first user, wherein the message comprises two emojis replacing text in the message each with a graphical icon;</claim-text><claim-text>determining a commercial payment context associated with a use of the two emojis in the message when transmitting the message via the mobile application, wherein the commercial payment context is associated with utilizing the graphical icon for each of the two emojis in the text to replace a term associated with a transaction corresponding to the message;</claim-text><claim-text>determining, using a machine learning model based on the two emojis and the commercial payment context, a single emoji that comprises a combination of the two emojis and replaces the two emojis in the message with the single emoji;</claim-text><claim-text>merging the two emojis into the single emoji; and</claim-text><claim-text>providing, in the mobile application with the message, an option to replace the two emojis with the single emoji.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The non-transitory machine-readable medium of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the commercial payment context is associated with a usage of one or more merchant-specific or item-specific emojis for the two emojis that replace the text in the message.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The non-transitory machine-readable medium of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the commercial payment context is associated with the transaction with a merchant or at a merchant location, and wherein the single emoji is specific to the merchant or the merchant location for the merchant payment.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The non-transitory machine-readable medium of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the determining the commercial payment context comprises:<claim-text>analyzing the text and the two emojis using a matchmaking logic for combining different emojis;</claim-text><claim-text>converting the two emojis to corresponding text for the two emojis; and</claim-text><claim-text>performing a text analysis of the text and the corresponding text for the two emojis.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The non-transitory machine-readable medium of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein each of the two emoji is specific to at least one of a merchant, a merchant location, or an item, and wherein the converting the two emojis utilizes the at least one of the merchant, the merchant location, or the item specific to each of the two emojis when converted to the corresponding text.</claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. The non-transitory machine-readable medium of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the merging is further based on matchmaking logic that merges the two emojis from a plurality of emojis in the digital message using the machine learning model.</claim-text></claim></claims></us-patent-application>