<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004345A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004345</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17606691</doc-number><date>20210318</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>IT</country><doc-number>102020000005716</doc-number><date>20200318</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>16</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>211</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>30</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>268</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>0482</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>451</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>09</class><subclass>B</subclass><main-group>21</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>167</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20200101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>211</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20200101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>30</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20200101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>268</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>0482</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20180201</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>451</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>09</class><subclass>B</subclass><main-group>21</main-group><subgroup>006</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">METHOD OF BROWSING A RESOURCE THROUGH VOICE INTERACTION</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>MEDIAVOICE S.R.L.</orgname><address><city>Rome</city><country>IT</country></address></addressbook><residence><country>IT</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>GIACOMELLI</last-name><first-name>Fabrizio</first-name><address><city>Rome</city><country>IT</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>GIANTOMASO</last-name><first-name>Pasquale</first-name><address><city>Rome</city><country>IT</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>GOYTHOM HABTE</last-name><first-name>Alexander</first-name><address><city>Rome</city><country>IT</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>MANES</last-name><first-name>Nicolamaria</first-name><address><city>Rome</city><country>IT</country></address></addressbook></inventor></inventors></us-parties><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/IB2021/052261</doc-number><date>20210318</date></document-id><us-371c12-date><date>20211026</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Computer-implemented method of browsing a resource through voice interaction comprising the following steps: A. acquiring (<b>100</b>) from a user a request aimed at browsing a resource; B. downloading (<b>130</b>) the requested resource; C. performing a syntactic parsing (<b>135</b>) of the downloaded resource; D. extracting (<b>150</b>) from the downloaded resource one or more lists, if any, of selectable shortcuts pointing to portions inside or outside the downloaded resource through a syntactic analysis and/or a semantic analysis and/or a morphological-visual analysis of extraction of lists of selectable shortcuts on the basis of an ontology (<b>245</b>) corresponding to the type of resource; E. on the basis of the ontology (<b>245</b>) corresponding to the type of resource, building (<b>225</b>) a list of one or more lists of selectable shortcuts extracted in step D ordered according to a list prioritisation; F. extracting (<b>150</b>) from the downloaded resource one or more content elements through a syntactic analysis and/or a semantic analysis and/or a morphological-visual analysis of extraction of content elements on the basis of the ontology (<b>245</b>) corresponding to the type of resource; G. on the basis of the ontology (<b>245</b>) corresponding to the type of resource, building (<b>290</b>) a list of content elements extracted in step F ordered according to a content element prioritisation; H. on the basis of the lists built in steps E and G and on the basis of the ontology (<b>245</b>) corresponding to the type of resource, building a final structure of lists of selectable shortcuts and of content elements; I. playing (<b>125</b>) a voice prompt based on the final structure and starting a voice interaction with the user for browsing the resource.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="221.91mm" wi="158.75mm" file="US20230004345A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="244.69mm" wi="175.60mm" file="US20230004345A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="210.31mm" wi="172.64mm" file="US20230004345A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="206.59mm" wi="139.45mm" file="US20230004345A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><p id="p-0002" num="0001">The present invention relates to a method, that is computer-implemented, of browsing a resource, such as for instance a website or a web page, a document or an app, through voice interaction, that allows a user, in particular, also if not necessarily, a blind or visually impaired user, to enjoy in a simple, fast, reliable, efficient and inexpensive way the maximum freedom of choice between the various browsing options without the need for any visual perception by the user, while maintaining a high level of conceptualization of the voice communication between the reached (and reachable) resource and the user himself/herself.</p><p id="p-0003" num="0002">The present invention further relates to a system and a device configured to perform such method of browsing a resource through voice interaction.</p><p id="p-0004" num="0003">In the following of the present description, reference will mainly be made to an application of the method and the apparatus according to the invention for blind or visually impaired users. However, it should be noted that the method and the apparatus according to the invention can also be used by non-disabled users who nevertheless are in situations in which they cannot visually perceive the screen of a computer or other processing device (e.g. a smartphone or tablet), for example while driving a vehicle (in this case, the screen of the vehicle's on-board computer or other processing device, such as a smartphone or tablet, could also be automatically darkened to prevent the user from distracting attention from the road).</p><p id="p-0005" num="0004">Also, in the following of the present description, reference will be mainly made to an application of the method according to the invention to a PC equipped with a screen. However, it should be noted that the method according to the invention can also be applied to different processing devices equipped with screens (i.e. it can be implemented and executed by them), such as computers different from PCs, smartphones and tablets, still remaining within the scope of protection of the present invention.</p><p id="p-0006" num="0005">Furthermore, in the following of the present description, reference will be mainly made to a website or web page as a resource to be browsed. However, it it should be noted that the computer-implemented method according to the invention permits to browse any resource, where in the present description and in the attached claims, in addition to a website or a web page, also a program or an application (e.g., a mail program such as Outlook&#xae;) or a document in any file format, such as a text document (e.g. Word&#xae;), a spreadsheet (e.g. Excel&#xae;), a presentation (e.g. Powerpoint&#xae;), an image is meant with resource.</p><p id="p-0007" num="0006">In the field of information technology, all those technologies specifically designed to make the IT (hardware and/or software) products themselves accessible and usable even to people with disabilities are called assistive technologies. Many of these concern man-machine interfaces made after the initial product conceived for non-disabled users, i.e. that requires a significant visual perception by the user, and, as a consequence of their sophisticated design, implementation and low economy of scale, they tend to be technologies extremely expensive for the end user, who is possibly disabled.</p><p id="p-0008" num="0007">The prior art assistive technologies dedicated to the use of the Internet without the need for any visual perception by the user include screen readers, braille keyboards for the blind, special mouse pointers and much more.</p><p id="p-0009" num="0008">Screen readers are the most spread aids for blind or visually impaired users as they allow the user, who cannot visually perceive what is shown on a screen, to have control of the PC operating system, on the basis of the transformation of the text appearing on the screen into voice.</p><p id="p-0010" num="0009">In particular, users who have technical competences use many software applications to browse the elements and functionalities of a computer and of the operating system, elements on web pages and all elements of a GUI (Graphical User Interface) shown on the screen of the computer or other processing device.</p><p id="p-0011" num="0010">The technologies underlying these softwares and in particular the screen readers are mainly based on the accessibility features offered by operating systems such as Windows or Linux and the use of speech synthesis engines to read the text or description associated with the selected element.</p><p id="p-0012" num="0011">In this regard, it is known that speech technology substantially consists of two distinct and complementary technologies: Speech Recognition or ASR (Automatic Speech Recognition), and Speech Synthesis or TTS (Text To Speech). ASR technology permits the recognition of the user's speech by a computer, equipped with a microphone and audio electronic devices and an appropriate computer program, or software. Electronic audio devices transduce the sounds of spoken words which reach the microphone into electrical signals which are interpreted by the software as corresponding character strings. Instead, TTS technology consists of the opposite operation, in which a computer, equipped with an appropriate software, electronic audio devices and a loudspeaker, performs transduction of words from a stored text into sounds, into spoken words. The software modules which perform speech recognition and speech synthesis operations are called speech engines. Thus, there exist speech recognition and speech synthesis engines.</p><p id="p-0013" num="0012">Some of the most widely used prior art screen readers are Jaws&#xae; available from Freedom Scientific Inc., Window-Eyes available from GW-Micro, Hal for Windows available from Dolphin Computer Access Ltd., and NVDA available from NV Access. Also in the world of mobile technologies screen reader software have spread such as TalkBack available from Google LLC and Voice Over available from Apple Inc., in addition to generalist voice assistants for mobile devices such as Siri available from Apple Inc., Google Assistant available from Google LLC, and Cortana available from Microsoft Corporation.</p><p id="p-0014" num="0013">In the prior art, some Voice Browsers have been developed which allow Internet browsing to blind users, such as those described by X. Zeng in &#x201c;<i>Evaluation and enhancement of web content accessibility for persons with disabilities</i>&#x201d;, 1 Jan. 2004, pp. 1-173, downloadable from the Internet http://d-scholarship.pittedu/7311/1/XiaomingZeng_Apri12004.pdf, and by I.V. Ramakrishnan et al. in &#x201c;<i>Bridging the Web Accessibility Divide</i>&#x201d;, Electronic Notes in Theoretical Computer Science, Elsevier, Amsterdam, NL, vol. 235, 1 Apr. 2009, pp. 107-124, and in documents WO 02/073599 A1, US 20040128136 A1, US20040218451 A1, US20140047337 A1, and EP3430619 A1.</p><p id="p-0015" num="0014">Furthermore, some scientific studies have been carried out always in relation to Voice Browsers, as described for example by Foad Ha midi et al. in <i>Web</i>-<i>speak: a customizable speech</i>- <i>based web navigation interface for people with disabilities, </i>2010, ResearchGate (https://www.researchgate.net/publication/228993980), by Josiah Poon et al., <i>Browsing the Web from a Speech</i>-<i>Based Interface, </i>2001, School of CSEE, U. Of Queensland, Australia. In particular, most of these studies have focused on the extraction of contents from a website or web page, as described for example by Hamza Yunis, in <i>Content Extraction from Webpages Using Machine Learning, </i>2016, Master's Thesis, Bauhaus-Universit&#xe4;t Weimar (https://webis.de/downloads/theses/papers/yunis_2017.pdf), that further describes an analysis related to noise reduction, i.e. to the optimization induced in the web pages and their management (starting from the download), for example in relation to the advertising contents contained therein which are managed separately from other contents, e.g. eliminating advertising, and by Marek Kowalkiewicz et al. in <i>myPortal: Robust Extraction and Aggregation of Web Content, </i>2006, VLDB Endowment, ACM 1-59593-385-9/06/09, by Matthew E. Peters et al. in <i>Content Extraction Using Diverse Feature Sets, </i>2013, WWW2013 Companion, Rio de Janeiro, Brazil, ACM 978-1-4503-2038-2/13/05, by Aanshi Bhardwaj et al. in <i>A novel Approach for Content Extraction from Web Pages, </i>2014, Proceedings of 2014 RAECSd UIET Panjab University Chandigarh, 978-1-4799-2291-8, and by Pan Suhan et al. in <i>An Effective Method to Extract Web Content Information, </i>2018, College of Information Engineering, Yangzhou University, Yangzhou, China. Doi: 10.17706/jsw.13.11.621-629. Other similar studies have focused on particular aspects of content extraction in general, such as:<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0000">    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0015">Page Segmentation for the extraction of contents from the web, as described by Varma Tanvi in <i>Web Page Segmentation: A Survey, </i>2012, International Journal of Engineering Research &#x26; Technology (IJERT), ISSN: 2278-0181, by Pooja Malhotra et al. in Web Page Segmentation Towards Information Extraction for Web Semantics, in S. Bhattacharyya et al. (eds.), <i>International Conference on Innovative Computing and Communications</i>, Lecture Notes in Networks and Systems 56. Springer Nature Singapore Pte Ltd. 2019, and by Judith Andrew et al. in <i>Web Page Segmentation for Non Visual Skimming</i>, the 33<sup>rd </sup>Pacific Asia Conference on Language, Information and computation (PACLIC33), September 2019, Hakodate, Japan. Hal-02309625; and</li>        <li id="ul0002-0002" num="0016">ontology and the web, as described by Valentina Presutti et al. in <i>Content Ontology Design Patterns as Practical Building Blocks for Web Ontologies, </i>2008, ISTC-CNR, Semantic Technology Lab, Italy, and by Na Li et al. (with reference to convolutional networks) in <i>Ontology Completion Using Graph Convolutional Networks</i>, in The Semantic Web. 18th International Semantic Web Conference 2019, Proceedings, Part I.</li>    </ul>    </li></ul></p><p id="p-0016" num="0017">However, prior art screen readers and Voice Browsers suffer from some drawbacks.</p><p id="p-0017" num="0018">First of all, they are suitable only for users, possibly blind or visually impaired, who are skilled in the use of PCs and keyboards, which are the only means for interacting with them.</p><p id="p-0018" num="0019">Also, in order to browse a website through the prior art screen readers and Voice Browsers, the user listens to the speech synthesis that recites the links and the contents on which the blind or visually impaired user places the focus by means of the directional keys. This implies that the prior art screen readers have a sequential approach, consequently very slow, tedious, frustrating and unsatisfactory for users.</p><p id="p-0019" num="0020">Furthermore, the prior art generalist voice assistants do not fully solve the problem of accessibility and usability of web services especially for blind and visually impaired users, since they try to interpret the activity requested by the user and in most of the cases they carry out a simple search on the web, without establishing a real dialogue with the user. In other words, these generalist voice assistants are limited to activating the application or browsing the web page considered the solution closest to the request made by the user, without making the dialogue evolve and without interacting with the contents of the page in a structured and precise manner, i.e. they do not permit a subsequent interaction to complete the activity until its completion as requested by the user.</p><p id="p-0020" num="0021">Finally, prior art screen readers and Voice Browsers are complex and usually limited to browsing a single resource, in particular Internet browsing or the use of a specific software program.</p><p id="p-0021" num="0022">The object of the present invention is therefore to allow a user, in particular, although not necessarily, a blind or visually impaired user, to browse a resource through voice interaction, without the need for any visual perception by the user, in a simple, fast, reliable, efficient and inexpensive way.</p><p id="p-0022" num="0023">It is specific subject-matter of the present invention a computer-implemented method of browsing a resource through voice interaction comprising the following steps:<ul id="ul0003" list-style="none">    <li id="ul0003-0001" num="0024">A. acquiring from a user a request aimed at browsing a resource;</li>    <li id="ul0003-0002" num="0025">B. downloading the requested resource;</li>    <li id="ul0003-0003" num="0026">C. performing a syntactic parsing of the downloaded resource;</li>    <li id="ul0003-0004" num="0027">D. extracting from the downloaded resource one or more lists, if any, of selectable shortcuts pointing to portions inside or outside the downloaded resource through at least one analysis of extraction of lists of selectable shortcuts of the downloaded resource selected from the group comprising a syntactic analysis of extraction of lists of selectable shortcuts, a semantic analysis of extraction of lists of selectable shortcuts and a morphological-visual analysis of extraction of lists of selectable shortcuts, wherein said at least one analysis of extraction of lists of selectable shortcuts of the downloaded resource is performed on the basis of an ontology corresponding to the type of resource provided by a resource type detector module;</li>    <li id="ul0003-0005" num="0028">E. on the basis of the ontology corresponding to the type of resource provided by the resource type detector module, building a list comprising one or more of said one or more lists, if any, of selectable shortcuts extracted in step D from the downloaded resource, wherein said one or more lists of selectable shortcuts are ordered in the list according to a list prioritisation;</li>    <li id="ul0003-0006" num="0029">F. extracting from the downloaded resource one or more one or more content elements, if any, different from lists of selectable shortcuts pointing to portions inside or outside the downloaded resource through at least one analysis of extraction of content elements of the downloaded resource selected from the group comprising a syntactic analysis of extraction of content elements, a semantic analysis of extraction of content elements and a morphological-visual analysis of extraction of content elements, wherein said at least one analysis of content elements of the downloaded resource is performed on the basis of the ontology corresponding to the type of resource provided by the resource type detector module;</li>    <li id="ul0003-0007" num="0030">G. on the basis of the ontology corresponding to the type of resource provided by the resource type detector module, building a list comprising one or more of said one or more content elements, if any, extracted in step F from the downloaded resource, wherein said one or more content elements are ordered in the list according to a content element prioritisation;</li>    <li id="ul0003-0008" num="0031">H. on the basis of the list of one or more lists of selectable shortcuts built in step E and of the list of one or more content elements built in step G, and on the basis of the ontology corresponding to the type of resource provided by a resource type detector module, building a final structure of lists of selectable shortcuts and of content elements;</li>    <li id="ul0003-0009" num="0032">I. playing a voice prompt based on the final structure of lists of selectable shortcuts and of content elements and starting a voice interaction with the user for browsing the resource. According to another aspect of the invention, the method may provide that in step D:    <ul id="ul0004" list-style="none">        <li id="ul0004-0001" num="0033">said syntactic analysis of extraction of lists of selectable shortcuts is performed through a neural network for syntactic detection of lists of selectable shortcuts,</li>        <li id="ul0004-0002" num="0034">said semantic analysis of extraction of lists of selectable shortcuts is performed through a neural network for semantic detection of lists of selectable shortcuts, and</li>        <li id="ul0004-0003" num="0035">said morphological-visual analysis of extraction of lists of selectable shortcuts is performed through a neural network for morphological-visual detection of lists of selectable shortcuts,<br/>and in step F:</li>        <li id="ul0004-0004" num="0036">said syntactic analysis of extraction of content elements is performed through a neural network for syntactic detection of content elements,</li>        <li id="ul0004-0005" num="0037">said semantic analysis of extraction of content elements is performed through a neural network for semantic detection of content elements, and</li>        <li id="ul0004-0006" num="0038">said morphological-visual analysis of extraction of content elements is performed through a neural network for morphological-visual detection of content elements.</li>    </ul>    </li></ul></p><p id="p-0023" num="0039">According to a further aspect of the invention, the method may provide that:<ul id="ul0005" list-style="none">    <li id="ul0005-0001" num="0000">    <ul id="ul0006" list-style="none">        <li id="ul0006-0001" num="0040">said one or more lists extracted in step D are filtered through at least one analysis of filtering of lists of selectable shortcuts of the downloaded resource selected from the group comprising a syntactic analysis of filtering of lists of selectable shortcuts, a semantic analysis of filtering of lists of selectable shortcuts and a morphological-visual analysis of filtering of lists of selectable shortcuts, wherein said at least one analysis of filtering of lists of selectable shortcuts of the downloaded resource is performed on the basis of an ontology corresponding to the type of resource provided by the resource type detector module; and</li>        <li id="ul0006-0002" num="0041">said one or more content elements extracted in step F are filtered through at least one analysis of filtering of content elements of the downloaded resource selected from the group comprising a syntactic analysis of filtering of content elements, a semantic analysis of filtering of content elements and a morphological-visual analysis of filtering of content elements, wherein said at least one analysis of filtering of content elements of the downloaded resource is performed on the basis of an ontology corresponding to the type of resource provided by the resource type detector module.</li>    </ul>    </li></ul></p><p id="p-0024" num="0042">According to an additional aspect of the invention, the method may provide that in step D:<ul id="ul0007" list-style="none">    <li id="ul0007-0001" num="0000">    <ul id="ul0008" list-style="none">        <li id="ul0008-0001" num="0043">said syntactic analysis of filtering of lists of selectable shortcuts is performed through a neural network for syntactic filtering of lists of selectable shortcuts,</li>        <li id="ul0008-0002" num="0044">said semantic analysis of filtering of lists of selectable shortcuts is performed through a neural network for semantic filtering of lists of selectable shortcuts, and</li>        <li id="ul0008-0003" num="0045">said morphological-visual analysis of filtering of lists of selectable shortcuts is performed through a neural network for morphological-visual filtering of lists of selectable shortcuts,<br/>and in step F:</li>        <li id="ul0008-0004" num="0046">said syntactic analysis of filtering of content elements is performed through a neural network of syntactic filtering of content elements,</li>        <li id="ul0008-0005" num="0047">said semantic analysis of filtering of content elements is performed through a neural network of semantic filtering of content elements, and</li>        <li id="ul0008-0006" num="0048">said morphological-visual analysis of filtering of content elements is performed through a neural network of morphological-visual filtering of content elements.</li>    </ul>    </li></ul></p><p id="p-0025" num="0049">According to another aspect of the invention, the method may provide that:<ul id="ul0009" list-style="none">    <li id="ul0009-0001" num="0000">    <ul id="ul0010" list-style="none">        <li id="ul0010-0001" num="0050">the resource type detector module receives in step C a first identification of the type of resource,</li>        <li id="ul0010-0002" num="0051">in step D, in the case where said at least one analysis of extraction of lists of selectable shortcuts of the downloaded resource recognises that said first identification is wrong, the resource type detector module receives a second identification of the type of resource, and execution of step D is repeated, wherein said at least one analysis of extraction of lists of selectable shortcuts of the downloaded resource is performed on the basis of the ontology corresponding to the second identification of the type of resource, and</li>        <li id="ul0010-0003" num="0052">in step F, in the case where the resource type detector module has not received a second identification of the type of resource and said at least one analysis of extraction of content elements of the downloaded resource recognises that said first identification is wrong, or in the case where the resource type detector module has received a second identification of the type of resource and said at least one analysis of extraction of content elements of the downloaded resource recognises that said second identification is wrong, the resource type detector module receives a third identification of the type of resource, and both execution of step D, wherein said at least one analysis of extraction of lists of selectable shortcuts of the downloaded resource is performed on the basis of the ontology corresponding to the third identification of the type of resource, and execution of step F, wherein said at least one analysis of extraction of content elements of the downloaded resource is performed on the basis of the ontology corresponding to the third identification of the type of resource.</li>    </ul>    </li></ul></p><p id="p-0026" num="0053">According to a further aspect of the invention, the method may provide that, in the case where in step C a main list of selectable shortcuts is identified on the basis of a syntactic self-declaration, in step E the list (namely, the list of lists) comprising one or more lists di selectable shortcuts is built including said main list of selectable shortcuts identified in step C as main list of selectable shortcuts.</p><p id="p-0027" num="0054">According to an additional aspect of the invention, the method may provide that, in step B, it is checked whether the downloaded resource has been browsed in a time interval preceding the request acquired in step A and:<ul id="ul0011" list-style="none">    <li id="ul0011-0001" num="0000">    <ul id="ul0012" list-style="none">        <li id="ul0012-0001" num="0055">if yes, the method loads the final structure of lists of selectable shortcuts and of content elements previously built for said downloaded resource and skips to step I, otherwise</li>        <li id="ul0012-0002" num="0056">if not, the method proceeds to step C.</li>    </ul>    </li></ul></p><p id="p-0028" num="0057">According to another aspect of the invention, the method may provide that immediately after step B spurious elements, if any, are eliminated from the resource and wherein between step C and step D multimedia elements, if any, are managed by a Multimedia_Detector/Manager module.</p><p id="p-0029" num="0058">According to a further aspect of the invention, the method may provide that in step A the request aimed at browsing a resource is analysed and, if the method recognises that it needs additional information, the method institutes a dialogue with the user to obtain said additional information.</p><p id="p-0030" num="0059">According to an additional aspect of the invention, the method may provide that step I is performed taking account of information stored in a user profile, optionally for a user voice recognition.</p><p id="p-0031" num="0060">According to another aspect of the invention, the method may provide that between step A and step B a search of the requested resource, optionally stored in a user profile, is performed.</p><p id="p-0032" num="0061">According to an additional aspect of the invention, the search of the requested resource may be stored in a user profile.</p><p id="p-0033" num="0062">According to a further aspect of the invention, the resource may be selected from the group comprising:<ul id="ul0013" list-style="none">    <li id="ul0013-0001" num="0000">    <ul id="ul0014" list-style="none">        <li id="ul0014-0001" num="0063">a web page, thereby the lists of selectable shortcuts pointing to portions inside or outside the downloaded resource consist of menus of the web page and the content elements are non-menu contents of the web page;</li>        <li id="ul0014-0002" num="0064">a document, thereby the lists of selectable shortcuts pointing to portions inside or outside the downloaded resource consist of indexes or summaries of the document; and</li>        <li id="ul0014-0003" num="0065">an app.</li>    </ul>    </li></ul></p><p id="p-0034" num="0066">It is still specific subject-matter of the present invention a system comprising a processing device, provided with microphone and at least one electronic audio playing device, and one or more servers, optionally one or more cloud servers, with which the processing device is configured to connect, wherein the processing device and/or said one or more servers are configured to perform the computer-implemented method of browsing a resource through voice interaction as previously described.</p><p id="p-0035" num="0067">It is further specific subject-matter of the present invention a set of one or more computer programs comprising instructions which, when executed by one or more processing units, cause said one or more processing units to carry out the computer-implemented method of browsing a resource through voice interaction as previously described.</p><p id="p-0036" num="0068">It is an additional specific subject-matter of the present invention a set of one or more computer-readable storage media having stored thereon the set of one or more computer programs just described.</p><p id="p-0037" num="0069">The computer-implemented method of browsing a resource through voice interaction implements a voice browser, based on a full voice interface (i.e., that speaks and listens) and artificial intelligence, that allows to browse with the voice any resource, such as a website or a web page, a document or an app, thanks to an automatic syntactic and/or semantic and/or morphological-visual extraction of its elements, for example menus and non-menu contents. Such voice browser is able to guide and vocally perform all the activities which lead to the conclusion of a specific task requested by a user, and in particular to guide the user to the use of all the significant parts relating to the contents and services present on a resource (e.g. an entire website or within any web page), through an organized and structured logical voice navigation depending on the contents and services present within the resource itself (e.g. the web page).</p><p id="p-0038" num="0070">In particular, the computer-implemented method of browsing a resource through voice interaction carries out a voice interaction with users, possibly blind and visually impaired, that is accurate from the point of view of semantic quality, that allows to correctly address situations of ambiguity and is reliable in the response and information provided as well as in the legibility and perception of naturalness of the interaction established with the user.</p><p id="p-0039" num="0071">The voice browser implemented by the method according to the invention is a tool that allows a user, in particular with visual disabilities, a simple and fast browsing within a resource, e.g. a web page, allowing him to browse within the resource in an &#x201c;intelligent&#x201d; way with the simple aid of voice interaction, based on the extraction of the elements within each resource so as to provide them clearly to the user himself/herself. This means that such voice browser is particularly indicated for users who have accessibility needs, permitting to access even those resources in which the prior art screen readers struggle to move easily due to a bad implementation, such as websites or web pages in which assistive technologies are rendered unusable by an inadequate structure that does not respect accessibility rules. In this regard, the combination of simplicity and speed makes the browsing experience for the user, possibly blind or visually impaired, fully satisfactory, avoiding situations of frustration due to the access to resources, e.g. websites, poorly structured, and also it makes any resource accessible to everyone, even to those users who do not want to or cannot learn manuals and technical guides of the current screen readers, as the interaction can also take place only with the use of the voice.</p><p id="p-0040" num="0072">By way of example, some embodiments of the method according to the invention manage some short cuts for the rapid scrolling of a web page, such as requesting the list of links or titles within a section of the page. In this case, the user has at his/her disposal a simple means such as voice commands, but also the possibility of quickly scrolling using the arrows of a remote control (with which the apparatus according to the invention is configured to interact) between the elements of the web page.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><p id="p-0041" num="0073">The present invention will be now described, for illustrative but not limiting purposes, according to its preferred embodiments, with particular reference to the Figures of the attached drawings, in which:</p><p id="p-0042" num="0074"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows a schematic flow chart of a preferred embodiment of the computer-implemented method of browsing a resource through voice interaction according to the invention;</p><p id="p-0043" num="0075"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows a flow chart of the building phase of neural networks that are part of the preferred embodiment of the system according to the invention;</p><p id="p-0044" num="0076"><figref idref="DRAWINGS">FIG. <b>3</b></figref> shows a schematic block diagram of the working phase of the neural networks of <figref idref="DRAWINGS">FIG. <b>2</b></figref>; and</p><p id="p-0045" num="0077"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows three variants of the preferred embodiment of the system according to the invention.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0046" num="0078">In the Figures identical reference numerals will be used for alike elements.</p><p id="p-0047" num="0079">The preferred embodiment of the computer-implemented method and of the system according to the invention which permit to browse a website or web page by voice interaction is illustrated in the following. Other embodiments that allow browsing through voice interaction of a resource, different from a website or web page such as for example a program or an application (e.g., a mail program such as Outlook&#xae;) or a document in any file format, such as a text document (e.g. Word&#xae;), a spreadsheet (e.g. Excel&#xae;), a presentation (e.g. Powerpoint&#xae;), an image, have similar technical features.</p><p id="p-0048" num="0080">The preferred embodiment of the method according to the invention is performed (i.e. implemented) through a processing device (such as a computer, a personal computer, a smartphone or a tablet) or a system of which the processing device is part (for example in a cloud architecture), wherein the processing device is equipped with a screen, speakers and microphone, wherein the microphone can also be incorporated into a remote control configured to communicate with the processing device (such as the Speaky Facile&#xae; remote control of Mediavoice s.r.l.&#x2014;see www.mediavoice.it&#x2014;that is configured to communicate with a personal computer).</p><p id="p-0049" num="0081">The preferred embodiment of the method according to the invention receives an initial request from the user related to the web page (or website) to be browsed, searches and downloads the requested web page, analyses it through artificial intelligence identifying menus, if present, and non-menu contents, if present, thereof and finally interacts with the user through voice interaction allowing him to use and browse the requested web page.</p><p id="p-0050" num="0082"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows a schematic flow chart of the preferred embodiment of the computer-implemented method according to the invention. Through the microphone of the processing device, the method according to the invention acquires (<b>100</b>) as input a browsing request from the user, e.g., &#x201c;go to site x&#x201d;, &#x201c;see site x&#x201d;, &#x201c;browse x&#x201d;, and the like. Advantageously, the user pronounces, with a natural voice, a question and/or a statement (which constitute a request aimed at browsing), in order to request to browse a web page or a specific website. Through its own ASR module that processes the sound signal acquired by the microphone, the method analyses (<b>105</b>) the browsing request pronounced by the user and, if it recognises that it has all the necessary information, it carries out the search for the web page or website. Otherwise, if the method recognises that it still needs additional information, it establishes a dialogue with the user using the TTS and ASR modules. In particular, the acquired request is broken down into preambles (if present), web page or website name and postambles (if present), possibly carrying out an interaction with the user for the purpose of domain disambiguation (.it, .com, .net etc.).</p><p id="p-0051" num="0083">Other embodiments of the method according to the invention, in which the resource to be browsed is different, such as, for example, a document or an app, operate in a similar way. By way of example, to browse an app the acquired request can be &#x201c;open app x&#x201d; or the like, to browse a document the acquired request can be &#x201c;open document x&#x201d; or the like. In this case, the acquired request is broken down into preambles (if present), app or document name and postambles (if present), possibly carrying out an interaction with the user for the purpose of disambiguation, for example, in the case of documents, of disambiguation on the type of document (doc, pdf, txt, etc.).</p><p id="p-0052" num="0084">To carry out the search (<b>110</b>), the method according to the invention uses an external search engine <b>200</b> and a processor <b>205</b> for its results which identifies a URI (Uniform Resource Identifier) of the resource, that in the preferred embodiment of the method according to the invention consists of a Uniform Resource Locator (URL). Advantageously, the search carried out for the specific user is stored in a user profile <b>210</b> (preferably identified through its own credentials with which the user has started the computer-implemented method on the processing device).</p><p id="p-0053" num="0085">Other embodiments of the method according to the invention, in which the resource to be browsed is different, such as, for example, a document or an app, operate in a similar way, and in this case the URI consists of a URN (Uniform Resource Name), e.g. the name of the document.</p><p id="p-0054" num="0086">Subsequently, the method checks (<b>115</b>) whether the requested resource identified by the URI, that in the preferred embodiment is the web page identified by the URL, has been in the cache of the processing device with which the user interacts for no more than a maximum period of time, i.e. if it has been browsed in the recent past, namely in a specific time interval prior to the request that can be optionally set, more optionally depending on the type of website (e.g., 2 hours for a news website and 1 day for a video game website). In the following of the present description, the &#x201c;website type&#x201d; is also referred to as &#x201c;domain type&#x201d;, and this definition has no relation to the DNS domain name system, thereby it must not be confused, for example, with the type of domain name, e.g. with the TLD (Top Level Domain).</p><p id="p-0055" num="0087">In the positive, i.e. if the web page identified by the URL has been in the cache of the processing device for no more than a maximum period of time, the method downloads (<b>120</b>) what is stored in the cache of the processing device for that URL and delivers (<b>125</b>) directly the related prompt voice, already prepared and stored by the analysis carried out during the preceding browsing of the web page identified by the URL that occurred within said maximum period of time, starting a voice interaction with the user to browse the web page.</p><p id="p-0056" num="0088">Alternatively in the negative case, i.e. if the web page identified by the URL is not in the cache of the processing device or has been present there for more than the maximum time period, the method downloads the web page (<b>130</b>) and proceeds with the steps of analysis of its content.</p><p id="p-0057" num="0089">Advantageously, immediately after the download (<b>130</b>) of the web page, the method eliminates possible spurious elements from the resource, such as for example the advertising contents. Such elimination in the case of a web page can occur either by deleting &#x201c;upstream&#x201d; (i.e. before the completion of the download) such spurious contents in order to optimise the download time, or &#x201c;downstream&#x201d; (i.e. at the completion of the download), inhibiting the related code and not deleting it so as to leave the formatting of the web page, i.e. its morphological-visual structure, unaltered.</p><p id="p-0058" num="0090">Subsequently, the method according to the invention performs a syntactic parsing (<b>135</b>) of the resource, i.e. of the web page, during which it identifies all its syntactic elements, including the syntactic self-declaration relating to the Main Menu. In this regard, also the embodiments of the method according to the invention in which the resource to be browsed is an app identify, among its syntactic elements, the Main Menu, while the embodiments of the method according to the invention in which the resource to be browsed is a document identify, instead of the Main Menu, a main index (or summary). In general, the method according to the invention identifies a main list of selectable shortcuts for pointing to portions inside or outside the resource downloaded with the download.</p><p id="p-0059" num="0091">The syntactic parsing (<b>135</b>) of the resource also provides a domain detector module <b>215</b> with a first identification (e.g., as highest probability hypothesis) of the domain type to which such web page belongs, e.g. discriminating between information, transport, training, entertainment, e-commerce, etc. (thereby the domain type is identified within a set of domain types). In particular, for such first identification, the syntactic parsing (<b>135</b>) searches, through pattern matching, in the web page for a series of elements the presence of which is indicative and typical of a domain. By way of example, and not by way of limitation, a home page of a site repeatedly containing the word train and containing an input box &#x201c;FROM&#x201d;, an input box &#x201c;TO&#x201d;, an input box &#x201c;Date&#x201d;, an input box &#x201c;TIMETABLE&#x201d; is most likely the home page of a rail transport site. In this regard, syntactic parsing (<b>135</b>) exploits a structure of analogous patterns for each domain type (e.g., sites for public administration, schools, hospitals, e-commerce, real estate, etc.), since each domain type has its specific features, such as keywords and patterns, and has its own specific ontology. Consequently, each domain type has its own pattern, its own ontology and some typical information slots (e.g., for a transport site &#x201c;where do you start from&#x201d; and &#x201c;where do you go&#x201d;, for an e-commerce site &#x201c;what type of product do you prefer?&#x201d; and &#x201c;What budget do you have?&#x201d;). The syntactic parsing identifies the site type from the typical information slots (i.e. it reads the information present in the web page and deduces the domain type). For the recognition of the structure of the web page, the syntactic parsing carried out by the preferred embodiment of the method according to the invention is based on the construction and analysis of the DOM (Document Object Model) tree corresponding to the page itself, that is then analysed and possibly modified during the syntactic parsing performed by the method. This operation allows to extrapolate the web page tags and to assign them a meaning in the case where they do not already have an intrinsic one (as in some HTML5 tags). lithe syntactic parsing (<b>135</b>) of the resource fails to provide a first identification of the domain type to which such web page belongs (e.g., because the probability value of the highest probability is lower than a minimum reliability threshold), then as a first identification of the website type to which such web page belongs, the domain detector module <b>215</b> is provided with the &#x201c;unknown&#x201d; domain type; no ontology is associated with the &#x201c;unknown&#x201d; domain type.</p><p id="p-0060" num="0092">Subsequently, all the Multimedia elements contained in the web page, such as Videos, Photographs, Audio and the like, are managed (<b>140</b>) by a Multimedia_Detector/Manager module based on conventional technology; by way of example, and not by way of limitation, the images are managed through an automatic descriptor currently available to the person skilled in the art.</p><p id="p-0061" num="0093">Subsequently, the method performs an automatic syntactic-semantic-morphological extraction of menus, if any (or, more generally, of lists, if any, of selectable shortcuts pointing to portions inside or outside the resource; in the case where the resource is a document, these lists are indexes or summaries).</p><p id="p-0062" num="0094">Such activity is carried out by a triple subsystem of interconnected neural networks, each of which investigates the syntactic, semantic and morphological-visual structure, respectively, of the resource (that in the illustrated preferred embodiment is a web page).</p><p id="p-0063" num="0095">In particular, the method checks (<b>145</b>) that the Main Menu has been already identified during the syntactic parsing (<b>135</b>) of the resource (i.e. on the basis of the syntactic self-declaration related to the Main Menu).</p><p id="p-0064" num="0096">In the positive, the method transmits the self-declared Main Menu <b>220</b> to a builder module <b>225</b> building a menu list and then proceeds with the automatic extraction of the other menus.</p><p id="p-0065" num="0097">In the negative, it then proceeds with the automatic extraction of both the Main Menu and the other menus.</p><p id="p-0066" num="0098">As mentioned, the method extracts (<b>150</b>) the menus on the downloaded web page through a neural network <b>230</b> for syntactic detection of menus, a neural network <b>235</b> for semantic detection of menus and a neural network <b>240</b> for morphological-visual detection of menus, each of which produces a list of menus (in general, a list of lists, if any, of selectable shortcuts pointing to portions inside or outside the resource) ordered according to a prioritisation (i.e. a ranking) of the menus thus extracted (and univocally identified through the relative html tag) that takes into account specific factors for each of the three detection neural networks. By way of example, and not by way of limitation, in the case where the method proceeds with the automatic extraction of both the Main Menu and the other menus, if there are two or more menus on the web pages, the main menu is identified: by the syntactic detection on the basis of the recommendations of the W3C-WAI (Web Accessibility Initiative of the World Wide Web Consortium) for the creation of web pages; by the semantic detection as that containing at least one of the items &#x201c;home&#x201d;, &#x201c;homepage&#x201d; and &#x201c;main page&#x201d;; and by the morphological-visual detection as the first menu present on the web page. The extraction process also checks for the presence of sub-menus (e.g., in expandable menus).</p><p id="p-0067" num="0099">In particular, the three neural networks <b>230</b>, <b>235</b> and <b>240</b> for menu detection are driven by the ontology <b>245</b> corresponding to the first identification of the domain type provided by the syntactic parsing (<b>135</b>) to the domain detector module <b>215</b>; if the domain type assigned to the web page is &#x201c;unknown&#x201d;, then no ontology is loaded to drive the three neural networks <b>230</b>, <b>235</b> and <b>240</b> for menu detection. Advantageously, the set of ontologies of domain types is updated over time on the basis of the processing carried out by the method according to the invention (to this end, the set of ontologies is managed and updated through a neural network for ontologies which communicates with the domain detector module <b>215</b>).</p><p id="p-0068" num="0100">It should be noted that other embodiments of the method according to the invention can proceed to the extraction (<b>150</b>) of the menus on the web page downloaded through only one or two neural networks, selected from the group comprising a neural network for syntactic detection, a network neural for semantic detection and a neural network for morphological-visual detection.</p><p id="p-0069" num="0101">Next, the method performs a filtering (<b>155</b>) of the menus provided by the three neural networks <b>230</b>, <b>235</b> and <b>240</b> for menu detection. In particular, such filtering is carried out through a neural network <b>245</b> for syntactic filtering of menus, that optionally (but not necessarily) receives the results provided by the neural network <b>230</b> for syntactic detection of menus, a neural network <b>250</b> for semantic filtering of menus, that optionally (but not necessarily) receives the results provided by the neural network <b>235</b> for semantic detection of menus, and a neural network <b>255</b> for morphological-visual filtering of menus, that optionally (but not necessarily) receives the results provided by the neural network <b>240</b> for morphological-visual detection of menus. Such three neural networks for filtering of menus eliminate the menus (in general, the lists, if any, of selectable shortcuts pointing to portions inside or outside the resource) produced by each of the neural networks for menu detection which perform the extraction (<b>150</b>) of the menus which are judged as incorrect, optionally according to heuristic symbolic logics of the other two neural networks for menu detection. By way of example, and not by way of limitation, a candidate menu extracted by the neural network <b>240</b> for morphological-visual detection of menus that is composed of a simple sentence that at least one of the other two neural networks for menu detection recognises as not being a list of selectable pointing shortcuts, or a candidate menu extracted by the neural network <b>230</b> for syntactic detection of menus that the neural network <b>235</b> for semantic detection of menus recognises as composed of options with unlikely semantic consistency and/or that the neural network <b>240</b> for morphological-visual detection of menus recognises as composed of options with unlikely morphological-visual distribution in the web page.</p><p id="p-0070" num="0102">It should be noted that other embodiments of the method according to the invention can proceed to filtering the results provided by the extraction (<b>150</b>) of the menus in the downloaded web page (provided through one, two or three neural networks for menu detection), through only one or two neural networks for menu filtering selected from the group comprising a neural network for syntactic filtering of menus, a neural network for semantic filtering of menus and a neural network for morphological-visual filtering of menus.</p><p id="p-0071" num="0103">Subsequently, the builder module <b>225</b> of a menu list receives the results obtained with the filtering (<b>155</b>) of the menus (and possibly the self-declared Main Menu <b>220</b>) and, on the basis of the ontology <b>245</b> corresponding to the domain type to which the downloaded web page belongs, produces the list of menus, if any, of the downloaded web page (more generally the list of lists, if any, of selectable shortcuts pointing to portions inside or outside of the resource) ordered according to a prioritisation (ie a ranking) of the menus.</p><p id="p-0072" num="0104">In particular, the three neural networks <b>245</b>, <b>250</b> and <b>255</b> for menu filtering are driven by the ontology <b>245</b> corresponding to the first identification of the domain type provided by the syntactic parsing (<b>135</b>) to the domain detector module <b>215</b>; if the domain type assigned to the web page is &#x201c;unknown&#x201d;, then no ontology is loaded to drive the three neural networks <b>245</b>, <b>250</b> and <b>255</b> for menu filtering. In the preferred embodiment of the method according to the invention, in the case where the extraction (<b>150</b>) of the menus, or the filtering (<b>155</b>) of the menus or the builder module <b>225</b> building a menu list recognise that the first identification of the domain type provided by the syntactic parsing (<b>135</b>) is incorrect, they provide the domain detector module <b>215</b> with a second identification of the domain type and the method is executed again starting from the extraction (<b>150</b>) of the menus wherein it is the ontology <b>245</b> of the new domain type that drives the extraction (<b>150</b>) of menus, the filtering (<b>155</b>) of menus and the builder module <b>225</b> building a menu list.</p><p id="p-0073" num="0105">Subsequently, the method performs the analysis of the non-menu contents of the web page (in general, of the elements different from the lists of selectable shortcuts pointing to portions inside or outside the resource, that in the following are also referred to as content elements), including the forms and/or boxes to be filled, through three neural networks for the detection of non-menu contents and three neural networks for the filtering of non-menu contents similar to what illustrated for the neural networks operating on the menus. Such neural networks for the detection and filtering of non-menu contents analyse all the content of the web page (in general of the resource) that is not part of menus.</p><p id="p-0074" num="0106">In detail, the method extracts (<b>160</b>) the non-menu contents in the downloaded web page through a neural network <b>260</b> for syntactic detection of non-menu contents, a neural network <b>265</b> for semantic detection of non-menu contents and a neural network <b>270</b> for morphological-visual detection of non-menu contents, each of which produces a list of non-menu contents (in general, a list of elements, if any, different from lists of selectable shortcut pointing to portions inside or outside the resource) ordered according to a prioritisation (i.e. a ranking) of the non-menu contents thus extracted (and univocally identified through the relative html tag) that takes into account specific factors for each one of the three neural networks for the detection of non-menu contents.</p><p id="p-0075" num="0107">In particular, the three neural networks <b>260</b>, <b>265</b> and <b>270</b> for detecting non-menu contents are driven by the ontology <b>245</b> corresponding to the identification of the domain type provided by the detector module <b>215</b> on the basis of the first identification provided by the syntactic parsing (<b>135</b>) or any subsequent identification provided by the extraction (<b>150</b>) of the menus or by the filtering (<b>155</b>) of the menus or by the builder module <b>225</b> building a menu list. If the domain type assigned to the web page is &#x201c;unknown&#x201d;, then no ontology is loaded to drive the three detection neural networks <b>260</b>, <b>265</b> and <b>270</b>.</p><p id="p-0076" num="0108">It should be noted that other embodiments of the method according to the invention can proceed to the extraction (<b>160</b>) of the non-menu contents in the downloaded web page through only one or two neural networks, selected from the group comprising a neural network for syntactic detection of non-menu contents, a neural network for semantic detection of non-menu contents and a neural network for morphological-visual detection of non-menu contents.</p><p id="p-0077" num="0109">Next, the method performs a filtering (<b>165</b>) of non-menu content of the results provided by the three neural networks <b>260</b>, <b>265</b> and <b>270</b> for detection of non-menu contents. In particular, such filtering is carried out through a neural network <b>275</b> for syntactic filtering of non-menu contents, that optionally (but not necessarily) receives the results provided by the neural network <b>260</b> for syntactic detection of non-menu contents, a neural network <b>280</b> for semantic filtering of non-menu contents, that optionally (but not necessarily) receives the results provided by the neural network <b>265</b> for semantic detection of non-menu contents, and a neural network <b>285</b> for morphological-visual filtering of non-menu contents, that optionally (but not necessarily) receives the results provided by the neural network <b>270</b> for morphological-visual detection of non-menu contents. Such three neural networks for filtering of non-menu content eliminate non-menu content produced by each one of the neural networks for detection of non-menu content which carry out the extraction (<b>160</b>) of the non-menu content which are judged to be incorrect, optionally according to heuristic symbolic logics of the other two neural networks for detection of non-menu contents.</p><p id="p-0078" num="0110">It should be noted that other embodiments of the method according to the invention can proceed to the filtering of non-menu contents of the results provided by the extraction (<b>160</b>) of the non-menu contents in the downloaded web page (provided by one, two or three neural networks), through only one or two neural networks for filtering of non-menu content selected from the group comprising a neural network for syntactic filtering of non-menu contents, a neural network for semantic filtering of non-menu contents and a neural network for morphological-visual filtering of non-menu contents.</p><p id="p-0079" num="0111">Subsequently, the builder module <b>290</b> building structure of menus and non-menu contents receives the results obtained with the filtering (<b>165</b>) of non-menu contents and, on the basis of the ontology <b>245</b> corresponding to the domain type to which the downloaded web page belongs, produces the list of non-menu contents, if any, of the downloaded web page (more generally the list of elements, if any, different from lists of selectable shortcut pointing to portions inside or outside the resource) ordered according to a prioritisation (i.e. a ranking) of the non-menu contents. Then, on the basis of the list of non-menu contents, if any, and of the list of menus received by the builder module <b>225</b> building a menu list, and again on the basis of the ontology <b>245</b> corresponding to the domain type to which the downloaded web page belongs, it builds the final structure of menus and non-menu contents and sends it to a Dialogue Manager that delivers (<b>125</b>) the relative voice prompt (that in the case of forms and/or boxes to be filled provides a dialogue for input and confirmation of the data to be input and/or selected), starting the voice interaction with the user to browse the web page.</p><p id="p-0080" num="0112">Advantageously, the Dialogue Manager can avail of the information stored in the user's profile <b>210</b>, e.g. for the recognition of the user's voice. Such voice prompt is made considering the usability rules, which limit, for example, the delivery of an item command prompt with a maximum number of items (optionally ranging from 7 to 10). In case of a higher number of items to be provided as options that can be selected through voice commands by the user, the method organises such items in sub-menus, for example by mechanically dividing (i.e. without grouping the items on the basis of their semantic proximity) the number of options by an integer and producing n sub-menus in cascade (e.g., producing through the TTS module the following sentence &#x201c;I have 18 options, I list them in 6 blocks of 3: item a, item b and item c; item d, item e and item f; . . . &#x201d;), or by semantically clustering the items in sub-menus of options semantically linked to each other (by way of example, a vector distance analyser can group in a sub-menu the items which have a distance between them not exceeding a threshold value and can deliver as sub-menu item grouping them the parent word that semantically has the minimum distance from all the options of the sub-menu under consideration).</p><p id="p-0081" num="0113">In particular, also the three neural networks <b>275</b>, <b>280</b> and <b>285</b> for filtering of non-menu contents are driven by the ontology <b>245</b> corresponding to the identification of the domain type provided by the detector module <b>215</b>; if the domain type assigned to the web page is &#x201c;unknown&#x201d;, then no ontology is loaded to drive the three neural networks <b>275</b>, <b>280</b> and <b>285</b> for filtering of non-menu content. In the preferred embodiment of the method according to the invention, in the case where the extraction (<b>160</b>) of the non-menu contents or the filtering (<b>165</b>) of the non-menu contents recognise that the identification of the domain type provided by the module detector <b>215</b> (on the basis of the first identification provided by the syntactic parsing (<b>135</b>) or of the possible subsequent identification provided by the extraction (<b>150</b>) of menus or by the filtering (<b>155</b>) of menus or by the builder module <b>225</b> building a menu list) is incorrect, they provide the domain detector module <b>215</b> with a third identification of the domain type and the method is executed again starting from the extraction (<b>150</b>) of the menus wherein it is the ontology <b>245</b> of the new domain type that drives the extraction (<b>150</b>) of menus, the filtering (<b>155</b>) of menus, the builder module <b>225</b> building a menu list, the extraction (<b>160</b>) of non-menu contents, the filtering (<b>165</b>) of non-menu contents and the builder module <b>290</b> building menus and non-menu contents.</p><p id="p-0082" num="0114">Also, the builder module <b>290</b> building menus and non-menu contents stores the list of menus and the list of non-menu contents in the cache of the processing device for that web page (identified by the URL).</p><p id="p-0083" num="0115">As mentioned, the method according to the invention makes use of Artificial Intelligence, more specifically of Machine Learning and even more specifically of Deep Learning. In fact, the method is based on the use of neural networks and in particular on Deep Learning configured to recognise patterns thanks to the Artificial Neural Networks (ANN).</p><p id="p-0084" num="0116">This is possible thanks to the fact that the ANNs operate in two phases: a first training phase with datasets containing the pattern to be recognised, wherein they store the pattern itself, and a second phase wherein the network is used to search for the pattern in a new dataset. The key point of their effectiveness is a correct training phase, wherein the pattern to be recognized is taught to the neural network. This teaching consists of showing the correct pattern in many different situations to the network. For example, to train a network to recognise a ship, many images of ships, of all types and colors and of all shapes and perspective types, are shown to the network; this activity, for example indicating a ship within a larger image possibly containing several objects, is defined as Tagging.</p><p id="p-0085" num="0117">The method according to the invention provides three neural networks for detection of menus (in general lists of selectable shortcuts pointing to portions inside or outside the resource): a syntactic one, a semantic one and a morphological-visual one. Similarly for the three neural networks for detection of non-menu contents (in general, elements different from lists of selectable shortcut pointing to portions inside or outside the resource). These networks are used in two different modes and moments: the first phase is when menus are searched on the web page, while the second one is when non-menu contents are searched on the web page.</p><p id="p-0086" num="0118">At the operational level, two different moments are provided for the networks: the construction (Building Phase) and the use (Working phase) schematically shown respectively in <figref idref="DRAWINGS">FIGS. <b>2</b> and <b>3</b></figref> for the preferred embodiment of the method according to the invention.</p><p id="p-0087" num="0119">During the operation phase, the neural networks are incrementally improved and refined in an automatic and/or supervised way. The construction is subdivided into two phases: the classic Training phase, where the patterns to be recognized, in very large quantities, are taught to the networks, and a Test and Tuning phase, for calibration and refinement.</p><p id="p-0088" num="0120">For the purposes of tagging of the web content, the method according to the invention exploits a tool configured to semi-automatically tag the menus and contents of the various sites present on the web. These data are used as the first level of training of the neural network. For example, the data patterns at the input of the syntactic neural network for detection of menus in the training phase consist of portions of HTML code related to menus, the data patterns at the input of the semantic neural network for detection of menus in the training phase consist of sets of data tuples (Sintactic/Morfological/Semantic chunks/clusters, Domains, Ontologies), the data patterns at the input of the morphological-visual neural network for detection of menus in the training phase consist of menu images.</p><p id="p-0089" num="0121">Referring to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, each one of the three neural networks produces its own list of menu candidates, each one univocally identified through the relative html tag. Namely: the neural network <b>230</b> for syntactic detection of menus produces a first list</p><p id="p-0090" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>M</i><sub>A1</sub><i>,M</i><sub>A2</sub><i>. . . M</i><sub>An </sub><?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0091" num="0000">the neural network <b>235</b> for semantic detection of menus produces a second list</p><p id="p-0092" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>M</i><sub>B1</sub><i>,M</i><sub>B2</sub><i>. . . M</i><sub>Bm </sub><?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0093" num="0000">and the neural network <b>240</b> for morphological-visual detection of menus produces a third list</p><p id="p-0094" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>M</i><sub>C1</sub><i>,M</i><sub>C2</sub><i>. . . M</i><sub>Ck </sub><?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0095" num="0122">After the filtering (<b>155</b>), the set intersection of these three lists may be non-empty, thereby there exist some menus which are candidates for all the three neural networks <b>230</b>, <b>235</b> and <b>240</b> for detection of menus: such menus will have more weight in the ordering or ranking of the extracted menus and, consequently, in the preparation phase of the voice prompt by the Dialog Manager. Obviously, the set intersection of these three lists could also be empty, when the three neural networks <b>230</b>, <b>235</b> and <b>240</b> for detection of menus are not in agreement.</p><p id="p-0096" num="0123">The builder module <b>225</b> of a menu list operates as follows to produce the list of menus ordered according to a prioritisation (i.e. a ranking) of the menus.</p><p id="p-0097" num="0124">Each one of the menus in each list has its own specific confidence value (equal to a real value between 0 and 1) assigned thereto by the relative neural network for detection of menus that represents the probability that, according to the specific neural network, that is a Menu: w(M<sub>Ai</sub>), w(M<sub>Bi</sub>), w(M<sub>Ci</sub>).</p><p id="p-0098" num="0125">Thus, each menu M<sub>i </sub>is characterized by a specific overall confidence value w<sub>Mi</sub>, that is a linear combination of such confidence values:</p><p id="p-0099" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>w</i><sub>Mi</sub>=&#x3b1;<sub>Dk</sub><i>*w</i>(<i>M</i><sub>Ai</sub>)+&#x3b2;<sub>Dk</sub><i>*w</i>(<i>M</i><sub>Bi</sub>)+&#x3b3;<sub>Dk</sub><i>*w</i>(<i>M</i><sub>Ci</sub>),<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0100" num="0000">where &#x3b1;<sub>Dk </sub>represents the weight of the neural network <b>230</b> for syntactic detection of menus, &#x3b2;<sub>Dk </sub>represents the weight of the neural network <b>235</b> for semantic detection of menus of the semantic analyser, and &#x3b3;<sub>Dk </sub>represents the weight of the neural network <b>240</b> for morphological-visual detection of menus (all real values between 0 and 1) depending on the specific ontology <b>245</b> of the type domain D<sub>k </sub>(possibly equal to &#x201c;unknown&#x201d;) provided by the domain detector module <b>215</b>. In fact, in a certain domain D<sub>k</sub>, governed by a specific ontology <b>245</b>, the three neural networks <b>230</b>, <b>235</b> and <b>240</b> of detection of menus have a differentiated relevance from each other, influencing the values of the weights &#x3b1;<sub>Dk</sub>, &#x3b2;<sub>Dk </sub>and &#x3b3;<sub>Dk </sub>thereof.</p><p id="p-0101" num="0126">In this way, the builder module <b>225</b> building a menu list is able to build the list of menus ordered according to a prioritisation (i.e. a ranking) of the menus on the basis of the overall confidence value w<sub>Mi </sub>of each one of them.</p><p id="p-0102" num="0127">In this regard, all the menus which, although detected by at least one of the neural networks <b>230</b>, <b>235</b> and <b>240</b> for detection of menus, are excluded from the list of ordered menus built by the builder module <b>225</b> (and/or the menus having a ranking assigned by the relative neural network for detection of menus the distance of which from the one finally assigned by the builder module <b>225</b> is higher than a maximum error threshold), produce a negative feedback on the relative neural network for detection of menus (that will have this Menu as an input negative instance), so as to improve the training of the neural network itself.</p><p id="p-0103" num="0128">In other words, both the construction of the list of the possible menus and the construction of the list of the possible non-menu contents of the web page are executed by the method according to the invention by carrying out at least one analysis selected from the group comprising a syntactic analysis, a semantic analysis and a morphological-visual analysis to determine a clustering (i.e. a grouping) of the elements of the web page in a plurality of homogeneous logical units (thereby menus and non-menu contents are grouped into distinct logical units) and the ranking (i.e. the order of importance) of such homogeneous logical units (thereby, for example, the main menu has a higher ranking than the other menus of the web page, just as a non-main menu content has a higher ranking than other non-menu contents of the web page).</p><p id="p-0104" num="0129">Consequently, the construction of the list of the possible menus gives as result one or more (if any) logical units of menus in each one of which one or more menus of equal importance are clustered and to each one of which a specific ranking is assigned in the set of said one or more (if any) logical units of menus; in this regard, it must noted that menus comprising a large number of items can be rearranged so as to have a lower number (optionally less than or equal to 10, more optionally less than or equal to 7) of primary items in at least some of which a plurality of original items of the menu are group, which are thus considered as belonging to sub-menus corresponding to the primary item.</p><p id="p-0105" num="0130">Similarly, the construction of the list of the possible non-menu contents gives as result one or more (if any) logical units of non-menu contents in each one of which one or more non-menu contents of the page of equal importance are clustered and to each one of which a specific ranking is assigned in the set of said one or more (if any) logical units of non-menu contents.</p><p id="p-0106" num="0131">Subsequently, the method delivers (<b>125</b>) the list of the homogeneous logical units of the menus and non-menu contents of the web page according to their ranking (optionally playing first the list of the menus and then that of the non-menu contents) through the TTS module, and it establishes a dialogue with the user through the TTS and ASR modules, allowing the user to select one of the logical units in the list (possibly even before that the list of homogeneous logical units is completely played).</p><p id="p-0107" num="0132">In the case where the user selects a logical unit of non-menu content comprising a single element (i.e. a single non-menu content) of the web page, the method plays that non-menu content (that in case of forms and/or boxes to be filled provides a dialogue for input and confirmation of the data to be input and/or selected); this also occurs in the case where the homogeneous logical units identified in the web page consist of a single non-menu content, that is therefore played. At the end of playing the non-menu content, the method plays a list of options, including repeating the playing of the non-menu content and returning to a previously visited web page.</p><p id="p-0108" num="0133">In the case where the user selects a logical unit of menu, the method plays the list of items in such menu; this also occurs in the case where the homogeneous logical units identified in the web page consist of a single (main) menu that is thus played. When the selection made by the user is a sub-menu, the dialogue with the user continues with the playing of the list of items in that sub-menu. When the user selects one of the items of the menu played that consists of a hyperlink to another web page, the method returns to execute starting from step <b>115</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0109" num="0134">To summarize what has been illustrated above by way of mere example, and not by way of limitation, the method according to the invention listens to the request for a resource to be browsed (e.g. a website) by the user, if necessary it makes a disambiguation of such request, searches and opens (i.e. downloads) the resource (e.g. the home page of the requested site), and on the elements of this it carries out at least one of a syntactic analysis, a semantic analysis and a morphological analysis. The analyses carried out are aimed at playing a voice menu for the user, following the theory of usability, with a maximum number of items (optionally variable from 7 to 10) to choose from, where these items represent all the salient contents of the web page under consideration, which include salient menus and salient non-menu contents. In other words, the analyses carried out concur and cooperate to perform a syntactic/semantic clustering and a prioritisation (i.e. a ranking) of the elements, i.e. menus and non-menu contents, of the web page under consideration.</p><p id="p-0110" num="0135">The results of each analysis are compared and/or combined with the results of the other analysis/analyses to produce a sort of voice meta-menu that best corresponds to the combination of the performed analyses.</p><p id="p-0111" num="0136">In a specific embodiment of the method according to the invention, the user can also make a voice search on a topic instead of a request for browsing a website. The method, through a voice dialogue through the TTS and ASR modules, leads the user with the necessary dialogue cycles to the web page or content/information sought.</p><p id="p-0112" num="0137">The method can be configured to operate independently from the user with whom it interacts, who thus acts in an anonymous/generic mode, or to operate taking into account the user's profile, wherein the method can record user characteristic data, for example establishing a dialogue to acquire such characteristic data (e.g., data relating to one or more interests of the user). Obviously, in the case where the user's profile is taken into account, the method permits optimization of browsing.</p><p id="p-0113" num="0138">Also, it must be noted that the ontology and the performed analyses may be function of the language of the web page, that is easily identified through known techniques.</p><p id="p-0114" num="0139">Advantageously, for the extraction of the elements and the browsing of a resource, the preferred embodiment of the method according to the invention implements a segmentation technique of the resource itself.</p><p id="p-0115" num="0140">With reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, it can be observed that the method according to the invention can be executed by a system comprising a processing device <b>400</b> (e.g., a computer, a smartphone or a tablet), equipped with a microphone <b>410</b> and audio playing electronic devices <b>420</b>, with which a user <b>500</b> vocally interacts, and one or more servers <b>450</b> advantageously in cloud to which the processing device <b>400</b> is able to connect. As shown in <figref idref="DRAWINGS">FIG. <b>4</b><i>a</i></figref>, the method can be executed entirely in cloud (with the exception of the voice interaction with the user <b>500</b> that is performed by the processing device <b>400</b> controlled by said one or more servers <b>450</b>). Alternatively, as shown in <figref idref="DRAWINGS">FIG. <b>4</b><i>b</i></figref>, the method can be executed partially in cloud and partially on the processing device <b>400</b>. Alternatively, as shown in <figref idref="DRAWINGS">FIG. <b>4</b><i>c</i></figref>, the method can be executed entirely by the processing device <b>400</b> (that may possibly connect to said one or more servers <b>450</b> to transmit data relating to the processing carried out or to receive software updates).</p><p id="p-0116" num="0141">In particular, in the case of <figref idref="DRAWINGS">FIGS. <b>4</b><i>a </i>and <b>4</b><i>b</i></figref>, the system can be based on an optimized management method of speech recognition and speech synthesis in cloud, such as the one disclosed in the Italian patent No. 102012902071971.</p><p id="p-0117" num="0142">The preferred embodiments of this invention have been described and a number of variations have been suggested hereinbefore, but it should be understood that those skilled in the art can make other variations and changes without so departing from the scope of protection thereof, as defined by the attached claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. Computer-implemented method of browsing a resource through voice interaction comprising the following steps:<claim-text>A. acquiring from a user a request aimed at browsing a resource;</claim-text><claim-text>B. downloading the requested resource;</claim-text><claim-text>C. performing a syntactic parsing of the downloaded resource;</claim-text><claim-text>D. extracting from the downloaded resource one or more lists, if any, of selectable shortcuts pointing to portions inside or outside the downloaded resource through at least one analysis of extraction of lists of selectable shortcuts of the downloaded resource selected from the group comprising:<claim-text>a syntactic analysis of extraction of lists of selectable shortcuts, wherein said syntactic analysis of extraction of lists of selectable shortcuts is performed through a neural network for syntactic detection of lists of selectable shortcuts,</claim-text><claim-text>a semantic analysis of extraction of lists of selectable shortcuts, wherein said semantic analysis of extraction of lists of selectable shortcuts is performed through a neural network for semantic detection of lists of selectable shortcuts, and</claim-text><claim-text>a morphological-visual analysis of extraction of lists of selectable shortcuts, wherein said morphological-visual analysis of extraction of lists of selectable shortcuts is performed through a neural network for morphological-visual detection of lists of selectable shortcuts,</claim-text></claim-text><claim-text>wherein said at least one analysis of extraction of lists of selectable shortcuts of the downloaded resource is performed on the basis of an ontology corresponding to the type of resource provided by a resource type detector module;</claim-text><claim-text>E. on the basis of the ontology corresponding to the type of resource provided by the resource type detector module, building a list comprising one or more of said one or more lists, if any, of selectable shortcuts extracted in step D from the downloaded resource, wherein said one or more lists of selectable shortcuts are ordered in the list according to a list prioritisation;</claim-text><claim-text>F. extracting from the downloaded resource one or more content elements, if any, different from lists of selectable shortcuts pointing to portions inside or outside the downloaded resource through at least one analysis of extraction of content elements of the downloaded resource selected from the group comprising:<claim-text>a syntactic analysis of extraction of content elements, wherein said syntactic analysis of extraction of content elements is performed through a neural network for syntactic detection of content elements,</claim-text><claim-text>a semantic analysis of extraction of content elements, wherein said semantic analysis of extraction of content elements is performed through a neural network for semantic detection of content elements, and</claim-text><claim-text>a morphological-visual analysis of extraction of content elements, wherein said morphological-visual analysis of extraction of content elements is performed through a neural network for morphological-visual detection of content elements,</claim-text></claim-text><claim-text>wherein said at least one analysis of content elements of the downloaded resource is performed on the basis of the ontology corresponding to the type of resource provided by the resource type detector module;</claim-text><claim-text>G. on the basis of the ontology corresponding to the type of resource provided by the resource type detector module, building a list comprising one or more of said one or more content elements, if any, extracted in step F from the downloaded resource, wherein said one or more content elements are ordered in the list according to a content element prioritisation;</claim-text><claim-text>H. on the basis of the list of one or more lists of selectable shortcuts built in step E and of the list of one or more content elements built in step G, and on the basis of the ontology corresponding to the type of resource provided by a resource type detector module, building a final structure of lists of selectable shortcuts and of content elements;</claim-text><claim-text>I. playing a voice prompt based on the final structure of lists of selectable shortcuts and of content elements and starting a voice interaction with the user for browsing the resource.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. Computer-implemented method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>said one or more lists extracted in step D are filtered through at least one analysis of filtering of lists of selectable shortcuts of the downloaded resource selected from the group comprising a syntactic analysis of filtering of lists of selectable shortcuts, a semantic analysis of filtering of lists of selectable shortcuts and a morphological-visual analysis of filtering of lists of selectable shortcuts, wherein said at least one analysis of filtering of lists of selectable shortcuts of the downloaded resource is performed on the basis of an ontology corresponding to the type of resource provided by the resource type detector module; and</claim-text><claim-text>said one or more content elements extracted in step F are filtered through at least one analysis of filtering of content elements of the downloaded resource selected from the group comprising a syntactic analysis of filtering of content elements, a semantic analysis of filtering of content elements and a morphological-visual analysis of filtering of content elements, wherein said at least one analysis of filtering of content elements of the downloaded resource is performed on the basis of an ontology corresponding to the type of resource provided by the resource type detector module.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. Computer-implemented method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>said one or more lists extracted in step D are filtered through at least one analysis of filtering of lists of selectable shortcuts of the downloaded resource selected from the group comprising:</claim-text><claim-text>a syntactic analysis of filtering of lists of selectable shortcuts, wherein said syntactic analysis of filtering of lists of selectable shortcuts is performed through a neural network for syntactic filtering of lists of selectable shortcuts,</claim-text><claim-text>a semantic analysis of filtering of lists of selectable shortcuts, wherein said semantic analysis of filtering of lists of selectable shortcuts is performed through a neural network for semantic filtering of lists of selectable shortcuts, and</claim-text><claim-text>a morphological-visual analysis of filtering of lists of selectable shortcuts, wherein said morphological-visual analysis of filtering of lists of selectable shortcuts is performed through a neural network for morphological-visual filtering of lists of selectable shortcuts,</claim-text></claim-text><claim-text>wherein said at least one analysis of filtering of lists of selectable shortcuts of the downloaded resource is performed on the basis of an ontology corresponding to the type of resource provided by the resource type detector module; and</claim-text><claim-text>said one or more content elements extracted in step F are filtered through at least one analysis of filtering of content elements of the downloaded resource selected from the group comprising:<claim-text>a syntactic analysis of filtering of content elements, wherein said syntactic analysis of filtering of content elements is performed through a neural network for syntactic filtering of content elements,</claim-text><claim-text>a semantic analysis of filtering of content elements, wherein said semantic analysis of filtering of content elements is performed through a neural network for semantic filtering of content elements, and</claim-text><claim-text>a morphological-visual analysis of filtering of content elements, wherein said morphological-visual analysis of filtering of content elements is performed through a neural network for morphological-visual filtering of content elements,</claim-text></claim-text><claim-text>wherein said at least one analysis of filtering of content elements of the downloaded resource is performed on the basis of an ontology corresponding to the type of resource provided by the resource type detector module.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. Computer-implemented method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>the resource type detector module receives in step C a first identification of the type of resource,</claim-text><claim-text>in step D, in the case where said at least one analysis of extraction of lists of selectable shortcuts of the downloaded resource recognises that said first identification is wrong, the resource type detector module receives a second identification of the type of resource, and execution of step D is repeated, wherein said at least one analysis of extraction of lists of selectable shortcuts of the downloaded resource is performed on the basis of the ontology corresponding to the second identification of the type of resource, and</claim-text><claim-text>in step F, in the case where the resource type detector module has not received a second identification of the type of resource and said at least one analysis of extraction of content elements of the downloaded resource recognises that said first identification is wrong, or in the case where the resource type detector module has received a second identification of the type of resource and said at least one analysis of extraction of content elements of the downloaded resource recognises that said second identification is wrong, the resource type detector module receives a third identification of the type of resource, and both execution of step D, wherein said at least one analysis of extraction of lists of selectable shortcuts of the downloaded resource is performed on the basis of the ontology corresponding to the third identification of the type of resource, and execution of step F, wherein said at least one analysis of extraction of content elements of the downloaded resource is performed on the basis of the ontology corresponding to the third identification of the type of resource.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. Computer-implemented method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein, in the case where in step C a main list of selectable shortcuts is identified on the basis of a syntactic self-declaration, in step E the list comprising one or more lists di selectable shortcuts is built including said main list of selectable shortcuts identified in step C as main list of selectable shortcuts.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. Computer-implemented method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein, in step B, it is checked whether the downloaded resource has been browsed in a time interval preceding the request acquired in step A and:<claim-text>if yes, the method loads the final structure of lists of selectable shortcuts and of content elements previously built for said downloaded resource and skips to step I, otherwise</claim-text><claim-text>if not, the method proceeds to step C.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. Computer-implemented method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein immediately after step B spurious elements, if any, are eliminated from the resource and wherein between step C and step D multimedia elements, if any, are managed by a Multimedia_Detector/Manager module.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. Computer-implemented method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein in step A the request aimed at browsing a resource is analysed and, if the method recognises that it needs additional information, the method institutes a dialogue with the user to obtain said additional information.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. Computer-implemented method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein step I is performed taking account of information stored in a user profile, optionally for a user voice recognition.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. Computer-implemented method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein between step A and step B a search of the requested resource is performed.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. Computer-implemented method according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the search of the requested resource is stored in a user profile.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. Computer-implemented method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the resource is selected from the group comprising:<claim-text>a web page, thereby the lists of selectable shortcuts pointing to portions inside or outside the downloaded resource consist of menus of the web page and the content elements are non-menu contents of the web page;</claim-text><claim-text>a document, thereby the lists of selectable shortcuts pointing to portions inside or outside the downloaded resource consist of indexes or summaries of the document; and</claim-text><claim-text>an app.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. System comprising a processing device, provided with microphone and at least one electronic audio playing device, and one or more servers, optionally one or more cloud servers, with which the processing device is configured to connect, wherein the processing device and/or said one or more servers are configured to perform a computer-implemented method of browsing a resource through voice interaction comprising the following steps:<claim-text>A. acquiring from a user a request aimed at browsing a resource;</claim-text><claim-text>B. downloading the requested resource;</claim-text><claim-text>C. performing a syntactic parsing of the downloaded resource;</claim-text><claim-text>D. extracting from the downloaded resource one or more lists, if any, of selectable shortcuts pointing to portions inside or outside the downloaded resource through at least one analysis of extraction of lists of selectable shortcuts of the downloaded resource selected from the group comprising:<claim-text>a syntactic analysis of extraction of lists of selectable shortcuts, wherein said syntactic analysis of extraction of lists of selectable shortcuts is performed through a neural network for syntactic detection of lists of selectable shortcuts,</claim-text><claim-text>a semantic analysis of extraction of lists of selectable shortcuts, wherein said semantic analysis of extraction of lists of selectable shortcuts is performed through a neural network for semantic detection of lists of selectable shortcuts, and</claim-text><claim-text>a morphological-visual analysis of extraction of lists of selectable shortcuts, wherein said morphological-visual analysis of extraction of lists of selectable shortcuts is performed through a neural network for morphological-visual detection of lists of selectable shortcuts,</claim-text></claim-text><claim-text>wherein said at least one analysis of extraction of lists of selectable shortcuts of the downloaded resource is performed on the basis of an ontology corresponding to the type of resource provided by a resource type detector module;</claim-text><claim-text>E. on the basis of the ontology corresponding to the type of resource provided by the resource type detector module, building a list comprising one or more of said one or more lists, if any, of selectable shortcuts extracted in step D from the downloaded resource, wherein said one or more lists of selectable shortcuts are ordered in the list according to a list prioritisation;</claim-text><claim-text>F. extracting from the downloaded resource one or more content elements, if any, different from lists of selectable shortcuts pointing to portions inside or outside the downloaded resource through at least one analysis of extraction of content elements of the downloaded resource selected from the group comprising:<claim-text>a syntactic analysis of extraction of content elements, wherein said syntactic analysis of extraction of content elements is performed through a neural network for syntactic detection of content elements,</claim-text><claim-text>a semantic analysis of extraction of content elements, wherein said semantic analysis of extraction of content elements is performed through a neural network for semantic detection of content elements, and</claim-text><claim-text>a morphological-visual analysis of extraction of content elements, wherein said morphological-visual analysis of extraction of content elements is performed through a neural network for morphological-visual detection of content elements,</claim-text></claim-text><claim-text>wherein said at least one analysis of content elements of the downloaded resource is performed on the basis of the ontology corresponding to the type of resource provided by the resource type detector module;</claim-text><claim-text>G. on the basis of the ontology corresponding to the type of resource provided by the resource type detector module, building a list comprising one or more of said one or more content elements, if any, extracted in step F from the downloaded resource, wherein said one or more content elements are ordered in the list according to a content element prioritisation;</claim-text><claim-text>H. on the basis of the list of one or more lists of selectable shortcuts built in step E and of the list of one or more content elements built in step G, and on the basis of the ontology corresponding to the type of resource provided by a resource type detector module, building a final structure of lists of selectable shortcuts and of content elements;</claim-text><claim-text>I. playing a voice prompt based on the final structure of lists of selectable shortcuts and of content elements and starting a voice interaction with the user for browsing the resource.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. (canceled)</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. Set of one or more computer-readable storage media having stored thereon a set of one or more computer programs comprising instructions which, when executed by one or more processing units, cause said one or more processing units to carry out the computer-implemented method of browsing a resource through voice interaction comprising the following steps:<claim-text>A. acquiring from a user a request aimed at browsing a resource;</claim-text><claim-text>B. downloading the requested resource;</claim-text><claim-text>C. performing a syntactic parsing of the downloaded resource;</claim-text><claim-text>D. extracting from the downloaded resource one or more lists, if any, of selectable shortcuts pointing to portions inside or outside the downloaded resource through at least one analysis of extraction of lists of selectable shortcuts of the downloaded resource selected from the group comprising:<claim-text>a syntactic analysis of extraction of lists of selectable shortcuts, wherein said syntactic analysis of extraction of lists of selectable shortcuts is performed through a neural network for syntactic detection of lists of selectable shortcuts,</claim-text><claim-text>a semantic analysis of extraction of lists of selectable shortcuts, wherein said semantic analysis of extraction of lists of selectable shortcuts is performed through a neural network for semantic detection of lists of selectable shortcuts, and</claim-text><claim-text>a morphological-visual analysis of extraction of lists of selectable shortcuts, wherein said morphological-visual analysis of extraction of lists of selectable shortcuts is performed through a neural network for morphological-visual detection of lists of selectable shortcuts,</claim-text></claim-text><claim-text>wherein said at least one analysis of extraction of lists of selectable shortcuts of the downloaded resource is performed on the basis of an ontology corresponding to the type of resource provided by a resource type detector module;</claim-text><claim-text>E. on the basis of the ontology corresponding to the type of resource provided by the resource type detector module, building a list comprising one or more of said one or more lists, if any, of selectable shortcuts extracted in step D from the downloaded resource, wherein said one or more lists of selectable shortcuts are ordered in the list according to a list prioritisation;</claim-text><claim-text>F. extracting from the downloaded resource one or more content elements, if any, different from lists of selectable shortcuts pointing to portions inside or outside the downloaded resource through at least one analysis of extraction of content elements of the downloaded resource selected from the group comprising:<claim-text>a syntactic analysis of extraction of content elements, wherein said syntactic analysis of extraction of content elements is performed through a neural network for syntactic detection of content elements,</claim-text><claim-text>a semantic analysis of extraction of content elements, wherein said semantic analysis of extraction of content elements is performed through a neural network for semantic detection of content elements, and</claim-text><claim-text>a morphological-visual analysis of extraction of content elements, wherein said morphological-visual analysis of extraction of content elements is performed through a neural network for morphological-visual detection of content elements,</claim-text></claim-text><claim-text>wherein said at least one analysis of content elements of the downloaded resource is performed on the basis of the ontology corresponding to the type of resource provided by the resource type detector module;</claim-text><claim-text>G. on the basis of the ontology corresponding to the type of resource provided by the resource type detector module, building a list comprising one or more of said one or more content elements, if any, extracted in step F from the downloaded resource, wherein said one or more content elements are ordered in the list according to a content element prioritisation;</claim-text><claim-text>H. on the basis of the list of one or more lists of selectable shortcuts built in step E and of the list of one or more content elements built in step G, and on the basis of the ontology corresponding to the type of resource provided by a resource type detector module, building a final structure of lists of selectable shortcuts and of content elements;</claim-text><claim-text>I. playing a voice prompt based on the final structure of lists of selectable shortcuts and of content elements and starting a voice interaction with the user for browsing the resource.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. Computer-implemented method according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein said information stored in a user profile are information for a user voice recognition.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. System according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein said one or more servers include one or more cloud servers.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. System according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein:<claim-text>said one or more lists extracted in step D are filtered through at least one analysis of filtering of lists of selectable shortcuts of the downloaded resource selected from the group comprising a syntactic analysis of filtering of lists of selectable shortcuts, a semantic analysis of filtering of lists of selectable shortcuts and a morphological-visual analysis of filtering of lists of selectable shortcuts, wherein said at least one analysis of filtering of lists of selectable shortcuts of the downloaded resource is performed on the basis of an ontology corresponding to the type of resource provided by the resource type detector module; and</claim-text><claim-text>said one or more content elements extracted in step F are filtered through at least one analysis of filtering of content elements of the downloaded resource selected from the group comprising a syntactic analysis of filtering of content elements, a semantic analysis of filtering of content elements and a morphological-visual analysis of filtering of content elements, wherein said at least one analysis of filtering of content elements of the downloaded resource is performed on the basis of an ontology corresponding to the type of resource provided by the resource type detector module.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. System according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein:<claim-text>said one or more lists extracted in step D are filtered through at least one analysis of filtering of lists of selectable shortcuts of the downloaded resource selected from the group comprising:<claim-text>a syntactic analysis of filtering of lists of selectable shortcuts, wherein said syntactic analysis of filtering of lists of selectable shortcuts is performed through a neural network for syntactic filtering of lists of selectable shortcuts,</claim-text><claim-text>a semantic analysis of filtering of lists of selectable shortcuts, wherein said semantic analysis of filtering of lists of selectable shortcuts is performed through a neural network for semantic filtering of lists of selectable shortcuts, and</claim-text><claim-text>a morphological-visual analysis of filtering of lists of selectable shortcuts, wherein said morphological-visual analysis of filtering of lists of selectable shortcuts is performed through a neural network for morphological-visual filtering of lists of selectable shortcuts,</claim-text></claim-text><claim-text>wherein said at least one analysis of filtering of lists of selectable shortcuts of the downloaded resource is performed on the basis of an ontology corresponding to the type of resource provided by the resource type detector module; and</claim-text><claim-text>said one or more content elements extracted in step F are filtered through at least one analysis of filtering of content elements of the downloaded resource selected from the group comprising:<claim-text>a syntactic analysis of filtering of content elements, wherein said syntactic analysis of filtering of content elements is performed through a neural network for syntactic filtering of content elements,</claim-text><claim-text>a semantic analysis of filtering of content elements, wherein said semantic analysis of filtering of content elements is performed through a neural network for semantic filtering of content elements, and</claim-text><claim-text>a morphological-visual analysis of filtering of content elements, wherein said morphological-visual analysis of filtering of content elements is performed through a neural network for morphological-visual filtering of content elements,</claim-text></claim-text><claim-text>wherein said at least one analysis of filtering of content elements of the downloaded resource is performed on the basis of an ontology corresponding to the type of resource provided by the resource type detector module.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. Set of one or more computer-readable storage media according to <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein:<claim-text>said one or more lists extracted in step D are filtered through at least one analysis of filtering of lists of selectable shortcuts of the downloaded resource selected from the group comprising a syntactic analysis of filtering of lists of selectable shortcuts, a semantic analysis of filtering of lists of selectable shortcuts and a morphological-visual analysis of filtering of lists of selectable shortcuts, wherein said at least one analysis of filtering of lists of selectable shortcuts of the downloaded resource is performed on the basis of an ontology corresponding to the type of resource provided by the resource type detector module; and</claim-text><claim-text>said one or more content elements extracted in step F are filtered through at least one analysis of filtering of content elements of the downloaded resource selected from the group comprising a syntactic analysis of filtering of content elements, a semantic analysis of filtering of content elements and a morphological-visual analysis of filtering of content elements, wherein said at least one analysis of filtering of content elements of the downloaded resource is performed on the basis of an ontology corresponding to the type of resource provided by the resource type detector module.</claim-text></claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. Set of one or more computer-readable storage media according to <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein:<claim-text>said one or more lists extracted in step D are filtered through at least one analysis of filtering of lists of selectable shortcuts of the downloaded resource selected from the group comprising:<claim-text>a syntactic analysis of filtering of lists of selectable shortcuts, wherein said syntactic analysis of filtering of lists of selectable shortcuts is performed through a neural network for syntactic filtering of lists of selectable shortcuts,</claim-text><claim-text>a semantic analysis of filtering of lists of selectable shortcuts, wherein said semantic analysis of filtering of lists of selectable shortcuts is performed through a neural network for semantic filtering of lists of selectable shortcuts, and</claim-text><claim-text>a morphological-visual analysis of filtering of lists of selectable shortcuts, wherein said morphological-visual analysis of filtering of lists of selectable shortcuts is performed through a neural network for morphological-visual filtering of lists of selectable shortcuts,</claim-text></claim-text><claim-text>wherein said at least one analysis of filtering of lists of selectable shortcuts of the downloaded resource is performed on the basis of an ontology corresponding to the type of resource provided by the resource type detector module; and</claim-text><claim-text>said one or more content elements extracted in step F are filtered through at least one analysis of filtering of content elements of the downloaded resource selected from the group comprising:<claim-text>a syntactic analysis of filtering of content elements, wherein said syntactic analysis of filtering of content elements is performed through a neural network for syntactic filtering of content elements,</claim-text><claim-text>a semantic analysis of filtering of content elements, wherein said semantic analysis of filtering of content elements is performed through a neural network for semantic filtering of content elements, and</claim-text><claim-text>a morphological-visual analysis of filtering of content elements, wherein said morphological-visual analysis of filtering of content elements is performed through a neural network for morphological-visual filtering of content elements,</claim-text></claim-text><claim-text>wherein said at least one analysis of filtering of content elements of the downloaded resource is performed on the basis of an ontology corresponding to the type of resource provided by the resource type detector module.</claim-text></claim-text></claim></claims></us-patent-application>