<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004204A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004204</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17899523</doc-number><date>20220830</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>1</main-group><subgroup>329</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>1</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>02</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>20</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>1</main-group><subgroup>329</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>1</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>02</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20190101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>20</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">NEURAL NETWORK POWER MANAGEMENT IN A MULTI-GPU SYSTEM</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16116286</doc-number><date>20180829</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11435813</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17899523</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Advanced Micro Devices, Inc.</orgname><address><city>Santa Clara</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Sadowski</last-name><first-name>Greg</first-name><address><city>Boxborough</city><state>MA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Systems, apparatuses, and methods for managing power consumption for a neural network implemented on multiple graphics processing units (GPUs) are disclosed. A computing system includes a plurality of GPUs implementing a neural network. In one implementation, the plurality of GPUs draw power from a common power supply. To prevent the power consumption of the system from exceeding a power limit for long durations, the GPUs coordinate the scheduling of tasks of the neural network. At least one or more first GPUs schedule their computation tasks so as not to overlap with the computation tasks of one or more second GPUs. In this way, the system spends less time consuming power in excess of a power limit, allowing the neural network to be implemented in a more power efficient manner.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="234.95mm" wi="106.00mm" file="US20230004204A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="169.93mm" wi="158.83mm" orientation="landscape" file="US20230004204A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="191.94mm" wi="140.63mm" orientation="landscape" file="US20230004204A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="218.27mm" wi="137.84mm" orientation="landscape" file="US20230004204A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="203.96mm" wi="137.33mm" orientation="landscape" file="US20230004204A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="253.83mm" wi="108.29mm" file="US20230004204A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="253.49mm" wi="124.80mm" file="US20230004204A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="246.89mm" wi="95.59mm" file="US20230004204A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="247.14mm" wi="95.67mm" file="US20230004204A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="247.23mm" wi="110.49mm" file="US20230004204A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="225.21mm" wi="144.10mm" file="US20230004204A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application is a continuation of U.S. patent application Ser. No. 16/116,286, now U.S. Pat. No. 11,435,813, entitled &#x201c;NEURAL NETWORK POWER MANAGEMENT IN A MULTI-GPU SYSTEM&#x201d;, filed Aug. 29, 2018, the entirety of which is incorporated herein by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND</heading><heading id="h-0003" level="1">Description of the Related Art</heading><p id="p-0003" num="0002">An emerging technology field is machine learning, with a neural network being one type of a machine learning model. Neural networks are used in a wide variety of applications (e.g., hand-written digit classification, face detection). However, neural networks often use significant amounts of processing resources that consume a large amount of power. For example, some systems implement neural networks using multiple graphics processing units (GPUs) (e.g. GPUs placed on the same card, GPUs located in the same server). These multi-GPU systems often have common power supplies with a fixed power limit. When multiple GPUs work together to implement a neural network, the performance of the system is limited by the total power that all GPUs have to share.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0004" num="0003">The advantages of the methods and mechanisms described herein may be better understood by referring to the following description in conjunction with the accompanying drawings, in which:</p><p id="p-0005" num="0004"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram of one implementation of a computing system.</p><p id="p-0006" num="0005"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram of another implementation of a computing system.</p><p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates a plot of power consumption over time by a plurality of GPUs implementing a neural network in accordance with one implementation.</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates a plot of a plurality of GPUs implementing a phase misalignment scheme in accordance with one implementation.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a generalized flow diagram illustrating one implementation of a method for scheduling execution of tasks to minimize execution time for a multi-GPU system with a common power supply.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a generalized flow diagram illustrating one implementation of a method for determining an alignment of different phases of neural network processing.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a generalized flow diagram illustrating one implementation of a method for implementing a phase misalignment scheme.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a generalized flow diagram illustrating one implementation of a method for adjusting an execution starting time of a task.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a generalized flow diagram illustrating one implementation of a method for detecting patterns in portions of a computing task being executed by a plurality of processing units.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a block diagram of another implementation of a computing system.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION OF IMPLEMENTATIONS</heading><p id="p-0015" num="0014">In the following description, numerous specific details are set forth to provide a thorough understanding of the methods and mechanisms presented herein. However, one having ordinary skill in the art should recognize that the various implementations may be practiced without these specific details. In some instances, well-known structures, components, signals, computer program instructions, and techniques have not been shown in detail to avoid obscuring the approaches described herein. It will be appreciated that for simplicity and clarity of illustration, elements shown in the figures have not necessarily been drawn to scale. For example, the dimensions of some of the elements may be exaggerated relative to other elements.</p><p id="p-0016" num="0015">Various systems, apparatuses, and methods for implementing a neural network on a multi-GPU system are disclosed herein. In one implementation, a computing system includes at least a plurality of processing units (e.g., GPUs) and a common power supply shared by the plurality of processing units. In one implementation, the computing system implements a neural network on the plurality of processing units. The plurality of processing units share information regarding power consumption and task execution phases with each other. In one implementation, the plurality of processing units are arranged together in a ring topology. In other implementations, the plurality of processing units are connected in other arrangements.</p><p id="p-0017" num="0016">In one implementation, the plurality of processing units monitor the amount of time that total power consumption exceeds or is equal to a power limit for the common power supply. The plurality of processing units also monitor the task execution phases and the alignment of these phases among the plurality of processing units. If task execution phases are aligned among a threshold number of processing units, and if this alignment is causing total power consumption to exceed the power limit of the common power supply, then the plurality of processing units initiate a change in the alignment of task execution phases (e.g., change the scheduling of execution of tasks and/or task phases to reduce overlap with tasks being executed by other processing units). In one implementation, at least one processing unit delays the start of execution of a given task phase. By delaying the start of execution of the given task phase, the at least one processing unit spreads out the power consumption over a previously unused interval which can help reduce the time the power supply is at its power limit. Additionally, delaying the start of execution of the given phase can result in a more efficient utilization of the available power.</p><p id="p-0018" num="0017">Referring now to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, a block diagram of one implementation of a computing system <b>100</b> is shown. In one implementation, computing system <b>100</b> includes at least processors <b>105</b>A-N, control unit <b>110</b>, input/output (I/O) interfaces <b>120</b>, bus <b>125</b>, memory controller(s) <b>130</b>, network interface <b>135</b>, memory device(s) <b>140</b>, power supply <b>145</b>, and power management unit <b>150</b>. In other implementations, computing system <b>100</b> includes other components and/or computing system <b>100</b> is arranged differently. Processors <b>105</b>A-N are representative of any number of processors which are included in system <b>100</b>.</p><p id="p-0019" num="0018">In one implementation, processor <b>105</b>A is a general purpose processor, such as a central processing unit (CPU). In one implementation, processor <b>105</b>N is a data parallel processor with a highly parallel architecture. Data parallel processors include graphics processing units (GPUs), digital signal processors (DSPs), field programmable gate arrays (FPGAs), application specific integrated circuits (ASICs), and so forth. In some implementations, processors <b>105</b>A-N include multiple data parallel processors. In one implementation, processors <b>105</b>A-N include a plurality of GPUs which are implementing a neural network while drawing power from a common power supply <b>145</b>. In various implementations, the plurality of GPUs are included on a single circuit card, are located on multiple circuit cards within a common enclosure, or otherwise. In these implementations, power supply <b>145</b> is limited in the amount of power it can deliver to the plurality of GPUs. To ensure optimal performance while operating under a given power limit, the plurality of GPUs communicate with each other and stagger the alignment of computation phases of the neural network layers being implemented. These techniques as well as other techniques for implementing a neural network while meeting strict power requirements and at the same time ensuring adequate performance are described in the remainder of the disclosure. Other techniques for meeting other goals and/or requirements are also described.</p><p id="p-0020" num="0019">Memory controller(s) <b>130</b> are representative of any number and type of memory controllers accessible by processors <b>105</b>A-N. Memory controller(s) <b>130</b> are coupled to any number and type of memory devices(s) <b>140</b>. Memory device(s) <b>140</b> are representative of any number and type of memory devices. For example, the type of memory in memory device(s) <b>140</b> includes Dynamic Random Access Memory (DRAM), Static Random Access Memory (SRAM), NAND Flash memory, NOR flash memory, Ferroelectric Random Access Memory (FeRAM), or others.</p><p id="p-0021" num="0020">I/O interfaces <b>120</b> are representative of any number and type of I/O interfaces (e.g., peripheral component interconnect (PCI) bus, PCI-Extended (PCI-X), PCIE (PCI Express) bus, gigabit Ethernet (GBE) bus, universal serial bus (USB)). Various types of peripheral devices (not shown) are coupled to I/O interfaces <b>120</b>. Such peripheral devices include (but are not limited to) displays, keyboards, mice, printers, scanners, media recording devices, external storage devices, network interface cards, and so forth. Network interface <b>135</b> is used to receive and send network messages across a network. Bus <b>125</b> is representative of any type of bus or fabric with any number of links for connecting together the different components of system <b>100</b>.</p><p id="p-0022" num="0021">In one implementation, power management unit <b>150</b> monitors and/or controls various power-performance states of components within system <b>100</b>. Responsive to detecting various events, the power management unit <b>150</b> causes other components within system <b>100</b> to either increase or decrease their current power-performance state. In various implementations, changing a power-performance state includes changing a current operating frequency of a device and/or changing a current voltage level of a device. In one implementation, if a power limit for power supply <b>145</b> is reached and/or exceeded, power management unit <b>150</b> reduces the power-performance states of processors <b>105</b>A-N. When the power-performance states of processors <b>105</b>A-N are reduced, this causes the computing tasks being executed by processors <b>105</b>A-N to take longer to complete. Also, in some cases, processors <b>105</b>A-N are in phase such that they are drawing peak power from power supply <b>145</b> at the same time while also drawing minimal power from power supply <b>145</b> at the same time. This alignment of phases by processors <b>105</b>A-N results in an inefficient use of power supply <b>145</b>.</p><p id="p-0023" num="0022">In one implementation, the power limit for power supply <b>145</b> is exceeded when a plurality of processors <b>105</b>A-N are implementing a neural network and when computation tasks performed by processors <b>105</b>A-N are aligned in sync with each other. In one implementation, to prevent the power limit for power supply <b>145</b> from being exceeded, or to minimize the amount of time that the power limit for power supply <b>145</b> is exceeded, one or more of processors <b>105</b>A-N delay the start of execution of their computation tasks. This causes a misalignment of phases in the work being executed by processors <b>105</b>A-N and reduces the peak power consumption of system <b>100</b>. The misalignment of phases also decreases the amount of time processors <b>105</b>A-N are required to operate in a reduced power-performance state. As a result, processors <b>105</b>A-N are able to complete their tasks in a faster, more efficient manner. In various implementations, processors <b>105</b>A-N and/or control unit <b>110</b> initiate the misalignment of phases. Control unit <b>110</b> is implemented using any suitable combination of hardware and/or software. In one implementation, control unit <b>110</b> is implemented as software executing on one or more of processors <b>105</b>A-N.</p><p id="p-0024" num="0023">In various implementations, computing system <b>100</b> is a computer, laptop, mobile device, server, or any of various other types of computing systems or devices. It is noted that the number of components of computing system <b>100</b> varies from implementation to implementation. For example, in other implementations, there are more or fewer of each component than the number shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. It is also noted that in other implementations, computing system <b>100</b> includes other components not shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref> and/or one or more of the components shown in computing system <b>100</b> are omitted. Additionally, in other implementations, computing system <b>100</b> is structured in other ways than shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0025" num="0024">Turning now to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, a block diagram of another implementation of a computing system <b>200</b> is shown. In one implementation, computing system <b>200</b> includes a plurality of GPUs <b>205</b>, <b>210</b>, and <b>215</b> connected together in a ring topology. GPUs <b>205</b>, <b>210</b>, and <b>215</b> are representative of any number of GPUs that are included in system <b>200</b>, with the number varying from implementation to implementation. Additionally, in other implementations, system <b>200</b> includes other types of processing units, such as FPGAs, ASICs, DSPs, or any combination thereof, arranged in a ring topology. GPUs <b>205</b>, <b>210</b>, and <b>215</b> use the ring topology to share information with each other about the global state, power consumption data, task starting times, task durations, and/or other metrics. In other implementations, GPUs <b>205</b>, <b>210</b>, and <b>215</b> are connected together using any of various other suitable topologies.</p><p id="p-0026" num="0025">In one implementation, GPUs <b>205</b>, <b>210</b>, and <b>215</b> work together to implement a distributed neural network. In various implementations, GPUs <b>205</b>, <b>210</b>, and <b>215</b> send information about the initiation of layers of a neural network. For example, when GPU <b>205</b> starts processing a first layer of the neural network, GPU <b>205</b> sends an indication of this to GPU <b>210</b>, which passes the indication on to the next GPU, and so on. GPUs <b>210</b> and <b>215</b> also send information about their status in regard to the initiation of the various layers of the neural network. In some implementations, GPUs <b>205</b>, <b>210</b>, and <b>215</b> also specify the type of propagation (e.g., forward propagation, back propagation) that is being implemented as well as additional information associated with the neural network.</p><p id="p-0027" num="0026">In some implementations, GPU <b>205</b> sends an indication of which type of phase is being performed. For example, when GPU <b>205</b> is fetching data associated with the first layer, GPU <b>205</b> sends an indication that a memory access phase is being performed. When GPU <b>205</b> is processing the fetched data, GPU <b>205</b> sends an indication that a compute phase is being performed. The other GPUs can also do likewise. Additionally, GPUs <b>205</b>, <b>210</b>, and <b>215</b> share information about their individual power consumption. For example, GPU <b>205</b> sends data specifying its latest power consumption status, GPU <b>210</b> sends data specifying its latest power consumption status, and so on. GPUs <b>205</b>, <b>210</b>, and <b>215</b> use this information to determine when a given power limit is being exceeded. When the given power limit is exceeded, this will cause the power supplied to each GPU of GPUs <b>205</b>, <b>210</b>, and <b>215</b> to be throttled. This will cause a slowdown in the implementation of the neural network. However, the GPUs <b>205</b>, <b>210</b>, and <b>215</b> can take corrective action to prevent or reduce the amount of time that the power limit is exceeded.</p><p id="p-0028" num="0027">In one implementation, if GPUs <b>205</b>, <b>210</b>, and <b>215</b> collectively determine that the given power limit will be exceeded for a given layer of the neural network, GPUs <b>205</b>, <b>210</b>, and <b>215</b> respond by staggering the alignment of the various phases of the given layer. For example, in one implementation, in response to determining that the given power limit has been exceeded or predicting that the given power limit will be exceeded, GPUs <b>205</b>, <b>210</b>, and <b>215</b> are subdivided into first and second groups. The first group of GPUs performs the phases of the layer in the normal fashion. The second group of GPUs delay the start of the computation phase so that it does not align with the computation phase performed by the first group of GPUs. By causing the computation phase of the first and second groups of GPUs to be misaligned, the power limit will be exceeded for a lower percentage of time. This allows the layers of the neural network to be processed in less time than if the computation phases of all GPUs <b>205</b>, <b>210</b>, and <b>215</b> were aligned.</p><p id="p-0029" num="0028">In one implementation, each GPU <b>205</b>, <b>210</b>, and <b>215</b> includes a corresponding register <b>220</b>A-C which stores the number of layers of the neural network being implemented. In this implementation, once a power consumption pattern is detected for the neural network, GPUs <b>205</b>, <b>210</b>, and <b>215</b> check the value stored in registers <b>220</b>A-C, respectively, to determine the number of layers remaining in the neural network during which this power consumption pattern will continue. GPUs <b>205</b>, <b>210</b>, and <b>215</b> are then able to stagger the alignment of phases for the correct number of remaining layers of the neural network. In one implementation, registers <b>220</b>A-C are programmed during the initiation of execution of the neural network. In other implementations, the total number of layers of the neural network is stored in other locations which are accessibly by GPUs <b>205</b>, <b>210</b>, and <b>215</b>.</p><p id="p-0030" num="0029">Referring now to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, a plot of one implementation of power consumption by a plurality of GPUs implementing a neural network is shown. Individual GPU power consumption waveforms <b>305</b> are shown at the top of <figref idref="DRAWINGS">FIG. <b>3</b></figref> for GPUs <b>310</b>A-C which are executing the phases of a computing task in parallel. In one implementation, the computing task is the implementation of a neural network. In other implementations, the computing task is any of various other types of workloads. GPUs <b>310</b>A-C are representative of any number of GPUs which are operating together as part of a computing system and drawing power from a common power supply. Total GPU power consumption waveform <b>325</b> represents the total power draw of all GPUs <b>310</b>A-C from the card, server, or other configuration to which they belong.</p><p id="p-0031" num="0030">As can be seen from the individual GPU power consumption waveforms <b>305</b> for GPUs <b>310</b>A-C, the power draw pattern of each GPU <b>310</b>A-C is aligned with the other GPUs. In other words, the GPUs <b>310</b>A-C are operating in synchronization with each other. In one implementation, each GPU <b>310</b>A-C initiates a given layer of the neural network at the same time, resulting in GPUs <b>310</b>A-C reaching peak power consumption with the same time pattern. In various embodiments, each of the GPUs are executing the same algorithm. In such an embodiment, each of the GPUs will generally have a similar power consumption profile during performance of the algorithm. If the total system power limit <b>320</b> is not reached when GPUs <b>310</b>A-C are operating in sync with each other, then GPUs <b>310</b>A-C will not be negatively affected by the alignment of their task phases. However, in most systems, operating multiple GPUs <b>310</b>A-C in parallel for the layers of a neural network will result in system power limit <b>320</b> being reached on a regular basis. When system power limit <b>320</b> is reached, GPUs <b>310</b>A-C will have to slow down to reduce power consumption, resulting in a longer execution time for each layer of the neural network. This is shown in total GPU power consumption waveform <b>325</b> as the peaks start to spread out as GPUs <b>310</b>A-C are prevented from operating at a highest power-performance state due to the system exceeding power limit <b>320</b>. Also, when GPUs <b>310</b>A-C use less power, they do so in sync with each other, which is far below power limit <b>320</b>, resulting in inefficient use of the available power.</p><p id="p-0032" num="0031">Turning now to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, a plot of one implementation of GPUs executing a misalignment scheme is shown. In one implementation, the misalignment scheme illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref> involves GPUs <b>410</b>A-C communicating regarding the power draw situations. Also, GPUs <b>410</b>A-C learn the patterns in power consumption over time so that the patterns can be exploited for energy optimization. GPUs <b>410</b>A-C exploit the deterministic nature of a neural network implementation to accurately predict and estimate the power consumption for subsequent layers based on the detected power consumption pattern. Once the power consumption pattern is determined, GPUs <b>410</b>A-C implement the misalignment scheme to more efficiently use the power provided by a common power supply.</p><p id="p-0033" num="0032">To implement the misalignment scheme, one or more GPUs <b>410</b>A-C delay execution when the total group of GPUs <b>410</b>A-C are predicted to draw more power than the system can deliver. Using this scheme, the overall execution time is minimized as more of the available power is consumed during the time intervals that otherwise would be under-utilized. Generally speaking, as a result of implementing the misalignment scheme, when some of the GPUs are drawing less power, other GPUs are drawing more power, allowing the available power to be used efficiently on a more consistent basis. This is achieved by causing the execution phase of at least on GPU to be misaligned with respect to the execution phases of the other GPUs working as part of the same neural network. This allows the overall execution of the neural network to be completed more quickly as compared with using the traditional approach.</p><p id="p-0034" num="0033">Individual GPU power consumption waveforms <b>405</b> are shown for GPUs <b>410</b>A-C at the top of <figref idref="DRAWINGS">FIG. <b>4</b></figref>. Rather than having all GPUs <b>410</b>A-C operating in alignment with each other, GPU <b>410</b>C delays the start of execution by delay amount <b>415</b>. By postponing the start of execution by delay amount <b>415</b>, GPU <b>410</b>C ensures that its peak power draw occurs when GPUs <b>410</b>A-B are drawing the least amount of power. In other implementations, other numbers of GPUs other than one GPU will delay the start of execution of a given layer of a neural network to create a misalignment in computation phases among the plurality of GPUs. In one implementation, GPU <b>410</b>C makes the decision to delay its execution by amount <b>415</b> based on the power consumption data and execution start time data received from the other GPUs <b>410</b>A-B. In another implementation, a single GPU is designated as the master GPU, and the master GPU (e.g., GPU <b>410</b>A) sends a command to GPU <b>410</b>C to delay its execution by amount <b>415</b>. In another implementation, control logic or a control unit (e.g., control unit <b>110</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>) performs an analysis and makes the decision for at least one GPU to delay its execution based on data received from the plurality of GPUs. In this implementation, based on the analysis, the control unit sends a command to GPU <b>410</b>C to delay its execution by amount <b>415</b>.</p><p id="p-0035" num="0034">The pattern of peak power consumption of GPUs <b>410</b>A-B aligning with the lowest power consumption of GPU <b>410</b>C and peak power consumption of GPU <b>410</b>C aligning with the lowest power consumption of GPUs <b>410</b>A-B continues for the remainder of the processing of the neural network layers. This approach results in a more efficient utilization of the total power available to the system. The total GPU power consumption waveform <b>425</b> at the bottom of <figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates how the misalignment approach better utilizes the available power as compared to the implementation illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>. The system power limit <b>420</b> is reached for shorter periods of time in waveform <b>425</b> as compared to waveform <b>325</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, and the power consumption does not dip as much for waveform <b>425</b> in between peaks as compared to waveform <b>325</b>. As a result, execution of the neural network for the misalignment scheme shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref> is able to complete faster as compared to execution of the neural network for the scheme shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0036" num="0035">Referring now to <figref idref="DRAWINGS">FIG. <b>5</b></figref>, one implementation of a method <b>500</b> for scheduling execution of tasks to minimize execution time for a multi-GPU system with a common power supply is shown. For purposes of discussion, the steps in this implementation and those of <figref idref="DRAWINGS">FIG. <b>6</b>-<b>9</b></figref> are shown in sequential order. However, it is noted that in various implementations of the described methods, one or more of the elements described are performed concurrently, in a different order than shown, or are omitted entirely. Other additional elements are also performed as desired. Any of the various systems or apparatuses described herein are configured to implement method <b>500</b>.</p><p id="p-0037" num="0036">A plurality of GPUs running on a common power supply execute code of a given computing task (block <b>505</b>). In one implementation, the given computing task is a neural network implementation. It should be understood that in other implementations, other types of processing units besides GPUs execute the code of the given computing task. The plurality of GPUs share power consumption data and the timing and duration of execution tasks with the other GPUs (block <b>510</b>). At least one GPU calculates the total power consumption for the plurality of GPUs and compares the total power consumption to a power limit (block <b>515</b>). In another implementation, a control unit (e.g., control unit <b>110</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>) calculates the total power consumption for the plurality of GPUs and compares the total power consumption to a power limit for the common power supply. In some implementations, the total power consumption for the plurality of GPUs is added to the expected or measured power consumption of other components of the system prior to being compared to the power limit for the common power supply. The other components of the system include those components which are also drawing power from the common power supply. In another implementation, the term &#x201c;power limit&#x201d; refers to the power available to the plurality of GPUs, and only the power consumption of the plurality of GPUs is compared to the power limit in this implementation.</p><p id="p-0038" num="0037">If the power consumption exceeds the power limit for more than a threshold amount of time (conditional block <b>520</b>, &#x201c;yes&#x201d; leg), then at least one GPU delays execution of a subsequent portion of the given computing task by a given delay amount (block <b>525</b>). In one implementation, the subsequent portion of the given computing task is a set of computations for a subsequent layer of a neural network. The value of the threshold amount of time varies from implementation to implementation. In one implementation, the threshold amount of time is equal to the amount of time it takes for a selected GPU to execute a portion of the given computing task. In another implementation, the given value is equal to an average of the duration of executing a portion of the given computing task by the plurality of GPUs. Next, the plurality of GPUs continue execution of subsequent portions of the given computing task (block <b>535</b>). After block <b>535</b>, method <b>500</b> returns to block <b>505</b>.</p><p id="p-0039" num="0038">Otherwise, if the power consumption does not exceed the power limit for more than the threshold amount of time (conditional block <b>520</b>, &#x201c;no&#x201d; leg), then the plurality of GPUs maintain their current alignment for executing portions of the given computing task (block <b>530</b>). After block <b>530</b>, the plurality of GPUs continue execution of subsequent portions of the given computing task (block <b>535</b>).</p><p id="p-0040" num="0039">Turning now to <figref idref="DRAWINGS">FIG. <b>6</b></figref>, one implementation of a method for determining an alignment of different phases of neural network processing is shown. A plurality of GPUs are programmed to implement a neural network (block <b>605</b>). It is assumed for the purposes of this discussion that the plurality of GPUs draw power from a common power supply. As part of implementing the neural network, the neural network functionality is partitioned into task portions and assigned to separate GPUs of the plurality of GPUs (block <b>610</b>). During implementation of the neural network, the plurality of GPUs communicate with each other regarding power consumption and the timing of task phase execution (block <b>615</b>).</p><p id="p-0041" num="0040">The plurality of GPUs also monitor the amount of time that is spent being power-limited during a given task (block <b>620</b>). The time spent being power-limited refers to the amount of time when the power limit of the power supply has been reached. If the time that the plurality of GPUs spend being power-limited during the given task is greater than a threshold (conditional block <b>625</b>, &#x201c;yes&#x201d; leg), then the plurality of GPUs initiate a phase misalignment scheme (block <b>630</b>). One example of implementing a phase misalignment scheme is described below during the discussion of method <b>700</b> of <figref idref="DRAWINGS">FIG. <b>7</b></figref>. Next, the initiated phase misalignment scheme is used for the remaining tasks of the neural network (block <b>635</b>). After block <b>635</b>, method <b>600</b> ends. If the time that the plurality of GPUs spend being power-limited during the given task is less than or equal to the threshold (conditional block <b>625</b>, &#x201c;no&#x201d; leg), then the plurality of GPUs maintain their current task phase alignment for one or more subsequent tasks (block <b>640</b>). After block <b>640</b>, method <b>600</b> returns to block <b>620</b>.</p><p id="p-0042" num="0041">Referring now to <figref idref="DRAWINGS">FIG. <b>7</b></figref>, one implementation of a method <b>700</b> for implementing a phase misalignment scheme is shown. As part of implementing a phase misalignment scheme, at the beginning of a given layer of a neural network, the plurality of GPUs are partitioned into a first group of GPUs and a second group of GPUs (block <b>705</b>). The specific number of GPUs in the first and second groups varies according to the implementation, with the possible number of GPUs per group ranging from 1 to (N&#x2212;1), wherein N is the total number of GPUs in the system. For example, in one implementation, if there are a total of 16 GPUs in the system, the first group has 8 GPUs and the second group has 8 GPUs. In other implementations, the GPUs do not have to be evenly divided into the first and second groups. For example, in another implementation, when the total number of GPUs is 16, the first group has 14 GPUs and the second group has 2 GPUs. It is noted that the first and second groups can also be referred to as subsets.</p><p id="p-0043" num="0042">After partitioning the GPUs into the first and second groups, the first group of GPUs implement a first phase of a given neural network layer while the second group of GPUs implement a second phase of the given neural network layer (block <b>710</b>). In one implementation, the first phase is a memory access phase and the second phase is a computation phase. For example, while the first group of GPUs are retrieving data associated with the given layer from memory, the second group of GPUs are computing values associated with the given layer. In one implementation, the computation phase uses a relatively large amount of power while the memory access phase uses a relatively small amount of power. In one implementation, this misalignment of phases is achieved by the first group of GPUs delaying the start of their execution. In a similar fashion, the first group of GPUs implement a second phase of the given neural network layer while the second group of GPUs implement a first phase of a subsequent neural network layer (block <b>715</b>). This staggering of phases between the two groups helps to reduce the amount of time that the total power consumption of the plurality of GPUs exceeds the power limit of the power supply. This staggering of phases between the two groups also helps to spread out the power consumption more evenly over time rather than having power consumption alternating between periods with relatively high power draws following by periods with relatively low power draws. The first and second groups of GPUs continue using the same misalignment pattern for the remaining layers of the neural network (block <b>720</b>). After block <b>720</b>, method <b>700</b> ends.</p><p id="p-0044" num="0043">Turning now to <figref idref="DRAWINGS">FIG. <b>8</b></figref>, one implementation of a method <b>800</b> for adjusting an execution starting time of a task is shown. A plurality of processing units, sharing a common power supply, execute a given portion of a computing task (block <b>805</b>). In one implementation, the computing task is the implementation of a neural network, and the given portion is a given layer of the neural network. The plurality of processing units monitor power consumption and execution durations by each processing unit for the given portion of the computing task (block <b>810</b>).</p><p id="p-0045" num="0044">If a first condition is detected from the execution of the given portion based on the monitored power consumption and execution durations (conditional block <b>815</b>, &#x201c;yes&#x201d; leg), then at least one processing unit adjusts the starting time of a subsequent portion of the computing task (block <b>820</b>). In one implementation, adjusting the starting time involves delaying the starting time of the subsequent portion so that the subsequent portion is executed after the other processing units execute their corresponding portions. By adjusting the starting time of the subsequent portion of the computing task, the plurality of processing units achieve more efficient use of the common power supply.</p><p id="p-0046" num="0045">If a first condition is not detected from the execution of the given portion (conditional block <b>815</b>, &#x201c;no&#x201d; leg), then the plurality of processing units maintain the existing alignment of the starting times for a subsequent portion of the computing task (block <b>825</b>). After block <b>825</b>, method <b>800</b> ends. In one implementation, the first condition is a threshold number of processing units having a synchronized alignment during execution of the given portion. In another implementation, the first condition is the total power consumption exceeding a power limit for a threshold amount of time. In a further implementation, the first condition is the total execution time needed to complete the given portion exceeding a specified duration. In a still further implementation, the first condition is the power consumption falling below a power threshold for a given duration. The power consumption can fall below the power threshold for the given duration if most or all processing units are performing a low-power task phase at the same time. This results in an inefficient usage of the available power supply. In other implementations, the first condition is any of various other conditions.</p><p id="p-0047" num="0046">Referring now to <figref idref="DRAWINGS">FIG. <b>9</b></figref>, one implementation of a method <b>900</b> for detecting patterns in portions of a computing task being executed by a plurality of processing units is shown. A plurality of processing units supplied by a common power supply execute a first plurality of portions of a computing task (block <b>905</b>). In one implementation, the processing units are GPUs, the computing task is a neural network, and the first plurality of portions are a plurality of layers of the neural network. In other implementations, other types of processing units, other types of computing tasks, and/or other types of portions of the computing task are performed in block <b>905</b>. In one implementation, the plurality of processing units are connected together in a ring topology. In other implementations, the plurality of processing units are connected together in other ways. The plurality of processing units monitor power consumption and portion execution durations for each processing unit for the first plurality of portions (block <b>910</b>).</p><p id="p-0048" num="0047">Next, the plurality of processing units determine if a pattern is detected in the individual power consumption and portion execution times for the first plurality of portions (conditional block <b>915</b>). Any of various pattern detection techniques are utilized, depending on the implementation. In one implementation, the plurality of processing units determine if the portion execution times are aligned among a threshold number of processing units. The threshold number varies according to the implementation. If the portion execution times are aligned for a threshold number of processing units, then the plurality of processing units determine if the total power consumption for the plurality of processing units exceeds a power limit during this alignment of portion execution times. In one implementation, if total power consumption for the plurality of processing units exceeds a power limit during this alignment of portion execution times, then the plurality of processing units will conclude that a pattern exists. In other implementations, other techniques for detecting patterns are possible and are contemplated.</p><p id="p-0049" num="0048">If a pattern of exceeding the power limit is not detected based on the individual power consumption and portion execution times for the first plurality of portions (conditional block <b>915</b>, &#x201c;no&#x201d; leg), then the plurality of processing units continue with the existing alignment of portion execution times (block <b>920</b>). After block <b>920</b>, method <b>900</b> ends. If a pattern of exceeding the power limit is detected based on the individual power consumption and portion execution times for the first plurality of portions (conditional block <b>915</b>, &#x201c;yes&#x201d; leg), then the plurality of processing units alter the alignment of portion execution times to disrupt the pattern (block <b>925</b>). In one implementation, the alignment of portion execution times is altered by having a first group of processing units delay the start of the execution of their portion of the computing task. In other implementations, other techniques for altering the alignment of portion execution times are possible and are contemplated.</p><p id="p-0050" num="0049">After block <b>925</b>, if the detected pattern has been disrupted after a given number of subsequent portions (conditional block <b>930</b>, &#x201c;yes&#x201d; leg), then the plurality of processing units continue with the altered alignment of portion execution times (block <b>935</b>). After block <b>935</b>, method <b>900</b> ends. The given number of subsequent portions varies according to the implementation. If the detected pattern has not been disrupted after the given number of subsequent portions (conditional block <b>930</b>, &#x201c;no&#x201d; leg), then the plurality of processing units try a different alteration to the alignment of portion execution times (block <b>940</b>). For example, in one implementation, a different alteration is selecting a different group of processing units for altering the start of executing their portions of the computing task. In another implementation, a different alteration involves adjusting the start of execution by a different delay amount. Other alterations are possible and are contemplated. After block <b>940</b>, method <b>900</b> returns to conditional block <b>930</b>.</p><p id="p-0051" num="0050">Turning now to <figref idref="DRAWINGS">FIG. <b>10</b></figref>, a block diagram of another implementation of a computing system <b>1000</b> is shown. In one implementation, computing system <b>1000</b> includes a central controller <b>1020</b> connected to a plurality of GPUs <b>1005</b>, <b>1010</b>, and <b>1015</b>. In one implementation, GPUs <b>1005</b>, <b>1010</b>, and <b>1015</b> are connected together in a ring topology. GPUs <b>1005</b>, <b>1010</b>, and <b>1015</b> are representative of any number of GPUs that are included in system <b>1000</b>, with the number varying from implementation to implementation. GPUs <b>1005</b>, <b>1010</b>, and <b>1015</b> use the ring topology to share information with each other about the global state, power consumption data, task starting times, task durations, and/or other metrics. In other implementations, GPUs <b>1005</b>, <b>1010</b>, and <b>1015</b> are connected together using any of various other suitable topologies. GPUs <b>1005</b>, <b>1010</b>, and <b>1015</b> also share information with central controller <b>1020</b>. In another implementation, central controller <b>1020</b> is connected in the ring topology with GPUs <b>1005</b>, <b>1010</b>, and <b>1015</b> instead of having individual connections to each GPU as is shown in <figref idref="DRAWINGS">FIG. <b>10</b></figref>.</p><p id="p-0052" num="0051">In one implementation, GPUs <b>1005</b>, <b>1010</b>, and <b>1015</b> work together to implement a distributed neural network. In various implementations, GPUs <b>1005</b>, <b>1010</b>, and <b>1015</b> send information about the initiation of layers of a neural network to each other and to central controller <b>1020</b>. For example, when GPU <b>1005</b> starts processing a first layer of the neural network, GPU <b>1005</b> sends an indication of this to GPU <b>1010</b>, which passes the indication on to the next GPU, and so on as well as sending the indication to central controller <b>1020</b>. GPUs <b>1010</b> and <b>1015</b> also send information about their status in regard to the initiation of the various layers of the neural network. In some implementations, GPUs <b>1005</b>, <b>1010</b>, and <b>1015</b> also specify the type of propagation (e.g., forward propagation, back propagation) that is being implemented as well as additional information associated with the neural network.</p><p id="p-0053" num="0052">In some implementations, GPU <b>1005</b> sends an indication of which type of phase is being performed to the other GPUs and to central controller <b>1020</b>. For example, when GPU <b>1005</b> is fetching data associated with the first layer, GPU <b>1005</b> sends an indication that a memory access phase is being performed. When GPU <b>1005</b> is processing the fetched data, GPU <b>1005</b> sends an indication that a compute phase is being performed. The other GPUs can also do likewise. Additionally, GPUs <b>1005</b>, <b>1010</b>, and <b>1015</b> share information with central controller <b>1020</b> about their individual power consumption. For example, GPU <b>1005</b> sends data specifying its latest power consumption status, GPU <b>1010</b> sends data specifying its latest power consumption status, and so on. Central controller <b>1020</b> uses this information to determine when a given power limit is being exceeded. When the given power limit is exceeded, this will cause the power supplied to each GPU of GPUs <b>1005</b>, <b>1010</b>, and <b>1015</b> to be throttled. This will cause a slowdown in the implementation of the neural network. However, the GPUs <b>1005</b>, <b>1010</b>, and <b>1015</b> can take corrective action to prevent or reduce the amount of time that the power limit is exceeded.</p><p id="p-0054" num="0053">In one implementation, if central controller <b>1020</b> determines that the given power limit will be exceeded for a given layer of the neural network, central controller <b>1020</b> sends requests to GPUs <b>1005</b>, <b>1010</b>, and <b>1015</b> to stagger the alignment of the various phases of the given layer. For example, in one implementation, in response to determining that the given power limit has been exceeded or predicting that the given power limit will be exceeded, central controller <b>1020</b> divides GPUs <b>1005</b>, <b>1010</b>, and <b>1015</b> into first and second groups. Central controller <b>1020</b> commands the first group of GPUs to perform the phases of the layer in the normal fashion. Central controller <b>1020</b> commands the second group of GPUs to delay the start of the computation phase so that it does not align with the computation phase performed by the first group of GPUs. By causing the computation phase of the first and second groups of GPUs to be misaligned, the power limit will be exceeded for a lower percentage of time. This allows the layers of the neural network to be processed in less time than if the computation phases of all GPUs <b>1005</b>, <b>1010</b>, and <b>1015</b> were aligned.</p><p id="p-0055" num="0054">In various implementations, program instructions of a software application are used to implement the methods and/or mechanisms described herein. For example, program instructions executable by a general or special purpose processor are contemplated. In various implementations, such program instructions are represented by a high level programming language. In other implementations, the program instructions are compiled from a high level programming language to a binary, intermediate, or other form. Alternatively, program instructions are written that describe the behavior or design of hardware. Such program instructions are represented by a high-level programming language, such as C. Alternatively, a hardware design language (HDL) such as Verilog, is used. In various implementations, the program instructions are stored on any of a variety of non-transitory computer readable storage mediums. The storage medium is accessible by a computing system during use to provide the program instructions to the computing system for program execution. Generally speaking, such a computing system includes at least one or more memories and one or more processors configured to execute program instructions.</p><p id="p-0056" num="0055">It should be emphasized that the above-described implementations are only non-limiting examples of implementations. Numerous variations and modifications will become apparent to those skilled in the art once the above disclosure is fully appreciated. It is intended that the following claims be interpreted to embrace all such variations and modifications.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A system comprising:<claim-text>a plurality of processing units; and</claim-text><claim-text>one or more links between the plurality of processing units;</claim-text><claim-text>wherein each processing unit of the plurality of processing units is configured to perform a computing task; and</claim-text><claim-text>wherein at least one processing unit is configured to change a time at which a given computing task is performed with respect to tasks being performed by other processing units, responsive to detecting a first condition.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The system as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first condition is at least one of:<claim-text>a power limit being exceeded; and</claim-text><claim-text>a number of processing units which share a computing task phase alignment is greater than a threshold.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The system as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the computing task performed by each of the plurality of processing units comprises a same algorithm.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The system as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the change in time implemented by the at least one processing unit comprises delaying processing of the given computing task to a later time.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The system as recited in <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the later time is selected to coincide with a period of higher power consumption by a computing task being performed by a processing unit other than the at least one processing unit.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The system as recited in <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the plurality of processing units are implementing at least one of a machine learning model and a neural network.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The system as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each of the plurality of processing units communicate information comprising one or more of power consumption and task execution phases via one or more links.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. A method comprising:<claim-text>performing, by each processing unit of a plurality of processing units, a computing task;</claim-text><claim-text>changing, by at least one processing unit, a time at which a given computing task is performed with respect to tasks being performed by other processing units, responsive to detecting a first condition.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method as recited in <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the first condition is at least one of:<claim-text>a power limit being exceeded; and</claim-text><claim-text>a number of processing units which share a computing task phase alignment is greater than a threshold.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method as recited in <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the computing task performed by each of the plurality of processing units comprises a same algorithm.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The system as recited in <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein changing the time by the at least one processing unit comprises delaying processing of the given computing task to a later time.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method as recited in <claim-ref idref="CLM-00011">claim 11</claim-ref>, further comprising selecting the later time to coincide with a period of higher power consumption by a computing task being performed by a processing unit other than the at least one processing unit.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method as recited in <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the plurality of processing units are implementing at least one of a machine learning model and a neural network.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method as recited in <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein each of the plurality of processing units communicate information comprising one or more of power consumption and task execution phases via one or more links.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. An apparatus comprising:<claim-text>a first processing unit; and</claim-text><claim-text>a second processing unit;</claim-text><claim-text>wherein the first processing unit is configured to:<claim-text>perform a first portion of a common computing task;</claim-text><claim-text>receive power consumption data and task status from the second processing unit performing a second portion of the common computing task; and</claim-text><claim-text>change a time at which a given computing task is performed with respect to a task being performed by the second processing unit, responsive to detecting a first condition.</claim-text></claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The apparatus as recited in <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the first condition is at least one of:<claim-text>a power limit being exceeded; and</claim-text><claim-text>a number of processing units which share a computing task phase alignment is greater than a threshold.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The apparatus as recited in <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the computing task performed by each of the first and second processing units comprises a same algorithm.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The apparatus as recited in <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the change in time implemented by the first processing unit comprises delaying processing of the given computing task to a later time.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The apparatus as recited in <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the later time is selected to coincide with a period of higher power consumption by a computing task being performed by the second processing unit.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The apparatus as recited in <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the change in time reduces an amount of a time during which total power consumption by the first and second processing units exceeds a power limit.</claim-text></claim></claims></us-patent-application>