<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004745A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004745</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17364749</doc-number><date>20210630</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>62</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>3</main-group><subgroup>40</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>70</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>B</section><class>60</class><subclass>W</subclass><main-group>40</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>00832</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>00375</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>00228</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>2054</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>6267</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>00355</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>3</main-group><subgroup>40</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>6217</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>70</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>209</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>60</class><subclass>W</subclass><main-group>40</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>0454</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30196</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20084</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30268</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>10048</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>60</class><subclass>W</subclass><main-group>2420</main-group><subgroup>42</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20200201</date></cpc-version-indicator><section>B</section><class>60</class><subclass>W</subclass><main-group>2540</main-group><subgroup>223</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20200201</date></cpc-version-indicator><section>B</section><class>60</class><subclass>W</subclass><main-group>2540</main-group><subgroup>221</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>60</class><subclass>W</subclass><main-group>2420</main-group><subgroup>40</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20200201</date></cpc-version-indicator><section>B</section><class>60</class><subclass>W</subclass><main-group>2540</main-group><subgroup>229</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">VEHICLE OCCUPANT MONITORING SYSTEM AND METHOD</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>FotoNation Limited</orgname><address><city>Galway</city><country>IE</country></address></addressbook><residence><country>IE</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>FULOP</last-name><first-name>Szabolcs</first-name><address><city>Brasov</city><country>RO</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>DINU</last-name><first-name>Dragos</first-name><address><city>Brasov</city><country>RO</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Ene</last-name><first-name>Radu</first-name><address><city>Brasov</city><country>RO</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>FotoNation Limited</orgname><role>03</role><address><city>Galway</city><country>IE</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A method for monitoring occupants of a vehicle comprises identifying a respective body region for one or more occupants of the vehicle within an obtained image; identifying within the body regions, skeletal points including points on an arm of a body; identifying one or more hand regions; and determining a hand region to be either a left or a right hand of an occupant in accordance with its spatial relationship with identified skeletal points of the body region of an occupant. The left or right hand region for the occupant are provided to a pair of classifiers to provide an activity classification for the occupant, a first classifier being trained with images of hands of occupants in states where objects involved are not visible and a second classifier being trained with images of occupants in the states where the objects are visible in at least one hand region.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="91.27mm" wi="158.75mm" file="US20230004745A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="173.57mm" wi="120.90mm" orientation="landscape" file="US20230004745A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="222.08mm" wi="138.26mm" orientation="landscape" file="US20230004745A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="254.25mm" wi="147.15mm" orientation="landscape" file="US20230004745A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="249.43mm" wi="107.70mm" orientation="landscape" file="US20230004745A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="258.83mm" wi="132.33mm" orientation="landscape" file="US20230004745A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">FIELD</heading><p id="p-0002" num="0001">The present invention relates to a vehicle occupant monitoring system and method.</p><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">Referring now to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, vehicle occupant monitoring systems typically comprise an image acquisition device <b>104</b> mounted in a cabin <b>110</b> of a vehicle in which one or more occupants <b>108</b>, <b>112</b> are seated within the field of view <b>114</b> of the camera. In <figref idref="DRAWINGS">FIG. <b>1</b></figref>, two occupants <b>108</b>, <b>112</b> are shown in front seats of the vehicle, with the image acquisition device <b>104</b> located forwardly and facing rearwardly. It will be appreciated that other occupants of the vehicle including back seat passengers (not shown) may also appear within the field of view <b>114</b> of the camera <b>104</b>.</p><p id="p-0004" num="0003"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows a typical image <b>200</b> acquired by the image acquisition device <b>104</b> where it will be seen that occupant <b>108</b> is the driver of the vehicle, while occupant <b>112</b> is a passenger.</p><p id="p-0005" num="0004">It is desirable and indeed future vehicle safety standards are based on being able to identify the activity of the occupants at any given time, especially to determine if any of these endanger the safe driving of the vehicle, for example, in the case of semi/fully autonomous cars, to determine if the person sitting behind the wheel is able to take control of the vehicle (if needed).</p><p id="p-0006" num="0005">Patrick Weyers et al, &#x201c;Action and Object Interaction Recognition for Driver Activity Classification&#x201d;, 2019 IEEE Intelligent Transportation Systems Conference (ITSC) Auckland, NZ, Oct. 27-30, 2019 discloses a driver monitoring system where in a first stage, body key points of the driver are localised and in a second stage, image regions around the localized hands are extracted. These regions and the determined 3D body key points are used as the input to a recurrent neural network for driver activity recognition.</p><p id="p-0007" num="0006">Sehyun Chun et al, &#x201c;NADS-Net: A Nimble Architecture for Driver and Seat Belt Detection via Convolutional Neural Networks&#x201d;, arXiv:1910.03695 utilizes a feature pyramid network (FPN) backbone with multiple detection heads for driver/passenger state detection tasks.</p><p id="p-0008" num="0007">Guillermo Garcia-Hernando et al, &#x201c;First-Person Hand Action Benchmark with RGB-D Videos and 3D Hand Pose Annotations&#x201d;, CVPR 2018, discloses using of 3D hand poses to recognize first-person dynamic hand actions interacting with 3D objects.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0009" num="0008">According to a first aspect of the present invention, there is provided a method according to claim <b>1</b>.</p><p id="p-0010" num="0009">According to a second aspect, there is provided a method according to claim <b>15</b>.</p><p id="p-0011" num="0010">In further aspects, there are provided vehicle occupant monitoring systems and computer program products for implementing methods according to the invention.</p><p id="p-0012" num="0011">Embodiments of the invention identify an occupant state which may be closely related to the occupant handling an object, without explicitly identifying the object.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0013" num="0012">An embodiment of the invention will now be described, by way of example, with reference to the accompanying drawings, in which:</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows schematically a cabin of a vehicle including a number of occupants as well as an image acquisition device for a vehicle occupant monitoring system according to an embodiment of the present invention;</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates body, face and hand regions detected within the field of view of the image acquisition device of <figref idref="DRAWINGS">FIG. <b>1</b></figref>;</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a schematic diagram of the components of the vehicle occupant monitoring system according to an embodiment of the present invention;</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates the output provided by a skeleton detector of <figref idref="DRAWINGS">FIG. <b>3</b></figref>; and</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIGS. <b>5</b> and <b>6</b></figref> illustrate the expansion of detected body and hand regions before being provided to the skeleton detector and classifiers of <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DESCRIPTION</heading><p id="p-0019" num="0018">Referring now to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, a vehicle occupant monitoring system according to an embodiment of the present invention acquires successive images <b>300</b> from an image acquisition device <b>104</b> mounted in a cabin <b>110</b> of the vehicle.</p><p id="p-0020" num="0019">The image acquisition device <b>104</b> can comprise any of: a visible wavelength color camera with RGB pixels, an infra-red (IR) camera, a camera including a Bayer array of pixels including R,G,B and IR sensitive pixels or a thermal camera. Alternatively, the camera could comprise a monochrome, for example, green sensitive, camera. In a still further alternative, the image acquisition device <b>104</b> could comprise an event camera of the type described in PCT Application No. PCT/EP2021/058973 (Ref: FN-675-PCT), the disclosure of which is incorporated herein by reference, where event information is accumulated in a tensor before being provided to downstream classifiers. Again, the pixels of such a camera can be sensitive to any of visible, infra-red or thermal radiation. As disclosed in PCT/EP2021/066440 (Ref: FN-668-PCT), the disclosure of which is herein incorporated by reference, dedicated hardware can be provided to produce such an image tensor from event camera information. In a still further alternative, the image acquisition device <b>104</b> could comprise a combined frame based and event camera such as a Davis <b>346</b> camera available at iniVation.com.</p><p id="p-0021" num="0020">It will also be appreciated that it is possible to swap between IR images and visible wavelength image source in accordance with a measured ambient light level of the vehicle cabin.</p><p id="p-0022" num="0021">In a typical implementation, the camera comprises a 1600&#xd7;1300 array of pixels. However, this is typically not sufficient to be able to explicitly identify the various objects which an occupant may have in their hands, and which can signal their current activity&#x2014;especially for back seat occupants who appear significantly smaller than front seat occupants and so would require even greater camera resolution in order to classify their activity through explicitly identifying objects in their hand. Also, for example, while identifying a cigarette can indicate that an occupant is smoking, identifying any one of a number of foodstuffs can indicate that an occupant is eating, and identifying a phone may indicate that an occupant is either talking on the phone without using hands-free mode or texting or otherwise interacting with their phone, this approach may often not produce satisfactory or reliable results. For example, objects may be held in an occupant's hand in a manner where most of the object is occluded.</p><p id="p-0023" num="0022">In embodiments of the present invention camera image information is downsampled to QVGA resolution, i.e. 320&#xd7;240, before being provided to respective a body detector <b>302</b>, hand detector <b>304</b> and face detector <b>306</b>.</p><p id="p-0024" num="0023">The body detector <b>302</b> is configured to identify one or more regions of the image bounding a human body. In <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the region <b>202</b> has been identified as bounding the driver <b>108</b> and the region <b>204</b> has been identified as bounding the passenger <b>112</b>. Similarly the face detector <b>306</b> identifies regions <b>206</b> and <b>208</b> bounding the faces of the passenger <b>112</b> and driver <b>108</b> respectively. Finally, the hand detector <b>304</b> identifies hand regions <b>210</b> and <b>212</b>.</p><p id="p-0025" num="0024">It will be appreciated that any suitable forms of detectors can be employed ranging from classical Haar type detectors to neural network based detectors. In the case of neural network detectors, networks with similar or identical structures, but trained to use different weights can be used for each of the body detector <b>302</b>, hand detector <b>304</b> and face detector <b>306</b>. These detectors can be run serially, where the intermediate or final output of one may boost the performance of another, or in parallel in particular using a multi-core neural network processor of the type disclosed in European Patent Nos. EP 3580691 (Ref: FN-618-EP) and EP 3668015 (Ref: FN-636-EP), the disclosures of which are incorporated herein by reference.</p><p id="p-0026" num="0025">Typically, such networks will produce a set of confidence levels for bounding boxes at different scales and proportions at respective locations across the image. In some embodiments, up to 4 mutually exclusive bounding boxes <b>308</b> with confidence levels above a threshold amount will be chosen as body regions. Thus, for each body region, an x,y location of its origin within the image, along with a width and height and a confidence level for the body region will be provided from the body detector <b>304</b>.</p><p id="p-0027" num="0026">Similarly, up to 4 mutually exclusive bounding boxes with confidence levels above a threshold amount will be chosen as face regions <b>310</b>. Thus, for each face region, an x,y location of its origin within the image, along with a width and height and a confidence level for the face region will be provided from the face detector <b>308</b>.</p><p id="p-0028" num="0027">Finally, up to 8 mutually exclusive bounding boxes <b>312</b> with confidence levels above a threshold amount will be chosen as hand regions. Again, for each hand region, an x,y location of its origin within the image, along with a width and height and a confidence level for the hand region will be provided from the hand detector <b>304</b>.</p><p id="p-0029" num="0028">It will be understood that where it is possible for there to be fewer or more occupants of a vehicle, the maximum number of body, face and hand boxes can be varied accordingly.</p><p id="p-0030" num="0029">In the embodiment, each bounding box <b>308</b> for a body region is provided to a body skeleton detector <b>314</b>. When such a detector is implemented with a neural network, it typically requires its input image to be normalised. In the illustrated embodiment, the input image map for the skeleton detector <b>314</b> is 192&#xd7;192 pixels. Given that the input image for the body detector <b>302</b> comprises only 320&#xd7;240 pixels, it is most likely that a detected body region <b>308</b> will be smaller than this size and typically will have a width less than its height. However, rather than scaling and resizing the detected bounding box <b>308</b> directly, in the embodiment, the system first re-shapes the body region, by expanding the width of body region to provide an expanded square region <b>326</b> centered around the originally detected body region <b>308</b>. The re-shaped body region can then be expanded to comprise a region <b>400</b> of 192&#xd7;192 pixels corresponding to the input map size of the skeleton detector <b>314</b>. It will be appreciated that the re-shaping and the expansion may mean that the boundary of the body region extends beyond the boundary of the input image <b>300</b>&#x2014;such regions <b>328</b> can be simply treated as blank. Nonetheless, the increased image area provided around the detected body region <b>308</b> can provide the skeleton detector <b>314</b> with improved context with which to make its inference.</p><p id="p-0031" num="0030">Referring now to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, conventional skeleton detectors <b>314</b> typically provide at their output a respective heat map for each joint to be identified with the peak value in each heat map generally indicating the location of the associated joint. These results can be filtered and analysed to the extent required, but in essence, an image crop <b>400</b> from the input image <b>300</b> corresponding to a re-shaped and expanded bounding box from the body detector <b>302</b> can be used to provide a set of x,y locations within the input image for each one a number of joints of the body region of interest. Note that, as per US-2021-0056701 (Ref: FN-651-US), the disclosure of which is incorporated herein by reference, the x,y locations for joints do not necessarily need to fall within the area of the re-shaped and expanded bounding box and some locations for joints can be inferred, even if they are not visible within the image crop <b>400</b>.</p><p id="p-0032" num="0031">As will be seen, in the present embodiment, two joints in particular are of interest and these are the left <b>402</b> and right <b>404</b> wrists of the occupant.</p><p id="p-0033" num="0032">Referring back to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, in the embodiment, a hand filter <b>317</b> looks at the bounding boxes <b>312</b> for each hand identified by the hand detector <b>304</b> and associates a hand bounding box <b>312</b> nearest and/or most overlapping a wrist location <b>402</b>, <b>404</b> as the left or right hand region corresponding to the body.</p><p id="p-0034" num="0033">It should also be noted, as shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, that rather than searching the input image <b>300</b> for all hand regions in one pass, it is also possible to use the expanded image crop <b>400</b> around each body region <b>308</b> as an input image for respective instances of the hand detector <b>304</b>. Now any hand regions that are detected within each input image crop <b>400</b> are automatically associated with the body from which the image crop <b>400</b> is derived. Again nonetheless, a hand bounding box <b>312</b> nearest and/or most overlapping a wrist location <b>402</b>, <b>404</b> is designated as the left <b>312</b>-L or right <b>312</b>-R hand region corresponding to the body.</p><p id="p-0035" num="0034">Similarly, a face region <b>318</b> which most overlaps with a body region or which is closest or most overlaps with the eye sockets or chin locations identified by the skeleton detector <b>314</b> for a given body region can be treated as the face region corresponding to the identified body.</p><p id="p-0036" num="0035">In the embodiment, pairs of left and right hand regions identified for a given body are fed to respective networks <b>316</b>, <b>318</b> which will be described in more detail below. Again, these networks <b>316</b>, <b>318</b> require their input images to be normalised. In the illustrated embodiment, each input image map for the networks <b>316</b>, <b>318</b> is 128&#xd7;128 pixels. Again, it is most likely that a detected hand region will be smaller than this size. Again, as with the detected body region(s), rather than scaling and resizing a detected bounding box <b>312</b> directly, in the embodiment, the system first re-shapes each hand region, by expanding the width of hand region, so that it comprises a square centered around the originally detected hand region, for example, region <b>320</b>-L in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. (In this case, hand region <b>312</b>-R is already square and so this step can be skipped.) The re-shaped hand region is then expanded to comprise a region <b>322</b>-L, <b>322</b>-R of 128&#xd7;128 pixels corresponding to the input map size of the networks <b>316</b>, <b>318</b>. Again, the increased image area provided around the detected hand region(s) can provide the networks <b>316</b>, <b>318</b> with improved context with which to make their inference. It will also be appreciated that where no hand regions are associated with a given body, the networks <b>316</b>, <b>318</b> will not be executed. However, when only one hand region is associated with a given body, one or both of the networks <b>316</b>, <b>318</b> can be executed with the second hand region input being blank.</p><p id="p-0037" num="0036">In the embodiment, each of the networks <b>316</b>, <b>318</b> comprise identical structures and produce corresponding outputs, with each output corresponding to a user activity of concern, for example: smoking, eating, talking on the phone, texting.</p><p id="p-0038" num="0037">However, the network <b>316</b> is trained with images of occupants involved in such activities where the objects involved in the activity, for example, a cigarette, food or a phone are not visible. On the other hand, the network <b>318</b> is trained with images of occupants involved in such activities where the objects are visible in at least one hand region of the input image.</p><p id="p-0039" num="0038">As will be seen later, this allows the networks <b>316</b>, <b>318</b> to provide improved inference because the appearance of a hand region during different activities tends to either show the hand or the object and so, rather than tending to provide an averaged output inference value when a hand region clearly shows a hand involved in an activity or an object indicative of an activity, in the present embodiment, in the first case when only an occupants hands are visible, network <b>316</b> should produce quite a high confidence level and the network <b>318</b> a low confidence level, whereas in the second case when an object is visible in at least one hand, network <b>318</b> should produce quite a high confidence level and the network <b>316</b> a lower confidence level.</p><p id="p-0040" num="0039">In the embodiment, the output produced by the network <b>316</b> is labelled handCompare and qualified by the type of (unseen) object associated with the activity, e.g. cigarette, food, phone. The output produced by the network <b>318</b> is labelled gestureCompare and qualified by the type of visible object associated with the activity, again: cigarette, food, phone.</p><p id="p-0041" num="0040">For each body region <b>308</b>, each of the classifications produced by the networks <b>316</b>, <b>318</b>, the left/right hand inference produced by the hand filter <b>317</b>, the skeleton data produced by the detector <b>314</b> and the location of the face region <b>310</b> are fed to occupant behaviour logic <b>320</b> to make a final classification of the activity for the occupant corresponding to the identified body region.</p><p id="p-0042" num="0041">It will be understood that all of the above information can be produced from a single image (or image tensor) <b>300</b> from image acquisition device <b>104</b>.</p><p id="p-0043" num="0042">However, it will be appreciated, that inference made on previously acquired image information can also be used to improve the operation of the various modules within the system of <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0044" num="0043">Furthermore, other modules can also be employed with a view to determining user activity without necessarily and explicitly identifying an object indicative of the activity. For example, as is well known, a face region <b>310</b> for a body can be tracked from one image frame to the next. In one embodiment, a smoking detector <b>322</b> makes a simple contrast measurement on each instance of face region detected. It will be appreciated that when an occupant smokes, they will periodically exhale smoke and this tends to lower the contrast of the image in the region of their face during this period. When such a periodic fluctuation in contrast is detected, then the smoking detector <b>322</b> can signal that the occupant corresponding to the face region may be smoking.</p><p id="p-0045" num="0044">The occupant behaviour logic <b>320</b> can now further process the information provided to generate further inferences about user activity.</p><p id="p-0046" num="0045">In a first example, the logic <b>320</b> attempts to identify whether any arm of an occupant is in a V shape indicative of a user talking on a phone, smoking, eating or drinking. In order to do so, the logic <b>320</b> attempts to estimate a Euclidian distance between each of 3 skeleton points from the arm: (W)rist, (S)houlder, (E)lbow. The Euclidian distance (d) can be estimated from a single 2D image, especially taking into account that the approximate location and orientation of a body relative to the image acquisition device <b>104</b> is known. So if d(W, S)&#x3c;d(S, E) and d(W, E), it can be inferred that the occupants arm is in a V shape. In such a case, a parameter armSkeletonCompare can be set to true for the occupant.</p><p id="p-0047" num="0046">It will be appreciated that non-geometrical techniques can also be employed to determine if the occupant's arm is in a V shape including applying a dedicated neural network to the body region <b>308</b> or re-shaped and expanded body region <b>400</b>.</p><p id="p-0048" num="0047">In another example, the behaviour logic <b>320</b> may analyze the skeleton data to determine if an occupant's arm is in an L shape which can be indicative of a user interacting with their phone other than talking. When such an L shape is detected, a parameter similar to armSkeletonCompare can be set to true.</p><p id="p-0049" num="0048">In the embodiment, another parameter which is determined by the behaviour logic <b>320</b> is based on detecting whether a hand region associated with a body overlaps or is closely adjacent the face region for a body. If so, a parameter handFaceCompare is set to true.</p><p id="p-0050" num="0049">So, for any given frame, the behaviour logic <b>320</b> is provided by the networks <b>316</b>, <b>318</b> with handCompare and gestureCompare values indicating a respective confidence levels for the various occupant activities being monitored for each occupant associated with a detected body region. In the embodiment, the behaviour logic <b>320</b> can analyse the skeleton joint locations to produce an indication, armSkeletonCompare, of whether one or more of the monitored occupant activities are true or not. Furthermore, in the embodiment, the behaviour logic <b>320</b> can analyse the spatial relationship of the hand and face regions to produce an indication, handFaceCompare, of an activity.</p><p id="p-0051" num="0050">All of these parameters, as well as the current value of smoking provided by the smoking detector <b>322</b>, when this is available, can be combined to produce a final value for the current activity of the occupant.</p><p id="p-0052" num="0051">So, for example, if:</p><p id="p-0053" num="0000"><tables id="TABLE-US-00001" num="00001"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="left"/><thead><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry>(handCompare == TRUE (for object_class == food or drink or phone));</entry></row><row><entry>or</entry></row><row><entry>gestureCompare == TRUE(for object_class == food or drink or phone));</entry></row><row><entry>and</entry></row><row><entry>handFaceCompare == TRUE; and</entry></row><row><entry>armSkeletonCompare == TRUE</entry></row><row><entry>then eating / drinking / talking on the phone is detected.</entry></row><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0054" num="0052">Thus, these behaviors are triggered only when the food/drink/phone is near the face, not just held in the hand.</p><p id="p-0055" num="0053">In some embodiments, the behaviour logic <b>320</b> can wait until eating/drinking/talking on the phone is detected in more than a threshold number of frames within a pre-defined time window before flagging the behaviour.</p><p id="p-0056" num="0054">In another example, if:</p><p id="p-0057" num="0000"><tables id="TABLE-US-00002" num="00002"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="offset" colwidth="14pt" align="left"/><colspec colname="1" colwidth="203pt" align="left"/><thead><row><entry/><entry namest="offset" nameend="1" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/><entry>(handCompare == TRUE (for object_class == phone); or</entry></row><row><entry/><entry>gestureCompare == TRUE (for object_class == phone)) and</entry></row><row><entry/><entry>handFaceCompare == FALSE; and</entry></row><row><entry/><entry>armSkeletonCompare == FALSE.</entry></row><row><entry/><entry>then texting is detected.</entry></row><row><entry/><entry namest="offset" nameend="1" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0058" num="0055">In this case, rather than using armSkeletonCompare as false, the value of the L shape skeleton detecting parameter being true could be used.</p><p id="p-0059" num="0056">In any case, it will be seen how this behaviour is detected only when a phone is held in the hand, away from the face.</p><p id="p-0060" num="0057">Again, the behaviour logic <b>320</b> can wait until this behaviour is detected in more than a threshold number of frames within a pre-defined time window before flagging the behaviour.</p><p id="p-0061" num="0058">Further analysis of the skeleton points provided by the detector <b>314</b> can also be employed to detect other behaviours.</p><p id="p-0062" num="0059">For example, the skeleton points (neck, left hip, right hip) can be used compute an upper body forward angle, where 0&#xb0;-30&#xb0; can indicate the occupant is bending down, while angles of approximately 90&#xb0; indicate the occupant is upright.</p><p id="p-0063" num="0060">If no hip skeleton points are detected, then the behaviour logic <b>320</b> can check if any of the head or shoulder skeleton points are located inside a &#x201c;bending&#x201d; region of the body region, for example, a lower 50% of the body region. If so, then bending down is detected.</p><p id="p-0064" num="0061">It is also possible to check if the face region associated with a body region intersects the lower bending region of the body region and if so, bending down can be indicated.</p><p id="p-0065" num="0062">These checks can also be combined and again the activity can be monitored so that if it is detected in more than a threshold number of frames within a pre-defined time window, the behaviour is flagged.</p><p id="p-0066" num="0063">As well as determining activities from the instantaneous values of the above parameters and the instantaneous location of skeleton joints for a given frame, the behaviour logic <b>320</b> can also keep track of parameter values over time to determine an occupant activity. So again, in relation to bending, if the aspect ratio of the body region changes over time, with the height decreasing from a height for the body region when it was detected as upright, then this too can be used to indicate bending.</p><p id="p-0067" num="0064">Once any particular occupant behaviour has been determined by the behaviour logic <b>320</b>, especially, when this indicates unsafe behaviour on the part of the occupant, and especially the driver, this can be signalled to a vehicle controller. The indication can be used by for example, a vehicle Advanced Driver-Assistance System (ADAS) either to signal to the occupant that they should cease their behaviour or in some cases, to automatically control the vehicle and for example to reduce the vehicle speed to below a safety threshold.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method for monitoring occupants of a vehicle comprising:<claim-text>obtaining successive images of an interior cabin of the vehicle from an occupant facing camera;</claim-text><claim-text>identifying a respective body region for one or more occupants of said vehicle within an obtained image;</claim-text><claim-text>identifying within at least one of said body regions, a plurality of skeletal points including one or more points on an arm of a body;</claim-text><claim-text>identifying one or more face regions within said obtained image;</claim-text><claim-text>identifying one or more hand regions within said obtained image;</claim-text><claim-text>determining a hand region to be either a left or a right hand of an occupant in accordance with its spatial relationship with identified skeletal points of said body region of said occupant;</claim-text><claim-text>determining a face region to be a face region of an occupant in accordance with its spatial relationship with said body region of said occupant;</claim-text><claim-text>analysing a spatial relationship of a determined left or right hand region and a determined face region to provisionally determine a state of said occupant;</claim-text><claim-text>providing at least said determined left or right hand region for said occupant to a classifier to provide an activity classification for said occupant; and</claim-text><claim-text>combining said provisional state of said occupant and said activity classification to provide a final state for said occupant.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref> comprising expanding around the or each determined left or right hand region within said obtained image to generate an expanded hand region of a shape and size corresponding to an input image shape and size of said classifier and providing said expanded hand region to said classifier.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref> comprising expanding around the body region within said image to generate an expanded body region of a shape and size corresponding to an input image shape and size of a neural network for identifying said plurality of skeletal points of said body.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method according to <claim-ref idref="CLM-00003">claim 3</claim-ref> wherein said neural network is configured to provide a heat map of potential locations for said skeletal points, the method further comprising filtering said heat map to provide final locations within said image for said skeletal points.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein said occupant is one of: a driver; or a passenger of said vehicle.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein said skeletal points include: eye sockets, chin, ears, shoulders, elbows, wrists, hips and knees.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> comprising identifying each of said body, face and hand regions using a neural network with a common architecture and with respective weights specific to each of said body, face and hands.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> comprising swapping between visible and infra-red camera images in accordance with an ambient light level of said cabin.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> further comprising: monitoring an identified face region through successively obtained images to make a provisional classification that said occupant is smoking; and combining said provisional classification that said occupant is smoking with said provisional state of said occupant and said activity classification to provide said final state for said occupant.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref> wherein said monitoring comprises: monitoring a contrast level of said identified face region through successively obtained images; and analysing changes in said contrast level over time to make said provisional classification that said occupant is smoking.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein said classifier comprises a pair of classifiers, a first being trained with images of hands of occupants in states where objects involved are not visible and a second trained with images of occupants in said states where the objects are visible in at least one hand region.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref> wherein said pair of classifiers comprise identical structures and produce corresponding outputs.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein said states include: smoking, eating, drinking, talking on the phone and texting.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> further comprising analysing the respective locations of skeletal points to provide a further provisional determination of an occupant activity.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. A method for monitoring occupants of a vehicle comprising:<claim-text>obtaining successive images of an interior cabin of the vehicle from an occupant facing camera;</claim-text><claim-text>identifying a respective body region for one or more occupants of said vehicle within an obtained image;</claim-text><claim-text>identifying within at least one of said body regions, a plurality of skeletal points including one or more points on an arm of a body;</claim-text><claim-text>identifying one or more hand regions within said obtained image;</claim-text><claim-text>determining a hand region to be either a left or a right hand of an occupant in accordance with its spatial relationship with identified skeletal points of said body region of said occupant; and</claim-text><claim-text>providing at least said determined left or right hand region for said occupant to a pair of classifiers to provide an activity classification for said occupant, a first classifier of said pair of classifiers being trained with images of hands of occupants in states where objects involved are not visible and a second classifier of said pair of classifiers being trained with images of occupants in said states where the objects are visible in at least one hand region.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The method of <claim-ref idref="CLM-00015">claim 15</claim-ref> wherein said pair of classifiers comprise identical structures and produce corresponding outputs.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. An occupant monitoring system for a vehicle comprising at least one image acquisition device configured to acquire successive images of an interior cabin of the vehicle with an occupant facing camera; and a processor configured to perform the steps of <claim-ref idref="CLM-00001">claim 1</claim-ref>.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. A computer program product comprising a computer readable medium on which instructions are stored which when executed on a processor of an occupant monitoring system for a vehicle are configured to perform the steps of <claim-ref idref="CLM-00001">claim 1</claim-ref>.</claim-text></claim></claims></us-patent-application>