<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005479A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005479</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17856146</doc-number><date>20220701</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>IT</country><doc-number>102021000017513</doc-number><date>20210702</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>22</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>25</main-group><subgroup>24</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>30</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>25</main-group><subgroup>51</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>21</main-group><subgroup>0308</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>21</main-group><subgroup>034</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>06</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>16</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>22</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>25</main-group><subgroup>24</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>30</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>25</main-group><subgroup>51</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>21</main-group><subgroup>0308</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>21</main-group><subgroup>034</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>063</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>16</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">METHOD FOR PROCESSING AN AUDIO STREAM AND CORRESPONDING SYSTEM</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>PRAGMA ETIMOS S.R.L.</orgname><address><city>Latina (LT)</city><country>IT</country></address></addressbook><residence><country>IT</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>LO PRESTI</last-name><first-name>Gaetano</first-name><address><city>Latina (LT)</city><country>IT</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>COLACINO</last-name><first-name>Fabio Vincenzo</first-name><address><city>Anzio (RM)</city><country>IT</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>IANNICOLA</last-name><first-name>Ilaria</first-name><address><city>Latina (LT)</city><country>IT</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A method and a system for processing an audio stream are described, wherein at least one database of classified voices and at least one database of classified background sounds are provided and a comparison between these classified voices and background sounds with the voices and the sounds extrapolated from a suitably re-processed audio stream is carried out in order to identify possible matches.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="85.68mm" wi="158.75mm" file="US20230005479A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="207.60mm" wi="128.52mm" orientation="landscape" file="US20230005479A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="222.33mm" wi="143.43mm" orientation="landscape" file="US20230005479A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="201.51mm" wi="136.48mm" orientation="landscape" file="US20230005479A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="201.51mm" wi="136.48mm" orientation="landscape" file="US20230005479A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">BENEFIT CLAIM</heading><p id="p-0002" num="0001">This application claims the benefit under 35 U.S.C. 119 of Italy patent application 102021000017513, filed Jul. 2, 2021, the entire contents of which are hereby incorporated by reference for all purposes as if fully set forth herein</p><heading id="h-0002" level="1">BACKGROUND</heading><heading id="h-0003" level="1">Technical Field</heading><p id="p-0003" num="0002">The present disclosure relates to a method for processing an audio stream and a corresponding system.</p><p id="p-0004" num="0003">The disclosure relates in particular, but not exclusively, to a method for processing an audio stream for the recognition of voices and/or background sounds, and the following description is made with reference to this field of application for the sole purpose of simplifying the disclosure thereof.</p><heading id="h-0004" level="1">Description of the Related Art</heading><p id="p-0005" num="0004">The approaches described in this section are approaches that could be pursued, but not necessarily approaches that have been previously conceived or pursued. Therefore, unless otherwise indicated, it should not be assumed that any of the approaches described in this section qualify as prior art merely by virtue of their inclusion in this section.</p><p id="p-0006" num="0005">As is well known, voice biometrics is a technology that allows people to be recognized through their voice.</p><p id="p-0007" num="0006">This technology is increasingly being used thanks to the latest developments in multimedia data processing, which have led to the creation of hardware and software tools capable of handling large amounts of such data very quickly.</p><p id="p-0008" num="0007">In particular, of great interest in this area are the so-called &#x201c;smart conversational systems&#x201d;, able to obtain information starting from a phone contact thanks to the biometric recognition of the voice and the consequent identification of people through the voice.</p><p id="p-0009" num="0008">It is possible to use such voice identification in business to increase the level of personalization of the services delivered over the phone, e.g. through the so-called call or contact centers, reducing the time that is normally spent at the beginning of the contact to collect caller's data, thereby improving the overall customer experience.</p><p id="p-0010" num="0009">Voice biometrics can be also used in the &#x201c;security&#x201d; field to facilitate physical access to gates, e.g. of controlled areas such as a police station, or to allow computer access to programs or Internet platforms, to create voice signatures with which to sign documents or authorize financial transactions, or even to allow access to personal data such as health data or confidential information by the public administration, with guaranteed security of access and with respect for the privacy of the data of the users involved. The main advantage of voice biometrics consists in that it is difficult to be counterfeited and can easily be combined with other recognition factors, thus increasing the level of security that can be achieved.</p><p id="p-0011" num="0010">The development of solutions using the identification of a person by voice in such diverse fields has also made increasingly sophisticated software available for processing and handling multimedia data, in particular comprising sounds, also referred to as audio files or streams.</p><p id="p-0012" num="0011">Some of this software is also used in the legal field for the management of interceptions, whether by phone or environmental, which however suffer greatly from the lack of sharpness of the collected sounds and the presence of background sounds.</p><heading id="h-0005" level="1">BRIEF SUMMARY</heading><p id="p-0013" num="0012">The method for processing an audio stream is able to correctly recognize the voices and/or background sounds contained in the audio stream, overcoming the limitations and drawbacks that still afflict methods according to the prior art. According to an aspect of the disclosure, at least one database of classified voices and at least one database of classified background sounds are provided and a comparison between these classified voices and background sounds with the voices and the sounds extrapolated from a suitably re-processed audio stream is carried out in order to identify possible matches.</p><p id="p-0014" num="0013">The method for processing an audio stream may comprise the steps of:<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0000">    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0014">receiving an audio stream signal;</li>        <li id="ul0002-0002" num="0015">providing at least one database comprising voice models and/or background sound models classified based on at least one characteristic parameter of model signals;</li>        <li id="ul0002-0003" num="0016">processing the audio stream signal by dividing it in a plurality of audio frames classified in a plurality of voice frames and in a plurality of background sound frames;</li>        <li id="ul0002-0004" num="0017">extracting the characteristic parameter from the plurality of voice frames and from the plurality of background sound frames;</li>        <li id="ul0002-0005" num="0018">comparing the characteristic parameters of said voice frames and of background sound frames contained in the audio stream signal with the classified voice models and/or background sound models contained in the database; and</li>        <li id="ul0002-0006" num="0019">generating a result comprising at least one matching percentage of the voice frames and the background sound frames with one or more voice models and/or background sound models of the database.</li>    </ul>    </li></ul></p><p id="p-0015" num="0020">According to another aspect of the disclosure, the step of processing audio stream signal may use at least one voice recognition algorithm for classifying the voice frames and the background sound frames, one frame containing both voice and background sound being preferably classified as a voice frame.</p><p id="p-0016" num="0021">Furthermore, according to a further aspect of the disclosure, the characteristic parameter extracted from the frames can be the MEL and the step of extracting generates numeric arrays corresponding to the voice frames and background sound frames extracted from the audio stream signal, which are compared to corresponding numeric arrays of the classified voice models and classified background sound models stored in the database.</p><p id="p-0017" num="0022">According to another aspect of the disclosure, the method may further comprise a step of generating an output signal following the step of generating the result, said output signal preferably comprising a graphic representation of the at least one matching percentage comprised in the result and possibly the audio frames which were extracted and possibly processed by the audio stream signal.</p><p id="p-0018" num="0023">The method may also further comprise a step of pre-processing the audio stream signal, preferably adapted to normalize said signal by equalizing the volume thereof, with suitable increases and decreases based on the amplitude of the signal itself, said step of pre-processing preceding said step of processing and subdividing the audio stream signal into frames.</p><p id="p-0019" num="0024">Furthermore, the method may comprise a step of post-processing the voice frames and the background sound frames extracted from the audio stream signal wherein the frequencies of the background sound frames are subtracted from the voice frames, said step of post-processing preceding the step of extracting the characteristic parameter.</p><p id="p-0020" num="0025">According to another aspect of the disclosure, the step of providing at least one database may in turn comprise the steps of:<ul id="ul0003" list-style="none">    <li id="ul0003-0001" num="0000">    <ul id="ul0004" list-style="none">        <li id="ul0004-0001" num="0026">receiving a model audio signal, corresponding to a voice or a background sound of interest;</li>        <li id="ul0004-0002" num="0027">dividing the model audio signal in a plurality of voice frames or background sound frames;</li>        <li id="ul0004-0003" num="0028">eliminating frames which are not compatible with said model audio signal;</li>        <li id="ul0004-0004" num="0029">extracting the characteristic parameter of the identified frames and creating the classified voice model or the classified background sound model; and</li>        <li id="ul0004-0005" num="0030">storing the classified model in the at least one database.</li>    </ul>    </li></ul></p><p id="p-0021" num="0031">According to another aspect of the disclosure, the step of creating a voice model or background sound model can be carried out by a neuronal model.</p><p id="p-0022" num="0032">Furthermore, the method can use a platform of Machine Learning and a voice recognition model which is trained based on the characteristics of the model signals subjected to training.</p><p id="p-0023" num="0033">A system for processing an audio stream is also provided, the system comprising:<ul id="ul0005" list-style="none">    <li id="ul0005-0001" num="0000">    <ul id="ul0006" list-style="none">        <li id="ul0006-0001" num="0034">a separation block adapted to receive an audio stream signal and divide it in a plurality of audio frames classified as appropriately separated voice frames and background sound frames;</li>        <li id="ul0006-0002" num="0035">a prediction and classification block adapted to receive the voice frames and the background sound frames and to extract at least one characteristic parameter therefrom; and</li>        <li id="ul0006-0003" num="0036">a storage system of classified audio signal models, comprising at least one database adapted to store classified voice models and/or classified background sound models,</li>    </ul>    </li></ul></p><p id="p-0024" num="0037">such a storage system being connected to the prediction and classification block which carries out a comparison of the characteristic parameters of the voice frames and of the background sound frames contained in the audio stream signal with the classified voice models and/or classified background sound models stored in the database and generates a result comprising at least one matching percentage of the voice frames and/or the background sound frames with one or more voice models and/or background sound models of the database.</p><p id="p-0025" num="0038">According to an aspect of the disclosure, the separation block may use at least one voice recognition algorithm for classifying voice frames and the background sound frames, one frame containing both voice and background sound being preferably classified as a voice frame.</p><p id="p-0026" num="0039">Additionally, the prediction and classification block may extract the characteristic parameter MEL from the voice frames and from the background sound frames and generate numeric arrays corresponding to the voice frames and to the background sound frames, and the voice models and/or background sound models of said database may comprise corresponding numeric arrays tied to the characteristic parameter MEL of model signals used for creating the voice models and/or the background sound models.</p><p id="p-0027" num="0040">The system may also comprise a generation block of an output signal, comprising a graphic representation of the at least one matching percentage comprised in the result and possibly the audio frames which were extracted and possibly processed by the audio stream signal.</p><p id="p-0028" num="0041">According to another aspect of the disclosure, the system may further comprise a pre-processing block of the audio stream signal adapted to normalize said audio stream signal to equalize the volume thereof, with suitable increases and decreases based on the amplitude of the signal itself, before providing it to the separation block.</p><p id="p-0029" num="0042">According to another aspect of the disclosure, the system may further comprise a post-processing block of the voice frames and of the background sound frames extracted from the audio stream signal by the separation block, said post-processing block subtracting the frequencies of the background sound frames from the voice frames before providing said frames to the prediction and classification block.</p><p id="p-0030" num="0043">Furthermore, according to another aspect of the disclosure, the system may comprise a recognition and classification system of at least one model audio signal, corresponding to a voice or to a background sound of interest, in turn including:<ul id="ul0007" list-style="none">    <li id="ul0007-0001" num="0000">    <ul id="ul0008" list-style="none">        <li id="ul0008-0001" num="0044">a processing block, which receives the model audio signal and decomposes it in a plurality of voice frames or of background sound frames, eliminating the frames which are not compatible with the model audio signal; and</li>        <li id="ul0008-0002" num="0045">a modeling block adapted to extract the characteristic parameter from the frames generated by the processing block and create the classified voice or background sound model, to be stored in the database.</li>    </ul>    </li></ul></p><p id="p-0031" num="0046">According to this aspect of the disclosure, the modeling block of the recognition and classification system can be based on a neuronal model.</p><p id="p-0032" num="0047">Furthermore, such a modeling block of the recognition and classification system can extract the characteristic parameter MEL and generate a classified voice model or classified background sound model in the form of an array of numeric values, processed by Machine Learning algorithms.</p><p id="p-0033" num="0048">The recognition and classification system may also comprise a pre-processing block, which receives the model audio signal and carries out the normalization thereof by equalizing the volume thereof before providing it to the processing block.</p><p id="p-0034" num="0049">Finally, according to yet another aspect of the disclosure, the audio stream signal can be obtained by an environmental interception.</p><p id="p-0035" num="0050">The characteristics and advantages of the method and system according to the disclosure will become clear from the description, made below, of an embodiment thereof, given by way of non-limiting example with reference to the attached drawings.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0006" level="1">BRIEF DESCRIPTION OF THE SEVERAL VIEWS OF THE DRAWINGS</heading><p id="p-0036" num="0051"><figref idref="DRAWINGS">FIG. <b>1</b></figref>: schematically shows a possible application to an environmental interception of a system for processing an audio stream according to the present disclosure;</p><p id="p-0037" num="0052"><figref idref="DRAWINGS">FIG. <b>2</b></figref>: shows a system for processing an audio stream implementing the method according to the present disclosure used in the application of <figref idref="DRAWINGS">FIG. <b>1</b></figref>; and</p><p id="p-0038" num="0053"><figref idref="DRAWINGS">FIGS. <b>3</b>A and <b>3</b>B</figref>: show recognition and classification systems for creating databases comprising classified voices and classified background sounds, respectively, used by the system of <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0007" level="1">DETAILED DESCRIPTION</heading><p id="p-0039" num="0054">With reference to these figures, and in particular to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, a system for processing an audio stream according to the present disclosure is indicated as a whole with <b>10</b>, in the exemplary case of an application to an environmental interception.</p><p id="p-0040" num="0055">It should be noted that the figures represent schematic views of the system according to the disclosure and of the elements thereof and are not drawn to scale, but are instead drawn in such a way as to emphasize the important features of the disclosure.</p><p id="p-0041" num="0056">Furthermore, the elements that make up the illustrated system are only shown schematically.</p><p id="p-0042" num="0057">Finally, the different aspects of the disclosure represented by way of example in the figures are obviously combinable with each other and interchangeable from one embodiment to another.</p><p id="p-0043" num="0058">In particular, <figref idref="DRAWINGS">FIG. <b>1</b></figref> shows the use of the system <b>10</b> for processing an audio stream when an audio stream signal FA is derived from an environmental interception. In this case, the audio stream signal FA comprises the sounds being in an environment <b>2</b>, such as a room as illustrated in the figure, and is detected thanks to an audio detection system <b>3</b> that generates an audio stream signal FA.</p><p id="p-0044" num="0059">In the example illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the audio detection system <b>3</b> comprises a plurality of audio detection microdevices <b>4</b> arranged within the environment <b>2</b>, such as miniaturized microphones, in particular suitably hidden and/or positioned at points of acoustical interest. The audio detection system <b>3</b> may further comprise one or more remote audio detection devices, such as a directional microphone <b>5</b>, suitably arranged to detect sounds from the environment <b>2</b>, as shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0045" num="0060">Obviously, it is also possible to consider an audio detection system <b>3</b> comprising various audio detection devices, chosen for example from a telephone, whether fixed or mobile, or a microphone integrated therein, a video camera provided with a microphone, a microphone integrated in a computer or in another hardware device such as a tablet or PDA device, an entertainment system for a home or a car, other types of microphone that may be placed in the environment <b>2</b> or capable of carrying out remote detections, however generating an audio stream signal FA.</p><p id="p-0046" num="0061">Similarly, it is possible to use the system <b>10</b> for processing an audio stream signal on an audio stream signal FA detected from an environment <b>2</b> other than a room, such as a private enclosed place, like an entire flat, a shed or a work environment, a public enclosed place, such as a public building, a hotel or a museum, or an open, public or private place, such as a garden, a street, a square or a car park, only naming a few.</p><p id="p-0047" num="0062">Suitably according to the present disclosure, the audio stream signal FA is transmitted, by means of a signal transceiver device <b>6</b>, such as a router, to an audio stream processing system <b>10</b>, adapted to suitably process the audio stream signal FA, as will be described in greater detail below with reference to <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0048" num="0063">The signal transceiver device <b>6</b> can also comprise storage means <b>7</b> adapted to store one or more audio stream signals FA prior to their transmission, and possibly timing means <b>8</b> capable of synchronizing the transmission of the stored audio stream signal(s) FA, e.g. according to predetermined and possibly modifiable timings.</p><p id="p-0049" num="0064">Referring to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the system <b>10</b> for processing an audio stream receives as input an audio stream signal FA to be processed, also referred to as input signal IN. Such an audio stream signal FA may derive for example from an environmental interception, like in the example shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0050" num="0065">The system <b>10</b> for processing an audio stream comprises at least a first block <b>11</b> of pre-processing the audio stream signal FA received as input, adapted to generate a pre-processed audio stream signal FAPRE. In particular, the first pre-processing block <b>11</b> is adapted to normalize the audio stream signal FA in order to equalize the volume thereof, with increases and decreases based on the amplitude of the signal, bringing possible peaks back to the same unit of measurement and thus making the voices or background sound contained therein more intelligible.</p><p id="p-0051" num="0066">It is also possible to use the first pre-processing block <b>11</b> to perform other processing of the audio stream signal FA, e.g. filtering operations to eliminate any frequencies that are not of interest. Such operations for pre-processing the audio stream signal FA, while extremely useful, can be avoided, for example, in the case of signals with constant volume, and are therefore optional.</p><p id="p-0052" num="0067">Suitably, the system for processing the audio stream <b>10</b> further comprises a second block <b>12</b> of separating the audio stream signal FA, adapted to receive the pre-processed signal FAPRE and divide it in a plurality of elementary units or audio frames; said second separation block <b>12</b> further identifying which frames belong to a voice signal and which frames belong to a background sound signal, classifying them as appropriately separated voice frames V* and background sound frames SF*. Obviously, in the event that the audio stream signal FA is not pre-processed, the second separation block <b>12</b> is able to operate directly on this audio stream signal FA, suitably provided to it as input, while still obtaining separated voice frames V* and background sound frames SF*.</p><p id="p-0053" num="0068">This second separation block <b>12</b> uses at least one voice recognition algorithm for identifying the voice frames V* and the background sound frames SF*. Conventionally, an audio frame that contains both voice and background sound is classified as a voice frame V*, which substantially prevails over the background sound.</p><p id="p-0054" num="0069">Suitably, the second separation block <b>12</b> may also eliminate the silence frames, i.e. comprising neither voice nor background sound, optimizing the process as a whole. In particular, silence frames are classified as such when the background sound, normally always present, is below a predetermined threshold.</p><p id="p-0055" num="0070">The system <b>10</b> for processing an audio stream further comprises a third block <b>13</b> of post-processing the voice frames V* and the background sound frames SF* received from the second separation block <b>12</b>, said third post-processing block <b>13</b> being adapted to generate corresponding pluralities of voice frames V and of background sound frames SF further processed.</p><p id="p-0056" num="0071">Specifically, the third post-processing block <b>13</b> performs a subtraction of the frequencies of the background sound frames SF* from those that are the voice frames V*, thus cleaning the voice frames from the background sounds, if any, in a phase that is commonly referred to as Noise Reduction. This post-processing operation is optional, since the system may not comprise any third post-processing block <b>13</b> in the case, for example, of an audio stream signal FA with very little background sound, as might be the case with recordings made in quiet environments.</p><p id="p-0057" num="0072">Advantageously according to the present disclosure, the system <b>10</b> for processing an audio stream also comprises a prediction and classification block <b>14</b>, connected to the third post-processing block <b>13</b> from which it receives the voice frames V and the background sound frames SF further processed, in particular cleaned up as explained above. Appropriately, in the event that no post-processing operation is performed, the prediction and classification block <b>14</b> would receive the voice frames V* and the background sound frames SF* directly from the second separation block <b>12</b>.</p><p id="p-0058" num="0073">The prediction and classification block <b>14</b> initially performs the extraction of at least one characteristic parameter of audio frames, preferably the so-called MEL (Spectrogram Frequency), in particular an array of values obtained from the transformation of an audio frame from the time scale to the frequency scale by means of the mathematical formula of the Fourier transform.</p><p id="p-0059" num="0074">In particular, the prediction and classification block <b>14</b> is connected to a system <b>20</b> for storing classified audio signal models, comprising at least a first database DB<b>1</b> adapted to store a plurality of numeric arrays, corresponding to a series of characteristic parameters of suitable model or sample voice signals, referred to as classified voice models VCLm, and a second database DB<b>2</b> adapted to store a plurality of numeric arrays, corresponding to a series of characteristic parameters of suitable model or sample background sound signals, referred to as classified background sound models SFCLm, as will be further described below; such classified voice models VCLm and classified background sound models SFCLm are suitably sent to the prediction and classification block <b>14</b>. Preferably, the first database DB<b>1</b> and the second database DB<b>2</b> comprise numeric arrays with the values of MEL of the respective model signals.</p><p id="p-0060" num="0075">The prediction and classification block <b>14</b> then carried out a comparison between arrays of numeric values corresponding to the plurality of voice frames V*, V and background sound frames SF*, SF detected and possibly re-processed starting from the audio stream signal FA, as explained above, with arrays of numeric values corresponding to classified voice models VCLm and classified background sound models SFCLm providing a matching percentage (or score), which allows the most probable matches among the signals involved to be predicted.</p><p id="p-0061" num="0076">In this way, the prediction and classification block <b>14</b> is able to verify the voice frames V*, V and background sound frames SF*, SF extracted from the audio stream signal FA and possibly processed to detect a match with models being in the databases DB<b>1</b> and DB<b>2</b> and to provide a result RES, i.e. the voices and the sounds identified in the audio stream signal FA with the probability percentages of matching with respective models, in addition to the re-processed audio files comprising the frames on the basis of which the result RES was generated.</p><p id="p-0062" num="0077">Finally, the system <b>10</b> for processing an audio stream comprises a fifth block <b>15</b> for generating an output signal REPORT, comprising in graphic form the matching percentages between the voices and the background sounds identified in the processed audio stream signal FA and those stored on the basis of model or sample signals, possibly also attaching the re-processed audio files.</p><p id="p-0063" num="0078">The output signal REPORT may comprise, for example, all the detected voices with their percentages or only the detection of one or more voices of interest, or even a grouping of voices based on a background sound of interest. In particular, advantageously according to the present disclosure, having classified the background sound signals, it is possible to use them to identify groups of voices that have the same background sound signal; furthermore, thanks to the classification of the background sound signals, it is also possible to perform a kind of geolocalization of voice signals precisely on the basis of these background sound signals.</p><p id="p-0064" num="0079">The classified voice models VCLm and the classified background sound models SFCLm are obtained thanks to a recognition and classification system <b>30</b>, illustrated schematically in <figref idref="DRAWINGS">FIGS. <b>3</b>A and <b>3</b>B</figref>, for the voice and background sound signals, respectively. Suitably, the different processing to which the model signals are subjected essentially correspond to those applied to the audio stream signal FA to be processed, so as to be able to obtain characteristic parameters, in particular arrays of numeric values, actually comparable among them.</p><p id="p-0065" num="0080">In a preferred embodiment of the disclosure, the recognition and classification system <b>30</b> is based on a neuronal model.</p><p id="p-0066" num="0081">Suitably, as illustrated in <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>, the voice recognition and classification system <b>30</b> may receive a model or sample audio signal SA<b>1</b><i>m</i>, in particular tied to a voice of interest.</p><p id="p-0067" num="0082">The recognition and classification system <b>30</b> comprises a first pre-processing block <b>31</b>, which receives the model audio signal SA<b>1</b><i>m </i>and performs the normalization thereof, providing a pre-processed signal SA<b>1</b><i>m</i>PRE to a second processing block <b>32</b>, which decomposes it in a plurality of audio frames and separates the voice frames and the background sound frames, in addition to the silence frames; suitably, the background sound frames and possibly the silence frames are then eliminated, so as to filter out unnecessary data. The audio stream is then divided in a plurality of frames with equal duration, for example equal to 3 seconds, obtaining a plurality of voice frames, referred to as a signal SAVm. Also in this case, the operations for pre-processing the model audio signal SA<b>1</b><i>m </i>may be optional, the second processing block <b>32</b> being able to directly decompose said model audio signal SA<b>1</b><i>m. </i></p><p id="p-0068" num="0083">Appropriately, the recognition and classification system <b>30</b> further comprises a third modeling block <b>33</b>, which is able to extract a characteristic parameter from the frames present in the signal SAVm, namely the parameter MEL In this way, the third modeling block <b>33</b> obtains an array of numeric values, which in fact constitute the classified voice model VCLm, processed thanks to Machine Learning algorithms.</p><p id="p-0069" num="0084">Additionally, the third modelling block <b>33</b> stores the classified voice model VCLm in the first database DB<b>1</b> of the classified audio signal storage system <b>20</b>.</p><p id="p-0070" num="0085">Similarly, as illustrated in <figref idref="DRAWINGS">FIG. <b>3</b>B</figref>, the voice recognition and classification system <b>30</b> may receive a model or sample audio signal SA<b>2</b><i>m </i>tied to a background sound.</p><p id="p-0071" num="0086">In such a case, the first (however optional) pre-processing block <b>31</b> performs the normalization of the model audio signal SA<b>2</b><i>m </i>and provides a processed signal SA<b>2</b><i>m</i>PRE to the second processing block <b>32</b>, which in turn decomposes it in a plurality of audio frames and separates the voice frames and the background sound frames, in addition to the silence frames; appropriately, the voice frames and the silence frames are then eliminated, so as to filter out superfluous data and obtain a plurality of background sound frames, referred to as the signal SASFm, for the third modeling block <b>33</b>.</p><p id="p-0072" num="0087">Furthermore, the third modeling block <b>33</b> re-processes the signal SASFm, in particular by extracting again the parameter MEL of the frames composing it, and obtains a classified background sound model SFCLm adapted to be stored in the second database DB<b>2</b> of the system <b>20</b> for storing classified audio signals.</p><p id="p-0073" num="0088">Appropriately, the system <b>10</b> for processing an audio stream is thus able to recognize a voice or background sound by comparing it to a classified neuronal model of voices and background sounds.</p><p id="p-0074" num="0089">The present disclosure also refers to a method for processing an audio stream adapted to obtain a classification of the sounds contained therein, implemented by the system <b>10</b> for processing an audio stream described above.</p><p id="p-0075" num="0090">Specifically, this method for processing an audio stream comprises the steps of:<ul id="ul0009" list-style="none">    <li id="ul0009-0001" num="0000">    <ul id="ul0010" list-style="none">        <li id="ul0010-0001" num="0091">receiving an audio stream signal FA;</li>        <li id="ul0010-0002" num="0092">providing at least one database DB<b>1</b>, DB<b>2</b> comprising voice models VCLm/or background sound models SFCLm classified based on at least one characteristic parameter of model signals;</li>        <li id="ul0010-0003" num="0093">processing the audio stream signal FA by dividing the same in a plurality of audio frames classified in a plurality of voice frames V*, V and in a plurality of background sound frames SF*, SF;</li>        <li id="ul0010-0004" num="0094">extracting said characteristic parameter from the plurality of voice frames V*, V and from the plurality of background sound frames SF*, SF;</li>        <li id="ul0010-0005" num="0095">comparing the characteristic parameters of the voice frames V*, V and of the background sound frames SF*, SF contained in the audio stream signal FA with the classified voice models VCLm or classified background sound models SFCLm contained in the database DB<b>1</b>, DB<b>2</b>; and</li>        <li id="ul0010-0006" num="0096">generating a result RES comprising one matching percentage of the voice frames V*, V and the background sound frames SF*, SF with one or more classified voice models VCLm and/or background sound SFCLm.</li>    </ul>    </li></ul></p><p id="p-0076" num="0097">Appropriately, the step of processing the audio stream signal FA uses at least one voice recognition algorithm for classifying the voice frames V*, V and the background sound frames SF*, SF. Preferably, when a frame contains both voice and background sound, it is still classified as a voice frame V*, V.</p><p id="p-0077" num="0098">In a preferred embodiment, the characteristic parameter extracted from the signals is the MEL and the extraction step generates numeric arrays corresponding to the voice frames V*, V and the background sound frames SF*, SF, which are compared with corresponding numeric arrays of the models stored in the databases DB<b>1</b>, DB<b>2</b>, these array of values being obtained by transforming an audio frame from the time scale to the frequency scale, using the mathematical formula of the Fourier transform.</p><p id="p-0078" num="0099">Suitably, the method may also comprise a final step of generating an output signal REPORT comprising a graphic representation of the matching percentages comprised in the result RES and possibly the audio frames that were extracted and processed from the audio stream signal FA. The output signal REPORT can comprise other ways of aggregating the values comprised in the result RES, e.g. providing only the model for voice or background sound that has the highest percentage, or all models that have a percentage above a pre-set threshold.</p><p id="p-0079" num="0100">Appropriately, the method for processing an audio stream may also comprise at least one step of pre-processing the audio stream signal FA, preferably adapted to normalize said audio stream signal FA by equalizing the volume thereof, with suitable increases and decreases based on the amplitude of the signal itself, said step of pre-processing preceding the step of processing and subdividing the audio stream signal FA into frames.</p><p id="p-0080" num="0101">The method for processing an audio stream may also comprise a step of post-processing the voice frames V*, V and the background sound frames SF*, SF extracted from the audio stream signal FA, wherein the frequencies of the background sound frames SF*, SF are subtracted from the voice frames V*, V, obtaining a cleaning of the voice frames V* in a so-called Noise Reduction operation.</p><p id="p-0081" num="0102">Appropriately, the step of providing at least one database DB<b>1</b>, DB<b>2</b> comprises in particular the following steps of:<ul id="ul0011" list-style="none">    <li id="ul0011-0001" num="0000">    <ul id="ul0012" list-style="none">        <li id="ul0012-0001" num="0103">receiving a model audio signal SA<b>1</b><i>m</i>, SA<b>2</b><i>m </i>corresponding to a voice or a background sound of interest;</li>    </ul>    </li></ul></p><p id="p-0082" num="0104">dividing the model audio signal SA<b>1</b><i>m</i>, SA<b>2</b><i>m </i>in a plurality of voice frames or background sound frames;<ul id="ul0013" list-style="none">    <li id="ul0013-0001" num="0000">    <ul id="ul0014" list-style="none">        <li id="ul0014-0001" num="0105">eliminating the frames which are not compatible with the model audio signal SA<b>1</b><i>m</i>, SA<b>2</b><i>m</i>, i.e. eliminating the background sound frames in the case of a model audio signal SA<b>1</b><i>m </i>tied to a voice and eliminating the voice frames in the case of a model audio signal SA<b>2</b><i>m </i>tied to a background sound signal;</li>        <li id="ul0014-0002" num="0106">extracting a characteristic parameter from the identified frames and creating a classified voice model VCLm or a classified background sound model SFCLm; and</li>        <li id="ul0014-0003" num="0107">storing the classified model VCLm, SFCLm in a database DB<b>1</b>, DB<b>2</b>.</li>    </ul>    </li></ul></p><p id="p-0083" num="0108">In a preferred embodiment, the step of creating a voice model or background sound model is carried out by a neuronal model.</p><p id="p-0084" num="0109">Appropriately, the step of extracting the characteristic parameter from the frames identified in the model signal comprises a step of extracting the parameter MEL and the step of creating the model comprises the creation of an array of numeric values.</p><p id="p-0085" num="0110">Additionally, a step of pre-processing the model signal prior to the separation thereof into frames can be envisaged, e.g. a normalization of this model signal by making the volume thereof uniform.</p><p id="p-0086" num="0111">As explained above, such classified voice models VCLm and classified background sound models SFCLm being in the database DB<b>1</b>, DB<b>2</b> are used in the step of comparing the voice frames V*, V or background sounds SF*, SF contained in the audio stream signal FA in the method for processing an audio stream according to the present disclosure.</p><p id="p-0087" num="0112">In a preferred embodiment, the method uses a platform of Machine Learning and a model on which recognition is implemented, which is trained on the basis of the characteristics of the samples subjected to training.</p><p id="p-0088" num="0113">More in particular, an audio sampling with frames of a predetermined minimum duration (equal to for example one minute) is performed on voices or background sounds of interest.</p><p id="p-0089" num="0114">It is also possible to use one or more of the following parameters as a characteristic parameter extracted from the frames for comparison via audio processing libraries:<ul id="ul0015" list-style="none">    <li id="ul0015-0001" num="0000">    <ul id="ul0016" list-style="none">        <li id="ul0016-0001" num="0115">MFCC (Mel Frequency Cepstral Coefficient) features extraction: time-dependent calculation of the vocal spectrum power;</li>        <li id="ul0016-0002" num="0116">Chroma: the pitch classes of the sounds;</li>        <li id="ul0016-0003" num="0117">Phonetic contrast: the minimal phonetic distinction between one pronunciation and another (for example P and B) in the language; and</li>        <li id="ul0016-0004" num="0118">Tonnetz: the tonal space of the sounds.</li>    </ul>    </li></ul></p><p id="p-0090" num="0119">Advantageously, therefore, thanks to the system for processing an audio stream according to the present disclosure, if a recording of a sample voice or a voice of interest is in the model or sample audio signal, it will be detected whenever an audio stream signal FA comprising that voice is processed.</p><p id="p-0091" num="0120">Similarly, advantageously, the system for processing an audio stream according to the present disclosure makes it possible to extend the recognition to all voices having a certain background sound in common, always identified on the basis of a model or sample audio signal tied to that background sound.</p><p id="p-0092" num="0121">It is emphasized that, advantageously in the method and in the system according to the present disclosure, the background sound, normally eliminated from the audio stream signals in the current voice recognition techniques, is instead used as an additional unit of information that makes it possible, for example, to aggregate voices that are even not in the sample voice models due to the presence of a background sound that is instead recognized.</p><p id="p-0093" num="0122">From the foregoing it will be appreciated that, although specific embodiments of the disclosure have been described herein for purposes of illustration, various modifications may be made without deviating from the spirit and scope of the disclosure.</p><p id="p-0094" num="0123">The various embodiments described above can be combined to provide further embodiments. These and other changes can be made to the embodiments in light of the above-detailed description. In general, in the following claims, the terms used should not be construed to limit the claims to the specific embodiments disclosed in the specification and the claims, but should be construed to include all possible embodiments along with the full scope of equivalents to which such claims are entitled. Accordingly, the claims are not limited by the disclosure.</p><p id="p-0095" num="0124">For example, it is possible to use the method and the system for analyzing audio files detected in real time or applying them to previously recorded files.</p><p id="p-0096" num="0125">Furthermore, other pitch classes can be envisaged, e.g. to distinguish repetitive noises from random noises or from possible disturbances in the transceiver line of the audio stream signal to be analyzed.</p><p id="p-0097" num="0126">Finally, it is possible to use the method for analyzing a plurality of audio stream signals, either simultaneously or sequentially, obtaining a single output signal that collectively illustrates the results of this analysis.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method for processing an audio stream comprising the steps of:<claim-text>receiving an audio stream signal;</claim-text><claim-text>providing at least one database comprising voice models and/or background sound models classified based on at least one characteristic parameter of model signals;</claim-text><claim-text>processing the audio stream signal by dividing it in a plurality of audio frames classified in a plurality of voice frames and in a plurality of background sound frames;</claim-text><claim-text>extracting the characteristic parameter from the plurality of voice frames and from the plurality of background sound frames;</claim-text><claim-text>comparing the characteristic parameters of the voice frames and of the background sound frames contained in the audio stream signal with the classified voice models and/or classified background sound models contained in the database; and</claim-text><claim-text>generating a result comprising at least one matching percentage of the voice frames and the background sound frames with one or more voice models and/or background sound models of the database.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the step of processing audio stream signal uses at least one voice recognition algorithm for classifying the voice frames and the background sound frames, a frame containing both voice and background sound being classified as a voice frame.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the characteristic parameter extracted from the frames is the MEL and wherein the step of extracting generates numeric arrays corresponding to the voice frames and the background sound frames extracted from the audio stream signal, which are compared to the corresponding numeric arrays of the classified voice models and classified background sound models stored in the database.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising a step of generating an output signal following the step of generating the result, the output signal comprising a graphic representation of the at least one matching percentage comprised in the result.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising a step of pre-processing the audio stream signal adapted to normalize the signal by equalizing the volume thereof, with suitable increases and decreases based on the amplitude of the signal itself, the step of pre-processing preceding the step of processing and subdividing the audio stream signal into frames.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising a step of post-processing the voice frames and the background sound frames extracted from the audio stream signal, wherein the frequencies of the background sound frames are subtracted from the voice frames, the step of post-processing preceding the step of extracting the characteristic parameter.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the step of providing at least one database in turn comprises the steps of:<claim-text>receiving a model audio signal, corresponding to a voice or a background sound of interest;</claim-text><claim-text>dividing the model audio signal in a plurality of voice frames or background sound frames;</claim-text><claim-text>eliminating frames which are not compatible with the model audio signal;</claim-text><claim-text>extracting the characteristic parameter of the identified frames and creating the classified voice model or the classified background sound model, respectively; and</claim-text><claim-text>storing the classified model in the at least one database.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the step of creating a voice model or background sound model is carried out by a neuronal model.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, using a platform of Machine Learning and a voice recognition model which is trained based on the characteristics of the model signals subjected to training.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. A system for processing an audio stream of the type comprising:<claim-text>a separation block adapted to receive an audio stream signal and divide it in a plurality of audio frames classified as appropriately separated voice frames and background sound frames;</claim-text><claim-text>a prediction and classification block adapted to receive the voice frames and the background sound frames and to extract at least one characteristic parameter therefrom; and</claim-text><claim-text>a storage system of classified audio signal models, comprising at least one database adapted to store classified voice models and/or classified background sound models,</claim-text><claim-text>the storage system being connected to the prediction and classification block which carries out a comparison of the characteristic parameters of the voice frames and of the background sound frames contained in the audio stream signal with the classified voice models and/or classified background sound models stored in the database and generates a result comprising at least one matching percentage of the voice frames and/or the background sound frames with one or more voice models and/or background sound models of the database.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the separation block uses at least one voice recognition algorithm for classifying the voice frames and the background sound frames, one frame containing both voice and background sound being classified as voice frame.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the prediction and classification block extracts the characteristic parameter MEL from the voice frames and from the background sound frames and generates numeric arrays corresponding to the voice frames and to the background sound frames and wherein the voice models and/or background sound models of the database comprise corresponding numeric arrays tied to the characteristic parameter MEL of model signals used for creating the voice models and/or the background sound models.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, further comprising a generation block of an output signal, comprising a graphic representation of the at least one matching percentage comprised in the result.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, further comprising a pre-processing block of the audio stream signal adapted to normalize the audio stream signal to equalize the volume thereof, with suitable increases and decreases based on the amplitude of the signal itself, before providing it to the separation block.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, further comprising a post-processing block of the voice frames and of the background sound frames extracted from the audio stream signal by the separation block, the post-processing block subtracting the frequencies of the background sound frames from the voice frames before providing the frames to the prediction and classification block.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, further comprising a recognition and classification system of at least one model audio signal, corresponding to a voice or to a background sound of interest, in turn including:<claim-text>a processing block, which receives the model audio signal and decomposes it in a plurality of voice frames or of background sound frames, eliminating the frames which are not compatible with the model audio signal; and</claim-text><claim-text>a modeling block adapted to extract the characteristic parameter from the frames generated by the processing block and create the classified voice or background sound model, to be stored in the database.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The system of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the modeling block of the recognition and classification system is based on a neuronal model.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The system of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the modeling block of the recognition and classification system extracts the characteristic parameter MEL and generates a classified voice or background sound model in the form of an array of numeric values, processed by Machine Learning algorithms.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The system of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the recognition and classification system further comprises a pre-processing block, which receives the model audio signal and carries out a normalization thereof by equalizing volume thereof before providing it to the processing block.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the audio stream signal is obtained by an environmental interception.</claim-text></claim></claims></us-patent-application>