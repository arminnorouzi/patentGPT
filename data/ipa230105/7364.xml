<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230007365A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230007365</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17366675</doc-number><date>20210702</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>21</main-group><subgroup>845</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>20</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>21</main-group><subgroup>8352</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>21</main-group><subgroup>466</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>21</main-group><subgroup>8456</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20190101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>20</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>21</main-group><subgroup>8352</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>21</main-group><subgroup>4662</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">Automated Content Segmentation and Identification of Fungible Content</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Disney Enterprises, Inc.</orgname><address><city>Burbank</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Farre Guiu</last-name><first-name>Miquel Angel</first-name><address><city>Bern</city><country>CH</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Martin</last-name><first-name>Marc Junyent</first-name><address><city>Barcelona</city><country>ES</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Pernias</last-name><first-name>Pablo</first-name><address><city>Sant Joan D&#x2019;Alacant</city><country>ES</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A content segmentation system includes a computing platform having processing hardware and a system memory storing a software code and a trained machine learning model. The processing hardware is configured to execute the software code to receive content, the content including multiple sections each having multiple content blocks in sequence, to select one of the sections for segmentation, and to identify, for each of the content blocks of the selected section, at least one respective representative unit of content. The software code is further executed to generate, using the at least one respective representative unit of content, a respective embedding vector for each of the content blocks of the selected section to provide a multiple embedding vectors, and to predict, using the trained machine learning model and the embedding vectors, subsections of the selected section, at least some of the subsections including more than one of the content blocks.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="221.32mm" wi="158.75mm" file="US20230007365A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="226.23mm" wi="165.44mm" file="US20230007365A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="226.48mm" wi="144.86mm" orientation="landscape" file="US20230007365A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="226.57mm" wi="164.25mm" orientation="landscape" file="US20230007365A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="182.88mm" wi="163.91mm" orientation="landscape" file="US20230007365A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="229.28mm" wi="149.61mm" orientation="landscape" file="US20230007365A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="226.91mm" wi="156.72mm" orientation="landscape" file="US20230007365A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">BACKGROUND</heading><p id="p-0002" num="0001">Due to its popularity as a content medium, ever more video in the form of episodic television (TV) and movie content is being produced and made available to consumers via streaming services. As a result, the efficiency with which segments of a video content stream having different bit-rate encoding requirements are identified has become increasingly important to the producers and distributors of that video content.</p><p id="p-0003" num="0002">Segmentation of video and other content has traditionally been performed manually by human editors. However, such manual segmentation of content is a labor intensive and time consuming process. Consequently, there is a need in the art for an automated solution for performing content segmentation that substantially minimizes the amount of content, such as audio content and video content, requiring manual processing.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0002" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0004" num="0003"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows a diagram of an exemplary system for performing content segmentation and identification of fungible content, according to one implementation;</p><p id="p-0005" num="0004"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows a diagram of an exemplary content segmentation software code suitable for use by the system in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, according to one implementation;</p><p id="p-0006" num="0005"><figref idref="DRAWINGS">FIG. <b>3</b></figref> depicts an exemplary process for generating embedding vectors, according to one implementation;</p><p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. <b>4</b>A</figref> depicts an initial stage of an exemplary process for predicting boundaries of content subsections, according to one implementation;</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>4</b>B</figref> depicts a subsequent stage of an exemplary process for predicting boundaries of content subsections, according to one implementation; and</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>5</b></figref> shows a diagram of an exemplary fungible content detection software code suitable for use by the system in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, according to one implementation.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0003" level="1">DETAILED DESCRIPTION</heading><p id="p-0010" num="0009">The following description contains specific information pertaining to implementations in the present disclosure. One skilled in the art will recognize that the present disclosure may be implemented in a manner different from that specifically discussed herein. The drawings in the present application and their accompanying detailed description are directed to merely exemplary implementations. Unless noted otherwise, like or corresponding elements among the figures may be indicated by like or corresponding reference numerals. Moreover, the drawings and illustrations in the present application are generally not to scale, and are not intended to correspond to actual relative dimensions.</p><p id="p-0011" num="0010">The present application discloses systems and computer-readable non-transitory storage media including instructions for performing content segmentation and identification of fungible content that overcome the drawbacks and deficiencies in the conventional art. In various implementations, the content processed using the systems and methods may include video games, movies, or episodic television (TV) content that includes episodes of TV shows that are broadcast, streamed, or otherwise available for download or purchase on the Internet or a user application. Moreover, in some implementations, the content segmentation and fungible content identification solution disclosed herein may advantageously be performed as an automated process.</p><p id="p-0012" num="0011">It is noted that, as used in the present application, the terms &#x201c;automation,&#x201d; &#x201c;automated,&#x201d; and &#x201c;automating&#x201d; refer to systems and processes that do not require the participation of a human user, such as a human editor or system administrator. For example, although in some implementations a human editor may confirm or reject a content segmentation boundary predicted by the present systems, that human involvement is optional. Thus, the processes described in the present application may be performed under the control of hardware processing components of the disclosed systems.</p><p id="p-0013" num="0012">It is further noted that, as used in the present application, the expression &#x201c;fungible content,&#x201d; also sometimes referred to as &#x201c;non-canon content&#x201d; or &#x201c;non-cannon content,&#x201d; refers to sections of content that are substantially the same from one version to another of content in which the fungible content is included. Examples of fungible content included in episodic TV content may include the introduction to the episode, credits, and short introductions following a commercial break, to name a few. In a movie, fungible content may include credits, as well as production and post-production short clips prior to the movie, while in a video game, fungible content may include short clips mentioning the game and its production and distribution companies.</p><p id="p-0014" num="0013">It is also noted that, as used in the present application, the expression &#x201c;computer-readable non-transitory storage medium,&#x201d; as defined in the present application, refers to any medium, excluding a carrier wave or other transitory signal that provides instructions to processing hardware of a computing platform. Thus, a computer-readable non-transitory medium may correspond to various types of media, such as volatile media and non-volatile media, for example. Volatile media may include dynamic memory, such as dynamic random access memory (dynamic RAM), while non-volatile memory may include optical, magnetic, or electrostatic storage devices. Common forms of computer-readable non-transitory storage media include, for example, optical discs, RAM, programmable read-only memory (PROM), erasable PROM (EPROM), and FLASH memory.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows exemplary system <b>100</b> for performing content segmentation and identification of fungible content, according to one implementation. As shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, system <b>100</b> includes computing platform <b>102</b> having processing hardware <b>104</b> and system memory <b>106</b> implemented as a computer-readable non-transitory storage medium. According to the present exemplary implementation, system memory <b>106</b> stores content segmentation software code <b>110</b>, fungible content detection software code <b>120</b>, and trained content subsection boundary prediction machine learning (ML) model <b>160</b>.</p><p id="p-0016" num="0015">Processing hardware <b>104</b> may include multiple hardware processing units, such as one or more central processing units, one or more graphics processing units, and one or more tensor processing units, one or more field-programmable gate arrays (FPGAs), custom hardware for machine-learning training or inferencing, and an application programming interface (API) server, for example. By way of definition, as used in the present application, the terms &#x201c;central processing unit&#x201d; (CPU), &#x201c;graphics processing unit&#x201d; (GPU), and &#x201c;tensor processing unit&#x201d; (TPU) have their customary meaning in the art. That is to say, a CPU includes an Arithmetic Logic Unit (ALU) for carrying out the arithmetic and logical operations of computing platform <b>102</b>, as well as a Control Unit (CU) for retrieving programs, such as content segmentation software code <b>110</b> and fungible content detection software code <b>120</b>, from system memory <b>106</b>, while a GPU may be implemented to reduce the processing overhead of the CPU by performing computationally intensive graphics or other processing tasks. A TPU is an application-specific integrated circuit (ASIC) configured specifically for artificial intelligence (AI) applications such as machine learning modeling.</p><p id="p-0017" num="0016">It is noted that, as defined in the present application, the expression &#x201c;machine learning model&#x201d; or &#x201c;ML model&#x201d; may refer to a mathematical model for making future predictions based on patterns learned from samples of data or &#x201c;training data.&#x201d; Various learning algorithms can be used to map correlations between input data and output data. These correlations form the mathematical model that can be used to make future predictions on new input data. Such a predictive model may include one or more logistic regression models, Bayesian models, or neural networks (NNs). Moreover, a &#x201c;deep neural network,&#x201d; in the context of deep learning, may refer to an NN that utilizes multiple hidden layers between input and output layers, which may allow for learning based on features not explicitly defined in raw data.</p><p id="p-0018" num="0017">As further shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, system <b>100</b> is implemented within a use environment including content provider <b>134</b> providing content <b>146</b> and comparison content <b>147</b>, content database <b>138</b>, communication network <b>130</b> and content editor <b>144</b> utilizing user system <b>140</b> including display <b>142</b>. Also shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref> are network communication links <b>132</b> communicatively coupling content provider <b>134</b>, content database <b>138</b>, and user system <b>140</b> with system <b>100</b> via communication network <b>130</b>, as well as manually segmented content sample <b>148</b> provided by content editor <b>144</b>, segmented content <b>152</b>, and fungible content identification data <b>154</b>. It is noted that although system <b>100</b> may receive content <b>146</b> from content provider <b>134</b> via communication network <b>130</b> and network communication links <b>132</b>, in some implementations, content provider <b>134</b> may take the form of a content source integrated with computing platform <b>102</b>, or may be in direct communication with system <b>100</b> as shown by dashed communication link <b>128</b>.</p><p id="p-0019" num="0018">Although the present application refers to content segmentation software code <b>110</b> as being stored in system memory <b>106</b> for conceptual clarity, more generally, system memory <b>106</b> may take the form of any computer-readable non-transitory storage medium, as described above. Moreover, although <figref idref="DRAWINGS">FIG. <b>1</b></figref> depicts content segmentation software code <b>110</b>, fungible content detection software code <b>120</b>, and trained content subsection boundary prediction ML model <b>160</b> as being stored together in system memory <b>106</b>, that representation is also provided merely as an aid to conceptual clarity. More generally, system <b>100</b> may include one or more computing platforms <b>102</b>, such as computer servers for example, which may be co-located, or may form an interactively linked but distributed system, such as a cloud-based system, for instance. As a result, processing hardware <b>104</b> and system memory <b>106</b> may correspond to distributed processor and system memory resources within system <b>100</b>. In one implementation, computing platform <b>102</b> of system <b>100</b> may correspond to one or more web servers, accessible over a packet-switched network such as the Internet, for example. Alternatively, computing platform <b>102</b> may correspond to one or more computer servers supporting a wide area network (WAN), a local area network (LAN), or included in another type of limited distribution or private network.</p><p id="p-0020" num="0019">Content segmentation software code <b>110</b>, executed by processing hardware <b>104</b>, may receive content <b>146</b> from content provider <b>134</b>, and may provide segmented content <b>152</b> corresponding to content <b>146</b> and produced using content subsection boundary prediction ML model <b>160</b> as an output in a substantially automated process. Alternatively, or in addition, in some implementations, content segmentation software code <b>110</b>, executed by processing hardware <b>104</b>, may receive manually segmented content sample <b>148</b> from user system <b>140</b>, and may utilize manually segmented content sample <b>148</b> as a template or recipe for adjusting one or more hyperparameters of content subsection boundary prediction ML model <b>160</b> so as to produce segmented content <b>152</b> according to editorial preferences of content editor <b>144</b>, also in a substantially automated process. It is noted that hyperparameters are configuration parameters of the architecture of content subsection boundary prediction ML model <b>160</b> that are adjusted during training, either full training or fine tuning. By way of example, the learning rate is a hyperparameter.</p><p id="p-0021" num="0020">In some implementations, manually segmented content sample <b>148</b> may include an unsegmented portion and a manually segmented portion provided as a template for completing segmentation of the unsegmented portion in an automated &#x201c;auto-complete&#x201d; process. That is to say, a content sample having a time duration of one hour, for example, may have only the first few minutes of content manually segmented by content editor <b>144</b>. Content segmentation software code <b>110</b>, executed by processing hardware <b>104</b>, may then adjust one or more hyperparameters of content subsection boundary prediction ML model <b>160</b> to complete segmentation of the unsegmented portion of manually segmented content sample <b>148</b> in accordance with the segmentation preferences of content editor <b>144</b>.</p><p id="p-0022" num="0021">Fungible content detection software code <b>120</b>, executed by processing hardware <b>104</b>, may receive content <b>146</b>, may obtain comparison content <b>147</b>, and may output fungible content identification data <b>154</b> identifying one or more sections of content <b>146</b> as fungible content sections. It is noted that, in various implementations, segmented content <b>152</b> and fungible content identification data <b>154</b>, when generated by system <b>100</b>, may be stored in system memory <b>106</b>, may be copied to non-volatile storage, or may be stored in system memory <b>106</b> and also be copied to non-volatile storage. Alternatively, or in addition, as shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, in some implementations, segmented content <b>152</b> and fungible content identification data <b>154</b> may be sent to user system <b>140</b> including display <b>142</b>, for example by being transferred via network communication links <b>132</b> of communication network <b>130</b>.</p><p id="p-0023" num="0022">Although user system <b>140</b> is shown as a desktop computer in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, that representation is provided merely as an example. More generally, user system <b>140</b> may be any suitable mobile or stationary computing device or system that implements data processing capabilities sufficient to provide a user interface, support connections to communication network <b>130</b>, and implement the functionality ascribed to user system <b>140</b> herein. For example, in some implementations, user system <b>140</b> may take the form of a laptop computer, tablet computer, or smartphone, for example. However, in other implementations user system <b>140</b> may be a &#x201c;dumb terminal&#x201d; peripheral component of system <b>100</b> that enables content editor <b>144</b> to provide inputs via a keyboard or other input device, as well as to view segmented content <b>152</b> and fungible content identification data <b>154</b> on display <b>142</b>. In those implementations, user system <b>140</b> and display <b>142</b> may be controlled by processing hardware <b>104</b> of system <b>100</b>.</p><p id="p-0024" num="0023">With respect to display <b>142</b> of user system <b>140</b>, display <b>142</b> may be physically integrated with user system <b>140</b> or may be communicatively coupled to but physically separate from user system <b>140</b>. For example, where user system <b>140</b> is implemented as a smartphone, laptop computer, or tablet computer, display <b>142</b> will typically be integrated with user system <b>140</b>. By contrast, where user system <b>140</b> is implemented as a desktop computer, display <b>142</b> may take the form of a monitor separate from user system <b>140</b> in the form of a computer tower. Moreover, display <b>142</b> may be implemented as a liquid crystal display (LCD), light-emitting diode (LED) display, organic light-emitting diode (OLED) display, quantum dot (QD) display, or any other suitable display screen that performs a physical transformation of signals to light.</p><p id="p-0025" num="0024">The functionality of content segmentation software code <b>110</b> and trained content subsection boundary prediction ML model <b>160</b> will be further described by reference to <figref idref="DRAWINGS">FIGS. <b>2</b>, <b>3</b>, <b>4</b>A, and <b>4</b>B</figref>, while the functionality of fungible content detection software code is further described below by reference to <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows a diagram of exemplary content segmentation software code <b>210</b> suitable for use by system <b>100</b>, in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, according to one implementation, together with trained content subsection boundary prediction ML model <b>260</b>. According to the exemplary implementation shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, content segmentation software code <b>210</b> includes content distribution module <b>211</b>, video encoder <b>262</b>, audio encoder <b>264</b>, and embedding vector combination module <b>266</b>. Also shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref> are content <b>246</b>, video component <b>212</b> of content <b>246</b>, audio component <b>213</b> of content <b>246</b>, video embedding vectors <b>214</b>, audio embedding vectors <b>215</b>, audio-video (AV) embedding vectors <b>216</b>, and segmented content <b>252</b>.</p><p id="p-0027" num="0026">It is noted that the exemplary implementation shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref> corresponds to use cases in which content <b>246</b> is AV content including both video and audio. However, the present novel and inventive concepts are equally applicable to video content without accompanying audio, and to audio content without accompanying video.</p><p id="p-0028" num="0027">Content <b>246</b>, content segmentation software code <b>210</b>, trained content subsection boundary prediction ML model <b>260</b>, and segmented content <b>252</b> correspond respectively in general to content <b>146</b>, content segmentation software code <b>110</b>, trained content subsection boundary prediction ML model <b>160</b>, and segmented content <b>152</b>, in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. Thus, content <b>146</b>, content segmentation software code <b>110</b>, trained content subsection boundary prediction ML model <b>160</b>, and segmented content <b>152</b> may share any of the characteristics attributed to respective content <b>246</b>, content segmentation software code <b>210</b>, trained content subsection boundary prediction ML model <b>260</b>, and segmented content <b>252</b> by the present disclosure, and vice versa. That is to say, although not shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, like content segmentation software code <b>210</b>, content segmentation software code <b>110</b> may include features corresponding to content distribution module <b>211</b>, video encoder <b>262</b>, audio encoder <b>264</b>, and embedding vector combination module <b>266</b>.</p><p id="p-0029" num="0028">Referring to <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>2</b></figref> in combination, in some implementations, content segmentation software code <b>110</b>/<b>210</b>, when executed by processing hardware <b>104</b> of computing platform <b>102</b>, is configured to receive content <b>146</b>/<b>246</b> including multiple sections each having multiple content blocks in sequence, and to select one of the sections for segmentation. In addition, when executed by processing hardware <b>104</b> content segmentation software code <b>110</b>/<b>210</b> is also configured to identify, for each of the content blocks included in the selected section, at least one representative unit of content, such as a single frame of video representative of a shot, and to generate, using only that at least one representative unit of content, a respective embedding vector for each of the content blocks of the selected section to provide multiple embedding vectors. Content segmentation software code <b>110</b>/<b>210</b> is further configured to predict, using trained content subsection boundary prediction ML model <b>160</b>/<b>260</b> and the embedding vectors, multiple subsections of the selected section, at least some of the subsections including more than one content block. It is noted that although it is possible to generate more than one embedding vector per content block, those multiple embeddings would typically be compressed to form a single representative embedding vector per content block. An example of this type of aggregation of embedding vectors would be using attention mechanisms, as known in the art.</p><p id="p-0030" num="0029">By way of example, where content <b>146</b>/<b>246</b> takes the form of AV content, such as episodic TV content or movie content, the sections included in content <b>146</b>/<b>246</b> may be acts, the content blocks included in each section may be shots of content, and the predicted subsections may be scenes of content. With respect to the expressions &#x201c;shot&#x201d; or &#x201c;shots,&#x201d; it is noted that, as used in the present application, the term &#x201c;shot&#x201d; refers to a sequence of frames within a video file that are captured from a unique camera perspective without cuts and/or other cinematic transitions. In addition, as used in the present application, the term &#x201c;scene&#x201d; refers to a series of related shots. For instance, a scene may include a series of shots that share a common background, or depict the same location. Thus, different scenes may include different numbers of shots, while different shots may include different numbers of video frames. Moreover, as used in the present application, the term &#x201c;act&#x201d; refers to a section of content that typically includes multiple scenes that together provide a coherent dramatic or narrative arc that includes the features of rising action, climax, and resolution.</p><p id="p-0031" num="0030">It is noted that the functionality attributed to content segmentation software code <b>110</b>/<b>210</b> above may be implemented to perform scene detection or location detection. Consequently, in some implementations, the content subsections predicted using content segmentation software code <b>110</b>/<b>210</b> and trained content subsection boundary prediction ML model <b>160</b>/<b>260</b> may each correspond to a different respective scene. However, in other implementations, the content subsections predicted using content segmentation software code <b>110</b>/<b>210</b> and trained content subsection boundary prediction ML model <b>160</b>/<b>260</b> may each correspond to a different respective location depicted by content <b>146</b>/<b>246</b>.</p><p id="p-0032" num="0031">According to the exemplary implementation shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, content segmentation software code <b>210</b> receives content <b>246</b> in the form of AV content including multiple sections each having video component <b>212</b> and audio component <b>213</b>. Content <b>246</b> may include a video game, a movie, or TV programming content including content broadcast, streamed, or otherwise downloaded or purchased on the Internet or a user application, for example. Content <b>246</b> may include a high-definition (HD) or ultra-HD (UHD) baseband video signal with embedded audio, captions, time code, and other ancillary metadata, such as ratings and parental guidelines. In some implementations, content <b>246</b> may be provided by content provider <b>134</b>, shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, such as a TV broadcast network or other media distribution entity (e.g., a movie studio, a streaming platform, etc.), utilizing secondary audio programming (SAP) or Descriptive Video Service (DVS), for example.</p><p id="p-0033" num="0032">Continuing to refer to <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>2</b></figref> in combination, content segmentation software code <b>110</b>/<b>210</b>, when executed by processing hardware <b>104</b> of computing platform <b>102</b>, uses content distribution module <b>211</b> to identify each section of content included in content <b>146</b>/<b>246</b>, and for each section, separate video component <b>212</b> from audio component <b>213</b>. Video component <b>212</b> is transferred to video encoder <b>262</b> for processing, while audio component <b>213</b> is transferred to audio encoder <b>264</b>. It is noted that in use cases in which content <b>246</b> includes video component <b>212</b> but not audio component <b>213</b>, audio encoder <b>264</b> is bypassed, while in use cases in which content <b>246</b> includes audio component <b>213</b> but not video component <b>212</b>, video encoder <b>262</b> is bypassed. It is further noted that when content <b>246</b> includes both video component <b>212</b> and audio component <b>213</b>, video encoder <b>262</b> and audio encoder <b>264</b> may be configured to process respective video component <b>212</b> and audio component <b>213</b> in parallel, i.e., substantially concurrently.</p><p id="p-0034" num="0033">As noted above, in some use cases, the sections of content included in content <b>146</b>/<b>246</b> may correspond to acts of a TV episode or movie. In some such cases, the sections or acts may be predetermined and segmented apart from one another within content <b>146</b>/<b>246</b>. However, in other cases, content <b>146</b>/<b>246</b> may include multiple contiguous sections. Where content <b>146</b>/<b>246</b> includes a sequence of contiguous sections, content segmentation software code <b>110</b>/<b>210</b> may be configured to use content distribution module <b>211</b> to identify the boundaries of each section and to perform an initial segmentation of content <b>146</b>/<b>246</b> by separating the discrete sections of content. In some implementations, content distribution module <b>211</b> may utilize a computer vision algorithm that first segments video component <b>212</b> of content <b>146</b>/<b>246</b> into shots and identifies acts as sections that start after a shot composed of black frames and ends in another shot composed of black frames, for example.</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>3</b></figref> shows diagram <b>300</b> depicting an exemplary process for generating embedding vectors including video embedding vectors <b>214</b>, audio embedding vectors <b>215</b>, and AV embedding vectors <b>216</b>, in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, according to one implementation. Referring to <figref idref="DRAWINGS">FIG. <b>3</b></figref> in combination with <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>2</b></figref>, <figref idref="DRAWINGS">FIG. <b>3</b></figref> shows video component <b>312</b> of an exemplary section of content <b>146</b>/<b>246</b>, audio component <b>313</b> of that same section, video embedding vectors <b>314</b><i>a</i>, <b>314</b><i>b</i>, <b>314</b><i>c</i>, <b>314</b><i>d</i>, <b>314</b><i>e</i>, and <b>314</b><i>f </i>(hereinafter &#x201c;video embedding vectors <b>314</b><i>a</i>-<b>314</b><i>f</i>&#x201d;), audio embedding vectors <b>315</b><i>a</i>, <b>315</b><i>b</i>, <b>315</b><i>c</i>, <b>315</b><i>d</i>, <b>315</b><i>e</i>, and <b>315</b><i>f </i>(hereinafter &#x201c;audio embedding vectors <b>315</b><i>a</i>-<b>315</b><i>f</i>&#x201d;), and AV embedding vectors <b>316</b><i>a</i>, <b>316</b><i>b</i>, <b>316</b><i>c</i>, <b>316</b><i>d</i>, <b>316</b><i>e</i>, and <b>316</b><i>f </i>(hereinafter &#x201c;AV embedding vectors <b>316</b><i>a</i>-<b>316</b><i>f</i>&#x201d;) each including a respective time duration <b>336</b><i>a</i>, <b>336</b><i>b</i>, <b>336</b><i>c</i>, <b>336</b><i>d</i>, <b>336</b><i>e</i>, and <b>336</b><i>f </i>(hereinafter &#x201c;time durations <b>336</b><i>a</i>-<b>336</b><i>f</i>&#x201d;). Also shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref> are content blocks <b>356</b><i>a</i>, <b>356</b><i>b</i>, <b>356</b><i>c</i>, <b>356</b><i>d</i>, <b>356</b><i>e</i>, and <b>356</b><i>f </i>(hereinafter &#x201c;content blocks <b>356</b><i>a</i>-<b>356</b><i>f</i>&#x201d;) included in video component <b>312</b> of content <b>146</b>/<b>246</b>, as well as embedding vector combination module <b>366</b>.</p><p id="p-0036" num="0035">It is noted that although <figref idref="DRAWINGS">FIG. <b>3</b></figref> depicts processing of a sequence of six content blocks <b>356</b><i>a</i>-<b>356</b><i>f </i>to produce six video embedding vectors <b>314</b><i>a</i>-<b>314</b><i>f</i>, six audio embedding vectors <b>315</b><i>a</i>-<b>315</b><i>f</i>, and six AV embedding vectors <b>316</b><i>a</i>-<b>316</b><i>f</i>, that representation is merely by way of example. In other implementations sequences of more than or less than six content blocks may be processed. Moreover, in some implementations, content blocks <b>356</b><i>a</i>-<b>356</b><i>f </i>may be processed to provide video embedding vectors <b>314</b><i>a</i>-<b>314</b><i>f </i>but not audio embedding vectors <b>315</b><i>a</i>-<b>315</b><i>f </i>or AV embedding vectors <b>316</b><i>a</i>-<b>316</b><i>f</i>, or to provide audio embedding vectors <b>315</b><i>a</i>-<b>315</b><i>f </i>but not video embedding vectors <b>314</b><i>a</i>-<b>314</b><i>f </i>or AV embedding vectors <b>316</b><i>a</i>-<b>316</b><i>f. </i></p><p id="p-0037" num="0036">Video component <b>312</b>, audio component <b>313</b>, and embedding vector combination module <b>366</b> correspond respectively in general to video component <b>212</b> and audio component <b>213</b>, and embedding vector combination module <b>266</b>, in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. That is to say, video component <b>212</b>, audio component <b>213</b>, and embedding vector combination module <b>266</b> may share any of the characteristics attributed to respective video component <b>312</b>, audio component <b>313</b>, and embedding vector combination module <b>366</b> by the present disclosure, and vice versa. In addition, each of video embedding vectors <b>314</b><i>a</i>-<b>314</b><i>f</i>, in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, corresponds in general to video embedding vectors <b>214</b>, in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, each of audio embedding vectors <b>315</b><i>a</i>-<b>315</b><i>f </i>corresponds in general to audio embedding vectors <b>215</b>, and each of AV embedding vectors <b>316</b><i>a</i>-<b>316</b><i>f </i>corresponds in general to AV embedding vectors <b>216</b>. Thus video embedding vectors <b>314</b><i>a</i>-<b>314</b><i>f</i>, audio embedding vectors <b>315</b>-<i>a</i>-<b>315</b><i>f</i>, and AV embedding vectors <b>316</b><i>a</i>-<b>316</b><i>f </i>may share any of the characteristics attributed to respective video embedding vectors <b>214</b>, audio embedding vectors <b>215</b>, and AV embedding vectors <b>216</b> by the present disclosure, and vice versa.</p><p id="p-0038" num="0037">Content segmentation software code <b>110</b>/<b>210</b>, when executed by processing hardware <b>104</b>, is configured to identify, for each of content blocks <b>356</b><i>a</i>-<b>356</b><i>f</i>, one or more representative units of content. In some use cases in which content blocks <b>356</b><i>a</i>-<b>356</b><i>f </i>are respective shots of video, for example, the representative unit of content for each shot may be a single image, i.e., a single video frame, from each shot. Alternatively, or in addition, the representative unit of content for each of content blocks <b>356</b><i>a</i>-<b>356</b><i>f </i>may include the portion of audio component <b>213</b>/<b>313</b> included in each content block. For example, an audio spectrogram may be extracted from audio component <b>213</b>/<b>313</b> for each of content units <b>356</b><i>a</i>-<b>356</b><i>f </i>and that audio spectrogram may be resized to match a fixed-sized sequence. Thus, the one or more representative unit or units of content for each of content blocks <b>356</b><i>a</i>-<b>356</b><i>f </i>may be one or both of a single image and an audio sample, such as one or both of a video frame and an audio spectrogram, for example.</p><p id="p-0039" num="0038">Content segmentation software code <b>110</b>/<b>210</b> then generates, using the one or more representative units of content for each content block, a respective embedding vector for each of content blocks <b>356</b><i>a</i>-<b>356</b><i>f </i>of the selected section of content to provide multiple embedding vectors. As shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, in various implementations, those multiple embedding vectors may include video embedding vectors <b>314</b><i>a</i>-<b>314</b><i>f</i>, multiple audio embedding vectors <b>315</b><i>a</i>-<b>315</b><i>f</i>, or multiple AV embedding vectors <b>316</b><i>a</i>-<b>316</b><i>f. </i></p><p id="p-0040" num="0039">As represented in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, in implementations in which video embedding vectors <b>314</b><i>a</i>-<b>314</b><i>f </i>and audio embedding vectors <b>315</b><i>a</i>-<b>315</b><i>f </i>are generated, the video embedding vector for a particular one of content blocks <b>356</b><i>a</i>-<b>356</b><i>f </i>may be concatenated or otherwise combined with the audio embedding vector for that content block to provide an AV embedding vector for that content block. For example, video embedding vector <b>314</b><i>a </i>corresponding to content block <b>356</b><i>a </i>may be combined with audio embedding vector <b>315</b><i>a </i>also corresponding to content block <b>356</b><i>a </i>to provide AV embedding vector <b>316</b><i>a </i>for content block <b>356</b><i>a</i>. Analogously, video embedding vector <b>314</b><i>b </i>corresponding to content block <b>356</b><i>b </i>may be combined with audio embedding vector <b>315</b><i>b </i>also corresponding to content block <b>356</b><i>b </i>to provide AV embedding vector <b>316</b><i>b </i>for content block <b>356</b><i>b</i>, and so forth.</p><p id="p-0041" num="0040">Because the process of generating and combining embedding vectors for a content block typically loses information about the duration of the content block, the time duration of each of respective content blocks <b>356</b><i>a</i>-<b>356</b><i>f </i>may be added to its corresponding AV embedding vector. That is to say time duration <b>336</b><i>a </i>of content block <b>356</b><i>a </i>may be added to and thereby be included in AV embedding vector <b>316</b><i>a </i>for content block <b>356</b><i>a</i>, time duration <b>336</b><i>b </i>of content block <b>356</b><i>b </i>may be added to and thereby be included in AV embedding vector <b>316</b><i>b </i>for content block <b>356</b><i>b</i>, and so forth.</p><p id="p-0042" num="0041">Video embedding vectors <b>214</b>/<b>314</b><i>a</i>-<b>314</b><i>f </i>may be generated based on video component <b>212</b>/<b>312</b> of content <b>146</b>/<b>246</b> by content segmentation software code <b>110</b>/<b>210</b>, executed by processing hardware <b>104</b>, and using video encoder <b>262</b>. Video encoder <b>262</b> may be a neural network (NN) based video encoder. For example, in some implementations, video encoder <b>262</b> may take the form of a pre-trained convolutional NN (CNN), such as a pre-trained ResNet34 for instance.</p><p id="p-0043" num="0042">Audio embedding vectors <b>215</b>/<b>315</b><i>a</i>-<b>315</b><i>f </i>may be generated based on audio component <b>213</b>/<b>313</b> of content <b>146</b>/<b>246</b> by content segmentation software code <b>110</b>/<b>210</b>, executed by processing hardware <b>104</b>, and using audio encoder <b>264</b>. Where audio component <b>213</b>/<b>313</b> takes the form of an audio spectrogram or other type of acoustic fingerprint, such as a Chromaprint or Mel Spectrogram using the Mel (melody) scale for example, audio component <b>213</b>/<b>313</b> may be generated by content distribution module <b>211</b> of content segmentation software code <b>110</b>/<b>210</b> using an open source acoustic fingerprint extraction resource, such as Chromaprint software, for example. Audio encoder <b>264</b> may be a NN based audio encoder. In one implementation, audio encoder <b>264</b> may take the form of a pre-trained NN using 1D temporal convolutions to compress the audio data. In some implementations, it may be advantageous or desirable to employ dilation in order to attend to a wider context window. For example, two blocks of six layers of dilated temporal convolutions may be utilized, with a strided temporal convolution between the blocks to reduce dimensionality, then an AveragePool1d layer may be used to compress the output into a single audio embedding for each of content blocks <b>356</b><i>a</i>-<b>356</b><i>f. </i></p><p id="p-0044" num="0043">In implementations in which video embedding vectors <b>314</b>-<i>a</i>-<b>314</b><i>f </i>are combined with audio embedding vectors <b>315</b><i>a</i>-<b>315</b><i>f </i>to produce AV embedding vectors <b>316</b><i>a</i>-<b>316</b><i>f</i>, as described above, AV embedding vectors <b>316</b><i>a</i>-<b>316</b><i>f </i>may be generated by content segmentation software code <b>110</b>/<b>210</b>, executed by processing hardware <b>104</b>, and using embedding vector combination module <b>266</b>/<b>366</b>.</p><p id="p-0045" num="0044">Content segmentation software code <b>110</b>/<b>210</b>, when executed by processing hardware <b>104</b>, is further configured to predict, using trained content subsection boundary prediction ML model <b>160</b>/<b>260</b> and video embedding vectors <b>314</b>-<i>a</i><b>314</b><i>f </i>or audio embedding vectors <b>315</b><i>a</i>-<b>315</b><i>f </i>or AV embedding vectors <b>316</b><i>a</i>-<b>316</b><i>f</i>, or any combination of video embedding vectors <b>314</b><i>a</i>-<b>314</b><i>f</i>, audio embedding vectors <b>315</b><i>a</i>-<b>315</b><i>f</i>, AV embedding vectors <b>316</b><i>a</i>-<b>316</b><i>f</i>, multiple subsections of each section of content <b>146</b>/<b>246</b>, at least some of the subsections including more than one of content blocks <b>356</b><i>a</i>-<b>356</b><i>f</i>. However, it is noted that once AV embedding vectors <b>316</b><i>a</i>-<b>316</b><i>f </i>are produced, trained content subsection boundary prediction</p><p id="p-0046" num="0045">ML model <b>160</b>/<b>260</b> can be used to predict the multiple subsections of each section of content <b>146</b>/<b>246</b> using AV embedding vectors <b>316</b><i>a</i>-<b>316</b><i>f </i>alone, i.e., video embedding vectors <b>314</b><i>a</i>-<b>314</b><i>f </i>and audio embedding vectors <b>315</b><i>a</i>-<b>315</b><i>f </i>may be disregarded by trained content subsection boundary prediction ML model <b>160</b>/<b>260</b>.</p><p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. <b>4</b>A</figref> depicts an initial stage of an exemplary process for predicting boundaries of content subsections, according to one implementation. <figref idref="DRAWINGS">FIG. <b>4</b>A</figref> shows trained content subsection boundary prediction ML model <b>460</b> having ingested AV embedding vectors <b>416</b><i>a</i>, <b>416</b><i>b</i>, <b>416</b><i>c</i>, <b>416</b><i>d</i>, <b>416</b><i>e</i>, and <b>416</b><i>f </i>(hereinafter &#x201c;AV embedding vectors <b>416</b><i>a</i>-<b>416</b><i>f</i>&#x201d;) and having generated similarity scores <b>458</b><i>ab</i>, <b>458</b><i>bc</i>, <b>458</b><i>cd</i>, <b>458</b><i>de</i>, and <b>458</b><i>ef</i>. Also shown in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref> are predicted subsections <b>470</b><i>a </i>and <b>470</b><i>b </i>of content <b>146</b>/<b>246</b> in <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>2</b></figref>, based on similarity scores <b>458</b><i>ab</i>, <b>458</b><i>bc</i>, <b>458</b><i>cd</i>, <b>458</b><i>de</i>, and <b>458</b><i>ef. </i></p><p id="p-0048" num="0047">Content subsection boundary prediction ML model <b>460</b> corresponds in general to content subsection boundary prediction ML model <b>160</b>/<b>260</b> in <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>2</b></figref>. Consequently, content subsection boundary prediction ML model <b>160</b>/<b>260</b> may share any of the characteristics attributed to content subsection boundary prediction ML model <b>460</b>, and vice versa. In addition, AV embedding vectors <b>416</b><i>a</i>-<b>416</b><i>f</i>, in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, correspond respectively in general to AV embedding vectors <b>216</b>/<b>316</b><i>a</i>-<b>316</b><i>f</i>, in <figref idref="DRAWINGS">FIGS. <b>2</b> and <b>3</b></figref>. Thus, AV embedding vectors <b>216</b>/<b>316</b><i>a</i>-<b>316</b><i>f </i>may share any of the characteristics attributed to AV embedding vectors <b>416</b><i>a</i>-<b>416</b><i>f </i>by the present disclosure, and vice versa.</p><p id="p-0049" num="0048">According to the exemplary implementation shown in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, similarity scores <b>458</b><i>ab</i>, <b>458</b><i>bc</i>, <b>458</b><i>cd</i>, <b>458</b><i>de</i>, and <b>458</b><i>ef </i>each compares consecutive AV embedding vectors <b>416</b><i>a</i>-<b>416</b><i>f</i>. That is to say similarity score <b>458</b><i>ab </i>corresponds to the similarity of AV embedding vector <b>416</b><i>a </i>to AV embedding vector <b>416</b><i>b</i>, similarity score <b>458</b><i>bc </i>corresponds to the similarity of AV embedding vector <b>416</b><i>b </i>to AV embedding vector <b>416</b><i>c</i>, similarity score <b>458</b><i>cd </i>corresponds to the similarity of AV embedding vector <b>416</b><i>c </i>to AV embedding vector <b>416</b><i>d</i>, and so forth. Thus, referring to <figref idref="DRAWINGS">FIGS. <b>1</b>, <b>2</b>, <b>3</b>, and <b>4</b>A</figref>, in some implementations processing hardware <b>104</b> executes content segmentation software code <b>110</b>/<b>210</b> to use trained content subsection boundary prediction ML model <b>160</b>/<b>260</b>/<b>460</b> to predict subsections <b>470</b><i>a </i>and <b>470</b><i>b </i>of a selected section of content <b>146</b>/<b>246</b> by comparing pairs of AV embedding vectors <b>216</b>/<b>316</b><i>a</i>-<b>316</b><i>f</i>/<b>416</b><i>a</i>-<b>416</b><i>f </i>corresponding respectively to consecutive pairs of content blocks <b>356</b><i>a</i>-<b>356</b><i>f. </i></p><p id="p-0050" num="0049">Content subsection boundary prediction ML model <b>160</b>/<b>260</b>/<b>460</b> may employ temporal 1D convolutions without dilation. The final layer of trained content subsection boundary prediction ML model <b>160</b>/<b>260</b>/<b>460</b> outputs a single value, i.e., one of similarity scores <b>458</b><i>ab</i>, <b>458</b><i>bc</i>, <b>458</b><i>cd</i>, <b>458</b><i>de</i>, and <b>458</b><i>ef </i>for each pair of AV embedding vectors <b>216</b>/<b>316</b><i>a</i>-<b>316</b><i>f</i>/<b>416</b><i>a</i>-<b>416</b><i>f</i>, so a sequence of six AV embedding vectors <b>216</b>/<b>316</b><i>a</i>-<b>316</b><i>f</i>/<b>416</b><i>a</i>-<b>416</b><i>f </i>will result in five similarity score outputs. It is noted that during training of content subsection boundary prediction ML model <b>160</b>/<b>260</b>/<b>460</b> it may be advantageous or desirable to output three different similarity score values for each pair of content blocks <b>356</b><i>a</i>-<b>356</b><i>f</i>: one using only video embedding vectors <b>214</b>/<b>314</b><i>a</i>-<b>314</b><i>f</i>, another using only audio embedding vectors <b>215</b>/<b>314</b><i>a</i>-<b>315</b><i>f</i>, and a third one using a combination of video embedding vectors <b>214</b>/<b>314</b><i>a</i>-<b>314</b><i>f </i>with respective audio embedding vectors <b>215</b>/<b>315</b><i>a</i>-<b>315</b><i>f. </i></p><p id="p-0051" num="0050">As shown in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, in some implementations, similarity scores <b>458</b><i>ab</i>, <b>458</b><i>bc</i>, <b>458</b><i>cd</i>, <b>458</b><i>de</i>, and <b>458</b><i>ef </i>may be normalized to have values in a range from zero (0.0) to one (1.0), with higher values corresponding to greater similarity and lower values being more likely to correspond to boundaries between content subsections. Thus, referring to the exemplary use case in which the content subsection predicted is a scene and each of AV embedding vectors <b>216</b>/<b>316</b><i>a</i>-<b>316</b><i>f</i>/<b>416</b><i>a</i>-<b>416</b><i>f </i>corresponds to a single shot, lowest similarity score <b>458</b><i>de </i>enables content subsection boundary prediction ML model <b>460</b> to predict that AV embedding vector <b>416</b><i>d </i>corresponds to the last shot in content subsection (scene) <b>470</b><i>a</i>, while subsequent AV embedding vector <b>416</b><i>e </i>corresponds to the first shot in content subsection (scene) <b>470</b><i>b. </i></p><p id="p-0052" num="0051">It is noted that although similarity score <b>458</b><i>de </i>is 0.0, in other instances differences between content subsections may predicted based on non-zero similarity scores. For example a predetermined or dynamically determined threshold similarity criterion may be an adjustable hyperparameter of trained content subsection boundary prediction ML model <b>460</b>, used to predict the boundaries of content subsections. For instance, in the exemplary use case shown in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, threshold similarity criterion of 0.2 or less, for example, would also result in prediction of AV embedding vector <b>416</b><i>d </i>as corresponding to the last shot in content subsection (scene) <b>470</b><i>a</i>, and AV embedding vector <b>416</b><i>e </i>corresponding to the first shot in content subsection (scene) <b>470</b><i>b</i>. Thus, boundaries of content subsections may be predicted based on the similarity score between consecutive embedding vectors being less than or equal to a threshold value.</p><p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. <b>4</b>B</figref> shows diagram <b>400</b> depicting a subsequent stage of an exemplary process for predicting boundaries of content subsections, according to one implementation. <figref idref="DRAWINGS">FIG. <b>4</b>B</figref> shows multiple AV embedding vectors represented by exemplary AV embedding vectors <b>416</b><i>a</i>, <b>416</b><i>b</i>, and <b>416</b><i>c </i>corresponding respectively to each content block included in section <b>472</b> of content <b>146</b>/<b>246</b> in <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>2</b></figref>. Also shown in <figref idref="DRAWINGS">FIG. <b>4</b>B</figref> is sliding window <b>474</b> used to compare different sequences of AV embedding vectors. For example, in implementations in which content subsection boundary prediction ML model <b>460</b>, in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, is trained with sequences of six content blocks, sliding window <b>474</b> of size six and step one can be used in order to predict content subsection boundaries on longer sequences, such as the entirety of section <b>472</b>. It is noted that the size of sliding window <b>474</b> is a parameter of trained content subsection boundary prediction ML model <b>460</b> that can be adjusted. For example, and referring to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, in use cases in which it is desirable to segment content based on manually segmented content sample <b>148</b>, the size of sliding window <b>474</b> may be adjusted to one that better replicates the manual segmentation.</p><p id="p-0054" num="0053">Use of sliding window <b>474</b> is typically an automated process and provides multiple predictions for most content blocks that can then be averaged, as depicted in <figref idref="DRAWINGS">FIG. <b>4</b>B</figref>, to get a final, more accurate, prediction. For example, each different window position provides a boundary prediction, and the averaging involves averaging the boundary predictions of each window position as it slides over a predetermined number of embedding vectors. As a specific example, where the window size is six, the boundary predictions resulting from moving sliding window <b>474</b> across six embedding vectors using step one would be averaged to provide final boundary predictions.</p><p id="p-0055" num="0054">In some implementations, the final prediction produced by system <b>100</b> can be used to retrain or further train content subsection boundary prediction ML model <b>160</b>/<b>260</b>/<b>460</b>. For example, content editor <b>144</b> may review segmented content <b>152</b>/<b>252</b> output by system <b>100</b>, and may provide feedback to system <b>100</b> correcting or ratifying segmentation predictions made by content subsection boundary prediction ML model <b>160</b>/<b>260</b>/<b>460</b>. In those implementations, content segmentation software code <b>110</b>/<b>210</b> may use the feedback provided by content editor <b>144</b> to tune hyperparameters of content subsection boundary prediction ML model <b>160</b>/<b>260</b>/<b>460</b> so as to improve its predictive performance.</p><p id="p-0056" num="0055">It is noted that in some implementations, system <b>100</b> may employ heuristics to identify sub-subsections of the subsections of content <b>146</b>/<b>246</b> having the boundaries predicted using the approach described above. For example, where subsections <b>470</b><i>a </i>and <b>470</b><i>b </i>in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref> correspond to different scenes, processing hardware <b>104</b> may execute content segmentation software code <b>110</b>/<b>210</b> to identify sub-scenes within a scene, based, for instance, on whether a sequence of content blocks is situated at or near the beginning of a scene, or at or near the end of the scene. As another example, if the first shot of a scene is an exterior location and the second shot of that scene is an interior location the first shot may be recognized as a placement shot and may be identified as one sub-scene and another portion or all other portions of the scene may be recognized as another sub-scene.</p><p id="p-0057" num="0056">Referring to <figref idref="DRAWINGS">FIGS. <b>1</b>, <b>2</b>, and <b>4</b>A</figref> in combination, as noted above, in some implementations, content segmentation software code <b>110</b>/<b>210</b>, executed by processing hardware <b>104</b>, may receive manually segmented content sample <b>148</b> from user system <b>140</b>, and may utilize manually segmented content sample <b>148</b> as a template or recipe for adjusting one or more hyperparameters of trained content subsection boundary prediction ML model <b>160</b>/<b>260</b>/<b>460</b>. As a result, trained content subsection boundary prediction ML model <b>160</b>/<b>260</b>/<b>460</b> may advantageously be used to produce segmented content <b>152</b>/<b>252</b> according to the editorial preferences of content editor <b>144</b>, also in a substantially automated process, e.g., an &#x201c;auto-complete&#x201d; process. It is noted that segmented content <b>152</b>/<b>252</b> is content <b>146</b>/<b>246</b> having the boundaries of its subsections identified. In other words, content segmentation software code <b>110</b>/<b>210</b>, when executed by processing hardware <b>104</b>, may receive manually segmented content sample <b>148</b>, and may adjust, based on manually segmented content sample <b>148</b> before using trained content subsection boundary prediction ML model <b>160</b>/<b>260</b>/<b>460</b> to predict subsections <b>470</b><i>a </i>and <b>470</b><i>b</i>, one or more hyperparameters of trained content subsection boundary prediction ML model <b>160</b>/<b>260</b>/<b>460</b> to substantially replicate manually segmented content sample <b>148</b>.</p><p id="p-0058" num="0057">In some implementations, it may be advantageous or desirable to identify sections of fungible content included in content <b>146</b>/<b>246</b> in order to reduce the processing overhead required to produce segmented content <b>152</b>/<b>252</b>. As noted above, fungible content refers to sections of content that are substantially the same from one version to another of content in which the fungible content is included. As further noted above, examples of fungible content included in episodic TV content may include the introduction to the episode, credits, and short introductions following a commercial break, to name a few. Identification of sections of content <b>146</b>/<b>246</b> as fungible content that is non-selectable by content segmentation software code <b>110</b>/<b>210</b> prior to processing of content <b>146</b>/<b>246</b> by content segmentation software code <b>110</b>/<b>210</b> advantageously increases the speed and efficiency with which segmented content <b>152</b>/<b>252</b> can be output, while concurrently reducing the cost of processing content <b>146</b>/<b>246</b>.</p><p id="p-0059" num="0058"><figref idref="DRAWINGS">FIG. <b>5</b></figref> shows a diagram of exemplary fungible content detection software code <b>520</b> suitable for use by system <b>100</b>, in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, according to one implementation. By way of overview, it is noted that fungible content detection software code <b>520</b>, when executed by processing hardware <b>104</b>, is configured to obtain content <b>146</b>/<b>246</b> from content segmentation software code <b>110</b>/<b>210</b>, obtain comparison content <b>147</b>, identify, using content <b>146</b>/<b>246</b> and comparison content <b>147</b> before content segmentation software code <b>110</b>/<b>210</b> is executed to select one of the multiple sections of content <b>146</b>/<b>246</b> for segmentation, one or more of those sections of content <b>146</b>/<b>246</b> as fungible content section(s), and flag the fungible content section(s) as non-selectable by content segmentation software code <b>110</b>/<b>210</b>.</p><p id="p-0060" num="0059">According to the exemplary implementation shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, fungible content detection software code <b>520</b> may include hashing module <b>522</b>, comparison module <b>526</b>, and fungibility determination module <b>528</b>. Also shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref> are content <b>546</b>, comparison content <b>547</b>, content hash vectors <b>524</b><i>a</i>, comparison content hash vectors <b>524</b><i>b</i>, comparison data <b>580</b>, and fungible content identification data <b>554</b>.</p><p id="p-0061" num="0060">Fungible content detection software code <b>520</b>, comparison content <b>547</b>, and fungible content identification data <b>554</b> correspond respectively in general to fungible content detection software code <b>120</b>, comparison content <b>147</b>, and fungible content identification data <b>154</b>, in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. Consequently, fungible content detection software code <b>120</b>, comparison content <b>147</b>, and fungible content identification data <b>154</b> may share any of the characteristics attributed to respective fungible content detection software code <b>520</b>, comparison content <b>547</b>, and fungible content identification data <b>554</b> by the present disclosure, and vice versa. Thus, although not shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, fungible content detection software code <b>120</b>, like fungible content detection software code <b>520</b>, may include features corresponding to hashing module <b>522</b>, comparison module <b>526</b>, and fungibility determination module <b>528</b>. Moreover, content <b>546</b> corresponds in general to content <b>146</b>/<b>246</b> in <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>2</b></figref>, and those corresponding features may share the characteristics attributed to any of those corresponding features by the present disclosure.</p><p id="p-0062" num="0061">As noted above, fungible content sections in a TV series, for example, are parts of the episodes that are identical in all episodes. Continuing to refer to the exemplary use case in which content <b>146</b>/<b>246</b>/<b>546</b> is episodic TV content, fungible content detection software code <b>120</b>/<b>520</b> takes as input two or more episodes from each season of a TV show, i.e., content <b>146</b>/<b>246</b>/<b>546</b> and comparison content <b>147</b>/<b>547</b>. However, in use cases in which content <b>146</b>/<b>246</b>/<b>546</b> is movie content or video game content, comparison content <b>147</b>/<b>547</b> may include short clips announcing one or more of production houses, post-production houses, and credits.</p><p id="p-0063" num="0062">According to the exemplary implementation shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, fungible content detection software code <b>120</b>/<b>520</b> may use hashing module <b>522</b> to generate a hash vector for each predetermined video frame or other predetermined content unit of content <b>146</b>/<b>246</b>/<b>546</b> and comparison content <b>147</b>/<b>547</b> to provide content hash vectors <b>524</b><i>a </i>and comparison content hash vectors <b>524</b><i>b</i>. Those hash vectors may be aggregated by similarity in hash blocks. By comparing these content hash vectors <b>524</b><i>a </i>and comparison content hash vectors <b>524</b><i>b </i>between episodes using comparison module <b>526</b> it is possible for fungible content detection software code <b>120</b>/<b>520</b> to identify sequences of hashes that are equal in two or more episodes. In one implementation, for example, comparison module <b>526</b> may use a data structure known as Generalized Suffix Tree to accelerate the comparison process.</p><p id="p-0064" num="0063">Once sequences of hash vectors that are equal are identified, they can be smoothed, and fungible content determination module may use comparison data <b>580</b> output by comparison module <b>526</b> to determine whether the content sections corresponding to those equal hash vector sequences are fungible content sections based further on one or both of their time duration and their locations within content <b>146</b>/<b>246</b>/<b>546</b> and comparison content <b>147</b>/<b>547</b>. Thus, more generally, processing hardware <b>104</b> may execute fungible content detection software code <b>120</b>/<b>520</b> to generate, for each content unit included in content <b>146</b>/<b>246</b>/<b>546</b> and for each comparison content unit included in comparison content <b>147</b>/<b>547</b>, a respective hash vector, and determine, for each of the content units and the comparison content units, respective time durations. A fungible content section can then be identified based on comparison of the respective hash vectors and time durations of each of the content units included in content <b>146</b>/<b>246</b>/<b>546</b> with the respective hash vectors and time durations of each of the comparison content units included in comparison content <b>147</b>/<b>547</b>.</p><p id="p-0065" num="0064">It is noted that, in addition to, or as an alternative to detecting fungible content, the ability of fungible content detection software code <b>120</b>/<b>520</b> to detect similar segments of content between two or more AV sequences can be used to map annotations or &#x201c;tags&#x201d; from one AV sequence to another, or to signal the differences or similarities between two AV sequences that are ostensibly the same, e.g., to track changes from one version of an episode to another version of the same episode. As another example, by using the functionality enabled by fungible content detection software code <b>120</b>/<b>520</b> to analyze multiple episodes from the same TV series, content appearing in a present episode as regular content and in future episodes as recap content can be identified. This identification could advantageously be used to recommend that a viewer of the TV series review an old episode or the original scenes that the recap refers to, and to notify the viewer which episode or scenes the content originally appeared in. Another use case for such identification of recurring content may be to flag the content that appears afterwards in recaps as key points in the storyline of the TV series.</p><p id="p-0066" num="0065">Thus, the present application discloses systems and computer-readable non-transitory storage media including instructions for performing content segmentation and identification of fungible content that overcome the drawbacks and deficiencies in the conventional art. From the above description it is manifest that various techniques can be used for implementing the concepts described in the present application without departing from the scope of those concepts. Moreover, while the concepts have been described with specific reference to certain implementations, a person of ordinary skill in the art would recognize that changes can be made in form and detail without departing from the scope of those concepts. As such, the described implementations are to be considered in all respects as illustrative and not restrictive. It should also be understood that the present application is not limited to the particular implementations described herein, but many rearrangements, modifications, and substitutions are possible without departing from the scope of the present disclosure.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A system comprising:<claim-text>a computing platform including processing hardware and a system memory;</claim-text><claim-text>a software code stored in the system memory; and</claim-text><claim-text>a trained machine learning model;</claim-text><claim-text>the processing hardware configured to execute the software code to:<claim-text>receive content, the content including a plurality of sections each having a plurality of content blocks in sequence;</claim-text><claim-text>select one of the plurality of sections for segmentation;</claim-text><claim-text>identify, for each of the plurality of content blocks of the selected section, at least one respective representative unit of content;</claim-text><claim-text>generate, using the at least one respective representative unit of content, a respective embedding vector for each of the plurality of content blocks of the selected section to provide a plurality of embedding vectors; and</claim-text><claim-text>predict, using the trained machine learning model and the plurality of embedding vectors, a plurality of subsections of the selected section, at least some of the plurality of subsections including more than one of the plurality of content blocks.</claim-text></claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the processing hardware is configured to execute the software code to use the trained machine learning model to predict the plurality of subsections of the selected section by comparing pairs of the plurality of embedding vectors corresponding respectively to consecutive pairs of the plurality of content blocks.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each of the plurality of content blocks comprises a shot of video.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each of the plurality of subsections of the selected section corresponds to a different respective scene of the content.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each of the plurality of subsections of the selected section corresponds to a different respective location depicted by the content.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the at least one respective representative unit of content comprises at least one of a single video frame or an audio spectrogram.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the at least one respective representative unit of content comprises at least one of a single image or an audio sample.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the processing hardware is further configured to execute the software code to:<claim-text>receive, from a user system, a manually segmented content sample prepared by a user; and</claim-text><claim-text>adjust, based on the manually segmented content sample before using the trained machine learning model to predict the plurality of subsections of the selected section, one or more hyperparameters of the trained machine learning model to substantially replicate the manually segmented content sample.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising a fungible content detection software code stored in the system memory, the processing hardware configured to execute the fungible content detection software code to:<claim-text>obtain the content;</claim-text><claim-text>obtain a comparison content;</claim-text><claim-text>before the software code is executed to select the one of the plurality of sections for segmentation, identify, using the content and the comparison content, at least one of the plurality of sections of the content as a fungible content section; and</claim-text><claim-text>flag the fungible content section as non-selectable by the software code.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the processing hardware is further configured to execute the fungible content detection software code to:<claim-text>generate, for each of a plurality of content units included in the content, and each of a plurality of comparison content units included in the comparison content, a respective hash vector; and</claim-text><claim-text>determine, for each of the plurality of content units included in the content, and each of the plurality of comparison content units included in the comparison content, a respective time duration;</claim-text><claim-text>wherein the fungible content section is identified based on comparison of the respective hash vector and time duration of each of the plurality of content units included in the content with the respective hash vector and time duration of each of the plurality of comparison content units included in the comparison content.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. A computer-readable non-transitory storage medium having stored thereon instructions, which when executed by a processing hardware of a system, instantiate a method comprising:<claim-text>receiving content, the content including a plurality of sections each having a plurality of content blocks in sequence;</claim-text><claim-text>selecting one of the plurality of sections for segmentation;</claim-text><claim-text>identifying, for each of the plurality of content blocks of the selected section, at least one respective representative unit of content;</claim-text><claim-text>generating, using the at least one respective representative unit of content, a respective embedding vector for each of the plurality of content blocks of the selected section to provide a plurality of embedding vectors; and</claim-text><claim-text>predicting, using a trained machine learning model and the plurality of embedding vectors, a plurality of subsections of the selected section, at least some of the plurality of subsections including more than one of the plurality of content blocks.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The computer-readable non-transitory storage medium of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein predicting the plurality of subsections of the selected section comprises comparing pairs of the plurality of embedding vectors corresponding respectively to consecutive pairs of the plurality of content blocks.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The computer-readable non-transitory storage medium of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein each of the plurality of content blocks comprises a shot of video.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The computer-readable non-transitory storage medium of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein each of the plurality of the subsections of the selected section corresponds to a different respective scene of the content.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The computer-readable non-transitory storage medium of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein each of the plurality of subsections of the selected section corresponds to a different respective location depicted by the content.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The computer-readable non-transitory storage medium of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the at least one respective representative unit of content comprises at least one of a single video frame or an audio spectrogram.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The computer-readable non-transitory storage medium of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the at least one respective representative unit of content comprises at least one of a single image or an audio sample.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The computer-readable non-transitory storage medium of <claim-ref idref="CLM-00011">claim 11</claim-ref>, having stored thereon additional instructions, which when executed by the processing hardware of the system, instantiate the method further comprising:<claim-text>receiving, from a user system, a manually segmented content sample prepared by a user; and</claim-text><claim-text>adjusting, based on the manually segmented content sample before using the trained machine learning model to predict the plurality of subsections of the selected section, one or more hyperparameters of the trained machine learning model to substantially replicate the manually segmented content sample.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. A computer-readable non-transitory storage medium having stored thereon instructions, which when executed by a processing hardware of a system, instantiate a fungible content detection method comprising:<claim-text>receiving content, the content including a plurality of sections each having a plurality of content blocks in sequence;</claim-text><claim-text>obtaining a comparison content;</claim-text><claim-text>identifying, using the content and the comparison content, at least one of the plurality of sections of the content as a fungible content section; and</claim-text><claim-text>flag the fungible content section as non-selectable.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The computer-readable non-transitory storage medium of <claim-ref idref="CLM-00019">claim 19</claim-ref>, having stored thereon additional instructions, which when executed by the processing hardware of the system, instantiate the fungible content detection method further comprising:<claim-text>generating, for each of a plurality of content units included in the content, and each of a plurality of comparison content units included in the comparison content, a respective hash vector; and</claim-text><claim-text>determining, for each of the plurality of content units included in the content, and each of the plurality of comparison content units included in the comparison content, a respective time duration;</claim-text><claim-text>wherein identifying the fungible content section comprises comparing the respective hash vector and time duration of each of the plurality of content units included in the content with the respective hash vector and time duration of each of the plurality of comparison content units included in the comparison content.</claim-text></claim-text></claim></claims></us-patent-application>