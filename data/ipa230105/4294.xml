<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004295A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004295</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17901439</doc-number><date>20220901</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>06</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>L</subclass><main-group>67</main-group><subgroup>10</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>0604</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>0653</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>0655</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>067</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>L</subclass><main-group>67</main-group><subgroup>10</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">ALLOCATING MEMORY AND REDIRECTING MEMORY WRITES IN A CLOUD COMPUTING SYSTEM BASED ON TEMPERATURE OF MEMORY MODULES</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16915303</doc-number><date>20200629</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11467729</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17901439</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Microsoft Technology Licensing, LLC</orgname><address><city>Redmond</city><state>WA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>KONO</last-name><first-name>Raymond-Noel Nkoulou</first-name><address><city>Snoqualmie</city><state>WA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>JOHN</last-name><first-name>Nisha Susan</first-name><address><city>Redmond</city><state>WA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Systems and methods for allocating memory and redirecting data writes based on temperature of memory modules in a cloud computing system are described. A method includes maintaining temperature profiles for a first plurality of memory modules and a second plurality of memory modules, The method includes automatically redirecting a first request to write to memory from a first compute entity being executed by the first processor to a selected one of a first plurality of memory chips, whose temperature does not meet or exceed the temperature threshold, included in at least the first plurality of memory modules and automatically redirecting a second request to write to memory from a second compute entity being executed by the second processor to a selected one of the second plurality of memory chips, whose temperature does not meet or exceed the temperature threshold, included in at least the second plurality of memory modules.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="88.05mm" wi="158.75mm" file="US20230004295A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="240.11mm" wi="150.20mm" orientation="landscape" file="US20230004295A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="231.22mm" wi="106.60mm" orientation="landscape" file="US20230004295A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="242.15mm" wi="175.94mm" orientation="landscape" file="US20230004295A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="238.76mm" wi="177.46mm" orientation="landscape" file="US20230004295A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="238.08mm" wi="124.29mm" orientation="landscape" file="US20230004295A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="221.91mm" wi="175.26mm" file="US20230004295A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="239.61mm" wi="181.61mm" file="US20230004295A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION(S)</heading><p id="p-0002" num="0001">This application is a continuation of U.S. patent application Ser. No. 16/915,303, filed on Jun. 29, 2020, entitled &#x201c;ALLOCATING MEMORY AND REDIRECTING MEMORY WRITES IN A CLOUD COMPUTING SYSTEM BASED ON TEMPERATURE OF MEMORY MODULES,&#x201d; the entire contents of which are hereby incorporated herein by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">Increasingly, computing, storage, and network resources are accessed via the public cloud, private cloud, or a hybrid of the two. The public cloud includes a global network of servers that perform a variety of functions, including storing and managing data, running applications, and delivering content or services, such as streaming videos, electronic mail, office productivity software, or social media. The servers and other components may be located in data centers across the world. While the public cloud offers services to the public over the Internet, businesses may use private clouds or hybrid clouds. Both private and hybrid clouds also include a network of servers housed in data centers. Cloud service providers offer access to these resources by offering cloud computing and storage resources to customers.</p><p id="p-0004" num="0003">There is a need for methods and systems to improve the reliability of the memory modules used in cloud computing systems.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0005" num="0004">One aspect of the present disclosure relates to a method in a cloud computing system including a host server, where the host server comprises at least a first plurality of memory modules coupled to a first processor and at least a second plurality of memory modules coupled to a second processor. The method may include maintaining a first temperature profile based on information received from a thermal sensor associated with each of a first plurality of memory chips included in at least the first plurality of memory modules. The method may further include maintaining a second temperature profile based on information received from a thermal sensor associated with each of a second plurality of memory chips included in at least the second plurality of memory modules. The method may further include based on at least the first temperature profile, automatically redirecting a first request to write to memory from a first compute entity being executed by the first processor to a selected one of the first plurality of memory chips, whose temperature does not meet or exceed the temperature threshold, included in at least the first plurality of memory modules. The method may further include based on at least the second temperature profile, automatically redirecting a second request to write to memory from a second compute entity being executed by the second processor to a selected one of the second plurality of memory chips, whose temperature does not meet or exceed the temperature threshold, included in at least the second plurality of memory modules.</p><p id="p-0006" num="0005">In yet another aspect, the present disclosure relates to a system including a host server comprising at least a first plurality of memory modules coupled to a first processor and at least a second plurality of memory modules coupled to a second processor. The system may further include a hypervisor, associated with the host server. The hypervisor may be configured to: (1) maintain a first temperature profile based on information received from a thermal sensor associated with each of a first plurality of memory chips included in at least the first plurality of memory modules, (2) maintain a second temperature profile based on information received from a thermal sensor associated with each of a second plurality of memory chips included in at least the second plurality of memory modules, (3) based on at least the first temperature profile, automatically redirecting a first request to write to memory from a first compute entity being executed by the first processor to a selected one of the first plurality of memory chips, whose temperature does not meet or exceed the temperature threshold, included in at least the first plurality of memory modules, and (4) based on at least the second temperature profile, automatically redirecting a second request to write to memory from a second compute entity being executed by the second processor to a selected one of the second plurality of memory chips, whose temperature does not meet or exceed the temperature threshold, included in at least the second plurality of memory modules.</p><p id="p-0007" num="0006">In another aspect, the present disclosure relates to a method in a cloud computing system including a first host server and a second host server, where the first host server comprises at least a first plurality of memory modules coupled to a first processor and at least a second plurality of memory modules coupled to a second processor, where the second host server comprises at least a third plurality of memory modules coupled to a third processor and at least a fourth plurality of memory modules coupled to a fourth processor, where the first host server comprises a first hypervisor for managing a first plurality of compute entities for execution by the first processor or the second processor and the second host server comprises a second hypervisor for managing a second plurality of compute entities for execution by the third processor or the fourth processor. The method may include maintaining a first temperature profile based on information received from a thermal sensor associated with each of a first plurality of memory chips included in at least the first plurality of memory modules and at least the second plurality of memory modules. The method may further include maintaining a second temperature profile based on information received from a thermal sensor associated with each of a second plurality of memory chips included in at least the third plurality of memory modules and at least the fourth plurality of memory modules. The method may further include based on at least the first temperature profile, automatically redirecting a first request to write to memory from a first compute entity being executed by the first processor to a selected one of the first plurality of memory chips, whose temperature does not meet or exceed the temperature threshold, included in at least the first plurality of memory modules and at least the second plurality of memory modules. The method may further include based on at least the second temperature profile, automatically redirecting a second request to write to memory from a second compute entity being executed by the second processor to a selected one of the second plurality of memory chips, whose temperature does not meet or exceed the temperature threshold, included in at least the third plurality of memory modules and the fourth plurality of memory modules. The method may further include upon determining a temperature of at least N of the first plurality of memory chips meets or exceeds a temperature threshold, wherein N is a positive integer, automatically migrating at least a subset of the first compute entities from the first host server to the second host server provided at least a temperature of at least one memory chip from among the second plurality of memory chips does not meet or exceed the temperature threshold.</p><p id="p-0008" num="0007">This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter, nor is it intended to be used to limit the scope of the claimed subject matter.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0009" num="0008">The present disclosure is illustrated by way of example and is not limited by the accompanying figures, in which like references indicate similar elements. Elements in the figures are illustrated for simplicity and clarity and have not necessarily been drawn to scale.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows a system for controlling memory allocation and data writes in a cloud computing system in accordance with one example;</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows a block diagram of a server in accordance with one example;</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>3</b></figref> shows a host server including memory modules in accordance with one example;</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows a system environment for implementing a system for controlling memory allocation and data writes in accordance with one example;</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>5</b></figref> shows a computing platform that may be used for performing certain methods in accordance with one example;</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>6</b></figref> shows a flowchart of a method in accordance with one example; and</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>7</b></figref> shows another flowchart of a method in accordance with one example.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0017" num="0016">Examples described in this disclosure relate to allocating memory and redirecting data writes based on temperature of memory modules in a cloud computing system. The memory modules may be included in a host server. Multiple host servers may be included in a rack of servers or a stack of servers. The host server may be any server in a cloud computing environment that is configured to serve tenants or other subscribers of the cloud computing service. Example memory technologies include, but are not limited to, volatile-memory technologies, non-volatile memory technologies, and quasi-volatile memory technologies. Example memory types include dynamic random access memory (DRAM), flash memory (e.g., NAND flash), ferroelectric random-access memory (FeRAM), magnetic random-access memory (MRAM), phase-change memory (PCM), and resistive random-access memory (RRAM). Broadly speaking, the present disclosure relates to increasing the reliability and uptime of any servers that have memory based on a technology that is susceptible to breakdown or failure because of heat and overuse.</p><p id="p-0018" num="0017">In certain examples, the methods and systems described herein may be deployed in cloud computing environments. Cloud computing may refer to a model for enabling on-demand network access to a shared pool of configurable computing resources. For example, cloud computing can be employed in the marketplace to offer ubiquitous and convenient on-demand access to the shared pool of configurable computing resources. The shared pool of configurable computing resources can be rapidly provisioned via virtualization and released with low management effort or service provider interaction, and then scaled accordingly. A cloud computing model can be composed of various characteristics such as, for example, on-demand self-service, broad network access, resource pooling, rapid elasticity, measured service, and so forth. A cloud computing model may be used to expose various service models, such as, for example, Hardware as a Service (&#x201c;HaaS&#x201d;), Software as a Service (&#x201c;SaaS&#x201d;), Platform as a Service (&#x201c;PaaS&#x201d;), and Infrastructure as a Service (&#x201c;IaaS&#x201d;). A cloud computing model can also be deployed using different deployment models such as private cloud, community cloud, public cloud, hybrid cloud, and so forth.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows a system <b>100</b> for controlling memory allocation and data writes in a cloud computing system in accordance with one example. In this example, system <b>100</b> may correspond to a cloud computing stack in a data center. System <b>100</b> may be implemented as a rack of servers. In this example, system <b>100</b> may include host servers <b>110</b>, <b>120</b>, and <b>130</b>. Each of the host servers may include one or more processors configured to provide at least some form of compute functionality. As an example, host server <b>110</b> may include CPU-0 <b>112</b> and CPU-1 <b>114</b>, host server <b>120</b> may include CPU-0 <b>122</b> and CPU-1 <b>124</b>, and host server <b>130</b> may include CPU-0 <b>132</b> and CPU-1 <b>134</b>. Host server <b>110</b> may further include memory modules <b>116</b> and <b>118</b>. Host server <b>120</b> may further include memory modules <b>126</b> and <b>128</b>. Host server <b>130</b> may further include memory modules <b>136</b> and <b>138</b>.</p><p id="p-0020" num="0019">With continued reference to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, host server <b>110</b> may be configured to execute instructions corresponding to hypervisor <b>140</b>. Hypervisor <b>140</b> may further be configured to interface with virtual machines (VMs) (e.g., VM <b>142</b>, <b>144</b>, and VM <b>146</b>). Instructions corresponding to the VMs may be executed using either of CPU-0 <b>112</b> or CPU-1 <b>114</b> associated with host server <b>110</b>. Hypervisor <b>150</b> may further be configured to interface with virtual machines (VMs) (e.g., VM <b>152</b>, <b>154</b>, and VM <b>156</b>). Instructions corresponding to these VMs may be executed using either of CPU-0 <b>122</b> or CPU-1 <b>124</b> associated with host <b>120</b>. Hypervisor <b>160</b> may further be configured to interface with virtual machines (VMs) (e.g., VM <b>162</b>, <b>164</b>, and VM <b>166</b>). Instructions corresponding to these VMs may be executed using either of CPU-0 <b>132</b> or CPU-1 <b>134</b> associated with host server <b>130</b>.</p><p id="p-0021" num="0020">Hypervisor <b>140</b> may share control information with hypervisor <b>150</b> via a control path. The control path may correspond to a path implemented using a bus system (e.g., a server rack bus system or other types of bus systems). Hypervisor <b>150</b> may share control information with hypervisor <b>160</b> via another control path. The control path may correspond to a path implemented using a bus system (e.g., a server rack bus system or other types of bus systems). Each of hypervisor <b>140</b>, hypervisor <b>150</b>, and hypervisor <b>160</b> may be a kernel-based virtual machine (KVM) hypervisor, a Hyper-V hypervisor, or another type of hypervisor. Although <figref idref="DRAWINGS">FIG. <b>1</b></figref> shows system <b>100</b> as including a certain number of components arranged and coupled in a certain way, it may include fewer or additional components arranged and coupled differently. As an example, although not shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, each host server may include an operating system for managing certain aspects of the host server. As another example, system <b>100</b> may include any number of host servers combined as part of a rack or a stack. As another example, each host server may include any number of CPUs, GPUs, memory modules, or other components, as needed for providing cloud computing, storage, and/or networking functions. In addition, the functionality associated with system <b>100</b> may be distributed or combined, as needed. Moreover, although <figref idref="DRAWINGS">FIG. <b>1</b></figref> describes the access to the host server's memory by VMs, other types of compute entities, such as containers, micro-VMs, microservices, and unikernels for serverless functions, may access the memory in a like manner. As used herein, the term &#x201c;compute entity&#x201d; encompasses, but is not limited to, any executable code (in the form of hardware, firmware, software, or in any combination of the foregoing) that implements a functionality, an application, a service, a micro-service, a container, a unikernel for serverless computing, or a part of the aforementioned.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows a block diagram of a server <b>200</b> for implementing a host server (e.g., any of host server <b>110</b>, host server <b>120</b>, or host server <b>130</b>) in accordance with one example. Server <b>200</b> may include server board <b>210</b> and server board <b>250</b>. Server board <b>210</b> and server board <b>250</b> may be coupled via interconnects, high speed cables, a bus system associated with a rack, or another structure for housing server board <b>210</b> and server board <b>250</b>. Server board <b>210</b> may include CPU-0 <b>212</b>, Dual-Inline Memory Modules (DIMMs) (e.g., DIMMs shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref> as part of server board <b>210</b>), and Solid-State Drives (SSDs) <b>245</b>, <b>247</b>, and <b>249</b>. In this example, CPU-0 <b>212</b> may include 24 cores (identified as blocks with capitalized letter C in <figref idref="DRAWINGS">FIG. <b>2</b></figref>). Each of the DIMMs may be installed in a DIMM slot/connector, In this example, six DIMMs may be arranged on one side of CPU-0 <b>212</b> and another six DIMMs may be arranged on the other side of CPU-0 <b>212</b>. In this example, DIMM 0 <b>222</b>, DIMM 1 <b>224</b>, DIMM 2 <b>226</b>, DIMM 3 <b>228</b>, DIMM 4 <b>230</b>, and DIMM 5 <b>232</b> may be arranged on the left side of CPU-0 <b>212</b>. DIMM 6 <b>234</b>, DIMM 7 <b>236</b>, DIMM 8 <b>238</b>, DIMM 9 <b>240</b>, DIMM 10 <b>242</b>, and DIMM 11 <b>244</b> may be arranged on the right-hand side of CPU-0 <b>212</b>.</p><p id="p-0023" num="0022">With continued reference to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, server board <b>250</b> may include CPU-1 <b>252</b>, Dual-Inline Memory Modules (DIMMs) (e.g., DIMMs shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref> as part of server board <b>250</b>), and Solid-State Drives (SSDs) <b>285</b>, <b>287</b>, and <b>289</b>. In this example, CPU-1 <b>252</b> may include 24 cores (identified as blocks with capitalized letter C in <figref idref="DRAWINGS">FIG. <b>2</b></figref>). Each of the DIMMs may be installed in a DIMM slot/connector. In this example, six DIMMs may be arranged on one side of CPU-1 <b>252</b> and another six DIMMs may be arranged on the other side of CPU-1 <b>252</b>. In this example, DIMM 0 <b>262</b>, DIMM 1 <b>264</b>, DIMM 2 <b>266</b>, DIMM 3 <b>268</b>, DIMM 4 <b>270</b>, and DIMM 5 <b>272</b> may be arranged on the left side of CPU-1 <b>252</b>. DIMM 6 <b>274</b>, DIMM 7 <b>276</b>, DIMM 8 <b>278</b>, DIMM 9 <b>280</b>, DIMM 10 <b>282</b>, and DIMM 11 <b>284</b> may be arranged on the right-hand side of CPU-1 <b>252</b>.</p><p id="p-0024" num="0023">Still referring to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, in this example, server <b>200</b> may be cooled using cooled air. As an example, cooled air may be provided, using inlets <b>292</b>, <b>294</b>, and <b>296</b>, to various portions of server board <b>210</b> and server board <b>250</b>. Although in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the cooling is accomplished using cooled air, liquids or other forms of matter may also be used to cool server <b>200</b>. Regardless of the cooling methods used, the various components incorporated in server <b>200</b> may have different temperatures. The non-uniformity of the temperature for the components may stem from several reasons. As an example, certain components mounted on any of server boards <b>210</b> and <b>250</b> may generate more heat than the other components. As another example, the cooled air received by some downstream components may be pre-heated by the upstream components (e.g., the SSDs shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>). In addition, the number and the arrangement of inlets may create non-uniform temperature inside a server rack, or another structure used for housing server boards <b>210</b> and <b>250</b>. The DIMMs may also experience non-uniform temperatures. Although <figref idref="DRAWINGS">FIG. <b>2</b></figref> shows server <b>200</b> as including a certain number of components arranged and coupled in a certain way, it may include fewer or additional components arranged and coupled differently. As an example, server <b>200</b> may include any number of server boards arranged inside a rack or any other structure. As another example, each server board may include any number of CPUs, GPUs, memory modules, or other components, as needed for providing computing, storage, and/or networking functions. In addition, although server boards <b>210</b> and <b>250</b> are described as having DIMMs, other types of memory modules may instead be included as part of server boards <b>210</b> and <b>250</b>. As an example, such memory modules may be Single-Inline Memory Modules (SIMMs).</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>3</b></figref> shows a host server <b>300</b> including memory modules <b>350</b> in accordance with one example. Host server <b>300</b> may include a portion <b>310</b>. In this example, portion <b>310</b> may correspond to a part of host server <b>300</b> that includes central processing functionality and bus/memory controllers. As an example, portion <b>310</b> may include CPU(s) <b>312</b>, cache <b>314</b>, Peripheral Component Interconnect express (PCIe) controller <b>316</b>, and memory controller (MC) <b>318</b>. CPU(s) <b>312</b> may be coupled to cache <b>314</b>, via bus <b>313</b>, to allow fast access to cached instructions or data. CPU(s) <b>312</b> may also be coupled to PCIe controller <b>316</b> via a bus <b>315</b>. CPU(s) <b>312</b> may also be coupled to a memory controller <b>318</b> via a bus <b>317</b>. Cache <b>314</b> may be coupled to memory controller <b>318</b> via bus <b>319</b>. In one example, CPU(s) <b>312</b>, cache <b>314</b>, and PCIe controller <b>316</b> may be incorporated in a single module (e.g., a CPU module).</p><p id="p-0026" num="0025">PCIe controller <b>316</b> may be coupled to a PCIe bridge <b>320</b> via a PCIe bus <b>321</b>. PCIe Bridge <b>320</b> may include a peer to peer (P2P) controller <b>322</b>. PCIe bridge <b>320</b> may also provide the functionality associated with a PCIe controller and other functionality, as needed, to enable interfacing with various storage and/or networking resources. In this example, P2P controller <b>322</b> may be coupled via bus <b>334</b> to P2P ports, including P2P <b>328</b>, P2P <b>330</b>, and P2P <b>332</b>. In this example, P2P <b>328</b> may be coupled to SSD <b>340</b>, P2P <b>330</b> may be coupled to SSD <b>342</b>, and P2P <b>332</b> may be coupled to SSD <b>344</b>.</p><p id="p-0027" num="0026">With continued reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, memory controller <b>318</b> may be coupled via busses <b>352</b> and <b>354</b> to memory modules <b>350</b>. In this example, the coupling to the memory modules may be made via advanced memory buffers (e.g., AMBs <b>362</b>, <b>372</b>, and <b>382</b>). In addition, in this example, bus <b>352</b> may transfer data/control/status signals from memory controller <b>318</b> to memory modules <b>350</b> and bus <b>354</b> may transfer data/control/status signals from memory modules <b>350</b> to memory controller <b>318</b>. In addition, clock source <b>346</b> may be used to synchronize signals, as needed. Clock source <b>346</b> may be implemented as a phase-locked loop (PLL) circuit or another type of clocking circuit.</p><p id="p-0028" num="0027">Each of memory modules <b>360</b>, <b>370</b>, and <b>380</b> may be a DIMM, as described earlier. Memory module <b>360</b> may include memory chips <b>363</b>, <b>364</b>, <b>365</b>, <b>366</b>, <b>367</b>, and <b>368</b>. Memory module <b>360</b> may further include a memory module controller (MMC) <b>361</b>. Memory module <b>370</b> may include memory chips <b>373</b>, <b>374</b>, <b>375</b>, <b>376</b>, <b>377</b>, and <b>378</b>. Memory module <b>370</b> may further include an MMC <b>371</b>. Memory module <b>380</b> may include memory chips <b>383</b>, <b>384</b>, <b>385</b>, <b>386</b>, <b>387</b>, and <b>388</b>. Memory module <b>380</b> may further include an MMC <b>381</b>. Each memory chip may include a temperature sensor (not shown) for continuously monitoring and tracking of the temperature inside the memory chip. Such temperature sensors may be implemented using semiconductor manufacturing techniques during fabrication of the memory chips. In this example, each of MMCs <b>361</b>, <b>371</b>, and <b>381</b> may be coupled via a bus <b>351</b> to memory controller <b>318</b>. Each MMC may be responsible, among other things, for collecting temperature sensor values from each of the respective memory chips. Memory controller <b>318</b> may obtain temperature related information from a respective MMC corresponding to each of memory modules <b>360</b>, <b>370</b>, and <b>380</b>. Alternatively, each of MMCs <b>361</b>, <b>371</b>, and <b>381</b> may periodically provide the temperature related information to memory controller <b>318</b> or another controller, which could then store the information in a manner that it is accessible to a hypervisor associated with the host server.</p><p id="p-0029" num="0028">Still referring to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, in one example, the collected temperature values may be stored in control/status registers or other types of memory structures accessible to CPU(s) <b>312</b>. In this example, memory controller may maintain a temperature profile for memory modules <b>360</b>, <b>370</b>, and <b>380</b>. An example temperature profile may include information concerning the most recently measured value of the temperature of each of the memory chips associated with each of the memory modules. In one example, the hypervisor may control a scanning of the temperature profile such that updated information may be accessible to the hypervisor on a periodic basis. The temperature profile may also include relative differences in temperatures compared with a baseline. Thus, in this example, memory chips may have a temperature that is lower or higher than the baseline temperature. The relative temperature difference among memory chips may be as much ten degrees Celsius or more, CPU(s) <b>312</b> may have access to temperature measurements associated with memory chips and less granular data, as needed. The hypervisor associated with the host server may access the temperature profiles for each memory module as part of making memory allocation decisions or as part of redirecting writes to other physical memory locations.</p><p id="p-0030" num="0029">With respect to the access to memory (e.g., DIMMs) associated with the host server, at a broad level, there may be two ways for a compute entity (e.g., a virtual machine (VM)) to access a host server's memory. In those instances, where the VM is accessing the physical memory associated with the CPU it is being executed, then load or store accesses may be translated to the bus transactions by the hardware associated with the system. However, when the VM is provided access to the physical memory associated with a different CPU, then, in one example, the hypervisor may manage this using hardware exceptions caused by an attempted access to unmapped pages. Each hypervisor may be allowed to access host-side page tables or other memory map tables. Access to unmapped pages may cause hardware exceptions, such as page faults. The hypervisor may access the host memory and install page table mappings, after moving the page to the local memory associated with the other host server.</p><p id="p-0031" num="0030">In one example, prior to any such memory operations (or I/O operations) being performed, control information may be exchanged between host servers that are part of a stack or group of servers. The exchange of information may occur between hypervisors (e.g., the hypervisors shown in FIG. <b>1</b>). To enable live migration each host server may reserve a portion of the total host memory to allow for the VM's restart within the host server. The host server may also be required to keep a certain amount of memory reserved for other purposes, including the stack infrastructure overhead and a resiliency reserve, which may relate to the memory reserved to allow for migration of VMs in case of a lack of availability of another host server due to server failure or other such issues. Thus, at least some of the control information may relate to each host server designating memory space that could be accessed by a virtual machine being executed by another host server to allow for live migration of the VMs. In one example, prior to initiating the live migration, the hypervisor may decide that a temperature of at least a certain number of the memory chips meets or exceeds a temperature threshold. Having so decided, the hypervisor may automatically migrate at least a subset of the compute entities from the host server to a different host server provided at least a temperature of a memory associated with that host server does not meet or exceed the temperature threshold. As part of this process, apart from ensuring the live migration to a cooler DIMM, the hypervisor may also ensure that there is enough physical memory in the other host server to allow for the live migration to occur.</p><p id="p-0032" num="0031">Still referring to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, in one example, as part of host server <b>300</b>, loads and stores may be performed using remote direct memory access (RDMA). RDMA may allow copying of the data directly from the memory of one system (e.g., host server <b>110</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>) into that of another (e.g., host server <b>120</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>) without any involvement of either system's operating system. This way, host servers that support RDMA may achieve the zero-copy benefit by transferring data directly to, or from, the memory space of processes, which may eliminate the extra data copy between the application memory and the data buffers in the operating system, In other words, in this example, by using address translation/mapping across the various software/hardware layers, only one copy of the data may be stored in a memory (or an I/O device) associated with the host server.</p><p id="p-0033" num="0032">With continued reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, as needed, a temperature profile of SSDs coupled via the PCIe bus may also be monitored and data corresponding to VMs may be stored in cooler SSDs. As described earlier, bus <b>321</b> may correspond to PCIe busses capable of functioning in accordance with the PCIe specification, including with support for non-transparent bridging, as needed. The PCIe transactions may be routed using address routing, ID-based routing (e.g., using bus, device, and function numbers), or implicit routing using messages. The transaction types may include transactions associated with memory read/write, I/O read/write, configuration read/write, and messaging operations. The endpoints of the PCIe system may be configured using base address registers (BARS). The type of BAR may be configured as a BAR for memory operations or I/O operations. Other set up and configuration may also be performed, as needed. The hardware associated with the PCIe system (e.g., any root complexes, and ports) may further provide functionality to enable the performance of the memory read/write operations and I/O operations. As an example, address translation logic associated with the PCIe system may be used for address translation for packet processing, including packet forwarding or packet dropping.</p><p id="p-0034" num="0033">In one example, a hypervisor executing on host server <b>300</b> may map a memory region associated with an SSD associated with host server <b>300</b> into the guest address space of a virtual machine executing using CPU(s) <b>312</b>. When a loading of data is needed by the VM, the bad may be directly translated into a PCIe transaction. For a store operation, PCIe controller <b>316</b> may transmit the PCIe packets to P2P controller <b>322</b> which then may send it to any of P2P ports <b>328</b>, <b>330</b>, or <b>332</b>. This way data may be stored in an I/O device (e.g., an SSD, an HD, or other I/O devices), which is associated with host server <b>300</b>. The forwarding may also include address translation by the PCIe system. Although <figref idref="DRAWINGS">FIG. <b>3</b></figref> shows host server <b>300</b> as including a certain number of components arranged and coupled in a certain way, it may include fewer or additional components arranged and coupled differently. In addition, the functionality associated with host server <b>300</b> may be distributed or combined, as needed. As an example, although <figref idref="DRAWINGS">FIG. <b>3</b></figref> shows P2P ports to enable the performance of I/O operations, other types of interconnects may also be used to enable such functionality. Alternatively, and/or additionally, any access operations to SSDs associated with the virtual machines being executed by CPU(s) <b>312</b> may be enabled using Remote Direct Memory Access (RDMA).</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows a system environment <b>400</b> for implementing systems and methods in accordance with one example. In this example, system environment <b>400</b> may correspond to a portion of a data center. As an example, the data center may include several clusters of racks including platform hardware, such as server nodes, storage nodes, networking nodes, or other types of nodes. Server nodes may be connected to switches to form a network. The network may enable connections between each possible combination of switches. System environment <b>400</b> may include server1 <b>410</b> and serverN <b>430</b>. System environment <b>400</b> may further include data center related functionality <b>460</b>, including deployment/monitoring <b>470</b>, directory/identity services <b>472</b>, load balancing <b>474</b>, data center controllers <b>476</b> (e.g., software defined networking (SDN) controllers and other controllers), and routers/switches <b>478</b>. Server1 <b>410</b> may include host processor(s) <b>411</b>, host hypervisor <b>412</b>, memory <b>413</b>, storage interface controller(s) (SIC(s)) <b>414</b>, cooling <b>415</b> (e.g., cooling fans or other cooling apparatus), network interface controller(s) (NIC(s)) <b>416</b>, and storage disks <b>417</b> and <b>418</b>. ServerN <b>430</b> may include host processor(s) <b>431</b>, host hypervisor <b>432</b>, memory <b>433</b>, storage interface controller(s) (SIC(s)) <b>434</b>, cooling <b>435</b> (e.g., cooling fans or other cooling apparatus), network interface controller(s) (NIC(s)) <b>436</b>, and storage disks <b>437</b> and <b>438</b>. Server1 <b>410</b> may be configured to support virtual machines, including VM1 <b>419</b>, VM2 <b>420</b>, and VMN <b>421</b>. The virtual machines may further be configured to support applications, such as APP1 <b>422</b>, APP2 <b>423</b>, and APPN <b>424</b>. ServerN <b>430</b> may be configured to support virtual machines, including VM1 <b>439</b>, VM2 <b>440</b>, and VMN <b>441</b>. The virtual machines may further be configured to support applications, such as APP1 <b>442</b>, APP2 <b>443</b>, and APPN <b>444</b>.</p><p id="p-0036" num="0035">With continued reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, in one example, system environment <b>400</b> may be enabled for multiple tenants using the Virtual eXtensible Local Area Network (VXLAN) framework. Each virtual machine (VM) may be allowed to communicate with VMs in the same VXLAN segment. Each VXLAN segment may be identified by a VXLAN Network Identifier (VNI). Although <figref idref="DRAWINGS">FIG. <b>4</b></figref> shows system environment <b>400</b> as including a certain number of components arranged and coupled in a certain way, it may include fewer or additional components arranged and coupled differently. In addition, the functionality associated with system environment <b>400</b> may be distributed or combined, as needed. Moreover, although <figref idref="DRAWINGS">FIG. <b>4</b></figref> describes the access to the unused resources by VMs, other types of compute entities, such as containers, micro-VMs microservices, and unikernels for serverless functions, may access the unused resources associated with the host server in a like manner.</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>5</b></figref> shows a block diagram of a computing platform <b>500</b> (e.g., for implementing certain aspects of the methods and algorithms associated with the present disclosure) in accordance with one example. Computing platform <b>500</b> may include a processor(s) <b>502</b>, I/O component(s) <b>504</b>, memory <b>506</b>, presentation component(s) <b>508</b>, sensor(s) <b>510</b>, database(s) <b>512</b>, networking interface(s) <b>514</b> and I/O Port(s), which may be interconnected via bus <b>520</b>. Processor(s) <b>502</b> may execute instructions stored in memory <b>506</b>. I/O component(s) <b>504</b> may include user interface devices such as a keyboard, a mouse, a voice recognition processor, or touch screens. Memory <b>506</b> may be any combination of non-volatile storage or volatile storage (e.g., flash memory, DRAM, SRAM, or other types of memories). Presentation component(s) <b>508</b> may be any type of display, such as LCD, LED, or other types of display. Sensor(s) <b>510</b> may include telemetry or other types of sensors configured to detect, and/or receive, information (e.g., conditions associated with the devices). Sensor(s) <b>510</b> may include sensors configured to sense conditions associated with CPUs, memory or other storage components, FPGAs, motherboards, baseboard management controllers, or the like. Sensor(s) <b>510</b> may also include sensors configured to sense conditions associated with racks, chassis, fans, power supply units (PSUs), or the like. Sensors <b>510</b> may also include sensors configured to sense conditions associated with Network Interface Controllers (NICs), Top-of-Rack (TOR) switches, Middle-of-Rack (MOR) switches, routers, power distribution units (PDUs), rack level uninterrupted power supply (UPS) systems, or the like.</p><p id="p-0038" num="0037">With continued reference to <figref idref="DRAWINGS">FIG. <b>5</b></figref>, sensor(s) <b>510</b> may be implemented in hardware, software, or a combination of hardware and software. Some sensor(s) <b>510</b> may be implemented using a sensor API that may allow sensor(s) <b>510</b> to receive information via the sensor API. Software configured to detect or listen to certain conditions or events may communicate via the sensor API any conditions associated with devices that are part of the data center or other like systems. Remote sensors or other telemetry devices may be incorporated within the data centers to sense conditions associated with the components installed therein. Remote sensors or other telemetry may also be used to monitor other adverse signals in the data center. As an example, if fans that are cooling a rack stop working then that may be read by the sensors and reported to the deployment and monitoring functions. This type of monitoring may ensure that any impact on the temperature profile-based redirecting of memory writes is detected, recorded, and corrected, as needed.</p><p id="p-0039" num="0038">Still referring to <figref idref="DRAWINGS">FIG. <b>5</b></figref>, database(s) <b>512</b> may be used to store records related to the temperature profiles for redirecting of memory writes and the migration of VMs, including policy records establishing which host servers may implement such functionality. In addition, database(s) <b>512</b> may also store data used for generating reports related to the redirecting of memory writes and migration of VMs based on the temperature profiles.</p><p id="p-0040" num="0039">Networking interface(s) <b>514</b> may include communication interfaces, such as Ethernet, cellular radio, Bluetooth radio, UWB radio, or other types of wireless or wired communication interfaces. I/O port(s) may include Ethernet ports, InfiniBand ports, Fiber Optic port(s), or other types of ports. Although <figref idref="DRAWINGS">FIG. <b>5</b></figref> shows computing platform <b>500</b> as including a certain number of components arranged and coupled in a certain way, it may include fewer or additional components arranged and coupled differently. In addition, the functionality associated with computing platform <b>500</b> may be distributed, as needed.</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>6</b></figref> shows a flowchart <b>600</b> of a method in accordance with one example. In this example, this method may be performed in a cloud computing system including a host server, where the host server comprises at least a first plurality of memory modules coupled to a first processor and at least a second plurality of memory modules coupled to a second processor. As an example, this method may be performed as part of a host server <b>300</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref> as part of system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. Step <b>610</b> may include maintaining a first temperature profile based on information received from a thermal sensor associated with each of a first plurality of memory chips included in at least the first plurality of memory modules. As an example, the first temperature profile may correspond to temperature data associated with memory chips included as part of one of the memory modules described earlier (e.g., one of memory modules <b>360</b>, <b>370</b>, and <b>380</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>). In one example, the hypervisor associated with the host server may manage the first temperature profile.</p><p id="p-0042" num="0041">Step <b>620</b> may include maintaining a second temperature profile based on information received from a thermal sensor associated with each of a second plurality of memory chips included in at least the second plurality of memory modules. As an example, the second temperature profile may correspond to temperature data associated with memory chips included as part of one of the other memory modules described earlier (e.g., one of memory modules <b>360</b>, <b>370</b>, and <b>380</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>). In one example, the hypervisor associated with the host server may manage the second temperature profile. In addition, the hypervisor may also periodically initiate a temperature scan for updating at least one of the first temperature profile or the second temperature profile. As explained earlier, either push or pull (or both in some combination) techniques may be used for updating the temperature profiles. Instructions corresponding to the hypervisor and related modules may be stored in a memory, including, as needed in memory <b>506</b> associated with computing platform <b>500</b>, as needed.</p><p id="p-0043" num="0042">Step <b>630</b> may include, based on at least the first temperature profile, automatically redirecting a first request to write to memory from a first compute entity being executed by the first processor to a selected one of the first plurality of memory chips, whose temperature does not meet or exceed the temperature threshold, included in at least the first plurality of memory modules. In one example, the hypervisor associated with the host server may help automatically redirect the memory write operations. The CPU initiating the write operation may write to the physical memory, with the help of memory controllers (e.g., the memory controllers described earlier with respect to <figref idref="DRAWINGS">FIG. <b>3</b></figref>), based on memory mapping tables maintained by the hypervisor for managing the host server's memory. Instructions corresponding to the hypervisor and related modules may be stored in a memory, including, as needed in memory <b>506</b> associated with computing platform <b>500</b>.</p><p id="p-0044" num="0043">Step <b>640</b> may include, based on at least the second temperature profile, automatically redirecting a second request to write to memory from a second compute entity being executed by the second processor to a selected one of the second plurality of memory chips, whose temperature does not meet or exceed the temperature threshold, included in at least the second plurality of memory modules. In one example, the hypervisor associated with the host server may help automatically redirect the memory write operations. The CPU initiating the write operation may write to the physical memory, with the help of memory controllers (e.g., the memory controllers described earlier with respect to <figref idref="DRAWINGS">FIG. <b>3</b></figref>), based on memory mapping tables maintained by the hypervisor for managing the host server's memory. Instructions corresponding to the hypervisor and related modules may be stored in a memory, including, as needed in memory <b>506</b> associated with computing platform <b>500</b>. Although <figref idref="DRAWINGS">FIG. <b>6</b></figref> describes flow chart <b>600</b> as including a certain number of steps being executed in a certain order, the method may include additional or fewer steps executed in a different order. As an example, the hypervisor may periodically initiate a temperature scan for updating at least one of the first temperature profile or the second temperature profiles. Moreover, the hypervisor may quarantine a memory module (e.g., any of the memory modules, including DIMMs described earlier), selected from at least one of the first memory modules or the second memory modules, based on an analysis of the first temperature profile and the second temperature profile if the memory module includes K number of memory chips, where K is a positive integer, having a temperature in excess of a temperature threshold during an entirety of a predetermined time frame or for a selected number of times during the entirety of the predetermined time frame. Finally, the hypervisor may keep track of a metric related to a use of each of the first plurality of memory modules and the second memory modules by compute entities to prevent overuse of a particular memory module relative to other memory modules. As an example, the metric related to the use of memory modules may relate to the number of times different memory modules were accessed in a predetermined time frame. A histogram may also be used to bin the memory modules experiencing overuse.</p><p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. <b>7</b></figref> shows another flowchart <b>700</b> of a method in accordance with one example. In this example, this method may be performed in a cloud computing system including a first host server and a second host server, wherein the first host server comprises at least a first plurality of memory modules coupled to a first processor and at least a second plurality of memory modules coupled to a second processor, wherein the second host server comprises at least a third plurality of memory modules coupled to a third processor and at least a fourth plurality of memory modules coupled to a fourth processor, wherein the first host server comprises a first hypervisor for managing a first plurality of compute entities for execution by the first processor or the second processor and the second host server comprises a second hypervisor for managing a second plurality of compute entities for execution by the third processor or the fourth processor. In this example, this method may be performed in a host server <b>300</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref> as part of system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0046" num="0045">Step <b>710</b> may include maintaining a first temperature profile based on information received from a thermal sensor associated with each of a first plurality of memory chips included in at least the first plurality of memory modules and at least the second plurality of memory modules. As an example, the first temperature profile may correspond to temperature data associated with memory chips included as part of the memory modules described earlier (e.g., memory modules <b>360</b>, <b>370</b>, and <b>380</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>). In one example, the hypervisor associated with the host server may manage the first temperature profile.</p><p id="p-0047" num="0046">Step <b>720</b> may include maintaining a second temperature profile based on information received from a thermal sensor associated with each of a second plurality of memory chips included in at least the third plurality of memory modules and at least the fourth plurality of memory modules. As an example, the second temperature profile may correspond to temperature data associated with memory chips included as part of other memory modules described earlier (e.g., one of memory modules <b>360</b>, <b>370</b>, and <b>380</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>). In one example, the hypervisor associated with the host server may manage the second temperature profile. In addition, the hypervisor may also periodically initiate a temperature scan for updating at least one of the first temperature profile or the second temperature profile. As explained earlier, either push or pull (or both in some combination) techniques may be used for updating the temperature profiles.</p><p id="p-0048" num="0047">Step <b>730</b> may include, based on at least the first temperature profile, automatically redirecting a first request to write to memory from a first compute entity being executed by the first processor to a selected one of the first plurality of memory chips, whose temperature does not meet or exceed the temperature threshold, included in at least the first plurality of memory modules and at least the second plurality of memory modules. In one example, the hypervisor associated with the host server may help automatically redirect the memory write operations. The CPU initiating the write operation may write to the physical memory, with the help of the memory controllers (e.g., the memory controllers described earlier with respect to <figref idref="DRAWINGS">FIG. <b>3</b></figref>), based on memory mapping tables maintained by the hypervisor for managing the host server's memory.</p><p id="p-0049" num="0048">Step <b>740</b> may include, based on at least the second temperature profile, automatically redirecting a second request to write to memory from a second compute entity being executed by the second processor to a selected one of the second plurality of memory chips, whose temperature does not meet or exceed the temperature threshold, included in at least the third plurality of memory modules and the fourth plurality of memory modules. In one example, the hypervisor associated with the host server may help automatically redirect the memory write operations. The CPU initiating the write operation may write to the physical memory, with the help of memory controllers (e.g., the memory controllers described earlier with respect to <figref idref="DRAWINGS">FIG. <b>3</b></figref>), based on memory mapping tables maintained by the hypervisor for managing the host server's memory.</p><p id="p-0050" num="0049">Step <b>750</b> may include upon determining a temperature of at least N of the first plurality of memory chips meets or exceeds a temperature threshold, where N is a positive integer, automatically migrating at least a subset of the first compute entities from the first host server to the second host server provided at least a temperature of at least one memory chip from among the second plurality of memory chips does not meet or exceed the temperature threshold. As described earlier, with respect to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, live migration of compute entities (e.g., VMs) may be performed by coordination between the hypervisors associated with the two host servers (e.g., the host server from which the VM is being migrated away from and the host server to which the VM is being migrated to). Although <figref idref="DRAWINGS">FIG. <b>7</b></figref> describes flow chart <b>700</b> as including a certain number of steps being executed in a certain order, the method of harvesting resources may include additional steps executed in a different order. As an example, the hypervisor upon determining a temperature of at least O of the second plurality of memory chips meets or exceeds a temperature threshold, where O is a positive integer, automatically migrate at least a subset of the first compute entities from the second host server to the first host server provided at least a temperature of at least one memory chip from among the first plurality of memory chips does not meet or exceed the temperature threshold. In other words, VMs may be migrated from one host server to another and then back to the same host server if the temperature profiles change.</p><p id="p-0051" num="0050">In conclusion, the present disclosure relates to a method in a cloud computing system including a host server, where the host server comprises at least a first plurality of memory modules coupled to a first processor and at least a second plurality of memory modules coupled to a second processor. The method may include maintaining a first temperature profile based on information received from a thermal sensor associated with each of a first plurality of memory chips included in at least the first plurality of memory modules. The method may further include maintaining a second temperature profile based on information received from a thermal sensor associated with each of a second plurality of memory chips included in at least the second plurality of memory modules. The method may further include based on at least the first temperature profile, automatically redirecting a first request to write to memory from a first compute entity being executed by the first processor to a selected one of the first plurality of memory chips, whose temperature does not meet or exceed the temperature threshold, included in at least the first plurality of memory modules. The method may further include based on at least the second temperature profile, automatically redirecting a second request to write to memory from a second compute entity being executed by the second processor to a selected one of the second plurality of memory chips, whose temperature does not meet or exceed the temperature threshold, included in at least the second plurality of memory modules.</p><p id="p-0052" num="0051">The host server may include a hypervisor for managing a plurality of compute entities for execution by the first processor or the second processor, and the hypervisor may be configured to maintain both the first temperature profile and the second temperature profile. The method may further include the hypervisor periodically initiating a temperature scan for updating at least one of the first temperature profile or the second temperature profile.</p><p id="p-0053" num="0052">The method may include the hypervisor quarantining a memory module, selected from at least one of the first memory modules or the second memory modules, based on an analysis of the first temperature profile and the second temperature profile if the memory module includes K number of memory chips, where K is a positive integer, having a temperature in excess of a temperature threshold during an entirety of a predetermined time frame or for a selected number of times during the entirety of the predetermined time frame. The method may further include the hypervisor keeping track of a metric related to a use of each of the first plurality of memory modules and the second memory modules by compute entities to prevent overuse of a particular memory module relative to other memory modules.</p><p id="p-0054" num="0053">The method may further include managing a mapping between a virtual memory allocated to a compute entity and a physical memory. Each of the first compute entity and the second compute entity may comprise at least one of a virtual machine (VM), a micro-VM, a microservice, or a unikernel for serverless functions.</p><p id="p-0055" num="0054">In yet another aspect, the present disclosure relates to a system including a host server comprising at least a first plurality of memory modules coupled to a first processor and at least a second plurality of memory modules coupled to a second processor. The system may further include a hypervisor, associated with the host server. The hypervisor may be configured to: (1) maintain a first temperature profile based on information received from a thermal sensor associated with each of a first plurality of memory chips included in at least the first plurality of memory modules, (2) maintain a second temperature profile based on information received from a thermal sensor associated with each of a second plurality of memory chips included in at least the second plurality of memory modules, (3) based on at least the first temperature profile, automatically redirecting a first request to write to memory from a first compute entity being executed by the first processor to a selected one of the first plurality of memory chips, whose temperature does not meet or exceed the temperature threshold, included in at least the first plurality of memory modules, and (4) based on at least the second temperature profile, automatically redirecting a second request to write to memory from a second compute entity being executed by the second processor to a selected one of the second plurality of memory chips, whose temperature does not meet or exceed the temperature threshold, included in at least the second plurality of memory modules.</p><p id="p-0056" num="0055">The hypervisor may further be configured to periodically initiate a temperature scan for updating at least one of the first temperature profile or the second temperature profile. The hypervisor may further be configured to quarantine a memory module, selected from at least one of the first memory modules or the second memory modules, by analyzing the first temperature profile and the second temperature profile to determine whether the memory module includes at least a K number of memory chips, where K is a positive integer, having a temperature in excess of a temperature threshold during an entirety of a predetermined time frame or for a selected number of times during the entirety of the predetermined time frame.</p><p id="p-0057" num="0056">The hypervisor may further be configured to keep track of a metric related to a use of each of the first plurality of memory modules and the second memory modules by compute entities to prevent overuse of a particular memory module relative to other memory modules. The hypervisor may further be configured to manage a mapping between a virtual memory allocated to a compute entity and a physical memory corresponding to the virtual memory. Each of the first compute entity and the second compute entity may comprise at least one of a virtual machine (VM), a micro-VM, a microservice, or a unikernel for serverless functions.</p><p id="p-0058" num="0057">In another aspect, the present disclosure relates to a method in a cloud computing system including a first host server and a second host server, where the first host server comprises at least a first plurality of memory modules coupled to a first processor and at least a second plurality of memory modules coupled to a second processor, where the second host server comprises at least a third plurality of memory modules coupled to a third processor and at least a fourth plurality of memory modules coupled to a fourth processor, where the first host server comprises a first hypervisor for managing a first plurality of compute entities for execution by the first processor or the second processor and the second host server comprises a second hypervisor for managing a second plurality of compute entities for execution by the third processor or the fourth processor. The method may include maintaining a first temperature profile based on information received from a thermal sensor associated with each of a first plurality of memory chips included in at least the first plurality of memory modules and at least the second plurality of memory modules. The method may further include maintaining a second temperature profile based on information received from a thermal sensor associated with each of a second plurality of memory chips included in at least the third plurality of memory modules and at least the fourth plurality of memory modules. The method may further include based on at least the first temperature profile, automatically redirecting a first request to write to memory from a first compute entity being executed by the first processor to a selected one of the first plurality of memory chips, whose temperature does not meet or exceed the temperature threshold, included in at least the first plurality of memory modules and at least the second plurality of memory modules. The method may further include based on at least the second temperature profile, automatically redirecting a second request to write to memory from a second compute entity being executed by the second processor to a selected one of the second plurality of memory chips, whose temperature does not meet or exceed the temperature threshold, included in at least the third plurality of memory modules and the fourth plurality of memory modules. The method may further include upon determining a temperature of at least N of the first plurality of memory chips meets or exceeds a temperature threshold, wherein N is a positive integer, automatically migrating at least a subset of the first compute entities from the first host server to the second host server provided at least a temperature of at least one memory chip from among the second plurality of memory chips does not meet or exceed the temperature threshold.</p><p id="p-0059" num="0058">The method may further include upon determining a temperature of at least O of the second plurality of memory chips meets or exceeds a temperature threshold, wherein O is a positive integer, automatically migrating at least a subset of the first compute entities from the second host server to the first host server provided at least a temperature of at least one memory chip from among the first plurality of memory chips does not meet or exceed the temperature threshold. The method may further include the first hypervisor periodically initiating a temperature scan for updating the first temperature profile and the second hypervisor periodically initiating a temperature scan for updating the second temperature profile.</p><p id="p-0060" num="0059">The method may further include the first hypervisor quarantining a memory module, selected from at least one of the first memory modules or the second memory modules, by analyzing the first temperature profile to determine whether the memory module includes at least a K number of memory chips, wherein K is a positive integer, having a temperature in excess of a temperature threshold during an entirety of a predetermined time frame or for a selected number of times during the entirety of the predetermined time frame. The method may further include the second hypervisor quarantining a memory module, selected from at least one of the third memory modules or the fourth memory modules, by analyzing the second temperature profile to determine whether the memory module includes at least a K number of memory chips, where K is a positive integer, having a temperature in excess of a temperature threshold during an entirety of a predetermined time frame or for a selected number of times during the entirety of the predetermined time frame.</p><p id="p-0061" num="0060">The method may further include the first hypervisor keeping track of a metric related to a use of each of the first plurality of memory modules and the second memory modules by compute entities to prevent overuse of a particular memory module relative to other memory modules. The method may further include the second hypervisor keeping track of a metric related to a use of each of the third plurality of memory modules and the fourth memory modules by compute entities to prevent overuse of a particular memory module relative to other memory modules.</p><p id="p-0062" num="0061">It is to be understood that the methods, modules, and components depicted herein are merely exemplary. Alternatively, or in addition, the functionality described herein can be performed, at least in part, by one or more hardware logic components. For example, and without limitation, illustrative types of hardware logic components that can be used include Field-Programmable Gate Arrays (FPGAs), Application-Specific Integrated Circuits (ASICs), Application-Specific Standard Products (ASSPs), System-on-a-Chip systems (SOCs), Complex Programmable Logic Devices (CPLDs), etc. In an abstract, but still definite sense, any arrangement of components to achieve the same functionality is effectively &#x201c;associated&#x201d; such that the desired functionality is achieved. Hence, any two components herein combined to achieve a particular functionality can be seen as &#x201c;associated with&#x201d; each other such that the desired functionality is achieved, irrespective of architectures or inter-medial components. Likewise, any two components so associated can also be viewed as being &#x201c;operably connected,&#x201d; or &#x201c;coupled,&#x201d; to each other to achieve the desired functionality.</p><p id="p-0063" num="0062">The functionality associated with some examples described in this disclosure can also include instructions stored in a non-transitory media. The term &#x201c;non-transitory media&#x201d; as used herein refers to any media storing data and/or instructions that cause a machine to operate in a specific manner. Exemplary non-transitory media include non-volatile media and/or volatile media. Non-volatile media include, for example, a hard disk, a solid state drive, a magnetic disk or tape, an optical disk or tape, a flash memory, an EPROM, NVRAM, PRAM, or other such media, or networked versions of such media. Volatile media include, for example, dynamic memory such as DRAM, SRAM, a cache, or other such media. Non-transitory media is distinct from but can be used in conjunction with transmission media. Transmission media is used for transferring data and/or instruction to or from a machine. Exemplary transmission media include coaxial cables, fiber-optic cables, copper wires, and wireless media, such as radio waves.</p><p id="p-0064" num="0063">Furthermore, those skilled in the art will recognize that boundaries between the functionality of the above described operations are merely illustrative. The functionality of multiple operations may be combined into a single operation, and/or the functionality of a single operation may be distributed in additional operations. Moreover, alternative embodiments may include multiple instances of a particular operation, and the order of operations may be altered in various other embodiments.</p><p id="p-0065" num="0064">Although the disclosure provides specific examples, various modifications and changes can be made without departing from the scope of the disclosure as set forth in the claims below. Accordingly, the specification and figures are to be regarded in an illustrative rather than a restrictive sense, and all such modifications are intended to be included within the scope of the present disclosure. Any benefits, advantages, or solutions to problems that are described herein with regard to a specific example are not intended to be construed as a critical, required, or essential feature or element of any or all the claims.</p><p id="p-0066" num="0065">Furthermore, the terms &#x201c;a&#x201d; or an, as used herein, are defined as one or more than one. Also, the use of introductory phrases such as &#x201c;at least one&#x201d; and &#x201c;one or more&#x201d; in the claims should not be construed to imply that the introduction of another claim element by the indefinite articles &#x201c;a&#x201d; or &#x201c;an&#x201d; limits any particular claim containing such introduced claim element to inventions containing only one such element, even when the same claim includes the introductory phrases &#x201c;one or more&#x201d; or &#x201c;at least one&#x201d; and indefinite articles such as &#x201c;a&#x201d; or &#x201c;an.&#x201d; The same holds true for the use of definite articles.</p><p id="p-0067" num="0066">Unless stated otherwise, terms such as &#x201c;first&#x201d; and &#x201c;second&#x201d; are used to arbitrarily distinguish between the elements such terms describe. Thus, these terms are not necessarily intended to indicate temporal or other prioritization of such elements.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-01-20" num="01-20"><claim-text><b>1</b>.-<b>20</b>. (canceled).</claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. A method in a cloud computing system including a first host server and a second host server, wherein the first host server comprises at least a first plurality of memory modules coupled to a first processor and at least a second plurality of memory modules coupled to a second processor, wherein the first plurality of memory modules includes a first plurality of memory chips and the second plurality of modules includes a second plurality of memory chips, wherein the second host server comprises at least a third plurality of memory modules coupled to a third processor and at least a fourth plurality of memory modules coupled to a fourth processor, and wherein the first host server comprises a first hypervisor for managing a first plurality of compute entities for execution by the first processor or the second processor and the second host server comprises a second hypervisor for managing a second plurality of compute entities for execution by the third processor or the fourth processor, the method comprising:<claim-text>maintaining, by the first hypervisor, a first temperature profile based on information received from one or more thermal sensors associated with the first plurality of memory modules;</claim-text><claim-text>maintaining, by the first hypervisor, a second temperature profile based on information received from one or more thermal sensors associated with the second plurality of memory modules;</claim-text><claim-text>based on at least the first temperature profile, the first hypervisor automatically redirecting a first request to write to memory from a first compute entity being executed by the first processor to a selected one of the first plurality of memory chips;</claim-text><claim-text>based on at least the second temperature profile, the first hypervisor automatically redirecting a second request to write to memory from a second compute entity being executed by the second processor to a selected one of the second plurality of memory chips; and</claim-text><claim-text>upon determining each of the first temperature profile and the second temperature profile meets a criterion associated with a change in temperature data associated with at least one of the first plurality of modules or the second plurality of memory modules, the first hypervisor automatically migrating at least a subset of the first compute entities from the first host server to the second host server provided, based on an exchange of information between the first hypervisor and the second hypervisor, a determination is made that the second host server has sufficient memory to allow for live migration to occur.</claim-text></claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. The method of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the third plurality of memory modules includes a third plurality of memory chips and the fourth plurality of modules includes a fourth plurality of memory chips, further comprising the first hypervisor and the second hypervisor exchanging control information to determine whether the third plurality of memory chips or the fourth plurality of memory chips have enough physical memory to allow for live migration to occur.</claim-text></claim><claim id="CLM-00023" num="00023"><claim-text><b>23</b>. The method of <claim-ref idref="CLM-00021">claim 21</claim-ref>, further comprising the first hypervisor periodically initiating a temperature scan for updating at least one of the first temperature profile or the second temperature profile.</claim-text></claim><claim id="CLM-00024" num="00024"><claim-text><b>24</b>. The method of <claim-ref idref="CLM-00021">claim 21</claim-ref>, further comprising the first hypervisor quarantining a memory module, selected from at least one of the first plurality of memory modules or the second plurality of memory modules, based on an analysis of the first temperature profile or the second temperature profile.</claim-text></claim><claim id="CLM-00025" num="00025"><claim-text><b>25</b>. The method of <claim-ref idref="CLM-00021">claim 21</claim-ref>, further comprising the first hypervisor keeping track of a metric related to a use of each of the first plurality of memory modules and the second plurality of memory modules to prevent overuse of a particular memory module relative to other memory modules.</claim-text></claim><claim id="CLM-00026" num="00026"><claim-text><b>26</b>. The method of <claim-ref idref="CLM-00021">claim 21</claim-ref>, further comprising managing a mapping between a virtual memory allocated to a compute entity and a physical memory.</claim-text></claim><claim id="CLM-00027" num="00027"><claim-text><b>27</b>. The method of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein each of the first compute entity and the second compute entity comprises at least one of a virtual machine (VM), a micro-VM, a microservice, or a unikernel for serverless functions.</claim-text></claim><claim id="CLM-00028" num="00028"><claim-text><b>28</b>. A system comprising:<claim-text>a first host server and a second host server, wherein the first host server comprises at least a first plurality of memory modules coupled to a first processor and at least a second plurality of memory modules coupled to a second processor, wherein the first plurality of memory modules includes a first plurality of memory chips and the second plurality of modules includes a second plurality of memory chips, wherein the second host server comprises at least a third plurality of memory modules coupled to a third processor and at least a fourth plurality of memory modules coupled to a fourth processor, and wherein the first host server comprises a first hypervisor for managing a first plurality of compute entities for execution by the first processor or the second processor and the second host server comprises a second hypervisor for managing a second plurality of compute entities for execution by the third processor or the fourth processor;</claim-text><claim-text>a first hypervisor, associated with the first host server, configured to:<claim-text>manage a first plurality of compute entities for execution by the first processor or the second processor,</claim-text><claim-text>maintain a first temperature profile based on information received from one or more thermal sensors associated with the first plurality of memory modules, and</claim-text><claim-text>maintain a second temperature profile based on information received from one or more thermal sensors associated with the second plurality of memory modules; and</claim-text></claim-text><claim-text>a second hypervisor, associated with the second host server, configured to manage a second plurality of compute entities for execution by the third processor or the fourth processor; and</claim-text><claim-text>wherein the first hypervisor is further configured to:<claim-text>based on at least the first temperature profile, automatically redirect a first request to write to memory from a first compute entity being executed by the first processor to a selected one of the first plurality of memory chips,</claim-text><claim-text>based on at least the second temperature profile, automatically redirect a second request to write to memory from a second compute entity being executed by the second processor to a selected one of the second plurality of memory chips, and</claim-text><claim-text>upon determining each of the first temperature profile and the second temperature profile meets a criterion associated with a change in temperature data associated with at least one of the first plurality of modules or the second plurality of memory modules, automatically migrate at least a subset of the first compute entities from the first host server to the second host server provided, based on an exchange of information between the first hypervisor and the second hypervisor, a determination is made that the second host server has sufficient memory to allow for live migration to occur.</claim-text></claim-text></claim-text></claim><claim id="CLM-00029" num="00029"><claim-text><b>29</b>. The system of <claim-ref idref="CLM-00028">claim 28</claim-ref>, wherein the first hypervisor is further configured to periodically initiate a temperature scan for updating at least one of the first temperature profile or the second temperature profile.</claim-text></claim><claim id="CLM-00030" num="00030"><claim-text><b>30</b>. The system of <claim-ref idref="CLM-00028">claim 28</claim-ref>, wherein the first hypervisor is further configured to quarantine a memory module, selected from at least one of the first memory modules or the second memory modules, by analyzing the first temperature profile or the second temperature profile.</claim-text></claim><claim id="CLM-00031" num="00031"><claim-text><b>31</b>. The system of <claim-ref idref="CLM-00028">claim 28</claim-ref>, wherein the first hypervisor is further configured to keep track of a metric related to a use of each of the first plurality of memory modules and the second memory modules to prevent overuse of a particular memory module relative to other memory modules.</claim-text></claim><claim id="CLM-00032" num="00032"><claim-text><b>32</b>. The system of <claim-ref idref="CLM-00028">claim 28</claim-ref>, wherein the first hypervisor is further configured to manage a mapping between a virtual memory allocated to a compute entity and a physical memory corresponding to the virtual memory.</claim-text></claim><claim id="CLM-00033" num="00033"><claim-text><b>33</b>. The system of <claim-ref idref="CLM-00028">claim 28</claim-ref>, wherein each of the first compute entity and the second compute entity comprises at least one of a virtual machine (VM), a micro-VM, a microservice, or a unikernel for serverless functions.</claim-text></claim><claim id="CLM-00034" num="00034"><claim-text><b>34</b>. A method in a cloud computing system including a first host server and a second host server, wherein the first host server comprises at least a first plurality of memory modules coupled to a first processor and at least a second plurality of memory modules coupled to a second processor, wherein the first plurality of memory modules includes a first plurality of memory chips and the second plurality of modules includes a second plurality of memory chips, wherein the second host server comprises at least a third plurality of memory modules coupled to a third processor and at least a fourth plurality of memory modules coupled to a fourth processor, and wherein the first host server comprises a first hypervisor for managing a first plurality of compute entities for execution by the first processor or the second processor and the second host server comprises a second hypervisor for managing a second plurality of compute entities for execution by the third processor or the fourth processor, the method comprising:<claim-text>maintaining; by the first hypervisor, a first temperature profile based on information received from one or more thermal sensors associated with the first plurality of memory modules;</claim-text><claim-text>maintaining, by the first hypervisor, a second temperature profile based on information received from one or more thermal sensors associated with the second plurality of memory modules;</claim-text><claim-text>based on at least the first temperature profile, the first hypervisor automatically redirecting a first request to write to memory from a first compute entity being executed by the first processor to a selected one of the first plurality of memory chips;</claim-text><claim-text>based on at least the second temperature profile, the first hypervisor automatically redirecting a second request to write to memory from a second compute entity being executed by the second processor to a selected one of the second plurality of memory chips;</claim-text><claim-text>upon determining each of the first temperature profile and the second temperature profile meets a criterion associated with a change in temperature data associated with at least one of the first plurality of modules or the second plurality of memory modules, the first hypervisor automatically migrating at least a subset of the first compute entities from the first host server to the second host server provided, based on an exchange of information between the first hypervisor and the second hypervisor, a determination is made that the second host server has sufficient memory to allow or live migration to occur; and</claim-text><claim-text>the first hypervisor keeping track of a first metric related to a use of each of the first plurality of memory modules and the second plurality of memory modules to prevent overuse of a particular memory module relative to other memory modules, and the second hypervisor keeping track of a second metric related to a use of each of the third plurality of memory modules and the fourth memory modules to prevent overuse of a particular memory module relative to other memory modules.</claim-text></claim-text></claim><claim id="CLM-00035" num="00035"><claim-text><b>35</b>. The method of <claim-ref idref="CLM-00034">claim 34</claim-ref>, wherein the third plurality of memory modules includes a third plurality of memory chips and the fourth plurality of modules includes a fourth plurality of memory chips, further comprising the first hypervisor and the second hypervisor exchanging control information to determine whether the third plurality of memory chips or the fourth plurality of memory chips have enough physical memory to allow for live migration to occur.</claim-text></claim><claim id="CLM-00036" num="00036"><claim-text><b>36</b>. The method of <claim-ref idref="CLM-00035">claim 35</claim-ref>, further comprising the first hypervisor periodically initiating a temperature scan for updating at least one of the first temperature profile or the second temperature profile.</claim-text></claim><claim id="CLM-00037" num="00037"><claim-text><b>37</b>. The method of <claim-ref idref="CLM-00036">claim 36</claim-ref>, further comprising the first hypervisor quarantining a memory module, selected from at least one of the first plurality of memory modules or the second plurality of memory modules, based on an analysis of the first temperature profile or the second temperature profile.</claim-text></claim><claim id="CLM-00038" num="00038"><claim-text><b>38</b>. The method of <claim-ref idref="CLM-00037">claim 37</claim-ref>, wherein the first hypervisor quarantining the memory module by analyzing at least one of the first temperature profile or the second temperature profile to determine whether the criterion associated with the change in the temperature data is met during an entirety of a predetermined time frame or for a selected number of times during the entirety of the predetermined time frame.</claim-text></claim><claim id="CLM-00039" num="00039"><claim-text><b>39</b>. The method of <claim-ref idref="CLM-00034">claim 34</claim-ref>, further comprising managing a mapping between a virtual memory allocated to a compute entity and a physical memory.</claim-text></claim><claim id="CLM-00040" num="00040"><claim-text><b>40</b>. The method of <claim-ref idref="CLM-00039">claim 39</claim-ref>, wherein each of the first compute entity and the second compute entity comprises at least one of a virtual machine (VM), a micro-VM, a microservice, or a unikernel for serverless functions.</claim-text></claim></claims></us-patent-application>