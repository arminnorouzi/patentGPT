<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230000377A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221220" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230000377</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17849414</doc-number><date>20220624</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>024</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>25</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>56</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>62</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>82</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>40</main-group><subgroup>10</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>02416</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>25</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>56</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>62</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>82</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>40</main-group><subgroup>15</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>40</main-group><subgroup>117</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>40</main-group><subgroup>11</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">CONTACTLESS IMAGE-BASED BLOOD OXYGEN ESTIMATION</invention-title><us-related-documents><us-provisional-application><document-id><country>US</country><doc-number>63214641</doc-number><date>20210624</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>UNIVERSITY OF MARYLAND, COLLEGE PARK</orgname><address><city>College Park</city><state>MD</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant><us-applicant sequence="01" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>NORTH CAROLINA STATE UNIVERSITY</orgname><address><city>Raleigh</city><state>NC</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>WU</last-name><first-name>Min</first-name><address><city>Clarksville</city><state>MD</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>TIAN</last-name><first-name>Xin</first-name><address><city>College Park</city><state>MD</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>MATHEW</last-name><first-name>Joshua Regi</first-name><address><city>Durham</city><state>NC</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>WONG</last-name><first-name>Chau-Wai</first-name><address><city>Apex</city><state>NC</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Systems, methods, apparatuses, and computer program products for contactless image-based blood oxygen estimation. A method may include receiving an image or video of a part of a subject captured by a camera of a computing device. The method may also include extracting a region of interest of the part of the subject from the image or video. The method may further include performing feature extraction of the region of interest. In addition, the method may include estimating a blood oxygen saturation level of the subject based on a spatial and temporal data analysis of more than two color channels. Feature extraction and estimation of the blood oxygen saturation level may include implementing a combination of spatial averaging, color channel mixing, and temporal trend analysis.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="41.83mm" wi="112.35mm" file="US20230000377A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="166.45mm" wi="174.58mm" file="US20230000377A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="186.86mm" wi="138.09mm" file="US20230000377A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="105.41mm" wi="79.76mm" file="US20230000377A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="217.85mm" wi="173.40mm" orientation="landscape" file="US20230000377A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="86.53mm" wi="46.31mm" file="US20230000377A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="178.48mm" wi="132.42mm" file="US20230000377A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="70.10mm" wi="120.23mm" file="US20230000377A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="164.59mm" wi="112.35mm" file="US20230000377A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="221.91mm" wi="119.04mm" orientation="landscape" file="US20230000377A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="80.01mm" wi="185.00mm" file="US20230000377A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="213.28mm" wi="166.29mm" file="US20230000377A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="174.07mm" wi="72.90mm" file="US20230000377A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="166.45mm" wi="124.29mm" file="US20230000377A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="177.72mm" wi="103.38mm" file="US20230000377A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="109.81mm" wi="78.91mm" file="US20230000377A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION</heading><p id="p-0002" num="0001">This application claims priority to U.S. provisional patent application No. 63/214,641 filed on Jun. 24, 2021. The contents of this earlier filed application are hereby incorporated by reference herein in their entirety.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?federal-research-statement description="Federal Research Statement" end="lead"?><heading id="h-0002" level="1">STATEMENT REGARDING FEDERALLY SPONSORED RESEARCH</heading><p id="p-0003" num="0002">This invention was made with government support under ECCS2030430 and ECCS2030502 awarded by the National Science Foundation. The government has certain rights in the invention.</p><?federal-research-statement description="Federal Research Statement" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0003" level="1">FIELD</heading><p id="p-0004" num="0003">Some example embodiments may generally relate to virtual and augmented reality multifocal displays. For example, certain embodiments may relate to apparatuses, systems, and/or methods for contactless image-based blood oxygen estimation.</p><heading id="h-0004" level="1">BACKGROUND</heading><p id="p-0005" num="0004">Peripheral blood oxygen saturation (SpO<sub>2</sub>) shows the ratio of oxygenated hemoglobin to total hemoglobin in the blood, which serves as a vital health signal for the operational functions of organs and tissues. Specifically, SpO<sub>2 </sub>is an important physiological parameter that represents the level of oxygen supply in the blood, and reflects the adequacy of respiratory function. Thus, the estimation and monitoring of SpO<sub>2 </sub>are essential for the assessment of lung function and the treatment of chronic pulmonary diseases.</p><p id="p-0006" num="0005">Conventional SpO<sub>2 </sub>measurement methods rely on contact-based sensing, including fingertip pulse oximetry and its variants in smartwatches and smartphones. The conventional approach of estimating SpO<sub>2 </sub>via pulse oximeter adopts the ratio-of-ratios (RoR). The RoR principle is based on the different optical absorption rates of the oxygenated hemoglobin (HbO<sub>2</sub>) and deoxygenated hemoglobin (Hb) at 660 nm (red) and 940 nm (infrared) wavelengths. By illuminating red and infrared lights on the peripheral microvascular bed of tissue such as the fingertip, the intensity of the transmitted light on the receiver end of the pulse oximeter contains pulsatile information to derive the level of blood oxygen saturation. Other ways of measuring SpO<sub>2 </sub>is blood gas analysis, which is invasive and painful, and requires well-trained healthcare providers to perform the test. In contrast, the pulse oximeter is noninvasive and provides readings in nearly real-time, and is therefore more tolerated and convenient for daily use. However, the pulse oximeter is known to have a deviation of &#xb1;2% when the blood oxygen saturation is in the range of 70% to 99%.</p><p id="p-0007" num="0006">Although conventional methods can provide measurements of SpO<sub>2</sub>, these conventional contact-based methods may cause discomfort and skin irritation, especially for people with sensitive skin, and are not always accessible to the public. However, with the ubiquity of smartphones and the growing market of smart fitness devices, the RoR principle has been applied to new non-clinical settings for SpO<sub>2 </sub>measurement. These methods require a user to use his/her fingertip to cover an optical sensor, and a nearby light source to capture the reemitted light from the illuminated tissue. As noted above, these SpO<sub>2 </sub>estimation methods are all contact-based. They may irritate sensitive skin, present risks of cross contamination, or cause a sense of burning from the heat built up if the fingertip is in contact with the flashlight on for an extended period of time. Thus, there is a need to provide a way to measure SpO<sub>2 </sub>by means of contactless techniques, which ahs the potential to be adopted in health screening and telehealth.</p><heading id="h-0005" level="1">SUMMARY</heading><p id="p-0008" num="0007">Some example embodiments may be directed to a method. The method may include receiving an image or video of a part of a subject captured by a camera of a computing device. The method may also include extracting a region of interest of the part of the subject from the image or video. The method may further include performing feature extraction of the region of interest. In addition, the method may include estimating a blood oxygen saturation level of the subject based on a spatial and temporal data analysis of more than two color channels. In certain embodiments, feature extraction and estimation of the blood oxygen saturation level may include implementing a combination of spatial averaging, color channel mixing, and temporal trend analysis.</p><p id="p-0009" num="0008">Other example embodiments may be directed to an apparatus. The apparatus may include at least one processor and at least one memory including computer program code. The at least one memory and computer program code may be configured to, with the at least one processor, cause the apparatus at least to receive an image or video of a part of a subject captured by a camera of a computing device. The apparatus may also be caused to extract a region of interest of the part of the subject from the image or video. The apparatus may further be caused to perform feature extraction of the region of interest. In addition, the apparatus may be caused to estimate a blood oxygen saturation level of the subject based on a spatial and temporal data analysis of more than two color channels. According to certain embodiments, feature extraction and estimation of the blood oxygen saturation level may include implementing a combination of spatial averaging, color channel mixing, and temporal trend analysis.</p><p id="p-0010" num="0009">Other example embodiments may be directed to an apparatus. The apparatus may include means for receiving an image or video of a part of a subject captured by a camera of a computing device. The apparatus may also include means for extracting a region of interest of the part of the subject from the image or video. The apparatus may further include means for performing feature extraction of the region of interest. In addition, the apparatus may include means for estimating a blood oxygen saturation level of the subject based on a spatial and temporal data analysis of more than two color channels. According to certain embodiments, feature extraction and estimation of the blood oxygen saturation level may include implementing a combination of spatial averaging, color channel mixing, and temporal trend analysis.</p><p id="p-0011" num="0010">In accordance with other example embodiments, a non-transitory computer-readable medium may be encoded with instructions that may, when executed in one or more machines or one or more hardware devices, perform a method. The method may include receiving an image or video of a part of a subject captured by a camera of a computing device. The method may also include extracting a region of interest of the part of the subject from the image or video. The method may further include performing feature extraction of the region of interest. In addition, the method may include estimating a blood oxygen saturation level of the subject based on a spatial and temporal data analysis of more than two color channels. According to certain embodiments, feature extraction and estimation of the blood oxygen saturation level may include implementing a combination of spatial averaging, color channel mixing, and temporal trend analysis.</p><p id="p-0012" num="0011">Other example embodiments may be directed to a computer program product that performs a method. The method may include receiving an image or video of a part of a subject captured by a camera of a computing device. The method may also include extracting a region of interest of the part of the subject from the image or video. The method may further include performing feature extraction of the region of interest. In addition, the method may include estimating a blood oxygen saturation level of the subject based on a spatial and temporal data analysis of more than two color channels. According to certain embodiments, feature extraction and estimation of the blood oxygen saturation level may include implementing a combination of spatial averaging, color channel mixing, and temporal trend analysis.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0006" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0013" num="0012">For a proper understanding of example embodiments, reference should be made to the accompanying drawings, wherein:</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>1</b>(<i>a</i>)</figref> illustrates an example system for SpO<sub>2 </sub>estimation, according to certain embodiments.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>1</b>(<i>b</i>)</figref> illustrates an example system for SpO<sub>2 </sub>prediction, according to certain embodiments.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>2</b>(<i>a</i>)</figref> illustrates an example of heart rate estimation, according to certain embodiments.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>2</b>(<i>b</i>)</figref> illustrates another example of heart rate estimation, according to certain embodiments.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates an experiment setup, according to certain embodiments.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates learning results of all participants, according to certain embodiments.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates distributions contrasting linear and support vector regressions (SVR), according to certain embodiments.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>6</b>(<i>a</i>)</figref> illustrates a bar plot of the SVR/PU case, according to certain embodiments.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>6</b>(<i>b</i>)</figref> illustrates another bar plot of the SVR/PU case, according to certain embodiments.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates an example blurring effect, according to certain embodiments.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates an example SpO<sub>2 </sub>estimation method, according to certain embodiments.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>9</b></figref> illustrates an example of extinction coefficient curves of hemoglobin, according to certain embodiments.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>10</b></figref> illustrates an example network structure for predicting an SpO<sub>2</sub>, according to certain embodiments.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>11</b>(<i>a</i>)</figref> illustrates an example breathing protocol, according to certain embodiments.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>11</b>(<i>b</i>)</figref> illustrates a histogram of SpO<sub>2 </sub>value sin a collected dataset, according to certain embodiments.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>12</b>(<i>a</i>)</figref> illustrates test predictions of varying performance with reference SpO<sub>2</sub>, according to certain embodiments.</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>12</b>(<i>b</i>)</figref> illustrates training vs. validation predictions, according to certain embodiments.</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>13</b>(<i>a</i>)</figref> illustrates boxplots comparing distributions of correlations, according to certain embodiments.</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>13</b>(<i>b</i>)</figref> illustrates boxplots comparing additional distributions of correlations, according to certain embodiments.</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>14</b></figref> illustrates plots of learned RGB channel weights, according to certain embodiments.</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>15</b></figref> illustrates an example of a flow diagram of a method, according to certain embodiments.</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>16</b></figref> illustrates an example apparatus, according to certain embodiments.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0007" level="1">DETAILED DESCRIPTION</heading><p id="p-0036" num="0035">It will be readily understood that the components of certain example embodiments, as generally described and illustrated in the figures herein, may be arranged and designed in a wide variety of different configurations. The following is a detailed description of some example embodiments of systems, methods, apparatuses, and computer program products for contactless image-based blood oxygen estimation.</p><p id="p-0037" num="0036">The features, structures, or characteristics of example embodiments described throughout this specification may be combined in any suitable manner in one or more example embodiments. For example, the usage of the phrases &#x201c;certain embodiments,&#x201d; &#x201c;an example embodiment,&#x201d; &#x201c;some embodiments,&#x201d; or other similar language, throughout this specification refers to the fact that a particular feature, structure, or characteristic described in connection with an embodiment may be included in at least one embodiment. Thus, appearances of the phrases &#x201c;in certain embodiments,&#x201d; &#x201c;an example embodiment,&#x201d; &#x201c;in some embodiments,&#x201d; &#x201c;in other embodiments,&#x201d; or other similar language, throughout this specification do not necessarily refer to the same group of embodiments, and the described features, structures, or characteristics may be combined in any suitable manner in one or more example embodiments.</p><p id="p-0038" num="0037">Additionally, if desired, the different functions or steps discussed below may be performed in a different order and/or concurrently with each other. Furthermore, if desired, one or more of the described functions or steps may be optional or may be combined. As such, the following description should be considered as merely illustrative of the principles and teachings of certain embodiments, and not in limitation thereof.</p><p id="p-0039" num="0038">Certain embodiments described herein may take advantage of contact-free sensing from a regular RGB camera as well as the conventional SpO<sub>2 </sub>sensing mechanism from pulse oximeters. For instance, certain embodiments may provide a strategic use of video data of skin regions of interests (ROIs) by performing spatial and temporal data analysis of more than two color channels. Additionally, feature extraction and SpO<sub>2 </sub>estimation of the certain embodiments may include a combination of spatial averaging of multiple pixels in the region of interest, color channel mixing, and analyzing the temporal trend. It may also be possible to take advantage of both biophysical imaging principles and the availability of participants' video and SpO<sub>2 </sub>data to learn and determine the details for obtaining SpO<sub>2</sub>-relevant features and making SpO<sub>2 </sub>estimation. Under such an synergistic framework, some of the embodiments may determine the specific features and the related detailed parameters explicitly from the biophysical imaging principles, while other embodiments use these principles to guide a neural network to learn ways to combining the input video signals and determining the corresponding parameters for making the estimation. These latter embodiments may &#x201c;learn&#x201d; the specific SpO<sub>2</sub>-relevant features to extract and carry out feature extraction and SpO<sub>2 </sub>estimation in a holistic manner.</p><p id="p-0040" num="0039">The pulse oximeter, designed by the RoR principle, may leverage the optical absorbance difference of Hb and HbO<sub>2 </sub>at two wavelengths, at the red and infrared wavelengths as illustrated in <figref idref="DRAWINGS">FIG. <b>9</b></figref>. As illustrated in <figref idref="DRAWINGS">FIG. <b>9</b></figref>, the difference between oxygenated hemoglobin (HbO<sub>2</sub>) and deoxygenated hemoglobin (Hb) means that these channels contain useful information for SpO<sub>2 </sub>prediction by means of optophysiological principles. For the commonly seen pulse oximeters, lights at the red and infrared wavelengths are emitted and attenuated by the blood and tissue, and received by an optical sensor, conveying information about pulsatile blood volume. The pulsatile blood volumes at the two wavelengths are further processed to obtain an SpO<sub>2 </sub>estimate.</p><p id="p-0041" num="0040">Based on the traditional RoR principle used to design the pulse oximetry, many contactless methods are developed with a similar spirit that may utilize two color channels of videos in lieu of traditional narrowband red and infrared wavelengths. Based on the setup of cameras and light sources, existing noncontact, video-based SpO<sub>2 </sub>estimation methods can be grouped into two categories. Methods from the first category may utilize monochromatic sensing similar to conventional pulse oximetry. They may use either high-end monochromatic cameras with selected optical filters or controlled monochromatic light sources. The other category may use consumer-grade RGB cameras, such as digital webcams. All these video-based non-contact SpO<sub>2 </sub>estimation methods may utilize the difference in the optophysiological characteristics of oxygenated hemoglobin and deoxygenated hemoglobin. The monochromatic light sources and sensors may be selected to have accurate control of the absorption effect of hemoglobin, while the consumer-grade digital cameras, including webcams and smartphone cameras, may have a wider sensing band and are more challenging for SpO<sub>2 </sub>sensing.</p><p id="p-0042" num="0041">As described above, certain embodiments may implement the RoR model for SpO<sub>2 </sub>measurement. For instance, a light source with spectral distribution I(&#x3bb;) illuminating the skin may be considered, and a remote color camera with spectral responsivity r(&#x3bb;) recording a video may be considered. The light from the source may travel through the tissue, and part of the light in the tissue may be reemitted to be received by the color camera. During each cardiac cycle, the heart muscle contracts and relaxes, so that the blood is pumped in the body and travels back to the heart. During this process, the blood volume increases and decreases in the arterial vessels, causing increased and decreased light absorption. According to a skin-reflection model, the color camera may receive the specularly reflected light from the skin surface, and the diffusely reemitted light from the tissue-light interaction that contains the cardiac-related pulsatile information. Based on the verified assumption that the specular reflection components can be ignored if the movement is minimized, the camera sensor response at time t can be expressed as:</p><p id="p-0043" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>S</i><sub>c</sub>(<i>t</i>)=&#x222b;<sub>&#x39b;</sub><sub><sub2>c</sub2></sub><i>I</i>(&#x3bb;)&#xb7;<i>e</i><sup>&#x3bc;d(&#x3bb;,t)</sup><i>&#xb7;r</i><sub>c</sub>(&#x3bb;)<i>d&#x3bb;.</i>&#x2003;&#x2003;(1)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0044" num="0000">In equation (1), the &#x3bb; is the wavelength, the integral range &#x39b;<sub>c </sub>captures the responsive wavelength band of channel c of the camera, I(&#x3bb;) is the spectral intensity of the light source, &#x3bc;<sub>d</sub>(&#x3bb;, t) is the diffusion coefficient, and r<sub>c</sub>(&#x3bb;) is the sensor response of channel c of the camera.</p><p id="p-0045" num="0042">According to the Beer-Lambert's law, the diffusion coefficient &#x3bc;<sub>d</sub>(&#x3bb;, t) can be expanded into:</p><p id="p-0046" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>&#x3bc;<sub>d</sub>(&#x3bb;,<i>t</i>)=&#x3f5;<sub>t</sub>(&#x3bb;)<i>C</i><sub>t</sub><i>l</i><sub>t</sub>+[&#x3b5;<sub>Hb</sub>(&#x3bb;)<i>C</i><sub>Hb</sub>&#x2014;&#x3b5;<sub>HbO</sub><sub><sub2>2</sub2></sub>(&#x3bb;)<i>C</i><sub>HbO</sub><sub><sub2>2</sub2></sub>]<i>l</i>(<i>t</i>),&#x2003;&#x2003;(2)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0047" num="0000">where &#x3b5;<sub>Hb</sub>, &#x3b5;<sub>HbO</sub><sub><sub2>2</sub2></sub>, and &#x3b5;<sub>t </sub>are the extinction coefficients of arterial deoxyhemoglobin, arterial oxyhemoglobin, and other tissues including the venous blood vessel, respectively; C<sub>t</sub>, C<sub>Hb</sub>, and C<sub>HbO</sub><sub><sub2>2 </sub2></sub>are the concentrations of the corresponding substances. Further, l<sub>t </sub>is the path length that the light travels in the tissue, and may be assumed to be a time-invariant; l(t) is the path length that the light travels in the arterial blood vessels. Further, l(t) is time-varying because the arteries dilate with increased blood during systole compared to diastole.</p><p id="p-0048" num="0043">The integral range &#x39b;<sub>c </sub>can be simplified to a single value &#x3bb;<sub>i </sub>when the camera is monochromatic, and incoming light may be filtered by a narrowband optical filter, or alternatively, the light source may be a narrowband LED. The response of the camera sensor in (1) may be written as:</p><p id="p-0049" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mtext>                </mtext>     <mrow>      <mrow>       <mstyle><mtext>?</mtext></mstyle>       <mrow>        <mo>(</mo>        <mi>t</mi>        <mo>)</mo>       </mrow>      </mrow>      <mo>=</mo>      <mrow>       <mi>I</mi>       <mstyle><mtext>?</mtext></mstyle>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>3</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths><maths id="MATH-US-00001-2" num="00001.2"><math overflow="scroll"> <mrow>  <mstyle><mtext>?</mtext></mstyle>  <mtext mathsize="6pt">indicates text missing or illegible when filed</mtext> </mrow></math></maths></p><p id="p-0050" num="0000">In equation (3), &#x394;l=l<sub>max</sub>&#x2212;l<sub>mindenote </sub>the difference of the light path of the pulsatile arterial blood between diastole when l(t)=l<sub>in </sub>and systole when l(t)=l<sub>max</sub>. The log-ratio of the response of the cth channel of the camera sensor during diastole and systole may then be written as:</p><p id="p-0051" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mi>R</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <msub>       <mi>&#x3bb;</mi>       <mi>i</mi>      </msub>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mrow>      <mi>log</mi>      <mo>(</mo>      <mfrac>       <msub>        <mrow>         <msub>          <mi>S</mi>          <mi>c</mi>         </msub>         <semantics definitionURL="">          <mo>&#x2758;</mo>          <annotation encoding="Mathematica">"\[RightBracketingBar]"</annotation>         </semantics>        </mrow>        <mrow>         <mrow>          <mi>l</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <mi>t</mi>          <mo>)</mo>         </mrow>         <mo>=</mo>         <msub>          <mi>l</mi>          <mi>min</mi>         </msub>        </mrow>       </msub>       <msub>        <mrow>         <msub>          <mi>S</mi>          <mi>c</mi>         </msub>         <semantics definitionURL="">          <mo>&#x2758;</mo>          <annotation encoding="Mathematica">"\[RightBracketingBar]"</annotation>         </semantics>        </mrow>        <mrow>         <mrow>          <mi>l</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <mi>t</mi>          <mo>)</mo>         </mrow>         <mo>=</mo>         <msub>          <mi>l</mi>          <mi>max</mi>         </msub>        </mrow>       </msub>      </mfrac>      <mo>)</mo>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mrow>      <mn>4</mn>      <mo>&#x2062;</mo>      <mi>a</mi>     </mrow>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths><maths id="MATH-US-00002-2" num="00002.2"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mpadded width="0em" lspace="0em" depth="-0.1ex" height="0.1ex">     <mrow>      <mo>=</mo>      <mrow>       <mrow>        <mo>[</mo>        <mrow>         <mrow>          <mrow>           <msub>            <mi>&#x3b5;</mi>            <mi>Hb</mi>           </msub>           <mo>(</mo>           <msub>            <mi>&#x3bb;</mi>            <mi>i</mi>           </msub>           <mo>)</mo>          </mrow>          <mo>&#x2062;</mo>          <msub>           <mi>C</mi>           <mi>Hb</mi>          </msub>         </mrow>         <mo>+</mo>         <mrow>          <mrow>           <msub>            <mi>&#x3b5;</mi>            <msub>             <mi>HbO</mi>             <mn>2</mn>            </msub>           </msub>           <mo>(</mo>           <msub>            <mi>&#x3bb;</mi>            <mi>i</mi>           </msub>           <mo>)</mo>          </mrow>          <mo>&#x2062;</mo>          <msub>           <mi>C</mi>           <msub>            <mi>HbO</mi>            <mn>2</mn>           </msub>          </msub>         </mrow>        </mrow>        <mo>]</mo>       </mrow>       <mo>&#x2062;</mo>       <mi>&#x394;</mi>       <mo>&#x2062;</mo>       <mrow>        <mi>l</mi>        <mo>.</mo>       </mrow>      </mrow>     </mrow>    </mpadded>   </mtd>   <mtd>    <mpadded width="0em" lspace="0em" depth="-0.1ex" height="0.1ex">     <mrow>      <mo>(</mo>      <mrow>       <mn>4</mn>       <mo>&#x2062;</mo>       <mi>b</mi>      </mrow>      <mo>)</mo>     </mrow>    </mpadded>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0052" num="0044">For two different wavelengths &#x3bb;<sub>1 </sub>and &#x3bb;<sub>2</sub>, the RoR can be defined as:</p><p id="p-0053" num="0000"><maths id="MATH-US-00003" num="00003"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mi>RoR</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mrow>       <msub>        <mi>&#x3bb;</mi>        <mn>1</mn>       </msub>       <mo>,</mo>       <msub>        <mi>&#x3bb;</mi>        <mn>2</mn>       </msub>      </mrow>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mrow>      <mfrac>       <mrow>        <mi>R</mi>        <mo>&#x2061;</mo>        <mo>(</mo>        <msub>         <mi>&#x3bb;</mi>         <mn>1</mn>        </msub>        <mo>)</mo>       </mrow>       <mrow>        <mi>R</mi>        <mo>&#x2061;</mo>        <mo>(</mo>        <msub>         <mi>&#x3bb;</mi>         <mn>2</mn>        </msub>        <mo>)</mo>       </mrow>      </mfrac>      <mo>=</mo>      <mrow>       <mfrac>        <mrow>         <mrow>          <mrow>           <msub>            <mi>&#x3b5;</mi>            <mi>Hb</mi>           </msub>           <mo>(</mo>           <msub>            <mi>&#x3bb;</mi>            <mn>1</mn>           </msub>           <mo>)</mo>          </mrow>          <mo>&#x2062;</mo>          <msub>           <mi>C</mi>           <mi>Hb</mi>          </msub>         </mrow>         <mo>+</mo>         <mrow>          <mrow>           <msub>            <mi>&#x3b5;</mi>            <msub>             <mi>HbO</mi>             <mn>2</mn>            </msub>           </msub>           <mo>(</mo>           <msub>            <mi>&#x3bb;</mi>            <mn>1</mn>           </msub>           <mo>)</mo>          </mrow>          <mo>&#x2062;</mo>          <msub>           <mi>C</mi>           <msub>            <mi>HbO</mi>            <mn>2</mn>           </msub>          </msub>         </mrow>        </mrow>        <mrow>         <mrow>          <mrow>           <msub>            <mi>&#x3b5;</mi>            <mi>Hb</mi>           </msub>           <mo>(</mo>           <msub>            <mi>&#x3bb;</mi>            <mn>2</mn>           </msub>           <mo>)</mo>          </mrow>          <mo>&#x2062;</mo>          <msub>           <mi>C</mi>           <mi>Hb</mi>          </msub>         </mrow>         <mo>+</mo>         <mrow>          <mrow>           <msub>            <mi>&#x3b5;</mi>            <msub>             <mi>HbO</mi>             <mn>2</mn>            </msub>           </msub>           <mo>(</mo>           <msub>            <mi>&#x3bb;</mi>            <mn>2</mn>           </msub>           <mo>)</mo>          </mrow>          <mo>&#x2062;</mo>          <msub>           <mi>C</mi>           <msub>            <mi>HbO</mi>            <mn>2</mn>           </msub>          </msub>         </mrow>        </mrow>       </mfrac>       <mo>.</mo>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>5</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0054" num="0000">Since SpO<sub>2</sub>=C<sub>HbO</sub><sub><sub2>2</sub2></sub>/(C<sub>HbO</sub><sub><sub2>2</sub2></sub>+C<sub>Hb</sub>), the relation between RoR and SpO<sub>2 </sub>can be written as:</p><p id="p-0055" num="0000"><maths id="MATH-US-00004" num="00004"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <msub>      <mi>SpO</mi>      <mn>2</mn>     </msub>     <mo>=</mo>     <mfrac>      <mrow>       <mrow>        <msub>         <mi>&#x3b5;</mi>         <mi>Hb</mi>        </msub>        <mo>(</mo>        <msub>         <mi>&#x3bb;</mi>         <mn>1</mn>        </msub>        <mo>)</mo>       </mrow>       <mo>-</mo>       <mrow>        <mrow>         <msub>          <mi>&#x3b5;</mi>          <mi>Hb</mi>         </msub>         <mo>(</mo>         <msub>          <mi>&#x3bb;</mi>          <mn>2</mn>         </msub>         <mo>)</mo>        </mrow>        <mo>&#xb7;</mo>        <mi>RoR</mi>       </mrow>      </mrow>      <mrow>       <mrow>        <msub>         <mi>&#x3b5;</mi>         <mi>Hb</mi>        </msub>        <mo>(</mo>        <msub>         <mi>&#x3bb;</mi>         <mn>1</mn>        </msub>        <mo>)</mo>       </mrow>       <mo>-</mo>       <mrow>        <msub>         <mi>&#x3b5;</mi>         <msub>          <mi>HbO</mi>          <mn>2</mn>         </msub>        </msub>        <mo>(</mo>        <msub>         <mi>&#x3bb;</mi>         <mn>1</mn>        </msub>        <mo>)</mo>       </mrow>       <mo>+</mo>       <mrow>        <mrow>         <mo>[</mo>         <mrow>          <mrow>           <msub>            <mi>&#x3b5;</mi>            <msub>             <mi>HbO</mi>             <mn>2</mn>            </msub>           </msub>           <mo>(</mo>           <msub>            <mi>&#x3bb;</mi>            <mn>2</mn>           </msub>           <mo>)</mo>          </mrow>          <mo>-</mo>          <mrow>           <msub>            <mi>&#x3b5;</mi>            <mi>Hb</mi>           </msub>           <mo>(</mo>           <msub>            <mi>&#x3bb;</mi>            <mn>2</mn>           </msub>           <mo>)</mo>          </mrow>         </mrow>         <mo>]</mo>        </mrow>        <mo>&#xb7;</mo>        <mi>RoR</mi>       </mrow>      </mrow>     </mfrac>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mrow>      <mn>6</mn>      <mo>&#x2062;</mo>      <mi>a</mi>     </mrow>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths><maths id="MATH-US-00004-2" num="00004.2"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mo>&#x2248;</mo>     <mrow>      <mrow>       <mi>&#x3b1;</mi>       <mo>&#xb7;</mo>       <mi>RoR</mi>      </mrow>      <mo>+</mo>      <mrow>       <mi>&#x3b2;</mi>       <mo>.</mo>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mrow>      <mn>6</mn>      <mo>&#x2062;</mo>      <mi>b</mi>     </mrow>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0056" num="0000">In equations (6a) and (6b), the linear approximation may be obtained by a Taylor expansion.</p><p id="p-0057" num="0045">The linear RoR model in (6b) may be applied under different SpO<sub>2 </sub>measurement scenarios. For instance, for pulse oximeters, &#x3bb;<sub>1</sub>=660 nm and &#x3bb;<sub>2</sub>=940 nm may be used to leverage the optical absorption difference of Hb and HbO<sub>2 </sub>at the two wavelengths. In other embodiments, when using narrowband light sources or monochromatic camera sensors for contactless SpO<sub>2 </sub>monitoring, different combinations of (&#x3bb;<sub>1</sub>, &#x3bb;<sub>2</sub>) may be explored. Further, when using consumer-grade RGB cameras two out of the three available RGB channels may be used for the linear RoR model.</p><p id="p-0058" num="0046">Among the above-mentioned SpO<sub>2 </sub>estimation methods using consumer-grade RGB cameras, the SpO<sub>2 </sub>data collected may cover a small dynamic range (mostly above 95%), which may not be very meaningful. However, a fitted linear relation between RoR and SpO<sub>2 </sub>may be achieved for data that last several minutes. These limitations can be attributed to that, unlike the signals captured in the narrowband setting that is modeled precisely by (3) and (4), all three RGB color channels capture a wide range of wavelengths from the ambient light, as is described in equation (1). The aggregation of the broad range of wavelengths lowers the optical difference between Hb and HbO<sub>2</sub>, and makes it less optically selective than narrowband sensors used in oximeters. Thus, to address this issue, certain embodiments may disentangle the aggregation through a combination of the pulsatile signals from all three channels of RGB videos to efficiently distill the SpO<sub>2 </sub>information.</p><p id="p-0059" num="0047">In certain embodiments, a multi-channel RoR method may be used for non-contact SpO<sub>2 </sub>monitoring using hand videos captured by cameras including, for example, smartphone cameras under ambient light. For instance, certain embodiments may exploit all three RGB channels to extract features for SpO<sub>2 </sub>prediction, instead of being limited to two wavelengths/color channels as in traditional RoR methods. Certain embodiments may also take into consideration the underlying optophysiological model given the smartphone camera as the remote sensor and the ambient light environment. In other embodiments, the multi-channel RoR based method may achieve a mean absolute error of 1.26% in SpO<sub>2 </sub>estimation with the pulse oximeter as the reference, which is 25% lower than that of the traditional RoR model.</p><p id="p-0060" num="0048">According to certain embodiments, the RGB signals may be filtered with a narrow adaptive bandpass (ABP) filter centered at an accurately estimated heart rate (HR) to obtain the most relevant cardiovascular-related AC component from each color channel for feature extraction. Certain embodiments may also systematically analyze and verify the important roles of both the narrow ABP filter and the accurate HR tracking for accurate SpO<sub>2 </sub>monitoring.</p><p id="p-0061" num="0049">According to other example embodiments, data collection may be accomplished by using the hand as the signal source instead of an individual's face. In doing so, it may be advantageous at least because there is less of a concern for privacy, and potentially being more tolerant to different skin tones than the face. Certain embodiments further analyze the impact of the sides of the hand and skin tones on the SpO<sub>2 </sub>estimation performance. Given the collected dataset of certain embodiments, it has been found that using the palm side for video capturing has a good SpO<sub>2 </sub>estimation performance regardless of the skin tones. There are also no significant performance differences between skin-tone subgroups if the palm side is used for video capturing.</p><p id="p-0062" num="0050">As described herein, some neural network work for SpO<sub>2 </sub>prediction may explore prediction, but not the model explainability. Explainability/interpretability may be highly desirable in many applications yet often not sufficiently addressed, partly due to the black box nature of neural networks. From a healthcare standpoint, explainability is a key factor which should be taken into account at the beginning of the design of a system. To extract features from the skin color signals and estimate SpO<sub>2</sub>, certain embodiments provide physiologically motivated neural network structures. These structures may be designed to be physically explainable. For heart rate sensing and respiratory rate sensing, the RGB skin color signals may be combined first, as in the plane-orthogonal-to-skin (POS) algorithm, followed by temporal feature extraction. In contrast, for SpO<sub>2 </sub>sensing methods such as the RoR, the color components are combined at the end. The neural network structures of certain embodiments explore different arrangements of channel combination and temporal feature extraction. As such, certain embodiments may systematically compare the performance of explainable model structures.</p><p id="p-0063" num="0051"><figref idref="DRAWINGS">FIG. <b>1</b>(<i>a</i>)</figref> illustrates an example system for SpO<sub>2 </sub>estimation, according to certain embodiments, and <figref idref="DRAWINGS">FIG. <b>1</b>(<i>b</i>)</figref> illustrates an example system for SpO<sub>2 </sub>prediction, according to certain embodiments. In particular, <figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates an example procedure for the SpO<sub>2 </sub>estimation from smartphone captured hand videos. According to certain embodiments, the pixels from the hand region may be utilized for prediction, and a remote photoplethysmogram (rPPG) signal may be extracted for heart rate (HR) estimation. Furthermore, multi-channel RoR features may be derived from the spatially combined RGB signals with the help of the HR-guided filters. The extracted features may then be used for SpO<sub>2 </sub>prediction.</p><p id="p-0064" num="0052">In certain embodiments, first, the hand is detected as the region of interest (ROI) for each frame. Second, the spatial average from the ROI is calculated to obtain three time-varying signals of RGB channels. In some embodiments, the averaged RGB signals may be extracted to estimate HR, and to acquire the filtered cardio-related AC components using an HR-based adaptive bandpass filter. Additionally, the ratio between the AC and the DC components for each color channel, and the pairwise ratios of the resulting three ratios may be computed as the features for a regression model where SpO<sub>2 </sub>is treated as the label.</p><p id="p-0065" num="0053">As illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, ROI may be generated via thresholding and spatial combining. For instance, to facilitate the data collection with good quality, certain embodiments may use a rectangle to enclose the target hand region. Other embodiments may use an interactive user interface for this step, which can be replaced by an automated hand detection algorithm if desired. Additionally, the pixels in this region may be converted from the RGB color space to the YCbCr color space, and the Cr channel may be used to determine a threshold that best differentiates the skin pixels from the background using the Otsu algorithm. Other embodiments may apply morphological erosion and dilation operations with a median filter to exclude noise pixels outside the region of the binary hand mask. The final hand-shaped mask may be considered as the ROI, and an example is shown in the second picture in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. For all n frames in the video, the spatial average values of the red, green, and blue channels in the ROI may be calculated, and denote them as <o ostyle="single">r</o>, <o ostyle="single">g</o>, <o ostyle="single">b</o>&#x2208;<img id="CUSTOM-CHARACTER-00001" he="3.22mm" wi="2.12mm" file="US20230000377A1-20230105-P00001.TIF" alt="custom-character" img-content="character" img-format="tif"/><sup>1&#xd7;n</sup>, and arrange them into a matrix A=[<o ostyle="single">r</o>; <o ostyle="single">g</o>; <o ostyle="single">b</o>]&#x2208;<img id="CUSTOM-CHARACTER-00002" he="3.22mm" wi="2.12mm" file="US20230000377A1-20230105-P00001.TIF" alt="custom-character" img-content="character" img-format="tif"/><sup>3&#xd7;n</sup>.</p><p id="p-0066" num="0054">As further illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the prediction procedure may include rPPG extraction and HR estimation. According to certain embodiments, in a RoR method, after matrix A is calculated, the AC component for each channel of A may be quantified by either the standard deviation or the peak-to-valley amplitude. Since the signal-to-noise ratio (SNR) is lower for the video capture by a smartphone in a contactless manner, certain embodiments may use an adaptive bandpass filter centered at the HR frequency to clean the RGB channel signals to extract the AC components more precisely.</p><p id="p-0067" num="0055">According to certain embodiments, HR can be measured contact-free by capturing the pulse-induced subtle color variations of the skin. The pulse signal (e.g., rPPG), can be obtained from applying the plane-orthogonal-to-skin (POS) algorithm, which defines a plane orthogonal to the skin tone in the RGB space for robust rPPG extraction. The HR may then be tracked from the rPPG signal via an adaptive multi-trace carving (AMTC) algorithm that tracks the HR from the spectrogram of rPPG by dynamic programming and adaptive trace compensation.</p><p id="p-0068" num="0056">To analyze the role of accurate HR tracking for feature extraction, certain embodiments may implement a peak-finding method and a weighted energy method for frequency estimation to compare with AMTC. The peak-finding method may take the peaks of the squared magnitude of the Fourier transform of rPPG as the estimated HR values. The weighted energy method may find the heart rate by weighing the frequency bins in the corresponding frame of the spectrogram of rPPG. Compared to the peak-finding method, the weighted energy method may be more robust to outliers in frequency. <figref idref="DRAWINGS">FIGS. <b>2</b>(<i>a</i>) and <b>2</b>(<i>b</i>)</figref> illustrate an example of the HR estimation results by the peak-finding method, the weighted energy algorithm, and AMTC, respectively, according to certain embodiments. In particular, <figref idref="DRAWINGS">FIG. <b>2</b>(<i>a</i>)</figref> illustrates a spectrogram of an rPPG signal, according to certain embodiments. Further, <figref idref="DRAWINGS">FIG. <b>2</b>(<i>b</i>)</figref> illustrates a reference HR signal and HR signals estimated by the peak-finding method, the weighted energy frequency estimation algorithm, and the AMTC algorithm, respectively, according to certain example embodiments. As shown in <figref idref="DRAWINGS">FIG. <b>2</b>(<i>b</i>)</figref>, the mean absolute errors (MAEs) of the HR estimation algorithms are 6.00, 4.94, and 2.50 bpm, respectively.</p><p id="p-0069" num="0057">As further illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the prediction procedure of SpO<sub>2 </sub>may include feature extraction. In feature extraction, certain embodiments may use a processing window of 10 seconds with a step size of 1 second to segment the whole video into L windows. Within each window, the DC and AC components of the RGB channels may be calculated to build a feature vector f.</p><p id="p-0070" num="0058">For the DC component, certain embodiments may use a second-order lowpass Butterworth filter with a cutoff frequency at 0.1 Hz. Additionally, the DC component may be estimated using the median of the lowpass filtered signal of each window. As for the AC component, the estimated HR values may be used as the center frequencies for the adaptive bandpass (ABP) filters to extract the AC components of the RGB channels, which eliminates frequency components that are unrelated to the cardiac pulse. Other embodiments may adopt an 8th-order Butterworth bandpass filter with &#xb1;0.1 Hz (&#xb1;0.6 bpm) bandwidth, centering at the estimated HR of the current window. The magnitude of the AC component may be estimated using the average of the peak-to-valley amplitudes of the filtered signals within the current processing window.</p><p id="p-0071" num="0059">In certain embodiments, the normalized AC components may be defined at the ith window as</p><p id="p-0072" num="0000"><maths id="MATH-US-00005" num="00005"><math overflow="scroll"> <mrow>  <mrow>   <mrow>    <mi>R</mi>    <mo>&#x2061;</mo>    <mo>(</mo>    <mrow>     <mi>i</mi>     <mo>,</mo>     <mi>c</mi>    </mrow>    <mo>)</mo>   </mrow>   <mo>=</mo>   <mfrac>    <mrow>     <mi>AC</mi>     <mo>&#x2061;</mo>     <mo>(</mo>     <mrow>      <mi>i</mi>      <mo>,</mo>      <mi>c</mi>     </mrow>     <mo>)</mo>    </mrow>    <mrow>     <mi>DC</mi>     <mo>&#x2061;</mo>     <mo>(</mo>     <mrow>      <mi>i</mi>      <mo>,</mo>      <mi>c</mi>     </mrow>     <mo>)</mo>    </mrow>   </mfrac>  </mrow>  <mo>,</mo> </mrow></math></maths></p><p id="p-0073" num="0000">where c&#x2208;{r, g, b} represents color channel, and i&#x2208;{1, 2, . . . , L}. Additionally, certain embodiments may define the multi-channel RoR based feature vector of the ith window as</p><p id="p-0074" num="0000"><maths id="MATH-US-00006" num="00006"><math overflow="scroll"> <mrow>  <msub>   <mi>f</mi>   <mi>i</mi>  </msub>  <mo>=</mo>  <mrow>   <mrow>    <mo>[</mo>    <mrow>     <mrow>      <mi>R</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mrow>       <mi>i</mi>       <mo>,</mo>       <mi>r</mi>      </mrow>      <mo>)</mo>     </mrow>     <mo>,</mo>     <mrow>      <mi>R</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mrow>       <mi>i</mi>       <mo>,</mo>       <mi>g</mi>      </mrow>      <mo>)</mo>     </mrow>     <mo>,</mo>     <mrow>      <mi>R</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mrow>       <mi>i</mi>       <mo>,</mo>       <mi>b</mi>      </mrow>      <mo>)</mo>     </mrow>     <mo>,</mo>     <mfrac>      <mrow>       <mi>R</mi>       <mo>&#x2061;</mo>       <mo>(</mo>       <mrow>        <mi>i</mi>        <mo>,</mo>        <mi>r</mi>       </mrow>       <mo>)</mo>      </mrow>      <mrow>       <mi>R</mi>       <mo>&#x2061;</mo>       <mo>(</mo>       <mrow>        <mi>i</mi>        <mo>,</mo>        <mi>g</mi>       </mrow>       <mo>)</mo>      </mrow>     </mfrac>     <mo>,</mo>     <mfrac>      <mrow>       <mi>R</mi>       <mo>&#x2061;</mo>       <mo>(</mo>       <mrow>        <mi>i</mi>        <mo>,</mo>        <mi>r</mi>       </mrow>       <mo>)</mo>      </mrow>      <mrow>       <mi>R</mi>       <mo>&#x2061;</mo>       <mo>(</mo>       <mrow>        <mi>i</mi>        <mo>,</mo>        <mi>b</mi>       </mrow>       <mo>)</mo>      </mrow>     </mfrac>     <mo>,</mo>     <mfrac>      <mrow>       <mi>R</mi>       <mo>&#x2061;</mo>       <mo>(</mo>       <mrow>        <mi>i</mi>        <mo>,</mo>        <mi>g</mi>       </mrow>       <mo>)</mo>      </mrow>      <mrow>       <mi>R</mi>       <mo>&#x2061;</mo>       <mo>(</mo>       <mrow>        <mi>i</mi>        <mo>,</mo>        <mi>b</mi>       </mrow>       <mo>)</mo>      </mrow>     </mfrac>    </mrow>    <mo>]</mo>   </mrow>   <mo>&#x2208;</mo>   <mrow>    <msup>     <mi>&#x211d;</mi>     <mrow>      <mn>1</mn>      <mo>&#xd7;</mo>      <mn>6</mn>     </mrow>    </msup>    <mo>.</mo>   </mrow>  </mrow> </mrow></math></maths></p><p id="p-0075" num="0060">According to certain embodiments, linear regression (LR) and support vector regression (SVR) may be used to learn the mapping between the features and the SpO<sub>2 </sub>level. Since LR captures the linear relationship, it has limited learning capability, and may serve as a baseline. The objective function is</p><p id="p-0076" num="0000"><maths id="MATH-US-00007" num="00007"><math overflow="scroll"> <mrow>  <mrow>   <mrow>    <munder>     <mi>min</mi>     <mi>w</mi>    </munder>    <msup>     <mrow>      <mo>&#xf605;</mo>      <mrow>       <mi>y</mi>       <mo>-</mo>       <mi>Fw</mi>      </mrow>      <mo>&#xf606;</mo>     </mrow>     <mn>2</mn>    </msup>   </mrow>   <mo>+</mo>   <mrow>    <mi>&#x3bb;</mi>    <mo>&#x2062;</mo>    <msubsup>     <mrow>      <mo>&#xf605;</mo>      <mi>w</mi>      <mo>&#xf606;</mo>     </mrow>     <mn>2</mn>     <mn>2</mn>    </msubsup>   </mrow>  </mrow>  <mo>,</mo> </mrow></math></maths></p><p id="p-0077" num="0000">where y=[y<sub>1</sub>, . . . y<sub>L</sub>]<sup>T</sup>&#x2208;<img id="CUSTOM-CHARACTER-00003" he="3.22mm" wi="2.12mm" file="US20230000377A1-20230105-P00001.TIF" alt="custom-character" img-content="character" img-format="tif"/><sup>L&#xd7;1 </sup>contains the target SpO<sub>2 </sub>values, F=[f<sub>1</sub>; . . . ; f<sub>L</sub>]&#x2208;<img id="CUSTOM-CHARACTER-00004" he="3.22mm" wi="2.12mm" file="US20230000377A1-20230105-P00001.TIF" alt="custom-character" img-content="character" img-format="tif"/><sup>L&#xd7;6 </sup>is the feature/data matrix derived from the input, w&#x2208;<img id="CUSTOM-CHARACTER-00005" he="3.22mm" wi="2.12mm" file="US20230000377A1-20230105-P00001.TIF" alt="custom-character" img-content="character" img-format="tif"/><sup>6&#xd7;1 </sup>and contains the weights. In certain embodiments, an l<sub>2</sub>-regularization term is added to the objective function to avoid rank deficiency caused by the collinearity among features. To select the optimal regularization parameter &#x3bb;, a 5-fold cross-validation may be used. In addition to LR, certain embodiments may use the SVR to capture the nonlinearity of the features. Additionally, the Libsvm library may be used for training the &#x3f5;-SVR,</p><p id="p-0078" num="0000"><maths id="MATH-US-00008" num="00008"><math overflow="scroll"> <mrow>  <mrow>   <mrow>    <munder>     <mi>min</mi>     <mrow>      <mi>w</mi>      <mo>,</mo>      <mi>b</mi>     </mrow>    </munder>    <mfrac>     <mn>1</mn>     <mn>2</mn>    </mfrac>    <mo>&#x2062;</mo>    <msup>     <mrow>      <mo>&#xf605;</mo>      <mi>w</mi>      <mo>&#xf606;</mo>     </mrow>     <mn>2</mn>    </msup>   </mrow>   <mo>+</mo>   <mrow>    <mi>C</mi>    <mo>&#x2062;</mo>    <mrow>     <msubsup>      <mo>&#x2211;</mo>      <mrow>       <mi>i</mi>       <mo>=</mo>       <mn>1</mn>      </mrow>      <mi>L</mi>     </msubsup>     <mrow>      <msub>       <mi>&#x2112;</mi>       <mi>&#x3f5;</mi>      </msub>      <mo>(</mo>      <mrow>       <msub>        <mi>y</mi>        <mi>i</mi>       </msub>       <mo>,</mo>       <mrow>        <mrow>         <msup>          <mi>w</mi>          <mi>T</mi>         </msup>         <mo>&#x2062;</mo>         <mrow>          <mi>&#x3d5;</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <msub>           <mi>f</mi>           <mi>i</mi>          </msub>          <mo>)</mo>         </mrow>        </mrow>        <mo>+</mo>        <mi>b</mi>       </mrow>      </mrow>      <mo>)</mo>     </mrow>    </mrow>   </mrow>  </mrow>  <mo>,</mo> </mrow></math></maths></p><p id="p-0079" num="0000">where <img id="CUSTOM-CHARACTER-00006" he="3.22mm" wi="2.46mm" file="US20230000377A1-20230105-P00002.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>E </sub>is the linear &#x3f5;-insensitive loss function. In certain embodiments, the implementation may use the radial basis function (RBF) kernel to capture the nonlinearity. Additionally, the hyperparameters, including the penalty cost C and the kernel parameter &#x3b3; of kernel function K(f<sub>i</sub>,f<sub>j</sub>)=&#x3d5;(f<sub>i</sub>)<sup>T</sup>&#x3d5;(f<sub>j</sub>)=exp(&#x2212;&#x3b3;&#x2225;f<sub>i</sub>&#x2212;f<sub>j</sub>&#x2225;<sup>2</sup>) are selected via a grid search over a 5-fold cross-validation loss.</p><p id="p-0080" num="0061">In certain embodiments, once an estimated weight vector &#x175; is learned from the linear or support vector regression, &#x175; may then be used to predict a preliminary SpO<sub>2 </sub>signal. Further, a 10-second moving average window may be applied to smooth out the preliminarily predicted signal to obtain the final predicted SpO<sub>2 </sub>signal.</p><p id="p-0081" num="0062">Examples of Multi-Channel RoR Based Estimation</p><p id="p-0082" num="0063">Fourteen volunteers, including eight females and six males, were enrolled, with an age range between 21 and 30, and Fitzpatrick skin types II-V. There were two, eight, one, and three participants having skin types II, III, IV, and V, respectively. None of the participants had any known cardiovascular or respiratory diseases. During the data collection, participants were asked to hold their breath to induce a wide dynamic range of SpO<sub>2 </sub>levels. In some embodiments, the SpO<sub>2 </sub>range for a healthy person may be from 95% to 100%. By holding their breath, the SpO<sub>2 </sub>level can drop below 90%. Once the participant resumes normal breathing, the SpO<sub>2 </sub>will return to the level before the breath-holding.</p><p id="p-0083" num="0064">During data collection, each participant was recorded for two sessions. During the recording, the participant sat comfortably in an upright position and put both hands on a clean dark foam sheet placed on a table. <figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates an experiment setup, according to certain embodiments. In particular, as shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the palm side of the right hand and the back side of the left hand were facing the camera. These two hand-video capturing positions are defined as palm up (PU) and palm down (PD), respectively. Simultaneously, a Contec CMS-50E pulse oximeter was clipped to the left index finger to measure the participant's SpO<sub>2 </sub>level at the sampling rate of 1 Hz. The oximeter may be adopted clinically as to be within a &#xb1;2% deviation from the invasive standard for SpO<sub>2</sub>. Thus, certain embodiments may use the oximeter measurement results as the reference in the experiments. A smartphone camera was used for video recording at the sampling rate of 30 fps, and the video started 30 seconds before the oximeter starts, and stopped immediately after the oximeter ends to allow for proper time synchronization. The participants were asked to hold their breath for 30-40 seconds to lower the SpO<sub>2 </sub>level, and were told to resume normal breathing for 30-40 seconds if they felt discomfort. The aforementioned process is defined as one breath-holding cycle. In each session, the breath-holding cycles were repeated three times, as illustrated in <figref idref="DRAWINGS">FIG. <b>11</b>(<i>a</i>)</figref>. After the first session, the participants were asked to relax for at least 15 minutes before attending the second session for data collection. From the data collection protocol using breath-holding, it was possible to obtain the SpO<sub>2 </sub>measurements with a dynamic range from 89% to 99%. A histogram of SpO<sub>2 </sub>values in the collected dataset is illustrated in <figref idref="DRAWINGS">FIG. <b>11</b>(<i>b</i>)</figref>.</p><p id="p-0084" num="0065">According to certain embodiments, the total length of recording time for all fourteen participants was 138.9 minutes. The data size was relatively small for large-scale neural network training. The available data, however, was adequate for the principled multi-channel signal based approach to SpO<sub>2 </sub>monitoring, showing a benefit of combining signal processing and biomedical knowledge and modeling with data than the primarily data-driven approach.</p><p id="p-0085" num="0066">In certain embodiments, when the CMS-50E oximeter was turned on and ready for measurement, the first reading is displayed a few seconds after the finger is inserted. This delay may be due to oximeter's internal firmware startup and algorithmic processing. Since the video and the oximeter readings need to be synchronized using their precise starting time stamps, the delay in the oximeter can introduce misalignment errors in the reference data used to train the regression model. To avoid the misalignment, the delay was first estimated, and then compensated for the delay in the training and testing. To do so, one participant was asked to repeatedly place the left index finger, middle finger, and ring finger into the oximeter 50 times each and obtained the average delay time of 1.8 s, 1.9 s, and 1.7 s, respectively. Since the left index finger is used for reference data collection in the setup, 1.8 s was taken as the delay. To further examine whether there exists any difference among the delays from the three fingers, a one-way ANOVA test was conducted. The p-value was 0.14, which shows no statistically significant different delays among the three fingers.</p><p id="p-0086" num="0067">The performance of the algorithm may be evaluated using the mean absolute error (MAE) (equation (7a)), and Pearson's correlation coefficient &#x3c1; (equation (7b)) given below:</p><p id="p-0087" num="0000"><maths id="MATH-US-00009" num="00009"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mrow>       <mi>M</mi>       <mo>&#x2062;</mo>       <mi>A</mi>       <mo>&#x2062;</mo>       <mrow>        <mi>E</mi>        <mo>&#x2061;</mo>        <mo>(</mo>        <mrow>         <mi>y</mi>         <mo>,</mo>         <mover>          <mi>y</mi>          <mo>^</mo>         </mover>        </mrow>        <mo>)</mo>       </mrow>      </mrow>      <mo>=</mo>      <mrow>       <mfrac>        <mn>1</mn>        <mi>N</mi>       </mfrac>       <mo>&#x2062;</mo>       <mrow>        <munderover>         <mo>&#x2211;</mo>         <mrow>          <mi>i</mi>          <mo>=</mo>          <mn>1</mn>         </mrow>         <mi>N</mi>        </munderover>        <mtext> </mtext>        <mrow>         <mo>&#xf605;</mo>         <mrow>          <msub>           <mi>y</mi>           <mi>i</mi>          </msub>          <mo>-</mo>          <msub>           <mover>            <mi>y</mi>            <mo>^</mo>           </mover>           <mi>i</mi>          </msub>         </mrow>         <mo>&#xf606;</mo>        </mrow>       </mrow>      </mrow>     </mrow>     <mo>,</mo>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mrow>      <mn>7</mn>      <mo>&#x2062;</mo>      <mi>a</mi>     </mrow>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths><maths id="MATH-US-00009-2" num="00009.2"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mi>&#x3c1;</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mrow>       <mi>y</mi>       <mo>,</mo>       <mover>        <mi>y</mi>        <mo>^</mo>       </mover>      </mrow>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mrow>      <mfrac>       <mrow>        <msup>         <mrow>          <mo>(</mo>          <mrow>           <mi>y</mi>           <mo>-</mo>           <mover>            <mi>y</mi>            <mo>_</mo>           </mover>          </mrow>          <mo>)</mo>         </mrow>         <mi>T</mi>        </msup>        <mo>&#x2062;</mo>        <mrow>         <mo>(</mo>         <mrow>          <mover>           <mi>y</mi>           <mo>^</mo>          </mover>          <mo>-</mo>          <mover>           <mover>            <mi>y</mi>            <mo>^</mo>           </mover>           <mo>_</mo>          </mover>         </mrow>         <mo>)</mo>        </mrow>       </mrow>       <mrow>        <msub>         <mrow>          <mo>&#xf605;</mo>          <mrow>           <mi>y</mi>           <mo>-</mo>           <mover>            <mi>y</mi>            <mo>_</mo>           </mover>          </mrow>          <mo>&#xf606;</mo>         </mrow>         <mn>2</mn>        </msub>        <mo>&#x2062;</mo>        <msub>         <mrow>          <mo>&#xf605;</mo>          <mrow>           <mover>            <mi>y</mi>            <mo>^</mo>           </mover>           <mo>-</mo>           <mover>            <mover>             <mi>y</mi>             <mo>^</mo>            </mover>            <mo>_</mo>           </mover>          </mrow>          <mo>&#xf606;</mo>         </mrow>         <mn>2</mn>        </msub>       </mrow>      </mfrac>      <mo>.</mo>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mrow>      <mn>7</mn>      <mo>&#x2062;</mo>      <mi>b</mi>     </mrow>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0088" num="0000">In equations (7a) and (7b), y=[y<sub>1</sub>, . . . , y<sub>N</sub>]<sup>T</sup>, &#x177;=[&#x177;<sub>1</sub>, . . . , &#x177;<sub>N</sub>]<sup>T</sup>, <o ostyle="single">y</o> and &#x177; denote the reference SpO<sub>2 </sub>signal, the estimated SpO<sub>2 </sub>signal, the average values of all coordinates of vectors y and &#x177;, respectively. Additionally, the correlation metric may be adopted to evaluate how well the trend of the SpO<sub>2 </sub>signal is tracked.</p><p id="p-0089" num="0068">In certain embodiments, the training data from one participant was used to train the regression model for the prediction of his/her testing session recorded a period of time later. In the aforementioned training and testing procedure the models were specifically learned for each participant.</p><p id="p-0090" num="0069"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates learning results of all participants, according to certain embodiments. In particular, <figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates predicted SpO<sub>2 </sub>signals for all participants using SVR when the palm is facing the camera (i.e., the palm-up scenario). As illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the prediction results of training and testing sessions are shown for each participant with reference SpO<sub>2 </sub>in dash lines, and predicted SpO<sub>2 </sub>in solid lines. The higher the correlation p and the lower the MAE, the better the predicted SpO<sub>2 </sub>captures the trend of the reference signal.</p><p id="p-0091" num="0070">In <figref idref="DRAWINGS">FIG. <b>4</b></figref>, both training and testing sessions are shown for each participant. The SpO<sub>2 </sub>curves in each session contain three dips that are a result from breath holding, except for participant #8 who had a shorter session due to limited tolerance of breath-holding. For each participant, the skin-tone information was provided in the subplot, and the accuracy indicators, MAE and &#x3c1;, for SpO<sub>2 </sub>prediction are shown. In all training sessions, MAE is below 2.4% and &#x3c1; is above 0.6. From this observation, we find that all the predicted SpO<sub>2 </sub>signals in the training sessions are closely following the reference signals. Furthermore, all testing MAE values are within 1.8%, suggesting that those trained models adapt well to the testing data. While there are a few cycles that the predicted signal does not fully follow the reference signal, such as the second dip for participant #4 and participant #11, the trends are consistent. From experimental summary of the training and testing SpO<sub>2 </sub>estimation performance of both LR and SVR based methods for both PU and PD cases, it suggests that there may exist a nonlinear relationship between the extracted features and the SpO<sub>2 </sub>values.</p><p id="p-0092" num="0071">To examine the impact of the side of a hand and the skin tone on the performance of SpO<sub>2 </sub>estimation, certain example embodiments may examine: (i) whether the side of hand makes a difference in lighter skin (types II and III) or darker skin (types IV and V) or mixed skins (all participants); and (ii) whether the different skin tones matter in PU or PD case.</p><p id="p-0093" num="0072">According to certain embodiments, to study the importance of the feature vector f containing pulsatile information from all RGB channels, the narrow ABP filter, and the passband of ABP filter centered at precise HR frequency tracked by AMTC, three controlled experiments were conducted by removing one factor at a time. The configurations of methods corresponding to the experiments are listed in Table 1 below.</p><p id="p-0094" num="0000"><tables id="TABLE-US-00001" num="00001"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="center"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE 1</entry></row></thead><tbody valign="top"><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>Configurations for the ablation study of the pipeline</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="42pt" align="center"/><colspec colname="2" colwidth="175pt" align="center"/><tbody valign="top"><row><entry/><entry>Configuration</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="4"><colspec colname="1" colwidth="42pt" align="center"/><colspec colname="2" colwidth="70pt" align="center"/><colspec colname="3" colwidth="35pt" align="center"/><colspec colname="4" colwidth="70pt" align="center"/><tbody valign="top"><row><entry/><entry>Multi-channel</entry><entry>Narrow</entry><entry>Accurate</entry></row><row><entry>Method Index</entry><entry>RoR features?</entry><entry>ABP filter?</entry><entry>HR tracking?</entry></row><row><entry namest="1" nameend="4" align="center" rowsep="1"/></row><row><entry>I</entry><entry>Two-channel RoR</entry><entry>&#x2713;</entry><entry>&#x2713;</entry></row><row><entry>II</entry><entry>&#x2713;</entry><entry>No ABP</entry><entry>n/a</entry></row><row><entry>III</entry><entry>&#x2713;</entry><entry>Wide ABP</entry><entry>&#x2713;</entry></row><row><entry>IV</entry><entry>&#x2713;</entry><entry>&#x2713;</entry><entry>Peak-finding</entry></row><row><entry>V</entry><entry>&#x2713;</entry><entry>&#x2713;</entry><entry>Weighted energy</entry></row><row><entry>Proposed</entry><entry>&#x2713;</entry><entry>&#x2713;</entry><entry>&#x2713;</entry></row><row><entry namest="1" nameend="4" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0095" num="0073"><figref idref="DRAWINGS">FIG. <b>6</b>(<i>a</i>)</figref> illustrates a bar plot of the SVR/PU case, according to certain embodiments, and <figref idref="DRAWINGS">FIG. <b>6</b>(<i>b</i>)</figref> illustrates another bar plot of the SVR/PU case, according to certain embodiments. As illustrated in <figref idref="DRAWINGS">FIGS. <b>6</b>(<i>a</i>) and <b>6</b>(<i>b</i>)</figref>, the height of each bin shows the average of correlation coefficient &#x3c1; or the MAE of SpO<sub>2 </sub>estimation results from testing sessions (SVR, PU case) of all participants. Each pair of error bars corresponds to the 95% confidence interval that is calculated as +1.96{circumflex over (&#x3c3;)}/&#x221a;{square root over (N)}, where {circumflex over (&#x3c3;)} is the sample standard deviation, and Nis the sample size/number of participants. Further, as illustrated in <figref idref="DRAWINGS">FIGS. <b>6</b>(<i>a</i>) and <b>6</b>(<i>b</i>)</figref>, it can be seen that the larger the correlation, the better; and the smaller the MAE, the better.</p><p id="p-0096" num="0074">Additionally, <figref idref="DRAWINGS">FIGS. <b>6</b>(<i>a</i>) and <b>6</b>(<i>b</i>)</figref> compares the method (method (I)) of RoR with narrow adaptive bandpass filter (nABP) (AMTC) corresponding to the feature extraction method described above. This method includes the nABP centered at AMTC-tracked HR. The only exception is that instead of using the feature vector f that contains multichannel information, only the RoR between the channels as in traditional RoR methods is used. <figref idref="DRAWINGS">FIGS. <b>6</b>(<i>a</i>) and <b>6</b>(<i>b</i>)</figref> also illustrate that the method of certain embodiments (i.e., &#x201c;proposed&#x201d;) described herein outperforms the method of RoR with nABP by a significant margin. More specifically, the method of certain embodiments improves the correlation coefficient from 0.22 to 0.68, and the MAE from 1.67% to 1.26%. This improvement confirms that the multi-channel feature of certain embodiments help with the more accurate SpO<sub>2 </sub>monitoring.</p><p id="p-0097" num="0075">The contribution of narrowband ABP filter for feature extraction was also analyzed. Here, two methods were compared to show the necessity of using a narrowband HR-guided bandpass filter. In method (II), feature vector without ABP uses a nonadaptive, generic bandpass filter with the passband over [1, 2] Hz, covering the normal range of HR in secondary mode to replace the HR-based narrow ABP filter. In method (III), the feature vector with wide ABP (AMTC) applied a wider ABP filter with &#xb1;0.5 Hz bandwidth than the &#xb1;0.1 Hz. This wider ABP filter's center frequency is provided by the AMTC tracking algorithm of the HR described above.</p><p id="p-0098" num="0076">In certain embodiments, the bandpass filters used for methods (II) and (III) have the same bandwidth, 1 Hz. In terms of center frequency, method (II) used a fixed setting at 1.5 Hz, while method (III) was adaptively centered at the estimated HR value. Compared to method (II), method (III) has an improved testing MAE by 18%. Furthermore, compared to method (III), the method of certain embodiments with a narrow ABP filter improves the correlation coefficient &#x3c1; for testing by 13% and MAE by 9%, suggesting the contribution of the narrow HR-based ABP filter strategy for AC computation.</p><p id="p-0099" num="0077">The importance of accurate HR tracing on SpO<sub>2 </sub>monitoring was considered. Specifically, two methods were considered to compare with the method of certain embodiments. In particular, method (IV) involves feature vector with narrow ABP (peak-finding). In method (IV), a narrow ABP filter of bandwidth &#xb1;0.1 Hz was applied for extracting the feature vector f. The center frequency of the ABP filter is the HR estimated from the peak-finding algorithm described above. Additionally, in method (V), feature vector with narrow ABP (weighted) may be similar to method (IV), except that the frequency estimation algorithm is replaced by the weighted energy described above.</p><p id="p-0100" num="0078">According to certain embodiments, the averaged MAE of the HR estimation for all participants by the peak-finding algorithm, weighted frequency estimation algorithm, and AMTC algorithm were 7.11 (&#xb1;3.66) bpm, 6.42 (&#xb1;3.02) bpm, and 4.14 (&#xb1;1.72) bpm, respectively.</p><p id="p-0101" num="0079"><figref idref="DRAWINGS">FIGS. <b>6</b>(<i>a</i>) and <b>6</b>(<i>b</i>)</figref> illustrate that methods (IV) and (V) perform similarly with 0.56 vs. 0.57 for correlation &#x3c1; and 1.43% vs. 1.40% for MAE, respectively. Our proposed method guided by the AMTC tracked HR outperforms methods (IV) and (V) by 21% and 19% in correlation, and by 12% and 10% in MAE, respectively. These results suggest that the accurate HR estimation for ABP filter design improves the quality of the AC magnitude by preserving the most cardiac-related signal from RGB channels, which in turn helps with the accurate SpO<sub>2 </sub>monitoring.</p><p id="p-0102" num="0080">In addition to contact-free SpO<sub>2 </sub>monitoring, the proposed algorithm of certain embodiments may be evaluated to determine whether it can be applied to a contact-based smartphone setup. To collect data, the left index finger covers the smartphone's illuminating flashlight and the nearby built-in camera, and the camera captures a pulse video at the fingertip. Another smartphone is used to simultaneously record a top view video of the back side of the right hand whose index finger is placed in the oximeter for SpO<sub>2 </sub>reference data collection. One participant took part in this extended experiment where one training session with three breath-holding cycles was recorded, and three testing sessions were recorded 30 minutes after the training session.</p><p id="p-0103" num="0081">In Table 2, the performance of the proposed algorithm in both the contact-based and contact-free SpO<sub>2 </sub>measurement settings were compared. The conventional RoR models used were implemented as baseline models for contact-based SpO<sub>2 </sub>measurement. Additionally, the mean and standard deviation of each window from the red and blue channels were calculated as the DC and AC components. A linear model was built to relate the ratio-of-ratios from the two color channels with SpO<sub>2</sub>. Further, the median of the pulsatile peak-to-valley amplitude was regarded as the AC component. For the two RoR methods, both LR and SVR were implemented. For contact-free SpO<sub>2 </sub>measurement, the traditional two-color channel RoR method was taken as the baseline to compare with the proposed method.</p><p id="p-0104" num="0000"><tables id="TABLE-US-00002" num="00002"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="center"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE 2</entry></row></thead><tbody valign="top"><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>Comparison of algorithm in both contact and </entry></row><row><entry>contact-free SpO<sub>2 </sub>estimation settings</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="4"><colspec colname="1" colwidth="35pt" align="left"/><colspec colname="2" colwidth="63pt" align="left"/><colspec colname="3" colwidth="63pt" align="center"/><colspec colname="4" colwidth="56pt" align="center"/><tbody valign="top"><row><entry/><entry/><entry>Training</entry><entry>Testing</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="6"><colspec colname="1" colwidth="35pt" align="left"/><colspec colname="2" colwidth="63pt" align="left"/><colspec colname="3" colwidth="35pt" align="left"/><colspec colname="4" colwidth="28pt" align="left"/><colspec colname="5" colwidth="35pt" align="left"/><colspec colname="6" colwidth="21pt" align="left"/><tbody valign="top"><row><entry/><entry/><entry>MAE</entry><entry>p</entry><entry>MAE</entry><entry>p</entry></row><row><entry namest="1" nameend="6" align="center" rowsep="1"/></row><row><entry>Contact</entry><entry>RoR |10| (LR)</entry><entry>l 60%</entry><entry>0.54</entry><entry>1.38% </entry><entry>0.64</entry></row><row><entry/><entry>RoR |10| (SVR)</entry><entry>1.14% </entry><entry>0.73</entry><entry>1.32% </entry><entry>0.60</entry></row><row><entry/><entry>RoR |11| (LR)</entry><entry>1.47% </entry><entry>0.62</entry><entry>1.39% </entry><entry>0.63</entry></row><row><entry/><entry>RoR |11| (SVR)</entry><entry>0.99% </entry><entry>0.83</entry><entry>1.27% </entry><entry>0.66</entry></row><row><entry/><entry>Proposed</entry><entry>0.91% </entry><entry>0.84</entry><entry>1.17%</entry><entry>0.81</entry></row><row><entry>Contact-</entry><entry>RoR (2-channel)</entry><entry>1.61%</entry><entry>0.73</entry><entry>1.75% </entry><entry>0.36</entry></row><row><entry>free</entry><entry>Proposed</entry><entry>1.36% </entry><entry>0.62</entry><entry>1.29% </entry><entry>0.68</entry></row><row><entry namest="1" nameend="6" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0105" num="0082">Additionally, Table 2 reveals that the proposed algorithm outperforms other conventional RoR models in the contact-based SpO<sub>2 </sub>monitoring. Even in the contact-free case, the proposed algorithm presents a comparable performance to that of the contact-based cases, despite that the SNR of the fingertip video is better than the SNR from a remote hand video.</p><p id="p-0106" num="0083">Further, it was found that the proposed algorithm demonstrated resilience against blurring. For instance, in the setup described above, the hands were placed on a stable table with a cellphone camera acquiring the skin color of both hands. Ideal laboratory conditions are often not satisfied under practical scenarios, and the hand images captured by the cellphone cameras may be blurred due to being out of focus. The point spread function is modeled as a 2D homogeneous Gaussian kernel. The finite support of the kernel is defined manually to generate perceptually different blurry effects and then the standard deviation &#x3c3; is computed based on the given support. To test different blurry effects, experiments were conducted with two different blurry levels &#x3c3;=1.1 (5&#xd7;5 pixels) and &#x3c3;=2.6 (15&#xd7;15 pixels), respectively. The blurring effects are demonstrated in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, which illustrates blurring effects using different blurry level &#x3c3; on hand videos according to certain embodiments. The wider the kernel was, the blurrier the videos were.</p><p id="p-0107" num="0084">Table 3 presents the SVR generated results for PU cases with different &#x3c3; and kernel sizes. The SVR, PU scenario was used to showcase here as it achieves the best SpO<sub>2 </sub>prediction performance, which is verified in the examples described above. From Table 5, it can be seen that the algorithm of certain embodiments is robust to the Gaussian blurring effect. After the &#x3c3;=1.1 blurring, the testing &#x3c1; remains the same, and testing MAE is 6.3% higher than the no blurring case. Additionally, after the &#x3c3;=2.6 blurring, the testing p is 1.5% lower and MAE is 4.0% higher than the no blurring case.</p><p id="p-0108" num="0000"><tables id="TABLE-US-00003" num="00003"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="center"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE 3</entry></row></thead><tbody valign="top"><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>SVR generated results</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="1" colwidth="63pt" align="left"/><colspec colname="2" colwidth="77pt" align="center"/><colspec colname="3" colwidth="77pt" align="center"/><tbody valign="top"><row><entry/><entry>Training</entry><entry>Testing</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="5"><colspec colname="1" colwidth="63pt" align="left"/><colspec colname="2" colwidth="42pt" align="left"/><colspec colname="3" colwidth="35pt" align="left"/><colspec colname="4" colwidth="42pt" align="left"/><colspec colname="5" colwidth="35pt" align="left"/><tbody valign="top"><row><entry/><entry>MAE</entry><entry>p</entry><entry>MAE</entry><entry>p</entry></row><row><entry namest="1" nameend="5" align="center" rowsep="1"/></row><row><entry>&#x3c3; = 2.6 blur</entry><entry>141%</entry><entry>0.72</entry><entry>1.31%</entry><entry>0.67</entry></row><row><entry>(15 &#xd7; 15 pixels)</entry><entry>(&#xb1;0.50%)</entry><entry>(&#xb1;0.11)</entry><entry>(&#xb1;0.35%)</entry><entry>(&#xb1;0.09)</entry></row><row><entry>&#x3c3; = 1.1 blur</entry><entry>1.42%</entry><entry>0.70</entry><entry>1.34%</entry><entry>0.68</entry></row><row><entry>(5 &#xd7; 5 pixels)</entry><entry>(&#xb1;0.59%)</entry><entry>(&#xb1;0.16)</entry><entry>(&#xb1;041%)</entry><entry>(&#xb1;0.10)</entry></row><row><entry>No blur</entry><entry>1.33%</entry><entry>0.76</entry><entry>1.26%</entry><entry>0.68</entry></row><row><entry/><entry>(&#xb1;0.54%)</entry><entry>(&#xb1;0.09)</entry><entry>(&#xb1;0.33%)</entry><entry>(&#xb1;0.10)</entry></row><row><entry namest="1" nameend="5" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0109" num="0085">From the recordings of the data collection protocol for voluntary breath-holding, it was observed that HR and SpO<sub>2 </sub>are correlated for many participants. That is, in one breath-holding cycle, when the participant starts to hold breath, his/her HR increases and SpO<sub>2 </sub>drops as the oxygen runs out. As he/she resumes normal breathing, his/her HR and SpO<sub>2 </sub>recovers to be within the normal range. Due to individuals' different physical conditions, in some participants, the peak of the HR signal and valley of the SpO<sub>2 </sub>signal happen in such a short time interval that HR and SpO<sub>2 </sub>are significantly negatively correlated. This observation is in line with the biological literature where it has been found that breath-holding exercises yield significant changes in the cardiovascular system. In the central circulation, they caused significant changes in heart rate, and in the peripheral circulation, they caused significant changes in arterial blood flow and oxygen saturation.</p><p id="p-0110" num="0086">According to other embodiments, convolutional neural networks (CNN) maybe utilized for contactless SpO<sub>2 </sub>monitoring from videos captured by cameras including, for example, mobile device cameras (e.g., smartphones). <figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates an example SpO<sub>2 </sub>estimation method, according to certain embodiments. First, the ROI, including the palm and back side of the hand, is extracted from the smartphone captured videos. Second, the ROI is spatially averaged to produce R, G, and B time series. Third, the three time series are fed into an optophysiology-inspired CNN to implicitly learn the features for SpO<sub>2 </sub>estimation. Certain embodiments described herein may use neural networks to address the problem of contactless SpO<sub>2 </sub>sensing using consumer-grade RGB cameras.</p><p id="p-0111" num="0087">According to certain embodiments may utilize deep learning aided camera-based physiological monitoring. Deep learning has demonstrated promising performance in camera-based physiological measurements, such as HR and breathing rate. An end-to-end convolutional attention network may estimate the blood volume pulse from face videos. Further, frequency analysis may be conducted on the estimated pulse signal for HR and breathing rate tracking. Thus, HR may be directly inferred using a convolutional network with spatial-temporal representation of the race videos as its input.</p><p id="p-0112" num="0088">For instance, certain embodiments may estimate SpO<sub>2 </sub>levels using a hand video by leveraging the fact that the color of the skin changes subtly when red cells in the blood carry/release oxygen. In certain embodiments, three-color time series may be extracted by spatial averaging from the skin area of the hand video. The extracted time series may then be fed to optophysiology-inspired neural networks designed to implicitly learn the features by color channel mixing and temporal trend analysis to achieve better and more explainable SpO<sub>2 </sub>predictions.</p><p id="p-0113" num="0089">In certain embodiments, the skin color signals may be split up into 10-second segments using a sliding window with a step size/stride of 0.2 seconds to serve as the inputs for neural networks. From an optophysiological perspective, the reflected/reemitted light from the skin for the duration of one cycle of heartbeat (i.e., 0.5-1 seconds for a heart rate of 60-120 bpm) should contain almost the complete information necessary to estimate the instantaneous SpO<sub>2</sub>. In certain embodiments, longer segments may be used to add resilience against sensing noise. Since the segment length is one order of magnitude longer than the minimally required length to contain the SpO2 information, a fully-connected or convolutional structure may be used to adequately capture the temporal dependencies without resorting to a recurrent neural network structure.</p><p id="p-0114" num="0090">Some neural network work for SpO<sub>2 </sub>prediction may explore prediction, but not the model explainability. Explainability/interpretability may be highly desirable in many applications yet often not sufficiently addressed, partly due to the black box nature of neural networks. From a healthcare standpoint, explainability is a key factor which should be taken into account at the beginning of the design of a system. To extract features from the skin color signals and estimate SpO<sub>2</sub>, certain embodiments provide three physiologically motivated neural network structures. These structures are inspired by domain knowledge-driven physiological sensing methods, and designed to be physically explainable. For heart rate sensing and respiratory rate sensing, the RGB skin color signals may be combined first, as in the plane-orthogonal-to-skin (POS) algorithm, followed by temporal feature extraction. In contrast, for SpO<sub>2 </sub>sensing methods such as the RoR, the color components are combined at the end. The neural network structures of certain embodiments explore different arrangements of channel combination and temporal feature extraction. As such, certain embodiments may systematically compare the performance of explainable model structures.</p><p id="p-0115" num="0091">In certain embodiments, channel mixing may be followed by feature extraction. <figref idref="DRAWINGS">FIG. <b>10</b></figref> illustrates an example network structure for predicting an SpO<sub>2</sub>, according to certain embodiments. In Model 1, shown as the leftmost structure depicted in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, the color channels are combined first using several channel combination layers, and then the resulting features are further processed by temporal convolutional and max pooling to extract the temporal information. A channel combination layer first linearly combines the Cn input channels/vectors into C<sub>out </sub>activation vectors, and then applies a rectified linear unit (ReLU) activation function to obtain the output channels/vectors. Mathematically, the channel combination layer is described as follows:</p><p id="p-0116" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>V</i>=&#x3c3;(<i>WU+b</i><img id="CUSTOM-CHARACTER-00007" he="3.22mm" wi="1.44mm" file="US20230000377A1-20230105-P00003.TIF" alt="custom-character" img-content="character" img-format="tif"/><sup>T</sup>),&#x2003;&#x2003;(8)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0117" num="0000">where U&#x2208;<img id="CUSTOM-CHARACTER-00008" he="3.22mm" wi="2.46mm" file="US20230000377A1-20230105-P00004.TIF" alt="custom-character" img-content="character" img-format="tif"/><sup>C</sup><sup><sub2>in</sub2></sup><sup>&#xd7;L </sup>is the input comprised of C<sub>in </sub>time series/vectors of length L. The initial channel combination layer has an input of three channels with 300 points along the time axis. W&#x2208;<img id="CUSTOM-CHARACTER-00009" he="3.22mm" wi="2.46mm" file="US20230000377A1-20230105-P00004.TIF" alt="custom-character" img-content="character" img-format="tif"/><sup>C</sup><sup><sub2>out</sub2></sup><sup>&#xd7;C</sup><sup><sub2>in </sub2></sup>is a weight matrix, where each of the C<sub>out </sub>rows of the matrix is a different linear combination for the input channels. A bias vector b&#x2208;<img id="CUSTOM-CHARACTER-00010" he="3.22mm" wi="2.46mm" file="US20230000377A1-20230105-P00004.TIF" alt="custom-character" img-content="character" img-format="tif"/><sup>C</sup><sup><sub2>out </sub2></sup>contains the bias terms for each of the C<sub>out </sub>output channels, which ensures that each data points in the artificially created segment of length L has the same intercept. <img id="CUSTOM-CHARACTER-00011" he="3.22mm" wi="1.44mm" file="US20230000377A1-20230105-P00003.TIF" alt="custom-character" img-content="character" img-format="tif"/><sup>T</sup>&#x2208;<img id="CUSTOM-CHARACTER-00012" he="3.22mm" wi="2.46mm" file="US20230000377A1-20230105-P00004.TIF" alt="custom-character" img-content="character" img-format="tif"/><sup>1&#xd7;L </sup>is a row vector of all ones. The nonlinear ReLU function &#x3c3;(x)=max(0, x) is applied elementwise to the activation map/matrix. The output of the channel combination layer V&#x2208;<img id="CUSTOM-CHARACTER-00013" he="3.22mm" wi="2.46mm" file="US20230000377A1-20230105-P00004.TIF" alt="custom-character" img-content="character" img-format="tif"/><sup>C</sup><sup><sub2>out</sub2></sup><sup>&#xd7;L </sup>contains C<sub>out </sub>channels of nonlinearly combined input channels.</p><p id="p-0118" num="0092">As further shown in Model 1, the channel mixing section concatenates multiple channel combination layers with decreasing channel counts to provide significant nonlinearity. The output of the last channel combination layer has seven channels. After the channel mixing, for temporal feature extraction, multiple convolutional and max pooling layers may be utilized with a downsampling factor of two to extract the temporal features of the channel-mixed signals. When there are multiple filters in the convolutional layer, then there may also be some additional channel combining with each filter outputting a channel-mixed signal. Finally, a single node may be used to represent the predicted SpO<sub>2 </sub>level.</p><p id="p-0119" num="0093">According to certain embodiments, feature extraction may be followed by channel mixing. In Model 2, the middle structure depicted in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, the order of channel mixing and temporal feature extraction is reversed from that in Model 1. The three color channels are separately fed for temporal feature extraction. Additionally, the convolutional layers learn different features unique to each channel. At the output of the temporal feature extraction section, each color channel has been downsampled to retain only the important temporal information. The color channels are then mixed together in the same way as described for Model 1 before outputting the SpO<sub>2 </sub>value.</p><p id="p-0120" num="0094">In certain embodiments, the feature extraction and channel mixing may be interleaved. As illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, in Model 3, the possibility of interleaving the color channel mixing and temporal feature extraction steps may be explored. As illustrated by the rightmost structure illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, the input is first put through a convolutional layer with many filters and then passed to max pooling layers, resulting in feature extraction along the time as well channel combinations through each filter. The number of filters is reduced with each successive convolutional layer, gradually decreasing the number of combined channels and downsampling the signal in the time domain.</p><p id="p-0121" num="0095">According to certain embodiments, the root-mean-squared-error (RMSE) may be used as the loss function for all models. During training, the model instance at the epoch may be saved with the lowest validation loss. The neural network inputs may be scaled to have zero mean and unit variance to improve the numerical stability of the learning. Additionally, the parameters and hyperparameters of each model structure were tuned using a HyperBand algorithm, which allows for faster and more efficient search over a large parameter space than grid search or random search. It does this by running random parameter configurations on a specific schedule of iterations per configuration, and uses earlier results to select candidates for longer runs. The parameters that were tuned include the learning rate, the number of filters and kernel size for convolutional layers, the number of nodes, the dropout probability, and whether to do batch normalization after each convolutional layer.</p><p id="p-0122" num="0096">Examples of SpO<sub>2 </sub>Estimation Synergy of Principled Mechanisms and Neural Networks</p><p id="p-0123" num="0097">The models of certain embodiments were evaluated on a self-collected dataset. The dataset consisted of hand video recordings and SpO<sub>2 </sub>data from 14 participants, of which there were six males and eight females between the ages of 21 and 30. Participants were asked to categorize their skin tone based on the Fitzpatrick skin types. The distribution of the participants' skin types is as follows: two participants of type II; eight participants of type III; one participant of type IV; and three participants of type V.</p><p id="p-0124" num="0098">The self-collected dataset may include hand video recordings and SpO<sub>2 </sub>data from fourteen participants, of which there were six males and eight females between the ages of 21 and 30. Participants were asked to categorize their skin tone based on the Fitzpatrick sin types. The Fitzpatrick skin types classify the skin by its reaction to exposure to sunlight and pigmentation. From type I to type VI, the skin color becomes darker and less prone to be burned by the sunlight. Among the fourteen participants, two are from type II, eight are from type III, one is from type IV, and three are from type V.</p><p id="p-0125" num="0099">Table 4 illustrates a comparison of correlations for lighter vs. darker skin types vs. all skin types in both PU and PD cases, according to certain embodiments. Specifically, Table 4 illustrates a supplement to <figref idref="DRAWINGS">FIG. <b>7</b></figref> with numerical values specified for the factor analysis, including the skin type and the side of the hand. This table presents a comparison of the test correlations from all the three proposed models in palm up (PU) and palm down (PD) data collection modes of lighter skin participants (types II and III), darker skin participants (types IV and V), and all participants. The following may be analyzed in view of Table 4: (i) Whether the different skin types matter in PU or PD case, and (ii) whether the side of the hand matters in lighter skin or darker skin.</p><p id="p-0126" num="0000"><tables id="TABLE-US-00004" num="00004"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="center"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE 4</entry></row></thead><tbody valign="top"><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>Comparison of correlations for lighter vs. darker skin types vs.</entry></row><row><entry>all skin types in both PU and PD cases.</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="4"><colspec colname="1" colwidth="49pt" align="center"/><colspec colname="2" colwidth="56pt" align="center"/><colspec colname="3" colwidth="56pt" align="center"/><colspec colname="4" colwidth="56pt" align="center"/><tbody valign="top"><row><entry/><entry>Lighter</entry><entry>Darker</entry><entry>Overall</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="7"><colspec colname="1" colwidth="49pt" align="center"/><colspec colname="2" colwidth="28pt" align="center"/><colspec colname="3" colwidth="28pt" align="center"/><colspec colname="4" colwidth="28pt" align="center"/><colspec colname="5" colwidth="28pt" align="center"/><colspec colname="6" colwidth="28pt" align="center"/><colspec colname="7" colwidth="28pt" align="center"/><tbody valign="top"><row><entry>Hand Mode</entry><entry>Median </entry><entry>IQR</entry><entry>Median </entry><entry>IQR</entry><entry>Median </entry><entry>IQR</entry></row><row><entry namest="1" nameend="7" align="center" rowsep="1"/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="49pt" align="center"/><colspec colname="2" colwidth="168pt" align="center"/><tbody valign="top"><row><entry/><entry>Participant-Specific</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="7"><colspec colname="1" colwidth="49pt" align="center"/><colspec colname="2" colwidth="28pt" align="center"/><colspec colname="3" colwidth="28pt" align="center"/><colspec colname="4" colwidth="28pt" align="center"/><colspec colname="5" colwidth="28pt" align="center"/><colspec colname="6" colwidth="28pt" align="center"/><colspec colname="7" colwidth="28pt" align="center"/><tbody valign="top"><row><entry>PD</entry><entry>0.44</entry><entry>0.50</entry><entry>0.48</entry><entry>0.20</entry><entry>0.45</entry><entry>0.41</entry></row><row><entry>PU</entry><entry>0.41</entry><entry>0.30</entry><entry>0.45</entry><entry>0.38</entry><entry>0.41</entry><entry>0.33</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="49pt" align="center"/><colspec colname="2" colwidth="168pt" align="center"/><tbody valign="top"><row><entry/><entry>Leave-One-Participant-Out</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="7"><colspec colname="1" colwidth="49pt" align="center"/><colspec colname="2" colwidth="28pt" align="center"/><colspec colname="3" colwidth="28pt" align="center"/><colspec colname="4" colwidth="28pt" align="center"/><colspec colname="5" colwidth="28pt" align="center"/><colspec colname="6" colwidth="28pt" align="center"/><colspec colname="7" colwidth="28pt" align="center"/><tbody valign="top"><row><entry>PD</entry><entry>0.14</entry><entry>0.41</entry><entry>0.43</entry><entry>0.19</entry><entry>0.24</entry><entry>0.39</entry></row><row><entry>PU</entry><entry>0.35</entry><entry>0.46</entry><entry>0.31</entry><entry>0.31</entry><entry>0.34</entry><entry>0.42</entry></row><row><entry namest="1" nameend="7" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0127" num="0100">According to certain embodiments, the top panel of Table 5 may be used to examine the participant-specific case. In the PD case, the darker skin group outperforms the light group since the former has a larger median of 0.48 and a smaller IQR of 0.20. In the PU case, the medians of the lighter skin group and darker skin group are 0.41 and 0.45, with IQR being 0.30 and 0.38, respectively. Even though the median from the darker group is 9.8% higher, the IQR is 26.7% worse. Thus, no significant performance difference was observed in the PU case. Additionally, the bottom panel of Table 4 to analyze the results from the leave-one-participant-out experiment. It was observed that in the PD case, the darker skin group with a median of 0.43 outperforms the lighter skin group with a median of 0.14, whereas in the PU case, the performances are comparable. This observation is consistent with the participant-specific experiments that when using the palm as the ROI, the skin color is not a factor to the accuracy of SpO<sub>2 </sub>estimation.</p><p id="p-0128" num="0101">Certain embodiments may focus on the participant-specific case in the top panel of Table 4. It was found that there is no significant difference between PU and PD cases in our current lighter skin and overall groups, whereas in the darker skin group, the PD case is better than the PU case. Then, focus was placed on the results under the leave-one-participant-out setup in the bottom panel of Table 4. For the darker skin group, the PD case outperforms the PU case, which is consistent with the results from the participant-specific experiments. In contrast, in both the lighter skin group and the mixed group, the PU cases significantly outperform the PD cases.</p><p id="p-0129" num="0102">The reference SpO<sub>2 </sub>signal was interpolated to 5 sample points per second to match the segment sampling rate using a smooth spline approximation. Each RGB segment and SpO<sub>2 </sub>value pair was fed into the models as a single data point, and the models output a single SpO<sub>2 </sub>estimate per segment. To evaluate a model on a recording, the model was sequentially fed all RGB segments from the recording to generate a time series of preliminarily predicted SpO<sub>2 </sub>values. All predictions greater than 100% SpO<sub>2 </sub>were clipped to 100% since they are physiologically impossible. Additionally, a 10-second long moving average filter was applied to generate a refined time series of predicted SpO<sub>2 </sub>values.</p><p id="p-0130" num="0103">To investigate how well the models could learn to estimate a specific individual's SpO<sub>2 </sub>from his/her own data, participant-specific experiments were conducted. That is, individualized models were learned for each participant. For instance, Two recordings per participant were captured with at least 15 minutes in between. One recording was used for training and validation of the model and the remaining recording was for testing. An example of the training and validation predictions curves are illustrated in <figref idref="DRAWINGS">FIG. <b>12</b>(<i>a</i>)</figref>. Each recording contains three breathing cycles, and for each training/validation recording, the first two breathing cycles were taken for training and the third cycle is used for validation. Splitting the recordings into cycles instead of randomly sampling the 10-sec overlapping RGB segments ensured that there are no overlapping segments of data between the training and validation set. Example test prediction curves and their correlation and MAE are illustrated in <figref idref="DRAWINGS">FIG. <b>12</b>(<i>b</i>)</figref>. It should be noted that if the correlation is low (e.g., a constant temporal estimate), then the MAE and RMSE metrics are less meaningful. For the participant-specific experiments, due to the small dataset size, the training and validation data were augmented by sampling with replacement. The oversampling also helps address the imbalance in SpO<sub>2 </sub>data values that is shown in <figref idref="DRAWINGS">FIG. <b>11</b>(<i>b</i>)</figref>.</p><p id="p-0131" num="0104">In each experiment, the model structure and hyper-parameters are first tuned using the training and validation data. Once the model has been tuned, multiple instances of the model were trained using the best-tuned hyper-parameters. Between each instance, the random seed used for model weights initialization and random oversampling were varied. Each model instance was evaluated on the training/validation recording, and the model instance that achieved the highest validation RMSE was selected for evaluation on the test recording. This model was then evaluated on the test recording to obtain the final test result.</p><p id="p-0132" num="0105">Performance comparison of the models of certain embodiments with an existing model reveals that Model 2 was the best in terms of correlation in both PD and PU cases, whereas Model 3 achieved the best in MAE and RMSE, showing that Model 2 and Model 3 are comparably the best in the individualized learning. All of the model configurations of certain embodiments described herein outperformed the existing model. It is worth noting that the international standard for clinically acceptable pulse oximeters tolerates an error of 4%, and the estimation errors were all within this range.</p><p id="p-0133" num="0106">There were two factors, including the skin type and the side of the hand, that may influence the performance of SpO<sub>2 </sub>estimation. Thus, the following were analyzed: (1) whether the different skin types matter in PU or PD case; and (2) whether the side of hand matters in lighter skin (types II+III) or darker skin (types IV+V). The box plots in <figref idref="DRAWINGS">FIGS. <b>13</b>(<i>a</i>) and <b>13</b>(<i>b</i>)</figref> illustrate the distributions of the test correlations from all three models in PU and PD modes of lighter skin and darker skin participants (<figref idref="DRAWINGS">FIG. <b>13</b>(<i>a</i>)</figref>), and all participants (<figref idref="DRAWINGS">FIG. <b>13</b>(<i>b</i>)</figref>).</p><p id="p-0134" num="0107">Certain embodiments focused on the left panel of <figref idref="DRAWINGS">FIG. <b>14</b>(<i>a</i>)</figref>. Overall, the medians of darker skin group were larger than those of the lighter skin group. Zooming into the PD case, it can be confirmed that the darker skin group indeed outperformed the light group since the former has a smaller interquartile range (IQR). However, for the PU case, there was no significant performance difference observed, because while the dark skin group was better in a larger median, the light skin group was better in a narrower IQR. As to the left panel of <figref idref="DRAWINGS">FIG. <b>13</b>(<i>b</i>)</figref>, no significant performance difference was observed between PD and PU given one had a better median and the other has a better IQR, when participants of all skin colors were considered together. However, looking at the subset of darker skin group as shown in the left panel of <figref idref="DRAWINGS">FIG. <b>11</b>(<i>a</i>)</figref>, it can be observed that PD was better than PU given its higher median and narrower IQR. Thus, in the participant-specific experiments, the darker skin group outperformed the lighter skin group when using the back side of the hand as the ROI for SpO<sub>2 </sub>prediction but they were comparable when using the palm of the hand. Additionally, the side of the hand had an impact on SpO<sub>2 </sub>prediction for the darker skin group but not for the lighter skin group.</p><p id="p-0135" num="0108">Table 5 shows ablation results of Model 1 in the leave-one-participant-out setup. This table presents the medians and IQRs specified for numerical comparison. The ablation studies justify the use of (i) nonlinear channel combinations and (ii) convolutional layers for temporal feature extraction. In ablation study 1, the nonlinear channel combination may be replaced with a single linear channel combination layer with no activation function as the first variant of Model 1. In ablation study 2, we replace the convolutional layers for temporal feature extraction with fully-connected dense layers as the second variant of Model 1.</p><p id="p-0136" num="0000"><tables id="TABLE-US-00005" num="00005"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="center"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE 5</entry></row></thead><tbody valign="top"><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>Ablation studies for Model 1 in the leave-one-participant-out</entry></row><row><entry>mode</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="5"><colspec colname="1" colwidth="105pt" align="left"/><colspec colname="2" colwidth="28pt" align="center"/><colspec colname="3" colwidth="21pt" align="center"/><colspec colname="4" colwidth="35pt" align="center"/><colspec colname="5" colwidth="28pt" align="center"/><tbody valign="top"><row><entry>Method</entry><entry/><entry>p</entry><entry>MAE (%)</entry><entry>RMSE (%)</entry></row><row><entry namest="1" nameend="5" align="center" rowsep="1"/></row><row><entry>Linear Ch. Comb. +</entry><entry>Median</entry><entry>0.46</entry><entry>2.14</entry><entry>2.66</entry></row><row><entry>Conv. layer for Feat Extra.</entry><entry>IQR</entry><entry>0.38</entry><entry>0.73</entry><entry>0.93</entry></row><row><entry>Nonlinear Ch. Comb. +</entry><entry>Median</entry><entry>0.41</entry><entry>2.29</entry><entry>2.66</entry></row><row><entry>Fully Connec, layer for Feat. Extra.</entry><entry>IQR</entry><entry>0.39</entry><entry>0.63</entry><entry>0.70</entry></row><row><entry>Model 1 (Proposed): Nonlinear Ch. </entry><entry>Median</entry><entry>0.46</entry><entry>197</entry><entry>2.32</entry></row><row><entry>Comb. + Conv. layer for Feat. Extra.</entry><entry>IQR</entry><entry>0.36</entry><entry>0.80</entry><entry>0.87</entry></row><row><entry namest="1" nameend="5" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0137" num="0109">First, the first and the third rows in Table 5 were compared for ablation study 1. The Model 1 achieves a better correlation and a better RMSE than its linear channel combination variant, suggesting the necessity of using the nonlinear channel combination method. Second, in ablation study 2, the second and the third rows in Table 8 were compared. As a result of the comparison, Model 1 outperformed its second variant with fully-connected layers for feature extraction with better medians in all metrics. This suggests that convolutional layers are better than fully connected layers for temporal feature extraction.</p><p id="p-0138" num="0110">In certain embodiments, RGB combination weights may be visualized. For instance, a separate investigation was conducted to visualize the learned weights for the RGB channels. In doing so, it was possible to combine the RGB channels for SpO<sub>2 </sub>prediction. Having an explainable model may be important for a physiological prediction task like this. The neural network models can be considered as nonlinear approximations of the hypothetically true function that can extract the physiological features related to SpO<sub>2 </sub>buried in the RGB videos. The ratio-of-ratios method, for example, is another such extractor that combines the information from the different color channels at the end of the pipeline. For this experiment, the modified version of Model 1 was used from the ablation studies that has a single linear channel combination at the beginning. Seeing that using a single linear channel combination did not significantly reduce model performance in the ablation studies, and understanding that the linear component may dominate the Taylor expansion of a nonlinear function, only linear combinations were used for this model to facilitate more interpretable visualizations.</p><p id="p-0139" num="0111"><figref idref="DRAWINGS">FIG. <b>14</b></figref> illustrates learned RGB channel weights, according to certain embodiments. 100 different instances of the model were trained on the first two cycles from all the recordings and tested on the third cycle from all recordings. The difference between each instance is that the weights are randomly initialized. The weights for each channel learned by the model instances were visualized as points representing the heads of the linear combination vector in RGB space. Each point is colored according to the average test correlation achieved by the model instance. <figref idref="DRAWINGS">FIG. <b>14</b></figref> illustrates the projections of these points onto the RB and RG planes. The subfigures reveal that the majority of the channel weights lay along certain line in the RGB space. For the weights on the line the ratio of the blue channel weight to the red channel weight is 0.87, the ratio of the green channel weight to red channel weight is 0.18. It is clear that the red and blue channels are the dominating factors for SpO<sub>2 </sub>prediction.</p><p id="p-0140" num="0112">To verify this result, this experiment was repeated. However, instead of using the data from all participants, for each model instance, seven participants were randomly selected, and their data was used for training and testing. In this case, the difference between each model instance is not only the initialized weights but also the random subset of participants that the model was trained on. <figref idref="DRAWINGS">FIG. <b>14</b></figref>, plot (d) reveals that most of the better performing instances have little contribution from the green channel. In plot (c), it can again be seen that most of the points lay on a line in the RB plane.</p><p id="p-0141" num="0113">These results are in accordance with the physical understanding of how light is absorbed by hemoglobin in the blood. <figref idref="DRAWINGS">FIG. <b>9</b></figref> revealed a large difference between the extinction coefficients, or the amount of light absorbed, by deoxygenated and oxygenated hemoglobin at the red wavelength. There is a significantly smaller difference at the blue wavelength and almost no difference at green. The amount of light absorbed influences the amount of light reflected which can be measured through the camera. A larger difference in extinction coefficients makes it easier to measure the ratio of light absorbed by oxygenated vs. deoxygenated hemoglobin over time. This ratio indicates the level of blood oxygen saturation. Therefore, from a physiological perspective, it makes sense for the neural networks to give larger weight to the red and then blue channels and give little to the green channel. These visualizations indicate that the models are learning physically meaningful features.</p><p id="p-0142" num="0114"><figref idref="DRAWINGS">FIG. <b>15</b></figref> illustrates an example flow diagram of a method, according to certain example embodiments. In certain example embodiments, the flow diagram of <figref idref="DRAWINGS">FIG. <b>15</b></figref> may be performed by a computing device/hardware, computer chip, or a system that includes one or more of a computer apparatus, computer system, network, neural network, apparatus, communication device, mobile computer, mobile communication device, or other similar device(s). According to certain embodiments, each of these apparatuses of the system may be represented by, for example, an apparatus similar to apparatus <b>10</b> illustrated in <figref idref="DRAWINGS">FIG. <b>16</b></figref>.</p><p id="p-0143" num="0115">According to one example embodiment, the method of <figref idref="DRAWINGS">FIG. <b>15</b></figref> may include, at <b>1500</b>, receiving an image or video of a part of a subject captured by a camera of a computing device. The method may also include, at <b>1505</b>, extracting a region of interest of the part of the subject from the image or video. The method may further include, at <b>1510</b>, performing feature extraction of the region of interest. In addition, the method may include, at <b>1515</b>, estimating a blood oxygen saturation level of the subject based on a spatial and temporal data analysis of more than two color channels. According to certain embodiments, feature extraction and estimation of the blood oxygen saturation level may include implementing a combination of spatial averaging, color channel mixing, and temporal trend analysis.</p><p id="p-0144" num="0116">According to certain embodiments, estimation of the blood oxygen saturation level may include implementing a multi-channel ratio-of-ratios feature vector with a narrow adaptive bandpass filter. According to other embodiments, estimation of the blood oxygen saturation level comprises implementing a neural network. According to some embodiments, the neural network may be a convolutional neural network, and estimating the blood oxygen saturation level may include feeding red, green, and blue time series data of spatial averaging into a convolutional neural network. According to further embodiments, the neural network may include a structure with at least one of feature extraction from skin color signals of the subject, and channel mixing of red, green, and blue color channels of the camera.</p><p id="p-0145" num="0117">In certain embodiments, the method may also include performing spatial averaging of the region of interest, extracting a remote photoplethysmogram signal from the spatial averaging, and determining a heart rate of the subject from the photoplethysmogram signal. For instance, in some example embodiments, the heart rate may be determined by dynamic programming and adaptive trace compensation. In some embodiments, the method may further include calculating, based on the heart rate, DC and AC components of red, green, and blue color channels of the camera based on the spatial averaging, and generating a regression model from pairwise ratios of the spatial averaging. In other embodiments, the method may also include estimating a weight vector from the regression model. In some embodiments, the blood oxygen saturation level may be estimated based on the estimated weight vector.</p><p id="p-0146" num="0118"><figref idref="DRAWINGS">FIG. <b>16</b></figref> illustrates an apparatus <b>10</b> according to an example embodiment. In certain embodiments, although only one apparatus <b>10</b> is illustrated, apparatus <b>10</b> may be an apparatus representing multiple apparatuses as part of a system or network. For example, in certain embodiments, apparatus <b>10</b> may be an apparatus, or chip, a communication device, a mobile computer or communication device, or computer apparatus that operates individually or together in a computer system or computer network system with other computer apparatuses.</p><p id="p-0147" num="0119">In some embodiments, the functionality of any of the methods, processes, algorithms, or flow charts described herein may be implemented by software and/or computer program code or portions of code stored in memory or other computer-readable or tangible media and executed by a processor.</p><p id="p-0148" num="0120">For example, in some embodiments, apparatus <b>10</b> may include one or more processors, one or more computer-readable storage mediums (for example, memory, storage, or the like), one or more radio access components (for example, a modem, a transceiver, or the like), and/or a user interface. It should be noted that one skilled in the art would understand that apparatus <b>10</b> may include components or features not shown in <figref idref="DRAWINGS">FIG. <b>16</b></figref>.</p><p id="p-0149" num="0121">As illustrated in the example of <figref idref="DRAWINGS">FIG. <b>16</b></figref>, apparatus <b>10</b> may include or be coupled to a processor <b>12</b> for processing information and executing instructions or operations. Processor <b>12</b> may be any type of general or specific purpose processor. In fact, processor <b>12</b> may include one or more of general-purpose computers, special purpose computers, microprocessors, digital signal processors (DSPs), field-programmable gate arrays (FPGAs), application-specific integrated circuits (ASICs), and processors based on a multi-core processor architecture, as examples. While a single processor <b>12</b> is shown in <figref idref="DRAWINGS">FIG. <b>16</b></figref>, multiple processors may be utilized according to other embodiments. For example, it should be understood that, in certain example embodiments, apparatus <b>10</b> may include two or more processors that may form a multiprocessor system (e.g., in this case processor <b>12</b> may represent a multiprocessor) that may support multiprocessing. According to certain example embodiments, the multiprocessor system may be tightly coupled or loosely coupled (e.g., to form a computer cluster).</p><p id="p-0150" num="0122">Processor <b>12</b> may perform functions associated with the operation of apparatus <b>10</b> including, as some examples, precoding of antenna gain/phase parameters, encoding and decoding of individual bits forming a communication message, formatting of information, and overall control of the apparatus <b>10</b>, including processes illustrated in <figref idref="DRAWINGS">FIGS. <b>1</b>-<b>15</b></figref>.</p><p id="p-0151" num="0123">Apparatus <b>10</b> may further include or be coupled to a memory <b>14</b> (internal or external), which may be coupled to processor <b>12</b>, for storing information and instructions that may be executed by processor <b>12</b>. Memory <b>14</b> may be one or more memories and of any type suitable to the local application environment, and may be implemented using any suitable volatile or nonvolatile data storage technology such as a semiconductor-based memory device, a magnetic memory device and system, an optical memory device and system, fixed memory, and/or removable memory. For example, memory <b>14</b> can be comprised of any combination of random access memory (RAM), read-only memory (ROM), static storage such as a magnetic or optical disk, hard disk drive (HDD), or any other type of non-transitory machine or computer-readable media. The instructions stored in memory <b>14</b> may include program instructions or computer program code that, when executed by processor <b>12</b>, enable the apparatus <b>10</b> to perform any of the various tasks described herein.</p><p id="p-0152" num="0124">In certain embodiments, apparatus <b>10</b> may further include or be coupled to (internal or external) a drive or port that is configured to accept and read an external computer-readable storage medium, such as an optical disc, USB drive, flash drive, or any other storage medium. For example, the external computer-readable storage medium may store a computer program or software for execution by processor <b>12</b> and/or apparatus <b>10</b> to perform any of the methods illustrated in <figref idref="DRAWINGS">FIGS. <b>1</b>-<b>15</b></figref>.</p><p id="p-0153" num="0125">Additionally or alternatively, in some embodiments, apparatus <b>10</b> may include an input and/or output device (I/O device). In certain embodiments, apparatus <b>10</b> may further include a user interface, such as a graphical user interface or touchscreen.</p><p id="p-0154" num="0126">In certain embodiments, memory <b>14</b> stores software modules that provide functionality when executed by processor <b>12</b>. The modules may include, for example, an operating system that provides operating system functionality for apparatus <b>10</b>. The memory may also store one or more functional modules, such as an application or program, to provide additional functionality for apparatus <b>10</b>. The components of apparatus <b>10</b> may be implemented in hardware, or as any suitable combination of hardware and software. According to certain example embodiments, processor <b>12</b> and memory <b>14</b> may be included in or may form a part of processing circuitry or control circuitry.</p><p id="p-0155" num="0127">As used herein, the term &#x201c;circuitry&#x201d; may refer to hardware-only circuitry implementations (e.g., analog and/or digital circuitry), combinations of hardware circuits and software, combinations of analog and/or digital hardware circuits with software/firmware, any portions of hardware processor(s) with software (including digital signal processors) that work together to cause an apparatus (e.g., apparatus <b>10</b>) to perform various functions, and/or hardware circuit(s) and/or processor(s), or portions thereof, that use software for operation but where the software may not be present when it is not needed for operation. As a further example, as used herein, the term &#x201c;circuitry&#x201d; may also cover an implementation of merely a hardware circuit or processor (or multiple processors), or portion of a hardware circuit or processor, and its accompanying software and/or firmware.</p><p id="p-0156" num="0128">According to certain embodiments, apparatus <b>10</b> may be controlled by memory <b>14</b> and processor <b>12</b> to receive an image or video of a part of a subject captured by a camera of a computing device. Apparatus <b>10</b> may also be controlled by memory <b>14</b> and processor <b>12</b> to extract a region of interest of the part of the subject from the image or video. Apparatus <b>10</b> may further be controlled by memory <b>14</b> and processor <b>12</b> to perform feature extraction of the region of interest. In addition, apparatus <b>10</b> may be controlled by memory <b>14</b> and processor <b>12</b> to estimate a blood oxygen saturation level of the subject based on a spatial and temporal data analysis of more than two color channels. According to certain embodiments, feature extraction and estimation of the blood oxygen saturation level may include implementing a combination of spatial averaging, color channel mixing, and temporal trend analysis.</p><p id="p-0157" num="0129">In some example embodiments, an apparatus (e.g., apparatus <b>10</b> and/or apparatus <b>20</b>) may include means for performing a method, a process, or any of the variants discussed herein. Examples of the means may include one or more processors, memory, controllers, transmitters, receivers, and/or computer program code for causing the performance of the operations.</p><p id="p-0158" num="0130">Certain example embodiments may be directed to an apparatus that includes means for receiving an image or video of a part of a subject captured by a camera of a computing device. The apparatus may also include means for extracting a region of interest of the part of the subject from the image or video. The apparatus may further include means for performing feature extraction of the region of interest. In addition, the apparatus may include means for estimating a blood oxygen saturation level of the subject based on a spatial and temporal data analysis of more than two color channels. According to certain embodiments, feature extraction and estimation of the blood oxygen saturation level may include implementing a combination of spatial averaging, color channel mixing, and temporal trend analysis.</p><p id="p-0159" num="0131">Certain embodiments described herein provide several technical improvements, enhancements, and/or advantages. In some embodiments, it may be possible to improve the correlation coefficient from 0.22 to 0.68, and the MAE from 1.67% to 1.26%. This improvement confirms that the multi-channel feature of certain embodiments helps with more accurate SpO<sub>2 </sub>monitoring. According to other embodiments, it may be possible to improve testing MAE by 18%, and improve the correlation coefficient for testing by 13%, and MAE by 9%, indicating the contribution of the narrow HR-based ABP filter. In further embodiments, it may be possible to provide accurate HR estimation for ABP filter design, and improve the quality of the AC magnitude by preserving the most cardiac-related signal from RGB channels, which in turn helps with the accurate SpO<sub>2 </sub>monitoring.</p><p id="p-0160" num="0132">A computer program product may include one or more computer-executable components which, when the program is run, are configured to carry out some example embodiments. The one or more computer-executable components may be at least one software code or portions of it. Modifications and configurations required for implementing operations of certain example embodiments may be performed as routine(s), which may be implemented as added or updated software routine(s). Software routine(s) may be downloaded into the apparatus.</p><p id="p-0161" num="0133">As an example, software or a computer program code or portions of it may be in a source code form, object code form, or in some intermediate form, and it may be stored in some sort of carrier, distribution medium, or computer-readable medium, which may be any entity or device capable of carrying the program. Such carriers may include a record medium, computer memory, read-only memory, photoelectrical and/or electrical carrier signal, telecommunications signal, and software distribution package, for example. Depending on the processing power needed, the computer program may be executed in a single electronic digital computer or it may be distributed amongst a number of computers. The computer-readable medium or computer-readable storage medium may be a non-transitory medium.</p><p id="p-0162" num="0134">In other example embodiments, the functionality may be performed by hardware or circuitry included in an apparatus (e.g., apparatus), for example through the use of an application-specific integrated circuit (ASIC), a programmable gate array (PGA), a field-programmable gate array (FPGA), or any other combination of hardware and software. In yet another example embodiment, the functionality may be implemented as a signal, a non-tangible means that can be carried by an electromagnetic signal downloaded from the Internet or other network.</p><p id="p-0163" num="0135">According to an example embodiment, an apparatus, such as a device, or a corresponding component, may be configured as circuitry, a computer or a microprocessor, such as a single-chip computer element, or as a chipset, including at least a memory for providing storage capacity used for arithmetic operation and an operation processor for executing the arithmetic operation.</p><p id="p-0164" num="0136">One having skill in the art will readily understand that the description as discussed above may be practiced with procedures in a different order, and/or with hardware elements in configurations, which are different than those which are disclosed. Therefore, although the present disclosure presents and describes certain example embodiments, it would be apparent to those of skill in the art that certain modifications, variations, and alternative constructions would be apparent while remaining within the spirit and scope of example embodiments.</p><heading id="h-0008" level="1">Partial Glossary</heading><p id="p-0165" num="0137">CNN Convolutional Neural Network</p><p id="p-0166" num="0138">HR Heart Rate</p><p id="p-0167" num="0139">CMOS Complementary Metal-Oxide-Semiconductor</p><p id="p-0168" num="0140">PD Palm Down</p><p id="p-0169" num="0141">PU Palm Up</p><p id="p-0170" num="0142">SpO<sub>2 </sub>Blood Oxygen Saturation</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001 MATH-US-00001-2" nb-file="US20230000377A1-20230105-M00001.NB"><img id="EMI-M00001" he="7.37mm" wi="76.20mm" file="US20230000377A1-20230105-M00001.TIF" alt="text missing or illegible when filed" img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002 MATH-US-00002-2" nb-file="US20230000377A1-20230105-M00002.NB"><img id="EMI-M00002" he="11.60mm" wi="76.20mm" file="US20230000377A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00003" nb-file="US20230000377A1-20230105-M00003.NB"><img id="EMI-M00003" he="6.69mm" wi="76.20mm" file="US20230000377A1-20230105-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00004 MATH-US-00004-2" nb-file="US20230000377A1-20230105-M00004.NB"><img id="EMI-M00004" he="11.26mm" wi="76.20mm" file="US20230000377A1-20230105-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00005" nb-file="US20230000377A1-20230105-M00005.NB"><img id="EMI-M00005" he="6.01mm" wi="76.20mm" file="US20230000377A1-20230105-M00005.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00006" nb-file="US20230000377A1-20230105-M00006.NB"><img id="EMI-M00006" he="6.01mm" wi="76.20mm" file="US20230000377A1-20230105-M00006.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00007" nb-file="US20230000377A1-20230105-M00007.NB"><img id="EMI-M00007" he="4.23mm" wi="76.20mm" file="US20230000377A1-20230105-M00007.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00008" nb-file="US20230000377A1-20230105-M00008.NB"><img id="EMI-M00008" he="6.01mm" wi="76.20mm" file="US20230000377A1-20230105-M00008.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00009 MATH-US-00009-2" nb-file="US20230000377A1-20230105-M00009.NB"><img id="EMI-M00009" he="17.27mm" wi="76.20mm" file="US20230000377A1-20230105-M00009.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-claim-statement>We claim:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method, comprising:<claim-text>receiving an image or video of a part of a subject captured by a camera of a computing device;</claim-text><claim-text>extracting a region of interest of the part of the subject from the image or video;</claim-text><claim-text>performing feature extraction of the region of interest; and</claim-text><claim-text>estimating a blood oxygen saturation level of the subject based on a spatial and temporal data analysis of more than two color channels,<claim-text>wherein feature extraction and estimation of the blood oxygen saturation level comprises implementing a combination of spatial averaging, color channel mixing, and temporal trend analysis.</claim-text></claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein estimation of the blood oxygen saturation level comprises implementing a multi-channel ratio-of-ratios feature vector with a narrow adaptive bandpass filter.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein estimation of the blood oxygen saturation level comprises implementing a neural network.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method according to <claim-ref idref="CLM-00003">claim 3</claim-ref>,<claim-text>wherein the neural network is a convolutional neural network, and</claim-text><claim-text>wherein estimating the blood oxygen saturation level comprises feeding red, green, and blue time series data of the spatial averaging into a convolutional neural network.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method according to <claim-ref idref="CLM-00003">claim 3</claim-ref>,<claim-text>wherein the neural network comprises a structure with at least one of feature extraction from skin color signals of the subject, and channel mixing of red, green, and blue color channels of the camera.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>extracting a remote photoplethysmogram signal from the spatial averaging; and</claim-text><claim-text>determining a heart rate of the subject from the photoplethysmogram signal.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, further comprising:<claim-text>calculating, based on the heart rate, DC and AC components of red, green, and blue color channels of the camera based on the spatial averaging; and</claim-text><claim-text>generating a regression model from pairwise ratios of the spatial averaging.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, further comprising:<claim-text>estimating a weight vector from the regression model,</claim-text><claim-text>wherein the blood oxygen saturation level is estimated based on the estimated weight vector.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. An apparatus, comprising:<claim-text>at least one processor; and</claim-text><claim-text>at least one memory comprising computer program code,</claim-text><claim-text>the at least one memory and the computer program code are configured, with the at least one processor, to cause the apparatus at least to</claim-text><claim-text>receive an image or video of a part of a subject captured by a camera of a computing device;</claim-text><claim-text>extract a region of interest of the part of the subject from the image or video;</claim-text><claim-text>perform feature extraction of the region of interest; and</claim-text><claim-text>estimate a blood oxygen saturation level of the subject based on a spatial and temporal data analysis of more than two color channels,</claim-text><claim-text>wherein feature extraction and estimation of the blood oxygen saturation level comprises implementing a combination of spatial averaging, color channel mixing, and temporal trend analysis.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The apparatus according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein estimation of the blood oxygen saturation level comprises implementing a multi-channel ratio-of-ratios feature vector with a narrow adaptive bandpass filter.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The Apparatus according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein estimation of the blood oxygen saturation level comprises implementing a neural network.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The apparatus according to <claim-ref idref="CLM-00011">claim 11</claim-ref>,<claim-text>wherein the neural network is a convolutional neural network, and</claim-text><claim-text>wherein estimating the blood oxygen saturation level comprises feeding red, green, and blue time series data of the spatial averaging into a convolutional neural network.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The apparatus according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the neural network comprises a structure with at least one of feature extraction from skin color signals of the subject, and channel mixing of red, green, and blue color channels of the camera.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The apparatus according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the at least one memory and the computer program code are further configured, with the at least one processor, to cause the apparatus at least to:<claim-text>extract a remote photoplethysmogram signal from the spatial averaging; and</claim-text><claim-text>determine a heart rate of the subject from the photoplethysmogram signal.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The apparatus according to <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the at least one memory and the computer program code are further configured, with the at least one processor, to cause the apparatus at least to:<claim-text>calculate, based on the heart rate, DC and AC components of red, green, and blue color channels of the camera based on the spatial averaging; and</claim-text><claim-text>generate a regression model from pairwise ratios of the spatial averaging.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The apparatus according to <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the at least one memory and the computer program code are further configured, with the at least one processor, to cause the apparatus at least to:<claim-text>estimate a weight vector from the regression model,</claim-text><claim-text>wherein the blood oxygen saturation level is estimated based on the estimated weight vector.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. A computer program, embodied on a non-transitory computer readable medium, the computer program comprising computer executable code, which, when executed by a processor, causes the processor to:<claim-text>receive an image or video of a part of a subject captured by a camera of a computing device;</claim-text><claim-text>extract a region of interest of the part of the subject from the image or video;</claim-text><claim-text>perform feature extraction of the region of interest; and</claim-text><claim-text>estimate a blood oxygen saturation level of the subject based on a spatial and temporal data analysis of more than two color channels,</claim-text><claim-text>wherein feature extraction and estimation of the blood oxygen saturation level comprises implementing a combination of spatial averaging, color channel mixing, and temporal trend analysis.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The computer program according to <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein estimation of the blood oxygen saturation level comprises implementing a multi-channel ratio-of-ratios feature vector with a narrow adaptive bandpass filter.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The computer program according to <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein estimation of the blood oxygen saturation level comprises implementing a neural network.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The computer program according to <claim-ref idref="CLM-00019">claim 19</claim-ref>,<claim-text>wherein the neural network is a convolutional neural network, and</claim-text><claim-text>wherein estimating the blood oxygen saturation level comprises feeding red, green, and blue time series data of the spatial averaging into a convolutional neural network.</claim-text></claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. The computer program according to <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein the neural network comprises a structure with at least one of feature extraction from skin color signals of the subject, and channel mixing of red, green, and blue color channels of the camera.</claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. The computer program according to <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the processor is further caused to:<claim-text>extract a remote photoplethysmogram signal from the spatial averaging; and</claim-text><claim-text>determine a heart rate of the subject from the photoplethysmogram signal.</claim-text></claim-text></claim><claim id="CLM-00023" num="00023"><claim-text><b>23</b>. The computer program according to <claim-ref idref="CLM-00022">claim 22</claim-ref>, wherein the processor is further caused to:<claim-text>calculate, based on the heart rate, DC and AC components of red, green, and blue color channels of the camera based on the spatial averaging; and</claim-text><claim-text>generate a regression model from pairwise ratios of the spatial averaging.</claim-text></claim-text></claim><claim id="CLM-00024" num="00024"><claim-text><b>24</b>. The computer program according to <claim-ref idref="CLM-00023">claim 23</claim-ref>, wherein the processor is further caused to:<claim-text>estimate a weight vector from the regression model,</claim-text><claim-text>wherein the blood oxygen saturation level is estimated based on the estimated weight vector.</claim-text></claim-text></claim></claims></us-patent-application>