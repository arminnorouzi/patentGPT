<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004326A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004326</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17684496</doc-number><date>20220302</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>JP</country><doc-number>2021-110549</doc-number><date>20210702</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>06</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>12</main-group><subgroup>0891</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>12</main-group><subgroup>084</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>0659</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>0619</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>0683</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>12</main-group><subgroup>0891</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>12</main-group><subgroup>084</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">STORAGE SYSTEM CONTROL METHOD AND STORAGE SYSTEM</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Hitachi, Ltd.</orgname><address><city>Tokyo</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Imaki</last-name><first-name>Tsuneyuki</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>Hitachi, Ltd.</orgname><role>03</role><address><city>Tokyo</city><country>JP</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">The storage system control method is implemented by a controller of a storage system. The method includes a step of storing data on a storage in a shared memory as cache data, a step of changing the cache data based on a writing request from outside, and a writeback step of writing back dirty cache data as the cache data which have been changed based on the writing request to the storage. The method further includes a step of storing the dirty cache data in a writeback processing memory prior to the writeback step. The writeback processing memory requires time for executing the writeback data process shorter than time required by the shared memory.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="139.28mm" wi="157.48mm" file="US20230004326A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="159.77mm" wi="159.51mm" file="US20230004326A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="203.62mm" wi="163.83mm" file="US20230004326A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="208.96mm" wi="159.51mm" file="US20230004326A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="201.68mm" wi="169.50mm" file="US20230004326A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="254.42mm" wi="168.32mm" file="US20230004326A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="197.44mm" wi="164.00mm" file="US20230004326A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="202.01mm" wi="169.33mm" file="US20230004326A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="242.91mm" wi="168.74mm" file="US20230004326A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="206.59mm" wi="131.91mm" file="US20230004326A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="212.34mm" wi="158.07mm" file="US20230004326A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="251.80mm" wi="165.78mm" file="US20230004326A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="253.92mm" wi="169.59mm" file="US20230004326A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">BACKGROUND</heading><p id="p-0002" num="0001">The present invention relates to a storage system control method and a storage system.</p><p id="p-0003" num="0002">The technique for access to the volume using the cache memory has been disclosed in WO 2015/087424. In the technique as disclosed in the document, the host device is provided with the expansion VOL that is not associated (mapped) with the final storage medium so that the access from the host device to the expansion VOL is accepted. The data written to the expansion VOL are compressed online using the cache memory, and the compressed data are associated with the compression VOL as the volume associated with the final storage medium. Simultaneously, the mapping information with respect to the area on the expansion VOL to which the data have been written, and the position on the compression VOL at which the compressed data of the written data are associated is maintained and managed. Upon reception of the reading request to the expansion VOL from the host device, the position information on the expansion VOL, which has been designated by the reading request is converted into the position information of the final storage medium based on the mapping information. The compressed data are then read out on the cache memory from the final storage medium. The compressed data are expanded using the cache memory, and transferred to the host device.</p><heading id="h-0002" level="1">SUMMARY</heading><p id="p-0004" num="0003">In the structure in which the controller associated with the storage includes the cache memory, and compresses the cache data which have been changed based on the writing request for writeback to the storage, the writing operation can be executed at high speeds. The structure, however, fails to manage and expand the processor and the memory separately.</p><p id="p-0005" num="0004">The processor and the memory can be managed and expanded separately by connecting the controller to the shared memory for storing the cache data therein in the heterogeneous connection environment, resulting in structural flexibility. The structure, however, prolongs the time taken for the controller to read and compress the cache data in the shared memory until the writeback operation.</p><p id="p-0006" num="0005">It is an object of the present invention to provide the storage system with high structural flexibility and high writeback processing performance.</p><p id="p-0007" num="0006">The storage system control method according to the present invention as a representative example is implemented by a controller of a storage system. The method includes a step of storing data on a storage in a shared memory as cache data, a step of changing the cache data based on a writing request from outside, and a writeback step of writing back dirty cache data as the cache data which have been changed based on the writing request to the storage. The method further includes a step of storing the dirty cache data in a writeback processing memory prior to execution of the writeback step. The writeback processing memory requires time for executing a writeback data process shorter than time required by the shared memory.</p><p id="p-0008" num="0007">A representative example of the storage system according to the present invention includes a storage for storing data, a controller for processing data stored in the storage, a first memory which allows access from multiple controllers, and a second memory which allows access from at least one controller. The controller stores data on the storage in the first memory as cache data, changes the cache data based on a writing request from outside, stores dirty cache data in the second memory as the cache data which have been changed based on the writing request, executes a process for writing back the dirty cache data stored in the second memory and subjected to a predetermined data process to the storage. The second memory requires time for executing the predetermined data process shorter than time required by the first memory.</p><p id="p-0009" num="0008">The present invention provides the storage system with high structural flexibility and high writeback processing performance. Problems, structures, and advantageous effects other than those described above will be clarified by the following description of the embodiment.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is an explanatory view of a first structure and an operation of a storage system;</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is an explanatory view of a second structure and an operation of the storage system;</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is an explanatory view of a third structure and an operation of the storage system;</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates a structure of the storage system;</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is an explanatory view of a functional structure of a controller;</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>6</b></figref> represents a specific example of a cache management method configuration file;</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>7</b></figref> represents a specific example of a cache management table;</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a flowchart representing a process procedure executed by a cache management method control unit;</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a flowchart representing a detailed storage data reading process;</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a flowchart representing a detailed dirty cache redundancy process;</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a flowchart representing a detailed dirty cache preceding storage process; and</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a flowchart representing a detailed cache writeback process.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0004" level="1">DETAILED DESCRIPTION</heading><p id="p-0022" num="0021">An embodiment will be described referring to the drawings.</p><heading id="h-0005" level="1">Embodiment</heading><p id="p-0023" num="0022">A structure and an operation of the storage system of the embodiment will be described. <figref idref="DRAWINGS">FIG. <b>1</b></figref> is an explanatory view of a first structure and an operation of the storage system.</p><p id="p-0024" num="0023">A controller <b>110</b> as shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref> as a node of the storage system allows a not shown host computer to execute reading from/writing to a storage <b>116</b>. The controller <b>110</b> is connected to shared memories A <b>131</b> and B <b>132</b> via a heterogeneous switch <b>130</b>.</p><p id="p-0025" num="0024">Similarly, a controller <b>120</b> as a node of the storage system allows the not shown host computer to execute reading from/writing to a storage <b>126</b>. The controller <b>120</b> is connected to the shared memories A and B via the heterogeneous switch <b>130</b>.</p><p id="p-0026" num="0025">The controller <b>110</b> includes a CPU (Central Processing Unit) <b>112</b> and a memory <b>113</b>. The CPU <b>112</b> is a processor for executing data processing that involves access to the storage <b>116</b>.</p><p id="p-0027" num="0026">The controller <b>110</b> stores cache data to be read from/written to the storage <b>116</b> in the shared memory A. The use of the shared memory A as the cache memory for storing the cache data improves latency of the access from the host computer. As the shared memory connected in the heterogeneous environment is used as the cache memory, the processor and the memory can be separately managed and expanded, resulting in the structural flexibility.</p><p id="p-0028" num="0027">Upon acceptance of the writing request from the host computer, the controller <b>110</b> changes the cache data in the shared memory A. The cache data changed based on the writing request become dirty cache data which do not match data on the storage <b>116</b>. The dirty cache data are written back to the storage <b>116</b> based on a sync request from the host computer, for example.</p><p id="p-0029" num="0028">Upon generation of the dirty cache data based on the writing request, the controller <b>110</b> copies the dirty cache data to the shared memory B for performing a redundancy operation. Prior to the writeback to the storage <b>116</b>, the dirty cache data are stored in the memory <b>113</b> of the controller <b>110</b>.</p><p id="p-0030" num="0029">The time required for the memory <b>113</b> of the controller <b>110</b> to execute the writeback data process is shorter compared with the shared memory A. Execution of the writeback while having the dirty cache data temporarily stored in the memory <b>113</b> improves the writeback latency to be better than execution of the writeback by directly reading the dirty cache data from the shared memory A. That is, the memory <b>113</b> is used as a writeback processing memory which needs not hold the cache data requiring no writeback (read cache). Accordingly, the required capacity of the memory <b>113</b> can be made significantly smaller than that of the shared memory A used as the cache memory.</p><p id="p-0031" num="0030">Upon the writeback operation, the CPU <b>112</b> reads the dirty cache data from the memory <b>113</b>, and executes the writeback data process so that the data are written to the storage <b>116</b>. The writeback data process may be exemplified by data compression, for example. If the storage <b>116</b> is configured to hold the compressed data, the CPU <b>112</b> compresses the data to be written to the storage <b>116</b>, and writes the compressed data to the storage <b>116</b>. Upon execution of the reading process from the storage <b>116</b>, the CPU <b>112</b> reads the compressed data from the storage <b>116</b>, and stores the data in the cache memory.</p><p id="p-0032" num="0031">The dirty cache data in the shared memory A may be stored in the memory <b>113</b> at the timing after acceptance of the writeback request, that is, the sync request. The dirty cache data may be stored in the memory <b>113</b> before accepting the sync request, that is, prior to the sync request. If the dirty cache data are stored in the memory <b>113</b> precedingly, the dirty cache data need not be made redundant for the shared memory B because the dirty cache data can be made redundant by the shared memory A and the memory <b>113</b>.</p><p id="p-0033" num="0032">The controller <b>120</b> includes a CPU <b>122</b> and a memory <b>123</b>. Operations of the controller <b>120</b> are the same as those of the controller <b>110</b>, and explanations thereof, thus will be omitted.</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is an explanatory view of a second structure and an operation of the storage system. In the structure as illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the controllers <b>110</b> and <b>120</b> are further connected to an accelerator A <b>133</b> via the heterogeneous switch <b>130</b>. Other structures are similar to those illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0035" num="0034">In the structure as illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the controller <b>110</b> uses the shared memory A as the cache memory.</p><p id="p-0036" num="0035">Upon generation of the dirty cache data in the shared memory A based on the writing request, the controller <b>110</b> copies the dirty cache data to the shared memory B for performing the redundancy operation. Prior to the writeback to the storage <b>116</b>, the dirty cache data are stored in the memory of the accelerator A<b>133</b>.</p><p id="p-0037" num="0036">The accelerator A <b>133</b> is a shared data processing device to be shared by multiple controllers, and configured to execute the writeback data process, for example, data compression.</p><p id="p-0038" num="0037">Storage of the dirty cache data in the memory of the accelerator A <b>133</b> reduces the required data processing time to be shorter than the one taken by directly reading the dirty cache data from the shared memory A. Accordingly, the writeback latency can be improved. That is, the memory of the accelerator A <b>133</b> is used as the writeback processing memory so that the cache data (read cache) requiring no writeback operation need not be held. Therefore, the capacity required for the memory of the accelerator A <b>113</b> can be made significantly smaller than that of the shared memory A used as the cache memory.</p><p id="p-0039" num="0038">Upon the writeback operation, the accelerator A <b>133</b> reads the dirty cache data from its memory to execute the writeback data process. The data processing results, for example, the compressed data are moved to the memory <b>113</b> of the controller <b>110</b>, and written to the storage <b>116</b> without the data processing executed by the CPU <b>112</b>.</p><p id="p-0040" num="0039">The dirty cache data in the shared memory A may be stored in the memory of the accelerator A <b>133</b> at the timing after acceptance of the writeback request, that is, the sync request. The dirty cache data may be stored in the memory of the accelerator A <b>133</b> before accepting the sync request, that is, prior to the sync request. If the dirty cache data are stored in the memory of the accelerator A <b>133</b> precedingly, the dirty cache data need not be made redundant for the shared memory B because the dirty cache data can be made redundant by the shared memory A and the memory of the accelerator A <b>133</b>.</p><p id="p-0041" num="0040">The controller <b>120</b> includes the CPU <b>122</b> and the memory <b>123</b>. Operations of the controller <b>120</b> are the same as those of the controller <b>110</b>, and explanations thereof, thus will be omitted.</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is an explanatory view of a third structure and an operation of the storage system. In the structure as illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the controllers <b>110</b> and <b>120</b> are connected to the shared memory A <b>131</b> and an accelerator B <b>134</b> via the heterogeneous switch <b>130</b>. The shared memory B is not required. The accelerator B <b>134</b> includes an onboard memory <b>135</b> with capacity that can be used for making the cache memory redundant. Other structures are similar to those illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0043" num="0042">In the structure as illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the controller <b>110</b> uses the shared memory A as the cache memory.</p><p id="p-0044" num="0043">Upon generation of the dirty cache data in the shared memory A based on the writing request, the controller <b>110</b> copies the dirty cache data to the onboard memory <b>135</b> of the accelerator B <b>134</b> for performing the redundancy operation. Prior to the writeback to the storage <b>116</b>, the controller stores the dirty cache data in the onboard memory <b>135</b>.</p><p id="p-0045" num="0044">The accelerator B <b>134</b> is a shared data processing device to be shared by multiple controllers, and configured to execute the writeback data process, for example, data compression.</p><p id="p-0046" num="0045">Storage of the dirty cache data in the onboard memory <b>135</b> of the accelerator B <b>134</b> reduces the required data processing time to be shorter than the one taken by directly reading the dirty cache data from the shared memory A. Accordingly, the writeback latency can be improved. That is, the onboard memory <b>135</b> is used as the writeback processing memory. The memory with sufficient capacity is allowed to hold the cache data (read cache) requiring no writeback operation.</p><p id="p-0047" num="0046">Upon the writeback operation, the accelerator B <b>134</b> reads the dirty cache data from the onboard memory <b>135</b> to execute the writeback data process. The data processing results, for example, the compressed data are moved to the memory <b>113</b> of the controller <b>110</b>, and written to the storage <b>116</b> without the data processing executed by the CPU <b>112</b>.</p><p id="p-0048" num="0047">The controller <b>120</b> includes the CPU <b>122</b> and the memory <b>123</b>. Operations of the controller <b>120</b> are the same as those of the controller <b>110</b>, and explanations thereof, thus will be omitted.</p><p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates a structure of the storage system. As <figref idref="DRAWINGS">FIG. <b>4</b></figref> shows, the controllers <b>110</b> and <b>120</b> are connected to host computers <b>101</b> and <b>102</b> via a network <b>103</b>.</p><p id="p-0050" num="0049">The controller <b>110</b> includes a front I/F <b>111</b>, the CPU <b>112</b>, the memory <b>113</b>, a heterogeneous I/F <b>114</b>, and a back I/F <b>115</b>. The memory <b>113</b> is connected to the CPU <b>112</b>. The CPU <b>112</b> is bus connected to the front I/F <b>111</b>, the heterogeneous I/F <b>114</b>, and the back I/F <b>115</b>.</p><p id="p-0051" num="0050">The front I/F <b>111</b> is an interface for connection to the network <b>103</b>. The heterogeneous I/F <b>114</b> is an interface for connection to the heterogeneous switch <b>130</b>. The back I/F <b>115</b> is an interface for connection to the storage <b>116</b>.</p><p id="p-0052" num="0051">The heterogeneous switch <b>130</b> allows the controller <b>110</b> to be heterogeneously connected to the shared memories A <b>131</b>, B <b>132</b>, the accelerators A <b>133</b> and B <b>134</b>.</p><p id="p-0053" num="0052">The controller <b>120</b> includes a front I/F <b>121</b>, the CPU <b>122</b>, the memory <b>123</b>, a heterogeneous I/F <b>124</b>, and a back I/F <b>125</b>. Since structures and operations of the controller <b>120</b> are the same as those of the controller <b>110</b>, explanations, thus, will be omitted.</p><p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is an explanatory view of functional structures of the controller <b>110</b>. The CPU <b>112</b> of the controller <b>110</b> serves as a cache management section <b>201</b> by developing a predetermined program to be executed in the memory <b>113</b>.</p><p id="p-0055" num="0054">The cache management section <b>201</b> is constituted by functional parts including a cache management method control unit <b>202</b>, a cache temporary storage unit <b>203</b>, a storage data reading/writing unit <b>204</b>, a storage data expansion unit <b>205</b>, a cache reading/writing unit <b>206</b>, a cache writeback processing mechanism association unit <b>207</b>, and a cache management table storage unit <b>208</b>.</p><p id="p-0056" num="0055">The cache management method control unit <b>202</b> acquires a cache management method configuration file <b>211</b> from the host computer <b>101</b>, and selects a cache control operation with reference to the cache management method configuration file <b>211</b>. The cache control operation is performed using the writeback processing memory and the processor for executing the writeback data process (for example, data compression), which will be described in detail later.</p><p id="p-0057" num="0056">Upon reception of a data access request <b>212</b> from the host computer <b>101</b>, the cache management method control unit <b>202</b> processes the request by executing the cache control in accordance with the selected operation.</p><p id="p-0058" num="0057">The cache temporary storage unit <b>203</b> serves as a processing unit which temporarily stores a cache <b>221</b>.</p><p id="p-0059" num="0058">The storage data reading/writing unit <b>204</b> serves as a processing unit which reads/writes storage data <b>213</b> from/to the storage <b>116</b>.</p><p id="p-0060" num="0059">The storage data expansion unit <b>205</b> expands the storage data read from the storage <b>116</b>, and passes the data to the cache temporary storage unit <b>203</b> as the readout cache <b>221</b>.</p><p id="p-0061" num="0060">The cache reading/writing unit <b>206</b> reads/writes the cache from/to a cache storage medium <b>231</b>. The cache storage medium <b>231</b> serves as a medium for storing the cache data and the dirty cache data, which is exemplified by the memories <b>113</b>, <b>123</b>, the shared memories A <b>131</b>, B <b>132</b>, the onboard memory <b>135</b>, and the like.</p><p id="p-0062" num="0061">The cache writeback processing mechanism association unit <b>207</b> is a functional part associated with a cache writeback processing mechanism <b>232</b>. The cache writeback processing mechanism <b>232</b> serves to execute the writeback data process such as compression, which is exemplified by the CPUs <b>112</b>, <b>122</b>, the accelerators A <b>133</b>, B <b>134</b>, and the like.</p><p id="p-0063" num="0062">The cache management table storage unit <b>208</b> stores a cache management table <b>222</b>.</p><p id="p-0064" num="0063">In the cache management table <b>222</b>, a data ID for identifying data in the storage <b>116</b> is associated with a cache state.</p><p id="p-0065" num="0064"><figref idref="DRAWINGS">FIG. <b>6</b></figref> represents a specific example of the cache management method configuration file <b>211</b>. Referring to <figref idref="DRAWINGS">FIG. <b>6</b></figref>, an arbitrary configuration file is selected from configuration examples <b>301</b> to <b>304</b>, and shared by the respective controllers of the storage system.</p><p id="p-0066" num="0065">Each of the configuration examples <b>301</b> to <b>304</b> includes such items as &#x201c;cache load medium&#x201d;, &#x201c;dirty cache redundancy medium&#x201d;, &#x201c;cache writeback processing mechanism&#x201d;, &#x201c;dirty cache preceding storage medium&#x201d;, and &#x201c;preceding storage dirty cache redundancy flag&#x201d;.</p><p id="p-0067" num="0066">The cache load medium serves as a cache memory which holds the data read from the storage and expanded as the cache data.</p><p id="p-0068" num="0067">The dirty cache redundancy medium serves to store a copy of the dirty cache data among cache data in the cache load medium, which have been changed based on the writing request.</p><p id="p-0069" num="0068">The cache writeback processing mechanism identifies the mechanism expected to execute the writeback data process such as compression.</p><p id="p-0070" num="0069">The dirty cache preceding storage medium serves as a memory, in other words, a writeback processing memory for holding the dirty cache data prior to acceptance of the sync request.</p><p id="p-0071" num="0070">The preceding storage dirty cache redundancy flag indicates whether or not the dirty cache data in the dirty cache data redundancy medium are cleared upon preceding storage of the dirty cache data prior to the sync request.</p><p id="p-0072" num="0071">If the preceding storage dirty cache redundancy flag indicates NO, the dirty cache data stored in the dirty cache preceding storage medium are handled as redundancy data of the dirty cache data in the cache load medium. Accordingly, the dirty cache data stored in the dirty cache preceding storage medium are deleted from the dirty cache redundancy medium.</p><p id="p-0073" num="0072">Meanwhile, if the preceding storage dirty cache redundancy flag indicates YES, the dirty cache data stored in the dirty cache preceding storage medium are kept stored in the dirty cache redundancy medium.</p><p id="p-0074" num="0073">Referring to the configuration example <b>301</b> as shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the &#x201c;shared memory A (2 TB)&#x201d; serves as the cache load medium, the &#x201c;shared memory B (2 TB)&#x201d; serves as the dirty cache redundancy medium, the &#x201c;controller CPU&#x201d; serves as the cache writeback processing mechanism, the &#x201c;controller memory (16 GB)&#x201d; serves as the dirty cache preceding storage medium, and the preceding storage dirty cache redundancy flag indicates &#x201c;NO&#x201d;.</p><p id="p-0075" num="0074">Referring to the configuration example <b>302</b> as shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the &#x201c;shared memory A (2 TB)&#x201d; serves as the cache load medium, the &#x201c;shared memory B (2 TB)&#x201d; serves as the dirty cache redundancy medium, the &#x201c;accelerator A&#x201d; serves as the cache writeback processing mechanism, no dirty cache preceding storage medium is used, that is, &#x201c;none&#x201d;, and the preceding storage dirty cache redundancy flag indicates &#x201c;-&#x201d;.</p><p id="p-0076" num="0075">Referring to the configuration example <b>303</b> as shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the &#x201c;shared memory A (2 TB)&#x201d; serves as the cache load medium, the &#x201c;shared memory B (2 TB)&#x201d; serves as the dirty cache redundancy medium, the &#x201c;accelerator B&#x201d; serves as the cache writeback processing mechanism, the &#x201c;onboard memory (12 GB)&#x201d; serves as the dirty cache preceding storage medium, and the preceding storage dirty cache redundancy flag indicates &#x201c;YES&#x201d;.</p><p id="p-0077" num="0076">Referring to the configuration example <b>304</b> as shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the &#x201c;shared memory A (2 TB)&#x201d; serves as the cache load medium, the &#x201c;shared memory B (2 TB)&#x201d; serves as the dirty cache redundancy medium, the &#x201c;accelerator B&#x201d; serves as the cache writeback processing mechanism, no dirty cache preceding storage medium is used, that is, &#x201c;none&#x201d;, and the preceding storage dirty cache redundancy flag indicates &#x201c;-&#x201d;.</p><p id="p-0078" num="0077"><figref idref="DRAWINGS">FIG. <b>7</b></figref> represents a specific example of the cache management table. A specific example <b>401</b> of <figref idref="DRAWINGS">FIG. <b>7</b></figref> represents an example of the cache management table <b>222</b> when the preceding storage dirty cache redundancy flag indicates &#x201c;NO&#x201d;. A specific example <b>402</b> represents an example of the cache management table <b>222</b> when the preceding storage dirty cache redundancy flag indicates &#x201c;YES&#x201d;.</p><p id="p-0079" num="0078">The cache management table <b>222</b> includes such items as the data ID, a load address, a redundancy address, a preceding storage address, and a data size.</p><p id="p-0080" num="0079">The data ID denotes identification information for identifying data on the storage.</p><p id="p-0081" num="0080">The load address denotes an address of the cache data on the cache load medium.</p><p id="p-0082" num="0081">The redundancy address denotes an address of the dirty cache data on the dirty cache redundancy medium.</p><p id="p-0083" num="0082">The preceding storage address denotes an address of the dirty cache data on the dirty cache preceding storage medium.</p><p id="p-0084" num="0083">The data size denotes size of data.</p><p id="p-0085" num="0084">Referring to the specific example <b>401</b>, when the preceding storage dirty cache redundancy flag indicates &#x201c;NO&#x201d;, the load address and the data size of the unwritten cache data are registered, while having the redundancy address and the preceding storage address kept unregistered.</p><p id="p-0086" num="0085">Upon writing operation, the preceding storage address to be precedingly stored is added. However, the redundancy address is unregistered.</p><p id="p-0087" num="0086">Upon occurrence of multiple writing operations before acceptance of the sync request to cause dirty cache data overflow from the dirty cache preceding storage medium, the overflown dirty cache data are copied to the dirty cache redundancy medium to clear the preceding storage address. The load address and the redundancy address of the overflown dirty cache data are registered, and the preceding storage address is unregistered.</p><p id="p-0088" num="0087">Referring to the specific example <b>402</b>, when the preceding storage dirty cache redundancy flag indicates &#x201c;YES&#x201d;, the load address and the data size of the unwritten cache data are registered while having the redundancy address and the preceding storage address kept unregistered.</p><p id="p-0089" num="0088">Upon writing operation, the dirty cache data are copied to the dirty cache redundancy medium so that the redundancy address is registered, and the preceding storage address of the data to be precedingly stored is added.</p><p id="p-0090" num="0089">Upon occurrence of multiple writing operations before acceptance of the sync request to cause dirty cache data overflow from the dirty cache preceding storage medium, the preceding storage address of the overflown dirty cache data is cleared. Accordingly, the load address and the redundancy address of the overflown dirty cache data are registered, and the preceding storage address is unregistered.</p><p id="p-0091" num="0090"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a flowchart representing a process procedure executed by the cache management method control unit. Upon start of the process (step S<b>501</b>), the cache management method control unit <b>202</b> reads the cache management method configuration file <b>211</b> (step S<b>502</b>).</p><p id="p-0092" num="0091">When the data access request from the host computer is read, (step S<b>503</b>), the cache management method control unit <b>202</b> determines whether the request corresponds to reading (Read) or writing (Write) (step S<b>504</b>).</p><p id="p-0093" num="0092">If the reading or writing request has been issued (YES in step S<b>504</b>), the cache management method control unit <b>202</b> determines whether or not the data ID of the object data exists in the cache management table <b>222</b> (step S<b>505</b>).</p><p id="p-0094" num="0093">If the data ID of the object data exists in the cache management table <b>222</b> (YES in step S<b>505</b>), the cache management method control unit <b>202</b> copies the cache stored at the load address registered in the cache management table <b>222</b> from the cache load medium to the cache temporary storage unit (step S<b>506</b>).</p><p id="p-0095" num="0094">If the data ID of the object data does not exist in the cache management table <b>222</b> (NO in step S<b>505</b>), the storage data reading/writing unit <b>204</b> reads the storage data (step S<b>507</b>).</p><p id="p-0096" num="0095">Subsequent to step S<b>506</b> or S<b>507</b>, the cache management method control unit <b>202</b> determines whether or not the writing request has been issued (step S<b>508</b>).</p><p id="p-0097" num="0096">If the writing request has been issued (YES in step S<b>508</b>), the cache management method control unit <b>202</b> updates the cache in the cache temporary storage unit (step S<b>509</b>), and executes the dirty cache redundancy process (step S<b>510</b>). The process then returns to step S<b>503</b>.</p><p id="p-0098" num="0097">If no writing request has been issued (NO in step S<b>508</b>), but the reading request has been issued, the cache management method control unit <b>202</b> transmits data (cache) in the cache temporary storage unit to the host computer (step S<b>511</b>). The process then returns to step S<b>503</b>.</p><p id="p-0099" num="0098">If no writing request nor reading request has been issued (NO in step S<b>504</b>), the cache management method control unit <b>202</b> determines whether or not the sync request (Sync) has been issued (step S<b>512</b>).</p><p id="p-0100" num="0099">If the sync request has been issued (YES in step S<b>512</b>), the storage data reading/writing unit <b>204</b> executes the cache writeback process (step S<b>513</b>). The process then returns to step S<b>503</b>.</p><p id="p-0101" num="0100">If no sync request has been issued (NO in step S<b>512</b>), the cache management method control unit <b>202</b> executes the process corresponding to an unauthorized request (step S<b>514</b>). The process then returns to step S<b>503</b>.</p><p id="p-0102" num="0101"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a flowchart representing a detailed storage data reading process. In other words, <figref idref="DRAWINGS">FIG. <b>9</b></figref> represents a detailed process to be executed in step S<b>507</b> as shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>.</p><p id="p-0103" num="0102">Upon start of the storage data reading process (step S<b>601</b>), the storage data reading/writing unit <b>204</b> reads the object data from the storage (step S<b>602</b>), and the storage data expansion unit <b>205</b> expands the object data (step S<b>603</b>).</p><p id="p-0104" num="0103">The storage data expansion unit <b>205</b> stores the expanded data (cache) in the cache temporary storage unit (step S<b>604</b>). The cache reading/writing unit <b>206</b> then secures a memory area for storing the cache on the cache load medium (step S<b>605</b>), and moves the cache to the memory area (step S<b>606</b>).</p><p id="p-0105" num="0104">The cache management method control unit <b>202</b> newly registers the data ID of the object data, the address and the size of the memory area in the cache management table <b>222</b> (step S<b>607</b>). The storage data reading process is then terminated (step S<b>608</b>).</p><p id="p-0106" num="0105"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a flowchart representing a detailed dirty cache redundancy process. In other words, <figref idref="DRAWINGS">FIG. <b>10</b></figref> represents a detailed process to be executed in step S<b>510</b> as shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>.</p><p id="p-0107" num="0106">Upon start of the dirty cache redundancy process (step S<b>701</b>), the cache management method control unit <b>202</b> determines whether or not the dirty cache preceding storage medium has been designated (step S<b>702</b>). If the dirty cache preceding storage medium has been designated (YES in step S<b>702</b>), the cache management method control unit <b>202</b> executes the dirty cache preceding storage process (step S<b>703</b>).</p><p id="p-0108" num="0107">Subsequent to step S<b>703</b>, the cache management method control unit <b>202</b> determines whether or not the preceding storage dirty cache redundancy flag indicates YES (step S<b>704</b>). If the preceding storage dirty cache redundancy flag indicates NO (NO in step S<b>704</b>), the dirty cache redundancy process is terminated (step S<b>707</b>).</p><p id="p-0109" num="0108">If the preceding storage dirty cache redundancy flag indicates YES (YES in step S<b>704</b>), or if the dirty cache preceding storage medium has not been designated (No in step S<b>702</b>), the cache reading/writing unit <b>206</b> secures the memory area for storing the object cache memory on the dirty cache redundancy medium (step S<b>705</b>), and updates the cache management table entry redundancy address of the object data (step S<b>706</b>). The dirty cache redundancy process is then terminated (step S<b>707</b>).</p><p id="p-0110" num="0109"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a flowchart representing a detailed dirty cache preceding storage process. In other words, <figref idref="DRAWINGS">FIG. <b>11</b></figref> represents a detailed process to be executed in step S<b>703</b> as shown in <figref idref="DRAWINGS">FIG. <b>10</b></figref>.</p><p id="p-0111" num="0110">Upon start of the dirty cache preceding storage process (step S<b>801</b>), the cache management method control unit <b>202</b> secures the memory area for storing the object cache on the dirty cache preceding storage medium (step S<b>802</b>), and determines whether or not cache size overflow has occurred (step S<b>803</b>). Specifically, a total value of the data size of the dirty cache data is obtained. If the data size exceeds the capacity of the dirty cache preceding storage medium, it is determined that the cache size overflow has occurred.</p><p id="p-0112" num="0111">If no cache size overflow has occurred (NO in step S<b>803</b>), the cache management method control unit <b>202</b> sets the preceding storage address of the object cache entry in the cache management table (step S<b>808</b>). The dirty cache preceding storage process is then terminated (step S<b>809</b>).</p><p id="p-0113" num="0112">If the cache size overflow has occurred (YES in step S<b>803</b>), the cache management method control unit <b>202</b> determines whether or not the preceding storage dirty cache redundancy flag indicates YES (step S<b>804</b>).</p><p id="p-0114" num="0113">If the preceding storage dirty cache redundancy flag indicates NO (NO in step S<b>804</b>), the cache management method control unit <b>202</b> secures the memory area for storing the overflown cache on the dirty cache redundancy medium, and moves the cache to the memory area (step S<b>805</b>). The cache management method control unit <b>202</b> then sets the redundancy address of the (overflown) cache entry in the cache management table <b>222</b> (step S<b>806</b>).</p><p id="p-0115" num="0114">If the preceding storage dirty cache redundancy flag indicates YES (YES in step S<b>804</b>), or subsequent to step S<b>806</b>, the cache management method control unit <b>202</b> clears the preceding storage address of the (overflown) cache entry in the cache management table <b>222</b> (step S<b>807</b>). The cache management method control unit <b>202</b> then sets the preceding storage address of the object cache entry in the cache management table (step S<b>808</b>). The dirty cache preceding storage process is then terminated (step S<b>809</b>).</p><p id="p-0116" num="0115"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a flowchart representing a detailed cache writeback process. In other words, <figref idref="DRAWINGS">FIG. <b>12</b></figref> represents a detailed process to be executed in step S<b>513</b> as shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>.</p><p id="p-0117" num="0116">Upon start of the cache writeback process (step S<b>901</b>), the cache management method control unit <b>202</b> determines whether or not the preceding storage address of the object cache has been set in the cache management table <b>222</b> (step S<b>902</b>).</p><p id="p-0118" num="0117">If the preceding storage address of the object cache has been set in the cache management table <b>222</b> (YES in step S<b>902</b>), the process proceeds to step S<b>904</b>.</p><p id="p-0119" num="0118">If the preceding storage address of the object cache has not been set in the cache management table <b>222</b> (NO in step S<b>902</b>), the cache management method control unit <b>202</b> determines whether or not the dirty cache preceding storage medium has been designated (step S<b>903</b>). If the dirty cache preceding storage medium has been designated (YES in step S<b>903</b>), the dirty cache preceding storage process (step S<b>703</b>) is executed. The process then proceeds to step S<b>904</b>. If the dirty cache preceding storage medium has not been designated (NO in step S<b>903</b>), the process proceeds to step S<b>905</b>.</p><p id="p-0120" num="0119">In step S<b>904</b>, the cache data stored in the dirty cache preceding storage medium are compressed by the cache writeback processing mechanism (step S<b>904</b>).</p><p id="p-0121" num="0120">In step S<b>905</b>, the cache data stored in the cache load medium are compressed by the cache writeback processing mechanism (step S<b>905</b>).</p><p id="p-0122" num="0121">Subsequent to step S<b>904</b> or S<b>905</b>, the cache writeback processing mechanism association unit <b>207</b> receives the compressed data from the cache writeback processing mechanism, and the storage data reading/writing unit <b>204</b> writes the data to the storage (step S<b>906</b>).</p><p id="p-0123" num="0122">Subsequent to step S<b>906</b>, the cache management method control unit <b>202</b> releases all memory areas at the respective registered addresses of the object cache entry in the cache management table <b>222</b> (step S<b>907</b>), and deletes the object cache entry in the cache management table <b>222</b> (step S<b>908</b>). The cache writeback process is then terminated (step S<b>909</b>).</p><p id="p-0124" num="0123">In the disclosed storage system, the controller <b>110</b> of the storage system executes the step of storing data on the storage in the shared memory as cache data, the step of changing the cache data based on the writing request from outside, and the writeback step of writing back dirty cache data as the cache data which have been changed based on the writing request to the storage. The controller further executes the step of storing the dirty cache data in the writeback processing memory prior to execution of the writeback step. The writeback processing memory requires time for executing the writeback data process shorter than time required by the shared memory.</p><p id="p-0125" num="0124">The storage system with high structural flexibility and high writeback processing performance can be attained.</p><p id="p-0126" num="0125">In the above-described structure, the controller <b>110</b> executes the writeback data process by copying the dirty cache data to another shared memory for performing the redundancy operation, and uses the memory of the controller as the writeback processing memory.</p><p id="p-0127" num="0126">The structure allows improvement both in the structural flexibility and writeback latency while suppressing the number of devices that constitute the system.</p><p id="p-0128" num="0127">In the above-described structure, the controller <b>110</b> copies the dirty cache data to another shared memory for performing a redundancy operation, and uses the memory of the shared data processing device as the writeback processing memory to allow the shared data processing device to execute the writeback data process.</p><p id="p-0129" num="0128">In the structure, the shared data processing device is allowed to execute the writeback process such as compression. This makes it possible to improve both the structural flexibility and the writeback latency while lowering the processing load to the controller.</p><p id="p-0130" num="0129">In the above-described structure, the controller <b>110</b> copies the dirty cache data to the memory of the shared data processing device for performing the redundancy operation, and makes the memory of the shared data processing device usable as the writeback processing memory to allow the shared data processing device to execute the writeback data process.</p><p id="p-0131" num="0130">In the structure, the shared data processing device can be used as the shared memory. This makes it possible to improve both the structural flexibility and the writeback latency while suppressing the number of devices that constitute the system and lowering the load to the controller.</p><p id="p-0132" num="0131">The controller is capable of storing the dirty cache data in the writeback processing memory prior to acceptance of the writeback request.</p><p id="p-0133" num="0132">The above-described structure and operation allow further reduction in the time required for completion of the writeback operation from acceptance of the writeback request.</p><p id="p-0134" num="0133">The controller may be configured to make the dirty cache data which have been precedingly stored in the writeback processing memory unapplicable to the redundancy operation for another storage medium.</p><p id="p-0135" num="0134">The above-described structure and operation efficiently make the dirty cache data redundant while suppressing the usable capacity.</p><p id="p-0136" num="0135">The controller selects the writeback processing memory and the processor for executing the writeback data process with reference to the preliminarily designated configuration information.</p><p id="p-0137" num="0136">The structure allows appropriate selection of the cache-related operation in accordance with the system configuration.</p><p id="p-0138" num="0137">The present invention is not limited to the above-described embodiment, but may be variously modified. The foregoing embodiment has been described in detail for readily understanding of the present invention which is not necessarily limited to the one equipped with all the structures as described above. It is possible to replace and add the structure as well as removal thereof.</p><p id="p-0139" num="0138">In the foregoing embodiment, the writeback data processing is exemplified by compression. However, any other processing may be executed.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A storage system control method implemented by a controller of a storage system, the method comprising:<claim-text>a step of storing data on a storage in a shared memory as cache data;</claim-text><claim-text>a step of changing the cache data based on a writing request from outside; and</claim-text><claim-text>a writeback step of writing back dirty cache data as the cache data which have been changed based on the writing request to the storage,</claim-text><claim-text>the method further comprising a step of storing the dirty cache data in a writeback processing memory prior to execution of the writeback step, the writeback processing memory requiring time for executing a writeback data process shorter than time required by the shared memory.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The storage system control method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the controller executes the writeback data process by copying the dirty cache data to another shared memory for performing a redundancy operation, and uses a memory of the controller as the writeback processing memory.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The storage system control method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the controller copies the dirty cache data to another shared memory for performing a redundancy operation, and uses a memory of a shared data processing device as the writeback processing memory to allow the shared data processing device to execute the writeback data process.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The storage system control method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the controller copies the dirty cache data to a memory of a shared data processing device for performing a redundancy operation, and makes a memory of the shared data processing device usable as the writeback processing memory to allow the shared data processing device to execute the writeback data process.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The storage system control method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the controller stores the dirty cache data in the writeback processing memory prior to acceptance of a writeback request.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The storage system control method according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the controller makes the dirty cache data which have been precedingly stored in the writeback processing memory unapplicable to a redundancy operation for another storage medium.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The storage system control method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the controller selects the writeback processing memory and a processor for executing the writeback data process with reference to preliminarily designated configuration information.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The storage system control method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the writeback data process is executed by compressing the dirty cache data.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. A storage system, comprising:<claim-text>a storage for storing data;</claim-text><claim-text>a controller for processing data stored in the storage;</claim-text><claim-text>a first memory which allows access from multiple controllers; and</claim-text><claim-text>a second memory which allows access from at least one controller, wherein:</claim-text><claim-text>the controller stores data on the storage in the first memory as cache data, changes the cache data based on a writing request from outside, stores dirty cache data in the second memory as the cache data which have been changed based on the writing request, executes a process for writing back the dirty cache data stored in the second memory and subjected to a predetermined data process to the storage; and</claim-text><claim-text>the second memory requires time for executing the predetermined data process shorter than time required by the first memory.</claim-text></claim-text></claim></claims></us-patent-application>