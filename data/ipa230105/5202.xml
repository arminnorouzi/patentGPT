<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005203A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005203</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17305229</doc-number><date>20210701</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>13</main-group><subgroup>40</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>70</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>272</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>13</main-group><subgroup>40</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>70</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>272</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>10016</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30196</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20084</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30221</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">ENHANCED ANIMATION GENERATION BASED ON VIDEO WITH LOCAL PHASE</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Electronic Arts Inc.</orgname><address><city>Redwood City</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Shi</last-name><first-name>Mingyi</first-name><address><city>Hong Kong</city><country>CN</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Zhao</last-name><first-name>Yiwei</first-name><address><city>Sunnyvale</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Starke</last-name><first-name>Wolfram Sebastian</first-name><address><city>Edinburgh</city><country>GB</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Sardari</last-name><first-name>Mohsen</first-name><address><city>Redwood City</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>Aghdaie</last-name><first-name>Navid</first-name><address><city>San Jose</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Embodiments of the systems and methods described herein provide a dynamic animation generation system that can apply a real-life video clip with a character in motion to a first neural network to receive rough motion data, such as pose information, for each of the frames of the video clip, and overlay the pose information on top of the video clip to generate a modified video clip. The system can identify a sliding window that includes a current frame, past frames, and future frames of the modified video clip, and apply the modified video clip to a second neural network to predict a next frame. The dynamic animation generation system can then move the sliding window to the next frame while including the predicted next frame, and apply the new sliding window to the second neural network to predict the following frame to the next frame.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="66.72mm" wi="101.77mm" file="US20230005203A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="222.84mm" wi="156.72mm" orientation="landscape" file="US20230005203A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="222.93mm" wi="157.31mm" orientation="landscape" file="US20230005203A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="224.28mm" wi="156.89mm" orientation="landscape" file="US20230005203A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="224.03mm" wi="156.63mm" orientation="landscape" file="US20230005203A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="238.68mm" wi="149.61mm" orientation="landscape" file="US20230005203A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="175.18mm" wi="127.25mm" file="US20230005203A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="211.41mm" wi="132.84mm" orientation="landscape" file="US20230005203A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="201.76mm" wi="122.68mm" orientation="landscape" file="US20230005203A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="197.10mm" wi="157.23mm" orientation="landscape" file="US20230005203A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0002" num="0001">The present disclosure relates to systems and techniques for animation generation. More specifically, this disclosure relates to machine learning techniques for dynamically generating animation of characters from motion capture video.</p><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">Electronic games are increasingly becoming more realistic due to an increase in available processing resources. This increase in realism may allow for more realistic gameplay experiences. For example, elements that form an in-game world, such as characters, may be more realistically presented. In this example, the elements may be increasingly rendered at higher resolutions, with more detailed textures, with more detailed underlying meshes, and so on. While this added realism may be beneficial to an end-user of an electronic game, it may place a substantial burden on electronic game developers. As an example, electronic game developers may be required to create very rich, and detailed, models of characters. As another example, electronic game designers may be required to create fluid, lifelike, movements of the characters.</p><p id="p-0004" num="0003">With respect to the example of movement, characters may be designed to realistically adjust their arms, legs, and so on, while traversing an in-game world. In this way, the characters may walk, run, jump, and so on, in a lifelike manner. With respect to a sports electronic game, substantial time may be spent ensuring that the characters appear to mimic real-world sports players. For example, electronic game designers may spend substantial time fine-tuning movements of an underlying character model. Movement of a character model may be, at least in part, implemented based on movement of an underlying skeleton. For example, a skeleton may include a multitude of objects (e.g., bones or joints) which may represent a portion of the character model. As an example, a first object may be a finger while a second object may correspond to a wrist. The skeleton may therefore represent an underlying form on which the character model is built. In this way, movement of the skeleton may cause a corresponding adjustment of the character model.</p><p id="p-0005" num="0004">To create realistic movement, an electronic game designer may therefore adjust positions of the above-described objects included in the skeleton. For example, the electronic game designer may create realistic running via adjustment of specific objects which form a character model's legs. This hand-tuned technique to enable movement of a character results in substantial complexity and usage of time.</p><heading id="h-0003" level="1">SUMMARY OF EMBODIMENTS</heading><p id="p-0006" num="0005">The systems, methods, and devices of this disclosure each have several innovative aspects, no single one of which is solely responsible for the all of the desirable attributes disclosed herein.</p><p id="p-0007" num="0006">Particular embodiments of the subject matter described in this specification can be implemented so as to realize one or more of the following advantages. Utilizing the techniques described herein, realistic motion may be rapidly generated for real life character models. For example, the realistic motion can be configured for use in electronic games. As will be described, a machine learning model may be trained based on motion capture information to generate local motion phase. Pose information can be determined based on video input. Subsequently, a window of frames can be used to predict the next frame and a predicted local motion phase for the next frame. The window of frames can be updated to include the next frame and the predicted local motion phase to be used to predict the following frame. Advantageously, the character animations can perform motions far smoother than traditional systems and the dynamic animation generation system can improve the quality of the animations when the initial prediction of pose has missing pose information in one or more frames of the real life video.</p><p id="p-0008" num="0007">One embodiment discloses a computer-implemented method for dynamically generating animation of characters from real life motion capture video, the method comprising: accessing motion capture video, the motion capture video including a motion capture actor in motion; inputting the motion capture video to a first neural network; receiving pose information of the motion capture actor for a plurality of frames in the motion capture video from the first neural network; identifying a first window of frames of the motion capture video, wherein the first window of frames comprises a current frame, one or more past frames to the current frame, and one or more future frames to the current frame; inputting the first window of frames of the motion capture video to a second neural network, wherein the second neural network predicts the next frame from the current frame; receiving as output of the second neural network a first predicted frame and a first local motion phase corresponding to the first predicted frame, wherein the first predicted frame comprises the predicted frame following the current frame; identifying a second window of frames, wherein the second window of frames comprises the generated first predicted frame; inputting the second window of frames and the first local motion phase to the second neural network; and receiving as output of the second neural network a second predicted frame and a second local motion phase corresponding to the second predicted frame, wherein the second predicted frame comprises the predicted frame following the first predicted frame.</p><p id="p-0009" num="0008">In some embodiments, the computer-implemented method further comprises overlaying the pose information on the motion capture video to generate a modified motion capture video, wherein identifying a first window of frames comprises a first window of frames of the modified motion capture video.</p><p id="p-0010" num="0009">In some embodiments, the motion capture video comprises a video of a real-life person in motion.</p><p id="p-0011" num="0010">In some embodiments, the pose information comprises velocity information corresponding to joints of the motion capture actor.</p><p id="p-0012" num="0011">In some embodiments, the second window of frames comprises one or more past frames to the first predicted frame, and one or more future frames to the first predicted frame.</p><p id="p-0013" num="0012">In some embodiments, the second window of frames drops the oldest frame from the first window of frames.</p><p id="p-0014" num="0013">In some embodiments, the first window of frames comprises sampled frames of the motion capture video at a predefined time threshold.</p><p id="p-0015" num="0014">In some embodiments, the first window of frames comprises a current frame, and the same number of past frames and future frames to the current frame, wherein the second window of frames drops the oldest frame from the first window of frames.</p><p id="p-0016" num="0015">In some embodiments, the motion capture video is captured from a camera on a user's mobile device.</p><p id="p-0017" num="0016">In some embodiments, the motion capture video comprises a video of a real-life sporting event.</p><p id="p-0018" num="0017">In some embodiments, the first local motion phase includes phase information for each joint of the motion capture actor.</p><p id="p-0019" num="0018">In some embodiments, the first local motion phase includes phase information for each bone of the motion capture actor.</p><p id="p-0020" num="0019">Some embodiments include a system comprising one or more processors and non-transitory computer storage media storing instructions that when executed by the one or more processors, cause the one or more processors to perform operations comprising: accessing motion capture video, the motion capture video including a motion capture actor in motion; inputting the motion capture video to a first neural network; receiving pose information of the motion capture actor for a plurality of frames in the motion capture video from the first neural network; identifying a first window of frames of the motion capture video, wherein the first window of frames comprises a current frame, one or more past frames to the current frame, and one or more future frames to the current frame; inputting the first window of frames of the motion capture video to a second neural network, wherein the second neural network predicts the next frame from the current frame; receiving as output of the second neural network a first predicted frame and a first local motion phase corresponding to the first predicted frame, wherein the first predicted frame comprises the predicted frame following the current frame; identifying a second window of frames, wherein the second window of frames comprises the generated first predicted frame; inputting the second window of frames and the first local motion phase to the second neural network; and receiving as output of the second neural network a second predicted frame and a second local motion phase corresponding to the second predicted frame, wherein the second predicted frame comprises the predicted frame following the first predicted frame.</p><p id="p-0021" num="0020">In some embodiments, the operations further comprise overlaying the pose information on the motion capture video to generate a modified motion capture video, wherein identifying a first window of frames comprises a first window of frames of the modified motion capture video.</p><p id="p-0022" num="0021">In some embodiments, the motion capture video comprises a video of a real-life person in motion.</p><p id="p-0023" num="0022">In some embodiments, the pose information comprises velocity information corresponding to joints of the motion capture actor.</p><p id="p-0024" num="0023">In some embodiments, the second neural network comprises an convolutional neural network.</p><p id="p-0025" num="0024">In some embodiments, the second neural network comprises an LSTM neural network.</p><p id="p-0026" num="0025">Some embodiments include a non-transitory computer storage media storing instructions that when executed by a system of one or more processors, cause the one or more processors to perform operations comprising: accessing motion capture video, the motion capture video including a motion capture actor in motion; inputting the motion capture video to a first neural network; receiving pose information of the motion capture actor for a plurality of frames in the motion capture video from the first neural network; identifying a first window of frames of the motion capture video, wherein the first window of frames comprises a current frame, one or more past frames to the current frame, and one or more future frames to the current frame; inputting the first window of frames of the motion capture video to a second neural network, wherein the second neural network predicts the next frame from the current frame; receiving as output of the second neural network a first predicted frame and a first local motion phase corresponding to the first predicted frame, wherein the first predicted frame comprises the predicted frame following the current frame; identifying a second window of frames, wherein the second window of frames comprises the generated first predicted frame; inputting the second window of frames and the first local motion phase to the second neural network; and receiving as output of the second neural network a second predicted frame and a second local motion phase corresponding to the second predicted frame, wherein the second predicted frame comprises the predicted frame following the first predicted frame.</p><p id="p-0027" num="0026">In some embodiments, the operations further comprise overlaying the pose information on the motion capture video to generate a modified motion capture video, wherein identifying a first window of frames comprises a first window of frames of the modified motion capture video.</p><p id="p-0028" num="0027">Although certain embodiments and examples are disclosed herein, inventive subject matter extends beyond the examples in the specifically disclosed embodiments to other alternative embodiments and/or uses, and to modifications and equivalents thereof.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0029" num="0028">Throughout the drawings, reference numbers are re-used to indicate correspondence between referenced elements. The drawings are provided to illustrate embodiments of the subject matter described herein and not to limit the scope thereof.</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIGS. <b>1</b>A-<b>1</b>H</figref> illustrate example animation generation by traditional systems according to some embodiments.</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIGS. <b>2</b>A-<b>2</b>H</figref> illustrate example animation generation by the dynamic animation generation system according to some embodiments.</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>3</b>A</figref> illustrates an embodiment of a sliding window of past pose data used to predict first pose data in the current frame, and a new sliding window that is adjusted by one frame to predict pose data for the next frame according to some embodiments.</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>3</b>B</figref> illustrates a flow diagram of the dynamic animation generation system applying the first and second neural network to generate predicted next frame data according to some embodiments.</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIGS. <b>4</b>A, <b>4</b>B, <b>4</b>C, and <b>4</b>D</figref> illustrate embodiments of differences with the local motion phase not in use, and with local motion phase in use according to some embodiments.</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIGS. <b>5</b>A, <b>5</b>B, <b>5</b>C, and <b>5</b>D</figref> illustrate additional embodiments of differences with the local motion phase not in use, and with local motion phase in use according to some embodiments.</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates an embodiment of computing device that may implement aspects of the present disclosure according to some embodiments.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><heading id="h-0006" level="2">Overview</heading><p id="p-0037" num="0036">This specification describes, among other things, technical improvements with respect to generation of motion for characters configured for use in electronic games. As will be described, a system described herein (e.g., the dynamic animation generation system) may implement a machine learning model to generate local phase information based on analyses of motion capture information. The dynamic animation generation system can then process a sliding window of frames to a second machine learning model to generate the next predicted frame and the local motion phase associated with the next predicted frame. The dynamic animation generation system can slide the window over in time to now include the next predicted frame while dropping the oldest frame, and the local motion phase associated with the next predicted frame, and apply the new window in the neural network to generate the frame following the next predicted frame and the local motion phase. Advantageously, the system may perform substantially automated analyses of the motion capture information such that complex machine learning labeling processes may be avoided. The dynamic animation generation system can combine local motion phase techniques with human motion reconstruction from captured real life video. While electronic games are described, it may be appreciated that the techniques described herein may be applied generally to movement of character models. For example, animated content (e.g., TV shows, movies) may employ the techniques described herein.</p><p id="p-0038" num="0037">Reconstruction of human motion from real life video is a promising technology for many businesses to generate animation content. When traditional systems reconstruct human motion using real life captured video, these traditional systems receive a real life video clip, extract human motion by determining what the person is doing and/or what kind of motion the human is performing, and determine location of joints of the human. While traditional systems have many advantages, such as a simple set-up, low cost, and applicability to different video resources, these traditional approaches have technical challenges, such as resulting in blurred or obstructed human video input causing discontinuities and an unsmoothed prediction of next frames. The resulting video can be very unrealistic and contain a lot of noise or disconnects in joints or even animation character.</p><p id="p-0039" num="0038">Moreover, traditional techniques that generate realistic motion for character models heavily rely upon designers adjusting character models to define different types of motion. For example, to define running, a designer may string together certain adjustments of joints on a skeleton of a character model. In this example, the designer may adjust the knees, cause a movement of the arms, and so on. While this may allow for motion to be generated, it may also involve a substantial burden on the designer.</p><p id="p-0040" num="0039">A first example technique in these traditional systems to, at least in part, automate generation of character motion, may include using software to automatically adjust a skeleton. For example, templates of running may be pre-defined. In this example, a designer may select a running template which may cause adjustment of the joints on a skeleton. Thus, the designer may more rapidly generate motion for characters in an in-game world. However, this first example technique may lack the realism of real-world movement. For example, since different templates are being selected, the lifelike differences in movement between real-world persons is lost. Moreover, the quality of the animation is limited to the quality of these templates. Furthermore, the prediction of frames in the human motion is also limited to the type of movement in these templates. For example, traditional systems that use templates may generate animations of running that are very similar to these templates.</p><p id="p-0041" num="0040">Motion may be defined, at least in part, based on distinct poses of an in-game character. As an example, each pose may represent a discrete sample of the motion to be performed by the in-game character. For this example, the pose may identify positions of bones or joints of the in-game character. Thus, if motion is to depict running, each pose may represent a snapshot of the running. For example, a first frame generated by an electronic game may include the in-game character with both feet on a surface within the game world. As another example, a second frame may include the in-game character beginning to move one of the feet upwards. It may be appreciated that subsequent frames may include the in-game character moving forward in a running motion.</p><p id="p-0042" num="0041">To generate motions for in-game characters, electronic game designers are increasingly leveraging motion capture studios. For example, a motion capture studio may be used to learn the realistic gait of an actor as he/she moves about the motion capture studio. Specific portions of the actor, such as joints or bones, may be monitored during this movement. Subsequently, movement of these portions may be extracted from image or video data of the actor. This movement may then be translated onto a skeleton or rig for use as an underlying framework of one or more in-game characters. The skeleton or rig may include bones, which may be adjusted based on the motion capture images or video. In this way, the skeleton or rig may be animated to reproduce motion performed by the actor.</p><p id="p-0043" num="0042">While motion capture studios allow for realistic motion, they are limited in the types of motion able to be reproduced. For example, the above-described skeleton may be animated to reproduced motions which were specifically performed by the actor. Other motions may thus need to be manually created by an electronic game designer. For example, and with respect to a sports electronic game, a real-life basketball player may be used as an actor to perform common basketball motions. While this actor may perform a wide breadth of motions typically performed during a basketball game, as may be appreciated there are other motions which will not be recorded. For example, these other motions may be produced naturally by the actor during a real-world basketball game depending on locations of opponents, the actor's current stamina level, a location of the actor with respect to the basketball court, and so on.</p><p id="p-0044" num="0043">In contrast, the techniques described herein allow for the rapid generation of character animation based on automated analyses of motion capture information. For example, an actor may be placed in a motion capture studio or the dynamic animation generation system may receive data on a real life soccer game. The actor may then perform different movements, and movement of different portions of the actor (e.g., joints) may be stored by a system. Additionally, contact with an external environment may be recorded. Thus, the specific foot fall pattern used by an upper echelon boxer or basketball player may be recorded. Additionally, the specific contact made by an actor's hands with respect to a basketball, football, and so on, may be recorded. This recorded information may be used to increase a realism associated with animation generation. In some embodiments, motion can be generated for biped and/or human characters. In some embodiments, motion can be generated for quadruped characters.</p><p id="p-0045" num="0044">In some embodiments, the dynamic animation generation system can improve on the quality of human reconstruction by combining human video input with local motion phase information. The dynamic animation generation system can first predict rough motion in real life video by applying real life capture data in a first model, such as a neural network, to receive the rough motion. The rough motion can include pose data, such as local motion phase. The machine learning model may be trained using local phase information extracted based on how individual body parts of a motion capture actor contacts external objects. This phase information may therefore represent local bone and/or joint phases corresponding to bones (e.g., arms, legs, hands) and/or joints (e.g., elbows, knees, knuckles) of the actor, and may be used to enhance the machine learning model's temporal alignment, and segmentation, of realistic motion.</p><p id="p-0046" num="0045">In some embodiments, by training a machine learning model to generate animation based on motion capture information, the model may allow for enhanced nuance associated with the animation. As an example, a real-life wrestler may be used as an actor. In this example, video of the wrestler may be recorded which depicts the wrestler moving about a ring, interacting with an opponent, performing different moves, and so on. The machine learning model may then be trained based on this video, such that the model can reproduce the highly stylized, and personal, movement of the wrestler.</p><p id="p-0047" num="0046">In some embodiments, in a second step, the dynamic animation generation system can process the output of the first model in a second model, such as an autoregression model, which can conditionally update the poses for the entire sequence going forward. The second step can include a model that applies a sliding window. The initial sliding window can include the rough motion data outputted by the first neural network. The model can apply the initial sliding window to predict a predicted pose and local phase information for the next window. Then the window can slide one frame forward which includes the predicted pose and local phase information, and the model can predict the next pose and local phase information.</p><p id="p-0048" num="0000">Example Traditional System Output with Output from the Dynamic Animation Generation System</p><p id="p-0049" num="0047"><figref idref="DRAWINGS">FIGS. <b>1</b>A-<b>1</b>H</figref> illustrate example animation generation by a first neural network to generate rough motions according to some embodiments. The rough motions can include pose data for each frame based on the video. The rough motion data can include a multidimensional signal that includes joint information, such as rotation information for each joint of the human. The rough motions can include calculations for each joint and for each frame in the video. The pose data is overlaid on top of the original video input to generate a modified video input.</p><p id="p-0050" num="0048"><figref idref="DRAWINGS">FIGS. <b>2</b>A-<b>2</b>H</figref> illustrate example animation generation using the modified video input as input to a second neural network that applies a sliding window according to some embodiments. <figref idref="DRAWINGS">FIGS. <b>1</b>A-<b>1</b>H</figref> and <figref idref="DRAWINGS">FIGS. <b>2</b>A-<b>2</b>H</figref> include a real life video capturing a human in motion (in dashed line) and a computer generated animation of the human (in solid line). The real life video can be captured by a studio, camera, or phone, or can be a stream of a real life event, such as a sporting event. The real life video can be a single view of a person performing a certain action or motion.</p><p id="p-0051" num="0049">As shown from <b>100</b> and <b>110</b> of <figref idref="DRAWINGS">FIGS. <b>1</b>A-<b>1</b>B and <b>200</b> and <b>210</b></figref> of <figref idref="DRAWINGS">FIGS. <b>2</b>A-<b>2</b>B</figref>, there is not much difference in the character animation <b>102</b>, <b>112</b>, <b>202</b>, <b>212</b> when the real life person is running in a straight line. One reason for this is that there is not much of an explosive change between the frames. Moreover, traditional systems can simply apply existing &#x201c;running&#x201d; templates and generate the character animation as shown in <figref idref="DRAWINGS">FIGS. <b>1</b>A-<b>1</b>B</figref>.</p><p id="p-0052" num="0050">As shown from <b>120</b> and <b>130</b> of <figref idref="DRAWINGS">FIGS. <b>1</b>C-<b>1</b>D</figref>, the character animation <b>122</b> is an unusual and unrealistic pose of a human running, and character animation <b>132</b> is non-existent. There are certain frames where there is no character animation because the system cannot determine pose data when there is a sharp motion, such as a character changing directions sharply. Moreover, the person in the real life video is moving very quickly and the image can sometimes get blurred, causing the system to fail to predict pose data for that frame. Because the first neural network can miss the connection between different frames, there are a few frames where pose data is completely missing. This is unlike <figref idref="DRAWINGS">FIGS. <b>1</b>A-<b>1</b>B</figref> where the human is moving slowly and the system can recover pose data for every frame. Thus, there is missing joint data for these frames. Although averaging or interpolation between frames can be used. However, such approaches cannot add new details to the recovered signals and the resulting animation loses certain details or result in a blurred or unsmoothed motion.</p><p id="p-0053" num="0051">In <b>220</b> and <b>230</b> of <figref idref="DRAWINGS">FIGS. <b>2</b>C-<b>2</b>D</figref>, the character animation <b>222</b> and <b>232</b> have been generated by the second neural network that use the sliding window technique. The neural network can process the sliding window by taking as input, pose data for the past frames and predict the current frame with local phase information. Then the system can move the sliding window by one frame, remove the oldest frame, and add the current frame to generate a new sliding window. The new sliding window can be passed to the neural network to generate the next frame with local phase information.</p><p id="p-0054" num="0052">Similar to <figref idref="DRAWINGS">FIGS. <b>1</b>D and <b>1</b>E</figref>, as shown in <b>140</b>, <b>150</b>, <b>160</b>, and <b>170</b> of <figref idref="DRAWINGS">FIGS. <b>1</b>E-<b>1</b>H</figref>, the character animations <b>142</b>, <b>162</b>, and <b>170</b> are unusual and unrealistic poses of a human running, and the character animation <b>152</b> is missing completely. However, in <b>240</b>, <b>250</b>, <b>260</b>, and <b>270</b> of <figref idref="DRAWINGS">FIGS. <b>2</b>E-<b>2</b>G</figref>, the character animations <b>242</b>, <b>252</b>, <b>262</b>, <b>272</b> have been generated by the second neural network using the sliding window technique and is able to generate a smooth pose of a human running. Moreover, the character animations <b>242</b>, <b>252</b>, <b>262</b>, <b>272</b>, as well as the other character animations in <figref idref="DRAWINGS">FIGS. <b>2</b>A-<b>2</b>D</figref> follow the real life person running very closely. Advantageously, the dynamic animation generation system can generate character animations from real life capture video producing a smoother resulting motion, and improves the quality of the generation when the initial prediction of pose data is missing.</p><heading id="h-0007" level="2">Sliding Window of Pose Data to Predict Next Pose Data</heading><p id="p-0055" num="0053"><figref idref="DRAWINGS">FIG. <b>3</b>A</figref> illustrates an embodiment <b>300</b> of a sliding window of past pose data used to predict first pose data in the current frame, and a new sliding window that is adjusted by one frame to predict pose data for the next frame according to some embodiments. <figref idref="DRAWINGS">FIG. <b>3</b>B</figref> illustrates a flow diagram <b>350</b> of the dynamic animation generation system applying the first and second neural network to generate predicted next frame data. For convenience, the flow diagram <b>350</b> will be described as being performed by a system of one or more computers (e.g., the dynamic animation generation system).</p><p id="p-0056" num="0054">At block <b>352</b>, the dynamic animation generation system can receive video input comprising a character in motion, such as a basketball player dribbling toward the hoop. The system accesses motion capture information. The information may be stored according to different motion capture formats, such as BVH and so on. Optionally, the motion capture information may represent image or video data taken at a particular frame rate. Thus, there may be 24, 30, 60, frames per second, which depict an actor moving about a motion capture studio. Optionally, the actor may have markers usable to track portions of the actor's body. Optionally, computer vision techniques may be used to identify specific feature of the actor's body (e.g., hands, arms, and so on). In some embodiments, an external object (e.g., a basketball) may have a marker on, or a sensor within, the object. Optionally, computer vision techniques may be used to analyze positions of the external object in the image or video data. While the description above described use of an actor, in some embodiments video may be obtained of real-world events. For example, video from a real-world sports game may be obtained and analyzed. In this example, a particular player may be analyzed identify specific portions of the player's body. Example portions may include the player's hands, feet, head, and so on.</p><p id="p-0057" num="0055">At block <b>354</b>, the dynamic animation generation system can input the video input to a first model, such as a first neural network. The first neural network can output pose information for each frame in the video input. The system may generate realistic motion using one or more deep-learning models. An example deep-learning model described herein includes a generative control model usable to inform generation of highly variable, and realistic, animations for characters. For example, the first model may be trained based on local bone and/or joint phases learned from motion capture information of real-world actors.</p><p id="p-0058" num="0056">Machine learning models may be used to enhance generation of motion based on motion capture information. For example, a machine learning model may analyze motion capture information. In this example, the machine learning model may then be used to generate animation for an in-game character which is based on the motion capture information, creating rough motion data. Since these machine learning models may directly output motion data for use in a second neural network (using a sliding window, as described further below) that can generate motion data for animating an in-game character automatically, they may substantially reduce development time of the electronic game. Additionally, since they are trained using motion capture information the output poses may appear lifelike.</p><p id="p-0059" num="0057">In some embodiments, the local motion phase includes phase information for each joint and/or bone of the character. In contrast, phase on a global level (e.g., one phase for the entire character) may not scale well when the character is moving asynchronously or when we want to combine movements for the animation. This is why the dynamic animation generation system can apply local motion phase which is determined on the local level by segmenting movements to joints, bones, limbs, ligaments, etc. Thus, the dynamic animation generation system inputs velocity data specific to joints, bones, limbs, ligaments, into a gaiting function to predict next pose data very granularly.</p><p id="p-0060" num="0058">At block <b>358</b>, the dynamic animation generation system can identify a sliding window to apply to a second model, such as a second neural network. The dynamic animation generation system can identify an initial window of frames that include a current frame, past frames, and future frames from the modified video input that includes the pose information outputted from the first model overlaid on top of the original real life video clip. For example, the dynamic animation generation system can take the real life video capture <b>302</b> of <figref idref="DRAWINGS">FIG. <b>3</b>A</figref> and generate a sliding window <b>304</b> that includes two past frames, a current frame, and two future frames.</p><p id="p-0061" num="0059">At block <b>360</b>, the dynamic animation generation system can apply the sliding window to the second model to generate a predicted next pose. For example, the sliding window <b>304</b> of <figref idref="DRAWINGS">FIG. <b>3</b>A</figref> can be applied to the second model, and a predicted next pose <b>306</b> can be received from the second model. Advantageously, the output of the first neural network can include frames that include very noisy potentially blurred images of the character, and/or missing pose data in certain frames. However, the second neural network is trained to generate next pose data based on the frames within the sliding window. The neural network can be trained to pose data in a predicted next frame even with incomplete or noisy input of frames within the sliding window.</p><p id="p-0062" num="0060">At block <b>362</b>, the dynamic animation generation system can receive a first predicted next pose <b>306</b>, and at block <b>364</b>, the dynamic animation generation system can identify a second window of frames that includes the first predicted next pose. For example, the dynamic animation generation system can receive the predicted next pose <b>306</b> of <figref idref="DRAWINGS">FIG. <b>3</b>A</figref> and a first local motion phase that is outputted by the first neural network. The dynamic animation generation system can then generate a new sliding window that <b>308</b> includes pose <b>310</b> (which is the same pose as the predicted next pose <b>306</b>), two past frames and two future frames. The phase information may be determined independently for each of the bones and/or joints. As will be described, phase information for a bone and/or joint may be determined based on contacts by the bone and/or joint with an external environment. For example, an actor's left hand contacting a ball, an opponent, a rim, a net, other portions of the actor, and so on, may be identified using video or images of the actor in the motion capture studio. Contacts with the left hand may be aggregated over a period of time and may be represented as a signal. The signal may, for example, indicate times at which contact occurred.</p><p id="p-0063" num="0061">At block <b>366</b>, the dynamic animation generation system can apply the second window of frames and the first local motion phase to the same second neural network as in block <b>360</b>. At block <b>368</b>, the dynamic animation generation system can receive a second predicted next frame and a second local motion phase from the second neural network. For example, the dynamic animation generation system applies the second window of frames <b>308</b> of <figref idref="DRAWINGS">FIG. <b>3</b>A</figref> to the second neural network to receive a second predicted next frame <b>312</b>. The dynamic animation generation system continues to repeat the sliding window to generate next predicted frames and local motion phases. The dynamic animation generation system can repeat the sliding window through the entire real life video clip, where for each frame, the local motion phase is predicted and outputted by the second neural network.</p><p id="p-0064" num="0062">In some embodiments, the second neural network is trained by applying training data that includes video clips with characters in motion. The output of the model can then be used to adjust the model, such as based on a comparison between the actual output of the model and the expected output of the model. For example, the trained data can include the precise local motion phase data. Then the model can be trained using the outputted motion and phase information.</p><p id="p-0065" num="0063">In some embodiments, the dynamic animation generation system can sample the real life video input to apply to the neural networks. The dynamic animation generation system can skip frames to generate the sliding window. For example, if the video includes a frame every 1/10th of a second, the dynamic animation generation system can take every second of video and apply the video to the first and second neural network, and drop the other 9 frames between each second.</p><p id="p-0066" num="0064">Advantageously since the dynamic animation generation system may directly generate character poses from real life video data, the dynamic animation generation system may allow for substantial storage savings with respect to character animations. For example, prior techniques to generate character animations have relied upon utilization of key-frames or animation clips. In this example, an electronic game may select a multitude of key-frames and interpolate them to generate animation for output to an end-user. These key-frames and animation clips may therefore have to be stored as information for use by the electronic game. This may increase a size associated with the electronic game, such as a download size, an installation size, and so on.</p><p id="p-0067" num="0065">In contrast, the techniques described herein may allow for generation of animation based on use of one or more machine learning models. As may be appreciated, these machine learning models may be represented as weights, biases, and so on, which may be of a substantially smaller size. In this way, an electronic game may have a reduced size, reduced download time, reduced installation time, and so on, as compared to other electronic games.</p><heading id="h-0008" level="2">Sliding Window of Pose Data to Predict Next Pose Data</heading><p id="p-0068" num="0066"><figref idref="DRAWINGS">FIGS. <b>4</b>A, <b>4</b>B, <b>4</b>C, and <b>4</b>D</figref> illustrate embodiments <b>400</b>, <b>420</b>, <b>440</b>, <b>460</b> of differences with the local motion phase not in use <b>402</b>, <b>422</b>, <b>442</b>, <b>462</b>, and with local motion phase in use <b>404</b>, <b>424</b>, <b>444</b>, <b>464</b> according to some embodiments. <figref idref="DRAWINGS">FIGS. <b>5</b>A, <b>5</b>B, <b>5</b>C, and <b>5</b>D</figref> illustrate additional embodiments <b>500</b>, <b>520</b>, <b>540</b>, <b>560</b> of differences with the local motion phase not in use <b>502</b>, <b>522</b>, <b>542</b>, <b>562</b>, and with local motion phase in use <b>504</b>, <b>524</b>, <b>544</b>, <b>564</b> according to some embodiments. Similar to <figref idref="DRAWINGS">FIGS. <b>5</b>A, <b>5</b>B, <b>5</b>C, and <b>5</b>D</figref>, the character animation of <b>402</b>, <b>422</b>, <b>442</b>, and <b>462</b> in <figref idref="DRAWINGS">FIGS. <b>4</b>A, <b>4</b>B, <b>4</b>C, and <b>4</b>D</figref>, without using local motion phase, can be blurred and result in unrealistic motion. The motion from <b>402</b> to <b>422</b> is unnatural, and the pose in <b>422</b> is unrealistic. Moreover, the character animation from <b>402</b>, <b>422</b>, <b>442</b>, and <b>462</b> may reach a similar final gesture but cannot find the movement timing accurately. In contrast, the character animation <b>404</b>, <b>424</b>, <b>444</b>, <b>464</b> is far more synchronized and the pose is appropriate for the timing of the frames. Thus, the use of local motion phase results in a better predictor of forward timing for specific posses.</p><heading id="h-0009" level="2">Overview of Computing Device</heading><p id="p-0069" num="0067"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates an embodiment of computing device <b>10</b> according to some embodiments. Other variations of the computing device <b>10</b> may be substituted for the examples explicitly presented herein, such as removing or adding components to the computing device <b>10</b>. The computing device <b>10</b> may include a game device, a smart phone, a tablet, a personal computer, a laptop, a smart television, a car console display, a server, and the like. As shown, the computing device <b>10</b> includes a processing unit <b>20</b> that interacts with other components of the computing device <b>10</b> and also external components to computing device <b>10</b>. A media reader <b>22</b> is included that communicates with media <b>12</b>. The media reader <b>22</b> may be an optical disc reader capable of reading optical discs, such as CD-ROM or DVDs, or any other type of reader that can receive and read data from game media <b>12</b>. One or more of the computing devices may be used to implement one or more of the systems disclosed herein.</p><p id="p-0070" num="0068">Computing device <b>10</b> may include a separate graphics processor <b>24</b>. In some cases, the graphics processor <b>24</b> may be built into the processing unit <b>20</b>. In some such cases, the graphics processor <b>24</b> may share Random Access Memory (RAM) with the processing unit <b>20</b>. Alternatively, or in addition, the computing device <b>10</b> may include a discrete graphics processor <b>24</b> that is separate from the processing unit <b>20</b>. In some such cases, the graphics processor <b>24</b> may have separate RAM from the processing unit <b>20</b>. Computing device <b>10</b> might be a handheld video game device, a dedicated game console computing system, a general-purpose laptop or desktop computer, a smart phone, a tablet, a car console, or other suitable system.</p><p id="p-0071" num="0069">Computing device <b>10</b> also includes various components for enabling input/output, such as an I/O <b>32</b>, a user I/O <b>34</b>, a display I/O <b>36</b>, and a network I/O <b>38</b>. I/O <b>32</b> interacts with storage element <b>40</b> and, through a device <b>42</b>, removable storage media <b>44</b> in order to provide storage for computing device <b>10</b>. Processing unit <b>20</b> can communicate through I/O <b>32</b> to store data, such as game state data and any shared data files. In addition to storage <b>40</b> and removable storage media <b>44</b>, computing device <b>10</b> is also shown including ROM (Read-Only Memory) <b>46</b> and RAM <b>48</b>. RAM <b>48</b> may be used for data that is accessed frequently, such as when a game is being played.</p><p id="p-0072" num="0070">User I/O <b>34</b> is used to send and receive commands between processing unit <b>20</b> and user devices, such as game controllers. In some embodiments, the user I/O can include a touchscreen inputs. The touchscreen can be capacitive touchscreen, a resistive touchscreen, or other type of touchscreen technology that is configured to receive user input through tactile inputs from the user. Display I/O <b>36</b> provides input/output functions that are used to display images from the game being played. Network I/O <b>38</b> is used for input/output functions for a network. Network I/O <b>38</b> may be used during execution of a game, such as when a game is being played online or being accessed online.</p><p id="p-0073" num="0071">Display output signals produced by display I/O <b>36</b> comprising signals for displaying visual content produced by computing device <b>10</b> on a display device, such as graphics, user interfaces, video, and/or other visual content. Computing device <b>10</b> may comprise one or more integrated displays configured to receive display output signals produced by display I/O <b>36</b>. According to some embodiments, display output signals produced by display I/O <b>36</b> may also be output to one or more display devices external to computing device <b>10</b>, such a display <b>16</b>.</p><p id="p-0074" num="0072">The computing device <b>10</b> can also include other features that may be used with a game, such as a clock <b>50</b>, flash memory <b>52</b>, and other components. An audio/video player <b>56</b> might also be used to play a video sequence, such as a movie. It should be understood that other components may be provided in computing device <b>10</b> and that a person skilled in the art will appreciate other variations of computing device <b>10</b>. The computing device <b>10</b> can include one or more components for the interactive computing system <b>160</b>, and/or a player computing system <b>152</b>A, <b>152</b>B. In some embodiments, the interactive computing system <b>160</b>, and/or a player computing system <b>152</b>A, <b>152</b>B can include one or more components of the computing device <b>10</b>.</p><p id="p-0075" num="0073">Program code can be stored in ROM <b>46</b>, RAM <b>48</b> or storage <b>40</b> (which might comprise hard disk, other magnetic storage, optical storage, other non-volatile storage or a combination or variation of these). Part of the program code can be stored in ROM that is programmable (ROM, PROM, EPROM, EEPROM, and so forth), part of the program code can be stored in storage <b>40</b>, and/or on removable media such as game media <b>12</b> (which can be a CD-ROM, cartridge, memory chip or the like, or obtained over a network or other electronic channel as needed). In general, program code can be found embodied in a tangible non-transitory signal-bearing medium.</p><p id="p-0076" num="0074">Random access memory (RAM) <b>48</b> (and possibly other storage) is usable to store variables and other game and processor data as needed. RAM is used and holds data that is generated during the execution of an application and portions thereof might also be reserved for frame buffers, application state information, and/or other data needed or usable for interpreting user input and generating display outputs. Generally, RAM <b>48</b> is volatile storage and data stored within RAM <b>48</b> may be lost when the computing device <b>10</b> is turned off or loses power.</p><p id="p-0077" num="0075">As computing device <b>10</b> reads media <b>12</b> and provides an application, information may be read from game media <b>12</b> and stored in a memory device, such as RAM <b>48</b>. Additionally, data from storage <b>40</b>, ROM <b>46</b>, servers accessed via a network (not shown), or removable storage media <b>46</b> may be read and loaded into RAM <b>48</b>. Although data is described as being found in RAM <b>48</b>, it will be understood that data does not have to be stored in RAM <b>48</b> and may be stored in other memory accessible to processing unit <b>20</b> or distributed among several media, such as media <b>12</b> and storage <b>40</b>.</p><p id="p-0078" num="0076">It is to be understood that not necessarily all objects or advantages may be achieved in accordance with any particular embodiment described herein. Thus, for example, those skilled in the art will recognize that certain embodiments may be configured to operate in a manner that achieves or optimizes one advantage or group of advantages as taught herein without necessarily achieving other objects or advantages as may be taught or suggested herein.</p><p id="p-0079" num="0077">All of the processes described herein may be embodied in, and fully automated via, software code modules executed by a computing system that includes one or more computers or processors. The code modules may be stored in any type of non-transitory computer-readable medium or other computer storage device. Some or all the methods may be embodied in specialized computer hardware.</p><p id="p-0080" num="0078">Many other variations than those described herein will be apparent from this disclosure. For example, depending on the embodiment, certain acts, events, or functions of any of the algorithms described herein can be performed in a different sequence, can be added, merged, or left out altogether (for example, not all described acts or events are necessary for the practice of the algorithms). Moreover, in certain embodiments, acts or events can be performed concurrently, for example, through multi-threaded processing, interrupt processing, or multiple processors or processor cores or on other parallel architectures, rather than sequentially. In addition, different tasks or processes can be performed by different machines and/or computing systems that can function together.</p><p id="p-0081" num="0079">The various illustrative logical blocks and modules described in connection with the embodiments disclosed herein can be implemented or performed by a machine, such as a processing unit or processor, a digital signal processor (DSP), an application specific integrated circuit (ASIC), a field programmable gate array (FPGA) or other programmable logic device, discrete gate or transistor logic, discrete hardware components, or any combination thereof designed to perform the functions described herein. A processor can be a microprocessor, but in the alternative, the processor can be a controller, microcontroller, or state machine, combinations of the same, or the like. A processor can include electrical circuitry configured to process computer-executable instructions. In another embodiment, a processor includes an FPGA or other programmable device that performs logic operations without processing computer-executable instructions. A processor can also be implemented as a combination of computing devices, for example, a combination of a DSP and a microprocessor, a plurality of microprocessors, one or more microprocessors in conjunction with a DSP core, or any other such configuration. Although described herein primarily with respect to digital technology, a processor may also include primarily analog components. For example, some or all of the signal processing algorithms described herein may be implemented in analog circuitry or mixed analog and digital circuitry. A computing environment can include any type of computer system, including, but not limited to, a computer system based on a microprocessor, a mainframe computer, a digital signal processor, a portable computing device, a device controller, or a computational engine within an appliance, to name a few.</p><p id="p-0082" num="0080">Conditional language such as, among others, &#x201c;can,&#x201d; &#x201c;could,&#x201d; &#x201c;might&#x201d; or &#x201c;may,&#x201d; unless specifically stated otherwise, are otherwise understood within the context as used in general to convey that certain embodiments include, while other embodiments do not include, certain features, elements and/or steps. Thus, such conditional language is not generally intended to imply that features, elements and/or steps are in any way required for one or more embodiments or that one or more embodiments necessarily include logic for deciding, with or without user input or prompting, whether these features, elements and/or steps are included or are to be performed in any particular embodiment.</p><p id="p-0083" num="0081">Disjunctive language such as the phrase &#x201c;at least one of X, Y, or Z,&#x201d; unless specifically stated otherwise, is otherwise understood with the context as used in general to present that an item, term, etc., may be either X, Y, or Z, or any combination thereof (for example, X, Y, and/or Z). Thus, such disjunctive language is not generally intended to, and should not, imply that certain embodiments require at least one of X, at least one of Y, or at least one of Z to each be present.</p><p id="p-0084" num="0082">Any process descriptions, elements or blocks in the flow diagrams described herein and/or depicted in the attached figures should be understood as potentially representing modules, segments, or portions of code which include one or more executable instructions for implementing specific logical functions or elements in the process. Alternate implementations are included within the scope of the embodiments described herein in which elements or functions may be deleted, executed out of order from that shown, or discussed, including substantially concurrently or in reverse order, depending on the functionality involved as would be understood by those skilled in the art.</p><p id="p-0085" num="0083">Unless otherwise explicitly stated, articles such as &#x201c;a&#x201d; or &#x201c;an&#x201d; should generally be interpreted to include one or more described items. Accordingly, phrases such as &#x201c;a device configured to&#x201d; are intended to include one or more recited devices. Such one or more recited devices can also be collectively configured to carry out the stated recitations. For example, &#x201c;a processor configured to carry out recitations A, B and C&#x201d; can include a first processor configured to carry out recitation A working in conjunction with a second processor configured to carry out recitations B and C.</p><p id="p-0086" num="0084">It should be emphasized that many variations and modifications may be made to the above-described embodiments, the elements of which are to be understood as being among other acceptable examples. All such modifications and variations are intended to be included herein within the scope of this disclosure.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A computer-implemented method for dynamically generating animation of characters from real life motion capture video, the method comprising:<claim-text>accessing motion capture video, the motion capture video including a motion capture actor in motion;</claim-text><claim-text>inputting the motion capture video to a first neural network;</claim-text><claim-text>receiving pose information of the motion capture actor for a plurality of frames in the motion capture video from the first neural network;</claim-text><claim-text>identifying a first window of frames of the motion capture video, wherein the first window of frames comprises a current frame, one or more past frames to the current frame, and one or more future frames to the current frame;</claim-text><claim-text>inputting the first window of frames of the motion capture video to a second neural network, wherein the second neural network predicts the next frame from the current frame;</claim-text><claim-text>receiving as output of the second neural network a first predicted frame and a first local motion phase corresponding to the first predicted frame, wherein the first predicted frame comprises the predicted frame following the current frame;</claim-text><claim-text>identifying a second window of frames, wherein the second window of frames comprises the generated first predicted frame;</claim-text><claim-text>inputting the second window of frames and the first local motion phase to the second neural network; and</claim-text><claim-text>receiving as output of the second neural network a second predicted frame and a second local motion phase corresponding to the second predicted frame, wherein the second predicted frame comprises the predicted frame following the first predicted frame.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the computer-implemented method further comprises overlaying the pose information on the motion capture video to generate a modified motion capture video, wherein identifying a first window of frames comprises a first window of frames of the modified motion capture video.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the motion capture video comprises a video of a real-life person in motion.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the pose information comprises velocity information corresponding to joints of the motion capture actor.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the second window of frames comprises one or more past frames to the first predicted frame, and one or more future frames to the first predicted frame.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the second window of frames drops the oldest frame from the first window of frames.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first window of frames comprises sampled frames of the motion capture video at a predefined time threshold.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first window of frames comprises a current frame, and the same number of past frames and future frames to the current frame, wherein the second window of frames drops the oldest frame from the first window of frames.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the motion capture video is captured from a camera on a user's mobile device.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the motion capture video comprises a video of a real-life sporting event.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first local motion phase includes phase information for each joint of the motion capture actor.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first local motion phase includes phase information for each bone of the motion capture actor.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. A system comprising one or more processors and non-transitory computer storage media storing instructions that when executed by the one or more processors, cause the one or more processors to perform operations comprising:<claim-text>accessing motion capture video, the motion capture video including a motion capture actor in motion;</claim-text><claim-text>inputting the motion capture video to a first neural network;</claim-text><claim-text>receiving pose information of the motion capture actor for a plurality of frames in the motion capture video from the first neural network;</claim-text><claim-text>identifying a first window of frames of the motion capture video, wherein the first window of frames comprises a current frame, one or more past frames to the current frame, and one or more future frames to the current frame;</claim-text><claim-text>inputting the first window of frames of the motion capture video to a second neural network, wherein the second neural network predicts the next frame from the current frame;</claim-text><claim-text>receiving as output of the second neural network a first predicted frame and a first local motion phase corresponding to the first predicted frame, wherein the first predicted frame comprises the predicted frame following the current frame;</claim-text><claim-text>identifying a second window of frames, wherein the second window of frames comprises the generated first predicted frame;</claim-text><claim-text>inputting the second window of frames and the first local motion phase to the second neural network; and</claim-text><claim-text>receiving as output of the second neural network a second predicted frame and a second local motion phase corresponding to the second predicted frame, wherein the second predicted frame comprises the predicted frame following the first predicted frame.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The system of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the operations further comprise overlaying the pose information on the motion capture video to generate a modified motion capture video, wherein identifying a first window of frames comprises a first window of frames of the modified motion capture video.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The system of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the motion capture video comprises a video of a real-life person in motion.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The system of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the pose information comprises velocity information corresponding to joints of the motion capture actor.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The system of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the second neural network comprises an convolutional neural network.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The system of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the second neural network comprises an LSTM neural network.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. A non-transitory computer storage media storing instructions that when executed by a system of one or more processors, cause the one or more processors to perform operations comprising:<claim-text>accessing motion capture video, the motion capture video including a motion capture actor in motion;</claim-text><claim-text>inputting the motion capture video to a first neural network;</claim-text><claim-text>receiving pose information of the motion capture actor for a plurality of frames in the motion capture video from the first neural network;</claim-text><claim-text>identifying a first window of frames of the motion capture video, wherein the first window of frames comprises a current frame, one or more past frames to the current frame, and one or more future frames to the current frame;</claim-text><claim-text>inputting the first window of frames of the motion capture video to a second neural network, wherein the second neural network predicts the next frame from the current frame;</claim-text><claim-text>receiving as output of the second neural network a first predicted frame and a first local motion phase corresponding to the first predicted frame, wherein the first predicted frame comprises the predicted frame following the current frame;</claim-text><claim-text>identifying a second window of frames, wherein the second window of frames comprises the generated first predicted frame;</claim-text><claim-text>inputting the second window of frames and the first local motion phase to the second neural network; and</claim-text><claim-text>receiving as output of the second neural network a second predicted frame and a second local motion phase corresponding to the second predicted frame, wherein the second predicted frame comprises the predicted frame following the first predicted frame.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The non-transitory computer storage media of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein the operations further comprise overlaying the pose information on the motion capture video to generate a modified motion capture video, wherein identifying a first window of frames comprises a first window of frames of the modified motion capture video.</claim-text></claim></claims></us-patent-application>