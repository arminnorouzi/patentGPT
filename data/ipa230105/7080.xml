<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230007081A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230007081</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17944328</doc-number><date>20220914</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20220101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>L</subclass><main-group>67</main-group><subgroup>1097</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20180101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>30</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>48</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20220101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>L</subclass><main-group>67</main-group><subgroup>01</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20220101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>L</subclass><main-group>67</main-group><subgroup>561</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>L</subclass><main-group>67</main-group><subgroup>1097</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>30076</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>4806</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220501</date></cpc-version-indicator><section>H</section><class>04</class><subclass>L</subclass><main-group>67</main-group><subgroup>01</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220501</date></cpc-version-indicator><section>H</section><class>04</class><subclass>L</subclass><main-group>67</main-group><subgroup>561</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">Data Object Delivery for Distributed Cluster Computing</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>17155694</doc-number><date>20210122</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11477281</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17944328</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>PCT/CN2020/135584</doc-number><date>20201211</date></document-id><parent-status>PENDING</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>17155694</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Citrix Systems, Inc.</orgname><address><city>Fort Lauderdale</city><state>FL</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Cheng</last-name><first-name>Zhuzhang</first-name><address><city>Santa Clara</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Wei</last-name><first-name>Jungang</first-name><address><city>Santa Clara</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Wang</last-name><first-name>Pei</first-name><address><city>Nanjing</city><country>CN</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Methods and systems for delivering data for cluster computing are described herein. A worker device may receive a dataset and store the dataset in a local storage media. This may prevent the need for the dataset to be sent over a network each time the applications are used to perform a task. Each application may be able to access the dataset in the local storage area. This may prevent the need to copy the dataset to memory associated with each application. A worker device may store a dataset, for example, if it determines that the frequency of updates to the dataset satisfy a threshold. The worker device may receive updates to the dataset via a messaging system and may store the updated data in the local storage media.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="180.34mm" wi="115.57mm" file="US20230007081A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="211.33mm" wi="117.60mm" file="US20230007081A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="220.30mm" wi="167.64mm" file="US20230007081A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="226.91mm" wi="168.06mm" orientation="landscape" file="US20230007081A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="215.05mm" wi="172.13mm" orientation="landscape" file="US20230007081A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="232.66mm" wi="160.95mm" orientation="landscape" file="US20230007081A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="230.89mm" wi="144.36mm" file="US20230007081A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="240.62mm" wi="145.71mm" file="US20230007081A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="216.41mm" wi="67.06mm" file="US20230007081A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION</heading><p id="p-0002" num="0001">This application is a continuation of U.S. patent application Ser. No. 17/155,694, filed on Jan. 1, 2021, and entitled &#x201c;Data Object Delivery For Distributed Cluster Computing,&#x201d; which is a continuation of PCT Application No. PCT/CN2020/135584, filed on Dec. 11, 2020, and entitled &#x201c;Data Object Delivery For Distributed Cluster Computing&#x201d;. The above-mentioned applications are incorporated herein by reference in their entireties and for all purposes.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">FIELD</heading><p id="p-0003" num="0002">Aspects described herein generally relate to computer networking, remote computer access, virtualization, and hardware and software related thereto. More specifically, one or more aspects described herein provide data for use in cluster computing.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">Big data applications may run on a cluster of servers and may handle large amounts of data. Distributed cluster-computing frameworks may use a cluster comprising a plurality of computing devices to perform a task. The cluster may comprise a master device and one or more worker devices that communicate with each other via a network. The master device may store a dataset. To perform a task, the master device may send the dataset to each worker device in the cluster.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0005" num="0004">The following presents a simplified summary of various aspects described herein. This summary is not an extensive overview, and is not intended to identify required or critical elements or to delineate the scope of the claims. The following summary merely presents some concepts in a simplified form as an introductory prelude to the more detailed description provided below.</p><p id="p-0006" num="0005">To perform a task using a cluster, a dataset may be sent to one or more worker devices in the cluster. A worker device may connect to remote data stores or databases to retrieve the dataset and may copy the dataset across multiple applications executing on the worker device. When computation is done or an application ceases execution, the data may be discarded. If a dataset is large, the time used for broadcasting the dataset, memory used to store the dataset, and/or the volume of data transferred over network, by every application on the servers may be significant. Furthermore, the memory used by each application may be private to each application. As a result the overall memory usage may be multiplied by the number of applications that need the dataset. If a database or another remote service is used, the application's performance may be decreased due to delays (e.g., latency) incurred from sending requests and receiving responses to the database.</p><p id="p-0007" num="0006">According to aspects described herein, a worker device may receive a dataset and store the dataset in a local storage media. This may prevent the need for the dataset to be sent over a network each time the applications are used to perform a task. Each application may be able to access the dataset in the local storage area. This may prevent the need to copy the dataset to memory associated with each application. A worker device may store a dataset, for example, if it determines that the frequency of updates to the dataset satisfy a threshold. The worker device may receive updates to the dataset via a messaging system and may store the updated data in the local storage media. The worker device may receive no-op messages periodically. A no-op message may indicate that there are no updates to the dataset.</p><p id="p-0008" num="0007">In one aspect, a computer implemented method may include subscribing, by a worker device of a cluster, to a dataset; receiving, via a first process executing on the worker device and from a master device of the cluster, the dataset; storing the dataset on a local storage media of the worker device; executing, via one or more processes different from the first process, a plurality of applications to perform a task on the dataset, wherein each application of the plurality of applications is configured to share, with each other application of the plurality of applications, the dataset from the local storage media; receiving, based on the subscribing and from the master device, a data update message indicating a change to the dataset; and updating, based on the data update message, the dataset on the local storage media.</p><p id="p-0009" num="0008">The method may further include ceasing, based on a determination that a no-op message has not been received within a threshold time period, execution of the plurality of applications. The no-op message may indicate that there have been no updates to the dataset. The storing the dataset may comprise determining, based on metadata associated with the dataset, that a quantity of updates to the dataset within a time period satisfy a threshold; and based on the determining that the quantity of updates satisfies the threshold, storing the dataset in the local storage media. The storing the dataset may comprise determining, based on metadata associated with the dataset, that a size of the dataset satisfies a threshold; and based on the determining that the size of the dataset satisfies the threshold, storing the dataset in the local storage media.</p><p id="p-0010" num="0009">The method may further include receiving, via the first process and during execution of the plurality of applications, a message comprising an update to the dataset; and updating, via the first process, the dataset in the local storage media. The local storage media may include a solid-state drive. The executing the plurality of applications may include joining the dataset from the local storage media with a second dataset received from the master device.</p><p id="p-0011" num="0010">In some aspects, a system may be configured to perform one or more aspects and/or methods described herein. In some aspects, an apparatus may be configured to perform one or more aspects and/or methods described herein. In some aspects, one or more computer readable media may store computer executed instructions that, when executed, configure a system to perform one or more aspects and/or methods described herein. These and additional aspects will be appreciated with the benefit of the disclosures discussed in further detail below.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0012" num="0011">A more complete understanding of aspects described herein and the advantages thereof may be acquired by referring to the following description in consideration of the accompanying drawings, in which like reference numbers indicate like features, and wherein:</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>1</b></figref> depicts an illustrative computer system architecture that may be used in accordance with one or more illustrative aspects described herein.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>2</b></figref> depicts an illustrative remote-access system architecture that may be used in accordance with one or more illustrative aspects described herein.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>3</b></figref> depicts an illustrative virtualized system architecture that may be used in accordance with one or more illustrative aspects described herein.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>4</b></figref> depicts an illustrative cloud-based system architecture that may be used in accordance with one or more illustrative aspects described herein.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>5</b></figref> depicts an illustrative cluster computing system that may be used in accordance with one or more illustrative aspects described herein.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>6</b></figref> depicts an illustrative dataset subscription user interface that may be used in accordance with one or more illustrative aspects described herein.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>7</b></figref> depicts an illustrative method for providing data to a distributed cluster that may be used in accordance with one or more illustrative aspects described herein.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>8</b></figref> depicts an illustrative method for providing data to a distributed cluster that may be used in accordance with one or more illustrative aspects described herein.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0021" num="0020">In the following description of the various embodiments, reference is made to the accompanying drawings identified above and which form a part hereof, and in which is shown by way of illustration various embodiments in which aspects described herein may be practiced. It is to be understood that other embodiments may be utilized and structural and functional modifications may be made without departing from the scope described herein. Various aspects are capable of other embodiments and of being practiced or being carried out in various different ways.</p><p id="p-0022" num="0021">It is to be understood that the phraseology and terminology used herein are for the purpose of description and should not be regarded as limiting. Rather, the phrases and terms used herein are to be given their broadest interpretation and meaning. The use of &#x201c;including&#x201d; and &#x201c;comprising&#x201d; and variations thereof is meant to encompass the items listed thereafter and equivalents thereof as well as additional items and equivalents thereof. The use of the terms &#x201c;mounted,&#x201d; &#x201c;connected,&#x201d; &#x201c;coupled,&#x201d; &#x201c;positioned,&#x201d; &#x201c;engaged&#x201d; and similar terms, is meant to include both direct and indirect mounting, connecting, coupling, positioning and engaging.</p><p id="p-0023" num="0022">Computing Architecture</p><p id="p-0024" num="0023">Computer software, hardware, and networks may be utilized in a variety of different system environments, including standalone, networked, remote-access (also known as remote desktop), virtualized, and/or cloud-based environments, among others. <figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates one example of a system architecture and data processing device that may be used to implement one or more illustrative aspects described herein in a standalone and/or networked environment. Various network nodes <b>103</b>, <b>105</b>, <b>107</b>, and <b>109</b> may be interconnected via a wide area network (WAN) <b>101</b>, such as the Internet. Other networks may also or alternatively be used, including private intranets, corporate networks, local area networks (LAN), metropolitan area networks (MAN), wireless networks, personal networks (PAN), and the like. Network <b>101</b> is for illustration purposes and may be replaced with fewer or additional computer networks. A local area network <b>133</b> may have one or more of any known LAN topology and may use one or more of a variety of different protocols, such as Ethernet. Devices <b>103</b>, <b>105</b>, <b>107</b>, and <b>109</b> and other devices (not shown) may be connected to one or more of the networks via twisted pair wires, coaxial cable, fiber optics, radio waves, or other communication media.</p><p id="p-0025" num="0024">The term &#x201c;network&#x201d; as used herein and depicted in the drawings refers not only to systems in which remote storage devices are coupled together via one or more communication paths, but also to stand-alone devices that may be coupled, from time to time, to such systems that have storage capability. Consequently, the term &#x201c;network&#x201d; includes not only a &#x201c;physical network&#x201d; but also a &#x201c;content network,&#x201d; which is comprised of the data&#x2014;attributable to a single entity&#x2014;which resides across all physical networks.</p><p id="p-0026" num="0025">The components may include data server <b>103</b>, web server <b>105</b>, and client computers <b>107</b>, <b>109</b>. Data server <b>103</b> provides overall access, control and administration of databases and control software for performing one or more illustrative aspects describe herein. Data server <b>103</b> may be connected to web server <b>105</b> through which users interact with and obtain data as requested. Alternatively, data server <b>103</b> may act as a web server itself and be directly connected to the Internet. Data server <b>103</b> may be connected to web server <b>105</b> through the local area network <b>133</b>, the wide area network <b>101</b> (e.g., the Internet), via direct or indirect connection, or via some other network. Users may interact with the data server <b>103</b> using remote computers <b>107</b>, <b>109</b>, e.g., using a web browser to connect to the data server <b>103</b> via one or more externally exposed web sites hosted by web server <b>105</b>. Client computers <b>107</b>, <b>109</b> may be used in concert with data server <b>103</b> to access data stored therein, or may be used for other purposes. For example, from client device <b>107</b> a user may access web server <b>105</b> using an Internet browser, as is known in the art, or by executing a software application that communicates with web server <b>105</b> and/or data server <b>103</b> over a computer network (such as the Internet).</p><p id="p-0027" num="0026">Servers and applications may be combined on the same physical machines, and retain separate virtual or logical addresses, or may reside on separate physical machines. <figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates just one example of a network architecture that may be used, and those of skill in the art will appreciate that the specific network architecture and data processing devices used may vary, and are secondary to the functionality that they provide, as further described herein. For example, services provided by web server <b>105</b> and data server <b>103</b> may be combined on a single server.</p><p id="p-0028" num="0027">Each component <b>103</b>, <b>105</b>, <b>107</b>, <b>109</b> may be any type of known computer, server, or data processing device. Data server <b>103</b>, e.g., may include a processor <b>111</b> controlling overall operation of the data server <b>103</b>. Data server <b>103</b> may further include random access memory (RAM) <b>113</b>, read only memory (ROM) <b>115</b>, network interface <b>117</b>, input/output interfaces <b>119</b> (e.g., keyboard, mouse, display, printer, etc.), and memory <b>121</b>. Input/output (I/O) <b>119</b> may include a variety of interface units and drives for reading, writing, displaying, and/or printing data or files. Memory <b>121</b> may further store operating system software <b>123</b> for controlling overall operation of the data processing device <b>103</b>, control logic <b>125</b> for instructing data server <b>103</b> to perform aspects described herein, and other application software <b>127</b> providing secondary, support, and/or other functionality which may or might not be used in conjunction with aspects described herein. The control logic <b>125</b> may also be referred to herein as the data server software <b>125</b>. Functionality of the data server software <b>125</b> may refer to operations or decisions made automatically based on rules coded into the control logic <b>125</b>, made manually by a user providing input into the system, and/or a combination of automatic processing based on user input (e.g., queries, data updates, etc.).</p><p id="p-0029" num="0028">Memory <b>121</b> may also store data used in performance of one or more aspects described herein, including a first database <b>129</b> and a second database <b>131</b>. In some embodiments, the first database <b>129</b> may include the second database <b>131</b> (e.g., as a separate table, report, etc.). That is, the information can be stored in a single database, or separated into different logical, virtual, or physical databases, depending on system design. Devices <b>105</b>, <b>107</b>, and <b>109</b> may have similar or different architecture as described with respect to device <b>103</b>. Those of skill in the art will appreciate that the functionality of data processing device <b>103</b> (or device <b>105</b>, <b>107</b>, or <b>109</b>) as described herein may be spread across multiple data processing devices, for example, to distribute processing load across multiple computers, to segregate transactions based on geographic location, user access level, quality of service (QoS), etc.</p><p id="p-0030" num="0029">One or more aspects may be embodied in computer-usable or readable data and/or computer-executable instructions, such as in one or more program modules, executed by one or more computers or other devices as described herein. Generally, program modules include routines, programs, objects, components, data structures, etc. that perform particular tasks or implement particular abstract data types when executed by a processor in a computer or other device. The modules may be written in a source code programming language that is subsequently compiled for execution, or may be written in a scripting language such as (but not limited to) HyperText Markup Language (HTML) or Extensible Markup Language (XML). The computer executable instructions may be stored on a computer readable medium such as a nonvolatile storage device. Any suitable computer readable storage media may be utilized, including hard disks, CD-ROMs, optical storage devices, magnetic storage devices, solid state storage devices, and/or any combination thereof. In addition, various transmission (non-storage) media representing data or events as described herein may be transferred between a source and a destination in the form of electromagnetic waves traveling through signal-conducting media such as metal wires, optical fibers, and/or wireless transmission media (e.g., air and/or space). Various aspects described herein may be embodied as a method, a data processing system, or a computer program product. Therefore, various functionalities may be embodied in whole or in part in software, firmware, and/or hardware or hardware equivalents such as integrated circuits, field programmable gate arrays (FPGA), and the like. Particular data structures may be used to more effectively implement one or more aspects described herein, and such data structures are contemplated within the scope of computer executable instructions and computer-usable data described herein.</p><p id="p-0031" num="0030">With further reference to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, one or more aspects described herein may be implemented in a remote-access environment. <figref idref="DRAWINGS">FIG. <b>2</b></figref> depicts an example system architecture including a computing device <b>201</b> in an illustrative computing environment <b>200</b> that may be used according to one or more illustrative aspects described herein. Computing device <b>201</b> may be used as a server <b>206</b><i>a </i>in a single-server or multi-server desktop virtualization system (e.g., a remote access or cloud system) and can be configured to provide virtual machines for client access devices. The computing device <b>201</b> may have a processor <b>203</b> for controlling overall operation of the device <b>201</b> and its associated components, including RAM <b>205</b>, ROM <b>207</b>, Input/Output (I/O) module <b>209</b>, and memory <b>215</b>.</p><p id="p-0032" num="0031">I/O module <b>209</b> may include a mouse, keypad, touch screen, scanner, optical reader, and/or stylus (or other input device(s)) through which a user of computing device <b>201</b> may provide input, and may also include one or more of a speaker for providing audio output and one or more of a video display device for providing textual, audiovisual, and/or graphical output. Software may be stored within memory <b>215</b> and/or other storage to provide instructions to processor <b>203</b> for configuring computing device <b>201</b> into a special purpose computing device in order to perform various functions as described herein. For example, memory <b>215</b> may store software used by the computing device <b>201</b>, such as an operating system <b>217</b>, application programs <b>219</b>, and an associated database <b>221</b>.</p><p id="p-0033" num="0032">Computing device <b>201</b> may operate in a networked environment supporting connections to one or more remote computers, such as terminals <b>240</b> (also referred to as client devices and/or client machines). The terminals <b>240</b> may be personal computers, mobile devices, laptop computers, tablets, or servers that include many or all of the elements described above with respect to the computing device <b>103</b> or <b>201</b>. The network connections depicted in <figref idref="DRAWINGS">FIG. <b>2</b></figref> include a local area network (LAN) <b>225</b> and a wide area network (WAN) <b>229</b>, but may also include other networks. When used in a LAN networking environment, computing device <b>201</b> may be connected to the LAN <b>225</b> through a network interface or adapter <b>223</b>. When used in a WAN networking environment, computing device <b>201</b> may include a modem or other wide area network interface <b>227</b> for establishing communications over the WAN <b>229</b>, such as computer network <b>230</b> (e.g., the Internet). It will be appreciated that the network connections shown are illustrative and other means of establishing a communications link between the computers may be used. Computing device <b>201</b> and/or terminals <b>240</b> may also be mobile terminals (e.g., mobile phones, smartphones, personal digital assistants (PDAs), notebooks, etc.) including various other components, such as a battery, speaker, and antennas (not shown).</p><p id="p-0034" num="0033">Aspects described herein may also be operational with numerous other general purpose or special purpose computing system environments or configurations. Examples of other computing systems, environments, and/or configurations that may be suitable for use with aspects described herein include, but are not limited to, personal computers, server computers, hand-held or laptop devices, multiprocessor systems, microprocessor-based systems, set top boxes, programmable consumer electronics, network personal computers (PCs), minicomputers, mainframe computers, distributed computing environments that include any of the above systems or devices, and the like.</p><p id="p-0035" num="0034">As shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, one or more client devices <b>240</b> may be in communication with one or more servers <b>206</b><i>a</i>-<b>206</b><i>n </i>(generally referred to herein as &#x201c;server(s) <b>206</b>&#x201d;). In one embodiment, the computing environment <b>200</b> may include a network appliance installed between the server(s) <b>206</b> and client machine(s) <b>240</b>. The network appliance may manage client/server connections, and in some cases can load balance client connections amongst a plurality of backend servers <b>206</b>.</p><p id="p-0036" num="0035">The client machine(s) <b>240</b> may in some embodiments be referred to as a single client machine <b>240</b> or a single group of client machines <b>240</b>, while server(s) <b>206</b> may be referred to as a single server <b>206</b> or a single group of servers <b>206</b>. In one embodiment a single client machine <b>240</b> communicates with more than one server <b>206</b>, while in another embodiment a single server <b>206</b> communicates with more than one client machine <b>240</b>. In yet another embodiment, a single client machine <b>240</b> communicates with a single server <b>206</b>.</p><p id="p-0037" num="0036">A client machine <b>240</b> can, in some embodiments, be referenced by any one of the following non-exhaustive terms: client machine(s); client(s); client computer(s); client device(s); client computing device(s); local machine; remote machine; client node(s); endpoint(s); or endpoint node(s). The server <b>206</b>, in some embodiments, may be referenced by any one of the following non-exhaustive terms: server(s), local machine; remote machine; server farm(s), or host computing device(s).</p><p id="p-0038" num="0037">In one embodiment, the client machine <b>240</b> may be a virtual machine. The virtual machine may be any virtual machine, while in some embodiments the virtual machine may be any virtual machine managed by a Type 1 or Type 2 hypervisor, for example, a hypervisor developed by Citrix Systems, IBM, VMware, or any other hypervisor. In some aspects, the virtual machine may be managed by a hypervisor, while in other aspects the virtual machine may be managed by a hypervisor executing on a server <b>206</b> or a hypervisor executing on a client <b>240</b>.</p><p id="p-0039" num="0038">Some embodiments include a client device <b>240</b> that displays application output generated by an application remotely executing on a server <b>206</b> or other remotely located machine. In these embodiments, the client device <b>240</b> may execute a virtual machine receiver program or application to display the output in an application window, a browser, or other output window. In one example, the application is a desktop, while in other examples the application is an application that generates or presents a desktop. A desktop may include a graphical shell providing a user interface for an instance of an operating system in which local and/or remote applications can be integrated. Applications, as used herein, are programs that execute after an instance of an operating system (and, optionally, also the desktop) has been loaded.</p><p id="p-0040" num="0039">The server <b>206</b>, in some embodiments, uses a remote presentation protocol or other program to send data to a thin-client or remote-display application executing on the client to present display output generated by an application executing on the server <b>206</b>. The thin-client or remote-display protocol can be any one of the following non-exhaustive list of protocols: the Independent Computing Architecture (ICA) protocol developed by Citrix</p><p id="p-0041" num="0040">Systems, Inc. of Ft. Lauderdale, Florida; or the Remote Desktop Protocol (RDP) manufactured by the Microsoft Corporation of Redmond, Washington.</p><p id="p-0042" num="0041">A remote computing environment may include more than one server <b>206</b><i>a</i>-<b>206</b><i>n </i>such that the servers <b>206</b><i>a</i>-<b>206</b><i>n </i>are logically grouped together into a server farm <b>206</b>, for example, in a cloud computing environment. The server farm <b>206</b> may include servers <b>206</b> that are geographically dispersed while logically grouped together, or servers <b>206</b> that are located proximate to each other while logically grouped together. Geographically dispersed servers <b>206</b><i>a</i>-<b>206</b><i>n </i>within a server farm <b>206</b> can, in some embodiments, communicate using a WAN (wide), MAN (metropolitan), or LAN (local), where different geographic regions can be characterized as: different continents; different regions of a continent; different countries; different states; different cities; different campuses; different rooms; or any combination of the preceding geographical locations. In some embodiments the server farm <b>206</b> may be administered as a single entity, while in other embodiments the server farm <b>206</b> can include multiple server farms.</p><p id="p-0043" num="0042">In some embodiments, a server farm may include servers <b>206</b> that execute a substantially similar type of operating system platform (e.g., WINDOWS, UNIX, LINUX, iOS, ANDROID, etc.) In other embodiments, server farm <b>206</b> may include a first group of one or more servers that execute a first type of operating system platform, and a second group of one or more servers that execute a second type of operating system platform.</p><p id="p-0044" num="0043">Server <b>206</b> may be configured as any type of server, as needed, e.g., a file server, an application server, a web server, a proxy server, an appliance, a network appliance, a gateway, an application gateway, a gateway server, a virtualization server, a deployment server, a Secure Sockets Layer (SSL) VPN server, a firewall, a web server, an application server or as a master application server, a server executing an active directory, or a server executing an application acceleration program that provides firewall functionality, application functionality, or load balancing functionality. Other server types may also be used.</p><p id="p-0045" num="0044">Some embodiments include a first server <b>206</b><i>a </i>that receives requests from a client machine <b>240</b>, forwards the request to a second server <b>206</b><i>b </i>(not shown), and responds to the request generated by the client machine <b>240</b> with a response from the second server <b>206</b><i>b </i>(not shown.) First server <b>206</b><i>a </i>may acquire an enumeration of applications available to the client machine <b>240</b> as well as address information associated with an application server <b>206</b> hosting an application identified within the enumeration of applications. First server <b>206</b><i>a </i>can then present a response to the client's request using a web interface, and communicate directly with the client <b>240</b> to provide the client <b>240</b> with access to an identified application. One or more clients <b>240</b> and/or one or more servers <b>206</b> may transmit data over network <b>230</b>, e.g., network <b>101</b>.</p><p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. <b>3</b></figref> shows a high-level architecture of an illustrative desktop virtualization system. As shown, the desktop virtualization system may be single-server or multi-server system, or cloud system, including at least one virtualization server <b>301</b> configured to provide virtual desktops and/or virtual applications to one or more client access devices <b>240</b>. As used herein, a desktop refers to a graphical environment or space in which one or more applications may be hosted and/or executed. A desktop may include a graphical shell providing a user interface for an instance of an operating system in which local and/or remote applications can be integrated. Applications may include programs that execute after an instance of an operating system (and, optionally, also the desktop) has been loaded. Each instance of the operating system may be physical (e.g., one operating system per device) or virtual (e.g., many instances of an OS running on a single device). Each application may be executed on a local device, or executed on a remotely located device (e.g., remoted).</p><p id="p-0047" num="0046">A computer device <b>301</b> may be configured as a virtualization server in a virtualization environment, for example, a single-server, multi-server, or cloud computing environment. Virtualization server <b>301</b> illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref> can be deployed as and/or implemented by one or more embodiments of the server <b>206</b> illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref> or by other known computing devices. Included in virtualization server <b>301</b> is a hardware layer that can include one or more physical disks <b>304</b>, one or more physical devices <b>306</b>, one or more physical processors <b>308</b>, and one or more physical memories <b>316</b>. In some embodiments, firmware <b>312</b> can be stored within a memory element in the physical memory <b>316</b> and can be executed by one or more of the physical processors <b>308</b>. Virtualization server <b>301</b> may further include an operating system <b>314</b> that may be stored in a memory element in the physical memory <b>316</b> and executed by one or more of the physical processors <b>308</b>. Still further, a hypervisor <b>302</b> may be stored in a memory element in the physical memory <b>316</b> and can be executed by one or more of the physical processors <b>308</b>.</p><p id="p-0048" num="0047">Executing on one or more of the physical processors <b>308</b> may be one or more virtual machines <b>332</b>A-C (generally <b>332</b>). Each virtual machine <b>332</b> may have a virtual disk <b>326</b>A-C and a virtual processor <b>328</b>A-C. In some embodiments, a first virtual machine <b>332</b>A may execute, using a virtual processor <b>328</b>A, a control program <b>320</b> that includes a tools stack <b>324</b>. Control program <b>320</b> may be referred to as a control virtual machine, Dom0, Domain 0, or other virtual machine used for system administration and/or control. In some embodiments, one or more virtual machines <b>332</b>B-C can execute, using a virtual processor <b>328</b>B-C, a guest operating system <b>330</b>A-B.</p><p id="p-0049" num="0048">Virtualization server <b>301</b> may include a hardware layer <b>310</b> with one or more pieces of hardware that communicate with the virtualization server <b>301</b>. In some embodiments, the hardware layer <b>310</b> can include one or more physical disks <b>304</b>, one or more physical devices <b>306</b>, one or more physical processors <b>308</b>, and one or more physical memory <b>316</b>. Physical components <b>304</b>, <b>306</b>, <b>308</b>, and <b>316</b> may include, for example, any of the components described above. Physical devices <b>306</b> may include, for example, a network interface card, a video card, a keyboard, a mouse, an input device, a monitor, a display device, speakers, an optical drive, a storage device, a universal serial bus connection, a printer, a scanner, a network element (e.g., router, firewall, network address translator, load balancer, virtual private network (VPN) gateway, Dynamic Host Configuration Protocol (DHCP) router, etc.), or any device connected to or communicating with virtualization server <b>301</b>. Physical memory <b>316</b> in the hardware layer <b>310</b> may include any type of memory. Physical memory <b>316</b> may store data, and in some embodiments may store one or more programs, or set of executable instructions. <figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates an embodiment where firmware <b>312</b> is stored within the physical memory <b>316</b> of virtualization server <b>301</b>. Programs or executable instructions stored in the physical memory <b>316</b> can be executed by the one or more processors <b>308</b> of virtualization server <b>301</b>.</p><p id="p-0050" num="0049">Virtualization server <b>301</b> may also include a hypervisor <b>302</b>. In some embodiments, hypervisor <b>302</b> may be a program executed by processors <b>308</b> on virtualization server <b>301</b> to create and manage any number of virtual machines <b>332</b>. Hypervisor <b>302</b> may be referred to as a virtual machine monitor, or platform virtualization software. In some embodiments, hypervisor <b>302</b> can be any combination of executable instructions and hardware that monitors virtual machines executing on a computing machine. Hypervisor <b>302</b> may be Type 2 hypervisor, where the hypervisor executes within an operating system <b>314</b> executing on the virtualization server <b>301</b>. Virtual machines may then execute at a level above the hypervisor <b>302</b>. In some embodiments, the Type 2 hypervisor may execute within the context of a user's operating system such that the Type 2 hypervisor interacts with the user's operating system. In other embodiments, one or more virtualization servers <b>301</b> in a virtualization environment may instead include a Type 1 hypervisor (not shown). A Type 1 hypervisor may execute on the virtualization server <b>301</b> by directly accessing the hardware and resources within the hardware layer <b>310</b>. That is, while a Type 2 hypervisor <b>302</b> accesses system resources through a host operating system <b>314</b>, as shown, a Type 1 hypervisor may directly access all system resources without the host operating system <b>314</b>. A Type 1 hypervisor may execute directly on one or more physical processors <b>308</b> of virtualization server <b>301</b>, and may include program data stored in the physical memory <b>316</b>.</p><p id="p-0051" num="0050">Hypervisor <b>302</b>, in some embodiments, can provide virtual resources to operating systems <b>330</b> or control programs <b>320</b> executing on virtual machines <b>332</b> in any manner that simulates the operating systems <b>330</b> or control programs <b>320</b> having direct access to system resources. System resources can include, but are not limited to, physical devices <b>306</b>, physical disks <b>304</b>, physical processors <b>308</b>, physical memory <b>316</b>, and any other component included in hardware layer <b>310</b> of the virtualization server <b>301</b>. Hypervisor <b>302</b> may be used to emulate virtual hardware, partition physical hardware, virtualize physical hardware, and/or execute virtual machines that provide access to computing environments. In still other embodiments, hypervisor <b>302</b> may control processor scheduling and memory partitioning for a virtual machine <b>332</b> executing on virtualization server <b>301</b>. Hypervisor <b>302</b> may include those manufactured by VMWare, Inc., of Palo Alto, Calif.; HyperV, VirtualServer or virtual PC hypervisors provided by Microsoft, or others. In some embodiments, virtualization server <b>301</b> may execute a hypervisor <b>302</b> that creates a virtual machine platform on which guest operating systems may execute. In these embodiments, the virtualization server <b>301</b> may be referred to as a host server. An example of such a virtualization server is the Citrix Hypervisor provided by Citrix Systems, Inc., of Fort Lauderdale, Fla.</p><p id="p-0052" num="0051">Hypervisor <b>302</b> may create one or more virtual machines <b>332</b>B-C (generally <b>332</b>) in which guest operating systems <b>330</b> execute. In some embodiments, hypervisor <b>302</b> may load a virtual machine image to create a virtual machine <b>332</b>. In other embodiments, the hypervisor <b>302</b> may execute a guest operating system <b>330</b> within virtual machine <b>332</b>. In still other embodiments, virtual machine <b>332</b> may execute guest operating system <b>330</b>.</p><p id="p-0053" num="0052">In addition to creating virtual machines <b>332</b>, hypervisor <b>302</b> may control the execution of at least one virtual machine <b>332</b>. In other embodiments, hypervisor <b>302</b> may present at least one virtual machine <b>332</b> with an abstraction of at least one hardware resource provided by the virtualization server <b>301</b> (e.g., any hardware resource available within the hardware layer <b>310</b>). In other embodiments, hypervisor <b>302</b> may control the manner in which virtual machines <b>332</b> access physical processors <b>308</b> available in virtualization server <b>301</b>. Controlling access to physical processors <b>308</b> may include determining whether a virtual machine <b>332</b> should have access to a processor <b>308</b>, and how physical processor capabilities are presented to the virtual machine <b>332</b>.</p><p id="p-0054" num="0053">As shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, virtualization server <b>301</b> may host or execute one or more virtual machines <b>332</b>. A virtual machine <b>332</b> is a set of executable instructions that, when executed by a processor <b>308</b>, may imitate the operation of a physical computer such that the virtual machine <b>332</b> can execute programs and processes much like a physical computing device. While <figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates an embodiment where a virtualization server <b>301</b> hosts three virtual machines <b>332</b>, in other embodiments virtualization server <b>301</b> can host any number of virtual machines <b>332</b>. Hypervisor <b>302</b>, in some embodiments, may provide each virtual machine <b>332</b> with a unique virtual view of the physical hardware, memory, processor, and other system resources available to that virtual machine <b>332</b>. In some embodiments, the unique virtual view can be based on one or more of virtual machine permissions, application of a policy engine to one or more virtual machine identifiers, a user accessing a virtual machine, the applications executing on a virtual machine, networks accessed by a virtual machine, or any other desired criteria. For instance, hypervisor <b>302</b> may create one or more unsecure virtual machines <b>332</b> and one or more secure virtual machines <b>332</b>. Unsecure virtual machines <b>332</b> may be prevented from accessing resources, hardware, memory locations, and programs that secure virtual machines <b>332</b> may be permitted to access. In other embodiments, hypervisor <b>302</b> may provide each virtual machine <b>332</b> with a substantially similar virtual view of the physical hardware, memory, processor, and other system resources available to the virtual machines <b>332</b>.</p><p id="p-0055" num="0054">Each virtual machine <b>332</b> may include a virtual disk <b>326</b>A-C (generally <b>326</b>) and a virtual processor <b>328</b>A-C (generally <b>328</b>.) The virtual disk <b>326</b>, in some embodiments, is a virtualized view of one or more physical disks <b>304</b> of the virtualization server <b>301</b>, or a portion of one or more physical disks <b>304</b> of the virtualization server <b>301</b>. The virtualized view of the physical disks <b>304</b> can be generated, provided, and managed by the hypervisor <b>302</b>. In some embodiments, hypervisor <b>302</b> provides each virtual machine <b>332</b> with a unique view of the physical disks <b>304</b>. Thus, in these embodiments, the particular virtual disk <b>326</b> included in each virtual machine <b>332</b> can be unique when compared with the other virtual disks <b>326</b>.</p><p id="p-0056" num="0055">A virtual processor <b>328</b> can be a virtualized view of one or more physical processors <b>308</b> of the virtualization server <b>301</b>. In some embodiments, the virtualized view of the physical processors <b>308</b> can be generated, provided, and managed by hypervisor <b>302</b>. In some embodiments, virtual processor <b>328</b> has substantially all of the same characteristics of at least one physical processor <b>308</b>. In other embodiments, virtual processor <b>308</b> provides a modified view of physical processors <b>308</b> such that at least some of the characteristics of the virtual processor <b>328</b> are different than the characteristics of the corresponding physical processor <b>308</b>.</p><p id="p-0057" num="0056">With further reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, some aspects described herein may be implemented in a cloud-based environment. <figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates an example of a cloud computing environment (or cloud system) <b>400</b>. As seen in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, client computers <b>411</b>-<b>414</b> may communicate with a cloud management server <b>410</b> to access the computing resources (e.g., host servers <b>403</b><i>a</i>-<b>403</b><i>b </i>(generally referred herein as &#x201c;host servers <b>403</b>&#x201d;), storage resources <b>404</b><i>a</i>-<b>404</b><i>b </i>(generally referred herein as &#x201c;storage resources <b>404</b>&#x201d;), and network elements <b>405</b><i>a</i>-<b>405</b><i>b </i>(generally referred herein as &#x201c;network resources <b>405</b>&#x201d;)) of the cloud system.</p><p id="p-0058" num="0057">Management server <b>410</b> may be implemented on one or more physical servers. The management server <b>410</b> may run, for example, Citrix Cloud by Citrix Systems, Inc. of Ft. Lauderdale, Fla., or OPENSTACK, among others. Management server <b>410</b> may manage various computing resources, including cloud hardware and software resources, for example, host computers <b>403</b>, data storage devices <b>404</b>, and networking devices <b>405</b>. The cloud hardware and software resources may include private and/or public components. For example, a cloud may be configured as a private cloud to be used by one or more particular customers or client computers <b>411</b>-<b>414</b> and/or over a private network. In other embodiments, public clouds or hybrid public-private clouds may be used by other customers over an open or hybrid networks.</p><p id="p-0059" num="0058">Management server <b>410</b> may be configured to provide user interfaces through which cloud operators and cloud customers may interact with the cloud system <b>400</b>. For example, the management server <b>410</b> may provide a set of application programming interfaces (APIs) and/or one or more cloud operator console applications (e.g., web-based or standalone applications) with user interfaces to allow cloud operators to manage the cloud resources, configure the virtualization layer, manage customer accounts, and perform other cloud administration tasks. The management server <b>410</b> also may include a set of APIs and/or one or more customer console applications with user interfaces configured to receive cloud computing requests from end users via client computers <b>411</b>-<b>414</b>, for example, requests to create, modify, or destroy virtual machines within the cloud. Client computers <b>411</b>-<b>414</b> may connect to management server <b>410</b> via the Internet or some other communication network, and may request access to one or more of the computing resources managed by management server <b>410</b>. In response to client requests, the management server <b>410</b> may include a resource manager configured to select and provision physical resources in the hardware layer of the cloud system based on the client requests. For example, the management server <b>410</b> and additional components of the cloud system may be configured to provision, create, and manage virtual machines and their operating environments (e.g., hypervisors, storage resources, services offered by the network elements, etc.) for customers at client computers <b>411</b>-<b>414</b>, over a network (e.g., the Internet), providing customers with computational resources, data storage services, networking capabilities, and computer platform and application support. Cloud systems also may be configured to provide various specific services, including security systems, development environments, user interfaces, and the like.</p><p id="p-0060" num="0059">Certain clients <b>411</b>-<b>414</b> may be related, for example, to different client computers creating virtual machines on behalf of the same end user, or different users affiliated with the same company or organization. In other examples, certain clients <b>411</b>-<b>414</b> may be unrelated, such as users affiliated with different companies or organizations. For unrelated clients, information on the virtual machines or storage of any one user may be hidden from other users.</p><p id="p-0061" num="0060">Referring now to the physical hardware layer of a cloud computing environment, availability zones <b>401</b>-<b>402</b> (or zones) may refer to a collocated set of physical computing resources. Zones may be geographically separated from other zones in the overall cloud of computing resources. For example, zone <b>401</b> may be a first cloud datacenter located in California, and zone <b>402</b> may be a second cloud datacenter located in Florida. Management server <b>410</b> may be located at one of the availability zones, or at a separate location. Each zone may include an internal network that interfaces with devices that are outside of the zone, such as the management server <b>410</b>, through a gateway. End users of the cloud (e.g., clients <b>411</b>-<b>414</b>) might or might not be aware of the distinctions between zones. For example, an end user may request the creation of a virtual machine having a specified amount of memory, processing power, and network capabilities. The management server <b>410</b> may respond to the user's request and may allocate the resources to create the virtual machine without the user knowing whether the virtual machine was created using resources from zone <b>401</b> or zone <b>402</b>. In other examples, the cloud system may allow end users to request that virtual machines (or other cloud resources) are allocated in a specific zone or on specific resources <b>403</b>-<b>405</b> within a zone.</p><p id="p-0062" num="0061">In this example, each zone <b>401</b>-<b>402</b> may include an arrangement of various physical hardware components (or computing resources) <b>403</b>-<b>405</b>, for example, physical hosting resources (or processing resources), physical network resources, physical storage resources, switches, and additional hardware resources that may be used to provide cloud computing services to customers. The physical hosting resources in a cloud zone <b>401</b>-<b>402</b> may include one or more computer servers <b>403</b>, such as the virtualization servers <b>301</b> described above, which may be configured to create and host virtual machine instances. The physical network resources in a cloud zone <b>401</b> or <b>402</b> may include one or more network elements <b>405</b> (e.g., network service providers) comprising hardware and/or software configured to provide a network service to cloud customers, such as firewalls, network address translators, load balancers, virtual private network (VPN) gateways, Dynamic Host Configuration Protocol (DHCP) routers, and the like. The storage resources in the cloud zone <b>401</b>-<b>402</b> may include storage disks (e.g., solid state drives (SSDs), magnetic hard disks, etc.) and other storage devices.</p><p id="p-0063" num="0062">The example cloud computing environment shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref> also may include a virtualization layer (e.g., as shown in <figref idref="DRAWINGS">FIGS. <b>1</b>-<b>3</b></figref>) with additional hardware and/or software resources configured to create and manage virtual machines and provide other services to customers using the physical resources in the cloud. The virtualization layer may include hypervisors, as described above in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, along with other components to provide network virtualizations, storage virtualizations, etc. The virtualization layer may be as a separate layer from the physical resource layer, or may share some or all of the same hardware and/or software resources with the physical resource layer. For example, the virtualization layer may include a hypervisor installed in each of the virtualization servers <b>403</b> with the physical computing resources. Known cloud systems may alternatively be used, e.g., WINDOWS AZURE (Microsoft Corporation of Redmond Washington), AMAZON EC<b>2</b> (Amazon.com Inc. of Seattle, Washington), IBM BLUE CLOUD (IBM Corporation of Armonk, New York), or others.</p><p id="p-0064" num="0063">Data Object Delivery for Cluster-Computing</p><p id="p-0065" num="0064"><figref idref="DRAWINGS">FIG. <b>5</b></figref> shows an example cluster computing system <b>500</b>. The cluster computing system <b>500</b> may comprise a master device <b>510</b>, a worker device <b>515</b>, and a computer network <b>530</b>. The cluster computing system <b>500</b> may comprise any component or device described in connection with <figref idref="DRAWINGS">FIGS. <b>1</b>-<b>4</b></figref>. For example, the master device <b>510</b> may comprise the management server <b>410</b>. The worker device <b>515</b> may comprise one or more of the client computers <b>411</b>-<b>414</b>. The master device <b>510</b> may be configured to communicate with the worker device <b>515</b> via a computer network <b>530</b>. Although only one worker device is shown in the cluster computing system <b>500</b>, the system <b>500</b> may comprise any number of worker devices (e.g., 5, 100, 5000, etc.). The worker devices may be duplicates of or similar to the worker device <b>515</b>, and may be configured to operate in the same or a similar manner. As discussed in more detail below, the worker device <b>515</b> may be configured to store datasets (e.g., datasets received from the master device <b>510</b>) in one or more local storage media. This may improve the efficiency of the cluster computing system <b>500</b> by reducing the need to transfer large datasets over the network <b>530</b> each time the cluster computing system <b>500</b> is used to perform a task.</p><p id="p-0066" num="0065">The master device <b>510</b> may store datasets used by the cluster computing system <b>500</b>. The master device <b>510</b> may send one or more datasets to one or more worker devices (e.g., the worker device <b>515</b>), to enable the worker devices to perform a task (e.g., data enrichment, machine learning, or any other task) using the one or more datasets. The master device <b>510</b> may comprise a messaging system <b>511</b>, a broadcaster <b>512</b>, and/or a database <b>506</b>, any of which may be implemented in software and/or hardware. The broadcaster <b>512</b> may be configured to send datasets to each worker device in the cluster computing system <b>500</b>. For example, the broadcaster <b>512</b> may be configured to send a snapshot of a dataset to the worker device <b>515</b>. A snapshot may comprise a set of reference markers for data at a point in time. A snapshot may be used to recreate a dataset and may allow the worker device <b>515</b> to create a dataset. The broadcaster <b>512</b> may generate a snapshot indicating a collection of data objects. The snapshot may be generated periodically based on data objects in a centralized data store, database, or other data sources.</p><p id="p-0067" num="0066">The messaging system <b>511</b> may be configured to send messages to the worker device <b>515</b> to enable the worker device to store datasets. A message may comprise updates to a dataset. Additionally or alternatively, a message may comprise a location (e.g., a URI) of a dataset. The messaging system <b>511</b> may encode a dataset into a series of messages. For example, each message in the series may comprise a portion of the dataset. The messaging system <b>511</b> may generate different types of messages. For example, one message type may indicate that a message is part of a series of messages that together comprise a full dataset. A different message type may indicate that a message comprises an update to an existing dataset stored at the worker device <b>515</b>. For example, a message comprising an update may indicate a change (e.g., adding data and/or removing data) to a dataset. The message may comprise the data that should be added and/or the data that should be removed from the dataset. A message type may indicate that the message comprises the location of a snapshot of a dataset.</p><p id="p-0068" num="0067">The worker device <b>515</b> may be configured to perform a task (e.g., as instructed by the master device <b>510</b>). The worker device <b>515</b> may receive, from the master device <b>510</b>, one or more datasets that may be used to perform the task. The worker device <b>515</b> may subscribe to one or more datasets. Subscribing to a dataset may cause the worker device <b>515</b> to receive the dataset and any subsequent data update messages from the master device <b>510</b>. The worker device <b>515</b> may comprise a local storage drive <b>516</b>, a data update service <b>517</b>, and/or a replicator <b>520</b>. The worker device <b>515</b> may be configured to execute one or more applications (e.g., an application <b>518</b> and/or an application <b>519</b>). Although only two applications <b>518</b>-<b>519</b> are shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the worker device <b>515</b> may be configured to execute any number (e.g., 5, 50, 1000, etc.) of applications to perform a task. The applications may comprise a virtual disk and/or a virtual processor (e.g., a virtual disk <b>326</b>A-C and a virtual processor <b>328</b>A-C as described in connection with <figref idref="DRAWINGS">FIG. <b>3</b></figref> above). An application (e.g., the application <b>518</b> and/or the application <b>519</b>) may execute, using a virtual processor (e.g., the virtual processor <b>328</b>A as described above in connection with <figref idref="DRAWINGS">FIG. <b>3</b></figref> above), and/or a control program (e.g., the control program <b>320</b> that includes a tools stack <b>324</b> as described above in connection with <figref idref="DRAWINGS">FIG. <b>3</b></figref>).</p><p id="p-0069" num="0068">The data update service <b>517</b> may subscribe to the messaging system <b>511</b> to receive updates for datasets. The data update service <b>517</b> may receive messages from the messaging system <b>511</b> to update the dataset. The data update service <b>517</b> may receive datasets from the broadcaster <b>512</b>. A dataset stored on the local storage media <b>516</b> may be updated based on the messages received from the messaging system <b>511</b>. The data update service <b>517</b> may compare data updates and/or datasets received with data in the local storage media <b>516</b> (e.g., to check for errors, inconsistencies, etc.). For example, after receiving message comprising a snapshot, the data update service <b>517</b> may compare the snapshot with the dataset stored on the local storage media <b>516</b>. If any errors are found, the data update service <b>517</b> may report them to the master device <b>510</b>. Additionally or alternatively, the data update service <b>517</b> may cause the worker device <b>515</b> to cease execution of one or more applications (e.g., applications <b>518</b>-<b>519</b>), for example, if any errors and/or inconsistencies with the dataset are found. The data update service <b>517</b> may be configured to build or rebuild a dataset using a snapshot of the dataset received from the master device <b>510</b>.</p><p id="p-0070" num="0069">The data update service <b>517</b> may communicate periodically with the messaging system <b>511</b>. The data update service <b>517</b> may expect to receive a message periodically or within a threshold period of time (e.g., every 500 ms, 3 seconds, 1 minute, etc.). For example, if there are no updates, the messaging system <b>511</b> may send a message (e.g., a no-op message) indicating that there are no changes to the dataset stored on the local storage media <b>516</b>. If a message is not received from the messaging system <b>511</b> within the threshold period of time, the worker device <b>515</b> may cease execution of one or more applications (e.g., the application <b>518</b>-<b>519</b>). If the message is not received, it may mean that there is an error with the master device <b>510</b> and/or with the computer network The worker device <b>515</b> may cease execution of the one or more applications to prevent errors that may arise due to its inability to communicate with the master device <b>510</b> and/or any inconsistencies that exist with one or more datasets used by the applications <b>518</b>-<b>519</b>.</p><p id="p-0071" num="0070">The local storage media <b>516</b> may store one or more datasets that the worker device <b>515</b> may use to complete a task. The local storage media <b>516</b> may be any type of storage media (e.g., solid-state drive, disk drive, etc.). The local storage media <b>516</b> may comprise a key-value store. The applications <b>518</b>-<b>519</b> may use remote procedure calls (RPC) within the worker device <b>515</b> (e.g., locally) to retrieve data from the local storage media <b>516</b>. The replicator <b>520</b> may be configured to retrieve one or more datasets from the local storage media <b>516</b> and may send the one or more datasets or a portion of the one or more datasets to the applications <b>518</b>-<b>519</b> for use in a task.</p><p id="p-0072" num="0071">Aspects of the cluster computing system <b>500</b> described herein may improve cluster-computing application performance by eliminating repeated broadcasting of datasets. The system <b>500</b> may reduce memory usage of applications executed by the system <b>500</b> (e.g., executed by the worker device <b>515</b>) because instead of copying the dataset to memory associated with each application, each application may share the dataset stored in the local storage media <b>516</b>. Aspects described herein may reduce the cost of running the applications. Aspects described herein may make memory usage of the worker device <b>515</b> and/or system <b>500</b> more predictable when joining datasets (e.g., combining two or more datasets).</p><p id="p-0073" num="0072"><figref idref="DRAWINGS">FIG. <b>6</b></figref> shows an example user interface <b>600</b> that may be used to subscribe to a dataset. A user may use the user interface to cause one or more worker devices (e.g., the worker device <b>515</b>) to subscribe to a dataset. As described above in connection with <figref idref="DRAWINGS">FIG. <b>5</b></figref>, by subscribing to a dataset, the worker device <b>515</b> may receive, from the master device <b>510</b>, messages that update a dataset stored in the local storage media <b>516</b> of the worker device <b>515</b>. The user interface <b>600</b> may comprise one or more elements <b>605</b> indicating datasets to which the worker device <b>515</b> may subscribe. The user interface may comprise one or more elements <b>610</b> that may correspond to one or more datasets. By interacting with the one or more elements <b>610</b>, a user may cause the worker device <b>515</b> to subscribe to an associated dataset.</p><p id="p-0074" num="0073">A worker device of a cluster may store one or more datasets on local storage media. This may reduce the need for a master device of the cluster to send large datasets over a network to the worker device each time a task is performed by the cluster. <figref idref="DRAWINGS">FIG. <b>7</b></figref> shows an example method for delivering data to a cluster computing system. The example method may be performed using any device or component described in connection with <figref idref="DRAWINGS">FIGS. <b>1</b>-<b>6</b></figref>. Although one or more steps of <figref idref="DRAWINGS">FIG. <b>7</b></figref> are described for convenience as being performed by the master device <b>510</b> or the worker device <b>515</b>, one, some, or all of such steps may be performed by the management server <b>410</b>, client computers <b>411</b>-<b>414</b>, zones <b>401</b>-<b>402</b>, or one or more other computing devices, and steps may be distributed among one or more computing devices, including any computing devices such as those described in connection with <figref idref="DRAWINGS">FIGS. <b>1</b>-<b>6</b></figref>. One or more steps of <figref idref="DRAWINGS">FIG. <b>7</b></figref> may be rearranged, modified, repeated, and/or omitted.</p><p id="p-0075" num="0074">At step <b>703</b>, the worker device <b>515</b> may subscribe to one or more datasets. The worker device <b>515</b> may send a request to the master device <b>510</b> to subscribe to a dataset. The request may comprise an identification of one or more datasets to which the worker device <b>515</b> requests subscription.</p><p id="p-0076" num="0075">At step <b>706</b>, the worker device <b>515</b> may receive data corresponding to the dataset subscribed to in step <b>703</b>. For example, the worker device <b>515</b> may receive a series of messages comprising the dataset. Additionally or alternatively, the worker device <b>515</b> may receive a message indicating a storage location where a snapshot of the dataset is stored (e.g., on the master device <b>510</b>, or other location). The worker device <b>515</b> may retrieve (e.g., via the computer network <b>530</b>) the dataset from the indicated location.</p><p id="p-0077" num="0076">At step <b>709</b>, the worker device <b>515</b> may determine whether a data update frequency satisfies a threshold. The worker device <b>515</b> may be configured to store datasets that change less frequently than other datasets. A dataset that changes less often may require less messages to be sent over the computer network <b>230</b> to keep the dataset up to date. The cluster computing system <b>500</b> may experience a greater improvement in efficiency (e.g., less bandwidth usage), for example, if datasets that change infrequently are stored at one or more worker devices of the system <b>500</b>.</p><p id="p-0078" num="0077">The dataset received in step <b>706</b> may comprise metadata indicating how often the dataset is updated. The worker device <b>515</b> may use the metadata to determine whether a data update frequency satisfies a threshold (e.g., whether the data update frequency of the dataset is below the threshold). The metadata may comprise a history of each update or change to the dataset. For example, the history may comprise a list of dates when the dataset was changed. Additionally or alternatively, the metadata may indicate a list of updates and a volume of data (e.g., megabytes, gigabytes, terabytes, petabytes, etc.) that was changed in each update. The worker device <b>515</b> may determine, based on the metadata, whether a data update frequency satisfies a threshold. For example, if the data is updated less than a threshold quantity of times in a time period (e.g., less than 10 times per day, less than once per month, etc.), the worker device may determine that the update frequency threshold is satisfied. Additionally or alternatively, the worker device <b>515</b> may determine, based on the metadata, whether a change in volume within a time period satisfies a threshold. For example, the worker device <b>515</b> may determine whether a threshold volume has changed in a time period (e.g., there has been less than 10 gigabytes of changes to the dataset within two days, or any other volume/time period). Step <b>712</b> may be performed if the worker device <b>515</b> determines that the data update frequency satisfies the threshold. Otherwise, step <b>721</b> may be performed.</p><p id="p-0079" num="0078">At step <b>712</b>, the worker device <b>515</b> may store the data received in step <b>706</b> in the local storage media <b>516</b>. If the update frequency (e.g., in terms of volume and/or quantity of updates) satisfies the threshold, the worker device <b>712</b> may store the dataset in the local storage media <b>516</b>. The worker device <b>515</b> may reduce traffic on the computer network <b>530</b> by reducing the need to receive datasets from the master device <b>510</b> each time a task is performed using the cluster computing system <b>500</b>. It may benefit the system <b>500</b>, for example, if worker devices store datasets locally so that the datasets do not need to be sent over a network each time a task is performed.</p><p id="p-0080" num="0079">At step <b>718</b>, the worker device may copy a dataset from the local storage media <b>516</b> to memory associated with one or more applications. The dataset may be shared by one or more applications executing on the worker device <b>515</b>. For example, applications <b>518</b>-<b>519</b> may share the dataset stored in local storage in step <b>712</b>. Each application may access portions of the dataset from the local storage media <b>516</b> and may store it in partitioned memory. The worker device <b>515</b> may use less memory for the dataset because instead of having to copy the dataset to each application, the applications may share the dataset.</p><p id="p-0081" num="0080">At step <b>721</b>, the worker device <b>515</b> may optionally join a dataset from remote storage with the dataset copied in step <b>718</b>. The task performed by the worker device <b>515</b> may require additional data beyond the dataset stored in the local storage media <b>516</b>. The worker device <b>515</b> may receive the additional data from the master device <b>510</b> and may join or otherwise combine it with the data copied in step <b>718</b>.</p><p id="p-0082" num="0081">At step <b>724</b>, the worker device <b>515</b> may perform a task using one or more applications (e.g., the applications <b>518</b>-<b>519</b>). The worker device may perform any task assigned by the master device <b>510</b> or any task that may be performed by a cluster computing system. For example, the worker device <b>515</b> may perform streaming ETL (extract, transform, load), data enrichment, machine learning, or any other task.</p><p id="p-0083" num="0082">At step <b>727</b>, the worker device <b>515</b> may determine whether a wait time to receive a message has expired. The worker device <b>515</b> may expect to receive a message (e.g., a data update message or a no-op message) from the master device <b>510</b> periodically and/or within a threshold period of time. For example, the threshold period of time may be 50 milliseconds, 500 milliseconds, 1 second, 1 minute, or any other period of time. Receiving a message within the threshold period of time may indicate that the worker device <b>515</b> may continue performing the task described above in connection with step <b>724</b>. If the wait time (e.g., the threshold period of time) has expired without the worker device <b>515</b> receiving a message, step <b>742</b> may be performed. Otherwise, step <b>728</b> may be performed.</p><p id="p-0084" num="0083">At step <b>728</b>, the worker device <b>515</b> may determine whether the message received in step <b>727</b> is a no-op message. A no-op message may indicate that there have been no changes to the data used to perform the task in step <b>724</b> and/or no changes to the data stored in the local storage media <b>516</b>. The no-op message may indicate that the worker device <b>515</b> is using the most up to date version of the dataset. If there are no updates to the data used by the applications <b>518</b>-<b>519</b>, the worker device <b>515</b> may expect to receive a no-op message from the master device <b>510</b> periodically and/or within a threshold period of time (e.g., the threshold period of time or wait time described above in connection with step <b>727</b>). Receiving the no-op message within the threshold period of time may indicate that the worker device <b>515</b> may continue performing the task described above in connection with step <b>724</b>.</p><p id="p-0085" num="0084">Additionally or alternatively, the worker device <b>515</b> may determine whether the message received is a data update message (e.g., if the message received in step <b>727</b> is not a no-op message then it may be a data update message). The data update message may indicate a change in data that is being used by the worker device <b>515</b> to perform the task in step <b>724</b> and/or data that is stored in local storage <b>712</b>. The data update message may indicate data that should be added to a dataset. Additionally or alternatively, the data update message may indicate data that should be removed from the dataset. If the worker device <b>515</b> determines that the message was not a no-op message, step <b>730</b> may be performed. Otherwise (e.g., the message received in step <b>727</b> was a no-op message), the worker device <b>515</b> may continue performing the task described above in connection with step <b>724</b>.</p><p id="p-0086" num="0085">At step <b>730</b>, the worker device <b>515</b> may determine whether the data update message comprises any errors. The worker device <b>515</b> may compare data indicated by the update message with data stored in the local storage media <b>516</b> to determine whether there are any inconsistencies. The worker device <b>515</b> may send a message to the master device <b>510</b> to notify the master device <b>510</b> of the error. If the worker device <b>515</b> determines that the data update message comprises errors, step <b>742</b> may be performed. If the worker device <b>515</b> determines that the data update message does not contain errors step <b>733</b> may be performed.</p><p id="p-0087" num="0086">At step <b>733</b>, the worker device <b>515</b> may update data in in the local storage media <b>516</b> based on the data update message received in step <b>727</b>. The worker device <b>515</b> may add new data and/or remove data from the local storage media <b>516</b>. At step <b>736</b>, the worker device may update data used by the applications to perform the task in step <b>724</b>. The applications (e.g., applications <b>518</b>-<b>519</b>) may access the local storage device <b>516</b> to access the updated data. The worker device <b>515</b> may continue to perform the task described in connection with step <b>724</b>.</p><p id="p-0088" num="0087">Step <b>742</b> may be performed, for example, if the wait time (e.g., the threshold period of time) has expired in step <b>727</b>, or if the data update contains errors (as described in connection with step <b>730</b>). At step <b>742</b>, the worker device <b>515</b> may stop performing the task from step <b>724</b>. The worker device <b>515</b> may stop performing the task to prevent any errors that may arise due to an inability to receive data update messages from the master device <b>510</b> and/or otherwise communicate with the master device <b>510</b>. Additionally or alternatively, the worker device <b>515</b> may stop performing the task because the data (e.g., in the data update message) contains errors. The worker device may <b>515</b> restart or continue the task when a no-op message or other message is received from the master device <b>510</b>.</p><p id="p-0089" num="0088">A master device of a cluster computing system may be configured to provide data to one or more worker devices of the cluster. The master device may use a messaging system to send datasets and/or updates to datasets to the one or more worker devices. The worker devices may store the datasets on a local storage media to prevent the need to send the datasets each time a task is performed by the cluster computing system. <figref idref="DRAWINGS">FIG. <b>8</b></figref> shows an example method for providing data for a cluster computing system. The example method may be performed using any device or component described in connection with <figref idref="DRAWINGS">FIGS. <b>1</b>-<b>7</b></figref>. Although one or more steps of <figref idref="DRAWINGS">FIG. <b>8</b></figref> are described for convenience as being performed by the master device <b>510</b> and/or the worker device <b>515</b>, one, some, or all of such steps may be performed by the management server <b>410</b>, client computers <b>411</b>-<b>414</b>, zones <b>401</b>-<b>402</b>, or one or more other computing devices, and steps may be distributed among one or more computing devices, including any computing devices such as those described in connection with <figref idref="DRAWINGS">FIGS. <b>1</b>-<b>7</b></figref>. One or more steps of <figref idref="DRAWINGS">FIG. <b>8</b></figref> may be rearranged, modified, repeated, and/or omitted.</p><p id="p-0090" num="0089">At step <b>803</b>, the master device <b>510</b> may monitor one or more data sources. The master device <b>510</b> may monitor data sources that are external to the cluster computing system <b>500</b> and may store data to be used by the cluster computing system. The master device <b>510</b> may record changes to any datasets stored by the master computing device <b>510</b> and may receive updates to the dataset.</p><p id="p-0091" num="0090">At step <b>806</b>, the master device <b>510</b> may determine whether an update threshold has been met. The master device <b>510</b> may be configured to share data with the one or more worker devices (e.g., the worker device <b>515</b>). The master device <b>510</b> may wait to send updates to the worker devices until a threshold amount of data has changed in the datasets. For example, the master device <b>510</b> may avoid sending an update if less than a threshold volume of data in the dataset has changed (e.g., less than 1 megabyte, 500 gigabytes, or any other volume of data). If it is determined that the data update threshold has been met, step <b>809</b> may be performed. Otherwise, the master device <b>510</b> may continue to monitor the one or more data sources as explained above in connection with step <b>803</b>.</p><p id="p-0092" num="0091">At step <b>809</b>, the master device <b>510</b> (e.g., the messaging system <b>511</b>) may generate one or more messages. The one or more messages may comprise a dataset or an update to a dataset as described above in connection with <figref idref="DRAWINGS">FIG. <b>5</b></figref>. The master device <b>510</b> may generate messages based on restart requirements of the messaging system <b>511</b> and/or fault tolerant requirements. For example, the messages may be sent based on a first threshold period of time indicating the maximum permitted time between two messages. For example, if the first threshold period of time is five seconds, the master device <b>510</b> may be configured to send a message (e.g., a no-op message when no change is made during the period of time) every five seconds. If the worker device <b>515</b> does not receive a message within a pre-defined time (e.g., a second threshold time (e.g., 15 seconds, or some other period of time that is greater than the first threshold period of time)), the worker device <b>515</b> may determine that it has lost communication with the master device <b>510</b>. The worker device <b>515</b> may have lost communication with the master device <b>510</b> because the master device <b>510</b> may have shut down or experienced a fault (e.g., an error). Additionally or alternatively, the worker device <b>515</b> may have lost communication with the master device <b>510</b> due to networking issues. At step <b>812</b>, the master device may send the message generated in step <b>809</b> to one or more worker devices (e.g., the worker device <b>515</b>).</p><p id="p-0093" num="0092">At step <b>815</b>, the master device <b>510</b> may determine whether to send a no-op message. The master device <b>510</b> may send a no-op message periodically to the one or more worker devices to indicate that there have been no updates to data. After a threshold period of time has expired, the master device <b>510</b> may send a no-op message. For example, the master device <b>510</b> may determine to send a no-op message once every 50 milliseconds, 500 milliseconds, 1 second, 1 minute, or any other period of time. If the master device <b>510</b> determines to not send a no-op message, the master device may continue to monitor one or more data sources in step <b>803</b>. If the master device <b>510</b> determines that a no-op message should be sent, step <b>818</b> may be performed. At step <b>818</b>, the master device <b>510</b> may send a no-op message to one or more worker devices (e.g., the worker device <b>515</b>).</p><p id="p-0094" num="0093">The following paragraphs (M1) through (M8) describe examples of methods that may be implemented in accordance with the present disclosure.</p><p id="p-0095" num="0094">(M1) A method comprising subscribing, by a worker device of a cluster, to a dataset; receiving, via a first process executing on the worker device and from a master device of the cluster, the dataset; storing the dataset on a local storage media of the worker device; executing, via one or more processes different from the first process, a plurality of applications to perform a task on the dataset, wherein each application of the plurality of applications is configured to share, with each other application of the plurality of applications, the dataset from the local storage media; receiving, based on the subscribing and from the master device, a data update message indicating a change to the dataset; and updating, based on the data update message, the dataset on the local storage media.</p><p id="p-0096" num="0095">(M2) A method may be performed as described in paragraph (M1), further comprising: ceasing, based on a determination that a no-op message has not been received within a threshold time period, execution of the plurality of applications.</p><p id="p-0097" num="0096">(M3) A method may be performed as described in any one of paragraphs (M1)-(M2), wherein the no-op message indicates that there have been no updates to the dataset.</p><p id="p-0098" num="0097">(M4) A method may be performed as described in any one of paragraphs (M1)-(M3), wherein the storing the dataset comprises: determining, based on metadata associated with the dataset, that a quantity of updates to the dataset within a time period satisfy a threshold; and based on the determining that the quantity of updates satisfies the threshold, storing the dataset in the local storage media.</p><p id="p-0099" num="0098">(M5) A method may be performed as described in any one of paragraphs (M1)-(M4), wherein the storing the dataset comprises: determining, based on metadata associated with the dataset, that a size of the dataset satisfies a threshold; and based on the determining that the size of the dataset satisfies the threshold, storing the dataset in the local storage media.</p><p id="p-0100" num="0099">(M6) A method may be performed as described in any one of paragraphs (M1)-(M5), further comprising: receiving, via the first process and during execution of the plurality of applications, a message comprising an update to the dataset; and updating, via the first process, the dataset in the local storage media.</p><p id="p-0101" num="0100">(M7) A method may be performed as described in any one of paragraphs (M1)-(M6), wherein the local storage media comprises a solid-state drive.</p><p id="p-0102" num="0101">(M8) A method may be performed as described in any one of paragraphs (M1)-(M8), wherein the executing the plurality of applications comprises joining the dataset from the local storage media with a second dataset received from the master device.</p><p id="p-0103" num="0102">The following paragraphs (A1) through (A8) describe examples of apparatuses that may be implemented in accordance with the present disclosure.</p><p id="p-0104" num="0103">(A1) A worker device comprising: at least one processor; and memory storing computer-readable instructions that, when executed by the at least one processor, cause the worker device to: subscribe to a dataset; receive, via a first process executing on the worker device and from a master device, the dataset; store the dataset on a local storage media of the worker device; execute, via one or more processes different from the first process, a plurality of applications to perform a task on the dataset, wherein each application of the plurality of applications is configured to share, with each other application of the plurality of applications, the dataset from the local storage media; receive, based on the subscribing and from the master device, a data update message indicating a change to the dataset; and update, based on the data update message, the dataset on the local storage media.</p><p id="p-0105" num="0104">(A2) An apparatus as described in paragraph (A1), wherein the computer-readable instructions that, when executed by the at least one processor, cause the worker device to:</p><p id="p-0106" num="0105">cease, based on a determination that a no-op message has not been received within a threshold time period, execution of the plurality of applications.</p><p id="p-0107" num="0106">(A3) An apparatus as described in any of paragraphs (A1)-(A2), wherein the no-op message indicates that there have been no updates to the dataset.</p><p id="p-0108" num="0107">(A4) An apparatus as described in any of paragraphs (A1)-(A3), wherein the storing the dataset comprises: determining, based on metadata associated with the dataset, that a quantity of updates to the dataset within a time period satisfy a threshold; and based on the determining that the quantity of updates satisfies the threshold, storing the dataset in the local storage media.</p><p id="p-0109" num="0108">(A5) An apparatus as described in any of paragraphs (A1)-(A4), wherein the storing the dataset comprises: determining, based on metadata associated with the dataset, that a size of the dataset satisfies a threshold; and based on the determining that the size of the dataset satisfies the threshold, storing the dataset in the local storage media.</p><p id="p-0110" num="0109">(A6) An apparatus as described in any of paragraphs (A1)-(A5), wherein the computer-readable instructions that, when executed by the at least one processor, cause the worker device to: receive, via the first process and during execution of the plurality of applications, a message comprising an update to the dataset; and update, via the first process, the dataset in the local storage media.</p><p id="p-0111" num="0110">(A7) An apparatus as described in any of paragraphs (A1)-(A6), wherein the local storage media comprises a solid-state drive.</p><p id="p-0112" num="0111">(A8) An apparatus as described in any of paragraphs (A1)-(A7), wherein the executing the plurality of applications comprises joining the dataset from the local storage media with a second dataset received from the master device.</p><p id="p-0113" num="0112">The following paragraphs (S1) through (S8) describe examples of one or more systems that may be implemented in accordance with the present disclosure.</p><p id="p-0114" num="0113">(S1) A cluster computing system comprising: a worker device and in communication with a master device; wherein the worker device is configured to: subscribe to a dataset; receive, via a first process executing on the worker device and from the master device, the dataset; store the dataset on a local storage media of the worker device; execute, via one or more processes different from the first process, a plurality of applications to perform a task on the dataset, wherein each application of the plurality of applications is configured to share, with each other application of the plurality of applications, the dataset from the local storage media; receive, based on the subscribing and from the master device, a data update message indicating a change to the dataset; and update, based on the data update message, the dataset on the local storage media.</p><p id="p-0115" num="0114">(S2) A system as described in paragraph (S1), wherein the worker device is further configured to: cease, based on a determination that a no-op message has not been received within a threshold time period, execution of the plurality of applications.</p><p id="p-0116" num="0115">(S3) A system as described in any of paragraphs (S1)-(S2), wherein the no-op message indicates that there have been no updates to the dataset.</p><p id="p-0117" num="0116">(S4) A system as described in any of paragraphs (S1)-(S3), wherein the storing the dataset comprises: determining, based on metadata associated with the dataset, that a quantity of updates to the dataset within a time period satisfy a threshold; and based on the determining that the quantity of updates satisfies the threshold, storing the dataset in the local storage media.</p><p id="p-0118" num="0117">(S5) A system as described in any of paragraphs (S1)-(S4), wherein the storing the dataset comprises: determining, based on metadata associated with the dataset, that a size of the dataset satisfies a threshold; and based on the determining that the size of the dataset satisfies the threshold, storing the dataset in the local storage media.</p><p id="p-0119" num="0118">(S6) A system as described in any of paragraphs (S1)-(S5), wherein the worker device is further configured to: receive, via the first process and during execution of the plurality of applications, a message comprising an update to the dataset; and update, via the first process, the dataset in the local storage media.</p><p id="p-0120" num="0119">(S7) A system as described in any of paragraphs (S1)-(S6), wherein the local storage media comprises a solid-state drive.</p><p id="p-0121" num="0120">(S8) A system as described in any of paragraphs (S1)-(S7), wherein the executing the plurality of applications comprises joining the dataset from the local storage media with a second dataset received from the master device.</p><p id="p-0122" num="0121">The following paragraphs (CRM1) through (CRMXX) describe examples of computer-readable media that may be implemented in accordance with the present disclosure.</p><p id="p-0123" num="0122">(CRM1) A non-transitory machine-readable medium storing instructions, that when executed by one or more processors, cause the one or more processors to: subscribe, by a worker device, to a dataset; receive, via a first process executing on the worker device and from a master device, the dataset; store the dataset on a local storage media of the worker device; execute, via one or more processes different from the first process, a plurality of applications to perform a task on the dataset, wherein each application of the plurality of applications is configured to share, with each other application of the plurality of applications, the dataset from the local storage media; receive, based on the subscribing and from the master device, a data update message indicating a change to the dataset; and update, based on the data update message, the dataset on the local storage media.</p><p id="p-0124" num="0123">(CRM2) A non-transitory machine-readable medium as described in paragraph (CRM1), wherein the instructions, when executed by the one or more processors, further cause the one or more processors to: cease, based on a determination that a no-op message has not been received within a threshold time period, execution of the plurality of applications.</p><p id="p-0125" num="0124">(CRM3) A non-transitory machine-readable medium as described in any of paragraphs (CRM1)-(CRM2), wherein the no-op message indicates that there have been no updates to the dataset.</p><p id="p-0126" num="0125">(CRM4) A non-transitory machine-readable medium as described in any of paragraphs (CRM1)-(CRM3), wherein the storing the dataset comprises: determining, based on metadata associated with the dataset, that a quantity of updates to the dataset within a time period satisfy a threshold; and based on the determining that the quantity of updates satisfies the threshold, storing the dataset in the local storage media.</p><p id="p-0127" num="0126">Although the subject matter has been described in language specific to structural features and/or methodological acts, it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather, the specific features and acts described above are described as example implementations of the following claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method comprising:<claim-text>subscribing, by a worker device of a plurality of worker devices, to a dataset;</claim-text><claim-text>receiving, from a master device of the plurality of worker devices the dataset;</claim-text><claim-text>determining, that a quantity of updates to the dataset within a time period satisfies a data update frequency threshold;</claim-text><claim-text>based on determining that the quantity of updates within the time period satisfies the data update frequency threshold, storing the dataset in a local storage media; and</claim-text><claim-text>executing, by the worker device, a plurality of applications to use the dataset, wherein each application of the plurality of applications is configured to share, with each other application of the plurality of applications, the dataset from the local storage media.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>ceasing, based on a determination that a no-op message has not been received within a threshold time period, execution of the plurality of applications.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the no-op message indicates that there have been no updates to the dataset.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the storing the dataset comprises:<claim-text>determining, based on metadata associated with the dataset, that a size of the dataset satisfies a threshold; and</claim-text><claim-text>based on the determining that the size of the dataset satisfies the threshold, storing the dataset in the local storage media.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>receiving, during execution of the plurality of applications, a message comprising an update to the dataset; and</claim-text><claim-text>updating the dataset in the local storage media.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the executing the plurality of applications comprises joining the dataset from the local storage media with a second dataset received from the master device.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each of the plurality of applications comprise:<claim-text>a virtual processor.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the determining that the quantity of updates to the dataset satisfies the data update frequency threshold further comprises:<claim-text>determining, based on metadata associated with the dataset, that the quantity of updates to the dataset within the time period satisfies the data update frequency threshold.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the receiving the dataset comprises:<claim-text>receiving, from the master device, an update message indicating change to the dataset; and</claim-text><claim-text>updating the dataset on the local storage media.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the determination that the quantity of updates to the dataset satisfies the data update frequency threshold further comprises:<claim-text>determining, based on a snapshot of the dataset, that the quantity of updates to the dataset satisfies the data update frequency threshold.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. A computing system comprising:<claim-text>a master device of a plurality of worker devices comprising:<claim-text>one or more processors; and</claim-text><claim-text>memory storing instructions that, when executed by the one or more processors cause the master device to:</claim-text><claim-text>monitor a dataset on the master device for changes to the dataset; and</claim-text></claim-text><claim-text>a worker device comprising:<claim-text>one or more second processors; and</claim-text><claim-text>second memory storing instructions that, when executed by the one or more second processors, cause the worker device to:<claim-text>subscribe to the dataset on the master device;</claim-text><claim-text>receive, from the master device, the dataset;</claim-text><claim-text>determine, that a quantity of updates to the dataset within a time period satisfies a data update frequency threshold;</claim-text><claim-text>based on the determination that the quantity of updates within the time period satisfies the data update frequency threshold, store the dataset on a local storage media; and</claim-text><claim-text>execute a plurality of applications to use the dataset, wherein each application of the plurality of applications is configured to share, with each other application of the plurality of applications, the dataset from the local storage media.</claim-text></claim-text></claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the determination that the quantity of updates to the dataset satisfies the data update frequency threshold is based on metadata associated with the dataset.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. A method comprising:<claim-text>monitoring, by a master device of a plurality of worker devices, a dataset for changes to the dataset;</claim-text><claim-text>receiving, by the master device and from a worker device of the plurality of worker devices, a request to subscribe to the dataset;</claim-text><claim-text>sending, by the master device and to the worker device, the dataset;</claim-text><claim-text>determining, by the master device, that a quantity of updates to the dataset within a time period satisfies a data update frequency threshold; and</claim-text><claim-text>based on determining that the quantity of updates to the dataset within the time period satisfies the data update frequency threshold, sending, by the master device, information to the worker device to update the dataset.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the information to update the dataset comprises:<claim-text>a snapshot of the dataset.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the snapshot of the dataset comprises:<claim-text>at least one of:<claim-text>a set of reference markers for data at a point in time; or</claim-text><claim-text>an indication of a collection of data objects at a point in time.</claim-text></claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the information to update the dataset comprises:<claim-text>metadata associated with the dataset.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, further comprising, sending by the master device and to the worker device a data update message indicating a change to the dataset.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The method of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the data update message is sent based on the request to subscribe to the dataset.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, further comprising:<claim-text>receiving, by the master device and from the worker device, a request for an updated dataset based on a determination that the quantity of updates within the time period satisfies a data update frequency threshold; and</claim-text><claim-text>sending, by the master device and to the worker device, the updated dataset.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The method of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein the receiving the request for the updated dataset is further based on a determination that a size of the dataset satisfies a threshold.</claim-text></claim></claims></us-patent-application>