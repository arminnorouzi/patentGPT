<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230001585A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230001585</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17664990</doc-number><date>20220525</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>KR</country><doc-number>10-2021-0088006</doc-number><date>20210705</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>B</section><class>25</class><subclass>J</subclass><main-group>11</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>05</class><subclass>B</subclass><main-group>13</main-group><subgroup>02</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>B</section><class>25</class><subclass>J</subclass><main-group>13</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>01</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>25</class><subclass>J</subclass><main-group>11</main-group><subgroup>0005</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>05</class><subclass>B</subclass><main-group>13</main-group><subgroup>027</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>25</class><subclass>J</subclass><main-group>13</main-group><subgroup>087</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>015</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">Rehabilitation Robot Control Apparatus and Method Thereof</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Hyundai Motor Company</orgname><address><city>Seoul</city><country>KR</country></address></addressbook><residence><country>KR</country></residence></us-applicant><us-applicant sequence="01" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>kia Corporation</orgname><address><city>Seoul</city><country>KR</country></address></addressbook><residence><country>KR</country></residence></us-applicant><us-applicant sequence="02" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Korea Advanced Institute of Science and Technology</orgname><address><city>Daejeon</city><country>KR</country></address></addressbook><residence><country>KR</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Nam</last-name><first-name>Seung Kyu</first-name><address><city>Seoul</city><country>KR</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Yoon</last-name><first-name>Ju Young</first-name><address><city>Suwon-si</city><country>KR</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Lee</last-name><first-name>Tae Jun</first-name><address><city>Suwon-si</city><country>KR</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Kim</last-name><first-name>Beom Su</first-name><address><city>Yongin-si</city><country>KR</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>Jeong</last-name><first-name>Jae Seung</first-name><address><city>Daejeon</city><country>KR</country></address></addressbook></inventor><inventor sequence="05" designation="us-only"><addressbook><last-name>Kim</last-name><first-name>Jae Hyun</first-name><address><city>Daejeon</city><country>KR</country></address></addressbook></inventor><inventor sequence="06" designation="us-only"><addressbook><last-name>Aderinwale</last-name><first-name>Adedoyin Olumuyiwa</first-name><address><city>Daejeon</city><country>KR</country></address></addressbook></inventor><inventor sequence="07" designation="us-only"><addressbook><last-name>Jung</last-name><first-name>Jun Ha</first-name><address><city>Seongnam-si</city><country>KR</country></address></addressbook></inventor><inventor sequence="08" designation="us-only"><addressbook><last-name>Jeong</last-name><first-name>Dong Hwa</first-name><address><city>Rusan</city><country>KR</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">An embodiment rehabilitation robot control apparatus includes a brainwave signal measuring device configured to measure a brainwave signal of a user, a preprocessing device configured to preprocess the measured brainwave signal, a classification device configured to classify a motor intention of the user based on the brainwave signal preprocessed by the preprocessing device, and a controller configured to reflect the motor intention of the user in real time to control an operation or a stop of a rehabilitation robot.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="60.45mm" wi="157.14mm" file="US20230001585A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="80.60mm" wi="159.17mm" file="US20230001585A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="98.04mm" wi="159.68mm" file="US20230001585A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="145.46mm" wi="144.36mm" file="US20230001585A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="222.76mm" wi="68.92mm" orientation="landscape" file="US20230001585A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="230.89mm" wi="107.53mm" orientation="landscape" file="US20230001585A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="206.42mm" wi="107.36mm" orientation="landscape" file="US20230001585A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="171.11mm" wi="121.16mm" orientation="landscape" file="US20230001585A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="180.76mm" wi="157.48mm" file="US20230001585A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="139.62mm" wi="134.03mm" file="US20230001585A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application claims the benefit of Korean Patent Application No. 10-2021-0088006, filed on Jul. 5, 2021, which application is hereby incorporated herein by reference.</p><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">The present disclosure relates to a rehabilitation robot control apparatus and a method thereof.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">Existing rehabilitation treatment proceeds in a manner which repeatedly performs rehabilitation operations passively by a rehabilitation therapist or a rehabilitation robot. However, it is obvious that active rehabilitation treatment in which the intention of a patient is reflected is more effective in restoring a motor function of the patient than the existing passive rehabilitation treatment. Furthermore, brain electrical activity may be identified by means of electrical signals measured by electrodes attached to a head surface of the patient. The measured brainwave (electroencephalogram (EEG)) may be analyzed to classify the intention of a user.</p><p id="p-0005" num="0004">Particularly, when the intention of the patient is reflected in real time to continue controlling a rehabilitation operation, effective treatment for restoring the motor function is facilitated due to an increase in patient concentration and participation in rehabilitation. Thus, there is a need for a technology capable of analyzing a brainwave of the user in real time and controlling rehabilitation treatment.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0006" num="0005">The present disclosure relates to a rehabilitation robot control apparatus and a method thereof. Particular embodiments relate to an apparatus for controlling a rehabilitation robot based on a brain computer interface (BCI) and a method therefor.</p><p id="p-0007" num="0006">Embodiments of the present disclosure can solve problems occurring in the prior art while advantages achieved by the prior art are maintained intact.</p><p id="p-0008" num="0007">An embodiment of the present disclosure provides an apparatus for controlling a rehabilitation robot based on a brain computer interface (BCI) and a method therefor.</p><p id="p-0009" num="0008">Another embodiment of the present disclosure provides a rehabilitation robot control apparatus for analyzing a brainwave of a patient and deriving a more effective treatment effect by means of a rehabilitation treatment in which an intention of the patient is reflected and a method therefor.</p><p id="p-0010" num="0009">Another embodiment of the present disclosure provides a rehabilitation robot control apparatus for analyzing a motor intention of a patient and continuing a rehabilitation exercise with suitable strength, when the patient shows an active will for the rehabilitation exercise, to optimize an effect of the rehabilitation exercise and a method therefor.</p><p id="p-0011" num="0010">Another embodiment of the present disclosure provides a rehabilitation robot control apparatus for controlling a rehabilitation exercise based on an intention of a patient in real time when the rehabilitation exercise is in progress, other than a process of starting the rehabilitation exercise in a stop state in a rehabilitation exercise process, to enhance satisfaction with the rehabilitation exercise of the patient and a method therefor.</p><p id="p-0012" num="0011">The technical problems that may be solved by embodiments of the present disclosure are not limited to the aforementioned problems, and any other technical problems not mentioned herein will be clearly understood from the following description by those skilled in the art to which the present disclosure pertains.</p><p id="p-0013" num="0012">According to an embodiment of the present disclosure, a rehabilitation robot control apparatus may include a brainwave signal measuring device that measures a brainwave signal of a user, a preprocessing device that preprocesses the measured brainwave signal, a classification device that classifies a motor intention of the user based on the brainwave signal preprocessed by the preprocessing device, and a controller that reflects the motor intention of the user in real time to control an operation or stop of a rehabilitation robot.</p><p id="p-0014" num="0013">In an embodiment, the classification device may classify the motor intention of the user using a deep learning model based on a convolutional neural network (CNN), a recurrent neural network (RNN), or a long short term memory (LSTM), or a machine learning model based on linear discriminant analysis (LDA) or support vector machine (SVM).</p><p id="p-0015" num="0014">In an embodiment, the classification device may include a first decoder that classifies the motor intention of the user while the rehabilitation robot is operating and a second decoder that classifies the motor intention of the user while the rehabilitation robot is stopped.</p><p id="p-0016" num="0015">In an embodiment, the rehabilitation robot control apparatus may further include a display that displays a real-time classification state for the motor intention of the user.</p><p id="p-0017" num="0016">In an embodiment, the classification device may delete buffer data stored in the first decoder, when the rehabilitation robot changes from an operation state to a stop state, and may delete buffer data stored in the second decoder, when the rehabilitation robot changes from the stop state to the operation state.</p><p id="p-0018" num="0017">In an embodiment, the controller may operate the rehabilitation robot, when a step of classifying the motor intention of the user arrives at an upper threshold in a state where the rehabilitation robot is stopped, and may stop the rehabilitation robot, when the step of classifying the motor intention of the user arrives at a lower threshold in a state where the rehabilitation robot is operating.</p><p id="p-0019" num="0018">In an embodiment, the display may display whether the rehabilitation robot is in an operation state or a stop state in real time by means of a color.</p><p id="p-0020" num="0019">In an embodiment, the display may display a step of classifying the motor intention of the user in real time by means of a graph.</p><p id="p-0021" num="0020">In an embodiment, the preprocessing device may extract a brainwave characteristic based on a band power for one or more brainwave measurement channels.</p><p id="p-0022" num="0021">In an embodiment, the preprocessing device may preprocess the brainwave signal measured by means of a filter including a high pass filter, a band pass filter, and a notch filter.</p><p id="p-0023" num="0022">In an embodiment, the classification device may apply the brainwave signal preprocessed by means of conversion of extracting a brainwave characteristic to a previously learned learning model to classify the motor intention of the user.</p><p id="p-0024" num="0023">In an embodiment, the classification device may apply the brainwave signal converted and preprocessed into an image sequence to a previously learned learning model composed of a CNN or an LSTM to classify the motor intention of the user.</p><p id="p-0025" num="0024">According to an embodiment of the present disclosure, a rehabilitation robot control method may include measuring, by a brainwave signal measuring device, a brainwave signal of a user, preprocessing, by a preprocessing device, the measured brainwave signal, classifying, by a classification device, a motor intention of the user based on the brainwave signal preprocessed by the preprocessing device, and reflecting, by a controller, the motor intention of the user in real time to control an operation or stop of a rehabilitation robot.</p><p id="p-0026" num="0025">In an embodiment, the classifying of the motor intention of the user based on the brainwave signal preprocessed by the preprocessing device by the classification device may include classifying, by the classification device, the motor intention of the user using a deep learning model based on a CNN, an RNN, or an LSTM, or a machine learning model based on LDA or SVM.</p><p id="p-0027" num="0026">In an embodiment, the classifying of the motor intention of the user based on the brainwave signal preprocessed by the preprocessing device by the classification device may include classifying, by a first decoder, the motor intention of the user while the rehabilitation robot is operating and classifying, by a second decoder, the motor intention of the user while the rehabilitation robot is stopped.</p><p id="p-0028" num="0027">In an embodiment, the rehabilitation robot control method may further include displaying, by a display, whether the rehabilitation robot is in an operation state or a stop state in real time by means of a color and displaying, by the display, a step of classifying the motor intention of the user in real time by means of a graph.</p><p id="p-0029" num="0028">In an embodiment, the rehabilitation robot control method may further include deleting, by the classification device, buffer data stored in the first decoder, when the rehabilitation robot changes from an operation state to a stop state, and deleting, by the classification device, buffer data stored in the second decoder, when the rehabilitation robot changes from the stop state to the operation state.</p><p id="p-0030" num="0029">In an embodiment, the reflecting of the motor intention of the user in real time to control the operation or stop of the rehabilitation robot by the controller may include operating, by the controller, the rehabilitation robot, when a step of classifying the motor intention of the user arrives at an upper threshold in a state where the rehabilitation robot is stopped, and stopping, by the controller, the rehabilitation robot, when the step of classifying the motor intention of the user arrives at a lower threshold in a state where the rehabilitation robot is operating.</p><p id="p-0031" num="0030">In an embodiment, the classifying of the motor intention of the user based on the brainwave signal preprocessed by the preprocessing device by the classification device may include applying, by the classification device, the brainwave signal preprocessed by means of conversion of extracting a brainwave characteristic to a previously learned learning model to classify the motor intention of the user.</p><p id="p-0032" num="0031">In an embodiment, the classifying of the motor intention of the user based on the brainwave signal preprocessed by the preprocessing device by the classification device may include applying, by the classification device, the brainwave signal converted and preprocessed into an image sequence to a previously learned learning model composed of a CNN or an LSTM to classify the motor intention of the user.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0033" num="0032">The above and other objects, features and advantages of embodiments of the present disclosure will be more apparent from the following detailed description taken in conjunction with the accompanying drawings, in which:</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram illustrating a rehabilitation robot control apparatus according to an embodiment of the present disclosure;</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram illustrating a rehabilitation robot control apparatus according to another embodiment of the present disclosure;</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a drawing illustrating a relationship among a rehabilitation robot control apparatus, a user, and a rehabilitation robot according to an embodiment of the present disclosure;</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a table illustrating a process of classifying a motor intention of a user in a rehabilitation robot control apparatus according to an embodiment of the present disclosure;</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a drawing illustrating a process of classifying a motor intention of a user based on machine learning in a rehabilitation robot control apparatus according to an embodiment of the present disclosure;</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a drawing illustrating a process of classifying a motor intention of a user based on deep learning in a rehabilitation robot control apparatus according to an embodiment of the present disclosure;</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a drawing illustrating displaying a state where a rehabilitation robot control apparatus classifies a motor intention of a user on a display according to an embodiment of the present disclosure;</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIGS. <b>8</b>A and <b>8</b>B</figref> are drawings illustrating a process of calculating a state where a rehabilitation robot control apparatus classifies a motor intention of a user, which is displayed on a display, according to another embodiment of the present disclosure; and</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a flowchart illustrating a rehabilitation robot control method according to an embodiment of the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DESCRIPTION OF ILLUSTRATIVE EMBODIMENTS</heading><p id="p-0043" num="0042">Hereinafter, some embodiments of the present disclosure will be described in detail with reference to the exemplary drawings. In adding the reference numerals to the components of each drawing, it should be noted that the identical or equivalent component is designated by the identical numeral even when it is displayed on other drawings. Further, in describing the embodiments of the present disclosure, a detailed description of well-known features or functions will be omitted in order not to unnecessarily obscure the gist of the present disclosure.</p><p id="p-0044" num="0043">In describing the components of the embodiments according to the present disclosure, terms such as first, second, &#x201c;A&#x201d;, &#x201c;B&#x201d;, (a), (b), and the like may be used. These terms are merely intended to distinguish one component from another component, and the terms do not limit the nature, sequence or order of the constituent components. Unless otherwise defined, all terms used herein, including technical or scientific terms, have the same meanings as those generally understood by those skilled in the art to which the present disclosure pertains. Such terms as those defined in a generally used dictionary are to be interpreted as having meanings equal to the contextual meanings in the relevant field of art, and are not to be interpreted as having ideal or excessively formal meanings unless clearly defined as having such in the present application.</p><p id="p-0045" num="0044">Hereinafter, embodiments of the present disclosure will be described in detail with reference to <figref idref="DRAWINGS">FIGS. <b>1</b> to <b>9</b></figref>.</p><p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram illustrating a rehabilitation robot control apparatus according to an embodiment of the present disclosure.</p><p id="p-0047" num="0046">Referring to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the rehabilitation robot control apparatus <b>100</b> may include a brainwave signal measuring device <b>110</b>, a preprocessing device <b>120</b>, a classification device <b>130</b>, and a controller <b>140</b>.</p><p id="p-0048" num="0047">The brainwave signal measuring device <b>110</b> may measure a brainwave signal of a user.</p><p id="p-0049" num="0048">As an example, the brainwave signal measuring device <b>110</b> may measure the electro-encephalography (EEG) of the user in real time.</p><p id="p-0050" num="0049">As an example, the brainwave signal measuring device <b>110</b> may measure the EEG in real time by means of a neurophysiological measurement method of brain electrical activity through an electrode attached to a scalp of a user.</p><p id="p-0051" num="0050">As an example, the brainwave signal measuring device <b>110</b> may be directly or indirectly connected with the preprocessing device <b>120</b> through wireless or wired communication to transmit a brainwave signal of the user, which is measured in real time, to the preprocessing device <b>120</b>.</p><p id="p-0052" num="0051">The preprocessing device <b>120</b> may preprocess the measured brainwave signal.</p><p id="p-0053" num="0052">As an example, the preprocessing device <b>120</b> may remove a signal and noise, which are not associated with a process of classifying a motor intention of the user, from the measured brainwave signal.</p><p id="p-0054" num="0053">As an example, the preprocessing device <b>120</b> may extract a brainwave characteristic obtained by adding band powers for one or more brainwave measurement channels.</p><p id="p-0055" num="0054">As an example, the preprocessing device <b>120</b> may preprocess a brainwave signal by means of conversion of extracting a brainwave characteristic such as power spectral density (PSD).</p><p id="p-0056" num="0055">As an example, the preprocessing device <b>120</b> may apply characteristic conversion of an alpha wave, a beta wave, a gamma wave, a delta wave, a theta wave, or the like to the brainwave signal to perform preprocessing.</p><p id="p-0057" num="0056">The alpha wave may refer to a brainwave signal having a frequency of 8-13 Hz. The beta wave may refer to a brainwave signal having a frequency of 13-30 Hz. The gamma wave may refer to a brainwave signal having a frequency of 30-60 Hz. The delta wave may refer to a brainwave signal having a frequency of 1-4 Hz. The theta wave may refer to a brainwave signal having a frequency of 4-8 Hz.</p><p id="p-0058" num="0057">As an example, the preprocessing device <b>120</b> may preprocess a brainwave signal measured by means of a filter including a high pass filter, a band pass filter, and a notch filter.</p><p id="p-0059" num="0058">As an example, the preprocessing device <b>120</b> may be directly or indirectly connected with the classification device <b>130</b> through wireless or wired communication to transmit the result of preprocessing the brainwave signal to the classification device <b>130</b>.</p><p id="p-0060" num="0059">The classification device <b>130</b> may classify a motor intention of the user based on the brainwave signal preprocessed by the preprocessing device <b>120</b>.</p><p id="p-0061" num="0060">As an example, the classification device <b>130</b> may classify the motor intention of the user using a deep learning model based on a convolutional neural network (CNN), a recurrent neural network (RNN), or a long short term memory (LSTM), or a machine learning model based on linear discriminant analysis (LDA) or support vector machine (SVM).</p><p id="p-0062" num="0061">As an example, the classification device <b>130</b> may apply information where the brainwave signal is preprocessed to a previously learned learning model based on machine learning or deep learning, thus determining whether the user has a motor intention.</p><p id="p-0063" num="0062">As an example, the learning model based on the machine learning or the deep learning may be learned by collecting brainwave data for four modes (passive, active, rest, and motor imagery modes which will be described below with reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>) depending on a data collection experimental paradigm which is preset by an experimenter in a brainwave learning step.</p><p id="p-0064" num="0063">In the brainwave learning step, the experimenter may represent an accurate intention of the user every time in each mode depending on audio and visual instructions.</p><p id="p-0065" num="0064">As an example, the learning model may be learned by means of same-day experimental data or cross-day experimental data. When the learning model is learned based on the cross-day experimental data, a transfer learning scheme may be used.</p><p id="p-0066" num="0065">The cross-day experimental data may refer to data rather than experimental data composed of only data obtained on the day.</p><p id="p-0067" num="0066">As an example, the classification device <b>130</b> may apply the brainwave signal preprocessed by means of the conversion of extracting the brainwave characteristic to the previously learned learning model to classify the motor intention of the user.</p><p id="p-0068" num="0067">A description will be given of the process of classifying the motor intention of the user by means of the conversion of extracting the brainwave characteristic and the learning model in the classification device <b>130</b> with reference to <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0069" num="0068">As an example, the classification device <b>130</b> may apply the brainwave signal converted and preprocessed into an image sequence to the previously learned learning model composed of the CNN or the LSTM to classify the motor intention of the user.</p><p id="p-0070" num="0069">A description will be given of the process of classifying the motor intention of the user by means of the learning model based on the CNN or the LSTM with reference to <figref idref="DRAWINGS">FIG. <b>6</b></figref>.</p><p id="p-0071" num="0070">As an example, the classification device <b>130</b> may be directly or indirectly connected with the controller <b>140</b> through wireless or wired communication to transmit the result of classifying the motor intention of the user to the controller <b>140</b>.</p><p id="p-0072" num="0071">The controller <b>140</b> may reflect the motor intention of the user in real time to control an operation or stop of a rehabilitation robot.</p><p id="p-0073" num="0072">As an example, the controller <b>140</b> may control an operation of the rehabilitation robot which performs a rehabilitation exercise of the user.</p><p id="p-0074" num="0073">As an example, the rehabilitation robot may operate to perform an upper extremity rehabilitation exercise of a stroke patient or the like.</p><p id="p-0075" num="0074">The rehabilitation robot may be implemented in the form of an exoskeleton robot and may be mounted on an arm of a patient to operate to help activities of daily living (ADL) rehabilitation exercise.</p><p id="p-0076" num="0075">As an example, when it is determined that the user has a motor intention in a state where the rehabilitation robot is stopped, the controller <b>140</b> may operate the rehabilitation robot.</p><p id="p-0077" num="0076">As an example, when it is determined that the user does not have a motor intention in the state where the rehabilitation robot is stopped, the controller <b>140</b> may keep the rehabilitation robot stopped.</p><p id="p-0078" num="0077">As an example, when it is determined that the user has a motor intention in a state where the rehabilitation robot is operating, the controller <b>140</b> may keep the rehabilitation robot operating.</p><p id="p-0079" num="0078">As an example, when it is determined that the user does not have a motor intention in the state where the rehabilitation robot is operating, the controller <b>140</b> may stop the rehabilitation robot.</p><p id="p-0080" num="0079">As an example, when the step of classifying the motor intention of the user arrives at an upper threshold in the state where the rehabilitation robot is stopped, the controller <b>140</b> may operate the rehabilitation robot. When the step of classifying the motor intention of the user arrives at a lower threshold in the state where the rehabilitation robot is operating, the controller <b>140</b> may stop the rehabilitation robot.</p><p id="p-0081" num="0080">As an example, the classification device <b>130</b> may classify the step of the motor intention of the user into a predetermined number of steps and may calculate a step of the motor intention of the user in real time. When the calculated step of the motor intention arrives at a predetermined upper threshold step, the controller <b>140</b> may control to operate the rehabilitation robot. When the calculated step of the motor intention arrives at a predetermined lower threshold step, the controller <b>140</b> may control to stop the rehabilitation robot.</p><p id="p-0082" num="0081"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram illustrating a rehabilitation robot control apparatus according to another embodiment of the present disclosure.</p><p id="p-0083" num="0082">Referring to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, a rehabilitation robot control apparatus <b>200</b> may include a brainwave signal measuring device <b>210</b>, a preprocessing device <b>220</b>, a classification device <b>230</b>, a controller <b>240</b>, and a display <b>250</b>.</p><p id="p-0084" num="0083">Because the brainwave signal measuring device <b>210</b> and the preprocessing device <b>220</b> may be the same as the brainwave signal measuring device <b>110</b> and the preprocessing device <b>120</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, a description thereof will be omitted herein.</p><p id="p-0085" num="0084">The classification device <b>230</b> may include a first decoder <b>231</b> and a second decoder <b>232</b>.</p><p id="p-0086" num="0085">As an example, the classification device <b>230</b> may receive information about whether a rehabilitation robot is operating or stopped from the controller <b>240</b>.</p><p id="p-0087" num="0086">The first decoder <b>231</b> may classify a motor intention of a user while the rehabilitation robot is operating.</p><p id="p-0088" num="0087">As an example, while the rehabilitation robot is operating, the first decoder <b>231</b> may determine whether the user is in a state (an active state) where he or she has a motor intention or whether the user is in a state (a passive state) where he or she does not have the motor intention.</p><p id="p-0089" num="0088">As an example, to classify the active state or the passive state according to the motor intention of the user, the first decoder <b>231</b> may use a previously learned learning model based on machine learning or deep learning.</p><p id="p-0090" num="0089">The second decoder <b>232</b> may classify the motor intention of the user while the rehabilitation robot is stopped.</p><p id="p-0091" num="0090">As an example, while the rehabilitation robot is stopped, the second decoder <b>232</b> may determine whether the user is in a state (a motor imagery state) where he or she has a motor intention or whether the user is in a state (a rest state) where he or she does not have the motor intention.</p><p id="p-0092" num="0091">As an example, to classify the motor imagery state or the rest state according to the motor intention of the user, the second decoder <b>232</b> may use a previously learned learning model based on machine learning or deep learning.</p><p id="p-0093" num="0092">As an example, the classification device <b>230</b> may delete buffer data stored in the first decoder <b>231</b>, when the rehabilitation robot changes from an operation state to a stop state, and may delete buffer data stored in the second decoder <b>232</b>, when the rehabilitation robot changes from the stop state to the operation state.</p><p id="p-0094" num="0093">As an example, when the operation state (On/Off) of the rehabilitation robot changes, the classification device <b>230</b> may delete buffer data accumulated in the first decoder <b>231</b> or the second decoder <b>232</b> and may proceed with a new classification process using new data according to a changed brainwave state of the user.</p><p id="p-0095" num="0094">The display <b>250</b> may display a real-time classification state for the motor intention of the user.</p><p id="p-0096" num="0095">As an example, the display <b>250</b> may display whether the rehabilitation robot is in the operation state or the stop state in real time by means of a color.</p><p id="p-0097" num="0096">As an example, the display <b>250</b> may display a green color, when the rehabilitation robot is in the operation state, and may display a red color, when the rehabilitation robot is in the stop state.</p><p id="p-0098" num="0097">The user may intuitively identify the operation state of the rehabilitation robot by means of the color displayed on the display <b>250</b> and may reflect it to improve his or her motor intention.</p><p id="p-0099" num="0098">As an example, the display <b>250</b> may display a step of classifying the motor intention of the user in real time by means of a graph.</p><p id="p-0100" num="0099">As an example, the display <b>250</b> may display a step according to a real-time motor intention of the user among a predetermined number of classification steps by means of a graph.</p><p id="p-0101" num="0100">The user may intuitively identify his or her current rehabilitation motor intention by means of the graph displayed on the display <b>250</b> and may reflect it to improve his or her motor intention.</p><p id="p-0102" num="0101">As an example, the display <b>250</b> may display an operation or stop state of the rehabilitation robot by means of a color of the graph and may simultaneously display a classification step according to the motor intention of the user by means of the graph.</p><p id="p-0103" num="0102"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a drawing illustrating a relationship among a rehabilitation robot control apparatus, a user, and a rehabilitation robot according to an embodiment of the present disclosure.</p><p id="p-0104" num="0103">Referring to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, a user <b>301</b> may provide a rehabilitation robot control apparatus <b>302</b> with information about real-time EEG.</p><p id="p-0105" num="0104">The rehabilitation robot control apparatus <b>302</b> may perform preprocessing <b>303</b> for the real-time EEG signal of the user <b>301</b>.</p><p id="p-0106" num="0105">As an example, the rehabilitation robot control apparatus <b>302</b> may remove noise of the real-time EEG signal, may extract a signal characteristic, and may perform the preprocessing <b>303</b>.</p><p id="p-0107" num="0106">After performing the preprocessing <b>303</b> for the real-time EEG signal of the user <b>301</b>, the rehabilitation robot control apparatus <b>302</b> may perform classification <b>304</b> for a motor intention of the user <b>301</b>.</p><p id="p-0108" num="0107">As an example, the rehabilitation robot control apparatus <b>302</b> may perform the classification <b>304</b> for the motor intention of the user <b>301</b>, based on the EEG signal preprocessed by means of a previously learned learning model based on machine learning or deep learning.</p><p id="p-0109" num="0108">After performing the classification <b>304</b> for the motor intention of the user <b>301</b>, the rehabilitation robot control apparatus <b>302</b> may perform rehabilitation robot control <b>305</b>.</p><p id="p-0110" num="0109">As an example, the rehabilitation robot control apparatus <b>302</b> may control the On/Off state of a rehabilitation robot <b>307</b> depending on the result of performing the classification <b>304</b> of the motor intention of the user <b>301</b>.</p><p id="p-0111" num="0110">As an example, the rehabilitation robot control apparatus <b>302</b> may perform the rehabilitation robot control <b>305</b> and may display information about a classification state for the motor intention of the user <b>301</b> or information about an operation state or a stop state of the rehabilitation robot <b>307</b> to the user <b>301</b> to visually feed back the information to the user <b>301</b>.</p><p id="p-0112" num="0111">After performing the rehabilitation robot control <b>305</b>, the rehabilitation robot control apparatus <b>302</b> may perform buffer data deletion <b>306</b>.</p><p id="p-0113" num="0112">When the operation or stop state of the rehabilitation robot <b>307</b> changes, the rehabilitation robot control apparatus <b>302</b> may perform the buffer data deletion <b>306</b> to proceed with an analysis of new data.</p><p id="p-0114" num="0113">After performing the buffer data deletion <b>306</b>, the rehabilitation robot control apparatus <b>302</b> may perform preprocessing <b>303</b> for the real-time EEG signal of the user <b>301</b> again.</p><p id="p-0115" num="0114">When the rehabilitation robot <b>307</b> is controlled according to the motor intention of the user <b>301</b> by the rehabilitation robot control apparatus <b>302</b>, the user <b>301</b> may perform a rehabilitation exercise depending on an operation of the rehabilitation robot <b>307</b> or may receive feedback by means of stopping of the rehabilitation exercise.</p><p id="p-0116" num="0115">Thus, as a result, when controlling the rehabilitation robot <b>307</b> depending on the motor intention of the user <b>301</b>, the rehabilitation robot control apparatus <b>302</b> may receive feedback on the rehabilitation exercise of the user <b>301</b>, may enhance efficiency of rehabilitation treatment, and may enhance satisfaction of the user <b>301</b>.</p><p id="p-0117" num="0116"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a table illustrating a process of classifying a motor intention of a user in a rehabilitation robot control apparatus according to an embodiment of the present disclosure.</p><p id="p-0118" num="0117">Referring to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, a rehabilitation robot control apparatus <b>100</b>, <b>200</b>, or <b>302</b> of <figref idref="DRAWINGS">FIG. <b>1</b>, <b>2</b></figref>, or <b>3</b> may classify a motor intention of a user as a passive mode, an active mode, a rest mode, or a motor imagery (MI) mode.</p><p id="p-0119" num="0118">The passive mode may refer to a state where there is motion of a robot and there is no motor intention of the user.</p><p id="p-0120" num="0119">The active mode may refer to a state where there is motion of the robot and there is a motor intention of the user.</p><p id="p-0121" num="0120">The rest mode may refer to a state where there is no motion of the robot and there is no motor intention of the user.</p><p id="p-0122" num="0121">The MI mode may refer to a state where there is no motion of the robot and there is a motor intention of the user.</p><p id="p-0123" num="0122">According to the above-described four states, because a brainwave signal of the user is able to be formed in different patterns, the rehabilitation robot control apparatus <b>100</b>, <b>200</b>, or <b>302</b> may analyze the brainwave signal of the user to classify whether the brainwave signal is in any of the four states.</p><p id="p-0124" num="0123">The rehabilitation robot control apparatus <b>100</b>, <b>200</b>, or <b>302</b> may analyze the brainwave signal of the user in the state where there is motion of the robot to determine whether to continue the operation of the robot by means of a motor intention of the user, and may control a rehabilitation robot depending on the determined result.</p><p id="p-0125" num="0124">The rehabilitation robot control apparatus <b>100</b>, <b>200</b>, or <b>302</b> may analyze the brainwave signal of the user in the state where there is no motion of the robot to determine whether to start an operation of the robot by means of a motor intention of the user, and may control the rehabilitation robot depending on the determined result.</p><p id="p-0126" num="0125"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a drawing illustrating a process of classifying a motor intention of a user based on machine learning in a rehabilitation robot control apparatus according to an embodiment of the present disclosure.</p><p id="p-0127" num="0126">Step (i) of <figref idref="DRAWINGS">FIG. <b>5</b></figref> is a drawing illustrating that a brainwave signal of a user is not preprocessed.</p><p id="p-0128" num="0127">A rehabilitation robot control apparatus <b>100</b>, <b>200</b>, or <b>302</b> of <figref idref="DRAWINGS">FIG. <b>1</b>, <b>2</b></figref>, or <b>3</b> may preprocess a brainwave signal of a user by means of a band pass filter, a notch filter, and a high pass filter in the preprocessing process.</p><p id="p-0129" num="0128">Step (ii) of <figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates data into which the brainwave signal of the user is converted for each channel.</p><p id="p-0130" num="0129">The rehabilitation robot control apparatus <b>100</b>, <b>200</b>, or <b>302</b> may preprocess the brainwave signal of the user by means of conversion for time based on a predetermined window size and a degree to which a window is slid and overlapped.</p><p id="p-0131" num="0130">Step (iii) of <figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates a value calculated based on band powers of delta, theta, alpha, beta, and gamma after the brainwave signal of the user is converted for each channel.</p><p id="p-0132" num="0131">The rehabilitation robot control apparatus <b>100</b>, <b>200</b> or <b>302</b> may extract the sum of the band powers of delta, theta, alpha, beta, and gamma or the like as a brainwave characteristic, for each brainwave measurement channel.</p><p id="p-0133" num="0132">Step (iv) of <figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates one final value calculated based on the value calculated based on the band powers of delta, theta, alpha, beta, and gamma for each brainwave measurement channel.</p><p id="p-0134" num="0133">Step (v) of <figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates classifying a motor intention of the user by applying the one final value, calculated based on the value calculated based on the band powers of delta, theta, alpha, beta, and gamma for each brainwave measurement channel, as a brainwave characteristic, to a previously learned learning model.</p><p id="p-0135" num="0134">The rehabilitation robot control apparatus <b>100</b>, <b>200</b>, or <b>302</b> may learn the learning model based on a brainwave characteristic extracted by means of conversion of extracting the brainwave characteristic, such that it is possible to classify a brainwave depending on a motor intention.</p><p id="p-0136" num="0135">Herein, the window size and the degree to which the sliding window is overlapped may be determined as different random values.</p><p id="p-0137" num="0136"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a drawing illustrating a process of classifying a motor intention of a user based on deep learning in a rehabilitation robot control apparatus according to an embodiment of the present disclosure.</p><p id="p-0138" num="0137">Step (i) of <figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates a brainwave signal of a user for each channel.</p><p id="p-0139" num="0138">A rehabilitation robot control apparatus <b>100</b>, <b>200</b>, or <b>302</b> of <figref idref="DRAWINGS">FIG. <b>1</b>, <b>2</b></figref>, or <b>3</b> may preprocess a brainwave signal of the user by passing the brainwave signal of the user through a band pass filter, a notch filter, and a high pass filter in a preprocessing process and down sampling the brainwave signal.</p><p id="p-0140" num="0139">The rehabilitation robot control apparatus <b>100</b>, <b>200</b>, or <b>302</b> may preprocess the brainwave signal of the user based on a predetermined window size and a degree to which a window is slid and overlapped.</p><p id="p-0141" num="0140">Step (ii) of <figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates that the measured brainwave signal data is converted into an image sequence in a channel array status with a predetermined size.</p><p id="p-0142" num="0141">Step (iii) of <figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates an image sequence of a channel array status with a predetermined size for every predetermined number of time points.</p><p id="p-0143" num="0142">Step (iv) of <figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates applying an image sequence of a channel array status with a predetermined size for every predetermined number of time points to a deep learning model based on a CNN and an LSTM.</p><p id="p-0144" num="0143">The rehabilitation robot control apparatus <b>100</b>, <b>200</b>, or <b>302</b> may convert brainwave signal data measured on a predetermined number of channels into an image sequence of a channel array status with a predetermined size and may learn the deep learning model based on the CNN and the LSTM, such that it is possible to classify a brainwave depending on a motor intention.</p><p id="p-0145" num="0144">Herein, a window size, a degree to which a sliding window is overlapped, and a size of a channel array may be determined as different random values.</p><p id="p-0146" num="0145"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a drawing illustrating displaying a state where a rehabilitation robot control apparatus classifies a motor intention of a user on a display according to an embodiment of the present disclosure.</p><p id="p-0147" num="0146">Referring to <figref idref="DRAWINGS">FIG. <b>7</b></figref>, a rehabilitation robot control apparatus <b>100</b>, <b>200</b> or <b>302</b> of <figref idref="DRAWINGS">FIG. <b>1</b>, <b>2</b></figref>, or <b>3</b> may display an upper boundary line, a lower boundary line, a bar graph, and a robot operation state on its display.</p><p id="p-0148" num="0147">The rehabilitation robot control apparatus <b>100</b>, <b>200</b>, or <b>302</b> may display whether a rehabilitation robot is in an operation state or a stop state, by means of a color of the bar graph on the display.</p><p id="p-0149" num="0148">As an example, the rehabilitation robot control apparatus <b>100</b>, <b>200</b>, or <b>302</b> may display the bar graph in a green color, when the rehabilitation robot is in the operation state, and may display the bar graph in a red color, when the rehabilitation robot is in the stop state.</p><p id="p-0150" num="0149">As an example, the rehabilitation robot control apparatus <b>100</b>, <b>200</b>, or <b>302</b> may display the robot operation state by means of a character such as ON or OFF on the display.</p><p id="p-0151" num="0150">As an example, the rehabilitation robot control apparatus <b>100</b>, <b>200</b>, or <b>302</b> may differently display a height of the bar graph depending on a real-time motor intention of a user.</p><p id="p-0152" num="0151">As an example, the higher the motor intention of the user, the higher the rehabilitation robot control apparatus <b>100</b>, <b>200</b>, or <b>302</b> may display the height of the bar graph to be.</p><p id="p-0153" num="0152">As an example, the rehabilitation robot control apparatus <b>100</b>, <b>200</b>, or <b>302</b> may classify the motor intention of the user into a predetermined number of steps and may differently display the height of the bar graph depending on the classified steps.</p><p id="p-0154" num="0153">In a state where the rehabilitation exercise robot is stopped, when the red bar graph, the height of which is changed in real time, arrives at the upper boundary line, the rehabilitation robot control apparatus <b>100</b>, <b>200</b>, or <b>302</b> may operate the rehabilitation exercise robot, may change the color of the bar graph to a green color, and may display the robot operation state as ON.</p><p id="p-0155" num="0154">In a state where the rehabilitation exercise robot is operating, when the green bar graph, the height of which is changed in real time, arrives at the lower boundary line, the rehabilitation robot control apparatus <b>100</b>, <b>200</b>, or <b>302</b> may stop the rehabilitation exercise robot, may change the color of the bar graph to a red color, and may display the robot operation state as OFF.</p><p id="p-0156" num="0155">Although not illustrated, the rehabilitation robot control apparatus <b>100</b>, <b>200</b>, or <b>302</b> may represent the motor intention of the user or the operation or stop state of the robot in the form of another graph such as a pie chart.</p><p id="p-0157" num="0156"><figref idref="DRAWINGS">FIGS. <b>8</b>A and <b>8</b>B</figref> are drawings illustrating a process of calculating a state where a rehabilitation robot control apparatus classifies a motor intention of a user, which is displayed on a display, according to another embodiment of the present disclosure.</p><p id="p-0158" num="0157"><figref idref="DRAWINGS">FIG. <b>8</b>A</figref> illustrates that a rehabilitation robot control apparatus <b>100</b>, <b>200</b>, or <b>302</b> of <figref idref="DRAWINGS">FIG. <b>1</b>, <b>2</b></figref>, or <b>3</b> represents consecutive classification values by means of a cumulative sum.</p><p id="p-0159" num="0158">Herein, that the consecutive classification values are 0 refers to a state (a rest or passive mode) where there is no motor intention, and that the consecutive classification values are 1 refers to a state (an MI or active mode) where there is a motor intention.</p><p id="p-0160" num="0159">The cumulative sum may refer to a scheme of calculating a classification step in a method where the classification step increases because the value is accumulated when the classification values are consecutively 1 and where the classification step decreases because the accumulated value decreases when the classification values are consecutively 0.</p><p id="p-0161" num="0160">The rehabilitation robot control apparatus <b>100</b>, <b>200</b>, or <b>302</b> may output a classification value (a decoder classification value) on a periodic basis and may thus calculate the result of performing the cumulative sum of the classification values in real time.</p><p id="p-0162" num="0161">When the predetermined upper threshold is 3 and when the predetermined lower threshold is 0, the rehabilitation robot control apparatus <b>100</b>, <b>200</b>, or <b>302</b> may change the color displayed on the display to a green color and may operate a rehabilitation robot, as soon as the cumulative sum arrives at a predetermined threshold.</p><p id="p-0163" num="0162">As an example, the predetermined threshold may be determined as 3.</p><p id="p-0164" num="0163">The rehabilitation robot control apparatus <b>100</b>, <b>200</b>, or <b>302</b> may change the color displayed on the display to a red color and may stop the rehabilitation robot, as soon as the cumulative sum arrives at 0 again.</p><p id="p-0165" num="0164"><figref idref="DRAWINGS">FIG. <b>8</b>B</figref> illustrates that the rehabilitation robot control apparatus <b>100</b>, <b>200</b>, or <b>302</b> represents consecutive classification values by means of a sliding window.</p><p id="p-0166" num="0165">The sliding window may refer to a scheme which calculates a value, obtained by adding a predetermined of number of classification values which are recently calculated, as a classification step.</p><p id="p-0167" num="0166">As an example, the predetermined number may be determined as 3.</p><p id="p-0168" num="0167">The rehabilitation robot control apparatus <b>100</b>, <b>200</b>, or <b>302</b> may output a classification value on a periodic basis and may thus calculate a classification step calculated for the classification value in real time in a sliding window scheme.</p><p id="p-0169" num="0168">When the predetermined upper threshold is 3 and when the predetermined lower threshold is 0, the rehabilitation robot control apparatus <b>100</b>, <b>200</b>, or <b>302</b> may change the color displayed on the display to a green color and may operate the rehabilitation robot, as soon as the classification step calculated in the sliding window scheme arrives at 3.</p><p id="p-0170" num="0169">The rehabilitation robot control apparatus <b>100</b>, <b>200</b>, or <b>302</b> may change the color displayed on the display to a red color and may stop the rehabilitation robot, as soon as the classification step calculated again in the sliding window scheme arrives at 0 again.</p><p id="p-0171" num="0170">The rehabilitation robot control apparatus <b>100</b>, <b>200</b>, or <b>302</b> may display such a process on the display to provide the user with visual feedback.</p><p id="p-0172" num="0171"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a flowchart illustrating a rehabilitation robot control method according to an embodiment of the present disclosure.</p><p id="p-0173" num="0172">Referring to <figref idref="DRAWINGS">FIG. <b>9</b></figref>, the rehabilitation robot control method may include measuring (S<b>910</b>) a brainwave signal of a user, preprocessing (S<b>920</b>) the measured brainwave signal, classifying (S<b>930</b>) a motor intention of the user based on the preprocessed brainwave signal, and reflecting (S<b>940</b>) the motor intention of the user in real time to operate or stop a rehabilitation robot.</p><p id="p-0174" num="0173">The measuring (S<b>910</b>) of the brainwave signal of the user may be performed by a brainwave signal measuring device <b>110</b> or <b>210</b> of <figref idref="DRAWINGS">FIG. <b>1</b> or <b>2</b></figref>.</p><p id="p-0175" num="0174">The preprocessing (S<b>920</b>) of the measured brainwave signal may be performed by a preprocessing device <b>120</b> or <b>220</b> of <figref idref="DRAWINGS">FIG. <b>1</b> or <b>2</b></figref>.</p><p id="p-0176" num="0175">The classifying (S<b>930</b>) of the motor intention of the user based on the preprocessed brainwave signal may be performed by a classification device <b>130</b> or <b>230</b> of <figref idref="DRAWINGS">FIG. <b>1</b> or <b>2</b></figref>.</p><p id="p-0177" num="0176">As an example, the classifying (S<b>930</b>) of the motor intention of the user based on the preprocessed brainwave signal may include classifying, by the classification device <b>130</b> or <b>230</b>, the motor intention of the user using a deep learning model based on a CNN, an RNN, or an LSTM, or a machine learning model based on an LDA or an SVM.</p><p id="p-0178" num="0177">As an example, the classifying (S<b>930</b>) of the motor intention of the user based on the preprocessed brainwave signal may include classifying, by a first decoder <b>231</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the motor intention of the user while the rehabilitation robot is operating and classifying, by a second decoder <b>232</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the motor intention of the user while the rehabilitation robot is stopped.</p><p id="p-0179" num="0178">As an example, the classifying (S<b>930</b>) of the motor intention of the user based on the preprocessed brainwave signal may include applying, by the classification device <b>130</b> or <b>230</b>, a brainwave signal preprocessed by means of conversion of extracting a brainwave characteristic to a previously learned learning model to classify the motor intention of the user.</p><p id="p-0180" num="0179">As an example, the classifying (S<b>930</b>) of the motor intention of the user based on the preprocessed brainwave signal may include applying, by the classification device <b>130</b> or <b>230</b>, a brainwave signal converted into an image sequence to a previously learned learning model composed of a CNN or an LSTM to classify the motor intention of the user.</p><p id="p-0181" num="0180">The reflecting (<b>940</b>) of the motor intention of the user in real time to operate or stop the rehabilitation robot may be performed by a controller <b>140</b> or <b>240</b> of <figref idref="DRAWINGS">FIG. <b>1</b> or <b>2</b></figref>.</p><p id="p-0182" num="0181">As an example, the reflecting (<b>940</b>) of the motor intention of the user in real time to operate or stop the rehabilitation robot may include operating, by the controller <b>140</b> or <b>240</b>, the rehabilitation robot, when the step of classifying the motor intention of the user arrives at an upper threshold in the state where the rehabilitation robot is stopped, and stopping, by the controller <b>140</b> or <b>240</b>, the rehabilitation robot, when the step of classifying the motor intention of the user arrives at a lower threshold in the state where the rehabilitation robot is operating.</p><p id="p-0183" num="0182">As an example, the rehabilitation robot control method may further include displaying, by a display <b>250</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, whether the rehabilitation robot is in an operation state or a stop state by means of a color and displaying, by the display <b>250</b>, the step of classifying the motor intention of the user by means of a graph.</p><p id="p-0184" num="0183">As an example, the rehabilitation robot control method may further include deleting, by the classification device <b>130</b> or <b>230</b>, buffer data stored in the first decoder <b>231</b>, when the rehabilitation robot changes from the operation state to the stop state, and deleting, by the classification device <b>130</b> or <b>230</b>, buffer data stored in the second decoder <b>232</b>, when the rehabilitation robot changes from the stop state to the operation state.</p><p id="p-0185" num="0184">The operations of the method or the algorithm described in connection with the embodiments disclosed herein may be embodied directly in hardware or a software module executed by the processor or in a combination thereof. The software module may reside on a storage medium (that is, the memory/or the storage) such as a RAM, a flash memory, a ROM, an EPROM, an EEPROM, a register, a hard disk, a removable disk, and a CD-ROM.</p><p id="p-0186" num="0185">The exemplary storage medium may be coupled to the processor, and the processor may read information out of the storage medium and may record information in the storage medium. Alternatively, the storage medium may be integrated with the processor. The processor and the storage medium may reside in an application specific integrated circuit (ASIC). The ASIC may reside within a user terminal. In another case, the processor and the storage medium may reside in the user terminal as separate components.</p><p id="p-0187" num="0186">A description will be given of effects of the rehabilitation robot control apparatus and the method thereof according to an embodiment of the present disclosure.</p><p id="p-0188" num="0187">According to at least one embodiment of the present disclosure, the apparatus and the method may be provided to control the rehabilitation robot based on the brain computer interface (BCI).</p><p id="p-0189" num="0188">According to at least one embodiment of the present disclosure, the rehabilitation robot control apparatus and the method thereof may be provided to analyze a brainwave of a patient and derive a more effective treatment effect by means of rehabilitation treatment in which an intention of the patient is reflected.</p><p id="p-0190" num="0189">Furthermore, according to at least one embodiment of the present disclosure, the rehabilitation robot control apparatus and the method thereof may be provided to analyze a motor intention of a patient and continue a rehabilitation exercise with suitable strength, when the patient shows an active will for the rehabilitation exercise, to optimize an effect of the rehabilitation exercise.</p><p id="p-0191" num="0190">Furthermore, according to at least one embodiment of the present disclosure, the rehabilitation robot control apparatus and the method thereof may be provided to control a rehabilitation exercise based on an intention of a patient in real time when the rehabilitation exercise is in progress, other than a process of starting the rehabilitation exercise in a stop state in a rehabilitation exercise process, thus enhancing satisfaction with the rehabilitation exercise of the patient.</p><p id="p-0192" num="0191">In addition, various effects ascertained directly or indirectly through the present disclosure may be provided.</p><p id="p-0193" num="0192">Hereinabove, although the present disclosure has been described with reference to exemplary embodiments and the accompanying drawings, the present disclosure is not limited thereto, but may be variously modified and altered by those skilled in the art to which the present disclosure pertains without departing from the spirit and scope of the present disclosure claimed in the following claims.</p><p id="p-0194" num="0193">Therefore, the exemplary embodiments of the present disclosure are provided to explain the spirit and scope of the present disclosure, but not to limit them, so that the spirit and scope of the present disclosure is not limited by the embodiments. The scope of the present disclosure should be construed on the basis of the accompanying claims, and all the technical ideas within the scope equivalent to the claims should be included in the scope of the present disclosure.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A rehabilitation robot control apparatus comprising:<claim-text>a brainwave signal measuring device configured to measure a brainwave signal of a user;</claim-text><claim-text>a preprocessing device configured to preprocess the measured brainwave signal;</claim-text><claim-text>a classification device configured to classify a motor intention of the user based on the brainwave signal preprocessed by the preprocessing device; and</claim-text><claim-text>a controller configured to reflect the motor intention of the user in real time to control an operation or a stop of a rehabilitation robot.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The rehabilitation robot control apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the classification device is configured to classify the motor intention of the user using a deep learning model based on a convolutional neural network (CNN), a recurrent neural network (RNN), or a long short term memory (LSTM), or a machine learning model based on linear discriminant analysis (LDA) or support vector machine (SVM).</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The rehabilitation robot control apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the classification device comprises:<claim-text>a first decoder configured to classify the motor intention of the user during an operation state of the rehabilitation robot; and</claim-text><claim-text>a second decoder configured to classify the motor intention of the user during a stop state of the rehabilitation robot.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The rehabilitation robot control apparatus of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the classification device is configured to delete buffer data stored in the first decoder in response to the rehabilitation robot changing from the operation state to the stop state, and to delete buffer data stored in the second decoder in response to the rehabilitation robot changing from the stop state to the operation state.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The rehabilitation robot control apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the controller is configured to operate the rehabilitation robot in response to a step of classifying the motor intention of the user arriving at an upper threshold in a state where the rehabilitation robot is stopped, and to stop the rehabilitation robot in response to the step of classifying the motor intention of the user arriving at a lower threshold in a state where the rehabilitation robot is operating.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The rehabilitation robot control apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising a display configured to display a real-time classification state for the motor intention of the user.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The rehabilitation robot control apparatus of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the display is configured to display whether the rehabilitation robot is in an operation state or a stop state in real time using a color system.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The rehabilitation robot control apparatus of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the display is configured to display a step of classifying the motor intention of the user in real time using a graph.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The rehabilitation robot control apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the preprocessing device is configured to extract a brainwave characteristic based on a band power for one or more brainwave measurement channels.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The rehabilitation robot control apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the preprocessing device is configured to preprocess the measured brainwave signal using a filter comprising a high pass filter, a band pass filter, and a notch filter.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The rehabilitation robot control apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the classification device is configured to apply the preprocessed brainwave signal preprocessed by a process of conversion of extracting a brainwave characteristic to a previously learned learning model to classify the motor intention of the user.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The rehabilitation robot control apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the classification device is configured to apply the brainwave signal converted and preprocessed into an image sequence to a previously learned learning model comprising a convolutional neural network (CNN) or a long short term memory (LSTM) to classify the motor intention of the user.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. A rehabilitation robot control method, the method comprising:<claim-text>measuring a brainwave signal of a user;</claim-text><claim-text>preprocessing the measured brainwave signal;</claim-text><claim-text>classifying a motor intention of the user based on the preprocessed brainwave signal; and</claim-text><claim-text>reflecting the motor intention of the user in real time to control an operation or a stop of a rehabilitation robot.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein classifying the motor intention of the user comprises classifying the motor intention of the user using a deep learning model based on a convolutional neural network (CNN), a recurrent neural network (RNN), or a long short term memory (LSTM), or a machine learning model based on linear discriminant analysis (LDA) or support vector machine (SVM).</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein classifying the motor intention of the user comprises:<claim-text>classifying, by a first decoder, the motor intention of the user while the rehabilitation robot is operating; and</claim-text><claim-text>classifying, by a second decoder, the motor intention of the user while the rehabilitation robot is stopped.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The method of <claim-ref idref="CLM-00015">claim 15</claim-ref>, further comprising:<claim-text>deleting first buffer data stored in the first decoder in response to the rehabilitation robot changing from an operation state to a stop state; and</claim-text><claim-text>deleting second buffer data stored in the second decoder in response to the rehabilitation robot changing from the stop state to the operation state.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, further comprising:<claim-text>displaying whether the rehabilitation robot is in an operation state or a stop state in real time using a color system; or</claim-text><claim-text>displaying a step of classifying the motor intention of the user in real time using a graph.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein reflecting the motor intention of the user in real time to control the operation or the stop of the rehabilitation robot comprises:<claim-text>operating the rehabilitation robot in response to a step of classifying the motor intention of the user arriving at an upper threshold in a state where the rehabilitation robot is stopped; and</claim-text><claim-text>stopping the rehabilitation robot in response to the step of classifying the motor intention of the user arriving at a lower threshold in a state where the rehabilitation robot is operating.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein classifying the motor intention of the user comprises applying the brainwave signal preprocessed by conversion of extracting a brainwave characteristic to a previously learned learning model to classify the motor intention of the user.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein classifying the motor intention of the user comprises applying the brainwave signal converted and preprocessed into an image sequence to a previously learned learning model comprising a convolutional neural network (CNN) or a long short term memory (LSTM) to classify the motor intention of the user.</claim-text></claim></claims></us-patent-application>