<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004168A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004168</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17857457</doc-number><date>20220705</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>DE</country><doc-number>10 2021 117 311.6</doc-number><date>20210705</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>05</class><subclass>D</subclass><main-group>1</main-group><subgroup>02</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>05</class><subclass>D</subclass><main-group>1</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>05</class><subclass>D</subclass><main-group>1</main-group><subgroup>0248</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>05</class><subclass>D</subclass><main-group>1</main-group><subgroup>0257</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>05</class><subclass>D</subclass><main-group>1</main-group><subgroup>0088</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">Control and Navigation Device for an Autonomously Moving System and Autonomously Moving System</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Spleenlab GmbH</orgname><address><city>Saalburg-Ebersdorf</city><country>DE</country></address></addressbook><residence><country>DE</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>&#xd6;lsner</last-name><first-name>Florian</first-name><address><city>Jena</city><country>DE</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Lewandowski</last-name><first-name>Benjamin</first-name><address><city>Erfurt</city><country>DE</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Hagen</last-name><first-name>Chris</first-name><address><city>Bad Lobenstein</city><country>DE</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Ammapalayam Ravichandran</last-name><first-name>Ashwanth</first-name><address><city>Bad Lobenstein</city><country>DE</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>Milz</last-name><first-name>Stefan</first-name><address><city>Saalburg-Ebersdorf</city><country>DE</country></address></addressbook></inventor><inventor sequence="05" designation="us-only"><addressbook><last-name>Karthigeyan</last-name><first-name>Sukumar</first-name><address><city>Bad Lobenstein</city><country>DE</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">The invention relates to a control and navigation device for an autonomously moving system, which comprises the following: a sensor device, which is configured to acquire sensor data, and for this purpose a LiDAR sensor installation, which is configured for 360-degree acquisition; a fisheye camera installation, which is configured for 360-degree acquisition; and a radar sensor installation, which is configured for 360-degree acquisition; a data processing installation with an AI-based software application, which is configured to determine control signals for purposes of navigating an autonomously moving system by means of processing of the sensor data; and a data communication interface, which is connected to the data processing installation, and is configured to provide the control signals for transmission to a controller of the autonomously moving system. The sensor device, the data processing installation, and the data communication interface are arranged at an assembly component, which is configured to assemble, in a detachable manner, the sensor device, the data processing installation, and the data communication interface together as a common module at the autonomously moving system. Furthermore, an autonomously moving system is provided.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="110.07mm" wi="132.08mm" file="US20230004168A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="130.81mm" wi="134.03mm" file="US20230004168A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="175.01mm" wi="126.24mm" orientation="landscape" file="US20230004168A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">FIELD OF THE INVENTION</heading><p id="p-0002" num="0001">The invention relates to a control and navigation device for an autonomously moving system and an autonomously moving system.</p><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">Systems are understood to act autonomously if they can act without direct human instruction, solve complex tasks, make decisions, learn independently and react to unforeseen events. In the case of an autonomously moving system, the movement or local displacement takes place without direct human instruction, i.e., based on machine-determined control and navigation signals.</p><p id="p-0004" num="0003">In addition to classic industrial and service robots, means of transport in particular are achieving an ever-higher level of automation. The mobility of the future will be determined by autonomous vehicles and aircraft. In addition to autonomously driving cars, autonomously flying aircraft will also come into use, both as unmanned drones and as passenger-carrying air taxis. Furthermore, other means of transport such as ships will also be able to navigate and travel unmanned in the future.</p><p id="p-0005" num="0004">However, such autonomous systems do not necessarily have to be self-sufficient in terms of information acquisition. The autonomous systems most commonly encountered today, such as unmanned aerial vehicles (UAVs), do not exclusively use their own sensor and computing units, but also depend on other sources of information such as a data cloud, an IT infrastructure, or the data of other aircraft or vehicles. However, the dependence on these external sources of information entails the risk that, in cases where the connection to the data sources is interrupted or is not even possible in the first place, the autonomously operating systems will not be able to fulfil their task and may not reach their target.</p><p id="p-0006" num="0005">At the present time there are various approaches that can give motor vehicles, unmanned aerial vehicles or other automata as high a degree of autonomy as possible. Furthermore, there are approaches to the retrofitting of existing vehicles and aircraft with a sensor and computing unit that can give these existing systems an autonomy that they do not possess in their original design.</p><p id="p-0007" num="0006">For example, there are systems of known art in which the sensor technology and computing unit can be installed in a system such as a vehicle or aircraft, so that the sensor and computing technologies do not have to have been originally installed in the autonomous systems. The documents WO 2018/140701 A1 and WO 2019/018315 A1 describe a module that can be attached to drones. The modules have access to a plurality of sensors such as LiDAR scanners and cameras.</p><p id="p-0008" num="0007">In the document WO 2019/222810 A1, a mapping and control system is disclosed, which can also be attached to an aircraft as a payload. This system uses various sensor technologies, such as GPS, LiDAR scanners and cameras. Furthermore, the system has a memory for the storage of flight plan data, as well as a communication interface. The latter establishes a connection to an external processing device, with which the position data and flight plan data are evaluated in order to generate control instructions for the aircraft, which data are then transmitted back to a control system of the aircraft via the communication interface.</p><p id="p-0009" num="0008">The document WO 2020/014740 A1 describes a method for the reconnaissance and mapping of an environment, wherein the method is performed using an aircraft and a user processing system that communicates wirelessly with the aircraft when the aircraft is within its communication range.</p><p id="p-0010" num="0009">The disadvantage of these solutions, however, is that if the wireless communication connection is interrupted, or in environments where such a connection cannot be established in the first place, autonomous flight is made significantly more difficult or, depending on the data situation, even impossible. In particular, in regions that are difficult to access, and which are not equipped with a wireless connection over a wide area, an autonomous flight would therefore not be feasible. The aircraft would therefore be limited to regions or local conditions where the infrastructure is already well developed. However, it is all the more crucial for areas that are difficult to access to be reached by unmanned aerial vehicles or other autonomous systems, for example, in order to be able to provide important supply services.</p><p id="p-0011" num="0010">In the document U.S. Pat. No. 10,489,976 B2, a drone-based system for the investigation and man-agement of traffic accidents is disclosed, in which various sensors can be used.</p><p id="p-0012" num="0011">The processing of the acquired data for the traffic accident should also be possible by means of a neural network.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0013" num="0012">It is the object of the invention to create a control and navigation device that can be used flexibly in various autonomously moving systems.</p><p id="p-0014" num="0013">The object is achieved by a control and navigation device for an autonomously moving system, and an autonomously moving system, according to the independent claims <b>1</b> and <b>7</b>. Configurations are the subject matter of dependent subsidiary claims.</p><p id="p-0015" num="0014">In accordance with one aspect, a control and navigation device for an autonomously moving system is created, with a sensor device that is configured to acquire sensor data, and for this purpose has the following: a LiDAR sensor device that is configured for 360-degree acquisition; a fisheye camera device that is configured for 360-degree acquisition; and a radar sensor device that is configured for 360-degree acquisition. The control and navigation device furthermore comprises the following: a data processing device with an AI-based software application, which by means of processing of the sensor data is configured to determine control signals for navigating an autonomously moving system; and a data communication interface, which is connected to the data processing device and is configured to provide the control signals for transmission to a controller of the autonomously moving system. The sensor device, the data processing device, and the data communication interface are arranged on an assembly component, which is configured in a detachable manner to assemble together the sensor device, the data processing device, and the data communication interface as a common module on the autonomously moving system.</p><p id="p-0016" num="0015">According to a further aspect, an autonomously moving system is created, in which the control and navigation device is assembled in a detachable manner on a system body.</p><p id="p-0017" num="0016">The embodiment of the control and navigation device with the assembly component, on which all module or device components are arranged, enables a flexible assembly and disas-sembly on autonomously moving systems of different designs. In particular, it is also possible to retrofit the autonomously moving system with the control and navigation device for autonomous control. The components required for detecting and processing the sensor signals can be assembled together as a build unit or module with the control and navigation device.</p><p id="p-0018" num="0017">The sensor device, the data processing device, and the data communication interface can be arranged in and/or on a common device housing formed on the assembly component.</p><p id="p-0019" num="0018">The sensor device can furthermore have the following: a near-infrared sensor device, which is configured for 360-degree acquisition; and a GPS sensor device.</p><p id="p-0020" num="0019">The sensor device is connected to the data processing device via a data bus.</p><p id="p-0021" num="0020">The control and navigation device has access to an integrated artificial intelligence, for example by means of a neural network. The data processing device can be configured by means of the AI-based software application to execute the data processing processes (on board the autonomous moving system) necessary for the navigation and control of the autonomous system, independently of other computing units external to the control and navigation device.</p><p id="p-0022" num="0021">The neural network can be designed to process all signals acquired by the sensor device such that a uniform 3D-environment map is projected from the different data sets. The indi-vidual sensor technologies, such as the LiDAR scanner, fisheye camera and radar, supply different image and signal data for the environment of the autonomous moving system, the quality of which depends on external factors such as visibility conditions, weather and time of day. Using the AI-based software application, a 3D-map of the environment is determined from the application with the aid of data fusion, which map forms the basis for navigation and control of the autonomously moving system.</p><p id="p-0023" num="0022">On the basis of the fused data, the AI-based software application determines a possible and advantageous route for the prescribed movement plan or flight plan.</p><p id="p-0024" num="0023">For this purpose, control instructions are generated, which are transmitted to the control system of the autonomous system via the communication interface. The control system is prompted to execute calculated manoeuvres, and thereby autonomously to execute the stored movement plan.</p><p id="p-0025" num="0024">The various types of sensors of the sensor device supply diverse sensor data, which com-plement each other depending on the environment and external conditions. By means of the AI-based software application, the diverse sensor data can be fused into a consistent 3D-environment map, which provides a most accurate image of the environment possible, by virtue of the variety of the diverse data.</p><p id="p-0026" num="0025">Depending on the area of application and configuration, the control and navigation device is suitable for use in autonomously flying aircraft, autonomously driving land vehicles, autonomously travelling water vehicles, and autonomously operating robots. The control and navigation device can even be attached to existing systems and be connected by means of the communication interface.</p><p id="p-0027" num="0026">The control and navigation device can include one or a plurality of measures to reduce weight (further). These include, for example, the use of FPGA chips for the hardware, on which the AI-based software application runs. A flat bus system can also be used for data communication, for example a flat ribbon bus system. The LiDAR sensor device can be designed as a LiDAR solid state system. The fisheye camera device can be designed in a weight-reduced form, without a cooling housing, for example.</p><p id="p-0028" num="0027">The control and navigation device can be embodied as an overall unit or common module (with the assembly component) with a total weight of not more than about 1 kg.</p><p id="p-0029" num="0028">The control and navigation device (with the assembly component) can have space-saving maximum external dimensions (L&#xd7;W&#xd7;H) of approximately 25 cm&#xd7;25 cm&#xd7;25 cm as an overall build unit or common module.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0030" num="0029">In what follows further examples of embodiment are explained with reference to figures of an illustration. Here:</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows in perspective a schematic illustration of a control and navigation device for an autonomously moving system, and</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows a schematic illustration of functional components of the control and navigation device.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS</heading><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows in perspective a schematic illustration of a control and navigation device <b>1</b> for an autonomously moving system with a housing <b>2</b>. In the form of embodiment shown, the housing <b>2</b> accommodates all functional components of the control and navigation device <b>1</b>. The control and navigation device <b>1</b> can thus be assembled and disassembled as a complete module on the autonomously moving system (not illustrated).</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows a schematic illustration of the functional components of the control and navigation device <b>1</b>. These include a sensor device <b>10</b> having: a LiDAR sensor device <b>11</b>, which is configured for 360-degree acquisition; a fisheye camera device <b>12</b>, which is configured for 360-degree acquisition; a radar sensor device <b>13</b>, which is configured for 360-degree acquisition; a near-infrared sensor device <b>14</b>, which is configured for 360-degree acquisition; and a GPS sensor device <b>15</b>. With the aid of the sensor device <b>10</b>, sensor signals or data are acquired for the environment of the autonomously moving system, on which the control and navigation device <b>1</b> is mounted.</p><p id="p-0035" num="0034">In the control and navigation device <b>1</b>, the LiDAR sensor device <b>11</b> for 360-degree acquisition is arranged in a lower region, as shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. The fisheye camera device <b>12</b> has a fisheye camera <b>12</b><i>a</i>, <b>12</b><i>b </i>(hidden) on respectively opposite sides <b>3</b><i>a</i>, <b>3</b><i>b </i>for 360-acquisition. In a comparable manner, the near-infrared sensor device <b>14</b> is designed with near-infrared sensors <b>14</b><i>a</i>, <b>14</b><i>b </i>in the observation direction B on a front face <b>4</b><i>a </i>and a rear face <b>4</b><i>b. </i></p><p id="p-0036" num="0035">A radar sensor <b>13</b><i>a</i>, <b>13</b><i>b </i>is respectively arranged above the fisheye camera <b>12</b><i>a </i>and the near-infrared sensor <b>14</b><i>a</i>. Further radar sensors are provided in a comparable manner on the opposite face <b>3</b><i>b </i>as well as on the rear face <b>14</b><i>b. </i></p><p id="p-0037" num="0036">Cable openings <b>2</b><i>a </i>are designed for guiding cables out of the housing <b>2</b>.</p><p id="p-0038" num="0037">By means of a data processing device <b>16</b> with one or a plurality of processors, the sensor data is processed (cf. <figref idref="DRAWINGS">FIG. <b>2</b></figref>) so as to generate control signals for navigating the autonomously moving system, and to provide them at a data communication interface <b>17</b>, so that the control signals can be transmitted via a data bus <b>18</b> to a controller <b>19</b> of the autonomously moving system, in order to navigate the latter in the environment.</p><p id="p-0039" num="0038">By means of an appropriate hardware and software configuration, the data processing device <b>16</b> implements a plurality of functional components or modules: a sensor fusion module <b>20</b>, a perception and localisation module <b>21</b>, a module for determining an environment model <b>22</b>, a path planning module <b>22</b><i>a</i>, and an AI-based perception module <b>23</b>.</p><p id="p-0040" num="0039">The sensor fusion module <b>20</b> is used to process or prepare the measurement or sensor signals acquired by means of the sensor device <b>10</b> for subsequent (further) processing. It may be necessary to calibrate the measurement or sensor signals spatially and/or temporally. Measurement or sensor signals can be located in a common coordinate system, for example the coordinate system of the autonomously moving system, for example for a land vehicle, a water vehicle or an aircraft, by means of calibration methods known per se. In terms of time, measurement or sensor signals can be calibrated consistently in time to a common clock.</p><p id="p-0041" num="0040">Alternatively or additionally, monitoring and/or an artefact reduction can be executed. For purposes of artefact reduction, for example, low-level processing can be provided, for example to reduce false positive detections with the LiDAR sensor device <b>11</b> in the event of fog and/or spray water. The low-level processing can be executed using deterministic processing. Another example of an artefact reduction is a so-called balancing, so as to reduce the disadvantageous effects of extreme backlighting in the image recordings of the fisheye camera device <b>12</b>.</p><p id="p-0042" num="0041">One or a plurality of raw data streams can be generated for the measurement or sensor signals. Provision can be made to separate one or a plurality of raw data streams for security reasons. In one embodiment, fused raw data streams are provided and used exclusively for processing in the AI-based perception module <b>23</b>.</p><p id="p-0043" num="0042">In the AI-based perception module <b>23</b>, data received from the sensor fusion module <b>20</b> is analysed using AI-based algorithms. In particular, this may relate to data originating from the measurement or sensor signals of the following sensor devices: the LiDAR sensor device <b>11</b>, the fisheye camera device <b>12</b>, and/or radar sensors <b>13</b><i>a</i>, <b>13</b><i>b</i>. Here, a neural network can be used for each module, which, starting from the data based on the sensor signals, respectively predicts classification, geometric environment (in particular distances of objects), and/or dy-namic environment (in particular the movement or non-movement of the objects) of the autonomously moving system.</p><p id="p-0044" num="0043">The environment perception in the perception and localisation module <b>21</b> is based on localisation (own position), geometric environment (distances of objects, for example LiDAR point cloud), classification (type of objects) and dynamics (movement of objects or static).</p><p id="p-0045" num="0044">A movement path for the autonomously moving system is then determined by means of the path planning module <b>22</b><i>a</i>, based on a spatial environment model, which was previously determined with the aid of the module for determining an environment model <b>22</b>, in order to transmit data indicating the movement path to the controller <b>19</b> of the autonomously moving system via the data communication interface <b>17</b>.</p><p id="p-0046" num="0045">Data and signals can be exchanged with one or a plurality of external devices <b>25</b> via a data interface <b>24</b>.</p><p id="p-0047" num="0046">The features disclosed in the above description, in the claims, and in the figures, can be of importance for the implementation of the various embodiments, both individually and in any combination.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A control and navigation device for an autonomously moving system, the device comprising:<claim-text>a sensor device, which is configured to acquire sensor data, and for this purpose has the following:</claim-text><claim-text>a LiDAR sensor device, which is configured for 360-degree acquisition;</claim-text><claim-text>a fisheye camera device, which is configured for 360-degree acquisition; and</claim-text><claim-text>a radar sensor device, which is configured for 360-degree acquisition;</claim-text><claim-text>a data processing device with an AI-based software application, which is configured to determine control signals for navigating an autonomously moving system by means of processing of the sensor data; and</claim-text><claim-text>a data communication interface, which is connected to the data processing device and is configured to provide the control signals for transmission to a control device of the autonomously moving system;</claim-text><claim-text>wherein the sensor device, the data processing device, and the data communication interface are arranged at an assembly component, which is configured to assemble, in a detachable manner, the sensor device, the data processing device, and the data communication interface together as a common module at the autonomously moving system.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The control and navigation device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the sensor device, the data processing device, and the data communication interface are arranged in and/or at a common device housing formed at the assembly component.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The control and navigation device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the sensor device furthermore comprises the following: a near-infrared sensor device, which is configured for 360-degree acquisition, and a GPS sensor device.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The control and navigation device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the sensor device is connected to the data processing device via a data bus.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. An autonomously moving system, in which a control and navigation device according to at least one of the preceding <claim-ref idref="CLM-00001">claim 1</claim-ref> is assembled in a detachable manner on a system body.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The autonomously moving system according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, selected from the following group: an autonomously flying aircraft, an autonomously driving land vehicle, an autonomously travelling water vehicle, and an autonomously moving robot.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. A method for controlling an autonomously moving system, the method comprising:<claim-text>acquiring sensor data with a sensor device, the sensor device including:<claim-text>a LiDAR sensor device, which is configured for 360-degree acquisition;</claim-text><claim-text>a fisheye camera device, which is configured for 360-degree acquisition; and</claim-text><claim-text>a radar sensor device, which is configured for 360-degree acquisition;</claim-text></claim-text><claim-text>determining control signals for navigating an autonomously moving system by processing the sensor data with an AI-based software application with a data processing device; and</claim-text><claim-text>transmitting, with a data communication interface, the control signals to a control device of the autonomously moving system;</claim-text><claim-text>wherein the sensor device, the data processing device, and the data communication interface are arranged at an assembly component, which is configured to assemble, in a detachable manner, the sensor device, the data processing device, and the data communication interface together as a common module at the autonomously moving system.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the sensor device, the data processing device, and the data communication interface are arranged in and/or at a common device housing formed at the assembly component.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the sensor device furthermore comprises the following: a near-infrared sensor device, which is configured for 360-degree acquisition, and a GPS sensor device.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the sensor device is connected to the data processing device via a data bus.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, further comprising assembling a control and navigation device in a detachable manner on a system body.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the autonomously moving system is selected from the following group: an autonomously flying aircraft, an autonomously driving land vehicle, an autonomously travelling water vehicle, and an autonomously moving robot.</claim-text></claim></claims></us-patent-application>