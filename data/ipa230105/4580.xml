<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004581A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004581</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17734153</doc-number><date>20220502</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>28</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>5</main-group><subgroup>02</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20190101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>285</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>5</main-group><subgroup>022</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">Computer-Implemented Method for Improving Classification of Labels and Categories of a Database</invention-title><us-related-documents><us-provisional-application><document-id><country>US</country><doc-number>63216070</doc-number><date>20210629</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Naver Corporation</orgname><address><city>Gyeonggi-do</city><country>KR</country></address></addressbook><residence><country>KR</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>LAGOS</last-name><first-name>Nikolaos</first-name><address><city>Grenoble</city><country>FR</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>CALAPODESCU</last-name><first-name>Ioan</first-name><address><city>Grenoble</city><country>FR</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>LEE</last-name><first-name>JinHee</first-name><address><city>Seoul</city><country>KR</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>AIT-MOKHTAR</last-name><first-name>Salah</first-name><address><city>MontBonnot-Saint-Martin</city><country>FR</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>Naver Corporation</orgname><role>03</role><address><city>Gyeonggi-do</city><country>KR</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">There is disclosed a method for using a computer to enable correction of misclassified labels in a database. The computer initially applies a dataset (including labels pointing respectively to categories) to a first classifier, which includes a first loss function. Pursuant to the initial application, the computer determines that one or more labels have been misclassified. Responsive to such determination, the computer changes the first loss function to a second loss function to form a second classifier including the second loss function. The computer then applies the dataset to the second classifier for enabling correction of the one or more misclassified labels.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="139.28mm" wi="158.75mm" file="US20230004581A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="148.59mm" wi="165.52mm" file="US20230004581A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="189.23mm" wi="134.87mm" orientation="landscape" file="US20230004581A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="199.64mm" wi="126.66mm" orientation="landscape" file="US20230004581A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="199.81mm" wi="127.17mm" orientation="landscape" file="US20230004581A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="240.03mm" wi="158.92mm" orientation="landscape" file="US20230004581A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="183.64mm" wi="121.75mm" orientation="landscape" file="US20230004581A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="210.82mm" wi="93.73mm" orientation="landscape" file="US20230004581A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="207.86mm" wi="92.88mm" orientation="landscape" file="US20230004581A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="219.03mm" wi="96.52mm" orientation="landscape" file="US20230004581A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="206.93mm" wi="90.76mm" orientation="landscape" file="US20230004581A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="171.28mm" wi="145.12mm" file="US20230004581A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">PRIORITY INFORMATION</heading><p id="p-0002" num="0001">The present application claims priority, under 35 USC &#xa7;119(e), from U.S. Provisional Patent Application, Ser. No. 63/216,070, filed on Jun. 29, 2021. The entire content of U.S. Provisional Patent Application, Ser. No. 63/216,070, filed on Jun. 29, 2021, is hereby incorporated by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">FIELD</heading><p id="p-0003" num="0002">The present disclosure relates to improved database development in which a computer is used to implement robust learning in the presence of annotations with under-specified hierarchical labels.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">Several production databases, such as Google Maps and Naver Maps, store information regarding points-of-interest (POIs), i.e., places that one might find interesting. In one approach, metadata, including tags or labels corresponding with one or more POI categories, are generally added, removed or modified in these databases. This metadata may be used not only to guide humans, but also as input data to several applications such as recommender systems and/or trip planners. However, in real-life applications, labels for POIs may be incomplete or even missing for unpopular or newly-established POIs.</p><p id="p-0005" num="0004">Machine learning based supervised category prediction has been proposed as a solution to impute missing labels. However, as recognized, it is unrealistic to count on the existence of a perfectly annotated training set. This may be due to the inadequate input of labels by using either automatic techniques (e.g., mining user comments), which necessarily comprises errors, or due to human error (e.g., caused by humans who often fail to annotate POIs comprehensively), especially when there are thousands of categories from which to select. Accordingly, training annotations may be noisy.</p><p id="p-0006" num="0005">Inadequate input of labels results in the presence of under-specified hierarchical labels. That is, for a given label hierarchy, a fully-specified label is one that provides a path from the root node to the most specific correct node. By contrast, an under-specified label has a path that terminates at a category found at higher levels of the hierarchy. This results in under-represented and over-represented categories. For instance, several POIs are tagged with a path terminating close to the top of the hierarchy e.g., &#x201c;Restaurant&#x2225;Korean Food&#x201d;, while the actual correct path terminates at a lower level e.g., &#x201c;Restaurant&#x2225;Korean Food&#x2225;Seafood&#x2225;Sliced Raw Fish&#x2225;Saebyeok Raw fish&#x201d;. In that case, &#x201c;Restaurant&#x2225;Korean Food&#x201d; could be considered a candidate of an over-represented class and &#x201c;Restaurant&#x2225;Korean Food&#x2225;Seafood&#x2225;Sliced Raw Fish&#x2225;Saebyeok Raw fish&#x201d; a candidate of an under-represented class. It would be desirable to provide a computer-implemented process permitting robust machine learning in the presence of under-developed hierarchical labels.</p><p id="p-0007" num="0006">Other known teachings may relate to the understanding of the disclosed embodiments. Most of the work regarding POI classification has taken place in the context of location based social networks. Two particular approaches are worth noting:</p><p id="p-0008" num="0007">The first one requires access to check-in data and uses such data as input to a prediction model. This includes, for instance, POI unique identifiers, user unique identifiers, the time and duration of the check-in, the number of check-ins, the latitude/longitude of the user's position, and sometimes users' demographic information (e.g., age range, gender). Based on this information, much of the existing work, attempts to categorize POIs in very coarse-grained categories (e.g., home vs. work, or nightlife/bar vs. restaurant) with the number of categories to predict ranging from 3 to 15. In addition to check-ins, others have used more fine-grained information about the POIs. Yet others use POI name and address tokens or, more particularly, token embeddings pre-computed on a domain-specific corpus.</p><p id="p-0009" num="0008">Recognizing that collecting personal information may be difficult for a large number of POIs, other works are based on POI metadata only. One approach focuses on increasing the POI classifier's coverage by using only the POI name, location, and time of opening attributes. Yet another approach uses only POI names and locations as input to their model. In addition, they propose a voting ensemble of hierarchical classifiers to predict leaf categories.</p><p id="p-0010" num="0009">Turning to classification approaches, flat classification approaches ignore the hierarchical relations between categories and treat leaf categories as an independent set of labels (i.e., each class is independent of other classes). While flat classification approaches are easy to implement, they tend to have worse results than hierarchical approaches when labels are organized in a large taxonomy. In contrast, hierarchical classification (HC) systems are particularly well suited for predicting a hierarchically organized path of labels. Hierarchical classifiers are usually divided into local and global approaches: Local approaches learn multiple independent classifiers with each classifier specializing with respect to each node, parent node or hierarchy level. Global approaches consist of a single model able to map samples to their corresponding category paths as a whole.</p><p id="p-0011" num="0010">State-of-art performance has been recently achieved with hybrid approaches combining local and global paradigms. Wehrmann et al., <i>Hierarchical Multi</i>-<i>Label Classification Networks </i>(Proceedings of the 35th International Conference on Machine Learning, PMLR 80:5075-5084, [2018]) [&#x201c;Wehrmann et.&#x201d;] discloses a classifier that is trained with both local and global losses. Another approach proposes coherent multi-label classification networks where labels predicted by the local and global classifiers are hierarchically consistent. It is not believed that designers of hierarchical classifiers have addressed the problems arising as a result of under-specified hierarchical category paths.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0012" num="0011">In embodiments there is disclosed one or more processors for improving classification of labels and categories of a database stored in memory. The one or more processors applying both a subset of a set of labels and a subset of a set of categories of the database stored in the memory to a first classifier for classifying the subset of labels, and analysing a label in the subset of labels of the database stored in the memory based on the classification. Based on the classification analysis, the one or more processors changing the first loss function of the first classifier to a second loss function to form a second classifier including the second loss function, and applying both the subset of labels and the subset of categories to the second classifier for classifying the subset of labels with respect to the subset of categories of the database stored in the memory for improving its classification of labels and categories.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0013" num="0012">The drawings are only for purposes of illustrating various embodiments and are not to be construed as limiting, wherein:</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a schematic diagram of a hardware platform for performing the disclosed methods for improving classification of labels and categories of a database.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a graph illustrating the distribution of the data in a manually annotated dataset before (silver standard) and after verification (gold standard), for 100 POI categories.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a bar chart illustrating the performance of the system in relation to the number of POIs attributed to corresponding category paths.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a schematic diagram illustrating an example of under-specified and fully-specified hierarchical labels.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flow chart relating to exemplary approaches of the disclosed embodiments.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a graph illustrating the distribution of the data used in the dataset of the disclosed embodiments.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>7</b></figref> includes bar graphs illustrating F1 scores for different cost values where misclassifications are related to predicting shorter paths than the ones in the training data of the disclosed embodiments.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>8</b></figref> includes bar graphs illustrating F1 scores for different cost values where misclassifications are related to predicting longer paths than the ones in the training data.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>9</b></figref> includes bar graphs illustrating how scores change according to the number of head categories considered as joker classes (i.e., over-represented categories).</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>10</b></figref> includes bar graphs illustrating how scores evolve as the hierarchical levels are increased from which categories are extracted.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>11</b></figref> sets forth a method in pseudo computer code for identifying candidate joker classes.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0025" num="0024">In the drawings, reference numbers may be reused to identify similar and/or identical elements.</p><heading id="h-0006" level="1">DESCRIPTION</heading><heading id="h-0007" level="1">A. Platform for System Implementation</heading><p id="p-0026" num="0025">Referring to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, a hardware platform is illustrated for performing the disclosed methods for improving classification of labels and categories of a database. More specifically, <figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a hardware platform of a system that permits robust learning in the presence of annotations with under-specified hierarchical labels is illustrated. The platform includes a first server <b>10</b><i>a </i>and a second server <b>10</b><i>b</i>. In one embodiment, the first server <b>10</b><i>a </i>is a dataset server for storing a training dataset (stored in memory <b>13</b><i>a</i>) and the second server <b>10</b><i>b </i>is a training server for implementing a method for training a model (stored in memory <b>13</b><i>b</i>), the training dataset including labels pointing to categories. In other embodiments, servers <b>10</b><i>a </i>and <b>10</b><i>b </i>may be merged or have reversed functionality.</p><p id="p-0027" num="0026">The servers <b>10</b><i>a </i>and <b>10</b><i>b </i>are typically remote computer equipment connected to an extended network <b>15</b> such as the Internet for data exchange. The platform of <figref idref="DRAWINGS">FIG. <b>1</b></figref> advantageously comprises one or more items of client equipment <b>11</b>, which may be any workstation <b>11</b><i>c</i>, robot <b>11</b><i>d</i>, or mobile device <b>11</b><i>e </i>(which are also connected to network <b>15</b>), preferably separate from the servers <b>10</b><i>a</i>, <b>10</b><i>b</i>, but possibly being merged with one and/or the other thereof. Each server <b>10</b><i>a</i>, <b>10</b><i>b </i>and client equipment <b>11</b><i>c</i>, <b>11</b><i>d</i>, <b>11</b><i>e </i>comprises, respectively, processors <b>12</b><i>a</i>, <b>12</b><i>b</i>, <b>12</b><i>c</i>, <b>12</b><i>d</i>, <b>12</b><i>e</i>, and optionally computer memories <b>13</b><i>a</i>, <b>13</b><i>b</i>, <b>13</b><i>c</i>, <b>13</b><i>d</i>, <b>13</b><i>e</i>. The operators (i.e., &#x201c;users&#x201d;) of client equipment <b>11</b> are typically &#x201c;clients&#x201d; in the commercial meaning of the term, of the service provider operating the first and/or second servers <b>10</b><i>a</i>, <b>10</b><i>b. </i></p><heading id="h-0008" level="1">B. System Functionality</heading><p id="p-0028" num="0027">B.1 Analysis</p><p id="p-0029" num="0028">In an exemplary development approach, a flat POI classifier, of the type disclosed in Lagos et al., <i>Point</i>-<i>Of</i>-<i>Interest Semantic Tag Completion in a Global Crowdsourced Search</i>-<i>and</i>-<i>Discovery Database </i>(ECAI 2020 -24th European Conference on Artificial Intelligence, 29 Aug.-8 Sep. 2020) [&#x201c;Lagos et al.&#x201d;], was implemented. Then a development dataset, including 828,000 POIs and 4093 unique category paths (only paths appearing as the label of at least one POI were counted) was provided. Each POI is labelled with exactly one path and the maximum depth of a corresponding categorization hierarchy is five. The exemplary hierarchy employed is very fine grained. For instance, 70 sub-categories of a Pizza category are located at the third level of the hierarchy.</p><p id="p-0030" num="0029">The development dataset is heavily imbalanced in terms of the number of POI instances attributed to each category, as illustrated <figref idref="DRAWINGS">FIG. <b>6</b></figref>. For instance, the top ten categories have more than 40% of the POIs attributed to them, while 461 categories have fewer than 5 POIs, resulting in a very long queue of sparsely represented categories. To generate training and test data, stratified sampling was used. Consequently, the 828,000 POIs were proportionally allocated into 70% for training, 20% for development, and 10% for testing purposes.</p><p id="p-0031" num="0030">In the present description, the test dataset is referred to as a &#x201c;silver standard&#x201d;, as labels are under-specified for a part of the POIs. By contrast, a gold standard dataset includes 1000 POIs that were carefully verified. More particularly, <figref idref="DRAWINGS">FIG. <b>2</b></figref>, illustrates the distribution of the data in a manually annotated dataset before (silver standard <b>210</b>) and after verification (gold standard <b>212</b>), for the top 100 most popular categories. Examples of under-represented categories <b>214</b> and over-represented categories <b>216</b> are highlighted. Differences between the two distributions are mainly due to POIs having been annotated with under-specified labels. As will appear, <figref idref="DRAWINGS">FIG. <b>2</b></figref> specifically illustrates how the distribution of the top one hundred most popular categories changes after verification of the manually annotated dataset.</p><p id="p-0032" num="0031">Pursuant to development, errors of which the system is capable were qualified prior to deployment. Also, in view of the very long tail of the development dataset, the behavior of the system on the corresponding POIs was considered. Referring to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the performance of the system in relation to the number of POIs attributed to corresponding category paths is illustrated. As demonstrated by <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the system performs very well on category paths in the middle of the range, and comparatively well for paths with very few POIs. However, performance was lower than expected for paths that were heavily populated and deteriorating when only one POI was related to a specific category path.</p><p id="p-0033" num="0032">To further qualify errors, a set of 1000 misclassifications was extracted for further analysis. This sample was representative of the prediction probabilities one could find in the misclassifications of the development set. That is, if 15% of the misclassifications on the development set had a probability of over 0.9, then the same ratio was maintained for the extracted set. Details of the analysis are presented in Table 1.</p><p id="p-0034" num="0000"><tables id="TABLE-US-00001" num="00001"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="1" colwidth="84pt" align="center"/><colspec colname="2" colwidth="42pt" align="center"/><colspec colname="3" colwidth="91pt" align="center"/><thead><row><entry namest="1" nameend="3" rowsep="1">TABLE 1</entry></row><row><entry namest="1" nameend="3" align="center" rowsep="1"/></row><row><entry>Probability</entry><entry>Correct(%)</entry><entry>Acceptable(%)</entry></row><row><entry namest="1" nameend="3" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="1" colwidth="84pt" align="center"/><colspec colname="2" colwidth="42pt" align="char" char="."/><colspec colname="3" colwidth="91pt" align="char" char="."/><tbody valign="top"><row><entry>&#x3e;0.9</entry><entry>66.63</entry><entry>7.62</entry></row><row><entry>0.7-0.9</entry><entry>32</entry><entry>&#x2002;7-12</entry></row><row><entry>0.4-0.7</entry><entry>13-15</entry><entry>31-40</entry></row><row><entry>&#x3c;0.4</entry><entry>&#x3c;3</entry><entry>&#x3c;5</entry></row><row><entry namest="1" nameend="3" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0035" num="0033">Table 1 represents a verification of disagreements between the silver test dataset and the prediction generated by the above-mentioned initial flat classifier. The model associated with the system is able to correct the human annotations at high probability threshold levels, or recommend correct alternative tags. The resulting verified dataset is considered as the gold standard in the rest of the paper.</p><p id="p-0036" num="0034">It follows from Table 1 that at high probability thresholds a resulting classification model actually identifies mistakes of the human annotators. At a probability of over 0.9 this accounted for identification of almost two thirds of the misclassifications, while when between 0.7 and 0.9, for almost one third. In addition, one third of the tags in the range 0.4-0.7 were considered acceptable i.e., the prediction would have been good enough to include it in a production database. Most of the time this amounted to categories semantically very close to each other such as &#x2225;Cafe, Dessert&#x2225;Cafe and &#x2225;Cafe, Dessert (where &#x201c;&#x2225;&#x201d; denotes a sub-level in a hierarchy and the root category &#x201c;Restaurant&#x201d; is omitted for clarity). It also follows from Table 1 that in the case when the probability is below 0.4, misclassifications are actual errors.</p><p id="p-0037" num="0035">The above analysis (in conjunction with the following description) leads to several exemplary observations:<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0000">    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0036">It is advantageous to both minimize under-specified labels and to predict a fully-specified label whenever possible;</li>        <li id="ul0002-0002" num="0037">Setting a probability threshold of 0.4, as demonstrated in Section B.3.(vi)(a) below, permits a gain of points in micro-F1; and</li>        <li id="ul0002-0003" num="0038">The classification model explored above may be used advantageously to not only impute labels for new POIs, but to curate existing annotations.</li>    </ul>    </li></ul></p><p id="p-0038" num="0039">In practice, the above-verified dataset is used as the gold standard for the embodiments. Additionally, it is noted that the distribution of the gold standard is different from that of the silver standard, as several POIs initially attributed to over-represented classes were re-attributed to more specific category paths, as shown below in Table 2, and, advantageously, to a large number of long-tail classes. Table 2 represents the percentage of POIs attributed to categories lower in the hierarchy after verification, for some heavily populated categories. The symbol &#x201c;&#x2225;&#x201d; indicates a sub-level in the hierarchy, and the symbol &#x201c;*&#x201d; is used to denote the percentage of alternative tags.</p><p id="p-0039" num="0000"><tables id="TABLE-US-00002" num="00002"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="offset" colwidth="14pt" align="left"/><colspec colname="1" colwidth="119pt" align="left"/><colspec colname="2" colwidth="84pt" align="center"/><thead><row><entry/><entry namest="offset" nameend="2" rowsep="1">TABLE 2</entry></row><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row><row><entry/><entry>Category</entry><entry>Re-attributed POIs(%)</entry></row><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="4"><colspec colname="offset" colwidth="14pt" align="left"/><colspec colname="1" colwidth="119pt" align="left"/><colspec colname="2" colwidth="42pt" align="right"/><colspec colname="3" colwidth="42pt" align="left"/><tbody valign="top"><row><entry/><entry>&#x2225;Korean Food</entry><entry>80.29</entry><entry>(1.46*)</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="offset" colwidth="14pt" align="left"/><colspec colname="1" colwidth="119pt" align="left"/><colspec colname="2" colwidth="84pt" align="char" char="."/><tbody valign="top"><row><entry/><entry>&#x2225;Cafe, Dessert<sup>5</sup></entry><entry>59.3*</entry></row><row><entry/><entry>&#x2225;Cafe, Dessert&#x2225;Cafe</entry><entry>0.0</entry></row><row><entry/><entry>&#x2225;Korean Food&#x2225;Meat, Food</entry><entry>15.69</entry></row><row><entry/><entry>&#x2225;Korean Food&#x2225;Meat, Food&#x2225;Grilled Pork</entry><entry>0.0</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="4"><colspec colname="offset" colwidth="14pt" align="left"/><colspec colname="1" colwidth="119pt" align="left"/><colspec colname="2" colwidth="42pt" align="right"/><colspec colname="3" colwidth="42pt" align="left"/><tbody valign="top"><row><entry/><entry>&#x2225;Bar&#x2225;Pub</entry><entry>0.91</entry><entry>(0.3*)</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="offset" colwidth="14pt" align="left"/><colspec colname="1" colwidth="119pt" align="left"/><colspec colname="2" colwidth="84pt" align="center"/><tbody valign="top"><row><entry/><entry>. . .</entry><entry>. . .</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="4"><colspec colname="offset" colwidth="14pt" align="left"/><colspec colname="1" colwidth="119pt" align="left"/><colspec colname="2" colwidth="42pt" align="right"/><colspec colname="3" colwidth="42pt" align="left"/><tbody valign="top"><row><entry/><entry>&#x2225;Korean Snack<sup>6</sup></entry><entry>50</entry><entry>(42.3*)</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="offset" colwidth="14pt" align="left"/><colspec colname="1" colwidth="119pt" align="left"/><colspec colname="2" colwidth="84pt" align="center"/><tbody valign="top"><row><entry/><entry>. . .</entry><entry>. . .</entry></row><row><entry/><entry>Restaurant</entry><entry>100</entry></row><row><entry/><entry>&#x2225;Night meal</entry><entry>71.43</entry></row><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0040" num="0040">B.2 Assessment</p><p id="p-0041" num="0041">In the embodiments, a POI p is represented as p={x, y}={x<sup>(1)</sup>, x<sup>(2)</sup>, y} where x is a vector representing the collection of POI's name, x<sup>(1)</sup>, address, x<sup>(2)</sup>, attributes, as well as a label y, representing a hierarchical category path.</p><p id="p-0042" num="0042">A tree structured hierarchy of categories T=(C, E) where C={c<sub>0 </sub><sup>0</sup>, . . , c<sub>n </sub><sup>k</sup>} is the set of n pre-defined categories with a maximum depth of k, such that E={(c<sub>l</sub><sup>h</sup>, c<sub>j</sub><sup>h+1</sup>)&#x2208;C|c<sub>l</sub><sup>h</sup><img id="CUSTOM-CHARACTER-00001" he="3.22mm" wi="2.46mm" file="US20230004581A1-20230105-P00001.TIF" alt="custom-character" img-content="character" img-format="tif"/>c<sub>j</sub><sup>h+1</sup>, h&#x2264;k}, where h is an index indicating the level of the hierarchy, namely hierarchy depth, and &#x201c;<img id="CUSTOM-CHARACTER-00002" he="3.22mm" wi="2.46mm" file="US20230004581A1-20230105-P00001.TIF" alt="custom-character" img-content="character" img-format="tif"/>&#x201d; denotes the sub-category-of relation. For instance, given the root-to-leaf of categories &#x201c;Restaurant&#x2225;Korean Food&#x2225;Seafood&#x2225;Sliced Raw Fish&#x2225;Sashimi&#x201d;, as shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, and c<sub>l</sub><sup>h </sup>is the path &#x201c;Restaurant&#x2225;Korean Food&#x201d; then c<sub>j</sub><sup>h+1 </sup>would be the path &#x201c;Restaurant&#x2225;Korean Food&#x2225;Seafood&#x201d;.</p><p id="p-0043" num="0043">Given T, y should ideally represent a fully-specified path of categories t=(c<sup>0</sup>, c<sup>1</sup>, . . . , c<sup>m</sup>. In at least one embodiment, correct non-terminal paths (where m&#x3c;k) may exist, meaning that a fully-specified correct path does not necessarily have to include categories up to the leaf nodes of the hierarchy, but could instead terminate at an internal node. Continuing consideration of the above example, the path &#x201c;Restaurant&#x2225;Korean Food&#x2225;Seafood&#x2225;Sliced Raw Fish&#x201d; could be correct if the corresponding POI served several different types of sliced raw fish. In addition, in a real-world case, it has been found that observed paths t&#x2032;=(c<sup>0</sup>, c<sup>1</sup>, . . . , c<sup>z</sup>), in the training data, may be under-specified i.e., z&#x3c;m, and thus incorrect. For instance, t&#x2032; could be &#x201c;Restaurant&#x2225;Korean Food&#x201d;. At least one of the disclosed embodiments is directed toward a classifier responsive to the above data characteristics.</p><p id="p-0044" num="0044">B.3 Exemplary Approaches</p><p id="p-0045" num="0045">B.3.(i) Overview</p><p id="p-0046" num="0046">To allow robust learning in the presence of annotations with under-specified hierarchical labels, the exemplary approaches described below include: (a) developing a hybrid hierarchical classifier that combines one global and potentially several local classifiers using standard categorical cross-entropy losses; (b) automatically detecting problematic categories, including candidate &#x201c;joker classes&#x201d; (i.e., over-represented categories), based on the misclassifications of the classifier of step (a); (c) introducing a weight to the global classifier's loss and re-training the model from scratch. The weight specifically penalizes misclassifications having shorter category paths than the ones found in the corresponding human annotations, while accordingly it assigns lower cost to misclassifications having longer category paths.</p><p id="p-0047" num="0047">Referring to <figref idref="DRAWINGS">FIG. <b>5</b></figref>, a flow chart relating generally to exemplary approaches described below in Sections B.3.(iii) and B.3.(iv) is shown. In particular, at <b>50</b>, each approach includes using the one or more processors <b>12</b> (<figref idref="DRAWINGS">FIG. <b>1</b></figref>) to develop a classifier. At <b>52</b>, a first training model, including a standard categorical cross-entropy loss function, is used to classify the training dataset. In accordance with the description above, at <b>54</b>, misclassifications resulting from the classification at <b>52</b> are determined.</p><p id="p-0048" num="0048">As described in further detail below, additional classification is performed in one of two modes with a first mode being agnostic to joker class detection and a second mode being responsive to joker class detection. At <b>56</b>, the mode to be used is set.</p><p id="p-0049" num="0049">In the first mode, at <b>58</b>, no detection of joker classes is attempted and the training dataset is classified with a second training model. The second training model employs a weighted-by-sample categorical cross-entropy loss function (instead of a standard categorical cross-entropy loss function) and the weighted-by-sample categorical cross-entropy loss function is applied to each category path (designated below as &#x201c;a<sub>longer </sub>&#x201d; and &#x201c;a<sub>shorter</sub>&#x201d;).</p><p id="p-0050" num="0050">In the second mode, at <b>60</b>, each joker class is identified. In one embodiment, a computer program of the type disclosed in detail below with reference to Method <b>1</b> shown in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, is used to identify each joker class. At <b>62</b>, classification is performed in the same manner as <b>58</b>, except that the weighted-by-sample categorical cross-entropy loss function is applied to each joker class rather than each category path.</p><p id="p-0051" num="0051">B.3.(ii) Hierarchical Classification Model</p><p id="p-0052" num="0052">Wehrmann et al. has shown that a hierarchical classifier operating both local and global optimization has significant advantages over a hierarchical classifier operating with just one of the two approaches. In view of such showing, a multiple-output deep neural network including the following was implemented: one local output per hierarchical level, with a corresponding local loss function for the classes in the corresponding level, and one global output for the final category path. The input of the first local classifier is composed of the initial inputs only, i.e., the LSTM (Long Term Short Memory) embeddings of the POI attributes. Each local classifier thereafter has as input the concatenation of the initial inputs and the intermediate embedding representing the feature space of the previous local classifiers i.e. , the last dense layer, before the output layer, of the previous local classifier. Dense layers are activated with a non-linear function (such as a rectified linear unit). The global classifier has as inputs the embedding of the last dense layer of the last local classifier, which as highlighted by Wehrmann et al., is the cumulative information of the feature space of all local classifiers, concatenated with the initial inputs.</p><p id="p-0053" num="0053">The final loss is the sum of the global output loss <img id="CUSTOM-CHARACTER-00003" he="3.22mm" wi="2.46mm" file="US20230004581A1-20230105-P00002.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>G </sub>and all local output losses <img id="CUSTOM-CHARACTER-00004" he="3.22mm" wi="2.46mm" file="US20230004581A1-20230105-P00002.TIF" alt="custom-character" img-content="character" img-format="tif"/>=<img id="CUSTOM-CHARACTER-00005" he="3.22mm" wi="2.46mm" file="US20230004581A1-20230105-P00002.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>G</sub>+&#x3a3;<sub>h=1</sub><sup>r</sup><img id="CUSTOM-CHARACTER-00006" he="3.22mm" wi="2.46mm" file="US20230004581A1-20230105-P00002.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>L</sub><sup>h</sup>,where r&#x2264;k. To make the classes mutually exclusive for each hierarchical level, the standard categorical cross-entropy loss for each one of <img id="CUSTOM-CHARACTER-00007" he="3.22mm" wi="2.46mm" file="US20230004581A1-20230105-P00002.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>L</sub><sup>h </sup>and <img id="CUSTOM-CHARACTER-00008" he="3.22mm" wi="2.46mm" file="US20230004581A1-20230105-P00002.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>G </sub>is employed.</p><p id="p-0054" num="0054">To account for non-terminal paths i.e., observed paths that do not terminate at a leaf node but at an internal one, a special category token to denote when the end of a non-terminal path has been reached is used. In contrast to the approach of Wehrmann et al., the embodiments permit r&#x3c;k, thus effectively allowing the implementation of different networks that incrementally cover more levels of the hierarchy, until an optimal depth is found.</p><p id="p-0055" num="0055">B.3.(iii) Joker Class-Agnostic Class</p><p id="p-0056" num="0056">In at least one embodiment, under-specified category paths are accounted for by penalizing more misclassifications with shorter paths than the ones observed in the training data, when that shorter path is shared by both the prediction and the observed label, than misclassifications with longer paths. In addition, it is desirable to penalize the latter case less than the rest of the errors, i.e., when the prediction and observed label do not share, at least in part, a common path.</p><p id="p-0057" num="0057">For instance, assume that y represents the path t&#x2032;=&#x201c;Restaurant&#x2225;Korean Food&#x2225;Seafood&#x201d;. If &#x177; denotes the prediction with shorter path {circumflex over (t)}=&#x201c;Restaurant&#x2225;Korean Food&#x201d;, then this prediction would be penalized more than if it represented the one with longer path &#x201c;Restaurant&#x2225;Korean Food&#x2225;Seafood&#x2225;Sliced Raw Fish&#x201d;. Specifically, let a&#x177;<sub>i,yi </sub>denote the cost associated with assigning the label &#x177;to the sample i that has an observed label y. Denoting a<sub>shorter </sub>the cost of predicting a shorter path than the observed one and a<sub>longer </sub>the cost of predicting a longer path, then a<sub>longer</sub>&#x3c;a<sub>shorter</sub>. Both a<sub>longer</sub>and a<sub>shorter </sub>may be set empirically. Accordingly, in at least one embodiment, <img id="CUSTOM-CHARACTER-00009" he="3.22mm" wi="2.46mm" file="US20230004581A1-20230105-P00002.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>G </sub>is changed from the standard categorical cross-entropy loss function to the following weighted-by-sample categorical cross-entropy loss function:</p><p id="p-0058" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mrow>  <msubsup>   <mi>&#x2112;</mi>   <mi>G</mi>   <mo>&#x2032;</mo>  </msubsup>  <mo>=</mo>  <mrow>   <mfrac>    <mn>1</mn>    <mi>N</mi>   </mfrac>   <mo>&#x2062;</mo>   <mrow>    <munderover>     <mo>&#x2211;</mo>     <mrow>      <mi>i</mi>      <mo>=</mo>      <mn>1</mn>     </mrow>     <mi>N</mi>    </munderover>    <mrow>     <msub>      <mi>a</mi>      <mrow>       <msub>        <mover>         <mi>y</mi>         <mo>^</mo>        </mover>        <mi>i</mi>       </msub>       <mo>,</mo>       <msub>        <mi>y</mi>        <mi>i</mi>       </msub>      </mrow>     </msub>     <mo>&#x2062;</mo>     <msub>      <mi>&#x2112;</mi>      <mrow>       <mi>G</mi>       <mo>,</mo>       <mi>i</mi>      </mrow>     </msub>    </mrow>   </mrow>  </mrow> </mrow></math></maths></p><p id="p-0059" num="0058">where <img id="CUSTOM-CHARACTER-00010" he="3.22mm" wi="2.46mm" file="US20230004581A1-20230105-P00002.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>G,l </sub>is the standard global categorical cross-entropy loss function for sample i and,</p><p id="p-0060" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mrow>  <msub>   <mi>a</mi>   <mrow>    <msub>     <mover>      <mi>y</mi>      <mo>^</mo>     </mover>     <mi>i</mi>    </msub>    <mo>,</mo>    <msub>     <mi>y</mi>     <mi>i</mi>    </msub>   </mrow>  </msub>  <mo>=</mo>  <mrow>   <mo>{</mo>   <mrow>    <mtable>     <mtr>      <mtd>       <mrow>        <msub>         <mi>a</mi>         <mi>shorter</mi>        </msub>        <mo>,</mo>       </mrow>      </mtd>      <mtd>       <mrow>        <mi>if</mi>        <mo>&#x2062;</mo>        <mtext>   </mtext>        <mrow>         <msub>          <mover>           <mi>t</mi>           <mo>^</mo>          </mover>          <mi>i</mi>         </msub>         <mo>.</mo>         <mtext>   </mtext>         <mi>prefix_path</mi>        </mrow>        <mo>&#x2062;</mo>        <mi>_of</mi>        <mo>&#x2062;</mo>        <mtext>   </mtext>        <mrow>         <mo>(</mo>         <msubsup>          <mi>t</mi>          <mi>i</mi>          <mo>&#x2032;</mo>         </msubsup>         <mo>)</mo>        </mrow>       </mrow>      </mtd>     </mtr>     <mtr>      <mtd>       <mrow>        <msub>         <mi>a</mi>         <mi>longer</mi>        </msub>        <mo>,</mo>       </mrow>      </mtd>      <mtd>       <mrow>        <mi>if</mi>        <mo>&#x2062;</mo>        <mtext>   </mtext>        <mrow>         <msubsup>          <mi>t</mi>          <mi>i</mi>          <mo>&#x2032;</mo>         </msubsup>         <mo>.</mo>         <mtext>   </mtext>         <mi>prefix_path</mi>        </mrow>        <mo>&#x2062;</mo>        <mi>_of</mi>        <mo>&#x2062;</mo>        <mtext>   </mtext>        <mrow>         <mo>(</mo>         <msub>          <mover>           <mi>t</mi>           <mo>^</mo>          </mover>          <mi>i</mi>         </msub>         <mo>)</mo>        </mrow>       </mrow>      </mtd>     </mtr>     <mtr>      <mtd>       <mrow>        <mn>1</mn>        <mo>,</mo>       </mrow>      </mtd>      <mtd>       <mi>otherwise</mi>      </mtd>     </mtr>    </mtable>    <mo>.</mo>   </mrow>  </mrow> </mrow></math></maths></p><p id="p-0061" num="0059">{circumflex over (t)} is the path corresponding to the &#x177;<sub>i </sub>prediction and t<sub>i</sub>&#x2032; is the observed path corresponding to y<sub>i</sub>. The prefix_path_of function indicates a &#x201c;strict&#x201d; prefix, i.e., the two paths cannot be identical.</p><p id="p-0062" num="0060">B.3.(iv) Joker Class-Specific Cost</p><p id="p-0063" num="0061">The global loss defined in Section B.3.(iii) above applies to all category paths. However, only a small set of unique paths concentrate the majority of real incorrect misclassifications, the ones referred to above as joker classes. By applying the cost in a joker class-agnostic manner, over training samples related to non-joker classes may be over-punished. That is, it would appear that the model might be made less confident for some correct annotations.</p><p id="p-0064" num="0062">To tackle this issue, the embodiment proposes to automatically identify candidate joker classes based on the misclassifications of the initial hierarchical classification model and apply the a<sub>longer </sub>and a<sub>shorter </sub>costs introduced in the previous section only to samples that have labels corresponding to these classes. More specifically, the following assumptions are employed:<ul id="ul0003" list-style="none">    <li id="ul0003-0001" num="0000">    <ul id="ul0004" list-style="none">        <li id="ul0004-0001" num="0063">Although the hierarchical classification model, i.e., the above-mentioned base learner, is not optimal, it is still able to predict the correct category for the majority of the samples for which the model is most confident. This is viewed as an indication of confidence the probability the model assigns to a prediction.</li>        <li id="ul0004-0002" num="0064">As found in a preliminary qualitative analysis of the development set, the majority of the misclassifications for which the embodied base learner is very confident (i.e., where the probability &#x3e;0.9) are related to predictions that have longer paths than the ones found in the corresponding manual annotations. So, finding frequent paths related to misclassifications with a high prediction probability, may be an indication that they correspond to joker classes.</li>    </ul>    </li></ul></p><p id="p-0065" num="0065">Based on the above, finding candidate joker classes amounts to identifying category paths that are frequently misclassified by the model with a high certainty. This is demonstrated by Method <b>1</b> shown in <figref idref="DRAWINGS">FIG. <b>11</b></figref> of pseudo computer code.</p><p id="p-0066" num="0066">In Method <b>1</b> shown in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, the minimum support s, i.e., the minimum number of samples that should be related to a category, and the maximum hierarchy depth is defined. The maximum hierarchy depth is related to the nature of at least one exemplary problem: as most joker classes tend to be located at higher levels of the hierarchy by definition, the benefit of Method <b>1</b> becomes potentially less important as misclassifications lower in the hierarchy are considered. In one experiment, the maximum depth was set to 3 and the minimum support to 100. In the same experiment, the probability threshold is set as greater than 0.9 and the ratio threshold is set to the median of all the categories remaining after the previous steps are applied, resulting in 22 candidate joker classes.</p><p id="p-0067" num="0067">B.3.(v) Exemplary Models and Related Implementation Details</p><p id="p-0068" num="0068">The following exemplary models were used to evaluate the disclosed system:<ul id="ul0005" list-style="none">    <li id="ul0005-0001" num="0000">    <ul id="ul0006" list-style="none">        <li id="ul0006-0001" num="0069">Base: The embodiments use the flat classifier of Lagos et al. as a baseline. In detail, an exemplary implementation includes an architecture with one hidden dense layer, followed by a dropout layer, and a softmax output layer. A Rectified Linear Unit serves as the activation function of the hidden layer. The dropout rate is set to 0.3. The loss used for the baseline is categorical cross entropy. An early stopping criterion is used for the training based on a pre-defined threshold that takes into account the delta of the loss between two consecutive epochs. The maximum number of epochs is set to 50. Both POI attributes are considered as sequential features with a length of 50. For the LSTM layer, the dimensions of the embedding layervector space are set to 128 and the number of the LSTM hidden units to 128. The LSTM has recurrent dropout rate of 0.3. The rest of the models described below share the same hyper-parameter values.</li>        <li id="ul0006-0002" num="0070">Hcls: &#x201c;Hcls&#x201d; stands for the hierarchical model described above in Section B.3.(ii). In Table 3 shown below only the best performing model (namely the one taking into account only the second level of the hierarchy) is described in detail. An ablation study is included in Section B.3.(vi)(c) below. For the Hcls model, a dropout layer is inserted after each dense layer with each dense layer being added in order to represent a hierarchical categorization level; otherwise, the rest of the implementation details for the Hcls model are the same as for the baseline.</li>        <li id="ul0006-0003" num="0071">Focal,cb: Given the fact that a lot of POIs are re-attributed in the gold dataset to long-tail categories, state-of-art approaches that counter the effect of a skewed data distribution by adjusting the weights of the samples from the small classes in the loss function have been considered. In that context, focal-loss (Lin et al., <i>Focal Loss for Dense Object Detection, </i>IEEE International Conference on Computer Vision [ICCV] 2999-3007 [2017]) as well as its combination with class-balanced loss have shown the most promising results recently (Cui et al., <i>Class</i>-<i>Balanced Loss Based on Effective Number of Samples, </i>IEEE/CVF Conference on Computer Vision and Pattern Recognition [CVPR] 9260-9269 [2019]). Focal loss adds a modulating parameter &#x3b3; to the cross-entropy loss to allow focusing on long tail samples. Class balanced loss offers an alternative to using inverse class frequency, by introducing the concept of the class effective number via the hyperparameter &#x3b2;, which is used to calculate the weight of each class in the loss term. For the present evaluation, the categorical cross-entropy loss of the global output with these losses were replaced. That is, the modulating parameter &#x3b3;of focal loss was set at 1.0 and the &#x3b2; parameter of the class-balanced loss at 0.2, after performing a grid search with step size of 0.1.</li>        <li id="ul0006-0004" num="0072">C<img id="CUSTOM-CHARACTER-00011" he="3.22mm" wi="2.46mm" file="US20230004581A1-20230105-P00002.TIF" alt="custom-character" img-content="character" img-format="tif"/>: This corresponds with the cost-based loss described in Section B.3.(iii), which is agnostic to the classes that the samples belong. The cost for misclassifications that have longer paths than the manually added labels is set to 0.5 and the cost for misclassifications with shorter paths is set to 1.4.</li>        <li id="ul0006-0005" num="0073">C<img id="CUSTOM-CHARACTER-00012" he="3.22mm" wi="2.12mm" file="US20230004581A1-20230105-P00003.TIF" alt="custom-character" img-content="character" img-format="tif"/><img id="CUSTOM-CHARACTER-00013" he="3.22mm" wi="2.46mm" file="US20230004581A1-20230105-P00002.TIF" alt="custom-character" img-content="character" img-format="tif"/>: This corresponds with the cost-based, joker-class specific loss described in Section B.3.(iv). The same costs are used for each one of the C<img id="CUSTOM-CHARACTER-00014" he="3.22mm" wi="2.46mm" file="US20230004581A1-20230105-P00002.TIF" alt="custom-character" img-content="character" img-format="tif"/>and C<img id="CUSTOM-CHARACTER-00015" he="3.22mm" wi="2.46mm" file="US20230004581A1-20230105-P00002.TIF" alt="custom-character" img-content="character" img-format="tif"/><img id="CUSTOM-CHARACTER-00016" he="3.22mm" wi="2.12mm" file="US20230004581A1-20230105-P00003.TIF" alt="custom-character" img-content="character" img-format="tif"/>models. A detailed ablation study showing how the costs influence the performance of the model is provided below.</li>    </ul>    </li></ul></p><p id="p-0069" num="0074">In the embodiments, experiments were performed on a single GPU (Graphics Processing Unit) instance (1 GPU with 16GB VRAM, 4 CPUs, with 256GB RAM). Training was performed with a batch size of 128. An Adam optimizer was used with conventionally recommended default parameters, along with standard macro and micro metrics for the evaluation calculated using the scikit-learn package.</p><p id="p-0070" num="0075">B.3.(vi) Exemplary Results</p><p id="p-0071" num="0076">B.3.(vi)(a) Overview of Exemplary Results</p><p id="p-0072" num="0077">The following Table 3 illustrates average performance (%) over 5 runs on the silver and gold standards. Best results per dataset are in bold. Standard deviation is also reported. Hcls+C<img id="CUSTOM-CHARACTER-00017" he="3.22mm" wi="2.12mm" file="US20230004581A1-20230105-P00003.TIF" alt="custom-character" img-content="character" img-format="tif"/><img id="CUSTOM-CHARACTER-00018" he="3.22mm" wi="2.46mm" file="US20230004581A1-20230105-P00002.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>1.4, 0.5 </sub>performs well on both the silver and gold standards (most balanced performance). Hcls+C<img id="CUSTOM-CHARACTER-00019" he="3.22mm" wi="2.12mm" file="US20230004581A1-20230105-P00003.TIF" alt="custom-character" img-content="character" img-format="tif"/><img id="CUSTOM-CHARACTER-00020" he="3.22mm" wi="2.46mm" file="US20230004581A1-20230105-P00002.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>5.0, 0.5 </sub>has the best overall performance on the gold standard. (<img id="CUSTOM-CHARACTER-00021" he="3.22mm" wi="2.12mm" file="US20230004581A1-20230105-P00004.TIF" alt="custom-character" img-content="character" img-format="tif"/>) denotes the delta in F1 between the two models above and the baseline on the gold standard, while (&#x2191;) denotes the delta in F1 to the hierarchical classifier, Hcls.</p><p id="p-0073" num="0000"><tables id="TABLE-US-00003" num="00003"><table frame="none" colsep="0" rowsep="0" pgwide="1"><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="1" colwidth="56pt" align="left"/><colspec colname="2" colwidth="126pt" align="center"/><colspec colname="3" colwidth="133pt" align="center"/><thead><row><entry namest="1" nameend="3" rowsep="1">TABLE 3</entry></row></thead><tbody valign="top"><row><entry namest="1" nameend="3" align="center" rowsep="1"/></row><row><entry>Dataset</entry><entry>Silver standard</entry><entry>Gold standard</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="56pt" align="left"/><colspec colname="2" colwidth="259pt" align="center"/><tbody valign="top"><row><entry>Model</entry><entry>Metric</entry></row><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="7"><colspec colname="1" colwidth="56pt" align="left"/><colspec colname="2" colwidth="42pt" align="center"/><colspec colname="3" colwidth="42pt" align="center"/><colspec colname="4" colwidth="42pt" align="center"/><colspec colname="5" colwidth="42pt" align="center"/><colspec colname="6" colwidth="42pt" align="center"/><colspec colname="7" colwidth="49pt" align="center"/><tbody valign="top"><row><entry/><entry>Micro-prec.</entry><entry>Micro-rec.</entry><entry>Micro-F1</entry><entry>Micro-prec.</entry><entry>Micro-rec.</entry><entry>Micro-F1</entry></row><row><entry namest="1" nameend="7" align="center" rowsep="1"/></row><row><entry>Base</entry><entry>70.41 (&#xb1;0.27)</entry><entry>63.78 (&#xb1;0.26)</entry><entry>66.93 (&#xb1;0.2)&#x2002;</entry><entry>46.57 (&#xb1;2.02)</entry><entry>32.62 (&#xb1;0.47)</entry><entry>38.36 (&#xb1;0.88)</entry></row><row><entry>Hcls</entry><entry>72.85 (&#xb1;0.26)</entry><entry>63.90 (&#xb1;0.26)</entry><entry>68.08 (&#xb1;0.05)</entry><entry>50.32 (&#xb1;1.14)</entry><entry>31.99 (&#xb1;0.46)</entry><entry>39.11 (&#xb1;0.58)</entry></row><row><entry>Hcls + Focal</entry><entry>72.64 (&#xb1;0.18)</entry><entry>63.59 (&#xb1;0.23)</entry><entry>67.81 (&#xb1;0.06)</entry><entry>51.39 (&#xb1;0.85)</entry><entry>32.41 (&#xb1;0.45)</entry><entry>39.75 (&#xb1;0.51)</entry></row><row><entry>Hcls + Focal-cb</entry><entry>72.96 (&#xb1;0.26)</entry><entry>63.30 (&#xb1;0.31)</entry><entry>67.79 (&#xb1;0.07)</entry><entry>50.99 (&#xb1;0.73)</entry><entry>32.00 (&#xb1;0.84)</entry><entry>39.32 (&#xb1;0.79)</entry></row><row><entry>Hcls + C&#x2009;<img id="CUSTOM-CHARACTER-00022" he="2.46mm" wi="1.78mm" file="US20230004581A1-20230105-P00005.TIF" alt="custom-character" img-content="character" img-format="tif"/> </entry><entry>73.27 (&#xb1;0.41)</entry><entry>62.90 (&#xb1;0.59)</entry><entry>67.69 (&#xb1;0.22)</entry><entry>52.67 (&#xb1;0.51)</entry><entry>31.94 (&#xb1;1.00)</entry><entry>39.76 (&#xb1;0.78)</entry></row><row><entry>Hcls + CJ&#x2009;<img id="CUSTOM-CHARACTER-00023" he="2.46mm" wi="1.78mm" file="US20230004581A1-20230105-P00005.TIF" alt="custom-character" img-content="character" img-format="tif"/> &#x2009;<sub>1.4,0.5</sub></entry><entry>73.34 (&#xb1;0.4)&#x2002;</entry><entry>63.37 (&#xb1;0.27)</entry><entry>67.99 (&#xb1;0.17)</entry><entry>53.60 (&#xb1;1.17)</entry><entry>33.51 (&#xb1;0.26)</entry><entry>41.23 (&#xb1;0.4)&#x2002;</entry></row><row><entry/><entry/><entry/><entry/><entry/><entry/><entry>(&#x2191;2.12) (&#x2b06;2.87)</entry></row><row><entry>Hcls + CJ&#x2009;<img id="CUSTOM-CHARACTER-00024" he="2.46mm" wi="1.78mm" file="US20230004581A1-20230105-P00005.TIF" alt="custom-character" img-content="character" img-format="tif"/> &#x2009;<sub>1.4,&#x2212;</sub></entry><entry>73.70 (&#xb1;0.17)</entry><entry>62.97 (&#xb1;0.18)</entry><entry>67.91 (&#xb1;0.05)</entry><entry>53.67 (&#xb1;1.64)</entry><entry>32.31 (&#xb1;0.34)</entry><entry>40.34 (&#xb1;0.70)</entry></row><row><entry>Hcls + CJ&#x2009;<img id="CUSTOM-CHARACTER-00025" he="2.46mm" wi="1.78mm" file="US20230004581A1-20230105-P00005.TIF" alt="custom-character" img-content="character" img-format="tif"/> &#x2009;<sub>&#x2212;,0.5</sub></entry><entry>72.84 (&#xb1;0.30)</entry><entry>63.93 (&#xb1;0.28)</entry><entry>68.09 (&#xb1;0.03)</entry><entry>51.41 (&#xb1;1.71)</entry><entry>33.22 (&#xb1;0.91)</entry><entry>40.36 (&#xb1;1.01)</entry></row><row><entry>Hcls + CJ&#x2009;<img id="CUSTOM-CHARACTER-00026" he="2.46mm" wi="1.78mm" file="US20230004581A1-20230105-P00005.TIF" alt="custom-character" img-content="character" img-format="tif"/> &#x2009;<sub>5.0,0.5</sub></entry><entry>73.58 (&#xb1;0.57)</entry><entry>59.09 (&#xb1;0.37)</entry><entry>65.54 (&#xb1;0.43)</entry><entry>62.62 (&#xb1;0.99)</entry><entry>33.04 (&#xb1;0.77)</entry><entry>43.25 (&#xb1;0.56)</entry></row><row><entry/><entry/><entry/><entry/><entry/><entry/><entry>(&#x2191;4.14) (&#x2b06;4.89)</entry></row><row><entry namest="1" nameend="7" align="center" rowsep="1"/></row><row><entry/><entry>Macro-prec.</entry><entry>Macro-rec.</entry><entry>Macro-F1</entry><entry>Macro-prec.</entry><entry>Macro-rec.</entry><entry>Macro-F1</entry></row><row><entry namest="1" nameend="7" align="center" rowsep="1"/></row><row><entry>Base</entry><entry>79.18 (&#xb1;1.00)</entry><entry>80.38 (&#xb1;0.86)</entry><entry>78.72 (&#xb1;0.91)</entry><entry>47.75 (&#xb1;1.6)&#x2002;</entry><entry>37.80 (&#xb1;1.39)</entry><entry>39.82 (&#xb1;1.33)</entry></row><row><entry>Hcls</entry><entry>80.29 (&#xb1;0.18)</entry><entry>82.23 (&#xb1;0.51)</entry><entry>80.08 (&#xb1;0.32)</entry><entry>49.66 (&#xb1;1.69)</entry><entry>38.38 (&#xb1;1.10)</entry><entry>40.92 (&#xb1;1.33)</entry></row><row><entry>Hcls + Focal</entry><entry>79.77 (&#xb1;1.12)</entry><entry>81.11 (&#xb1;0.61)</entry><entry>79.19 (&#xb1;0.62)</entry><entry>49.10 (&#xb1;1.63)</entry><entry>38.34 (&#xb1;0.72)</entry><entry>40.86 (&#xb1;0.59)</entry></row><row><entry>Hcls + Focal-cb</entry><entry>80.23 (&#xb1;0.69)</entry><entry>82.01 (&#xb1;0.58)</entry><entry>79.83 (&#xb1;0.39)</entry><entry>49.34 (&#xb1;0.83)</entry><entry>38.40 (&#xb1;0.86)</entry><entry>40.77 (&#xb1;0.78)</entry></row><row><entry>Hcls + C&#x2009;<img id="CUSTOM-CHARACTER-00027" he="2.46mm" wi="1.78mm" file="US20230004581A1-20230105-P00005.TIF" alt="custom-character" img-content="character" img-format="tif"/> </entry><entry>80.50 (&#xb1;0.36)</entry><entry>82.89 (&#xb1;0.18)</entry><entry>80.51 (&#xb1;0.29)</entry><entry>50.60 (&#xb1;1.5)&#x2002;</entry><entry>40.87 (&#xb1;1.4)&#x2002;</entry><entry>43.08 (&#xb1;1.04)</entry></row><row><entry>Hcls + CJ&#x2009;<img id="CUSTOM-CHARACTER-00028" he="2.46mm" wi="1.78mm" file="US20230004581A1-20230105-P00005.TIF" alt="custom-character" img-content="character" img-format="tif"/> &#x2009;<sub>1.4,0.5</sub></entry><entry>80.72 (&#xb1;0.2)&#x2002;</entry><entry>83.46 (&#xb1;0.29)</entry><entry>80.87 (&#xb1;0.22)</entry><entry>51.04 (&#xb1;0.92)</entry><entry>40.54 (&#xb1;0.51)</entry><entry>42.98 (&#xb1;0.41)</entry></row><row><entry/><entry/><entry/><entry/><entry/><entry/><entry>(&#x2191;2.06) (&#x2b06;3.16)</entry></row><row><entry>Hcls + CJ&#x2009;<img id="CUSTOM-CHARACTER-00029" he="2.46mm" wi="1.78mm" file="US20230004581A1-20230105-P00005.TIF" alt="custom-character" img-content="character" img-format="tif"/> &#x2009;<sub>1.4,&#x2212;</sub></entry><entry>80.65 (&#xb1;0.13)</entry><entry>83.10 (&#xb1;0.22)</entry><entry>80.67 (&#xb1;0.09)</entry><entry>52.12 (&#xb1;0.77)</entry><entry>41.48 (&#xb1;0.82)</entry><entry>43.93 (&#xb1;0.74)</entry></row><row><entry>Hcls + CJ&#x2009;<img id="CUSTOM-CHARACTER-00030" he="2.46mm" wi="1.78mm" file="US20230004581A1-20230105-P00005.TIF" alt="custom-character" img-content="character" img-format="tif"/> &#x2009;<sub>&#x2212;,0.5</sub></entry><entry>80.76 (&#xb1;0.14)</entry><entry>83.50 (&#xb1;0.43)</entry><entry>80.90 (&#xb1;0.14)</entry><entry>49.94 (&#xb1;0.67)</entry><entry>40.37 (&#xb1;0.21)</entry><entry>42.57 (&#xb1;0.36)</entry></row><row><entry>Hcls + CJ&#x2009;<img id="CUSTOM-CHARACTER-00031" he="2.46mm" wi="1.78mm" file="US20230004581A1-20230105-P00005.TIF" alt="custom-character" img-content="character" img-format="tif"/> &#x2009;<sub>5.0,0.5</sub></entry><entry>80.34 (&#xb1;0.48)</entry><entry>83.37 (&#xb1;0.39)</entry><entry>80.55 (&#xb1;0.42)</entry><entry>51.04 (&#xb1;1.66)</entry><entry>41.50 (&#xb1;1.45)</entry><entry>43.80 (&#xb1;1.51)</entry></row><row><entry/><entry/><entry/><entry/><entry/><entry/><entry>(&#x2191;2.88) (&#x2b06;3.98)</entry></row><row><entry namest="1" nameend="7" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0074" num="0078">B.3.(vi)(b) Assessment of Exemplary Results</p><p id="p-0075" num="0079">Referring to Table 3, the models of the embodiments achieve the best results with respect to the gold standard. An improvement of 2.87 points in micro-F1 compared to the baseline and 2.12 points compared to the initial hierarchical model for the Hcls+C<img id="CUSTOM-CHARACTER-00032" he="3.22mm" wi="2.12mm" file="US20230004581A1-20230105-P00003.TIF" alt="custom-character" img-content="character" img-format="tif"/><img id="CUSTOM-CHARACTER-00033" he="3.22mm" wi="2.46mm" file="US20230004581A1-20230105-P00002.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>1.4, 0.5 </sub>model is observed. The improvement reaches 3.16 and 2.06 points respectively in macro-F1. It is noted that the absolute scores may seem quite low, however, the gold standard consists exclusively of a subset of examples that the preliminary flat model fails to classify correctly as per the silver dataset, thus being very difficult to categorize. Hcls+C<img id="CUSTOM-CHARACTER-00034" he="3.22mm" wi="2.12mm" file="US20230004581A1-20230105-P00003.TIF" alt="custom-character" img-content="character" img-format="tif"/><img id="CUSTOM-CHARACTER-00035" he="3.22mm" wi="2.46mm" file="US20230004581A1-20230105-P00002.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>1.4, 0.5 </sub>performs better than the class-agnostic model, Hcls+C<img id="CUSTOM-CHARACTER-00036" he="3.22mm" wi="2.46mm" file="US20230004581A1-20230105-P00002.TIF" alt="custom-character" img-content="character" img-format="tif"/>, in terms of micro-F1. However, Hcls+C<img id="CUSTOM-CHARACTER-00037" he="3.22mm" wi="2.46mm" file="US20230004581A1-20230105-P00002.TIF" alt="custom-character" img-content="character" img-format="tif"/> is comparable, if not slightly better (by 0.1%), in macro-F1.</p><p id="p-0076" num="0080">Considering the results further, Hcls+C<img id="CUSTOM-CHARACTER-00038" he="3.22mm" wi="2.46mm" file="US20230004581A1-20230105-P00002.TIF" alt="custom-character" img-content="character" img-format="tif"/> applies the costs to all misclassifications, implicitly pushing the model to predict long-tail categories in a stronger manner than in the case of Hcls+C<img id="CUSTOM-CHARACTER-00039" he="3.22mm" wi="2.12mm" file="US20230004581A1-20230105-P00003.TIF" alt="custom-character" img-content="character" img-format="tif"/><img id="CUSTOM-CHARACTER-00040" he="3.22mm" wi="2.46mm" file="US20230004581A1-20230105-P00002.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>1.4, 0.5</sub>. Because of that, more POIs from head categories&#x2014;most heavily populated categories found at the head, i.e., the left-most part of the data distribution of <figref idref="DRAWINGS">FIG. <b>6</b></figref>, are wrongly misclassified, resulting in the drop in micro-F1. To some extent, the additional POls attributed to long-tail categories (i.e., less heavily populated categories) smooth out this difference in macro-F1. However, Hcls+C<img id="CUSTOM-CHARACTER-00041" he="3.22mm" wi="2.12mm" file="US20230004581A1-20230105-P00003.TIF" alt="custom-character" img-content="character" img-format="tif"/><img id="CUSTOM-CHARACTER-00042" he="3.22mm" wi="2.46mm" file="US20230004581A1-20230105-P00002.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>1.4, 0.5 </sub>has better overall performance.</p><p id="p-0077" num="0081">On the silver standard, the Hcls+C<img id="CUSTOM-CHARACTER-00043" he="3.22mm" wi="2.12mm" file="US20230004581A1-20230105-P00003.TIF" alt="custom-character" img-content="character" img-format="tif"/><img id="CUSTOM-CHARACTER-00044" he="3.22mm" wi="2.46mm" file="US20230004581A1-20230105-P00002.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>1.4, 0.5 </sub>model achieves better results than the no-cost models in terms of macro-F1, gaining 2.15 points compared to the baseline and 0.79 points to the initial hierarchical model, Hcls. This may be due to long-tail POIs being predicted more often. On the other hand, the Hcls model has comparable (or even slightly better) micro-F1 to the Hcls+C<img id="CUSTOM-CHARACTER-00045" he="3.22mm" wi="2.12mm" file="US20230004581A1-20230105-P00003.TIF" alt="custom-character" img-content="character" img-format="tif"/><img id="CUSTOM-CHARACTER-00046" he="3.22mm" wi="2.46mm" file="US20230004581A1-20230105-P00002.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>1.4, 0.5 </sub>model (0.09 points decrease). This is somewhat expected since the silver standard shares the same issue of joker classes with the training data. The results on the gold standard, where the Hcls+C<img id="CUSTOM-CHARACTER-00047" he="3.22mm" wi="2.12mm" file="US20230004581A1-20230105-P00003.TIF" alt="custom-character" img-content="character" img-format="tif"/><img id="CUSTOM-CHARACTER-00048" he="3.22mm" wi="2.46mm" file="US20230004581A1-20230105-P00002.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>1.4, 0.5 </sub>model has significantly better scores than the Hcls one, is also a strong indication of that.</p><p id="p-0078" num="0082">Surprisingly, the Hcls+focal-cb model does not have significantly better scores when compared to the initial Hcls model on the gold standard. This is in contrast to the results of the Base+focal-cb model, as shown in Section B.3.(vi)(b) below, which outperforms the baseline as expected, since it tends to favor long-tail rather than head categories. Detailed scores of all flat classifier-based models are included in Section B.3.(vi)(c) below. The absolute scores for the flat based models are lower than in the hierarchical case.</p><p id="p-0079" num="0083">Further assessment of the impact of misclassification costs and of selecting different sets of joker classes follows. All of the following results reported are three-run averages.</p><p id="p-0080" num="0084"><figref idref="DRAWINGS">FIG. <b>7</b></figref> and <figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrate how the results change per modulation of the value of each of the two costs that have been defined. The Hcls+C<img id="CUSTOM-CHARACTER-00049" he="3.22mm" wi="2.12mm" file="US20230004581A1-20230105-P00003.TIF" alt="custom-character" img-content="character" img-format="tif"/><img id="CUSTOM-CHARACTER-00050" he="3.22mm" wi="2.46mm" file="US20230004581A1-20230105-P00002.TIF" alt="custom-character" img-content="character" img-format="tif"/> model is used for both <figref idref="DRAWINGS">FIG. <b>7</b></figref> and <figref idref="DRAWINGS">FIG. <b>8</b></figref>. <figref idref="DRAWINGS">FIG. <b>7</b></figref> includes F1 scores for different cost values where misclassifications relate to predicting shorter paths than the ones in the training data. <figref idref="DRAWINGS">FIG. <b>8</b></figref> includes F1 scores for different cost values where misclassifications relate to predicting longer paths than the ones in the training data.</p><p id="p-0081" num="0085">Increasing the cost of misclassifications related to predicting shorter paths improves the results on the gold standard, reaching a maximum score at the value of 5.0 for both micro and macro-F1. The scores on the silver standard deteriorate, as the dataset has the same issue as the training data i.e., skewed distribution because of joker classes. As the cost increases, more POIs are re-attributed from head to long-tail classes, causing the drop in the silver standard and the increase in the gold one.</p><p id="p-0082" num="0086">Decreasing the cost of misclassifications related to longer path predictions improves micro-F1 on the gold standard. Top values are reached at costs 0.5 and 0.4. Macro-F1 results are less stable. They decrease up to the cost of 0.5. However, at 0.4 there is a sudden peak, with the model predicting significantly more long-tail categories.</p><p id="p-0083" num="0087">It is noteworthy that both costs not only help but are actually rather complementary. For instance, as shown in Table 3, keeping only the cost related to shorter paths gives high precision scores in both datasets. On the other hand, micro-recall mainly benefits from the cost given to longer paths. On the macro-scores, the latter cost does very well on the silver standard, while the former on the gold standard.</p><p id="p-0084" num="0088">In one example, the combination of the costs &#x3c;5.0,0.5&#x3e; results in the best model (best balance of micro and macro-F1 scores) on the gold standard. The model gains 4.89 points on the micro-F1 when compared to the baseline, and 4.14 points compared to the hierarchical model. The improvement reaches respectively 3.98% and 2.88% in macro-F1.</p><p id="p-0085" num="0089">As observed with respect to the results of Table 3, selecting a set of joker classes rather than applying the costs in a class-agnostic manner, results in better micro-F1 and more balanced overall performance. However, as a direct implication of the embodiments, it is natural to assume that most of joker classes should be (1) part of the head categories and (2) located at the top levels of the categorization hierarchy.</p><p id="p-0086" num="0090"><figref idref="DRAWINGS">FIG. <b>9</b></figref> demonstrates how the scores change according to the number of head categories considered as joker classes. Costs &#x3c;1.4,0.5&#x3e; were applied to misclassifications in all experiments. As follows, the macro-F1 increases with more categories, reaching a maximum at fifty head categories. At the same point, when compared to the set of categories selected using the above-disclosed Method <b>1</b> shown in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, the macro-F1 is higher, while the micro-F1 lower. A similar behavior to the joker class-agnostic class model is thus observed, as more POIs are re-attributed to long-tail categories, albeit the absolute scores being better in this case.</p><p id="p-0087" num="0091"><figref idref="DRAWINGS">FIG. <b>10</b></figref> illustrates how the scores evolve as the hierarchical levels are increased from which categories are extracted. In more detail, 17 categories are taken from the second level and 903 categories from the third level. It is then observed that as more levels are added, the macro-F1 increases on the gold standard, while the micro-F1 remains relatively unchanged. Compared to using the technique of the joker class-specific cost, micro-F1s are lower in both datasets, while the macro-F1 is slightly higher in the gold standard after the third level of the hierarchy. These results are not surprising, as after the third level, the misclassification costs are applied to significantly more categories (22 categories for C<img id="CUSTOM-CHARACTER-00051" he="3.22mm" wi="2.12mm" file="US20230004581A1-20230105-P00003.TIF" alt="custom-character" img-content="character" img-format="tif"/> and 920 for the level<sub>2</sub>, 3 model). Thus, the model tends to predict long-tail categories in a stronger fashion. While this results in a small improvement in macro-F1, the decrease in terms of micro-F1 is rather noteworthy. Overall, the Hcls+C<img id="CUSTOM-CHARACTER-00052" he="3.22mm" wi="2.12mm" file="US20230004581A1-20230105-P00003.TIF" alt="custom-character" img-content="character" img-format="tif"/><img id="CUSTOM-CHARACTER-00053" he="3.22mm" wi="2.46mm" file="US20230004581A1-20230105-P00002.TIF" alt="custom-character" img-content="character" img-format="tif"/> model shows a more balanced performance.</p><p id="p-0088" num="0092">In view of the above description, various advantages of the embodiments should be readily apparent to those skilled in the art:</p><p id="p-0089" num="0093">For instance, when classifying hierarchically arranged POIs in a production database with ML-based supervised category prediction, it is unrealistic to count on the existence of a perfectly annotated training set. The embodiments disclose that many of the training labels of the training set tend to be noisy or under-specified (i.e., they point to categories found at higher levels of hierarchy than the correct ones). This precludes straightforward classification. Hence, the embodiments teach a robust learning approach that accommodates for noisy training sets by (1) detecting problematic categories, i.e., over-represented categories, based on the misclassifications of an initial hierarchical classifier, and then (2) re-training the classifier from scratch, introducing a weight to a standard cross-entropy loss function that specifically targets incorrect predictions of the detected categories.</p><p id="p-0090" num="0094">After extensive experiments it has been found that, through use of a gold standard, improvements of up to 4.89% in micro-F1 and 3.98% in macro-F1 are achievable. Predictions based on the embodiments have been used to enable correction of existing annotations in a production database. For example, on the dataset used in the embodiments, more than 11,000 POIs were corrected as a focus was placed on misclassifications with a probability greater than 0.9. Accordingly, it has been found that the resulting classifier may be used not only to impute categories to new POIs, but to curate and correct manually added ones as well.</p><p id="p-0091" num="0095">B.3.(vii) Exemplary Assessments</p><p id="p-0092" num="0096">B.3.(vii)(a) Preliminary Classifiers</p><p id="p-0093" num="0097">As baseline, the flat classifier proposed by Lagos et al., which encodes text using 1-gram character LSTMs, is used. All reported results have been computed on the development dataset disclosed in the description. As transformer-based architectures and sub-word-based representations are state-of-art in NLP (Natural Language Processing), experiments were performed with the fairseq standard transformer-based model (see Ott et al., fairseq: <i>A Fast, Extensible Toolkit for Sequence Modeling, </i>In Proceedings of NAACL-HLT 2019: Demonstrations) [&#x201c;Ott et al.&#x201d;], with and without byte-pair encoding (BPE) (see Sennrich et al., <i>Neural Machine Translation of Rare Words with Subword Units, </i>in Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Association for Computational Linguistics, Berlin, Germany, 1715-1725 (2016)) pre-processing step for both models [the extension &#x201c;_bpe&#x201d; is used in Table 4]. More specifically, Table 4 is a comparison of the fairseq standard transformer-based model Ott et al. and the flat classifier proposed by Lagos et al. based on 1-gram character and BPE LSTMs in the setting described above, where best results are in bold.</p><p id="p-0094" num="0000"><tables id="TABLE-US-00004" num="00004"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="4"><colspec colname="1" colwidth="77pt" align="left"/><colspec colname="2" colwidth="49pt" align="center"/><colspec colname="3" colwidth="49pt" align="center"/><colspec colname="4" colwidth="42pt" align="center"/><thead><row><entry namest="1" nameend="4" rowsep="1">TABLE 4</entry></row><row><entry namest="1" nameend="4" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry>Model</entry><entry>Micro-prec.</entry><entry>Micro-rec.</entry><entry>Micro-F1</entry></row><row><entry namest="1" nameend="4" align="center" rowsep="1"/></row><row><entry>Transformer</entry><entry>0.6511</entry><entry>0.6511</entry><entry>0.6511</entry></row><row><entry>Transformer_bpe</entry><entry>0.6599</entry><entry>0.6599</entry><entry>0.6599</entry></row><row><entry>Lagos et al. [14]</entry><entry>0.7034</entry><entry>0.6338</entry><entry><b>0.6668</b></entry></row><row><entry>Lagos et al. [14]_bpe</entry><entry>0.6889</entry><entry>0.5794</entry><entry>0.6294</entry></row><row><entry namest="1" nameend="4" align="center" rowsep="1"/></row><row><entry/><entry>Macro-prec.</entry><entry>Macro-rec.</entry><entry>Macro-F1</entry></row><row><entry namest="1" nameend="4" align="center" rowsep="1"/></row><row><entry>Transformer</entry><entry>0.7561</entry><entry>0.7724</entry><entry>0.7518</entry></row><row><entry>Transformer_bpe</entry><entry>0.7469</entry><entry>0.7679</entry><entry>0.7445</entry></row><row><entry>Lagos et al. [14]</entry><entry>0.7619</entry><entry>0.7723</entry><entry><b>0.7541</b></entry></row><row><entry>Lagos et al. [14]_bpe</entry><entry>0.7736</entry><entry>0.7479</entry><entry>0.7484</entry></row><row><entry namest="1" nameend="4" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0095" num="0098">As shown in Table 4, the results are comparable. Only the results of the final optimal configuration for Lagos et al. are reported here, counting only predictions having a minimum probability threshold of 0.4. It is worth noting that POI attributes in Naver's database are, for the most part, written in Korean (although some multi-script names may be found), and the NFC (form C) unicode normalization format has generally been used.</p><p id="p-0096" num="0099">B.3.(vii)(b) Dataset Details</p><p id="p-0097" num="0100">In Table 5 shows the top ten most popular categories and the percentage of POIs attributed to them in the dataset.</p><p id="p-0098" num="0000"><tables id="TABLE-US-00005" num="00005"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="offset" colwidth="14pt" align="left"/><colspec colname="1" colwidth="147pt" align="left"/><colspec colname="2" colwidth="56pt" align="center"/><thead><row><entry/><entry namest="offset" nameend="2" rowsep="1">TABLE 5</entry></row><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row><row><entry/><entry>Category</entry><entry>POI perc.</entry></row><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="offset" colwidth="14pt" align="left"/><colspec colname="1" colwidth="147pt" align="left"/><colspec colname="2" colwidth="56pt" align="char" char="."/><tbody valign="top"><row><entry/><entry>&#x2225;Korean Food</entry><entry>15.61%</entry></row><row><entry/><entry>&#x2225;Cafe, Dessert</entry><entry>5.33%</entry></row><row><entry/><entry>&#x2225;Korean Food&#x2225;Meat, Food</entry><entry>3.92%</entry></row><row><entry/><entry>&#x2225;Cafe, Dessert&#x2225;Cafe</entry><entry>3.86%</entry></row><row><entry/><entry>&#x2225;Chinese Food&#x2225;Chinese Restaurant</entry><entry>3.38%</entry></row><row><entry/><entry>&#x2225;Bar&#x2225;Beer, Hof, Pub&#x2225;Beer, Hof</entry><entry>3.27%</entry></row><row><entry/><entry>&#x2225;Chicken dakgangjeong</entry><entry>2.48%</entry></row><row><entry/><entry>&#x2225;Korean Food&#x2225;Seafood&#x2225;Sliced Raw Fish&#x2225;Sashimi</entry><entry>2.44%</entry></row><row><entry/><entry>&#x2225;Korean Snack</entry><entry>2.4%</entry></row><row><entry/><entry>&#x2225;Korean Food&#x2225;Meat, Food&#x2225;Pork&#x2225;Grilled Pork</entry><entry>1.69%</entry></row><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0099" num="0101">These 10 categories account for 44% of all the POIs. The dataset has a very long queue of scarcely used categories, as shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, with the 461 tail categories having fewer than 5 POIs each. In Table 5, the root category &#x201c;&#x2225;Restaurant&#x201d; is common to each class, so it has been omitted for the sake of saving space, and all categories have been translated from Korean into English.</p><p id="p-0100" num="0102">B..3.(vii)(c) Hierarchical and Baseline Models</p><p id="p-0101" num="0103">Table 6 illustrates how the micro and macro scores evolve as more local classifier layers are added to the embodied architecture, in order to take into account more hierarchical levels. More specifically, Table 6 sets out the performance of hierarchical model in the silver standard as more levels of the hierarchy are added. The number in the suffix indicates the levels included in the model. As shown in Table 6 performance degrades as more levels are added. Contrary to state-of-the-art where the addition of hierarchy levels improves results, the results of the disclosed embodiments are comparable or even deteriorate with such addition.</p><p id="p-0102" num="0000"><tables id="TABLE-US-00006" num="00006"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="offset" colwidth="14pt" align="left"/><colspec colname="1" colwidth="35pt" align="left"/><colspec colname="2" colwidth="168pt" align="center"/><thead><row><entry/><entry namest="offset" nameend="2" rowsep="1">TABLE 6</entry></row><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row><row><entry/><entry/><entry>Dataset</entry></row><row><entry/><entry/><entry>Silver standard</entry></row><row><entry/><entry>Model</entry><entry>Metric</entry></row><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="5"><colspec colname="offset" colwidth="14pt" align="left"/><colspec colname="1" colwidth="35pt" align="left"/><colspec colname="2" colwidth="63pt" align="center"/><colspec colname="3" colwidth="49pt" align="center"/><colspec colname="4" colwidth="56pt" align="center"/><tbody valign="top"><row><entry/><entry/><entry>Micro-prec.</entry><entry>Micro-rec.</entry><entry>Micro-F1</entry></row><row><entry/><entry namest="offset" nameend="4" align="center" rowsep="1"/></row><row><entry/><entry>Hcls<sub>2</sub></entry><entry>72.85(&#xb1;0.26)</entry><entry>63.90(&#xb1;0.26)</entry><entry>68.08(&#xb1;0.05)</entry></row><row><entry/><entry>Hcls<sub>2, 3</sub></entry><entry>73.17(&#xb1;0.16)</entry><entry>63.52(&#xb1;0.26)</entry><entry>68.00(&#xb1;0.1)&#x2002;</entry></row><row><entry/><entry>Hcls<sub>2, 3, 4</sub></entry><entry>73.10(&#xb1;0.18)</entry><entry>63.48(&#xb1;0.25)</entry><entry>67.95(&#xb1;0.12)</entry></row><row><entry/><entry namest="offset" nameend="4" align="center" rowsep="1"/></row><row><entry/><entry/><entry>Macro-prec.</entry><entry>Macro-rec.</entry><entry>Macro-F1</entry></row><row><entry/><entry namest="offset" nameend="4" align="center" rowsep="1"/></row><row><entry/><entry>Hcls<sub>2</sub></entry><entry>80.29(&#xb1;0.18)</entry><entry>82.23(&#xb1;0.51)</entry><entry>80.08(&#xb1;0.32)</entry></row><row><entry/><entry>Hcls<sub>2, 3</sub></entry><entry>78.69(&#xb1;0.62)</entry><entry>80.66(&#xb1;0.82)</entry><entry>78.44(&#xb1;0.69)</entry></row><row><entry/><entry>Hcls<sub>2, 3, 4</sub></entry><entry>76.97(&#xb1;0.86)</entry><entry>78.52(&#xb1;0.95)</entry><entry>76.51(&#xb1;0.87)</entry></row><row><entry/><entry namest="offset" nameend="4" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0103" num="0104">The following Table 7 illustrates accordingly the evolution of the scores on the gold standard. More specifically, Table 7 shows performance of hierarchical model on the gold standard as more levels of the hierarchy are added. The number in the suffix indicates the levels included in the model. As shown in Table 7, no improvement is observed contrary to findings in related work.</p><p id="p-0104" num="0000"><tables id="TABLE-US-00007" num="00007"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="offset" colwidth="14pt" align="left"/><colspec colname="1" colwidth="35pt" align="left"/><colspec colname="2" colwidth="168pt" align="center"/><thead><row><entry/><entry namest="offset" nameend="2" rowsep="1">TABLE 7</entry></row><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row><row><entry/><entry/><entry>Dataset</entry></row><row><entry/><entry/><entry>Gold standard</entry></row><row><entry/><entry>Model</entry><entry>Metric</entry></row><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="5"><colspec colname="offset" colwidth="14pt" align="left"/><colspec colname="1" colwidth="35pt" align="left"/><colspec colname="2" colwidth="63pt" align="center"/><colspec colname="3" colwidth="49pt" align="center"/><colspec colname="4" colwidth="56pt" align="center"/><tbody valign="top"><row><entry/><entry/><entry>Micro-prec.</entry><entry>Micro-rec.</entry><entry>Micro-F1</entry></row><row><entry/><entry namest="offset" nameend="4" align="center" rowsep="1"/></row><row><entry/><entry>Hcls<sub>2</sub></entry><entry>50.32(&#xb1;1.14)</entry><entry>31.99(&#xb1;0.46)</entry><entry>39.11(&#xb1;0.58)</entry></row><row><entry/><entry>Hcls<sub>2, 3</sub></entry><entry>49.92(&#xb1;1.02)</entry><entry>31.74(&#xb1;0.48)</entry><entry>38.80(&#xb1;0.61)</entry></row><row><entry/><entry>Hcls<sub>2, 3, 4</sub></entry><entry>50.78(&#xb1;0.82)</entry><entry>32.03(&#xb1;1.49)</entry><entry>39.26(&#xb1;1.19)</entry></row><row><entry/><entry namest="offset" nameend="4" align="center" rowsep="1"/></row><row><entry/><entry/><entry>Macro-prec.</entry><entry>Macro-rec.</entry><entry>Macro-F1</entry></row><row><entry/><entry namest="offset" nameend="4" align="center" rowsep="1"/></row><row><entry/><entry>Hcls<sub>2</sub></entry><entry>49.66(&#xb1;1.69)</entry><entry>38.38(&#xb1;1.10)</entry><entry>40.92(&#xb1;1.33)</entry></row><row><entry/><entry>Hcls<sub>2, 3</sub></entry><entry>48.72(&#xb1;1.84)</entry><entry>38.20(&#xb1;1.09)</entry><entry>40.47(&#xb1;1.43)</entry></row><row><entry/><entry>Hcls<sub>2, 3, 4</sub></entry><entry>48.19(&#xb1;1.52)</entry><entry>37.46(&#xb1;1.1)&#x2002;</entry><entry>39.78(&#xb1;1.18)</entry></row><row><entry/><entry namest="offset" nameend="4" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0105" num="0105">Tables 8 and 9 below illustrate the results of the flat classifier-based models on the silver and gold standards. More specifically, Tables 8 and 9 show average performance (%) over 5 runs on the silver and gold standards of the flat classifier-based models, respectively. Standard deviation is also reported in Tables 8 and 9. As in the hierarchical case, the disclosed flat models achieve the best results in both the silver and gold standards when compared to the rest flat classifiers. Note, however, that absolute scores of all models are lower than in the hierarchical case.</p><p id="p-0106" num="0000"><tables id="TABLE-US-00008" num="00008"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="70pt" align="left"/><colspec colname="2" colwidth="147pt" align="center"/><thead><row><entry namest="1" nameend="2" rowsep="1">TABLE 8</entry></row><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row><row><entry/><entry>Dataset</entry></row><row><entry/><entry>Gold standard</entry></row><row><entry>Model</entry><entry>Metric</entry></row><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="4"><colspec colname="1" colwidth="70pt" align="left"/><colspec colname="2" colwidth="49pt" align="center"/><colspec colname="3" colwidth="49pt" align="center"/><colspec colname="4" colwidth="49pt" align="center"/><tbody valign="top"><row><entry/><entry>Micro-prec.</entry><entry>Micro-rec.</entry><entry>Micro-F1</entry></row><row><entry namest="1" nameend="4" align="center" rowsep="1"/></row><row><entry>Base</entry><entry>46.57(&#xb1;2.02)</entry><entry>32.62(&#xb1;0.47)</entry><entry>38.36(&#xb1;0.88)</entry></row><row><entry>Base + Focal</entry><entry>46.77(&#xb1;2.23)</entry><entry>32.69(&#xb1;1.33)</entry><entry>38.48(&#xb1;1.67)</entry></row><row><entry>Base + Focal-cb</entry><entry>47.67(&#xb1;1.34)</entry><entry>33.42(&#xb1;0.82)</entry><entry>39.29(&#xb1;0.98)</entry></row><row><entry>Base + C&#x2009;<img id="CUSTOM-CHARACTER-00054" he="2.79mm" wi="2.12mm" file="US20230004581A1-20230105-P00006.TIF" alt="custom-character" img-content="character" img-format="tif"/> </entry><entry>50.52(&#xb1;0.74)</entry><entry>33.31(&#xb1;0.7)&#x2002;</entry><entry>40.14(&#xb1;0.45)</entry></row><row><entry>Base + CJ&#x2009;<img id="CUSTOM-CHARACTER-00055" he="2.79mm" wi="2.12mm" file="US20230004581A1-20230105-P00006.TIF" alt="custom-character" img-content="character" img-format="tif"/> &#x2009;<sub>1.4, 0.5</sub></entry><entry>49.76(&#xb1;0.73)</entry><entry>&#x2002;33.8(&#xb1;0.58)</entry><entry>40.25(&#xb1;0.48)</entry></row><row><entry namest="1" nameend="4" align="center" rowsep="1"/></row><row><entry/><entry>Macro-prec.</entry><entry>Macro-rec.</entry><entry>Macro-F1</entry></row><row><entry namest="1" nameend="4" align="center" rowsep="1"/></row><row><entry>Base</entry><entry>47.75(&#xb1;1.6)&#x2002;</entry><entry>37.80(&#xb1;1.39)</entry><entry>39.82(&#xb1;1.33)</entry></row><row><entry>Base + Focal</entry><entry>46.91(&#xb1;1.83)</entry><entry>36.82(&#xb1;0.57)</entry><entry>39.15(&#xb1;0.77)</entry></row><row><entry>Base + Focal-cb</entry><entry>47.80(&#xb1;0.45)</entry><entry>38.13(&#xb1;0.66)</entry><entry>40.26(&#xb1;0.67)</entry></row><row><entry>Base + C&#x2009;<img id="CUSTOM-CHARACTER-00056" he="2.79mm" wi="2.12mm" file="US20230004581A1-20230105-P00006.TIF" alt="custom-character" img-content="character" img-format="tif"/> </entry><entry>50.13(&#xb1;1.7)&#x2002;</entry><entry>40.42(&#xb1;1.62)</entry><entry>42.49(&#xb1;1.60)</entry></row><row><entry>Base + CJ&#x2009;<img id="CUSTOM-CHARACTER-00057" he="2.79mm" wi="2.12mm" file="US20230004581A1-20230105-P00006.TIF" alt="custom-character" img-content="character" img-format="tif"/> &#x2009;<sub>1.4, 0.5</sub></entry><entry>48.38(&#xb1;1.23)</entry><entry>40.26(&#xb1;1.22)</entry><entry>41.99(&#xb1;1.23)</entry></row><row><entry namest="1" nameend="4" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0107" num="0000"><tables id="TABLE-US-00009" num="00009"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="70pt" align="left"/><colspec colname="2" colwidth="147pt" align="center"/><thead><row><entry namest="1" nameend="2" rowsep="1">TABLE 9</entry></row><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row><row><entry/><entry>Dataset</entry></row><row><entry/><entry>Silver standard</entry></row><row><entry>Model</entry><entry>Metric</entry></row><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="4"><colspec colname="1" colwidth="70pt" align="left"/><colspec colname="2" colwidth="49pt" align="center"/><colspec colname="3" colwidth="49pt" align="center"/><colspec colname="4" colwidth="49pt" align="center"/><tbody valign="top"><row><entry/><entry>Micro-prec.</entry><entry>Micro-rec.</entry><entry>Micro-F1</entry></row><row><entry namest="1" nameend="4" align="center" rowsep="1"/></row><row><entry>Base</entry><entry>70.41(&#xb1;0.27)</entry><entry>63.78(&#xb1;0.26)</entry><entry>66.93(&#xb1;0.2)&#x2002;</entry></row><row><entry>Base + Focal</entry><entry>70.09(&#xb1;0.16)</entry><entry>63.56(&#xb1;0.17)</entry><entry>66.67(&#xb1;0.08)</entry></row><row><entry>Base + Focal-cb</entry><entry>70.22(&#xb1;0.21)</entry><entry>63.56(&#xb1;0.16)</entry><entry>66.73(&#xb1;0.07)</entry></row><row><entry>Base + C&#x2009;<img id="CUSTOM-CHARACTER-00058" he="2.79mm" wi="2.12mm" file="US20230004581A1-20230105-P00006.TIF" alt="custom-character" img-content="character" img-format="tif"/> </entry><entry>71.15(&#xb1;0.21)</entry><entry>62.88(&#xb1;0.19)</entry><entry>66.76(&#xb1;0.11)</entry></row><row><entry>Base + CJ&#x2009;<img id="CUSTOM-CHARACTER-00059" he="2.79mm" wi="2.12mm" file="US20230004581A1-20230105-P00006.TIF" alt="custom-character" img-content="character" img-format="tif"/> &#x2009;<sub>1.4, 0.5</sub></entry><entry>70.94(&#xb1;0.36)</entry><entry>63.63(&#xb1;0.13)</entry><entry>67.08(&#xb1;0.13)</entry></row><row><entry namest="1" nameend="4" align="center" rowsep="1"/></row><row><entry/><entry>Macro-prec.</entry><entry>Macro-rec.</entry><entry>Macro-F1</entry></row><row><entry namest="1" nameend="4" align="center" rowsep="1"/></row><row><entry>Base</entry><entry>79.18(&#xb1;1.00)</entry><entry>80.38(&#xb1;0.86)</entry><entry>78.72(&#xb1;0.91)</entry></row><row><entry>Base + Focal</entry><entry>78.20(&#xb1;0.68)</entry><entry>78.50(&#xb1;0.76)</entry><entry>77.28(&#xb1;0.72)</entry></row><row><entry>Base + Focal-cb</entry><entry>78.32(&#xb1;0.24)</entry><entry>79.01(&#xb1;0.48)</entry><entry>77.60(&#xb1;0.36)</entry></row><row><entry>Base + C&#x2009;<img id="CUSTOM-CHARACTER-00060" he="2.79mm" wi="2.12mm" file="US20230004581A1-20230105-P00006.TIF" alt="custom-character" img-content="character" img-format="tif"/> </entry><entry>80.00(&#xb1;0.15)</entry><entry>81.62(&#xb1;0.81)</entry><entry>79.70(&#xb1;0.14)</entry></row><row><entry>Base + CJ&#x2009;<img id="CUSTOM-CHARACTER-00061" he="2.79mm" wi="2.12mm" file="US20230004581A1-20230105-P00006.TIF" alt="custom-character" img-content="character" img-format="tif"/> &#x2009;<sub>1.4, 0.5</sub></entry><entry>79.97(&#xb1;0.31)</entry><entry>81.46(&#xb1;0.63)</entry><entry>79.64(&#xb1;0.45)</entry></row><row><entry namest="1" nameend="4" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><heading id="h-0009" level="1">C. General</heading><p id="p-0108" num="0106">In one embodiment there is disclosed a method implemented with one or more processors for improving classification of labels and categories of a database stored in memory that includes a set of labels and a set of categories where (1) each label in the set of labels points to at least one of the categories in the set of categories, and (2) each label in the set of labels is associated with a hierarchical category path. The one or more processors applying both a subset of the set of labels and a subset of the set of categories of the database stored in the memory to a first classifier for classifying the subset of labels with respect to the subset of categories, the first classifier including a first loss function, and determining, based on applying both the subset of labels and the subset of categories to the first classifier, whether at least one label in the subset of labels of the database stored in the memory has been misclassified. In response to determining that at least one label in the subset of labels of the database stored in the memory has been misclassified based on applying both the subset of labels and the subset of categories to the first classifier: the one or more processors changing the first loss function of the first classifier to a second loss function to form a second classifier including the second loss function, and applying both the subset of labels and the subset of categories to the second classifier for classifying the subset of labels with respect to the subset of categories of the database stored in the memory for improving its classification of labels and categories.</p><p id="p-0109" num="0107">In one example, the first loss function comprises a global categorical cross-entropy loss function, and wherein changing the first loss function to the second loss function comprises changing the global categorical cross-entropy loss function to a weighted-by-sample categorical cross entropy loss function.</p><p id="p-0110" num="0108">In another example, the weighted-by-sample categorical cross entropy loss function includes:</p><p id="p-0111" num="0000"><maths id="MATH-US-00003" num="00003"><math overflow="scroll"> <mrow>  <msubsup>   <mi>&#x2112;</mi>   <mi>G</mi>   <mo>&#x2032;</mo>  </msubsup>  <mo>=</mo>  <mrow>   <mfrac>    <mn>1</mn>    <mi>N</mi>   </mfrac>   <mo>&#x2062;</mo>   <mrow>    <munderover>     <mo>&#x2211;</mo>     <mrow>      <mi>i</mi>      <mo>=</mo>      <mn>1</mn>     </mrow>     <mi>N</mi>    </munderover>    <mrow>     <msub>      <mi>a</mi>      <mrow>       <msub>        <mover>         <mi>y</mi>         <mo>^</mo>        </mover>        <mi>i</mi>       </msub>       <mo>,</mo>       <msub>        <mi>y</mi>        <mi>i</mi>       </msub>      </mrow>     </msub>     <mo>&#x2062;</mo>     <msub>      <mi>&#x2112;</mi>      <mrow>       <mi>G</mi>       <mo>,</mo>       <mi>i</mi>      </mrow>     </msub>    </mrow>   </mrow>  </mrow> </mrow></math></maths></p><p id="p-0112" num="0109">where:</p><p id="p-0113" num="0110"><img id="CUSTOM-CHARACTER-00062" he="3.22mm" wi="2.46mm" file="US20230004581A1-20230105-P00002.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>G,i </sub>is the global catigorical cross-entory loss for sample i and</p><p id="p-0114" num="0000"><maths id="MATH-US-00004" num="00004"><math overflow="scroll"> <mrow>  <msub>   <mi>a</mi>   <mrow>    <msub>     <mover>      <mi>y</mi>      <mo>^</mo>     </mover>     <mi>i</mi>    </msub>    <mo>,</mo>    <msub>     <mi>y</mi>     <mi>i</mi>    </msub>   </mrow>  </msub>  <mo>=</mo>  <mrow>   <mo>{</mo>   <mrow>    <mtable>     <mtr>      <mtd>       <mrow>        <msub>         <mi>a</mi>         <mi>shorter</mi>        </msub>        <mo>,</mo>       </mrow>      </mtd>      <mtd>       <mrow>        <mi>if</mi>        <mo>&#x2062;</mo>        <mtext>   </mtext>        <mrow>         <msub>          <mover>           <mi>t</mi>           <mo>^</mo>          </mover>          <mi>i</mi>         </msub>         <mo>.</mo>         <mtext>   </mtext>         <mi>prefix_path</mi>        </mrow>        <mo>&#x2062;</mo>        <mi>_of</mi>        <mo>&#x2062;</mo>        <mtext>   </mtext>        <mrow>         <mo>(</mo>         <msubsup>          <mi>t</mi>          <mi>i</mi>          <mo>&#x2032;</mo>         </msubsup>         <mo>)</mo>        </mrow>       </mrow>      </mtd>     </mtr>     <mtr>      <mtd>       <mrow>        <msub>         <mi>a</mi>         <mi>longer</mi>        </msub>        <mo>,</mo>       </mrow>      </mtd>      <mtd>       <mrow>        <mi>if</mi>        <mo>&#x2062;</mo>        <mtext>   </mtext>        <mrow>         <msubsup>          <mi>t</mi>          <mi>i</mi>          <mo>&#x2032;</mo>         </msubsup>         <mo>.</mo>         <mtext>   </mtext>         <mi>prefix_path</mi>        </mrow>        <mo>&#x2062;</mo>        <mi>_of</mi>        <mo>&#x2062;</mo>        <mtext>   </mtext>        <mrow>         <mo>(</mo>         <msub>          <mover>           <mi>t</mi>           <mo>^</mo>          </mover>          <mi>i</mi>         </msub>         <mo>)</mo>        </mrow>       </mrow>      </mtd>     </mtr>     <mtr>      <mtd>       <mrow>        <mn>1</mn>        <mo>,</mo>       </mrow>      </mtd>      <mtd>       <mi>otherwise</mi>      </mtd>     </mtr>    </mtable>    <mo>.</mo>   </mrow>  </mrow> </mrow></math></maths></p><p id="p-0115" num="0000">{circumflex over (t)}<sub>i </sub>is the path corresponding to the &#x177;<sub>i </sub>prediction;<br/>t<sub>i</sub>&#x2032; is the observed path corresponding to y<sub>i</sub>;<br/>a<sub>shorter </sub>denotes the cost of predicting a shorter path than an observed one; and<br/>a<sub>longer </sub>denotes the cost of predicting a longer path than the observed one.</p><p id="p-0116" num="0111">In yet other examples: the cost assigned to a<sub>shorter </sub>is greater than the cost assigned to a<sub>longer</sub>; each one of the first classifier and second classifier comprises a hybrid hierarchical classifier with the hybrid hierarchical classifier including a global classifier and at least one local classifier; the global classifier includes a loss, and wherein changing the first loss function in the first classifier to the second loss function in the second classifier comprises introducing a weight to the global classifier's loss; the determining determines that at least one label in the subset of labels has been misclassified using a probability threshold; the labels and categories relate to points-of-interest; the applying both the subset of labels and the subset of categories to the second classifier enables correction of the at least one label in the subset of labels that has been determined to be misclassified.</p><p id="p-0117" num="0112">In another embodiment there is disclosed a method implemented with one or more processors for improving classification of labels and categories of a database stored in memory that includes a set of labels and a set of categories where (1) each label in the set of labels points to at least one of the categories in the set of categories, (2) the labels in the set of labels are disposed in a label hierarchy with both the labels and the categories being arranged throughout a plurality of levels, and (3) each label in the set of labels is associated with a path. The one or more processors applying both a subset of the set of labels and a subset of the set of categories of the database stored in the memory to a first classifier for classifying the subset of labels with respect to the subset of categories, the first classifier including a first loss function, and determining, based on applying both the subset of labels and the subset of categories to the first classifier, whether at least one label in the subset of labels of the database stored in the memory has a path terminating at a category found at one of the levels in the label hierarchy, and whether the at least one label in the subset of labels of the database stored in the memory corresponds to an over-represented category. In response to determining that at least one label in the subset of labels of the database stored in the memory has a path terminating at a category found at one of the levels in the label hierarchy, and that the at least one label in the subset of labels of the database stored in the memory corresponds to an over-represented category: the one or more processors changing the first loss function of the first classifier to a second loss function to form a second classifier including the second loss function, and applying both the subset of labels and the subset of categories to the second classifier for classifying the subset of labels with respect to the subset of categories of the database stored in the memory for improving its classification of labels and categories.</p><p id="p-0118" num="0113">In other examples: the determining determines using a maximum hierarchy depth to preclude consideration of misclassifications at the lower level in the label hierarchy; the determining determines using a maximum hierarchy depth to preclude consideration of misclassifications at the lower level in the label hierarchy; the labels and categories relate to points-of-interest and wherein the database includes maps.</p><p id="p-0119" num="0114">In yet other embodiments: a computer program product comprising code instructions which, when said program is executed on a computer, cause the computer to perform one or more of the methods above; a computer-readable medium having stored thereon the computer program product; and/or a data processing device comprising one or more processors configured to perform one or more of the methods above.</p><p id="p-0120" num="0115">The foregoing description is merely illustrative in nature and is in no way intended to limit the disclosure, its application, or uses. The broad teachings of the disclosure may be implemented in a variety of forms. Therefore, while this disclosure includes particular examples, the true scope of the disclosure should not be so limited since other modifications will become apparent upon a study of the drawings, the specification, and the following claims. It should be understood that one or more steps within a method may be executed in different order (or concurrently) without altering the principles of the present disclosure. Further, although each of the embodiments is described above as having certain features, any one or more of those features described with respect to any embodiment of the disclosure may be implemented in and/or combined with features of any of the other embodiments, even if that combination is not explicitly described. In other words, the described embodiments are not mutually exclusive, and permutations of one or more embodiments with one another remain within the scope of this disclosure. All documents cited herein are hereby incorporated by reference in their entirety, without an admission that any of these documents constitute prior art.</p><p id="p-0121" num="0116">The term computer-readable medium, as used herein, does not encompass transitory electrical or electromagnetic signals propagating through a medium (such as on a carrier wave); the term computer-readable medium may therefore be considered tangible and non-transitory. Non-limiting examples of a non-transitory, tangible computer-readable medium are nonvolatile memory circuits (such as a flash memory circuit, an erasable programmable read-only memory circuit, or a mask read-only memory circuit), volatile memory circuits (such as a static random access memory circuit or a dynamic random access memory circuit), magnetic storage media (such as an analog or digital magnetic tape or a hard disk drive), and optical storage media (such as a CD, a DVD, or a Blu-ray Disc).</p><p id="p-0122" num="0117">The systems and methods described in this application may be partially or fully implemented by a special-purpose computer created by configuring a general-purpose computer to execute one or more particular functions embodied in computer programs implementing the methods described above. The functional blocks, flowchart components, and other elements described above serve as software specifications, which may be translated into the computer programs by the routine work of a skilled technician or programmer.</p><p id="p-0123" num="0118">The computer programs include processor-executable instructions that are stored on at least one non-transitory, tangible computer-readable medium. The computer programs may also include or rely on stored data. The computer programs may encompass a basic input/output system (BIOS) that interacts with hardware of the special purpose computer, device drivers that interact with particular devices of the special purpose computer, one or more operating systems, user applications, background services, background applications, etc.</p><p id="p-0124" num="0119">It will be appreciated that variations of the above-disclosed embodiments and other features and functions, or alternatives thereof, may be desirably combined into many other different systems or applications. Also, various presently unforeseen or unanticipated alternatives, modifications, variations, or improvements therein may be subsequently made by those skilled in the art which are also intended to be encompassed by the description above and the following claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230004581A1-20230105-M00001.NB"><img id="EMI-M00001" he="8.13mm" wi="76.20mm" file="US20230004581A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002" nb-file="US20230004581A1-20230105-M00002.NB"><img id="EMI-M00002" he="10.58mm" wi="76.20mm" file="US20230004581A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00003" nb-file="US20230004581A1-20230105-M00003.NB"><img id="EMI-M00003" he="8.13mm" wi="76.20mm" file="US20230004581A1-20230105-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00004" nb-file="US20230004581A1-20230105-M00004.NB"><img id="EMI-M00004" he="10.58mm" wi="76.20mm" file="US20230004581A1-20230105-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00005" nb-file="US20230004581A1-20230105-M00005.NB"><img id="EMI-M00005" he="8.13mm" wi="76.20mm" file="US20230004581A1-20230105-M00005.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00006" nb-file="US20230004581A1-20230105-M00006.NB"><img id="EMI-M00006" he="10.58mm" wi="76.20mm" file="US20230004581A1-20230105-M00006.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00007" nb-file="US20230004581A1-20230105-M00007.NB"><img id="EMI-M00007" he="8.13mm" wi="76.20mm" file="US20230004581A1-20230105-M00007.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00008" nb-file="US20230004581A1-20230105-M00008.NB"><img id="EMI-M00008" he="10.58mm" wi="76.20mm" file="US20230004581A1-20230105-M00008.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method implemented with one or more processors for improving classification of labels and categories of a database stored in memory that includes a set of labels and a set of categories where (1) each label in the set of labels points to at least one of the categories in the set of categories, and (2) each label in the set of labels is associated with a hierarchical category path, comprising:<claim-text>the one or more processors applying both a subset of the set of labels and a subset of the set of categories of the database stored in the memory to a first classifier for classifying the subset of labels with respect to the subset of categories, the first classifier including a first loss function;</claim-text><claim-text>the one or more processors determining, based on said applying both the subset of labels and the subset of categories to the first classifier, whether at least one label in the subset of labels of the database stored in the memory has been misclassified; and</claim-text><claim-text>in response to said determining that at least one label in the subset of labels of the database stored in the memory has been misclassified based on said applying both the subset of labels and the subset of categories to the first classifier:<claim-text>the one or more processors changing the first loss function of the first classifier to a second loss function to form a second classifier including the second loss function; and</claim-text><claim-text>the one or more processors applying both the subset of labels and the subset of categories to the second classifier for classifying the subset of labels with respect to the subset of categories of the database stored in the memory for improving its classification of labels and categories.</claim-text></claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first loss function comprises a global categorical cross-entropy loss function, and wherein said changing the first loss function to the second loss function comprises changing the global categorical cross-entropy loss function to a weighted-by-sample categorical cross entropy loss function.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the weighted-by-sample categorical cross entropy loss function comprises:</claim-text><claim-text><maths id="MATH-US-00005" num="00005"><math overflow="scroll"> <mrow>  <msubsup>   <mi>&#x2112;</mi>   <mi>G</mi>   <mo>&#x2032;</mo>  </msubsup>  <mo>=</mo>  <mrow>   <mfrac>    <mn>1</mn>    <mi>N</mi>   </mfrac>   <mo>&#x2062;</mo>   <mrow>    <munderover>     <mo>&#x2211;</mo>     <mrow>      <mi>i</mi>      <mo>=</mo>      <mn>1</mn>     </mrow>     <mi>N</mi>    </munderover>    <mrow>     <msub>      <mi>a</mi>      <mrow>       <msub>        <mover>         <mi>y</mi>         <mo>^</mo>        </mover>        <mi>i</mi>       </msub>       <mo>,</mo>       <msub>        <mi>y</mi>        <mi>i</mi>       </msub>      </mrow>     </msub>     <mo>&#x2062;</mo>     <msub>      <mi>&#x2112;</mi>      <mrow>       <mi>G</mi>       <mo>,</mo>       <mi>i</mi>      </mrow>     </msub>    </mrow>   </mrow>  </mrow> </mrow></math></maths><claim-text>where:</claim-text><claim-text><img id="CUSTOM-CHARACTER-00063" he="3.22mm" wi="2.46mm" file="US20230004581A1-20230105-P00002.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>G,i </sub>is the global catigorical cross-entory loss for sample i and</claim-text></claim-text><claim-text><maths id="MATH-US-00006" num="00006"><math overflow="scroll"> <mrow>  <msub>   <mi>a</mi>   <mrow>    <msub>     <mover>      <mi>y</mi>      <mo>^</mo>     </mover>     <mi>i</mi>    </msub>    <mo>,</mo>    <msub>     <mi>y</mi>     <mi>i</mi>    </msub>   </mrow>  </msub>  <mo>=</mo>  <mrow>   <mo>{</mo>   <mrow>    <mtable>     <mtr>      <mtd>       <mrow>        <msub>         <mi>a</mi>         <mi>shorter</mi>        </msub>        <mo>,</mo>       </mrow>      </mtd>      <mtd>       <mrow>        <mi>if</mi>        <mo>&#x2062;</mo>        <mtext>   </mtext>        <mrow>         <mover>          <mi>t</mi>          <mo>^</mo>         </mover>         <mo>.</mo>         <mtext>   </mtext>         <mi>prefix_path</mi>        </mrow>        <mo>&#x2062;</mo>        <mi>_of</mi>        <mo>&#x2062;</mo>        <mtext>   </mtext>        <mrow>         <mo>(</mo>         <msubsup>          <mi>t</mi>          <mi>i</mi>          <mo>&#x2032;</mo>         </msubsup>         <mo>)</mo>        </mrow>       </mrow>      </mtd>     </mtr>     <mtr>      <mtd>       <mrow>        <msub>         <mi>a</mi>         <mi>longer</mi>        </msub>        <mo>,</mo>       </mrow>      </mtd>      <mtd>       <mrow>        <mi>if</mi>        <mo>&#x2062;</mo>        <mtext>   </mtext>        <mrow>         <msubsup>          <mi>t</mi>          <mi>i</mi>          <mo>&#x2032;</mo>         </msubsup>         <mo>.</mo>         <mtext>   </mtext>         <mi>prefix_path</mi>        </mrow>        <mo>&#x2062;</mo>        <mi>_of</mi>        <mo>&#x2062;</mo>        <mtext>   </mtext>        <mrow>         <mo>(</mo>         <msub>          <mover>           <mi>t</mi>           <mo>^</mo>          </mover>          <mi>i</mi>         </msub>         <mo>)</mo>        </mrow>       </mrow>      </mtd>     </mtr>     <mtr>      <mtd>       <mrow>        <mn>1</mn>        <mo>,</mo>       </mrow>      </mtd>      <mtd>       <mi>otherwise</mi>      </mtd>     </mtr>    </mtable>    <mo>.</mo>   </mrow>  </mrow> </mrow></math></maths><claim-text>{circumflex over (t)}<sub>i </sub>is the path corresponding to the &#x176;<sub>i</sub>prediction;</claim-text><claim-text>t<sub>i</sub>&#x2032; is the observed path corresponding to y<sub>i</sub>;</claim-text><claim-text>a <sub>shorter </sub>denotes the cost of predicting a shorter path than an observed one; and</claim-text><claim-text>a<sub>longer </sub>denotes the cost of predicting a longer path than the observed one.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the cost assigned to a<sub>shorter </sub>is greater than the cost assigned to a<sub>longer</sub>.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each one of the first classifier and second classifier comprises a hybrid hierarchical classifier with the hybrid hierarchical classifier including a global classifier and at least one local classifier.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the global classifier includes a loss, and wherein said changing the first loss function in the first classifier to the second loss function in the second classifier includes introducing a weight to the global classifier's loss.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said determining determines that the at least one label in the subset of labels has been misclassified using a probability threshold.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the labels and categories relate to points-of-interest.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said applying both the subset of labels and the subset of categories to the second classifier enables correction of the at least one label in the subset of labels that has been determined to be misclassified.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said applying both the subset of labels and the subset of categories to the second classifier imputes labels for new points-of-interest.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. A method implemented with one or more processors for improving classification of labels and categories of a database stored in memory that includes a set of labels and a set of categories where (1) each label in the set of labels points to at least one of the categories in the set of categories, (2) the labels in the set of labels are disposed in a label hierarchy with both the labels and the categories being arranged throughout a plurality of levels, and (3) each label in the set of labels is associated with a path, comprising:<claim-text>the one or more processors applying both a subset of the set of labels and a subset of the set of categories of the database stored in the memory to a first classifier for classifying the subset of labels with respect to the subset of categories, the first classifier including a first loss function;</claim-text><claim-text>the one or more processors determining, based on said applying both the subset of labels and the subset of categories to the first classifier, whether at least one label in the subset of labels of the database stored in the memory has a path terminating at a category found at one of the levels in the label hierarchy, and whether the at least one label in the subset of labels of the database stored in the memory corresponds to an over-represented category; and</claim-text><claim-text>in response to said determining that at least one label in the subset of labels of the database stored in the memory has a path terminating at a category found at one of the levels in the label hierarchy, and that the at least one label in the subset of labels of the database stored in the memory corresponds to an over-represented category:</claim-text><claim-text>the one or more processors changing the first loss function of the first classifier to a second loss function to form a second classifier including the second loss function; and</claim-text><claim-text>the one or more processors applying both the subset of labels and the subset of categories to the second classifier for classifying the subset of labels with respect to the subset of categories of the database stored in the memory for improving its classification of labels and categories.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the first loss function comprises a global categorical cross-entropy loss function, and wherein said changing the first loss function to the second loss function comprises changing the global categorical cross-entropy loss function to a weighted-by-sample categorical cross entropy loss function.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the weighted-by-sample categorical cross entropy loss function comprises:</claim-text><claim-text><maths id="MATH-US-00007" num="00007"><math overflow="scroll"> <mrow>  <msubsup>   <mi>&#x2112;</mi>   <mi>G</mi>   <mo>&#x2032;</mo>  </msubsup>  <mo>=</mo>  <mrow>   <mfrac>    <mn>1</mn>    <mi>N</mi>   </mfrac>   <mo>&#x2062;</mo>   <mrow>    <munderover>     <mo>&#x2211;</mo>     <mrow>      <mi>i</mi>      <mo>=</mo>      <mn>1</mn>     </mrow>     <mi>N</mi>    </munderover>    <mrow>     <msub>      <mi>a</mi>      <mrow>       <msub>        <mover>         <mi>y</mi>         <mo>^</mo>        </mover>        <mi>i</mi>       </msub>       <mo>,</mo>       <msub>        <mi>y</mi>        <mi>i</mi>       </msub>      </mrow>     </msub>     <mo>&#x2062;</mo>     <msub>      <mi>&#x2112;</mi>      <mrow>       <mi>G</mi>       <mo>,</mo>       <mi>i</mi>      </mrow>     </msub>    </mrow>   </mrow>  </mrow> </mrow></math></maths><claim-text>where:</claim-text><claim-text><img id="CUSTOM-CHARACTER-00064" he="3.22mm" wi="2.46mm" file="US20230004581A1-20230105-P00002.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>G,i </sub>is the global catigorical cross-entory loss for sample i and</claim-text></claim-text><claim-text><maths id="MATH-US-00008" num="00008"><math overflow="scroll"> <mrow>  <msub>   <mi>a</mi>   <mrow>    <msub>     <mover>      <mi>y</mi>      <mo>^</mo>     </mover>     <mi>i</mi>    </msub>    <mo>,</mo>    <msub>     <mi>y</mi>     <mi>i</mi>    </msub>   </mrow>  </msub>  <mo>=</mo>  <mrow>   <mo>{</mo>   <mrow>    <mtable>     <mtr>      <mtd>       <mrow>        <msub>         <mi>a</mi>         <mi>shorter</mi>        </msub>        <mo>,</mo>       </mrow>      </mtd>      <mtd>       <mrow>        <mi>if</mi>        <mo>&#x2062;</mo>        <mtext>   </mtext>        <mrow>         <msub>          <mover>           <mi>t</mi>           <mo>^</mo>          </mover>          <mi>i</mi>         </msub>         <mo>.</mo>         <mtext>   </mtext>         <mi>prefix_path</mi>        </mrow>        <mo>&#x2062;</mo>        <mi>_of</mi>        <mo>&#x2062;</mo>        <mtext>   </mtext>        <mrow>         <mo>(</mo>         <msubsup>          <mi>t</mi>          <mi>i</mi>          <mo>&#x2032;</mo>         </msubsup>         <mo>)</mo>        </mrow>       </mrow>      </mtd>     </mtr>     <mtr>      <mtd>       <mrow>        <msub>         <mi>a</mi>         <mi>longer</mi>        </msub>        <mo>,</mo>       </mrow>      </mtd>      <mtd>       <mrow>        <mi>if</mi>        <mo>&#x2062;</mo>        <mtext>   </mtext>        <mrow>         <msubsup>          <mi>t</mi>          <mi>i</mi>          <mo>&#x2032;</mo>         </msubsup>         <mo>.</mo>         <mtext>   </mtext>         <mi>prefix_path</mi>        </mrow>        <mo>&#x2062;</mo>        <mi>_of</mi>        <mo>&#x2062;</mo>        <mtext>   </mtext>        <mrow>         <mo>(</mo>         <msub>          <mover>           <mi>t</mi>           <mo>^</mo>          </mover>          <mi>i</mi>         </msub>         <mo>)</mo>        </mrow>       </mrow>      </mtd>     </mtr>     <mtr>      <mtd>       <mrow>        <mn>1</mn>        <mo>,</mo>       </mrow>      </mtd>      <mtd>       <mi>otherwise</mi>      </mtd>     </mtr>    </mtable>    <mo>.</mo>   </mrow>  </mrow> </mrow></math></maths><claim-text>{circumflex over (t)}<sub>i </sub>is the path corresponding to the &#x177;<sub>i </sub>prediction;</claim-text><claim-text>t<sub>i</sub>&#x2032; is the observed path corresponding to y<sub>i</sub>;</claim-text><claim-text>a<sub>shorter </sub>denotes the cost of predicting a shorter path than an observed one; and</claim-text><claim-text>a<sub>longer </sub>denotes the cost of predicting a longer path than the observed one.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the labels and categories relate to points-of-interest and wherein the database includes maps.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein each one of the first classifier and second classifier comprises a hybrid hierarchical classifier with the hybrid hierarchical classifier including a global classifier and at least one local classifier.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The method of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the global classifier includes a loss, and wherein said changing the first loss function in the first classifier to the second loss function in the second classifier includes introducing a weight to the global classifier's loss.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein said determining determines that at least one label in the subset of labels has been misclassified using a probability threshold.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the labels and categories relate to points-of-interest.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein said applying both the subset of labels and the subset of categories to the second classifier enables correction of the at least one label in the subset of labels that has been determined to be misclassified.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein said applying both the subset of labels and the subset of categories to the second classifier imputes labels for new points-of-interest.</claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, where the plurality of levels includes a lower level and a higher level, and wherein said determining includes determining that the at least one of the labels in the subset of labels has a path terminating at a category found at the higher level in the label hierarchy.</claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. The method of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein said determining determines using a maximum hierarchy depth to preclude consideration of misclassifications at the lower level in the label hierarchy.</claim-text></claim></claims></us-patent-application>