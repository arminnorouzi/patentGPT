<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230000451A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221220" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230000451</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17895948</doc-number><date>20220825</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>6</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>6</main-group><subgroup>12</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>764</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>82</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>11</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>60</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>30</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>6</main-group><subgroup>463</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>6</main-group><subgroup>505</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>6</main-group><subgroup>12</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>6</main-group><subgroup>487</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>764</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>82</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>11</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>60</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>30</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2200</main-group><subgroup>24</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>2201</main-group><subgroup>033</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>2201</main-group><subgroup>034</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>10064</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>10116</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">ARTIFICIAL INTELLIGENCE INTRA-OPERATIVE SURGICAL GUIDANCE SYSTEM AND METHOD OF USE</invention-title><us-related-documents><division><relation><parent-doc><document-id><country>US</country><doc-number>17668319</doc-number><date>20220209</date></document-id><parent-status>PENDING</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>17895948</doc-number></document-id></child-doc></relation></division><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16916876</doc-number><date>20200630</date></document-id><parent-status>PENDING</parent-status><parent-pct-document><document-id><country>WO</country><doc-number>PCT/US2019/050745</doc-number><date>20190912</date></document-id></parent-pct-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17668319</doc-number></document-id></child-doc></relation></continuation><us-provisional-application><document-id><country>US</country><doc-number>62730112</doc-number><date>20180912</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Orthogrid Systems Holdings, LLC</orgname><address><city>Salt Lake City</city><state>UT</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Boddington</last-name><first-name>Richard</first-name><address><city>Salt Lake City</city><state>UT</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Saget</last-name><first-name>Edouard</first-name><address><city>Boise</city><state>ID</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Cates</last-name><first-name>Joshua</first-name><address><city>Salt Lake City</city><state>UT</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Oulhaj</last-name><first-name>Hind</first-name><address><city>Strasbourg</city><country>FR</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>Kubiak</last-name><first-name>Erik Noble</first-name><address><city>Las Vegas</city><state>NV</state><country>US</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>Orthogrid Systems Holdings LLC</orgname><role>02</role><address><city>Salt Lake City</city><state>UT</state><country>US</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">The inventive subject matter is directed to a computing platform configured to execute one or more automated artificial intelligence models, wherein the one or more automated artificial intelligence models includes a neural network model, wherein the one or more automated artificial intelligence models are trained on a plurality of radiographic images from a data layer to detect a plurality of anatomical structures or a plurality of hardware, wherein at least one anatomical structure is a pelvic teardrop and a symphysis pubis joint; detecting at a plurality of anatomical structures in a radiographic image of a subject, wherein the plurality of anatomical structures are detected by the computing platform by the step of classifying the radiographic image with reference to a subject good side radiographic image; and constructing a graphical representation of data, wherein the graphical representation is a subject specific functional pelvis grid; the subject specific functional pelvis grid generated based upon the anatomical structures detected by the computing platform in the radiographic image. Various types of functional grids can be generated based on the situation detected.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="196.85mm" wi="149.44mm" file="US20230000451A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="209.55mm" wi="129.37mm" orientation="landscape" file="US20230000451A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="215.98mm" wi="151.47mm" file="US20230000451A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="171.62mm" wi="156.89mm" file="US20230000451A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="232.41mm" wi="137.92mm" orientation="landscape" file="US20230000451A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="235.97mm" wi="92.37mm" orientation="landscape" file="US20230000451A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="235.80mm" wi="124.97mm" orientation="landscape" file="US20230000451A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="222.33mm" wi="136.57mm" orientation="landscape" file="US20230000451A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="225.64mm" wi="159.68mm" orientation="landscape" file="US20230000451A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="229.45mm" wi="155.70mm" orientation="landscape" file="US20230000451A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="214.88mm" wi="158.24mm" orientation="landscape" file="US20230000451A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="228.01mm" wi="158.41mm" orientation="landscape" file="US20230000451A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="219.71mm" wi="157.82mm" orientation="landscape" file="US20230000451A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="220.64mm" wi="133.52mm" orientation="landscape" file="US20230000451A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="200.91mm" wi="125.56mm" orientation="landscape" file="US20230000451A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="127.00mm" wi="117.94mm" orientation="landscape" file="US20230000451A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="201.17mm" wi="129.88mm" orientation="landscape" file="US20230000451A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="221.23mm" wi="131.23mm" orientation="landscape" file="US20230000451A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="202.61mm" wi="124.63mm" orientation="landscape" file="US20230000451A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00019" num="00019"><img id="EMI-D00019" he="171.70mm" wi="125.48mm" orientation="landscape" file="US20230000451A1-20230105-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00020" num="00020"><img id="EMI-D00020" he="199.22mm" wi="131.23mm" orientation="landscape" file="US20230000451A1-20230105-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00021" num="00021"><img id="EMI-D00021" he="171.79mm" wi="155.70mm" orientation="landscape" file="US20230000451A1-20230105-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00022" num="00022"><img id="EMI-D00022" he="96.52mm" wi="121.92mm" orientation="landscape" file="US20230000451A1-20230105-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00023" num="00023"><img id="EMI-D00023" he="217.76mm" wi="123.53mm" orientation="landscape" file="US20230000451A1-20230105-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00024" num="00024"><img id="EMI-D00024" he="233.17mm" wi="158.83mm" orientation="landscape" file="US20230000451A1-20230105-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00025" num="00025"><img id="EMI-D00025" he="222.76mm" wi="154.01mm" orientation="landscape" file="US20230000451A1-20230105-D00025.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00026" num="00026"><img id="EMI-D00026" he="209.30mm" wi="132.16mm" orientation="landscape" file="US20230000451A1-20230105-D00026.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00027" num="00027"><img id="EMI-D00027" he="219.54mm" wi="116.08mm" orientation="landscape" file="US20230000451A1-20230105-D00027.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00028" num="00028"><img id="EMI-D00028" he="217.00mm" wi="111.51mm" orientation="landscape" file="US20230000451A1-20230105-D00028.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00029" num="00029"><img id="EMI-D00029" he="126.32mm" wi="137.50mm" orientation="landscape" file="US20230000451A1-20230105-D00029.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00030" num="00030"><img id="EMI-D00030" he="219.54mm" wi="114.38mm" orientation="landscape" file="US20230000451A1-20230105-D00030.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00031" num="00031"><img id="EMI-D00031" he="128.86mm" wi="137.84mm" orientation="landscape" file="US20230000451A1-20230105-D00031.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00032" num="00032"><img id="EMI-D00032" he="224.11mm" wi="130.81mm" orientation="landscape" file="US20230000451A1-20230105-D00032.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00033" num="00033"><img id="EMI-D00033" he="206.16mm" wi="119.55mm" orientation="landscape" file="US20230000451A1-20230105-D00033.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00034" num="00034"><img id="EMI-D00034" he="222.08mm" wi="125.65mm" orientation="landscape" file="US20230000451A1-20230105-D00034.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00035" num="00035"><img id="EMI-D00035" he="203.88mm" wi="113.45mm" orientation="landscape" file="US20230000451A1-20230105-D00035.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00036" num="00036"><img id="EMI-D00036" he="221.49mm" wi="123.19mm" orientation="landscape" file="US20230000451A1-20230105-D00036.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00037" num="00037"><img id="EMI-D00037" he="173.40mm" wi="125.65mm" orientation="landscape" file="US20230000451A1-20230105-D00037.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00038" num="00038"><img id="EMI-D00038" he="223.69mm" wi="116.59mm" orientation="landscape" file="US20230000451A1-20230105-D00038.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00039" num="00039"><img id="EMI-D00039" he="220.98mm" wi="128.02mm" orientation="landscape" file="US20230000451A1-20230105-D00039.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00040" num="00040"><img id="EMI-D00040" he="216.75mm" wi="130.22mm" orientation="landscape" file="US20230000451A1-20230105-D00040.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00041" num="00041"><img id="EMI-D00041" he="226.31mm" wi="153.84mm" orientation="landscape" file="US20230000451A1-20230105-D00041.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00042" num="00042"><img id="EMI-D00042" he="217.51mm" wi="126.49mm" orientation="landscape" file="US20230000451A1-20230105-D00042.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00043" num="00043"><img id="EMI-D00043" he="200.66mm" wi="59.61mm" file="US20230000451A1-20230105-D00043.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00044" num="00044"><img id="EMI-D00044" he="207.18mm" wi="157.99mm" file="US20230000451A1-20230105-D00044.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00045" num="00045"><img id="EMI-D00045" he="235.80mm" wi="157.48mm" orientation="landscape" file="US20230000451A1-20230105-D00045.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00046" num="00046"><img id="EMI-D00046" he="112.95mm" wi="121.41mm" file="US20230000451A1-20230105-D00046.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00047" num="00047"><img id="EMI-D00047" he="98.21mm" wi="101.26mm" file="US20230000451A1-20230105-D00047.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00048" num="00048"><img id="EMI-D00048" he="172.30mm" wi="151.55mm" file="US20230000451A1-20230105-D00048.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00049" num="00049"><img id="EMI-D00049" he="142.83mm" wi="131.57mm" file="US20230000451A1-20230105-D00049.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00050" num="00050"><img id="EMI-D00050" he="142.41mm" wi="120.90mm" file="US20230000451A1-20230105-D00050.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00051" num="00051"><img id="EMI-D00051" he="216.32mm" wi="153.50mm" file="US20230000451A1-20230105-D00051.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00052" num="00052"><img id="EMI-D00052" he="219.03mm" wi="152.40mm" file="US20230000451A1-20230105-D00052.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00053" num="00053"><img id="EMI-D00053" he="151.89mm" wi="126.07mm" file="US20230000451A1-20230105-D00053.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00054" num="00054"><img id="EMI-D00054" he="114.89mm" wi="89.49mm" file="US20230000451A1-20230105-D00054.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00055" num="00055"><img id="EMI-D00055" he="144.95mm" wi="132.67mm" file="US20230000451A1-20230105-D00055.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00056" num="00056"><img id="EMI-D00056" he="150.11mm" wi="125.31mm" file="US20230000451A1-20230105-D00056.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00057" num="00057"><img id="EMI-D00057" he="156.89mm" wi="136.91mm" file="US20230000451A1-20230105-D00057.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00058" num="00058"><img id="EMI-D00058" he="202.78mm" wi="140.72mm" file="US20230000451A1-20230105-D00058.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00059" num="00059"><img id="EMI-D00059" he="204.30mm" wi="180.17mm" file="US20230000451A1-20230105-D00059.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00060" num="00060"><img id="EMI-D00060" he="204.30mm" wi="180.17mm" file="US20230000451A1-20230105-D00060.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00061" num="00061"><img id="EMI-D00061" he="168.49mm" wi="137.16mm" file="US20230000451A1-20230105-D00061.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00062" num="00062"><img id="EMI-D00062" he="178.56mm" wi="155.28mm" file="US20230000451A1-20230105-D00062.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00063" num="00063"><img id="EMI-D00063" he="178.39mm" wi="149.10mm" file="US20230000451A1-20230105-D00063.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application is a divisional of prior application Ser. No. 17/668,319 filed Feb. 9, 2022, which is a continuation in part of U.S. Ser. No. 16/916,876 filed Jun. 30, 2020, and also claims the benefit of PCT/US2019/050745 filed Sep. 12, 2019, under 35 USC Sec. 371 and U.S. provisional patent application No. 62/730,112 filed Sep. 12, 2018, under 35 U.S.C. Sec. 119(e) (hereby incorporated by reference in their entirety).</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">STATEMENT REGARDING FEDERALLY SPONSORED RESEARCH OR DEVELOPMENT</heading><p id="p-0003" num="0002">None.</p><p id="p-0004" num="0003">Reference to a &#x201c;Sequence Listing,&#x201d; a table, or a computer program listing appendix submitted on a compact disc and an incorporation-by-reference of the material on the compact disc: None.</p><heading id="h-0003" level="1">FIELD OF THE INVENTION</heading><p id="p-0005" num="0004">The subject of this invention is an artificial intelligence intraoperative surgical guidance system in joint replacements, spine, trauma fracture reductions and deformity correction, and implant placement/alignment. A method is provided for analyzing subject image data, calculating surgical decision risks and autonomously providing recommended pathways or actions that support the decision-making process of a surgeon to predict optimized implant and subject outcomes (ex. implant guidance, fracture reduction, anatomical alignment) by a graphical user interface.</p><heading id="h-0004" level="1">BACKGROUND OF THE INVENTION</heading><p id="p-0006" num="0005">Many of the radiographic parameters essential to total hip arthroplasty (THA) model performance, such as wear and stability, can be assessed intraoperatively with fluoroscopy. However even with intraoperative fluoroscopic guidance, the placement of an implant or the reduction of a bone fragment can still not be as close as desired by the surgeon. For example, mal positioning of the acetabular model during hip arthroplasty can lead to problems. For the acetabular implant to be inserted in the proper position relative to the pelvis during hip arthroplasty requires that the surgeon know the position of the patient's pelvis during surgery. Unfortunately, the position of the patient's pelvis varies widely during surgery and from patient to patient. During trauma surgery, proper fracture management, especially in the case of an intra-articular fracture, requires a surgeon to reduce the bone fragment optimally with respect to the original anatomy in order to: provide the anatomical with joint the best chance to rehabilitate properly; minimize further long-term damage; and, if possible, to regain its normal function. Unfortunately, in a fracture scenario, the original anatomical position of these bone fragments has been compromised and their natural relationship with the correct anatomy is uncertain and requires the surgeon to use his/her best judgment in order to promote a successful repair and subsequent positive outcome. During a surgery, a surgeon is required to make real-time decisions that can be further complicated by the fact that there are multiple decisions needing to be made at the same time. At any given time, there can be a need for a decision made on a fracture reduction guidance for example and simultaneously a decision required on implant placement and an error at any stage will increase the potential for a sub-optimal outcome and potential surgical failure. Unfortunately, most of these problems are only diagnosed and detected postoperatively and oftentimes lead to revision surgery. These risks and patterns need to be identified in real-time during the surgical or medical event. As surgeons and medical professionals must often rely solely on themselves to identify hazards and risks or make decisions on critical factors in, and surrounding, a surgical event, a need exists for a system and method that can provide intraoperative automated intelligence guided surgical and medical situational awareness support and guidance.</p><heading id="h-0005" level="1">SUMMARY OF THE INVENTION</heading><p id="p-0007" num="0006">This summary describes several embodiments of the presently disclosed subject matter and, in many cases, lists variations and permutations of these embodiments. This summary is merely exemplary of the numerous and varied embodiments. Mention of one or more representative features of a given embodiment is likewise exemplary. Such an embodiment can typically exist with or without the feature(s) mentioned; likewise, those features can be applied to other embodiments of the presently disclosed subject matter, whether listed in this summary or not. To avoid excessive repetition, this summary does not list or suggest all combinations of such features.</p><p id="p-0008" num="0007">The novel subject matter includes an artificial intelligence intra-operative surgical guidance system made of: a computing platform configured to execute one or more automated artificial intelligence models, wherein the one or more automated artificial intelligence models are trained on data from a data layer, wherein the data layer includes at least surgical images, to calculate intra-operative surgical decision risks, and to provide an intra-operative surgical guidance to a user. More specifically, the computing platform is configured to provide an intra-operative visual display to the user showing surgical guidance. The computing platform is trained to show intra-operative surgical decision risks by applying an at least one classification algorithm.</p><p id="p-0009" num="0008">The novel subject matter includes: a computing platform configured to execute one or more automated artificial intelligence models, wherein the one or more automated artificial intelligence models includes a neural network model, wherein the one or more automated artificial intelligence models are trained on a plurality of radiographic images from a data layer to detect a plurality of anatomical structures or a plurality of hardware, wherein at least one anatomical structure is selected from the group consisting of: a pelvic teardrop and a symphysis pubis joint; detecting at a plurality of anatomical structures in a radiographic image of a subject, wherein the plurality of anatomical structures are detected by the computing platform by the step of classifying the radiographic image with reference to a subject good side radiographic image; and constructing a graphical representation of data, wherein the graphical representation is a subject specific functional pelvis grid; the subject specific functional pelvis grid generated based upon the anatomical structures detected by the computing platform in the radiographic image.</p><p id="p-0010" num="0009">In another embodiment, the inventive subject matter includes: an artificial intelligence assisted total hip arthroplasty involving the steps of: providing a computing platform comprised of an at least one image processing algorithm for the classification of a plurality of intra-operative medical images, the computing platform configured to execute one or more automated artificial intelligence models, wherein the one or more automated artificial intelligence models including a neural network model wherein the one or more automated artificial intelligence models are trained on data from a data layer to identify a plurality of orthopedic structures and a plurality of hardware; receiving a preoperative radiographic image of a subject; detecting a plurality of anatomical structures in the preoperative radiographic image of the subject; generating a subject specific functional pelvis grid from the plurality of anatomical structures detected in the preoperative radiographic image of the subject, receiving an intraoperative anteroposterior pelvis radiographic image of the subject; identifying an object selected from the group consisting of: an at least one anatomical landmark and an at least one hardware in the intraoperative anteroposterior pelvis radiographic image, whereby the computing platform performs the step of: selecting a situation specific grid selected from the group consisting of: functional pelvis grid, spinopelvic grid, level pelvis grid, reference grid, neck cut grid, reamer depth grid, center of rotation grid, cup grid, leg length and offset grid, and femur abduction grid. In another embodiment, the inventive subject matter includes an artificial intelligence based intra-operative surgical guidance system comprising: a non-transitory computer-readable storage medium encoded with computer-readable instructions which form a software module and a processor to process the instructions, wherein the software module is comprised of a data layer, an algorithm layer and an application layer, wherein the artificial intelligence based intra-operative surgical guidance system is trained to detect a plurality of anatomical structure, wherein the computing software is configured to detect a plurality of anatomical structures in a radiographic image of a subject, wherein the plurality of anatomical structures are detected by the computing platform by the step of classifying the radiographic image with reference to a subject good side radiographic image; and construct a graphical representation of data, wherein the graphical representation is a subject specific functional pelvis grid; the subject specific functional pelvis grid generated based upon the plurality of anatomical structures detected by the computing platform in the radiographic image. More specifically, the method includes: constructing a graphical representation of data, wherein the graphical representation is an anatomy map of an entire anatomical region; the anatomy map generated based upon the plurality of anatomical structures detected by the computing platform; quantify measurements between the plurality of anatomical structures in the anatomy map; receiving required output from a user; and calculating the required output based on a vector or vertical distances measured from structures in said plurality of anatomical structures in the anatomy map.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0006" level="1">BRIEF DESCRIPTION OF THE SEVERAL IMAGES OF THE DRAWINGS</heading><p id="p-0011" num="0010">The drawings show the apparatus and method of use according to an example form of the present invention. The invention description refers to the accompanying drawings:</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>1</b>A</figref> is a diagram of the system for automated intraoperative surgical guidance.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>1</b>B</figref> shows an exemplary view of a head-up display image of the system.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>2</b>A</figref> is a diagram of the computing platform.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>2</b>B</figref> is a diagram of an artificial intelligence computing system.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>3</b>A</figref>. is a schematic illustration of Deep learning as applied to automated intraoperative surgical guidance.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>3</b>B</figref> is a schematic illustration of reinforcement learning automated intraoperative surgical guidance.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIGS. <b>4</b>A and <b>4</b>B</figref> are block diagrams of the software modules.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>5</b>A</figref> is an overview of preoperative workflow of the present invention.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>5</b>B</figref> is an overview of intraoperative workflow of the present invention.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>5</b>C</figref> is an image of a postoperative workflow of the present invention.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a preoperative image.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>7</b>A</figref> is a preoperative image shown with procedure specific application relevant information as the input with the resultant tasks and actions.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>7</b>B</figref> is the output of the anatomical position model showing a guide template of good pose estimation.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>8</b>A</figref> is a graphical user interface showing image with grid template showing the anatomical outlines of what is a good pose.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>8</b>B</figref> shows the best image pose guidance process.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>8</b>C</figref> shows the output of the use of a reference image.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>9</b>A</figref> is a graphical user interface showing an image with anatomical features defined.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>9</b>B</figref> shows user inputs, task and actions.</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a graphical user interface showing anatomical measurement grid positioned on an image.</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>11</b>A</figref> is a graphical user interface showing an image of the affected side.</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>11</b>B</figref> shows user inputs, task and actions.</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>12</b>A</figref> is a graphical user interface showing a display of accepted image classification.</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>12</b>B</figref> shows user inputs, task and actions relating to the formulation of a treatment plan.</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>13</b>A</figref> is a graphical user interface showing ghosting.</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>13</b>B</figref> shows the data flow in ghosting.</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>14</b>A</figref> is a graphical user interface showing grid similarity with match confidence display.</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>14</b>B</figref> shows the output of a graphical user interface showing confirmation of good side for further use.</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>14</b>C</figref> shows the good side acceptance.</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>15</b>A</figref> is a graphical user interface showing an image of the good-side overlay with grid alignment and measurements.</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>15</b>B</figref> is a graphical user interface showing an image of the good-side overlay with grid alignment and measurements for an ankle.</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>15</b>C</figref> is a graphical user interface showing an image of the good-side overlay with grid alignment and measurements for a nail.</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. <b>16</b></figref> is a representation of statistical inference of 3D models and an example of the use of representation of statistical inference of 3D models.</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. <b>17</b>A</figref> shows an image of anatomy alignment and fracture reduction guidance.</p><p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. <b>17</b>B</figref> shows user inputs, task and actions related to the datasets.</p><p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. <b>18</b>A</figref> is a graphical user interface instrument guidance.</p><p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. <b>18</b>B</figref> shows user inputs, task and actions related to the datasets.</p><p id="p-0048" num="0047"><figref idref="DRAWINGS">FIG. <b>18</b>C</figref> shows various outputs.</p><p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. <b>18</b>D</figref> shows the predictive and contra-side matched ideal entry point.</p><p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. <b>19</b>A</figref> is a graphical user interface showing instrument guidance and or virtual implant placement.</p><p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. <b>19</b>B</figref> is a graphical user interface showing instrument guidance and or virtual implant placement of lag screw placement.</p><p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. <b>20</b>A</figref> is the workflow of an artificial intelligence assisted total hip arthroplasty (THA).</p><p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. <b>20</b>B</figref> is the workflow of an artificial intelligence landmark and hardware detection.</p><p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. <b>20</b>C</figref> is the workflow of an artificial intelligence assisted grid detection.</p><p id="p-0055" num="0054"><figref idref="DRAWINGS">FIG. <b>21</b>A</figref> shows images of an anatomy map generated by artificial intelligence assistance for a total hip arthroplasty (THA).</p><p id="p-0056" num="0055"><figref idref="DRAWINGS">FIG. <b>21</b>B</figref> shows images of a virtual functional pelvis grid generated by artificial intelligence assistance for a total hip arthroplasty (THA).</p><p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. <b>22</b></figref> shows images of intra-operative image calibration.</p><p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. <b>23</b>A</figref> show intraoperative image of an anteroposterior pelvis with instruments and an implant.</p><p id="p-0059" num="0058"><figref idref="DRAWINGS">FIG. <b>23</b>B</figref> show intraoperative image of an anteroposterior pelvis with instruments and an implant.</p><p id="p-0060" num="0059"><figref idref="DRAWINGS">FIG. <b>23</b>C</figref> shows intraoperative image of an anteroposterior pelvis with instruments and multiple implants.</p><p id="p-0061" num="0060"><figref idref="DRAWINGS">FIG. <b>24</b></figref> shows intraoperative image of the anteroposterior pelvis implant (cup) and stem (trial).</p><p id="p-0062" num="0061"><figref idref="DRAWINGS">FIG. <b>25</b></figref> shows intraoperative images of an anteroposterior hip and no cup or stem.</p><p id="p-0063" num="0062"><figref idref="DRAWINGS">FIG. <b>26</b>A</figref> shows an intraoperative image of an AP pelvis along with an implant cup and stem.</p><p id="p-0064" num="0063"><figref idref="DRAWINGS">FIG. <b>26</b>B</figref> show an intraoperative image of an AP pelvis along with an implant cup and stem.</p><p id="p-0065" num="0064"><figref idref="DRAWINGS">FIG. <b>26</b>C</figref> show an intraoperative image of an AP pelvis along with an implant cup and stem.</p><p id="p-0066" num="0065"><figref idref="DRAWINGS">FIG. <b>27</b>A</figref> shows use of a grid template in a hip arthroplasty (THA).</p><p id="p-0067" num="0066"><figref idref="DRAWINGS">FIG. <b>27</b>B</figref> shows use of a grid template providing leg length and hip offset data to a user in a graphical format.</p><p id="p-0068" num="0067"><figref idref="DRAWINGS">FIG. <b>28</b></figref> shows a functional pelvis grid.</p><p id="p-0069" num="0068"><figref idref="DRAWINGS">FIG. <b>29</b></figref> shows a neck cut grid.</p><p id="p-0070" num="0069"><figref idref="DRAWINGS">FIG. <b>30</b></figref> shows a reamer depth grid.</p><p id="p-0071" num="0070"><figref idref="DRAWINGS">FIG. <b>31</b></figref> shows a center of rotation grid.</p><p id="p-0072" num="0071"><figref idref="DRAWINGS">FIG. <b>32</b></figref> shows a cup abduction grid.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0007" level="1">DESCRIPTION OF EXEMPLARY EMBODIMENTS</heading><p id="p-0073" num="0072">The present invention can be understood more readily by reference to the following detailed description of the invention. It is to be understood that this invention is not limited to the specific devices, methods, conditions or parameters described herein, and that the terminology used herein is for describing embodiments by way of example only and is not intended to be limiting of the claimed invention. Also, as used in the specification including the appended claims, the singular forms &#x201c;a,&#x201d; &#x201c;an,&#x201d; and &#x201c;the&#x201d; include the plural, and reference to a numerical value includes at least that value, unless the context clearly dictates otherwise. Ranges can be expressed herein as from &#x201c;about&#x201d; or &#x201c;approximately&#x201d; one value and/or to &#x201c;about&#x201d; or &#x201c;approximately&#x201d; another value. When such a range is expressed, another embodiment includes from the one value and/or to the other value. Similarly, when values are expressed as approximations, by use of the antecedent &#x201c;about,&#x201d; it will be understood that the value forms another embodiment. All combinations of method or process steps as used herein can be performed in any order, unless otherwise specified or clearly implied to the contrary by the context in which the referenced combination is made. These and other aspects, features and advantages of the invention will be understood with reference to the detailed description herein and will be realized by means of the various elements and combinations particularly pointed out in the appended claims. It is to be understood that both the foregoing general description and the following detailed description of the invention are exemplary and explanatory of preferred embodiments of the inventions and are not restrictive of the invention as claimed. Unless defined otherwise, all technical and scientific terms used herein have the same meaning as commonly understood by one of ordinary skill in the art to which this invention belongs.</p><p id="p-0074" num="0073">The following system and method relate to a computing platform having a graphical user interface for displaying subject image data and apply data science techniques such as machine and deep learning to: calculate surgical decision risks, to predict a problem and provide guidance in real-time situations. The system autonomously displays recommended actions through a display such as graphical user interface to provide an optimized implant and subject outcome by calculating the probability of a successful procedural outcome (ex. Implant guidance, fracture reduction, anatomical alignment). The inventive subject matter is directed to an artificial intelligence intra-operative surgical guidance system and method of use. The system in its most basic form included: a computer executing one or more automated artificial intelligence models trained on at least intra-operative surgical images, to calculate surgical decision risks, and to provide an intra-operative surgical guidance, and a visual display configured to provide the intra-operative surgical guidance to a user.</p><p id="p-0075" num="0074">Artificial Intelligence is the ability of machines to perform tasks that are characteristics of human intelligence. Machine learning is a way of achieving Artificial Intelligence. AI is the ability of machines to carry out tasks in an intelligent way. Machine learning is an application of Artificial Intelligence that involves a data analysis to automatically build analytical models. Machine learning operates on the premise that computers learn statistical and deterministic classification or prediction models from data; the computers and their models adapt independently as more data is inputted to the computing system. Misinterpretation of data can lead to mistakes and a failed outcome. Artificial Intelligence can integrate and infer from a much larger and smarter dataset than any human can discerning patterns and features that are difficult to appreciate from a human perspective. This becomes particularly relevant in the alignment of anatomy and correct placement of implants. The system analyzes and interprets the information and provides guidance based upon a correlation to a known set of patterns and inference from novel sets of data. The artificial intelligence intra-operative surgical guidance system is made of a computer executing one or more automated artificial intelligence models trained on data layer datasets collections to calculate surgical decision risks and provide intra-operative surgical guidance; and a display configured to provide visual guidance to a user.</p><p id="p-0076" num="0075">Now referring to <figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref>, the artificial intelligence intra-operative surgical guidance system <b>1</b> includes an imaging system <b>110</b>. The imaging system <b>110</b> receives subject image data such as images <b>120</b> (radiographic, ultrasound, CT, MRI, 3D, terahertz) of a subject's anatomy. Exemplary medical images that can be analyzed for intraoperative surgical guidance can include a radiographic image such as an image generated by portable fluoroscopy machine called a C-arm. In some embodiments, the computing platform <b>100</b> can be configured to perform one or more aspects associated with automated intraoperative surgical guidance in medical images. For example, computing platform <b>100</b> and/or a related module can receive an inter-operative surgical image, e.g., a fluoroscopic image of the knee.</p><p id="p-0077" num="0076">The artificial intelligence intra-operative surgical guidance system <b>1</b> includes an input of a series of x-ray or fluoroscopic images of a selected surgical site, a computing platform <b>100</b> to process the surgical images and an overlay of a virtual, augmented, or holographic dimensioned grid, with an output to an electronic display device <b>150</b>. The electronic display device <b>150</b> provides a displayed composite image and graphical user interface <b>151</b>. The graphical user interface <b>151</b> is configured to: allow manipulation of a dimensioned grid <b>200</b> by a user <b>155</b>, such as a surgeon, physician assistant, surgical scrub nurse, imaging assistant and support personnel. The computing platform <b>100</b> is configured to synchronize with a sensor <b>130</b> to (a) provide intraoperative anatomical (for example, bone) or implant positional information; and (b) provide postoperative anatomical or implant or external alignment and correction device information to an Artificial Intelligence Engine for guidance.</p><p id="p-0078" num="0077">The computing platform <b>100</b> is configured to synchronize with a surgical facilitator <b>160</b> such as a robot or a haptic feedback device <b>162</b> to provide the same predictive guidance as described throughout as an enabler for robotic surgery. The computing platform <b>100</b> is configured to synchronize with an intelligence guided trackable capable of creating augmented grids or avatars of implants, instruments or anatomy to provide the same predictive guidance as described throughout as an enabler for intelligence guided artificial reality trackable navigation.</p><p id="p-0079" num="0078">The system components include an input of a series of x-ray or fluoroscopic images of a selected surgical site, a dynamic surgical guidance system <b>1</b> to process the surgical images and an overlay of a virtual, augmented, or holographic dimensioned grid <b>200</b> with an image <b>120</b>, with an input device to provide manipulation of the dimensioned grid <b>200</b> by a user <b>155</b>, such as a surgeon. In one embodiment, the electronic display device <b>150</b> is an electronic display device, such as a computer monitor, or a heads-up display, such as GLASS (Google). In another embodiment, the electronic display screen <b>150</b> is a video fpv goggle. An out-put to an electronic display device <b>150</b> is provided for the user <b>155</b> to image the overlay of the series of images and the dimensioned grid <b>200</b>.</p><p id="p-0080" num="0079">The augmented reality or holographic dimensioned grid <b>200</b> can be manipulated by the user <b>155</b> by looking at anatomic structures, the shown on the electronic display device <b>150</b> that will facilitate locking on the correct alignment/placement of surgical device. The artificial intelligence intra-operative surgical guidance system <b>1</b> allows the user <b>155</b> to see critical work information right in their field-of-image using a see-through visual display and then interact with it using familiar gestures, voice commands, and motion tracking. The data can be stored in data storage. The artificial intelligence intra-operative surgical guidance system <b>1</b> allows the user <b>155</b> to see critical work information in their field-of-image using a see-through visual display device <b>150</b> and then interact with it using familiar gestures, voice commands, and motion tracking through a graphical user interface <b>151</b> such as by an augmented reality controller. The graphical user interface <b>151</b>, such as augmented reality or holographic dimensioned grid, can be manipulated by the user <b>155</b> by looking at anatomic structures, then shown on the electronic display device <b>150</b> that will facilitate locking on the correct alignment/placement of surgical device.</p><p id="p-0081" num="0080"><figref idref="DRAWINGS">FIGS. <b>2</b>A &#x26; <b>2</b>B</figref> are diagrams illustrating an exemplary artificial intelligence surgical guidance system <b>1</b> including a computing platform <b>100</b> for dynamic surgical guidance according to an embodiment of the subject matter described herein. A computer platform is a system that includes a hardware device and an operating system that an application, program or process runs upon.</p><p id="p-0082" num="0081">The subject matter described herein can be implemented in software in combination with hardware and/or firmware. For example, the subject matter described herein can be implemented in software executed by at least one processor <b>101</b>. In one exemplary implementation, the subject matter described herein can be implemented using a computer readable medium having stored thereon computer executable instructions that when executed by the processor of a computer platform to perform the steps. Exemplary computer readable media suitable for implementing the subject matter described herein include non-transitory devices, such as disk memory devices, chip memory devices, programmable logic devices, and application specific integrated circuits. In addition, a computer readable medium that implements the subject matter described herein can be located on a single device or computing platform or can be distributed across multiple devices or computing platforms. As used herein, the terms' &#x201c;function&#x201d; or &#x201c;module&#x201d; refer to hardware, firmware, or software in combination with hardware and/or firmware for implementing features described herein.</p><p id="p-0083" num="0082">The computing platform <b>100</b> includes at least one processor <b>101</b> and memory <b>102</b>. The computing device can invoke/request one or more servers from the Cloud Computing Environment <b>98</b> other clinical metadata can be efficiently retrieved from at least one server from the Cloud Environment if it is sorted in only one server or from separate servers if the dataset was sorted partially in different servers; some outcomes derived from the AI engine can be directly sent and sorted in one or more servers in the Cloud platform (privacy is preserved).</p><p id="p-0084" num="0083">The computing platform <b>100</b> analyzes an image for risk factors that the user cannot see due to their human inability to interpret an overwhelming amount of information at any specific moment. If the implant placement and the alignment does not match this data pattern, it will create an awareness in this specific situation and provide a hazard alert to the user. Essentially, identifying and predicting problems ahead of the user encountering them. This can lead to avoidance of complications and prevention of errors. The computing platform <b>100</b> includes a plurality of software modules <b>103</b> to receive and process medical image data, including modules for image distortion correction, image feature detection, image annotation and segmentation, image to image registration, three-dimensional estimation from two-dimensional images, medical image visualization, and one or more surgical guidance modules that use artificial intelligence models to classify images as predictive of optimal or suboptimal surgical outcomes. The term dynamic or dynamically means automated artificial intelligence and can include various artificial intelligence models such as for example: machine learning, deep learning, reinforcement learning or any other strategies to dynamically learn. In a trauma event, such as fracture reduction or deformity correction, or in an arthroplasty event such as hip or knee anatomical alignment or bone cut guidance, or in the event of a spine procedure with implant alignment correction, or in a sports medicine event with ACL reconstruction alignment, these surgical procedure specific datasets coupled with domain knowledge that are useful to an event can be accessed. They will be used to interpret critical failure mode factors of an implant or anatomical alignment and combined will provide the user with a Failure Risk Score with the output to the user as a confidence percentage recommendation of a suboptimal or optimal performance metric. This will be presented to the user in the form of intelligent predictors and scores to support decisions encountered in a real time event.</p><p id="p-0085" num="0084">The software module <b>103</b> includes a plurality of layers. The data layer <b>105</b> is made of a collection of data from various managed distributed data collection networks. This collection of data represents the knowledge that is necessary to address specific tasks. Data layer (detailed in <figref idref="DRAWINGS">FIG. <b>2</b>B</figref>) is the collection of data, which is the input for algorithm layers <b>106</b>, and it contains data sets from different systems which makes Data layer <b>105</b> a rich source information needed for the different deployment tasks. These data layers <b>105</b> include surgical image data and related outcomes, medical advisor knowledge base and sensor data.</p><p id="p-0086" num="0085">The algorithm layer <b>106</b> includes computer-implemented methods specially designed to target application layer using inputs provided in Data Layer <b>105</b>. The algorithm layer <b>106</b> can also be referred to as an AI engine. The algorithm layer <b>106</b> includes various image processing algorithms which use different machine learning techniques (engineering features) and artificial intelligence techniques (hierarchical learning features/learned representation of the features). All algorithms are designed to solve different tasks such as image enhancement, edge detection, segmentation, registration, etc. With the help of Algorithm layer <b>105</b>, these tasks will be performed automatically, which will contribute to the understating of the high-level complexity of medical data and also to the understanding of dependencies among the data provided in Data Layer. The algorithm layer <b>106</b> also includes learning algorithms such as statistical models and prediction models. Representative examples include image quality scoring algorithm, Deep learning algorithm, Machine Learning based algorithms, and image registration algorithms.</p><p id="p-0087" num="0086">The computing platform <b>100</b> is configured to execute one or more automated artificial intelligence models. The one or more automated artificial intelligence models are trained on data from the data layer <b>105</b>. The data layer <b>105</b> includes at least a plurality of surgical images. The artificial intelligence intra-operative surgical guidance system <b>1</b> includes a computing platform trained to calculate intra-operative surgical decision risks by applying an at least one classifier. More specifically the computing platform is trained to perform the classification of intra-operative medical images of implants fixation into discrete categories that are predictive of surgical outcomes, for instance, optimal and sub-optimal. The automated artificial intelligence models are trained to calculate intra-operative surgical decision risks and to provide an intra-operative surgical guidance, and a visual display configured to provide the intra-operative surgical guidance to a user. The application layer <b>107</b> includes but is not limited to: clinical decision support, surgical guidance, risk factor and other post processing actions such as image interpretation and visual display.</p><p id="p-0088" num="0087">Now referring to <figref idref="DRAWINGS">FIGS. <b>3</b>A</figref>&#x26;B example of automated artificial intelligence models are shown. The computing platform <b>100</b> is configured to execute one or more automated artificial intelligence models. These one or more automated artificial intelligence models are trained on data from a data layer <b>105</b>.</p><p id="p-0089" num="0088">These automated artificial intelligence models include: Deep Learning, machine learning and reinforcement learning based techniques. For example, a Convolutional Neural Network (CNN) is trained using annotated/labeled images which include good and bad images to learn local image features linked to low-resolution, presence of noise/artifact, contrast/lighting conditions, etc. The CNN model uses the learning features to make predictions about a new image. The CNN model can include a number of conventional layers and a number of pooling layers which proceed to subsampling (or down sampling) of each feature map while retaining the most informative feature. The stack of the layers can include various Conventional Kernels of size N&#xd7;M; N and M are positive integers and stand respectively for Kernel width and height.</p><p id="p-0090" num="0089"><figref idref="DRAWINGS">FIG. <b>3</b>A</figref> illustrates a deep learning model architecture. The AI deep CCN architecture shown in <figref idref="DRAWINGS">FIG. <b>3</b>A</figref> is for the classification for surgical outcomes (optimal vs suboptimal) with a score quantifying this classification using Convolutional Neural Networks. This architecture comprises several (deep) layers involving linear and non-linear learnable operators which enable building high-level information making the process of construction of discriminative information automated. The first input layer of the deep learning network learns how to reconstruct the original dataset which is the collection of data layer <b>105</b>. The subsequent hidden layers learn how to reconstruct the probability distributions of the activations of the previous layer. The number of the hidden layers define the depth level of the deep learning architecture. The output layer of a neural network is tied to the overall task.</p><p id="p-0091" num="0090">As illustrated in the figure <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>, the classification CNN provides an output probability score for surgical outcome. This score quantifies the quality of positioning of an implant and/or bone alignment. CNN hyperparameters including the number of layers as well as the filter sizes were derived empirically upon testing the performance of the designed network on data collection from Data Layer. The CNN architecture is adjustable in a way to provide high sensitivity detection for the positioning of the implant.</p><p id="p-0092" num="0091"><figref idref="DRAWINGS">FIG. <b>3</b>B</figref> is a schematic illustration of multi-scale reinforcement leaning (RL) as applied to the task of intraoperative screw insertion. This type of reinforcement learning can intraoperatively illustrate insertion trajectory and pedicle screw trajectories.</p><p id="p-0093" num="0092">Now referring to <figref idref="DRAWINGS">FIGS. <b>4</b>A</figref>&#x26;B, the software is organized into modules and each module has at least one function block as shown in this figure. The non-transitory computer-readable storage medium is coupled to a microprocessor, wherein the non-transitory computer-readable storage medium is encoded with computer-readable instructions that implement functionalities of the following modules, wherein the computer-readable instructions are executed by a microprocessor. The computing platform <b>100</b> of the artificial intelligence intra-operative surgical guidance system <b>1</b> can include the following modules. Module <b>5</b> is made of an image quality scoring algorithm to assess the quality of an acquired medical image for its intended use. The image quality scoring algorithm is an image processing algorithm that is based on Machine Learning or Deep learning from a good and bad medical image training dataset for a specific application. For Machine Learning based algorithms, image quality score of as given image is computed based on quality metrics which quantify the level of accuracy in which a weighted combination of technical image factors (e.g., brightness, sharpness, etc.) relate to how clearly and accurately the image captures the original anatomical structures of an image. These weighed combinations of factors are known predictors of optimal or sub-optimal outcomes or performance measures. Examples: &#x201c;adequacy of reduction&#x201d; (<figref idref="DRAWINGS">FIG. <b>28</b>B</figref>). The weighted combination of technical factors is a parametrized combination of key elements which quantify how good the image is. It can be seen as an indicator of relevancy of the image and determines if the acquired image is sufficient to work with or not. In this invention, it is used to define the quality metric/image score</p><p id="p-0094" num="0093">For Deep learning-based techniques, a Convolutional Neural Network (CNN) is trained using annotated/labeled images which include good and bad images to learn local image features linked to low-resolution, presence of noise/artifact, contrast/lighting conditions, etc. The CNN model uses the learning features to predict, for a new image, its image quality score.</p><p id="p-0095" num="0094">As can be seen in <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>, the CNN model can include a number of conventional layers and a number of pooling layers which proceed to subsampling or down sampling of each feature map while retaining the most informative feature. The stack of the layers can include various Conventional Kernels of size N&#xd7;M; N and M are positive integers and stand respectively for Kernel width and height. Module <b>5</b> maximizes the performance of further computer vision and image processing tasks. Module <b>5</b> can also include a grid-based pose guide to assist the user in acquisition of a better image as appropriate to the application.</p><p id="p-0096" num="0095">Module <b>6</b> includes one or more algorithms to detect and correct for distortion inherent in medical imaging modalities, for example the fluoroscopic distortion inherent in intraoperative C-arm imaging.</p><p id="p-0097" num="0096">Module <b>7</b> is an image annotation module that includes image processing algorithms or advanced Deep learning-based techniques for detecting anatomical structures in a medical image and identifying contours or boundaries of anatomical objects in a medical image, such as bone or soft tissue boundaries. Anatomical Landmark detection stands for the identification of key elements of an anatomical body part that potentially have a high level of similarity with the same anatomical body part of other patients. The Deep learning algorithm encompasses various conventional layers, and its final output layer provides self-driven data, including, but not limited to, the system coordinates of important points in the image. In the current invention, landmark detection can be also applied to determine some key positions of anatomical parts in the body, for example, left/right of the femur, and left/right of the shoulder. The Deep Neural Network output is the annotated positions of these anatomical parts. In this case, the Deep learning algorithm uses a training dataset which needs to meet some requirements: the first landmark in the first image used in the training must be consistent across different images in the training dataset. Identifying contours of anatomical objects refers to providing an edge map consisting of rich hierarchical features of an image while preserving anatomical structure boundaries using Deep learning techniques. A variety of highly configurable Deep learning architectures with an optimized hyperparameters tuning are used to help with solving specific tasks. The trained Conventional Neural Network in one embodiment includes tuned hyperparameters stored in one or many processor-readable storage mediums and/or in the Cloud Computing Environment <b>98</b>.</p><p id="p-0098" num="0097">Module <b>8</b> is a preoperative image database including computer algorithms and data structures for storage and retrieval of preoperative medical images, including any metadata associated with these images and the ability to query those metadata. Preoperative images can include multiple imaging modalities such as X-ray, fluoroscopy, ultrasound, computed tomography, terahertz imaging, or magnetic resonance imaging and can include imagery of the nonoperative, or contralateral, side of a patient's anatomy.</p><p id="p-0099" num="0098">Module <b>9</b> is the Image Registration which includes one or more image registration algorithms.</p><p id="p-0100" num="0099">Module <b>10</b> is composed of computer algorithms and data structures for the reconstruction and fitting of three-dimensional (3D) statistical models of anatomical shape to intraoperative two-dimensional or three-dimensional image data. Module <b>11</b> is composed of image processing algorithms and data structures for composing multiple medical images, image annotations, and alignment grids into image-based visualizations for surgical guidance.</p><p id="p-0101" num="0100">Module <b>12</b> is an Artificial Intelligence Engine that is composed of image processing algorithms based on Machine and/or Deep learning techniques for the classification of intraoperative medical images of reduction and alignment procedures into discrete categories that are predictive of differing surgical outcomes, such as suboptimal or optimal outcomes. Classifications produced by Module <b>12</b> can also include an associated score that indicates a statistical likelihood of the classification and is derived from the model underlying the image classifier algorithm, i.e., a classifier.</p><p id="p-0102" num="0101">Module <b>13</b> is an Artificial Intelligence Engine that is made of image processing algorithms which uses Machine Learning or Deep learning methods for the classification of intraoperative medical images of implant fixation procedures into discrete categories that are predictive of differing surgical outcomes, such as suboptimal or optimal. Classifications produced by Module <b>13</b> can also include an associated score that indicates a statistical likelihood of the classification and is derived from the model underlying the image classifier algorithm.</p><p id="p-0103" num="0102">Module <b>14</b> is a postoperative image database made of computer algorithms and data structures for storage and retrieval of postoperative medical images, including any metadata associated with these images and the ability to query those metadata. Postoperative images can include images acquired during routine follow-up clinic visits or surgical revisions.</p><p id="p-0104" num="0103">Module <b>15</b> is an Artificial Intelligence Engine that is made of image processing algorithms for the classification of a time series of postoperative medical images into discrete categories that are predictive of differing surgical outcomes, such as suboptimal or optimal outcomes. Classifications produced by Module <b>15</b> can also include an associated score that indicates a statistical likelihood of the classification and is derived from the model underlying the image classifier algorithm.</p><p id="p-0105" num="0104">Module <b>16</b> is a fracture identification and reduction module with access to an AO/OTA Classification Dataset interprets the image and makes a classification of the bone, bone section, type and group of the fracture.</p><p id="p-0106" num="0105">Now referring to <figref idref="DRAWINGS">FIG. <b>2</b>B</figref>, the computing platform <b>100</b>, which includes one or more Artificial Intelligence (AI) Engines, including <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, Modules <b>12</b>, <b>13</b>, and <b>15</b>, and information from a series of datasets. Here deep neural networks and other image classifiers are trained to analyze and interpret visual features in one or more images to anticipate problems and predict outcomes in a surgery or in a postoperative follow-up period. Training relies on one or more medical image datasets with associated known outcomes data. A trained neural network in this context can thus be thought of as a predictive model that produces a surgical outcome classification from an input set of medical image features.</p><p id="p-0107" num="0106">The outcome classification is typically also accompanied by a statistical likelihood that the classification is correct. Together, the classification and its likelihood can be thought of as an outcome prediction and a confidence level of that prediction, respectively. In the case of a suboptimal outcome prediction, we can consider the confidence level to be a Failure Risk Score for a suboptimal outcome. The classification and Failure Risk Score can thus be used by the surgeon to support decisions that lead to optimal outcomes and avoid suboptimal outcomes. Any number of classical machine learning approaches can be used, as well as more modern Deep learning networks [LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. &#x201c;Deep learning.&#x201d; nature 521.7553 (2015): 436], such as Convolutional Neural Networks [e.g., Lawrence, Steve, et al. &#x201c;Face recognition: A convolutional neural-network approach.&#x201d; IEEE transactions on neural networks 8.1 (1997): 98-113.] A surgical outcomes classifier with a confidence score can also be constructed using any number of methods in multivariate statistics, including a Cox proportional hazards model or other common regression-based time-to-failure models constructed from clinical research data. In the case of a classifier constructed using a multivariate statistical model, the inputs include at least in part feature sets derived from the medical image data. For instance, in order to identify surgical outcomes using &#x201c;non-image&#x201d; datasets, for example diagnosis reports derived from datasets (e.g. &#x201c;Dataset Outcomes Surgical Variables&#x201d; in <figref idref="DRAWINGS">FIG. <b>4</b>B</figref>), Natural Language Processing (NPL) can be used to process clinical text data.</p><p id="p-0108" num="0107">The systems and methods describe uses for the artificial intelligent platform, such as the ability to read and interpret subject image data, and calculate surgical decision risks, and provide the end user with a confidence score of the probability of an optimal outcome and predictor of performance metrics for implants and surgical factors. This occurs by dynamically updating, by the computing platform, the composite image with the at least one surgical guidance.</p><p id="p-0109" num="0108">The computing platform <b>100</b>, which includes an artificial intelligence engine, utilizes and analyzes the information from the datasets. These information sets have been analyzed and structured and based upon the specific surgical application can include: procedural medical image datasets, such as intraoperative fluoroscopic images and pre- and postoperative x-ray, MRI or computed tomography data; an AO/OTA Danis-Weber fracture classification dataset; Lauge-Hansen classification system dataset; implant 3D CAD model datasets, biomechanical testing such as Von Mises Stresses failure modes datasets; medical image feature datasets and learned models for anatomical feature tracking; best-pose grid datasets; fracture reduction image datasets with associated outcomes data; other surgical outcomes datasets: peer-reviewed literature and clinical studies datasets; known predictors and indicators of complications datasets; 3D statistical models of human anatomy datasets; other medical image datasets; an expert physician domain knowledge datasets; bone quality index datasets; failure risk score datasets; subject HER information data; and outcomes surgical variables datasets such as trauma outcomes data, arthroplasty outcomes scoring data, ACL outcome rating scales, and spine scoring systems.</p><p id="p-0110" num="0109">In addition to these surgical and procedure specific datasets, information from subject health records such as comorbidity data, the presence of deformity, and bone quality index scores can be accessed. These datasets are configured to include information that will potentially have an impact on the outcome of the procedure. The datasets are used to interpret critical failure mode factors of an implant or anatomical alignment and when used to train an outcomes classifier for an Artificial Intelligence Engine provides the user with a prediction of optimal or suboptimal outcome and an associated Failure Risk Score. The AI engine include multiple CNNs based classifiers which can be selected using the specific dataset (one or more dataset, most importantly uncorrelated data that make the CNN learn new relevant features) from Data Layer for solving a well-defined task, for example, determine the position of implants, etc.</p><p id="p-0111" num="0110">The information from independent datasets can be accessed at any given time, or alternatively a situation during the event can require the input from various datasets simultaneously. In this situation information from the relevant datasets will be selected for inclusion in the Artificial Intelligence (AI) Engine in the form of multiple trained classifiers, each with a weighted contribution to the final surgical outcome prediction. In this case, Machine and/or Deep learning techniques are intended to identify relevant image features from input space of these datasets and the AI Engine seeks an individual customized software solution to a specific task, for example a decision regarding implant positioning or surgical guidance, and datasets involved to solve that task. This multiple prediction model utilizes information from datasets that have a relationship from the perspective of sharing uncorrelated or partially correlated predictors of a specific outcome. The AI Engine can further weight the outcome prediction data based upon the relative level of criticality regarding performance or failure. The model outputs decision support and outcome predictions for example the probability of a successful and optimal long-term outcome.</p><p id="p-0112" num="0111">The computing platform <b>100</b> is configured to synchronize with a Computer Assisted Surgery (CAS) system to provide the same predictive guidance as described throughout as an enabler for computer assisted surgery. The dynamic surgical guidance system <b>1</b> described herein, has the capability to provide predictive guidance or act as an enabler for subject specific, or custom, matched block guided technology. For example, the present invention can be applicable to other musculoskeletal applications such as arthroplasty surgery for hip, knee, ankle and shoulder as well as trauma surgery for musculoskeletal repair and for spine applications. Typical applications include hip, knee, shoulder, elbow, and ankle arthroplasty, trauma fractures and limb deformity correction, spine, and sports medicine procedures such as femoroacetabular impingement/(FAI)/Periacetabular Osteotomy (PAO). The artificial intelligence intra-operative surgical guidance system <b>1</b> is configured to implement a method including the steps of: obtaining subject image data; dynamically displaying the subject image data on a graphical user interface; selecting an anatomical structure within the subject image data and mapping a grid template to the anatomical structure to provide a registered image data; providing an artificial intelligence engine and at least one dataset configured to generate surgical guidance; providing as a data output, the registered image data, to the artificial intelligence engine to generate at least one surgical guidance; and dynamically updating, by the computing platform, the composite image of the registered image data with the at least one surgical guidance. The surgical guidance is related to: deformity correction, an anatomy alignment, a fracture reduction and an anatomy reduction. The process of surgical guidance will be discussed in the following section for these applications. The method further includes the step of generating a trackable location and orientation guided by the grid template. These steps will be more fully described as they are implemented in <figref idref="DRAWINGS">FIGS. <b>5</b>A-<b>31</b></figref>.</p><p id="p-0113" num="0112">Now referring to <figref idref="DRAWINGS">FIGS. <b>4</b>A</figref>,&#x26; <b>5</b>A-<b>5</b>C, an over image of the use of an artificial intelligence engine and the data sets applied to registered pre-, intra-, and postoperative images yields a graphical user interface for use in reduction and alignment and implant fixation procedures. The preoperative workflow is provided. The workflow proceeds as follows. The imaging system <b>110</b> of the artificial intelligence intra-operative surgical guidance system <b>1</b> receives subject image data such as one or more preoperative images and computes an image quality score using Image Quality Scoring and Pose Guide Module <b>5</b>. The user is presented with a choice to either accept or reject the image based on the image quality score and pose guide guidance. If the image is rejected, the operator tries again to acquire an acceptable image. If accepted, distortion in the image is detected and corrected using the Distortion Correction Module <b>6</b>. The image is then annotated with anatomical structures and image segmentations using the Image Annotation Module <b>7</b>. Images are then stored for later use in the intraoperative and postoperative workflows via the Preoperative Image Database Module <b>8</b>. The process is then repeated to acquire any number of images necessary for later reference in the intraoperative and postoperative workflows.</p><p id="p-0114" num="0113">Now referring to <figref idref="DRAWINGS">FIGS. <b>4</b>A &#x26; <b>5</b>B</figref>, the intraoperative workflow is provided. The process proceeds as follows. The artificial intelligence intra-operative surgical guidance system <b>1</b> receives one or more preoperative images and computes an image quality score using Image Quality Scoring and Pose guide Module <b>5</b>. The user is presented with a choice to either accept or reject the image based on the image quality score and pose guide guidance. If the image is rejected, the operator tries again to acquire an acceptable image. If accepted, distortion in the image is detected and corrected using the Distortion Correction Module (<b>6</b>). The image is then annotated with anatomical structures and image segmentations using the Image Annotation Module <b>7</b>. The artificial intelligence intra-operative surgical guidance system <b>1</b> then registers the image to the best matching corresponding image in Preoperative Image Database module <b>8</b> and computes a matching score using the Image Registration Module <b>9</b>. The user accepts or rejects the image and registration based on the registration match and quality score. If accepted, three-dimensional anatomical shape information can be computed using the 3D Shape Modeling Module <b>10</b> followed by a registration (mapping) of an alignment grid to annotated image using the Image Registration Module <b>9</b>.</p><p id="p-0115" num="0114">The step of registration is the process of transforming images of preoperative of nonoperative side (the fixed image, f(x),) and intraoperative of the operative side (the current moving image, m(x),) to a common coordinate system so that corresponding pixels represent homologous biological points. This means recovering the transform, T(x), which maps points in f(x) to m(x). This is accomplished by the steps of: (1) define the transformation model, (2) determine the similarity metrics describing the objective function to be minimized, and (3) the optimization algorithm that solves the minimization problem. The effective alignment of these images will allow the surgeon to highlight different characteristics and therefore establish a better comparison of these images. It should be noted that the images that are registered do not have to be imaged from the same modality; it can be MRI to CT or CT to CT, and so on.</p><p id="p-0116" num="0115">The computing platform <b>100</b> of the artificial intelligence intra-operative surgical guidance system <b>1</b> produces a composite image or images for display to the user using any combination of the current acquired image, the aligned preoperative image, the registered alignment grid using the Image Composition Module <b>11</b>. Here different processes are followed depending on the type of procedure. For reduction &#x26; alignment, the system computes an outcome classification and Failure Risk Score using the Reduction and Alignment Outcomes Prediction Module <b>12</b>. For implant fixation, the system computes an outcome classification and Failure Risk Score using the Implant Fixation Outcomes Prediction Module <b>13</b>.</p><p id="p-0117" num="0116">The artificial intelligence intra-operative surgical guidance system <b>1</b> then annotates the displayed composite image and graphical user interface with the outcome classification and Failure Risk Score, along with any surgical guidance information. Surgical guidance directives can then be communicated to a surgical facilitator <b>160</b> such as a haptic feedback device, a robot, a trackable guide such as tracked Implant or object, a cutting block, a computer assisted surgery device, IoT device and a mixed reality device.</p><p id="p-0118" num="0117">Now referring to <figref idref="DRAWINGS">FIGS. <b>4</b>A &#x26; <b>5</b>C</figref>, the postoperative workflow is provided. The process proceeds as follows. The artificial intelligence intra-operative surgical guidance system <b>1</b> receives one or more postoperative images and computes an image quality score using Image Quality Scoring and Pose guide Module (<b>5</b>). The user is presented with a choice to either accept or reject the image based on the image quality score and pose guide guidance. If the image is rejected, the operator tries again to acquire an acceptable image. If accepted, distortion in the image is detected and corrected using the Distortion Correction Module <b>6</b>. The image is then annotated with anatomical structures and image segmentations using the Image Annotation Module <b>7</b>. The artificial intelligence intra-operative surgical guidance system <b>1</b> then registers the image to all preceding time series images in the Postoperative Image Database <b>14</b> and computes matching scores using the Image Registration Module <b>9</b>. The user accepts or rejects the image and registration based on the registration match and quality score. If accepted, three-dimensional anatomical shape information can be computed using the 3D Shape Modeling Module <b>10</b>, followed by a registration (mapping) of an alignment grid to annotated image using the Image Registration Module <b>9</b>.</p><p id="p-0119" num="0118">The artificial intelligence intra-operative surgical guidance system <b>1</b> produces a composite image or images for display to the user using any combination of the current acquired image, the aligned preoperative image, the registered alignment grid using the Image Composition Module <b>11</b>. The system then computes an outcome classification and Failure Risk Score using the Postoperative Outcomes Prediction Module <b>13</b>. The artificial intelligence intra-operative surgical guidance system <b>1</b> then annotates the displayed composite image and graphical user interface with the outcome classification and Failure Risk Score, along with any guidance information.</p><p id="p-0120" num="0119">Now referring to <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the subject is prepared and positioned for a medical or surgical event in a standard manner as indicated for the specific procedure, for example, joint replacement, orthopedic trauma, deformity correction, sports medicine, and spine. The preoperative image <b>115</b> or data is imported and is shown as <figref idref="DRAWINGS">FIG. <b>6</b></figref>. The preoperative image <b>115</b> shows a grid template <b>200</b> super imposed over the subject's anatomical image.</p><p id="p-0121" num="0120">A grid template <b>200</b> has a plurality of dimensioned radio-opaque lines, e.g., <b>230</b> relating to surgical variables. The portion of the grid template <b>200</b> that is not opaque is radiolucent. The grid template <b>200</b> can include any shape or pattern of geometric nature or text to reference angles, length positioning or targeting. The grid template <b>200</b> can be a single line, a geometrical pattern, number, letter or a complex pattern of multiple lines and geometries that correspond to surgical variables. The grid patterns can be predesigned or constructed intraoperatively in real-time based upon the surgeon's knowledge of anatomy and clinical experience including interpretation of morphometric literature and studies identifying key relationships and dimensions between anatomical structures and its application in supporting good surgical technique as it relates to specific procedures. With respect to a digital dimensioned grid, this form of the grid template <b>200</b> is generated by the application software.</p><p id="p-0122" num="0121">The subject is prepared and positioned for a medical or surgical event in a standard manner as indicated for the specific procedure, for example, joint replacement, orthopedic trauma, deformity correction, sports medicine, and spine. The procedure specific information for the respective application is extracted from the preoperative image <b>115</b> or data and mapped into live intraoperative images <b>120</b>. Mapping is defined as computing a best-fit image transformation from the preoperative to the intraoperative image space. The transformation is made of the composition of a deformation field and an affine or rigid transformation. The best fit transformation is computed using a variety of established methods, including gradient descent on mutual information, cross-correlation, or the identification of corresponding specific anatomical structures in preoperative <b>115</b> and intraoperative images <b>120</b>. See, e.g., U.S. Pat. No. 9,610,134 specifically incorporated by reference in its entirety.</p><p id="p-0123" num="0122">Now referring to <figref idref="DRAWINGS">FIGS. <b>7</b>A</figref>, a new image <b>120</b> of the unaffected anatomy is acquired and transmitted (can be wirelessly) to the computing platform <b>100</b>. At the beginning of the procedure, the user will use the software of the computing platform <b>100</b> to assist with anatomical positioning, namely the Image QS and Pose module as shown in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>. The computing platform <b>100</b> identifies structures on the intraoperative image <b>120</b> and determine an optimal pose for the image to be taken. Structures are identified based on classifiers learned from the medical image datasets. Outcome classifiers can take the form of a deep neural network, a template matching algorithm, or a rule-based classifier or decision tree.</p><p id="p-0124" num="0123">Now referring to <figref idref="DRAWINGS">FIG. <b>7</b>B</figref>, the is computing platform <b>100</b> provides a real-time guide template <b>250</b> of good pose estimation. The guide template <b>250</b> is a guide for the user to acquire a best-fit opportunity for the artificial intelligence intra-operative surgical guidance system <b>1</b> to match subsequent new images <b>120</b> with the dataset of optimal-pose-images as shown on an electronic display device <b>150</b>. For example, in an ankle, once a pose is selected, the lateral image can be segmented into specific anatomical features and a guide template <b>250</b> mapped to these features&#x2014;tibia, fibula, and talus.</p><p id="p-0125" num="0124"><figref idref="DRAWINGS">FIGS. <b>8</b>A-C</figref> shows the overlay mapping of images and pose guide template <b>250</b> to provide a pose-guide image <b>260</b>. Once the user has acquired the preferred or correct image pose, then that image and/or guide template <b>250</b> can be used as the guidance pose-guide image <b>260</b> for the remainder of the procedure. The guidance pose-guide image <b>260</b> can be that of a contralateral or unaffected side of the body, or a best-fit image from a dataset, or a statistical shape model, or a geometric virtual grid.</p><p id="p-0126" num="0125">The user takes subsequent images until satisfied it matched the guidance pose-guide image <b>260</b> required or a threshold is detected. The correct pose can be acquired in two ways, 1) by adjusting position of anatomy (subject), or 2) by adjusting pose/angle of imaging equipment (ex C-arm). This process can be manually instructed by the user or autonomously performed by the software module of the computing platform <b>100</b>. The computing platform <b>100</b> attempts to determine whether the matched image <b>270</b> is a good match for one of the images <b>120</b> in the preoperative image database. Here the computing platform <b>100</b> uses the Image Registration Module <b>9</b> as shown in <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>.</p><p id="p-0127" num="0126">This process involves a multi-scale image-based registration metric that can be quickly applied to the image pairs. If a match above a threshold is detected, the computing platform <b>100</b> attempts to automatically identify relevant anatomical structures in the new image using any of the techniques for image landmark classifiers. Landmark information and optionally other image information is used to compute a transformation T of the new image to the coordinate space of the preoperative image.</p><p id="p-0128" num="0127">Now referring to <figref idref="DRAWINGS">FIG. <b>8</b>B</figref>, auto landmarking and smart image registration occur at this time whereby the computing platform <b>100</b> attempts to automatically identify relevant anatomical structures, such as posterior distal fibula <b>271</b>, in the matched image <b>270</b>. Any of the aforementioned techniques for landmark classifiers can be used. In addition, image nail approaches based on edge and feature recognition, landmark identification, statistical atlas registration, and deformable models can be used to segment relevant areas of anatomy from the image, such as the talus <b>272</b>. In these images, the matched image <b>270</b> is a template that acts a pose guide <b>250</b> in this situation.</p><p id="p-0129" num="0128">Now referring to <figref idref="DRAWINGS">FIG. <b>8</b>C</figref> the best image pose guidance good side with image registration module <b>9</b> and pose guide module <b>5</b> in addition to expert domain knowledge dataset is accessed as shown. Once the best pose image is accepted, the image registration module <b>9</b> identifies the application specific feature(s) <b>280</b>, for example the talus in an ankle fracture procedure. The output is an automatic identification and selection of the desired anatomical features and a grid template <b>200</b> positioned relative to these features <b>280</b> displaying image and grid alignment.</p><p id="p-0130" num="0129">Now referring to <figref idref="DRAWINGS">FIGS. <b>4</b>A and <b>5</b></figref>, in the image registration module <b>9</b>, a user such as a surgeon, selects at least one anatomical landmark in the anatomical image on the graphical user interface <b>151</b>. Anatomical landmark selection can be accomplished by a various methods including but not limited to: auto-segmentation where the software of the computing platform <b>100</b> uses feature/pattern recognition process to auto-detect known and targeted anatomical structures; use of a remote infrared device, such as a gyro mouse; voice command; air gestures; gaze (surgeon uses gaze and direction of eye or head motion to control targeting) or touching the visualization screen at the selected anatomical structures.</p><p id="p-0131" num="0130">In one illustrative embedment, the surgeon inputs the selection of the anatomical structures to the workstation manually or using a variety of input devices such as, an infrared wand or an augmented reality device. The application software of the computing platform <b>100</b> registers a grid template <b>200</b> with the selected anatomical structures. The method includes the step of registering an anatomical image to a grid template <b>200</b> by selecting at least one anatomical landmark to provide a grid template <b>200</b> with at least one grid indicator <b>280</b>. A grid indicator <b>280</b> is an anatomical feature defined and located on an image that correlates with a known position on a virtual grid template <b>200</b> for purposes of registration. If needed a registration procedure is used to either unwarp the image or warp the digital grid indicators according to the image warping.</p><p id="p-0132" num="0131">The software of the computing platform <b>100</b> identifies and recognizes calibration points that are radiopaque in the image. These are of known dimensioned geometries. A grouping of these points is a distortion calibration array. The distortion calibration array is placed on the image intensifier or in the field of image of any imaging system so that the known distortion calibration array lines/points are identified when an image is taken and captured. These known patterns are saved for use in the distortion adaptation/correction process. The distortion calibration array is removed from visualization on the display medium to not obscure and clutter the image with unnecessary lines/points. A distortion calibration array can be made a series of lines or points that are placed to support the distortion adaptation of the grid template <b>200</b>. The distortion calibration array points or lines are radiopaque so that the distortion process can calculate the location of these points/lines relative to the anatomy and quantify the amount of distortion during each image taken. Once these points/lines are identified and used in the distortion process, there is another process that removes the visualization of these points/lines from the anatomical image so that they are not obstructing the surgeon's image when he or she sees the grid template <b>200</b> and the anatomical image.</p><p id="p-0133" num="0132">In one embodiment, the registration process involves manually or automatically detecting grid structures (such as grid line intersections, points, and line segments) on the grid template <b>200</b> superimposed on the anatomical image and then aligning those structures via an Affine Registration and a deformation field with corresponding structures on a distortion calibration array of known geometry, which is a represented digitally. The method includes the step of deforming the calibrated dimensioned grid to correct for the distortion of the anatomical image to generate a deformed calibrated dimensioned grid image. Known radiopaque lines/points (from distortion calibration array) are used to provide a measure of EM distortion in each anatomical image. The distortion is quantified and then the software of the computing platform <b>100</b> generated virtual grid is adapted to match the distorted anatomy in each anatomical image.</p><p id="p-0134" num="0133">The distortion calibration array is of non-uniform design, such that the selected anatomical structures are clustered more densely in regions of interest to the surgeon, in order that the deformation correction can be estimated with greater precision in those regions. The deformation estimation proceeds as follows: once selected anatomical structures have been identified (either manually or automatically) on the array image, an Affine Transformation that produces the best mapping between corresponding selected anatomical structures from the grid template <b>200</b> to the array image is computed. Following transformation of the grid points by the Affine Transformation, which adjusts the structures for translation, rotation, and scaling with respect to the array image structures in the Deformation Field (which is the residual difference between transformed grid points and the array image points) is modeled using Thin-Plate Splines or any other suitable radial basis functions. Parameters of the Thin-Plate Splines or radial basis functions are estimated by solving a linear system of equations. U.S. patent application Ser. No. 15/383,975 (hereby specifically incorporated by reference). The array image becomes the reference image or the calibrated image.</p><p id="p-0135" num="0134">Once the deformation field has been computed, the dimensioned grid is adapted in real-time intraoperatively to fit the subject anatomy, thus producing a distorted grid indicator, such as lines curving that can be used to match or fit the musculoskeletal anatomy or the shape of the implant. The deformation of the grid indicators is then applied in real-time by first applying the Affine Transformation and then warping the grid indicators along the Deformation Field. A grid pattern based upon the anatomical points that was defined and targeted in landmark identification is generated. The software of the computing platform <b>100</b> is configured to compute the amount of distortion in each image and it quantifies this amount relative to the anatomical image and then displays the calculated grid/Image relationship displaying an image of the subject's anatomy with the quantitatively distorted dimensioned grid image. These deformed grids are tracked in real time with each new image taken. The deformed grid can be positioned relative to anatomy, implant, and fractures auto or manually by the user such as a surgeon. Numerous equations and formulas are used within the algorithms to calculate: measurements, differences, angles, grid and implant positions, fracture deviations to determine at least one measurement of surgical variables involving the implant or trauma.</p><p id="p-0136" num="0135">In auto-segmentation, the at least one anatomical landmark selected by the surgeon is automatically selected for each successive anatomical image. Auto-segmentation allows a surgeon to work more rapidly. Auto-segmentation is accomplished through a combination of one or more of the following techniques: intensity thresholding; feature detection on the intensity edges in the image, including shape detection via the Hough Transform or other methods; feature detection followed by registration of a 2D or 3D anatomical atlas with predefined landmark positions.</p><p id="p-0137" num="0136">Now referring to <figref idref="DRAWINGS">FIGS. <b>9</b>A</figref>&#x26;B and <b>10</b>, an intraoperative image with anatomical features defined is shown as a graphical user interface. A grid template <b>200</b> is mapped to the identified anatomical object or implant feature to form a matched grid map image <b>290</b>. In this image the object is a lateral talus <b>280</b>. The computing platform <b>100</b> is configured to auto track features and calculate new coordinate positions and integrated grid map of anatomy, objects, and implants. The graphical user interface is i) locked in auto tracking of subsequent images and, ii) auto integrate and position an alignment grid. Using the graphical user interface, an alignment grid <b>350</b> is positioned in the matched image. The user adjustments are accepted and the computing platform <b>100</b> is configured to provide a matched grid map <b>290</b>.</p><p id="p-0138" num="0137">Now referring to <figref idref="DRAWINGS">FIGS. <b>11</b>A</figref>&#x26;B, an image <b>121</b> of the affected side is obtained. The affected side could show a deformed, injured or diseased limb. The process previously described for the good side is repeated for the affected side. In this process, the best pose and auto identify anatomical features, objects and implants occurs by matching to the good side of the subject. The inputs to the process are: affected side image, good side pose guide data and feature identification of the data set. The task conducted by the computing platform <b>100</b> is to estimate the best pose match with the good side and auto map the feature and the grid map. The action required is calculate the best pose, auto identify features and calculate the matched grid map. The output is the good side (contralateral), and afforded side are matched with the alignment grid is positioned on the image.</p><p id="p-0139" num="0138">Now referring to <figref idref="DRAWINGS">FIGS. <b>12</b>A</figref>&#x26;B, a fracture identification and reduction module <b>16</b> with access to an AO/OTA Classification Dataset interprets the image and makes a classification of the bone, bone section, type and group of the fracture. As shown in <figref idref="DRAWINGS">FIG. <b>12</b>A</figref> as a graphical user interface, the user then can accept the classification. In an exemplary embodiment, in the event that the procedure involves a fracture of the bone, the affected side image is analyzed for a fracture and the type of fracture classified <b>45</b> according to the AO/OTA classification system. For example, an ankle fracture selection classified as metaphyseal complex (43-A3). Next if the classification is accepted, then the task and actions include providing the treatment option <b>47</b>.</p><p id="p-0140" num="0139">As shown in <figref idref="DRAWINGS">FIGS. <b>12</b></figref> B&#x26;C, a treatment option <b>47</b> is provided based on the classification selected. Treatment option <b>47</b> is shown as a graphical user interface. Here a trauma plate implant <b>49</b> is shown for use in this fracture situation. This is accomplished by the computing platform <b>100</b> configured to acknowledge the selection of fracture type from the relevant dataset, and algorithmic modules are accessed for a treatment plan analysis and suggestion for the user <b>155</b>. The treatment option <b>47</b> will include a determination of the recommended implants for fixation of this type of fracture classification, for example an implant plate with specific screw configurations and selections <b>49</b>. Automatic fracture identification can be accomplished using a classifier trained with various machine learning approaches, including deep convolutional neural networks or rule-based classifiers.</p><p id="p-0141" num="0140">More specifically, the CNN model is trained on datasets which include images with one or more fractures and other images without fractures. Then, the CNN model determines whether there is a fracture or not and also localizes the region of interest which contains the identified fracture and/or the abnormality. Precise identification of the fracture area in the image is critical information required to support the classification, in addition to providing evidence regarding the fixation process for the type of fracture. In practice, the input image is processed by a CNN model (representative architecture is illustrated in <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>, which includes various conventional and max-pooling layers, in order to produce feature maps. At the last layer, the network is modified to target a candidate &#x2018;fracture region of interest&#x2019; on the image based upon feature map information giving the location and size of the region of interest. Candidate regions, including all fracture location and suspected abnormality detections, marked by the CNN model are then shown.</p><p id="p-0142" num="0141">Now referring to <figref idref="DRAWINGS">FIGS. <b>13</b>A</figref>&#x26;B, a guidance pose-guide image <b>260</b> can also be used as the good-side reference image to be used throughout the procedure as a template image for the similarity evaluation and mapping module whereby the anatomy of the good, or unaffected, side is matched with the image of the affected side anatomy <b>121</b>. This is accomplished by the computing platform <b>100</b> configured to use image analysis and segmentation techniques to autonomously perform a similarity evaluation to identify and register bony anatomical features, in real-time, of the affected and contralateral images. This technique is identified as ghosting. In ghosting, an overlay image <b>50</b> is obtained by overlaying the good-side image <b>120</b> versus the bad side image <b>121</b> with reference to the guidance pose-guide image <b>260</b>. Matching of the operative and contralateral-side images is accomplished by computing a best-fit image transformation. The transformation can include the composition of a deformation field and an affine or rigid transformation. The best fit transformation can be computed using a variety of established methods, including gradient descent on mutual information, cross-correlation, or the identification of corresponding specific anatomical structures in preoperative and intraoperative images.</p><p id="p-0143" num="0142">More specifically, as shown in <figref idref="DRAWINGS">FIGS. <b>5</b>A-<b>5</b>C</figref>, the ghosting procedure involves the steps of: 1) Before the surgery, operating room personnel acquires preoperative image(s) of the nonoperative side of the patient's or subject's anatomy. For example, in an ankle fracture case, standard anterior-posterior, lateral, and mortise images of the uninjured ankle would be taken. These images might also be ordered by a surgeon to be acquired by radiology using standard x-ray before the case and then loaded onto our device before the surgery. 2) The nonoperative-side images acquired in step 1 are processed by the computing platform <b>100</b> to identify key structures that will later be used for image registration with the operative side. Additionally, these images may be corrected for distortion and may have a grid template overlaid. 3) During the reduction phase of the surgery, images of the operative side are acquired that the surgeon uses for reduction of the fractured anatomy. These operative side images are processed to identify key structures that will be used for image registration with the nonoperative side. Additionally, these images may be corrected for distortion. 4) The computing platform <b>100</b> identifies the best-matching nonoperative side image to the current operative side image using an image similarity metric. The best-matching nonoperative side image is registered to the current operative side image. The registration process computes an image transformation with is made of a transformation matrix (affine or rigid), a deformation grid, or both. 5) The computing platform <b>100</b> uses the image transformation to align the non-operative-side image with the current operative-side image and produce an image overlay that illustrates the difference in the anatomical positioning of the non-operative and operative-side images. This overlay image is a guidance pose-guide image <b>260</b> that is a template that the surgeon can use to restore the patient's normal anatomy on the operative side (based on the non-operative side anatomical positioning). 6) Any dimensioned grids or other annotations placed on the non-operative side image can be transformed to their corresponding position on the operative-side image to augment the registered composite image.</p><p id="p-0144" num="0143">Now referring to <figref idref="DRAWINGS">FIG. <b>14</b>A</figref>, a similarity evaluation <b>52</b> is performed and the registration match confidence is calculated based in the Image Registration Module <b>9</b>, which includes one or more image registration algorithms. The similarity evaluation <b>52</b> shows similarity evaluation and mapping.</p><p id="p-0145" num="0144">In <figref idref="DRAWINGS">FIGS. <b>14</b></figref> B-C, the grid similarity mapping of the confidence percentage is shown in a graphical user interface display by aligning the non-operative-side image with the current operative side. A rendering display of the &#x2018;ghosting&#x2019; or good vs bad mapping/matching/overlay of difference <b>53</b> that has been calculated by the computing platform <b>100</b>. The key color is the red showing the area difference between the two images. Using the highlighted guide, the match measurement <b>54</b> are shown to the user to accept or decline the information provided. In the user confirms good-side acceptable for use in for example alignment.</p><p id="p-0146" num="0145">In <figref idref="DRAWINGS">FIGS. <b>15</b>A-C</figref>, a graphical user interface displays the match measurement <b>54</b> for the user to accept or decline the information provided. In an exemplary example, a fibula length match of &#x2212;5.5 mm is shown. In <figref idref="DRAWINGS">FIG. <b>15</b></figref> B, an example of a graphical user interface display shows an ankle. The good side is over-laid with the grid alignment and measurements. A ghosting interpretation of an ankle <b>55</b> is shown. <figref idref="DRAWINGS">FIG. <b>15</b>C</figref> is an example of a hip graphical user interface demonstrating a ghosting interpretation of a hip <b>56</b> on the good side overlay with grid alignment and measurements for a nail example.</p><p id="p-0147" num="0146">In <figref idref="DRAWINGS">FIGS. <b>16</b></figref>, the output of the Shape Modeling Module <b>10</b> is data for intraoperative calculation and guidance based on the 3D to 2D registration (or fitting) of a statistical shape model of the application anatomy to the 2D. A statistical shape model is used to predict the rotation in nailing applications. A statistical shape model is a representation of the variability of anatomy in a population that is encoded as one or more sample mean shapes plus modes of variability. A variety of image processing methods exist whereby a statistical shape model can be fit to images of patient anatomy in order to augment missing information in the medical image. For example, the three-dimensional pose, including rotation, of a two-dimensional fluoroscopic image of an implant such as an intertrochanteric nail can be inferred using a statistical model of implant nail geometries. The pose inference is computed by a simultaneous fitting of the statistical model to the two-dimensional image and an iterative registration of the two-dimensional image with simulated projects of the statistical model of the anatomy.</p><p id="p-0148" num="0147">Now referring to <figref idref="DRAWINGS">FIGS. <b>17</b>A</figref>&#x26;B for example, the computing platform <b>100</b> identifies a varus reduction <b>60</b> of the femoral head during a procedure and provides the user with a varus reduction warning <b>61</b> indicating a high risk of failure based upon the calculations of the weighted intelligence model. A grid template of an optimal reduction can be provided to provide guidance and assist the user in obtaining a satisfactory outcome.</p><p id="p-0149" num="0148">The computing platform <b>100</b> is configured to analyze and interpret the information and provide guidance based upon a correlation to a known set of patterns and inference from datasets as set out in <figref idref="DRAWINGS">FIGS. <b>4</b>A&#x26; <b>4</b>B</figref>, the outputs related to surgical guidance include implant selection recommendations, implant placement, performance predictions, probability of good outcomes, and failure risk scores. The structure of the deep learning platform demonstrating the different layers is indicative of the configuration of the flow of information and how the information is applied. The data layer is made of a collection of data from various networks. In a trauma event, such as fracture reduction or deformity correction, or in an arthroplasty event such as hip or knee anatomical alignment or bone cut guidance, or in the event of a spine procedure with implant alignment correction, or in a sports medicine event with ACL reconstruction alignment, these surgical and procedure specific datasets coupled with domain knowledge and information from subject health records that are critical to an event can be accessed. The data set are used to interpret critical failure mode factors of an implant or anatomical alignment and combined will provide the user with a Failure Risk Score with the output to the user as a confidence percentage recommendation of a suboptimal or optimal performance metric. The output is presented to the user in the form of intelligent predictors and scores to support decisions encountered in a real time event.</p><p id="p-0150" num="0149">The is computing platform <b>100</b> analyzes an image for risk factors that the user cannot see due to their human inability to interpret an overwhelming amount of information at any specific moment. If the implant placement and the alignment does not match this data pattern, it will create an awareness in this specific situation and provide a hazard alert to the user. Essentially, identifying and predicting problems ahead of the user encountering them. This can lead to avoidance of complications and prevention of errors. The surgical guidance is related to: deformity correction, an anatomy alignment, a fracture reduction and an anatomy reduction. The process of surgical guidance will be discussed in the following section for these applications. The following sections show how the computing platform <b>100</b> interprets the information and provides guidance based upon a correlation to a known set of patterns and inference from data sets as applied to different surgical procedures.</p><p id="p-0151" num="0150">TRAUMA EXAMPLE&#x2014;HIP FRACTURE. The most frequent fractures hospitalized in US hospitals in 2015 were for those of the hip, according to data from the HCUP (Healthcare Cost and Utilization Project of the Agency for Healthcare Research and Quality (AHRQ)). There are known reasons for the failure modes of these procedures. For example, it is documented that determining and utilizing the correct entry point for nailing of a bone can prevent malreduction of the fracture and ultimately failure of the implant or the compromising of an optimal outcome.</p><p id="p-0152" num="0151">Subject anatomy is unique and using a single-entry point for all subjects is not desirable. Once the subject has been prepared for surgery in a standard manner, the artificial intelligence intra-operative surgical guidance system <b>1</b> is turned on and the platform is now activated. A new image of the subject's anatomy is taken. The image is of the contralateral unaffected, or good, side of the subject. The platform is instructed, or will determine, if the information it receives is 3D or 2D. If the information is 3D, then the artificial intelligence intra-operative surgical guidance system <b>1</b> will call the relevant module to perform a 2D to 3D registration or utilize the 3D model for statistical inference. If the image is 2D, the artificial intelligence intra-operative surgical guidance system <b>1</b> will capture the image and an initial grid pose module will analyze the image and determine if the pose of the image is adequate for use as a &#x2018;true&#x2019; image. A true image is a datum or base image that will be utilized throughout the procedure as a good anatomical reference image that the software algorithm is able to reproducibly recognize. With each new image acquired during this step of the procedure, the algorithm will access a dataset of annotated good anatomy pose images and provide a virtual grid pose template to guide the user to establish the correct pose. An example of the grid pose in this situation would be to advise the user to &#x2018;internally rotate the hips 15 degrees in the AP image with the beam centered at the teardrop&#x2019;. Once the correct image is accepted, the computing platform <b>100</b> accessed the automated image segmentation algorithm to identify the relevant anatomical features. In this specific application (<figref idref="DRAWINGS">FIG. <b>18</b>A</figref>), the anatomical features to be identified can include the tip of the greater trochanter (GT) <b>70</b>, femoral shaft axis identified by center of canal <b>71</b>, and the femoral neck axis identified by center of femoral head and center of neck <b>71</b>. The neck-shaft angle <b>71</b> is measured as the medial arc between the shaft and neck axes. The affected side image is now taken with a similar matching image pose. The feature recognition and segmentation algorithm are accessed as well. The image registration module <b>9</b> is accessed here and a good-side match model is calculated. A good vs bad grid map similarity evaluation provides the user with a match confidence percentage. This step involves: mapping a grid template to the anatomical structure to register an image for the nonoperative side of the subject's anatomy with an image of the intraoperative image of the operative side of the subject's anatomy to provide a registered composite image. The registered composite image is provided to the artificial intelligence engine to generate an at least one graphical surgical guidance in this step, the implant dataset is accessed. The dataset includes information on the three-dimensional geometry of the implant options. The machine learning algorithm associated with this dataset analyzes, measures, and calculates the relevant variables and has the capability to identify suboptimal outputs and provide the user with situational awareness and hazard alerts leading to complication avoidance and error prevention. The output is presented to the user as surgical guidance, such as an optimal or sub-optimal risk score for failure if the user proceeds with the pathway along which he intends follow.</p><p id="p-0153" num="0152">The computing platform <b>100</b> predicts performance based upon these pathways and can also provide the user with a probability of an optimal or sub-optimal outcome. The computing platform <b>100</b> provides the user with an implant recommendation based upon the known predictors of a successful outcome. The computing platform <b>100</b> dynamically updates the registered composite image with the at least one graphical surgical guidance as the surgeon changes interoperative variables. The surgical variable depends upon the ongoing surgery and includes the position of the patient, the pose estimation, the implant position, or the nail entry point in a femur fracture surgical procedure.</p><p id="p-0154" num="0153">Now referring to <figref idref="DRAWINGS">FIGS. <b>1</b>A, <b>4</b>A</figref> &#x26;. <b>18</b>B, the correct nail entry-site prediction information is now accessed within the known indicators and predictors of complications dataset. The dataset uses information from industry gold-standard or historical peer-literature reviewed studies to perform analytical calculations and determine optimal and suboptimal performance predictions. In addition, the computing platform <b>100</b> determines if there is a probability of an error occurring by a specific placement of an entry-site or orientation of a guidewire or reamer <b>75</b>. The optimal entry point prediction utilizes a combination of the various algorithmic modules for information.</p><p id="p-0155" num="0154">In this application, the relevant structures are identified using the segmentation machine learning algorithm, the implant dataset is used for subject best entry-point grid templating, the nail grid template is optimally auto-placed on the image using the deep learning algorithm with access to the known-literature and study dataset, the distances between the various features are measured and the output values will predict an outcome using the literature and study dataset with known optimal versus suboptimal risk values.</p><p id="p-0156" num="0155">Now referring to <figref idref="DRAWINGS">FIG. <b>18</b>C</figref>, for example, the user can want to use an entry-point at the GT (Greater Trochanter of Femur) tip <b>70</b>, but the implant design and subject's anatomy predicts this pathway will likely result in a valgus malreduction. Once an optimal entry point is quantified <b>76</b>, it is displayed on all subsequent images acquired. The entry point is dynamically tracked image to image by the anatomical and implant segmentation tracking module. The updated entry point is calculated relative to the known and selected anatomical feature tracking points.</p><p id="p-0157" num="0156">Now referring to <figref idref="DRAWINGS">FIG. <b>18</b>D</figref>, a new image is acquired and the guidewire <b>77</b> or starter reamer is recognized on the image. An ideal system predicted entry point is recommended and displayed <b>76</b>, and if the user accepts these suggestions, the user then places and orientates the guidewire or reamer in the positions guided by the accepted and displayed virtual or augmented grid or avatar <b>78</b>. The user completes this step of the procedure with imaging or CAS (traditional Computer Assisted Surgery system) tracking and intelligence guidance from the systems' algorithmic modules.</p><p id="p-0158" num="0157">Now referring to <figref idref="DRAWINGS">FIG. <b>19</b>A</figref>, this step is defined by the lag-screw placement. The lag-screw module will calculate and provide guidance on the depth, and rotation, of nail placement <b>80</b>. Once the nail is begun to be inserted, the lag-screw placement <b>81</b> will be used as the determinant of the depth the nail needs to be seated in the bone. When an image is acquired, the artificial intelligence intra-operative surgical guidance system <b>1</b> uses the hip application segmentation machine learning module to identify the relevant anatomical, instrument, and implant features such as the nail alignment jig, and nail and lag-screw.</p><p id="p-0159" num="0158">In addition, the artificial intelligence intra-operative surgical guidance system <b>1</b> provides an automatic determination of screw trajectories and more generally in situations of instrumentation trajectories. For example, to determine if the instrumentation is within the right plane while simultaneously tracking anatomical, implant and instrument considerations in different views. This is achieved using deep learning techniques, more specifically, a Reinforcement Learning (RL) technique. RL strategies are used to train an artificial RL agent to precisely and robustly identify/localize the optimal trajectory/path by navigating in an environment, in our case the acquired fluoroscopic images. The agent makes decisions upon which direction it has to proceed towards an optimal path. By using such a decision-making process, the RL agent learns how to reach the final optimal trajectory. An RL agent learns by interacting with an environment E. At every state (S), the region of interest in this situation, a single decision is made to choose an action (A), which consists of modifying the coordinates of the trajectory, from a set of multiple discrete actions (A). Each valid action choice results in an associated scalar reward, defining the reward signal (R). The agent attempts to learn a policy to maximize both immediate and subsequent future rewards. The reward encourages the agent to move towards the best trajectory while still being learnable. With these considerations, we define the reward R=sgn(ED(Pi&#x2212;1, Pt)&#x2014;D(Pi, Pt)), where D is a function taking the Euclidean distance between plane parameters. Pi (Px, Py, Pz) is the current predicted trajectory coordinate at step I and Pt is the target ground truth coordinates. The difference of the parameter distances, between the previous and current steps, signifies whether the agent is moving closer to or further away from the desired plane parameters. Finally, the terminate state is reached once the RL agent reaches the target plane coordinates. The task is to learn an optimal policy that maximizes the intermediate rewards but also to subsequent future rewards.</p><p id="p-0160" num="0159">Now referring to <figref idref="DRAWINGS">FIG. <b>19</b></figref> B, the nail is tracked in real-time using sensors or images and is guided to the correct depth and the lag-screw configuration and placement information is provided as an input for interpretation. The location of the lag-screw in the specific anatomy <b>82</b> is analyzed to statistically determine its probability of cutting out of the femoral head <b>83</b>.</p><p id="p-0161" num="0160">Now referring to <figref idref="DRAWINGS">FIGS. <b>20</b>A-C</figref>, the workflow of an artificial intelligence assisted total hip arthroplasty (THA) is provided. In step 1, a reference image is acquired and processed for a specific subject. The workflow to obtain a reference image of a subject depends on the hardware and anatomical features in the image of the subject. Scene interpretation is the process of applying automated artificial intelligence models, including a neural network model, wherein the one or more automated artificial intelligence models are trained on a plurality of radiographic images from a data layer to automatically analyze a radiographic image of a subject for relevant data. Scene interpretation can be used to generate the workflow.</p><p id="p-0162" num="0161">Pre-Op Reference Image:</p><p id="p-0163" num="0162">A radiographic image of the subject is acquired in the system (<b>425</b>). If no cup or stem, the process to acquire the correct reference image involves detection of anatomical structures (known landmarks for a specific procedure) and/or hardware in the pelvis/hip image. The anteroposterior (AP) pelvis can be detected when these points are available: left and right teardrop, contra/ipsil and symphysis pubis landmark. For a contralateral approach: both sides of the subject are imaged. The required image is an anteroposterior hip view or an anteroposterior pelvis view. All structures that the system can recognize need to be in the field of view. For an Ipsilateral approach: anatomical structures such as teardrop and symphysis pubis are detected.</p><p id="p-0164" num="0163">Next at least one reference image metric is defined by the user. The metrics can include: leg length and offset, ipsilateral, leg length and offset, contralateral, pelvic tilt, pelvic rotation, femoral abduction (ipsil &#x26; contra), femoral rotation (ipsil &#x26; contra) and femoral head center of rotation (manual, if added). The step of landmark and hardware detection (<b>428</b>) is specific to what is identified from an anatomical landmark, an instrument and/or an implant perspective.</p><p id="p-0165" num="0164">The reference image can be a preoperative image that does not change once taken. Depending on the type of procedure, calibration of the images of the subject may be required. The image is calibrated by identification of a known shape and size of an object or anatomy within the image. For example, using the femoral head resection method, the step of calibration includes manually entering a measuring of the resected femoral head diameter in the reference image when placing femoral head calibration. Leg length offset is calibrated separately using the contralateral comparison on the anteroposterior pelvis stem image. A benefit of calibration is that quantitative leg length offset measurements can be reported to the user to make intraoperative decisions and pre-/post-op comparisons of images.</p><p id="p-0166" num="0165">The next step involves evaluation of structures or hardware, such as image identification in the pelvis/hip reference image (<b>430</b>). The level anteroposterior (AP) pelvis can be detected when these points are available: both left and right teardrops and symphysis pubis structures. The system identifies metrics that include contralateral, pelvic tilt, pelvic rotation, femoral abduction contra only, femoral rotation contra only and femoral head center of rotation (manual, if added). Landmark detection involves image interpretation. The landmark is detected by an object detector model in real time by locating the landmark and classify it with reference to the subject good side image. The step of image identification (<b>430</b>) is specific to then providing an output based upon the step of landmark and hardware detection (<b>428</b>) findings and then classifying the image as an anteroposterior Hip/Cup (<b>434</b>) or anteroposterior Pelvis image (<b>432</b>).</p><p id="p-0167" num="0166">The artificial intelligence intra-operative surgical guidance system is made of a computer executing one or more automated artificial intelligence models trained on data layer datasets collections to identify specific features or structures of anatomy or instruments and implants (collectively hardware), and provide intra-operative surgical guidance, and an output to a user, surgeon, or robot; and a display configured to provide visual guidance to a user. The automated artificial intelligence models include: Deep Learning, machine learning and reinforcement learning based techniques. For example, a Convolutional Neural Network (CNN) is trained using annotated labeled images which include good and bad images to learn local image features linked to low-resolution, presence of noise/artifact, contrast/lighting conditions, etc. The Convolutional Neural Network model uses the learning features to make feature or landmark identifications in a new image. Module <b>7</b> provides an image annotation module that includes image processing algorithms or advanced deep learning-based techniques for detecting anatomical structures in a medical image and identifying contours or boundaries of anatomical objects in a medical image, such as bone or soft tissue boundaries.</p><p id="p-0168" num="0167">Anatomical Landmark detection stands for the identification of key elements of an anatomical body part that potentially have a high level of similarity with the same anatomical body part of other patients. The deep learning algorithm encompasses various conventional layers, and its final output layer provides self-driven data, including, but not limited to, the system coordinates of important points in the image. In the current invention, landmark detection can be also applied to determine some key positions of anatomical parts in the body, for example, left/right of the femur, and left/right of the shoulder.</p><p id="p-0169" num="0168">The deep neural network output is the annotated positions of these anatomical parts. In this case, the deep learning algorithm uses a training dataset which needs to meet some requirements: the first landmark in the first image used in the training must be consistent across different images in the training dataset. Identifying contours of anatomical objects refers to providing an edge map consisting of rich hierarchical features of an image while preserving anatomical structure boundaries using deep learning techniques. A variety of highly configurable deep learning architectures with an optimized hyperparameters tuning are used to help with solving specific tasks. Calibration is optional for pre/post operative quantitative comparison. The image is calibrated by identification of a known shape and size of an object or anatomy within the image.</p><p id="p-0170" num="0169">Intra-Op Reaming for Cup Placement:</p><p id="p-0171" num="0170">An intraoperative anteroposterior radiographic image is obtained can be either anteroposterior hip or anteroposterior pelvis. Another step in an artificial intelligence assisted total hip arthroplasty (THA) is reaming (step 1.5). Hardware detection determines the workflow in the reaming operation. The reamer is detected by the software and identified by its shape and unique features such as the multiple holes in the instrument. If the femoral head center of rotation is defined, then structures determine anteroposterior pelvis/hip view.</p><p id="p-0172" num="0171">The next step in reaming involves determination of structures in the pelvis/hip image. A functional pelvis grid/template can be defined when these points are available: left/right teardrop, contra/ipsiIL structures, and symphysis pubis structures. In another alternative surgical procedural process, anteroposterior hip (detected when limited points available) involves detection of ipsilateral teardrop, ipsiIL and symphysis pubis structures.</p><p id="p-0173" num="0172">The next step in the reaming process involves comparing the reference image to the intraoperative images. The differences in the images provide: visual reference of defined center or rotation (contra or ipsi based on availability), and provides difference from reference image for: the pelvic tilt, pelvic rotation, contralateral femoral rotation/abduction (availability based on ref image type). The differences in the images provide a visual reference of ipsilateral center of rotation, difference from reference image for: pelvic tilt and pelvic rotation. In the instance when center of rotation not defined.</p><p id="p-0174" num="0173">The next step in reaming involves determination of structures in the pelvis/hip image. The anteroposterior pelvis can be detected when these points are available: left/right teardrop, contra/ipsiILT and symphysis pubis structures.</p><p id="p-0175" num="0174">The next step in the reaming process involves comparing the reference image to the intraoperative images. The differences in the images look at: pelvic tilt, pelvic rotation, and contralateral femoral rotation/abduction in an alternative embodiment, taken from the ipsilateral side: anteroposterior Hip (detected when limited points available). These points include: ipsilateral teardrop, ipsiILT and symphysis pubis structures. The next step in the reaming process involves comparing the reference image to the intraoperative images. The differences in the images look at images for: pelvic tilt and pelvic rotation.</p><p id="p-0176" num="0175">INTRA-OP CUP PLACEMENT Step 2 is cup placement. An intraoperative anteroposterior cup radiographic image is obtained (<b>434</b>). Hardware detection determines the workflow in the cup placement operation. If a cup is detected and no ipsilateral stem, or no reamer is detected, then cup placement procedure proceeds. The next step in cup placement involves determination of structures in the pelvis/hip image. An anteroposterior pelvis image is detected when these points are available: Various detections contra/ipsilateral structures, such as left/right teardrop and symphysis pubis structures. The next step is cup virtual ellipse grid/template calculation and generation followed by automatically snapping (registering) specific points on the virtual grid template to the known matching relevant anatomical landmark or implant points. The implanted cup in this situation is positioned in the anatomy and the sequence of images allow for identification of the cup by the software and subsequently identifying the opening of the cup which has the shape of an ellipse. The major and minor axes of the cup are calculated, and the axes end points are quantified on the image. Next cup measurements are reported: the cup opening, and inclination and version angle measurements are provided. The image can be calibrated based on the cup if other measurements are required.</p><p id="p-0177" num="0176">In an alternative embodiment, if only the ipsilateral side is imaged then, an anteroposterior hip workflow is intended (detected when limited points available), these points include: detection of ipsilateral teardrop; ipsiIL and symphysis pubis structures. Difference is compared to the reference image including: pelvic tilt and pelvic rotation. Next cup measurements are reported: Cup ellipse registers to cup opening and inclination and version measurements are provided (image can be calibrated based on cup if other measurements are desired).</p><p id="p-0178" num="0177">INTRA-OP STEM PLACEMENT and LLO GUIDANCE. An intraoperative anteroposterior pelvis radiographic image is obtained (<b>432</b>). The next step, step 3, is stem placement. Hardware detection determines the workflow in the stem placement operation. If a cup and a stem is detected for example, or a reamer is detected, then stem placement procedure proceeds. The next step in stem placement involves determination of structures in the pelvis/hip image, for example anteroposterior pelvis (detected when these points are available) left/right teardrop, contra/ipsiILT and symphysis pubis structures. The next step in the stem placement process involves comparing the reference image to the intraoperative images. The differences in the images look at: difference from reference image for: pelvic tilt, pelvic rotation, contralateral and ipsilateral femoral rotation/abduction.</p><p id="p-0179" num="0178">In the next step of the process, calibration is performed, and leg length and offset measurements are reported to the user. The image may be calibrated by autodetection of objects, anatomy, instruments or implants, and a manual value option may also be entered, and the leg length and offset are reported in mm (+/&#x2212;2 mm). Additionally, calibration object (detected or selected) with manual value entered leg length and offset reported in mm (+/&#x2212;2 mm). If the image is not calibrated, then leg length and offset is displayed as a visual grid from the reference image.</p><p id="p-0180" num="0179">In an alternative embodiment, the next step in cup placement involves determination of structures in the pelvis/hip image. An anteroposterior pelvis is detected when these points are available: anteroposterior hip (detected when limited points available) Detects ipsilateral teardrop, ipsiILT, symphysis pubis structures, Ipsilateral approach only needs ipsilateral image and Contralateral approach needs ipsilateral and contralateral approach needs ipsilateral and contralateral. If the image is calibrated, then autodetection of cup opening or cup diameter with manual value entered leg length and offset reported in mm (+/&#x2212;2 mm) or a calibration object (detected or selected) with manual value entered leg length and offset reported in mm (+/&#x2212;2 mm). If the image is not calibrated, then leg length and offset is displayed as a visual grid from the reference image.</p><p id="p-0181" num="0180">REGISTRATION: The process of interoperative registration of a grid/template can be automated by image interpretation and smart grid logic. More specifically, for example, if a cup reamer or cup implants are detected in the image, then the system registers a cup grid to these structures: left teardrop and right teardrop and major axis of the points of the cup. A grid/template is a graphical representation of data generated based upon identified and relevant registration to anatomical structures. The grid/template provides a visual presentation in a manner that provides the user with measures for implant placement and guidance. In this specific case, a cup grid is a grid that is generated based upon anatomical structures tear drop, symphysis pubis and the cup implant that is specific for a patient. Once the grid/template is registered to the structure in the intraoperative image, then then geometry is measured and data is outputted (for example for Cup Abduction, Cup Version, such as Fit ellipse to cup for version angle), and measure angles relative to True Pelvis Grid (tear drops and symphysis pubis triangle). The system provides situational awareness and warning if numbers outside of safe zones or acceptable parameters by applying Module <b>13</b>.</p><p id="p-0182" num="0181">Now referring to <figref idref="DRAWINGS">FIG. <b>20</b>C</figref>, structures are detected by the system in every image that is taken of the subject. The system is trained to search for specific structures such as symphysis pubis (SP's), teardrop (TD's), Lesser Trochanter (LT's) etc. The system determines if a subject reference grid has been previously saved (<b>450</b>). If not, a functional pelvis grid/template <b>200</b> is generated by the system. A functional pelvis grid/template <b>200</b> is a subject specific grid that is specifically generated by the system based upon known specific structures of the anatomy it is identifying, such as teardrops and symphysis pubis. The system is configured to automatically identify anatomical structures, instruments and implants and then based upon logic and scene interpretation; the system identifies the relevant structures needed for a specific virtual grid for a specific image scene.</p><p id="p-0183" num="0182">Now referring to <figref idref="DRAWINGS">FIGS. <b>28</b>-<b>32</b></figref>, a situation specific grid can be generated by the computing platform. These situation specific grids include: functional pelvis grid, spinopelvic grid, level pelvis grid, reference grid, neck cut grid, reamer depth grid, center of rotation grid, cup grid, leg length and offset grid, and femur abduction grid. If a functional pelvis grid/template is saved then, then the system determines if two tear drops and an ipsilateral teardrop are detected in the image <b>452</b>. If detected, then an anteroposterior pelvis view of the subject is received into the system. If various structures can be detected in these views, including for example, two teardrops, ipsilateral teardrop, ipsiIL and symphysis pubis, then the computing platform determines if any hardware is detected in the image (<b>454</b>). If the system detects hardware in the anteroposterior pelvis image, then the system determines if the anteroposterior pelvis image has a stem present on the operative side (<b>456</b>). If no hardware is detected in the anteroposterior pelvis image of the subject, then the functional pelvis grid/template is registered to the intraoperative image. If a stem is detected in the anteroposterior pelvis image on the operative side, then the leg length offset grid and the functional pelvis grid/template are registered to the intraoperative image, If the subject does not have a stem on the operative side, cup (ellipse tool and reference grid) is registered to the intraoperative image. If the cup is not centered, then the functional pelvis grid/template angle is locked. The computing platform determines if one tear drop is detected (<b>458</b>). If yes, then an anteroposterior hip image of the subject is received into the system. This image is evaluated to determine if any of these structures are present: teardrop, symphysis pubis and one side ipsilateral is detected. If these structures are detected, then the computing platform determines if hardware (<b>460</b>) is present in the image. The computing platform determines if a stem is present in the image (<b>462</b>). If a stem is detected in the image, the leg length and off set grid and the reference grid are registered to the intraoperative image. If no stem is detected in the image, the cup (ellipse tool and the reference grid). If no hardware present in the image, the computing platform registers the functional pelvis grid/template to the intraoperative image.</p><p id="p-0184" num="0183">Now referring to <figref idref="DRAWINGS">FIGS. <b>21</b>A-B</figref>, an artificial intelligence assisted total hip arthroplasty (THA) image is shown. The actions required are to generate a subject specific functional pelvis grid template <b>200</b> and an anatomy map <b>201</b> (<figref idref="DRAWINGS">FIG. <b>21</b>A</figref>), The anatomy map <b>201</b> (as shown in <figref idref="DRAWINGS">FIG. <b>21</b>A</figref>) is a map of anatomical structures i.e., landmarks used in the procedure such as Lesser Trochanter points <b>05</b>, <b>06</b> and <b>07</b> left (L) and right (R) used for leg length and offset determination. For example, to determine leg length using the generated anatomy map, the steps include: auto identification of all the relevant individual anatomical structures i.e. landmarks (such as Teardrop, Symphysis Pubis) and then use these multitude of points to generate a map of the entire anatomical region (for example pelvis, hip), which are then used to quantify measurements between these various points/structures i.e. landmarks, and then based upon the output required (in this instance leg length), the vector or vertical distances measured from specific map landmarks or coordinates such as the Teardrop and Lesser Trochanter will be a value used to define leg length. Other landmarks, such as center of femoral head to trans teardrop line are also used to measure leg length. The steps involved in using an anatomy map to determine Offset include: auto identification of all the relevant individual anatomical landmarks (such as Teardrop, Lesser Trochanter) and then use these multitude of points to generate a map of the entire anatomical region (for example pelvis, hip), which are then used to quantify measurements between these various points/landmarks, and then based upon the output required (in this case Offset), the vector or horizontal distances measured from specific map landmarks or coordinates such as the Teardrop and Lesser Trochanter will be a value used to define Offset. Other landmarks, such as center of femoral head to trans teardrop line are also used to measure Offset.</p><p id="p-0185" num="0184">For grid generation, this is accomplished by the steps of: identifying the left teardrop (TD) <b>01</b>L; identifying the right teardrop (TD) <b>01</b>R, identifying the symphysis pubis (SP) points <b>02</b> and <b>03</b>, or other relevant features such as numbered <b>1</b>-<b>13</b> L&#x26;R; calibrating the reference image; drawing and measuring the trans-teardrop lines (TTD); drawing and measuring the teardrop (TD) <b>01</b> to symphysis pubis (SP) <b>02</b> line and drawing and measuring the vertical and horizontal distances from the various structures. The output of this is: a functional pelvis grid or template <b>200</b> (as shown in <figref idref="DRAWINGS">FIG. <b>21</b>B</figref>). The functional pelvis grid <b>200</b> is a subject specific grid for the specific purpose of defining a true or level functional pelvis as a reference guide for the entire procedure/surgery.</p><p id="p-0186" num="0185">Now referring to <figref idref="DRAWINGS">FIG. <b>22</b></figref> the intra-operative image is calibrated. In these images there is no stem or cup, but an instrument in the field of view, for example a caliper which is recognized by the system as an object used for calibration such as caliper, ball, cup, or anatomy. The output of the scaling process is the scale of the image. Calibration is achieved by the system identifying a known object <b>1000</b> and shape and size and based upon that known measurement apply dimensions to the anatomy data map. In this image hardware <b>1010</b> is shown.</p><p id="p-0187" num="0186">Now referring to <figref idref="DRAWINGS">FIGS. <b>23</b>A-C</figref> an intraoperative image is shown of an anteroposterior pelvis with instruments and an implant: for example, Hohmann retractor, a cup impactor with an implant cup (with no stem) <b>1001</b> are shown. The action is registering the cup grid <b>1005</b> to the anatomical structures <b>1000</b> (<figref idref="DRAWINGS">FIG. <b>23</b>A</figref>) and fitting an ellipse <b>1003</b> (<figref idref="DRAWINGS">FIG. <b>23</b>C</figref>) to the cup <b>1001</b> (<figref idref="DRAWINGS">FIG. <b>23</b>B</figref>). The ellipse <b>1003</b> is the outline of the edge of the cup <b>1001</b> when viewed at an angle such as from a fluoroscopic image pose. The output is a cup inclination and cup version numbers shown in <figref idref="DRAWINGS">FIG. <b>23</b>B</figref>. Cup version is the anteversion angle of a cup (short axis opening/measure of the ellipse) relative to a pelvis in the anteroposterior image view. This angle is displayed as a cup version angle number. Here a hidden virtual functional pelvis grid or template and the implant (cup) <b>1001</b> is shown in <figref idref="DRAWINGS">FIG. <b>23</b>C</figref>. In <figref idref="DRAWINGS">FIG. <b>23</b>C</figref>, the graphical user interface shows a cup grid <b>1002</b> and a cup ellipse <b>1003</b>.</p><p id="p-0188" num="0187">Now referring to <figref idref="DRAWINGS">FIG. <b>24</b></figref> in this intraoperative image, the anteroposterior pelvis implant cup <b>1001</b> and stem (trial) <b>1004</b> are shown. The cup opening (ellipse) <b>1003</b> is shown, the action involves: providing a functional pelvis grid or template <b>1005</b> and then registering: a leg length (LL) and offset (OFF) grid <b>1005</b> to the anatomical structures, for example Teardrops and Lesser Trochanters to the leg length and off set grid <b>1005</b> is related to the length and offset of the leg. The output is leg length and offset data. The leg and off set grid <b>1005</b> grid is registered <b>1012</b> good side over bad side.</p><p id="p-0189" num="0188">Now referring to <figref idref="DRAWINGS">FIGS. <b>25</b>A-B</figref>, an intraoperative anteroposterior image of a hip with no cup or stem is shown. The action, if one ipsilateral teardrop plus one femoral head, plus one teardrop and no implants are identified in the image scene, then identify the anatomical structures and register a femur abduction grid/template to the anatomical structures or acquisition of a good side image for use in the overlay or registration to the bad side image. The output is an image of the femur showing the overlay (<figref idref="DRAWINGS">FIG. <b>26</b>A</figref>). An intraoperative image of an anteroposterior pelvis is shown along with an implant cup and stem (<figref idref="DRAWINGS">FIG. <b>26</b></figref> B). Here the good side is registered to the bad side and that differences can be quantified between the two images (<figref idref="DRAWINGS">FIG. <b>26</b>A</figref>). The action involves the identification of one ipsilateral teardrop <b>1</b> plus one other anatomical landmark <b>9</b>-<b>25</b> plus one Lesser Trochanter <b>11</b> and implants. This image is for registration/overlay of good side over bad side. The output of the registration of good side and bad side can be shown in <figref idref="DRAWINGS">FIG. <b>26</b>A</figref>. The virtual position of the good/contralateral side Lesser Trochanter, which the target position of the bad/affected side hip length. The output is leg length and offset data.</p><p id="p-0190" num="0189">Now referring to <figref idref="DRAWINGS">FIGS. <b>27</b>A-B</figref>, if the computing platform detects teardrops, symphysis pubis, ipsilateral teardrop, implant (cup) and stem (implant) and femoral head implant, then a functional pelvis grid/template <b>200</b> is registered to the image. If the cup Implant and stem implant are in the image, then the computing platform registers the leg length (LL) and offset (OFF) grid <b>200</b> to provide Leg Length &#x26; Offset data. The data output used by the surgeon to make implant decisions.</p><p id="p-0191" num="0190">The foregoing detailed description has been presented for purposes of illustration and description. It is not intended to be exhaustive or to limit the invention to the precise form disclosed. Many modifications and variations are possible considering the above teachings. The described embodiments were chosen to best explain the principles involved and their practical applications, to thereby enable others skilled in the art to best utilize the various embodiments and with various modifications as are suited to the use contemplated. It is intended that the scope of the invention be defined by the claims appended here.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>We claim:</us-claim-statement><claims id="claims"><claim id="CLM-001-8" num="001-8"><claim-text><b>1</b>.-<b>8</b>. (canceled)</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. An artificial intelligence assisted total hip arthroplasty comprising:<claim-text>providing a computing platform comprised of an at least one image processing algorithm for the classification of a plurality of intra-operative medical images, the computing platform configured to execute one or more automated artificial intelligence models, wherein the one or more automated artificial intelligence models comprises a neural network model, wherein the one or more automated artificial intelligence models are trained on data from a data layer to identify a plurality of anatomical structures and a plurality of hardware;</claim-text><claim-text>receiving a preoperative radiographic image of a subject;</claim-text><claim-text>detecting the plurality of anatomical structures in the preoperative radiographic image of the subject;</claim-text><claim-text>generating a subject specific functional pelvis grid from the plurality of anatomical structures detected in the preoperative radiographic image of the subject,</claim-text><claim-text>receiving an intraoperative anteroposterior pelvis radiographic image of the subject;</claim-text><claim-text>identifying an object selected from the group consisting of: an at least one anatomical landmark and an at least one hardware in the intraoperative anteroposterior pelvis radiographic image, whereby the computing platform performs the step of: selecting a situation specific grid selected from the group consisting of: functional pelvis grid, spinopelvic grid, level pelvis grid, reference grid, neck cut grid, reamer depth grid, center of rotation grid, cup grid, leg length and offset grid, and femur abduction grid; and registering the selected grid to the anatomical landmark in the intraoperative image.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, comprising the step of registering the subject good side radiographic image to the intraoperative tide radiographic image.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the data layer includes at least a plurality of fluoroscopic surgical images, wherein the automated artificial intelligence models are trained to calculate intra-operative surgical decision risks, further comprising the step of: arranging the graphical representation within a visual display to provide situational awareness to a user as a function of the classification of surgical outcomes.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, further comprising the step of:<claim-text>constructing a graphical representation of data, wherein the graphical representation is an anatomy map of an entire anatomical region; the anatomy map generated based upon the anatomical landmark detected by the computing platform.</claim-text></claim-text></claim><claim id="CLM-13-15" num="13-15"><claim-text><b>13</b>.-<b>15</b>. (canceled)</claim-text></claim></claims></us-patent-application>