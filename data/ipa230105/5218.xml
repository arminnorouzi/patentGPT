<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005219A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005219</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17941920</doc-number><date>20220909</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>CN</country><doc-number>201710262366.X</doc-number><date>20170420</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>17</main-group><subgroup>05</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>S</subclass><main-group>17</main-group><subgroup>89</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>19</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>19</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>S</subclass><main-group>7</main-group><subgroup>48</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>17</main-group><subgroup>05</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>S</subclass><main-group>17</main-group><subgroup>89</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>19</main-group><subgroup>003</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>19</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>S</subclass><main-group>7</main-group><subgroup>4808</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2219</main-group><subgroup>2012</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2200</main-group><subgroup>24</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>0482</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">METHOD AND DEVICE OF LABELING LASER POINT CLOUD</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>17013150</doc-number><date>20200904</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11455772</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17941920</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>15957487</doc-number><date>20180419</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>10777001</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17013150</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Beijing Tusen Zhitu Technology Co., Ltd.</orgname><address><city>Beijing</city><country>CN</country></address></addressbook><residence><country>CN</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>WANG</last-name><first-name>Zhimeng</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>LI</last-name><first-name>Jieshu</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>ZHENG</last-name><first-name>He</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">The present application discloses a method and device of labeling laser point cloud. The method comprises: receiving data of a laser point cloud; constructing a <b>3</b>D scene and establishing a 3D coordinate system corresponding to the <b>3</b>D scene; converting a coordinate of each laser point in the laser point cloud into a <b>3</b>D coordinate in the <b>3</b>D coordinate system; mapping laser points included in the laser point cloud into the <b>3</b>D scene respectively according to the <b>3</b>D coordinate of the laser points; labeling the laser points in the <b>3</b>D scene.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="71.63mm" wi="146.98mm" file="US20230005219A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="136.14mm" wi="120.99mm" file="US20230005219A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="146.73mm" wi="106.51mm" file="US20230005219A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="168.40mm" wi="153.25mm" file="US20230005219A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="159.43mm" wi="149.52mm" file="US20230005219A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="169.08mm" wi="163.83mm" file="US20230005219A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCES TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">The present application is a continuation of U.S. patent application Ser. No. 17/013,150, entitled, &#x201c;METHOD AND DEVICE OF LABELING LASER POINT CLOUD,&#x201d; filed Sep. 4, 2020, published as U.S. 2020-0402302, which in turn claims priority to Ser. No. 15/957,487, entitled, &#x201c;METHOD AND DEVICE OF LABELING LASER POINT CLOUD,&#x201d; filed Apr. 19, 2018, published as U.S. 2018-0308283, now U.S. Pat. No. 10,777,001, which in turn claims priority to Chinese Patent Application No. 201710262366.X, filed with the Chinese Patent Office on Apr. 20, 2017, and entitled &#x201c;METHOD AND DEVICE OF LABELING LASER POINT CLOUD&#x201d;, each of which is hereby incorporated by reference in its entirety.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">FIELD OF INVENTION</heading><p id="p-0003" num="0002">The present application relates to the computer field and particularly to a method and a device of labeling laser point cloud.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">At present in the development of the automatic driving technique, it is especially important to identify target objects (such as vehicles, move agents, tricycles, bicycles or the like) around a vehicle. A more commonly-used way nowadays is to detect targets objects around the vehicle by a LIDAR (such as a LIDAR with <b>8</b>-line, <b>16</b>-line, <b>32</b>-line or <b>64</b>-line), where the LIDAR emits pulsed laser light to the surroundings, when the pulsed laser light encounter objects, light returns and light point cloud is generated, by which target objects in the surroundings and its size, position and movement velocity could be identified.</p><p id="p-0005" num="0004">At present, the main way of identifying target objects by using laser point clouds is as follows: labeling a received laser point cloud point by point manually in advance to obtain sample data from the laser point cloud corresponding to a target object; performing machine learning on the sample data to obtain an object recognition model; and identifying a target object corresponding to the laser point cloud by using the object recognition model.</p><heading id="h-0004" level="1">BRIEF SUMMARY</heading><p id="p-0006" num="0005">At an aspect, embodiments of the present application provide a method of labeling laser point cloud, which includes:<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0000">    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0006">receiving data of a laser point cloud;</li>        <li id="ul0002-0002" num="0007">constructing a 3D scene and establishing a 3D coordinate system corresponding to the 3D scene;</li>        <li id="ul0002-0003" num="0008">converting a coordinate of each laser point in the laser point cloud into a 3D coordinate in the 3D coordinate system;</li>        <li id="ul0002-0004" num="0009">mapping laser points included in the laser point cloud into the 3D scene according to the respective 3D coordinate of the laser points;</li>        <li id="ul0002-0005" num="0010">labeling the laser points in the 3D scene.</li>    </ul>    </li></ul></p><p id="p-0007" num="0011">At another aspect, embodiments of the present application further provide a device of labeling laser point cloud, which includes:<ul id="ul0003" list-style="none">    <li id="ul0003-0001" num="0000">    <ul id="ul0004" list-style="none">        <li id="ul0004-0001" num="0012">a receiving unit configured to receive data of a laser point cloud;</li>        <li id="ul0004-0002" num="0013">a constructing unit configured to construct a 3D scene and establish a 3D coordinate system corresponding to the 3D scene;</li>        <li id="ul0004-0003" num="0014">a converting unit configured to convert a coordinate of each laser point in the laser point cloud into a 3D coordinate in the 3D coordinate system;</li>        <li id="ul0004-0004" num="0015">a mapping unit configured to map laser points included in the laser point cloud into the 3D scene respectively according to the 3D coordinate of the laser points;</li>        <li id="ul0004-0005" num="0016">a labeling unit configured to label the laser points in the 3D scene.</li>    </ul>    </li></ul></p><p id="p-0008" num="0017">At another aspect, embodiments of the present application further provide a device of labeling laser point cloud, which includes: a processor and at least one memory, the at least one memory storing at least one machine executable instruction which is executed by the processor to:<ul id="ul0005" list-style="none">    <li id="ul0005-0001" num="0000">    <ul id="ul0006" list-style="none">        <li id="ul0006-0001" num="0018">receive data of a laser point cloud;</li>        <li id="ul0006-0002" num="0019">construct a 3D scene and establish a 3D coordinate system corresponding to the 3D scene;</li>        <li id="ul0006-0003" num="0020">convert a coordinate of each laser point in the laser point cloud into a 3D coordinate in the 3D coordinate system;</li>        <li id="ul0006-0004" num="0021">map laser points included in the laser point cloud into the 3D scene according to the respective 3D coordinate of the laser points;</li>        <li id="ul0006-0005" num="0022">label the laser points in the 3D scene.</li>    </ul>    </li></ul></p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0009" num="0023">The accompanying drawings are used to provide the further understanding of the present application and constitute a part of the specification and serve to explain the present application together with the embodiments of the present application but not limit the present application.</p><p id="p-0010" num="0024"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a flow chart of a method of labeling laser point cloud in an embodiment of the present application;</p><p id="p-0011" num="0025"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is another flow chart of a method of labeling laser point cloud in an embodiment of the present application;</p><p id="p-0012" num="0026"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a schematic diagram of setting a camera in a 3D scene in an embodiment of the present application;</p><p id="p-0013" num="0027"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a schematic diagram of setting two cameras in a 3D scene in an embodiment of the present application;</p><p id="p-0014" num="0028"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a schematic diagram of labeling laser point cloud in the 3D scene in an embodiment of the present application;</p><p id="p-0015" num="0029"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a structural schematic diagram of a device of labeling laser point cloud in an embodiment of the present application;</p><p id="p-0016" num="0030"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is another structural schematic diagram of a device of labeling laser point cloud in an embodiment of the present application; and</p><p id="p-0017" num="0031"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is another structural schematic diagram of a device of labeling laser point cloud in an embodiment of the present application.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DESCRIPTION OF THE EMBODIMENTS</heading><p id="p-0018" num="0032">In order to make those skilled in the art better understand the technical solution in the present application, the technical solution in the embodiments of the present application will be described clearly and completely below in combination with the accompanying drawings. Obviously, the described embodiments are just a part of the embodiments of the present application but not all the embodiments. Based upon the embodiments of the present application, all of other embodiments obtained by those ordinary skilled in the art without creative work should pertain to the protection scope of the present application.</p><p id="p-0019" num="0033">In the related art, as object types of laser points in a laser point cloud are labeled one point by one point manually, the number of laser points included in the laser point cloud are huge, and lots of points included the laser cloud and excluded from a target object may be labeled, then low speed and efficiency of labeling operation may be resulted.</p><p id="p-0020" num="0034">For addressing the problem of low speed and efficiency of labeling a laser point cloud existed in the related art, the embodiments of the present application provide a method and device for labeling laser point cloud to improve the speed and efficiency of labeling operation.</p><p id="p-0021" num="0035">Referring to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, which is a flow chart of a method of labeling laser point cloud provided by embodiments of the present application, the method includes:</p><p id="p-0022" num="0036">Step <b>101</b>: receiving, by a labeling device, data of a laser point cloud.</p><p id="p-0023" num="0037">Step <b>102</b>: constructing a 3D scene and establishing a 3D coordinate system corresponding to the 3D scene.</p><p id="p-0024" num="0038">In an embodiment of the present application, the 3D scene could be constructed by using the WebGL technique, the OSG (Open Scene Graph) or the STK (Satellite Tool Kit) but not limited thereto. The way of constructing the 3D scene is not limited strictly by the present application.</p><p id="p-0025" num="0039">Step <b>103</b>: converting a coordinate of each laser point in the laser point cloud into a 3D coordinate in the 3D coordinate system.</p><p id="p-0026" num="0040">Step <b>104</b>: mapping laser points included in the laser point cloud into the 3D scene according to the respective 3D coordinate of the laser points.</p><p id="p-0027" num="0041">Step <b>105</b>: labeling the laser points in the 3D scene.</p><p id="p-0028" num="0042">In the technical solution of the present application, a labeling device receives the data of the laser point cloud, reconstructs the 3D scene, map each laser point in the laser point cloud into the 3D scene, and label each laser point in the 3D scene. With the technical solution of the present application, since the laser point cloud is mapped into the 3D scene, and the laser points feedback from a same target object are relatively centralized and able to show the substantial contour of this target object, the labeling personnel could judge the laser points belonging to target types more visually and rapidly, and thus could label the laser points belonging to the target types rapidly. There is no need to process all laser points in the laser point cloud one point by one point before labeling the laser points belonging to certain target types. Thus, the processing speed and the efficiency of labeling laser points could be improved.</p><p id="p-0029" num="0043">In some embodiments, in order to distinguish the labeling results from the different types of laser points, in the above step <b>105</b>, the laser points with different types are labeled in different labeling manners. Therefore, the specific implementation of the above step <b>105</b> may be implemented by the following steps A<b>1</b> to A<b>2</b>:</p><p id="p-0030" num="0044">Step A<b>1</b>: determining types to which the laser points in the 3D scene belong;</p><p id="p-0031" num="0045">Step A<b>2</b>: labeling the laser points by using labeling manners corresponding to the types to which the laser points belong, where different types correspond to different labeling manners.</p><p id="p-0032" num="0046">In some embodiments, since the laser point cloud contains a large number of laser points, in order to avoid the labeling omission and improve the comprehensiveness and the integrity of the labeling operation, in the technical solution of the present application, after the 3D scene is constructed, a camera is constructed in the 3D scene, and the laser points in the 3D scene may be viewed by adjusting the position and the direction of the camera in the 3D scene, to ensure that the labeling personnel could view the laser points in 360 degrees in the 3D scene. Therefore, a step <b>106</b> may further be included before or after the step <b>103</b>, or before or after the step <b>104</b>, where the step <b>103</b> and step <b>104</b> of the method are described above as shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. As shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the step <b>106</b> may be further included between the steps <b>104</b> and <b>105</b>:</p><p id="p-0033" num="0047">Step <b>106</b>: constructing at least one camera in the 3D scene, to adjust an angle and a range of viewing the laser point cloud in the 3D scene by adjusting a position and a direction of the at least one camera in the 3D scene, during the process of labeling the laser points in the 3D scene.</p><p id="p-0034" num="0048">In some embodiments, the method of labeling the laser point cloud may be applicable used in a browser, where after receiving the data of the laser point cloud sent by a server, the labeling device labels the received laser point cloud by using the method of labeling the laser point cloud described above and feeds back the labeling result to the server.</p><p id="p-0035" num="0049">In order to further improve the accuracy of determining by the labeling personnel the types to which the laser points in the 3D scene belong, in the technical solution of the present application, at least two cameras are constructed in the 3D scene. The positions and the directions of the cameras are different at the same moment, so that the labeling personnel could view the laser points at different viewing angles to improve the accuracy of judging the types to which the laser points belong.</p><p id="p-0036" num="0050">The at least two cameras described above could be switched according to the user's selection. When the user selects one of the cameras, the position and the direction of the selected camera are taken as the corresponding angle and range for viewing the 3D scene presented to the user; and for the unselected camera, the angle and range for viewing the 3D scene corresponding to the unselected camera in the 3D scene are presented by way of thumbnails. <figref idref="DRAWINGS">FIG. <b>3</b></figref> shows a schematic diagram of one camera, and <figref idref="DRAWINGS">FIG. <b>4</b></figref> shows a schematic diagram of two cameras.</p><p id="p-0037" num="0051">In a practical application, the number of the laser points belonging to a same target object is relatively large and these laser points are relatively centralized. In the case that the labeling personnel determines a plurality of laser points belonging to a same target object in a one-by-one labeling manner, the operation speed is relatively slow. Thus, in the technical solution of the present application, in order to further improve the speed of labeling laser points, a plurality of laser points of a same type may be labeled uniformly at one or more times. The above steps A<b>1</b> to A<b>2</b> may be implemented by two following modes but not limited thereto.</p><p id="p-0038" num="0052">First mode: a plurality of laser points which need to be labeled uniformly are selected by a 3D selecting box with a certain cubical space, and the labeling personnel moves the 3D selecting box in the 3D scene and labels the laser points falling within the 3D selecting box at this time uniformly when determining that the selection is needed. Specifically, the above step A<b>1</b> may be implemented by the following steps B<b>1</b> to B<b>3</b>:</p><p id="p-0039" num="0053">Step B <b>1</b>: generating a 3D selecting box according to first size information set by a user.</p><p id="p-0040" num="0054">The size of the 3D selecting box may be entered by the user (e.g., the labeling personnel) in a corresponding input box of a preset labeling page, or the size of the 3D selecting box may be selected from multiple drop-down options of the input box, or the size of the 3D selecting box may be set in a preset size bar. There are a variety of specific implementations, which is not limited strictly by the present application.</p><p id="p-0041" num="0055">Step B<b>2</b>: moving a position of the 3D selecting box in the 3D scene according to a first moving instruction input by the user.</p><p id="p-0042" num="0056">In the embodiment of the present application, the 3D selecting box may be moved according to the user moving the mouse or clicking on the particular button, and the first moving instruction is generated according to the user operating on the mouse or the particular button. In the actual operation, the 3D selecting box is moved to the position where the laser points which need to be labeled are located by the labeling personnel moving the mouse or clicking on the particular button. For example, when the moving of the mouse is stopped (the duration of stopping the moving of the mouse exceeds the set duration threshold) or the clicking-on of the particular button is stopped (e.g., the duration of stopping the clicking-on of the particular button exceeds the set duration threshold), or the laser points falling within the 3D selecting box are determined as the laser points which need to be labeled (referred as target laser points hereinafter).</p><p id="p-0043" num="0057">Step B<b>3</b>: when receiving a labeling instruction carrying a target type, taking laser points currently falling within the 3D selecting box as target laser points and taking the target type in the labeling instruction as a type to which the target laser points belong.</p><p id="p-0044" num="0058">In the embodiment of the present application, after determining the target laser points, the labeling personnel sets the type of the target laser points, for example: enters the type to which the target laser points belong in a corresponding input box on a preset labeling page, or selects a type to which the target laser points belongs in multiple drop-down options of a input box. After setting the type to which the target laser points belong, the labeling personnel clicks on the submit button, and the labeling instruction is generated according to the set type (referred as target type hereinafter).</p><p id="p-0045" num="0059">In the embodiment of the present application, the size of the 3D selecting box could be adjusted by the labeling personnel in real time.</p><p id="p-0046" num="0060">Second mode: the user selects the positions, at which the laser points which need to be labeled are located, on the screen (referred as screen position points hereinafter), determines a plurality of laser points which need to be labeled (i.e., target laser points) according to the position of a camera and the screen position points, and labels the target laser points uniformly. Specifically, the above step A<b>1</b> may be implemented by the following steps C<b>1</b> to C<b>3</b>:</p><p id="p-0047" num="0061">Step C<b>1</b>: generating a ray by taking the position of a camera of the at least one camera as a start point and taking a screen position point selected by the user as an end point.</p><p id="p-0048" num="0062">When a plurality of cameras is set in the 3D scene, the start point in the step C<b>1</b> is the position of the currently-selected camera.</p><p id="p-0049" num="0063">In the embodiment of the present application, the screen position point selected by the user could be a position point which is clicked on by the user on the screen via the mouse. In the actual operation, when the labeling personnel determines that a certain part of laser points centralized relatively in the 3D scene belong to a same type, he selects the screen position point at the position where this part of laser points are located.</p><p id="p-0050" num="0064">Step C<b>2</b>: taking laser points meeting the following conditions as target laser points: a distance of a laser point from the start point is between a first distance threshold and a second distance threshold, and a vertical distance of the laser point from the ray is less than or equal to a third distance threshold.</p><p id="p-0051" num="0065">Where the first distance threshold, the second distance threshold and the third distance threshold could be set flexibly according to actual requirements. In general case, the larger the cubical space constituted by the laser points belonging to a same type around the screen position point is, the larger the values of the first distance threshold, the second distance threshold and the third distance threshold are.</p><p id="p-0052" num="0066">Step C<b>3</b>: when receiving a labeling instruction carrying a target type, taking the target type in the labeling instruction as a type to which the target laser points belong.</p><p id="p-0053" num="0067">After the user selects the screen position point, he could enter the type to which the target laser points belong in a corresponding input box on a preset labeling page or select the type to which the target laser points belong in multiple drop-down options of a input box, and click on a submit button after setting the type to which the target laser points belong. Then the labeling instruction is generated according to the set type (i.e., target type).</p><p id="p-0054" num="0068">The specific implementation of the above step A<b>2</b> may be implemented by the following modes but not limited thereto.</p><p id="p-0055" num="0069">First mode: color differentiation is used for different types of the laser points belonging to, which is specifically implemented as: the labeling instruction further contains a labeling color corresponding to the target type, and the step A<b>2</b> is to set a color of the target laser points to be the labeling color in the labeling instruction, where different target types correspond to different labeling colors. For example, when the target type is vehicle, the corresponding labeling color is dark blue; when the target type is ground, the corresponding labeling color is black; when the target type is tree, the corresponding labeling color is green; which is not limited strictly by the present application.</p><p id="p-0056" num="0070">In the first mode, the labeling color in the labeling instruction is the labeling color corresponding to the type and could be set by the labeling personnel while he sets the type to which the target laser points belong. The manner of setting the labeling color may refer to the manner of setting the type, and a detailed description thereof will be omitted here.</p><p id="p-0057" num="0071">In the first mode, setting the color of the target laser point to be the labeling color in the labeling instruction may include that setting a RGB value of the target laser points to be the RGB value corresponding to the labeling color, or painting the color of the target laser points into the labeling color via a brush tool. If the color of the target laser points is painted as the labeling color via the brush tool, then the operation that setting the color of the target laser points to be the labeling color in the labeling instruction described above may be achieved specifically by the following steps D<b>1</b> to D<b>2</b>:</p><p id="p-0058" num="0072">Step D<b>1</b>: generating a 3D coloring box according to second size information set by the user.</p><p id="p-0059" num="0073">The 3D coloring box has a certain space volume, and the 3D coloring box is a brush tool.</p><p id="p-0060" num="0074">Step D<b>2</b>: moving the 3D coloring box according to a second moving instruction input by the user, to paint the color of the target laser points falling within the 3D coloring box during the moving process into the labeling color in the labeling instruction.</p><p id="p-0061" num="0075">The manner of inputting the second size information and the second moving instruction by the user could refer to the manner of inputting the first size information and the first moving instruction by the user described above, and a detailed description thereof will be omitted here.</p><p id="p-0062" num="0076">In order to implement color modification to the target laser points, in an embodiment of the present application, the 3D coloring box has a brush function and an eraser function. When the attribute of the 3D coloring box is set to be the brush function, the color of the laser points is painted into the color corresponding to the type to which the laser points belong by the 3D coloring box; and when the attribute of the 3D coloring box is set to be the eraser function, the color of the laser points of which the color needs to be modified is erased by the 3D coloring box. In an embodiment of the present application, a brush button and an eraser button could be preset. When the user clicks on the brush button, the 3D coloring box is set with the brush function; and when the user clicks on the eraser button, the 3D coloring box is set with the eraser function.</p><p id="p-0063" num="0077">Thus, the method of setting the color of the target laser points to be the labeling color in the labeling instruction described above further includes steps D<b>3</b> to D<b>4</b>:</p><p id="p-0064" num="0078">Step D<b>3</b>: setting the 3D coloring box with an eraser attribute according to an erasure instruction input by the user;</p><p id="p-0065" num="0079">Step D<b>4</b>: moving the 3D coloring box according to a third moving instruction input by the user, to erase the color of the laser points falling within the 3D coloring box during the moving process.</p><p id="p-0066" num="0080">Second mode: the labeling instruction further contains a labeling box pattern corresponding to the target type, and the step A<b>2</b> may be specifically implemented as follows: generating a 3D labeling box according to the labeling box pattern, and labeling each target laser point respectively by using the 3D labeling box, where different target types correspond to different labeling box patterns. For example, when the target type is vehicle, the corresponding labeling box pattern is a transparent cuboid with dark blue borders; when the target type is pedestrian, the corresponding labeling box pattern is a transparent cuboid with red borders; which is not limited strictly by the present application. Where the size of the cuboid is related to the volume of the geometric space constituted by the target laser points in the 3D scene. The larger the volume is, the larger the size of the corresponding cuboid is.</p><p id="p-0067" num="0081">In the second mode, the labeling box pattern in the labeling instruction is the labeling box pattern corresponding to the type set by the labeling personnel when he sets the type to which the target laser points belong. The manner of setting the labeling box pattern could refer to the manner of setting the type, and a detailed description thereof will be omitted here.</p><p id="p-0068" num="0082">Third mode: determining a labeling color corresponding to the type to which the laser points belong according to a preset correspondence between types and labeling colors, and setting a color of the laser points to be the labeling color corresponding to the type to which the laser points belong.</p><p id="p-0069" num="0083">In the third mode, the correspondence between types and labeling colors is preset. When the labeling instruction is received, the labeling color corresponding to the target type in the labeling instruction is obtained from the correspondence.</p><p id="p-0070" num="0084">Fourth mode: determining a labeling box pattern corresponding to the type to which the laser points belong according to a preset correspondence between types and labeling box patterns, generating a labeling box according to the labeling box pattern corresponding to the type to which the laser points belong, and labeling the laser points by using the labeling box.</p><p id="p-0071" num="0085">In the fourth mode, the correspondence between types and labeling box patterns is preset. When the labeling instruction is received, the labeling box pattern corresponding to the target type in the labeling instruction is obtained from the correspondence.</p><p id="p-0072" num="0086">A specific instance is taken as an example, in which two cameras are set in the 3D scene. The labeling results for different types are differentiated by different colors. The 3D coloring box is set, as shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>. The option box of the target type, the size bar of the 3D coloring box, the function selection button, and the setting box of the first and second distance thresholds are set on the labeling interface.</p><p id="p-0073" num="0087">In order to improve the efficiency and speed of transmission the results of labeling the laser points, in an embodiment of the present application, the laser points in the received laser point cloud are provided with serial numbers, where different laser points have different serial numbers; after the operation of labeling the laser points which need to be labeled in the 3D scene is completed, the serial numbers and the labeling results of the laser points are packaged and transmitted. Since the transmission data does not contain the coordinate information of the laser points, the data transmission amount is reduced, and the data transmission efficiency is improved to certain extent. For example, when the laser points are labeled by different colors, the serial numbers of the laser points and the color values of the labeling colors are packaged and transmitted.</p><p id="p-0074" num="0088">Based upon the same concept of the method of labeling laser point cloud provided above, some embodiments of the present application further provide a device of labeling laser point cloud. The structure of the device is as shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, which includes:<ul id="ul0007" list-style="none">    <li id="ul0007-0001" num="0000">    <ul id="ul0008" list-style="none">        <li id="ul0008-0001" num="0089">a receiving unit <b>61</b> configured to receive data of a laser point cloud;</li>        <li id="ul0008-0002" num="0090">a constructing unit <b>62</b> configured to construct a 3D scene and establish a 3D coordinate system corresponding to the 3D scene;</li>        <li id="ul0008-0003" num="0091">a converting unit <b>63</b> configured to convert a coordinate of each laser point in the laser point cloud into a 3D coordinate in the 3D coordinate system;</li>        <li id="ul0008-0004" num="0092">a mapping unit <b>64</b> configured to map laser points included in the laser point cloud into the 3D scene respectively according to the 3D coordinate of the laser points;</li>        <li id="ul0008-0005" num="0093">a labeling unit <b>65</b> configured to label the laser points in the 3D scene.</li>    </ul>    </li></ul></p><p id="p-0075" num="0094">The constructing unit <b>62</b> is specifically configured to construct the 3D scene by the WebGL technique.</p><p id="p-0076" num="0095">In some embodiments of the present application, the constructing unit <b>62</b> may also specifically construct the 3D scene by the OSG (Open Scene Graph) or the STK (Satellite Tool Kit). The way of constructing the 3D scene is not limited strictly by the present application.</p><p id="p-0077" num="0096">In order to differentiate the labeling results from the different types of laser points, the labeling unit <b>65</b> uses different labeling manners to label the different types of laser points. The labeling unit <b>65</b> specifically includes:<ul id="ul0009" list-style="none">    <li id="ul0009-0001" num="0000">    <ul id="ul0010" list-style="none">        <li id="ul0010-0001" num="0097">a determining subunit (which is not illustrated on the figures) configured to determine types to which the laser points in the 3D scene belong;</li>        <li id="ul0010-0002" num="0098">a labeling subunit (which is not illustrated on the figures) configured to label the laser points by using labeling manners corresponding to the types to which the laser points belong, wherein different types correspond to different labeling manners.</li>    </ul>    </li></ul></p><p id="p-0078" num="0099">Since the laser point cloud contains a large number of laser points, in order to avoid the labeling omission and improve the comprehensiveness and the integrity of the labeling operation, in an embodiment of the present application, after the 3D scene is constructed, a camera is constructed in the 3D scene, and the laser points in the 3D scene are viewed by adjusting the position and the direction of the camera in the 3D scene, to ensure that the labeling personnel could view the laser points in 360 degrees in the 3D scene. The above device as shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref> further includes a camera constructing unit <b>66</b>, as shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>:</p><p id="p-0079" num="0100">the camera constructing unit <b>66</b> is configured to construct at least one camera in the 3D scene, to adjust an angle and a range of viewing the laser point cloud in the 3D scene by adjusting a position and a direction of the at least one camera in the 3D scene, during the process of labeling the laser points in the 3D scene.</p><p id="p-0080" num="0101">In order to further improve the accuracy of determining by the labeling personnel the types to which the laser points in the 3D scene belong, in some embodiments of the present application, the camera constructing unit <b>66</b> constructs at least two cameras in the 3D scene. The positions and the directions of the cameras are different at the same moment, so that the labeling personnel could view the laser points at different viewing angles to improve the accuracy of judging the types to which the laser points belong.</p><p id="p-0081" num="0102">The at least two cameras described above could be switched according to the user's selection. When the user selects one of the cameras, the position and the direction of the selected camera are taken as the corresponding angle and range for viewing the 3D scene presented to the user; and for the unselected camera, the angle and range for viewing the 3D scene corresponding to the unselected camera in the 3D scene are presented by way of thumbnails.</p><p id="p-0082" num="0103">In a practical application, the number of the laser points belonging to a same target object is relatively large and these laser points are relatively centralized. In the case that the labeling personnel determines a plurality of laser points belonging to a target object in a one-by-one labeling manner, the operation speed is relatively slow. Thus, in the technical solution of the present application, in order to further improve the speed of labeling laser points, the determining subunit could label a plurality of laser points of a same type uniformly at one or more times. Therefore, the specific implementation of the determining subunit may be achieved by two following modes.</p><p id="p-0083" num="0104">First mode: the determining subunit is configured to: generate a 3D selecting box according to first size information set by a user; move a position of the 3D selecting box in the 3D scene according to a first moving instruction input by the user; when receiving a labeling instruction carrying a target type, take laser points currently falling within the 3D selecting box as target laser points and take the target type in the labeling instruction as a type to which the target laser points belong.</p><p id="p-0084" num="0105">In an embodiment of the present application, the size of the 3D selecting box could be adjusted by the labeling personnel in real time.</p><p id="p-0085" num="0106">Second mode: the determining subunit is configured to: generate a ray by taking the position of a camera of the at least one camera as a start point and taking a screen position point selected by the user as an end point; take laser points meeting the following conditions as target laser points: a distance of a laser point from the start point is between a first distance threshold and a second distance threshold, and a vertical distance of the laser point from the ray is less than or equal to a third distance threshold; when receiving a labeling instruction carrying a target type, take the target type in the labeling instruction as a type to which the target laser points belong.</p><p id="p-0086" num="0107">Where the first distance threshold, the second distance threshold and the third distance threshold could be set flexibly according to actual requirements. In general case, the larger the cubical space constituted by the laser points belonging to a same type around the screen position point is, the larger the values of the first distance threshold, the second distance threshold and the third distance threshold are.</p><p id="p-0087" num="0108">In some embodiments, the labeling instruction further contains a labeling color corresponding to the target type, and the labeling subunit is specifically configured to: set a color of the target laser points to be the labeling color in the labeling instruction, where different target types correspond to different labeling colors;</p><p id="p-0088" num="0109">or, the labeling instruction further contains a labeling box pattern corresponding to the target type, and the labeling subunit is specifically configured to: generate a 3D labeling box according to the labeling box pattern, and label each target laser point respectively by using the 3D labeling box, where different target types correspond to different labeling box patterns.</p><p id="p-0089" num="0110">In some embodiments, the labeling subunit sets the color of the target laser point to be the labeling color in the labeling instruction, which is configured to:<ul id="ul0011" list-style="none">    <li id="ul0011-0001" num="0000">    <ul id="ul0012" list-style="none">        <li id="ul0012-0001" num="0111">generate a 3D coloring box according to second size information set by the user;</li>        <li id="ul0012-0002" num="0112">move the 3D coloring box according to a second moving instruction input by the user, to paint the color of the target laser points falling within the 3D coloring box during the moving process into the labeling color in the labeling instruction.</li>    </ul>    </li></ul></p><p id="p-0090" num="0113">In order to implement the color modification to the target laser points, in some embodiments of the present application, the 3D coloring box has a brush function and an eraser function. When the attribute of the 3D coloring box is set to be the brush function, the color of the laser points is painted into the color corresponding to the type to which the laser points belong by the 3D coloring box; and when the attribute of the 3D coloring box is set to be the eraser function, the color of the laser points of which the color needs to be modified is erased by the 3D coloring box. In an embodiment of the present application, a brush button and an eraser button could be preset. When the user clicks on the brush button, the 3D coloring box is set with the brush function; and when the user clicks on the eraser button, the 3D coloring box is set with the eraser function. The labeling subunit is further configured to: set the 3D coloring box with an eraser attribute according to an erasure instruction input by the user; move the 3D coloring box according to a third moving instruction input by the user, to erase the color of the laser points falling within the 3D coloring box during the moving process.</p><p id="p-0091" num="0114">The labeling subunit is configured to: determine a labeling color corresponding to the type to which the laser points belong according to a preset correspondence between types and labeling colors, and set a color of the laser points to be the labeling color corresponding to the type to which the laser points belong; or determine a labeling box pattern corresponding to the type to which the laser points belong according to a preset correspondence between types and labeling box patterns, generate a labeling box according to the labeling box pattern corresponding to the type to which the laser points belong, and label the laser points by using the labeling box.</p><p id="p-0092" num="0115">In some embodiments, the device of labeling the laser point cloud could be set on a browser, where after receiving the data of the laser point cloud sent by a server, the browser labels the received laser point cloud by the device of labeling the laser point cloud and feeds back the labeling result to the server.</p><p id="p-0093" num="0116">Based upon the same concept of the method of labeling laser point cloud provided above, some embodiments of the present application further provide a device of labeling laser point cloud. The structure of the device is as shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, which includes: a processor <b>81</b> and at least one memory <b>82</b>, where the at least one memory <b>82</b> stores at least one machine executable instruction.</p><p id="p-0094" num="0117">The processor <b>81</b> executes the at least one machine executable instruction to: receive data of a laser point cloud; construct a 3D scene and establish a 3D coordinate system corresponding to the 3D scene; convert a coordinate of each laser point in the laser point cloud into a 3D coordinate in the 3D coordinate system; map laser points included in the laser point cloud into the 3D scene respectively according to the 3D coordinate of the laser points; label the laser points in the 3D scene.</p><p id="p-0095" num="0118">In some embodiments, the processor <b>81</b> executes the at least one machine executable instruction to label the laser point in the 3D scene, which comprises: determine types to which the laser points in the 3D scene belong; label the laser points by using labeling manners corresponding to the types to which the laser points belong, where different types correspond to different labeling manners.</p><p id="p-0096" num="0119">In some embodiments, the processor <b>81</b> executes the at least one machine executable instruction to construct the 3D scene and then further to: construct at least one camera in the 3D scene, to adjust an angle and a range of viewing the laser point cloud in the 3D scene by adjusting a position and a direction of the at least one camera in the 3D scene, during the process of labeling the laser points in the 3D scene.</p><p id="p-0097" num="0120">In some embodiments, the processor <b>81</b> executes the at least one machine executable instruction to determine types to which the laser points in the 3D scene belong according to a first mode or a second mode:</p><p id="p-0098" num="0121">First mode: generating a 3D selecting box according to first size information set by a user; moving a position of the 3D selecting box in the 3D scene according to a first moving instruction input by the user; when receiving a labeling instruction carrying a target type, taking laser points currently falling within the 3D selecting box as target laser points and taking the target type in the labeling instruction as a type to which the target laser points belong;</p><p id="p-0099" num="0122">Second mode: generating a ray by taking the position of a camera of the at least one camera as a start point and taking a screen position point selected by the user as an end point; taking laser points meeting the following conditions as target laser points: a distance of a laser point from the start point is between a first distance threshold and a second distance threshold, and a vertical distance of the laser point from the ray is less than or equal to a third distance threshold; when receiving a labeling instruction carrying a target type, taking the target type in the labeling instruction as a type to which the target laser points belong.</p><p id="p-0100" num="0123">In some embodiments, the labeling instruction further contains a labeling color corresponding to the target type, then the processor <b>81</b> executes the at least one machine executable instruction to label the laser points according to the labeling manner corresponding to the type to which the laser points belong, which comprises: set a color of the target laser points to be the labeling color in the labeling instruction, where different target types correspond to different labeling colors;</p><p id="p-0101" num="0124">or, the labeling instruction further contains a labeling box pattern corresponding to the target type, then the processor executes the at least one machine executable instruction to label the laser points according to the labeling manner corresponding to the type to which the laser points belong, which specifically comprises: generate a 3D labeling box according to the labeling box pattern, and label each target laser point respectively by using the 3D labeling box, where different target types correspond to different labeling box patterns;</p><p id="p-0102" num="0125">where the processor <b>81</b> executes the at least one machine executable instruction to set the color of the target laser points to be the labeling color in the labeling instruction, which comprises: generate a 3D coloring box according to second size information set by the user; move the 3D coloring box according to a second moving instruction input by the user, to paint the color of the target laser points falling within the 3D coloring box during the moving process into the labeling color in the labeling instruction.</p><p id="p-0103" num="0126">In some embodiments, the processor <b>81</b> further executes the at least one machine executable instruction to: set the 3D coloring box with an eraser attribute according to an erasure instruction input by the user; move the 3D coloring box according to a third moving instruction input by the user, to erase the color of the laser points falling within the 3D coloring box during the moving process.</p><p id="p-0104" num="0127">In some embodiments, the processor <b>81</b> executes the at least one machine executable instruction to label the laser points according to the labeling manner corresponding to the type to which the laser points belong, which comprises: determine a labeling color corresponding to the type to which the laser points belong according to a preset correspondence between types and labeling colors, and set a color of the laser point to be the labeling color corresponding to the type to which the laser points belong; or determine a labeling box pattern corresponding to the type to which the laser points belong according to a preset correspondence between types and labeling box patterns, generate a 3D labeling box according to the labeling box pattern corresponding to the type to which the laser points belong, and label the laser points by using the 3D labeling box.</p><p id="p-0105" num="0128">It should be understood by those skilled in the art that the embodiments of the present application can provide methods, systems and computer program products. Thus, the present application can take the form of hardware embodiments alone, application software embodiments alone, or embodiments combining the application software and hardware aspects. Also, the present application can take the form of computer program products implemented on one or more computer usable storage mediums (including but not limited to magnetic disk memories, CD-ROMs, optical memories and the like) containing computer usable program codes therein.</p><p id="p-0106" num="0129">The present application is described by reference to the flow charts and/or the box diagrams of the methods, the devices (systems) and the computer program products according to the embodiments of the present application. It should be understood that each process and/or box in the flow charts and/or the box diagrams, and a combination of processes and/or blocks in the flow charts and/or the box diagrams can be implemented by the computer program instructions. These computer program instructions can be provided to a general-purpose computer, a dedicated computer, an embedded processor, or a processor of another programmable data processing device to produce a machine, so that an apparatus for implementing the functions specified in one or more processes of the flow charts and/or one or more blocks of the box diagrams is produced by the instructions executed by the computer or the processor of another programmable data processing device.</p><p id="p-0107" num="0130">These computer program instructions can also be stored in a computer readable memory which is capable of guiding the computer or another programmable data processing device to operate in a particular way, so that the instructions stored in the computer readable memory produce a manufacture including the instruction apparatus which implements the functions specified in one or more processes of the flow charts and/or one or more blocks of the box diagrams.</p><p id="p-0108" num="0131">These computer program instructions can also be loaded onto the computer or another programmable data processing device, so that a series of operation steps are performed on the computer or another programmable device to produce the computer-implemented processing. Thus, the instructions executed on the computer or on another programmable device provide steps for implementing the functions specified in one or more processes of the flow charts and/or one or more blocks of the box diagrams.</p><p id="p-0109" num="0132">Although the preferred embodiments of the present application have been described, those skilled in the art can make additional alterations and modifications to these embodiments once they learn about the basic creative concepts. Thus, the attached claims are intended to be interpreted to include the preferred embodiments as well as all the alterations and modifications falling within the scope of the present application.</p><p id="p-0110" num="0133">Evidently those skilled in the art can make various modifications and variations to the present application without departing from the spirit and scope of the present application. Thus, the present application is also intended to encompass these modifications and variations therein as long as these modifications and variations to the present application come into the scope of the claims of the present application and their equivalents.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method of labeling a laser point cloud, comprising:<claim-text>receiving, by a device for labeling laser point clouds, data of the laser point cloud;</claim-text><claim-text>constructing a 3D scene and establishing a 3D coordinate system corresponding to the 3D scene;</claim-text><claim-text>converting a coordinate of each laser point in the laser point cloud into a 3D coordinate in the 3D coordinate system;</claim-text><claim-text>mapping laser points in the laser point cloud into the 3D scene respectively according to the 3D coordinates of the laser points;</claim-text><claim-text>labeling the laser points in the 3D scene, in which labeling the laser points in the 3D scene comprises:</claim-text><claim-text>determining types to which the laser points in the 3D scene belong according to a first mode comprising:<claim-text>when receiving a labeling instruction including a target type, taking laser points currently falling within a 3D selecting box as target laser points and taking the target type in the labeling instruction as a type to which the target laser points belong; and</claim-text></claim-text><claim-text>labeling the laser points by using labeling manners corresponding to the types to which the laser points belong, wherein different types correspond to different labeling manners.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein determining the types to which the laser points in the 3D scene belong is according to a second mode, comprising:<claim-text>generating a ray between a start point and an end point;</claim-text><claim-text>taking laser points meeting the following conditions as target laser points: a distance of a laser point from the start point is between a first distance threshold and a second distance threshold, and a distance of the laser point from the ray is less than or equal to a third distance threshold; and</claim-text><claim-text>when receiving a labeling instruction including a target type, taking the target type in the labeling instruction as a type to which the target laser points belong.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first mode further comprises:<claim-text>generating a 3D selecting box according to a first size information; and</claim-text><claim-text>moving a position of the 3D selecting box in the 3D scene in response to a first moving instruction.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>constructing at least one camera in the 3D scene, to adjust an angle and a range of viewing the laser point cloud in the 3D scene by adjusting a position and a direction of the at least one camera in the 3D scene, during the process of labeling the laser points in the 3D scene.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the labeling instruction includes a labeling box pattern corresponding to the target type, and labeling the laser points by using the labeling manner corresponding to the type to which the laser points belong comprises:<claim-text>generating a 3D labeling box according to the labeling box pattern; and labeling each target laser point respectively by using the 3D labeling box, wherein different target types correspond to different labeling box patterns.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the labeling instruction includes a labeling color corresponding to the target type, and labeling the laser points according to the labeling manners corresponding to the type to which the laser points belong comprises:<claim-text>setting a color of the target laser points to be the labeling color in the labeling instruction, wherein different target types correspond to different labeling colors.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein setting the color of the target laser points to be the labeling color in the labeling instruction comprises:<claim-text>generating a 3D coloring box according to second size information; and</claim-text><claim-text>moving the 3D coloring box according to a second moving instruction to paint the color of the target laser points falling within the 3D coloring box during the moving process into the labeling color in the labeling instruction.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, further comprising:<claim-text>setting the 3D coloring box with an eraser attribute according to an erasure instruction; and</claim-text><claim-text>moving the 3D coloring box according to a third moving instruction to erase the color of the laser points falling within the 3D coloring box during the moving process.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein labeling the laser point according to the labeling manner corresponding to the type to which the laser points belong comprises:<claim-text>determining a labeling color corresponding to the type to which the laser points belong according to a preset correspondence between types and labeling colors, and setting a color of the laser points to be the labeling color corresponding to the type to which the laser points belong; or</claim-text><claim-text>determining a labeling box pattern corresponding to the type to which the laser points belong according to a preset correspondence between types and labeling box patterns, generating a 3D labeling box according to the labeling box pattern corresponding to the type to which the laser points belong, and labeling the laser points by using the 3D labeling box.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. A device for labeling a laser point cloud, configured to:<claim-text>receive data of the laser point cloud;</claim-text><claim-text>construct a 3D scene and establish a 3D coordinate system corresponding to the 3D scene;</claim-text><claim-text>convert a coordinate of each laser point in the laser point cloud into a 3D coordinate in the 3D coordinate system;</claim-text><claim-text>map laser points included in the laser point cloud into the 3D scene using the 3D coordinates of the laser points;<claim-text>determine types to which the laser points in the 3D scene belong according to a first mode comprising:</claim-text><claim-text>when receiving a labeling instruction including a target type, take laser points currently falling within a 3D selecting box as target laser points and take the target type in the labeling instruction as a type to which the target laser points belong; and</claim-text><claim-text>label the laser points by using labeling manners corresponding to the types to which the laser points belong, wherein different types correspond to different labeling manners.</claim-text></claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The device according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein to determine the types to which the laser points in the 3D scene belong is according to a second mode, the device is configured to:<claim-text>generate a ray between a start point and an end point;</claim-text><claim-text>take laser points meeting the following conditions as target laser points: a distance of a laser point from the start point is between a first distance threshold and a second distance threshold, and a distance of the laser point from the ray is less than or equal to a third distance threshold; and</claim-text><claim-text>when receiving a labeling instruction including a target type, take the target type in the labeling instruction as a type to which the target laser points belong.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The device according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein in the first mode the device is configured to:<claim-text>generate the 3D selecting box according to a first size information; and</claim-text><claim-text>move a position of the 3D selecting box in the 3D scene in response to a first moving instruction.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The device according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, configured to:<claim-text>construct at least one camera in the 3D scene, to adjust an angle and a range of viewing the laser point cloud in the 3D scene by adjusting a position and a direction of the at least one camera in the 3D scene, during the process of labeling the laser points in the 3D scene.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The device according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the labeling instruction includes a labeling box pattern corresponding to the target type, the device is configured to:<claim-text>generate a 3D labeling box according to the labeling box pattern; and</claim-text><claim-text>label each target laser point respectively by using the 3D labeling box, wherein different target types correspond to different labeling box patterns.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The device according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the labeling instruction includes a labeling color corresponding to the target type, the device is configured to:<claim-text>set a color of the target laser points to be the labeling color in the labeling instruction, wherein different target types correspond to different labeling colors.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The device according to <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein setting the color of the target laser points to be the labeling color in the labeling instruction, the device is configured to:<claim-text>generate a 3D coloring box according to a second size information; and</claim-text><claim-text>move the 3D coloring box according to a second moving instruction to paint the color of the target laser points falling within the 3D coloring box during the moving process into the labeling color in the labeling instruction.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The device according to <claim-ref idref="CLM-00016">claim 16</claim-ref>, further configured to:<claim-text>set the 3D coloring box with an eraser attribute according to an erasure instruction; and</claim-text><claim-text>move the 3D coloring box according to a third moving instruction, to erase the color of the laser points falling within the 3D coloring box during the moving process.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The device according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, further configured to:<claim-text>determine a labeling color corresponding to the type to which the laser points belong according to a preset correspondence between types and labeling colors, and set a color of the laser points to be the labeling color corresponding to the type to which the laser points belong; or</claim-text><claim-text>determine a labeling box pattern corresponding to the type to which the laser points belong according to a preset correspondence between types and labeling box patterns, generating a 3D labeling box according to the labeling box pattern corresponding to the type to which the laser points belong, and labeling the laser points by using the 3D labeling box.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. A non-transitory machine-useable storage medium embodying instructions which, when executed by a machine, cause the machine to:<claim-text>receive data of a laser point cloud;</claim-text><claim-text>construct a 3D scene and establish a 3D coordinate system corresponding to the 3D scene;</claim-text><claim-text>convert a coordinate of each laser point in the laser point cloud into a 3D coordinate in the 3D coordinate system;</claim-text><claim-text>map laser points in the laser point cloud into the 3D scene respectively according to the 3D coordinates of the laser points;</claim-text><claim-text>determine types to which the laser points in the 3D scene belong;</claim-text><claim-text>receive a labeling instruction including a target type;</claim-text><claim-text>take laser points currently falling within a 3D selecting box as target laser points;</claim-text><claim-text>take the target type in the labeling instruction as a type to which the target laser points belong; and</claim-text><claim-text>label the laser points according to labeling manners corresponding to the types to which the laser points belong, wherein different types correspond to different labeling manners.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The non-transitory machine-useable storage medium according to <claim-ref idref="CLM-00019">claim 19</claim-ref>, causing the machine to:<claim-text>generate a 3D selecting box according to a first size information; and</claim-text><claim-text>move a position of the 3D selecting box in the 3D scene in response to a first moving instruction.</claim-text></claim-text></claim></claims></us-patent-application>